<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 198]
- [cs.AI](#cs.AI) [Total: 133]
- [cs.CR](#cs.CR) [Total: 54]
- [cs.LG](#cs.LG) [Total: 296]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Are you sure? Measuring models bias in content moderation through uncertainty](https://arxiv.org/abs/2509.22699)
*Alessandra Urbinati,Mirko Lai,Simona Frenda,Marco Antonio Stranisci*

Main category: cs.CL

TL;DR: This paper presents an unsupervised approach using conformal prediction to measure bias in language model-based content moderation systems against vulnerable groups, showing that high accuracy doesn't always correlate with high confidence.


<details>
  <summary>Details</summary>
Motivation: Language model-based classifiers for content moderation perpetuate racial and social biases, and existing metrics based on performance (like F1 score) don't adequately measure fairness against vulnerable groups.

Method: Unsupervised approach using conformal prediction technique to compute model uncertainty when classifying messages annotated by people from vulnerable groups (women and non-white annotators).

Result: Some pre-trained models predict labels from minority groups with high accuracy but low confidence, revealing biases that performance metrics alone don't capture.

Conclusion: Measuring model confidence through uncertainty analysis helps identify which annotator groups are better represented in pre-trained models and guides debiasing before deployment.

Abstract: Automatic content moderation is crucial to ensuring safety in social media.
Language Model-based classifiers are being increasingly adopted for this task,
but it has been shown that they perpetuate racial and social biases. Even if
several resources and benchmark corpora have been developed to challenge this
issue, measuring the fairness of models in content moderation remains an open
issue. In this work, we present an unsupervised approach that benchmarks models
on the basis of their uncertainty in classifying messages annotated by people
belonging to vulnerable groups. We use uncertainty, computed by means of the
conformal prediction technique, as a proxy to analyze the bias of 11 models
against women and non-white annotators and observe to what extent it diverges
from metrics based on performance, such as the $F_1$ score. The results show
that some pre-trained models predict with high accuracy the labels coming from
minority groups, even if the confidence in their prediction is low. Therefore,
by measuring the confidence of models, we are able to see which groups of
annotators are better represented in pre-trained models and lead the debiasing
process of these models before their effective use.

</details>


### [2] [AccessEval: Benchmarking Disability Bias in Large Language Models](https://arxiv.org/abs/2509.22703)
*Srikant Panda,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.CL

TL;DR: AccessEval benchmark evaluates 21 LLMs across 6 domains and 9 disability types, revealing that disability-aware queries receive more negative, stereotyped, and factually inaccurate responses compared to neutral queries, with hearing, speech, and mobility disabilities being disproportionately affected.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate disparities in how LLMs handle real-life queries across various disability contexts, as these models are increasingly deployed across diverse domains but may exhibit biases against disabled users.

Method: Introduces AccessEval benchmark evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9 disability types using paired Neutral and Disability-Aware Queries, with metrics for sentiment, social perception, and factual accuracy.

Result: Responses to disability-aware queries have more negative tone, increased stereotyping, and higher factual errors compared to neutral queries. These effects vary by domain and disability type, with hearing, speech, and mobility disabilities most affected, reflecting embedded ableism in model behavior.

Conclusion: The study illuminates how biases in LLMs can translate into tangible harms for disabled users, bridging the gap between technical evaluation and user impact, and reinforcing the importance of bias mitigation in day-to-day applications.

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse domains
but often exhibit disparities in how they handle real-life queries. To
systematically investigate these effects within various disability contexts, we
introduce \textbf{AccessEval (Accessibility Evaluation)}, a benchmark
evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9
disability types using paired Neutral and Disability-Aware Queries. We
evaluated model outputs with metrics for sentiment, social perception, and
factual accuracy.
  Our analysis reveals that responses to disability-aware queries tend to have
a more negative tone, increased stereotyping, and higher factual error compared
to neutral queries. These effects show notable variation by domain and
disability type, with disabilities affecting hearing, speech, and mobility
disproportionately impacted. These disparities reflect persistent forms of
ableism embedded in model behavior.
  By examining model performance in real-world decision-making contexts, we
better illuminate how such biases can translate into tangible harms for
disabled users. This framing helps bridges the gap between technical evaluation
and user impact, reinforcing importance of bias mitigation in day-to-day
applications. Our dataset is publicly available at:
https://huggingface.co/datasets/Srikant86/AccessEval

</details>


### [3] [RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval](https://arxiv.org/abs/2509.22713)
*Kaishuai Xu,Wenjun Hou,Yi Cheng,Wenjie Li*

Main category: cs.CL

TL;DR: RAR² is a joint learning framework that enhances both reasoning-augmented retrieval and retrieval-augmented reasoning for medical QA, outperforming standard RAG approaches.


<details>
  <summary>Details</summary>
Motivation: Standard RAG struggles with complex medical questions requiring intensive reasoning, as surface-level queries fail to capture true knowledge needs. Existing methods lack explicit reasoning process modeling.

Method: Proposes RAR² framework that constructs thought processes to uncover implicit knowledge requirements, uses DPO training on mixed preference pairs, and implements test-time scaling strategies.

Result: RAR² demonstrates effectiveness across multiple biomedical QA datasets, outperforming RAG baselines with and without fine-tuning.

Conclusion: The joint reasoning-retrieval approach in RAR² successfully addresses limitations of standard RAG for complex medical reasoning tasks.

Abstract: Large Language Models (LLMs) have shown promising performance on diverse
medical benchmarks, highlighting their potential in supporting real-world
clinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key
approach for mitigating knowledge gaps and hallucinations by incorporating
external medical information. However, RAG still struggles with complex medical
questions that require intensive reasoning, as surface-level input often fails
to reflect the true knowledge needs of the task. Existing methods typically
focus on refining queries without explicitly modeling the reasoning process,
limiting their ability to retrieve and integrate clinically relevant knowledge.
In this work, we propose RAR$^2$, a joint learning framework that improves both
Reasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$
constructs a thought process to uncover implicit knowledge requirements and
uses it to guide retrieval and answer generation. We build a training dataset
of mixed preference pairs and apply Direct Preference Optimization (DPO) to
train the model. Moreover, we design two test-time scaling strategies to
explore the boundaries of our framework. Experiments demonstrate the
effectiveness of RAR$^2$ across several biomedical question answering datasets,
outperforming RAG baselines with or without fine-tuning.

</details>


### [4] [TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?](https://arxiv.org/abs/2509.22715)
*Jiho Park,Jongyoon Song,Minjin Choi,Kyuho Heo,Taehun Huh,Ji Won Kim*

Main category: cs.CL

TL;DR: TRUEBench is a new benchmark for evaluating LLM-based productivity assistants that addresses limitations of existing benchmarks by incorporating multilingual inputs, implicit constraints, and multi-turn dialogues with rigorous evaluation criteria.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to adequately evaluate LLMs' real-world instruction-following capabilities due to insufficient multilinguality, inability to capture implicit constraints, and overlooking multi-turn dialogue complexities.

Method: Developed TRUEBench with prompts across 12 languages, intra-instance multilingual instructions, rigorous evaluation criteria for explicit/implicit constraints, complex multi-turn dialogues with accumulating constraints and context switches, and LLM-based constraint validation.

Result: TRUEBench proved significantly more challenging than existing benchmarks - even strong models like OpenAI o1 achieved only 69.07% overall pass rate, demonstrating the benchmark's demanding nature.

Conclusion: TRUEBench provides a realistic and rigorous assessment of LLMs in practical productivity settings, effectively highlighting both their capabilities and limitations in real-world usage scenarios.

Abstract: Large language models (LLMs) are increasingly integral as productivity
assistants, but existing benchmarks fall short in rigorously evaluating their
real-world instruction-following capabilities. Current benchmarks often (i)
lack sufficient multilinguality, (ii) fail to capture the implicit constraints
inherent in user requests, and (iii) overlook the complexities of multi-turn
dialogue. To address these critical gaps and provide a more realistic
assessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation
Benchmark)1, a novel benchmark specifically designed for LLM-based productivity
assistants. TRUEBench distinguishes itself by featuring input prompts across 12
languages, incorporating intra-instance multilingual instructions, employing
rigorous evaluation criteria to capture both explicit and implicit constraints,
and including complex multi-turn dialogue scenarios with both accumulating
constraints and context switches. Furthermore, to ensure reliability in
evaluation, we refined constraints using an LLM validator. Extensive
experiments demonstrate that TRUEBench presents significantly greater
challenges than existing benchmarks; for instance, a strong model like OpenAI
o1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and
realistic assessment of LLMs in practical productivity settings, highlighting
their capabilities and limitations.

</details>


### [5] [Multi-Modal Sentiment Analysis with Dynamic Attention Fusion](https://arxiv.org/abs/2509.22729)
*Sadia Abdulhalim,Muaz Albaghdadi,Moshiur Farazi*

Main category: cs.CL

TL;DR: DAF is a lightweight multimodal framework that combines text and acoustic features using adaptive attention, outperforming unimodal and static fusion methods on sentiment analysis without finetuning encoders.


<details>
  <summary>Details</summary>
Motivation: Traditional sentiment analysis relies only on text, missing important non-verbal cues like vocal tone and prosody that are essential for capturing true emotional intent.

Method: Dynamic Attention Fusion (DAF) combines frozen text embeddings from pretrained language models with acoustic features from speech encoders, using adaptive attention mechanism to weight each modality per utterance.

Result: DAF consistently outperforms static fusion and unimodal baselines on large multimodal benchmarks, with notable gains in F1-score and reductions in prediction error. Ablation studies confirm dynamic weighting is crucial for emotionally complex inputs.

Conclusion: By effectively integrating verbal and non-verbal information, DAF offers a more robust foundation for sentiment prediction with broader impact for affective computing applications including emotion recognition, mental health assessment, and human-computer interaction.

Abstract: Traditional sentiment analysis has long been a unimodal task, relying solely
on text. This approach overlooks non-verbal cues such as vocal tone and prosody
that are essential for capturing true emotional intent. We introduce Dynamic
Attention Fusion (DAF), a lightweight framework that combines frozen text
embeddings from a pretrained language model with acoustic features from a
speech encoder, using an adaptive attention mechanism to weight each modality
per utterance. Without any finetuning of the underlying encoders, our proposed
DAF model consistently outperforms both static fusion and unimodal baselines on
a large multimodal benchmark. We report notable gains in F1-score and
reductions in prediction error and perform a variety of ablation studies that
support our hypothesis that the dynamic weighting strategy is crucial for
modeling emotionally complex inputs. By effectively integrating verbal and
non-verbal information, our approach offers a more robust foundation for
sentiment prediction and carries broader impact for affective computing
applications -- from emotion recognition and mental health assessment to more
natural human computer interaction.

</details>


### [6] [Enabling Approximate Joint Sampling in Diffusion LMs](https://arxiv.org/abs/2509.22738)
*Parikshit Bansal,Sujay Sanghavi*

Main category: cs.CL

TL;DR: Proposes a lightweight sampler layer for diffusion language models to enable approximate joint sampling of multiple tokens in parallel, improving generation speed while maintaining distribution quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models generate text by unmasking tokens out of order, but parallel unmasking moves away from the true joint distribution, causing accuracy drops. Need to balance speed and distribution quality.

Method: Develop a lightweight single-layer sampler on top of existing diffusion LM. One full-model forward pass followed by multiple sampler-only passes to yield multiple unmasked tokens. Sampler trained to mimic exact joint sampling from frozen full model.

Result: Achieves MAUVE score of 0.87 (vs 0.31 baseline) when unmasking 4 tokens per denoising step. Effective on both pretrained (Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models for language modeling and math/coding tasks.

Conclusion: The proposed approximate joint sampling method successfully enables parallel token generation while maintaining high distribution quality, significantly improving generation speed without major accuracy loss.

Abstract: In autoregressive language models, each token is sampled by conditioning on
all the past tokens; the overall string has thus been sampled from the correct
underlying joint distribution represented by the model. In contrast, masked
diffusion language models generate text by unmasking tokens out of order and
potentially in parallel. Generating an overall string sampled from the correct
underlying joint distribution would (again) require exactly one token unmasking
in every full-model forward pass. The more tokens unmasked in parallel, the
further away the string is from the true joint; this can be seen in the
resulting drop in accuracy (but, increase in speed). In this paper we devise a
way to {\em approximately} sample multiple tokens from the joint distribution
in a single full-model forward pass; we do so by developing a new lightweight
single-layer ``sampler" on top of an existing large diffusion LM. One forward
pass of the full model can now be followed by multiple forward passes of only
this sampler layer, to yield multiple unmasked tokens. Our sampler is trained
to mimic exact joint sampling from the (frozen) full model. We show the
effectiveness of our approximate joint sampling for both pretrained-only
(Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models on language
modeling and math \& coding tasks. When four tokens are unmasked for each
full-model denoising step, our sampling algorithm achieves a MAUVE score of
0.87 (vs marginal baseline of 0.31) with respect to the true joint
distribution.

</details>


### [7] [Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models](https://arxiv.org/abs/2509.22739)
*Sasha Cui,Zhongren Chen*

Main category: cs.CL

TL;DR: PAS is an automated activation steering method that enables precise control over language models without manual prompt engineering or feature annotation, providing fast and lightweight steering vectors that can be easily trained and deployed.


<details>
  <summary>Details</summary>
Motivation: Existing post-training methods like weight-based steering are expensive and time-consuming, while prompt-based steering lacks precision and requires manual trial-and-error. Activation steering offers a cheaper alternative but current methods need hand-crafted prompts or labor-intensive annotation.

Method: PAS is a family of fully automated activation steering methods that work with any labeled dataset without requiring prompt construction, feature labeling, or human intervention. It constructs lightweight activation vectors that can be cheaply trained and stored.

Result: PAS reliably improves performance on behavior tasks (10.1% on Bias, 5.2% on Morality, 34.8% on Alignment) but not on intelligence-oriented tasks. It delivers additional gains on top of In-Context Learning and Supervised Fine-Tuning.

Conclusion: PAS provides a practical, automated alternative for LM post-training, characterizing where activation steering helps and where it fails, offering fast, lightweight control that can be easily deployed.

Abstract: Language models (LMs) are typically post-trained for desired capabilities and
behaviors via weight-based or prompt-based steering, but the former is
time-consuming and expensive, and the latter is not precisely controllable and
often requires manual trial-and-error. While activation steering (AS) promises
a cheap, fast, and controllable alternative to the two existing post-training
methods, current AS techniques require hand-crafted prompt pairs or
labor-intensive feature annotation, making them more inconvenient than the
plug-and-play methods such as Reinforcement Learning (RL) and Supervised
Fine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of
fully automated methods that make AS readily usable with any given labeled
dataset, with no need for prompt construction, feature labeling, or human
intervention. We evaluate PAS on three open-weight models
(Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks;
we find that PAS reliably improves performance for behavior tasks, but not for
intelligence-oriented tasks. The introspective variant (iPAS) delivers the
strongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8%
on Alignment). We also show PAS delivers additional gains on top of In-Context
Learning (ICL) and SFT. PAS constructs a fast, lightweight activation vector
that can be cheaply trained, easily stored, and activated at will. Our results
provide a characterization of where AS helps, where it fails, and how to deploy
it as a practical, automated LM post-training option.

</details>


### [8] [MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions](https://arxiv.org/abs/2509.22750)
*Jeonghyun Park,Ingeol Baek,Seunghyun Yoon,Haeun Jang,Aparna Garimella,Akriti Jain,Nedim Lipka,Hwanhee Lee*

Main category: cs.CL

TL;DR: MIRAGE benchmark introduces ambiguous multi-hop QA where models must handle multiple ambiguous reasoning paths, showing current LLMs struggle with this challenge.


<details>
  <summary>Details</summary>
Motivation: Real-world multi-hop QA involves inherent ambiguity where multiple reasoning paths emerge from single questions, requiring ambiguity resolution at every step - a challenge current LLMs fail to handle properly.

Method: Created MIRAGE benchmark with 1,142 ambiguous multi-hop questions categorized by ambiguity types (syntactic, general, semantic), verified through multi-LLM pipeline. Proposed CLARION multi-agent framework to address this challenge.

Result: Experiments show state-of-the-art models struggle significantly on MIRAGE, confirming that combining ambiguity resolution with multi-step inference is a distinct and challenging problem.

Conclusion: CLARION framework significantly outperforms existing approaches on MIRAGE, establishing a robust baseline and paving way for more adaptive reasoning systems that can handle ambiguity in multi-hop QA.

Abstract: Real-world Multi-hop Question Answering (QA) often involves ambiguity that is
inseparable from the reasoning process itself. This ambiguity creates a
distinct challenge, where multiple reasoning paths emerge from a single
question, each requiring independent resolution. Since each sub-question is
ambiguous, the model must resolve ambiguity at every step. Thus, answering a
single question requires handling multiple layers of ambiguity throughout the
reasoning chain. We find that current Large Language Models (LLMs) struggle in
this setting, typically exploring wrong reasoning paths and producing
incomplete answers. To facilitate research on multi-hop ambiguity, we introduce
MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE),
a benchmark designed to analyze and evaluate this challenging intersection of
ambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142
high-quality examples of ambiguous multi-hop questions, categorized under a
taxonomy of syntactic, general, and semantic ambiguity, and curated through a
rigorous multi-LLM verification pipeline. Our experiments reveal that even
state-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity
combined with multi-step inference is a distinct and significant challenge. To
establish a robust baseline, we propose CLarifying Ambiguity with a Reasoning
and InstructiON (CLARION), a multi-agent framework that significantly
outperforms existing approaches on MIRAGE, paving the way for more adaptive and
robust reasoning systems.

</details>


### [9] [ML2B: Multi-Lingual ML Benchmark For AutoML](https://arxiv.org/abs/2509.22768)
*Ekaterina Trofimova,Zosia Shamina,Maria Selifanova,Artem Zaitsev,Remi Savchuk,Maxim Minets,Daria Ozerova,Emil Sataev,Denis Zuenko,Andrey E. Ustyuzhanin*

Main category: cs.CL

TL;DR: ML2B is the first benchmark for evaluating multilingual machine learning code generation, covering 30 Kaggle competitions translated into 13 languages, revealing 15-45% performance degradation on non-English tasks.


<details>
  <summary>Details</summary>
Motivation: Existing ML code generation benchmarks are mainly restricted to English, overlooking the global and multilingual nature of ML research and practice.

Method: Created ML2B benchmark with 30 Kaggle competitions translated into 13 natural languages, covering tabular, text, and image data types, using AIDE automated framework for end-to-end assessment.

Result: Substantial 15-45% performance degradation on non-English tasks compared to English, highlighting challenges in multilingual representation learning for code generation.

Conclusion: The benchmark and evaluation framework are made available to facilitate future research in multilingual ML code generation, addressing critical gaps in current evaluation practices.

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating machine learning (ML) code, enabling end-to-end pipeline
construction from natural language instructions. However, existing benchmarks
for ML code generation are mainly restricted to English, overlooking the global
and multilingual nature of ML research and practice. To address this gap, we
present ML2B, the first benchmark for evaluating multilingual ML code
generation. ML2B consists of 30 Kaggle competitions translated into 13 natural
languages, covering tabular, text, and image data types, with structured
metadata and validated human-reviewed translations. For evaluation, we employ
AIDE, an automated framework for end-to-end assessment of data science
pipelines, and provide insights into cross-lingual model performance. Our
results reveal substantial 15-45% performance degradation on non-English tasks,
highlighting critical challenges in multilingual representation learning for
code generation. The benchmark, evaluation framework, and comprehensive results
are made available through our GitHub repository to facilitate future research
in multilingual ML code generation: https://github.com/enaix/ml2b.

</details>


### [10] [ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection](https://arxiv.org/abs/2509.22808)
*Mohamed Maged,Alhassan Ehab,Ali Mekky,Besher Hassan,Shady Shehata*

Main category: cs.CL

TL;DR: This paper introduces the first multi-dialect Arabic spoofed speech dataset and evaluates various TTS models to identify which produces the most challenging synthetic speech for spoof detection.


<details>
  <summary>Details</summary>
Motivation: With the rise of generative text-to-speech models, distinguishing real from synthetic speech has become challenging, especially for Arabic which has received limited research attention compared to English.

Method: Created multi-dialect Arabic spoofed speech dataset; evaluated TTS models using classifiers (embedding-based methods with classifier heads, classical ML on MFCC features, RawNet2 architecture), Mean Opinion Score from human ratings, and Word Error Rate from ASR processing.

Result: FishSpeech outperforms other TTS models in Arabic voice cloning on the Casablanca corpus, producing more realistic and challenging synthetic speech samples.

Conclusion: FishSpeech produces the most challenging synthetic speech for Arabic, but relying on a single TTS for dataset creation may limit generalizability.

Abstract: With the rise of generative text-to-speech models, distinguishing between
real and synthetic speech has become challenging, especially for Arabic that
have received limited research attention. Most spoof detection efforts have
focused on English, leaving a significant gap for Arabic and its many dialects.
In this work, we introduce the first multi-dialect Arabic spoofed speech
dataset. To evaluate the difficulty of the synthesized audio from each model
and determine which produces the most challenging samples, we aimed to guide
the construction of our final dataset either by merging audios from multiple
models or by selecting the best-performing model, we conducted an evaluation
pipeline that included training classifiers using two approaches: modern
embedding-based methods combined with classifier heads; classical machine
learning algorithms applied to MFCC features; and the RawNet2 architecture. The
pipeline further incorporated the calculation of Mean Opinion Score based on
human ratings, as well as processing both original and synthesized datasets
through an Automatic Speech Recognition model to measure the Word Error Rate.
Our results demonstrate that FishSpeech outperforms other TTS models in Arabic
voice cloning on the Casablanca corpus, producing more realistic and
challenging synthetic speech samples. However, relying on a single TTS for
dataset creation may limit generalizability.

</details>


### [11] [EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation](https://arxiv.org/abs/2509.22812)
*Kai Zhang,Christopher Malon,Lichao Sun,Martin Renqiang Min*

Main category: cs.CL

TL;DR: EditGRPO is a mixed-policy RL algorithm that optimizes radiology report generation using clinically motivated rewards, outperforming SFT and vanilla GRPO baselines with improved metrics and superior out-of-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs for radiology report generation use SFT objectives not explicitly aligned with clinical efficacy, lacking optimization for clinical effectiveness.

Method: EditGRPO integrates on-policy exploration with off-policy guidance through sentence-level corrections during training, addressing RL exploration and sampling efficiency issues.

Result: Applied to Qwen2.5-VL-3B MLLM, EditGRPO achieved 3.4% average improvement in CheXbert, GREEN, Radgraph, and RATEScore metrics across four chest X-ray datasets, with 5.9% gain on unseen datasets.

Conclusion: EditGRPO effectively optimizes radiology report generation for clinical efficacy through mixed-policy RL with clinically motivated rewards, demonstrating strong performance and generalization.

Abstract: Radiology report generation requires advanced medical image analysis,
effective temporal reasoning, and accurate text generation. Although recent
innovations, particularly multimodal large language models (MLLMs), have shown
improved performance, their supervised fine-tuning (SFT) objective is not
explicitly aligned with clinical efficacy. In this work, we introduce EditGRPO,
a mixed-policy reinforcement learning (RL) algorithm designed specifically to
optimize the generation through clinically motivated rewards. EditGRPO
integrates on-policy exploration with off-policy guidance by injecting
sentence-level detailed corrections during training rollouts. This mixed-policy
approach addresses the exploration dilemma and sampling efficiency issues
typically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with
supervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO
baselines, achieving an average improvement of 3.4% in CheXbert, GREEN,
Radgraph, and RATEScore metrics across four major chest X-ray report generation
datasets. Notably, EditGRPO also demonstrates superior out-of-domain
generalization, with an average performance gain of 5.9% on unseen datasets.

</details>


### [12] [Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning](https://arxiv.org/abs/2509.22824)
*Chi Ruan,Dongfu Jiang,Yubo Wang,Wenhu Chen*

Main category: cs.CL

TL;DR: Critique Reinforcement Learning (CRL) enhances standard RL by training models to generate critiques, improving both coding and general reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Standard RL focuses on response generation but lacks explicit critique mechanisms. Recent studies show benefits of teaching LLMs to critique, motivating CRL development.

Method: Propose CRL where model generates critiques for (question, solution) pairs, rewarded by alignment of judgment labels. Introduce Critique-Coder trained on hybrid RL+CRL data (20% CRL substitution).

Result: Critique-Coder consistently outperforms RL-only baselines, achieving over 60% on LiveCodeBench (v5) and better performance on BBEH logic reasoning tasks.

Conclusion: CRL effectively complements standard RL, enhancing general reasoning and critique abilities transferable across diverse tasks.

Abstract: Reinforcement Learning (RL) has emerged as a popular training paradigm,
particularly when paired with reasoning models. While effective, it primarily
focuses on generating responses and lacks mechanisms to explicitly foster
critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)
and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly
teaching LLMs how to critique. Motivated by them, we propose Critique
Reinforcement Learning (CRL), where the model is tasked with generating a
critique for a given (question, solution) pair. The reward is determined solely
by whether the final judgment label $c \in \{\texttt{True}, \texttt{False}\}$
of the generated critique aligns with the ground-truth judgment $c^*$. Building
on this point, we introduce \textsc{Critique-Coder}, which is trained on a
hybrid of RL and CRL by substituting 20\% of the standard RL data with CRL
data. We fine-tune multiple models (\textsc{Critique-Coder}) and evaluate them
on different benchmarks to show their advantages over RL-only models. We show
that \textsc{Critique-Coder} consistently outperforms RL-only baselines on all
the evaluated benchmarks. Notably, our \textsc{Critique-Coder-8B} can reach
over 60\% on LiveCodeBench (v5), outperforming other reasoning models like
DeepCoder-14B and GPT-o1. Beyond code generation, \textsc{Critique-Coder} also
demonstrates enhanced general reasoning abilities, as evidenced by its better
performance on logic reasoning tasks from the BBEH dataset. This indicates that
the application of CRL on coding datasets enhances general reasoning and
critique abilities, which are transferable across a broad range of tasks.
Hence, we believe that CRL works as a great complement to standard RL for LLM
reasoning.

</details>


### [13] [ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents](https://arxiv.org/abs/2509.22830)
*Hwan Chang,Yonghyun Jun,Hwanhee Lee*

Main category: cs.CL

TL;DR: ChatInject is a novel attack method that exploits LLM agents' chat template dependencies through structured malicious payloads and multi-turn persuasion dialogues, achieving significantly higher success rates than traditional prompt injection attacks.


<details>
  <summary>Details</summary>
Motivation: The growing deployment of LLM-based agents interacting with external environments creates new attack surfaces, particularly for indirect prompt injection where attackers embed malicious instructions in environment output.

Method: ChatInject formats malicious payloads to mimic native chat templates and uses multi-turn persuasion dialogues to prime agents into accepting suspicious actions, exploiting LLMs' instruction-following tendencies.

Result: ChatInject achieved average attack success rates of 32.05% on AgentDojo (vs 5.18% traditional) and 45.90% on InjecAgent (vs 15.13% traditional), with multi-turn variants reaching 52.33% success on InjecAgent. The attack shows strong transferability across models and bypasses existing defenses.

Conclusion: Current agent systems have significant vulnerabilities to chat-template-based attacks, and existing prompt-based defenses are largely ineffective, especially against multi-turn persuasion variants.

Abstract: The growing deployment of large language model (LLM) based agents that
interact with external environments has created new attack surfaces for
adversarial manipulation. One major threat is indirect prompt injection, where
attackers embed malicious instructions in external environment output, causing
agents to interpret and execute them as if they were legitimate prompts. While
previous research has focused primarily on plain-text injection attacks, we
find a significant yet underexplored vulnerability: LLMs' dependence on
structured chat templates and their susceptibility to contextual manipulation
through persuasive multi-turn dialogues. To this end, we introduce ChatInject,
an attack that formats malicious payloads to mimic native chat templates,
thereby exploiting the model's inherent instruction-following tendencies.
Building on this foundation, we develop a persuasion-driven Multi-turn variant
that primes the agent across conversational turns to accept and execute
otherwise suspicious actions. Through comprehensive experiments across frontier
LLMs, we demonstrate three critical findings: (1) ChatInject achieves
significantly higher average attack success rates than traditional prompt
injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13%
to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong
performance at average 52.33% success rate on InjecAgent, (2)
chat-template-based payloads demonstrate strong transferability across models
and remain effective even against closed-source LLMs, despite their unknown
template structures, and (3) existing prompt-based defenses are largely
ineffective against this attack approach, especially against Multi-turn
variants. These findings highlight vulnerabilities in current agent systems.

</details>


### [14] [Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems](https://arxiv.org/abs/2509.22845)
*Kai Hua,Zhiyuan Feng,Chongyang Tao,Rui Yan,Lu Zhang*

Main category: cs.CL

TL;DR: Proposes RSM-DCK model for multi-turn response selection that detects relevant parts of context and knowledge to improve dialogue matching performance.


<details>
  <summary>Details</summary>
Motivation: Existing retrieval-based dialogue systems use all context and knowledge content for matching, but much information is irrelevant due to topic shifts, which negatively impacts performance.

Method: Uses recent context as query to pre-select relevant context/knowledge at word/utterance levels, then interacts response candidate with selected content, and finally fuses context-response representation to post-select knowledge for confident matching.

Result: Achieves better performance than existing methods on two benchmark datasets and effectively detects relevant context and knowledge.

Conclusion: The proposed RSM-DCK model successfully addresses the problem of excessive irrelevant information in dialogue systems by detecting relevant parts of context and knowledge for improved response selection.

Abstract: Recently, knowledge-grounded conversations in the open domain gain great
attention from researchers. Existing works on retrieval-based dialogue systems
have paid tremendous efforts to utilize neural networks to build a matching
model, where all of the context and knowledge contents are used to match the
response candidate with various representation methods. Actually, different
parts of the context and knowledge are differentially important for recognizing
the proper response candidate, as many utterances are useless due to the topic
shift. Those excessive useless information in the context and knowledge can
influence the matching process and leads to inferior performance. To address
this problem, we propose a multi-turn \textbf{R}esponse \textbf{S}election
\textbf{M}odel that can \textbf{D}etect the relevant parts of the
\textbf{C}ontext and \textbf{K}nowledge collection (\textbf{RSM-DCK}). Our
model first uses the recent context as a query to pre-select relevant parts of
the context and knowledge collection at the word-level and utterance-level
semantics. Further, the response candidate interacts with the selected context
and knowledge collection respectively. In the end, The fused representation of
the context and response candidate is utilized to post-select the relevant
parts of the knowledge collection more confidently for matching. We test our
proposed model on two benchmark datasets. Evaluation results indicate that our
model achieves better performance than the existing methods, and can
effectively detect the relevant context and knowledge for response selection.

</details>


### [15] [Towards Generalizable Implicit In-Context Learning with Attention Routing](https://arxiv.org/abs/2509.22854)
*Jiaqian Li,Yanshu Li,Ligong Han,Ruixiang Tang,Wenya Wang*

Main category: cs.CL

TL;DR: Proposes In-Context Routing (ICR), a novel implicit ICL method that extracts reusable structural patterns from attention logits to achieve few-shot performance at zero-shot cost, outperforming previous methods on 12 datasets.


<details>
  <summary>Details</summary>
Motivation: Existing implicit ICL methods rely on injecting shift vectors into residual flows from labeled demonstrations or task-specific alignment, which fails to utilize the structural mechanisms of ICL and has limited generalizability.

Method: ICR internalizes generalizable ICL patterns at the attention logits level, extracts reusable structural directions that emerge during ICL, and uses a learnable input-conditioned router to modulate attention logits, enabling a train-once-and-reuse framework.

Result: ICR consistently outperforms prior implicit ICL methods on 12 real-world datasets across diverse domains and multiple LLMs, while demonstrating robust generalization to out-of-domain tasks where existing methods struggle.

Conclusion: ICR pushes the boundary of ICL's practical value by providing a more generalizable and effective approach to implicit in-context learning.

Abstract: Implicit in-context learning (ICL) has newly emerged as a promising paradigm
that simulates ICL behaviors in the representation space of Large Language
Models (LLMs), aiming to attain few-shot performance at zero-shot cost.
However, existing approaches largely rely on injecting shift vectors into
residual flows, which are typically constructed from labeled demonstrations or
task-specific alignment. Such designs fall short of utilizing the structural
mechanisms underlying ICL and suffer from limited generalizability. To address
this, we propose In-Context Routing (ICR), a novel implicit ICL method that
internalizes generalizable ICL patterns at the attention logits level. It
extracts reusable structural directions that emerge during ICL and employs a
learnable input-conditioned router to modulate attention logits accordingly,
enabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world
datasets spanning diverse domains and multiple LLMs. The results show that ICR
consistently outperforms prior implicit ICL methods that require task-specific
retrieval or training, while demonstrating robust generalization to
out-of-domain tasks where existing methods struggle. These findings position
ICR to push the boundary of ICL's practical value.

</details>


### [16] [The Bias is in the Details: An Assessment of Cognitive Bias in LLMs](https://arxiv.org/abs/2509.22856)
*R. Alexander Knipper,Charles S. Knipper,Kaiqi Zhang,Valerie Sims,Clint Bowers,Santu Karmaker*

Main category: cs.CL

TL;DR: Large-scale evaluation of 45 LLMs reveals they exhibit cognitive biases in 17.8-57.3% of cases across 8 bias types, with model size and prompt specificity significantly affecting bias susceptibility.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used in real-world decision-making, it's crucial to examine whether they exhibit the same cognitive biases that systematically distort human judgments.

Method: Developed novel evaluation framework using multiple-choice tasks, curated dataset of 220 decision scenarios with psychologists, generated diverse prompts from human-authored templates, analyzed over 2.8 million LLM responses across 45 models.

Result: LLMs showed bias-consistent behavior in 17.8-57.3% of instances across 8 cognitive biases. Larger models (>32B parameters) reduced bias in 39.5% of cases, while detailed prompts reduced most biases by up to 14.9% (except Overattribution bias, which increased by 8.8%).

Conclusion: LLMs do exhibit cognitive biases similar to humans, with bias susceptibility influenced by model size and prompt design, highlighting the need for careful consideration when deploying LLMs in decision-making contexts.

Abstract: As Large Language Models (LLMs) are increasingly embedded in real-world
decision-making processes, it becomes crucial to examine the extent to which
they exhibit cognitive biases. Extensively studied in the field of psychology,
cognitive biases appear as systematic distortions commonly observed in human
judgments. This paper presents a large-scale evaluation of eight
well-established cognitive biases across 45 LLMs, analyzing over 2.8 million
LLM responses generated through controlled prompt variations. To achieve this,
we introduce a novel evaluation framework based on multiple-choice tasks,
hand-curate a dataset of 220 decision scenarios targeting fundamental cognitive
biases in collaboration with psychologists, and propose a scalable approach for
generating diverse prompts from human-authored scenario templates. Our analysis
shows that LLMs exhibit bias-consistent behavior in 17.8-57.3% of instances
across a range of judgment and decision-making contexts targeting anchoring,
availability, confirmation, framing, interpretation, overattribution, prospect
theory, and representativeness biases. We find that both model size and prompt
specificity play a significant role on bias susceptibility as follows: larger
size (>32B parameters) can reduce bias in 39.5% of cases, while higher prompt
detail reduces most biases by up to 14.9%, except in one case
(Overattribution), which is exacerbated by up to 8.8%.

</details>


### [17] [Lexicon-Enriched Graph Modeling for Arabic Document Readability Prediction](https://arxiv.org/abs/2509.22870)
*Passant Elchafei,Mayar Osama,Mohamed Rageh,Mervat Abuelkheir*

Main category: cs.CL

TL;DR: A graph-based approach with lexicons for Arabic document readability prediction using sentence-level graphs with GNN and transformer branches combined via late fusion.


<details>
  <summary>Details</summary>
Motivation: To develop an effective method for predicting document-level readability in Arabic by leveraging linguistic relationships and contextual embeddings.

Method: Model documents as sentence-level graphs with nodes for sentences/lemmas and edges for linguistic relationships. Use SAMER lexicon features and Arabic transformer embeddings. Train GNN and transformer branches independently and combine via late fusion with max pooling aggregation.

Result: The hybrid method outperforms standalone GNN or transformer branches across multiple readability metrics. Fusion works better at document level, while GNN-only is stronger for sentence-level prediction.

Conclusion: Graph-based approaches enriched with lexicons and transformer embeddings effectively predict Arabic readability, with fusion strategies offering advantages at document level.

Abstract: We present a graph-based approach enriched with lexicons to predict
document-level readability in Arabic, developed as part of the Constrained
Track of the BAREC Shared Task 2025. Our system models each document as a
sentence-level graph, where nodes represent sentences and lemmas, and edges
capture linguistic relationships such as lexical co-occurrence and class
membership. Sentence nodes are enriched with features from the SAMER lexicon as
well as contextual embeddings from the Arabic transformer model. The graph
neural network (GNN) and transformer sentence encoder are trained as two
independent branches, and their predictions are combined via late fusion at
inference. For document-level prediction, sentence-level outputs are aggregated
using max pooling to reflect the most difficult sentence. Experimental results
show that this hybrid method outperforms standalone GNN or transformer branches
across multiple readability metrics. Overall, the findings highlight that
fusion offers advantages at the document level, but the GNN-only approach
remains stronger for precise prediction of sentence-level readability.

</details>


### [18] [HEART: Emotionally-driven test-time scaling of Language Models](https://arxiv.org/abs/2509.22876)
*Gabriela Pinto,Palash Goyal,Yiwen Song,Souradip Chakraborty,Zifeng Wang,Tomas Pfister,Hamid Palangi*

Main category: cs.CL

TL;DR: HEART is a novel framework that uses emotionally-driven prompts for iterative self-correction in language models, showing significant performance improvements on reasoning tasks when guided by an oracle verifier, but struggles in verifier-free settings.


<details>
  <summary>Details</summary>
Motivation: Current test-time scaling strategies focus on logical refinement but ignore the potential of affective feedback. Psychological research shows emotions can modulate cognitive performance, suggesting emotional guidance could improve reasoning.

Method: HEART provides feedback on incorrect responses using concise, emotionally charged phrases based on Ekman's six universal emotions. It systematically varies emotional tone across iterations to guide models away from flawed reasoning paths.

Result: When guided by an oracle verifier, HEART unlocks significantly deeper reasoning and achieves substantial accuracy improvements over state-of-the-art baselines on benchmarks like OlympiadBench, Humanity's Last Exam, and SimpleQA.

Conclusion: The next frontier in machine reasoning may involve not just refining logic but also understanding and leveraging emotional aspects ('HEART') of models, though practical deployment faces challenges in verifier-free settings.

Abstract: Test-time scaling has shown considerable success in improving the performance
of language models on complex reasoning tasks without requiring fine-tuning.
However, current strategies such as self-reflection primarily focus on logical
or structural refinement. They do not leverage the guiding potential of
affective feedback. Inspired by psychological research showing that emotions
can modulate cognitive performance, we introduce HEART--a novel framework that
uses emotionally-driven prompts for iterative self-correction. HEART provides
feedback on a model's incorrect response using a curated set of concise,
emotionally charged phrases based on the six universal emotions categorized by
Dr. Paul Ekman. By systematically varying the emotional tone of the feedback
across iterations, our method guides the model to escape flawed reasoning paths
and explore more promising alternatives. We evaluate our framework on
challenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam,
and SimpleQA. Our results reveal a significant new phenomenon: when guided by
an oracle verifier, this affective iteration protocol unlocks significantly
deeper reasoning, leading to consistent and substantial increases in accuracy
over state-of-the-art baselines with the same verifier. However, we also
identify a critical bottleneck for practical deployment. In a verifier-free
setting, it struggles to harness these gains consistently, highlighting as a
key challenge for future work. Our findings suggest that the next frontier in
machine reasoning may lie not just in refining logic, but also in understanding
and leveraging the `HEART' of the models.

</details>


### [19] [Infusing Theory of Mind into Socially Intelligent LLM Agents](https://arxiv.org/abs/2509.22887)
*EunJeong Hwang,Yuwei Yin,Giuseppe Carenini,Peter West,Vered Shwartz*

Main category: cs.CL

TL;DR: LLMs with explicit Theory of Mind (ToM) reasoning achieve better dialogue performance and goal achievement. ToMAgent (ToMA) combines ToM with dialogue lookahead to generate maximally useful mental states.


<details>
  <summary>Details</summary>
Motivation: Current chatbots and LLM-based social agents lack Theory of Mind understanding, which is crucial for human social intelligence. Integrating ToM can improve dialogue effectiveness and goal achievement.

Method: Introduces ToMAgent (ToMA), which pairs Theory of Mind with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Simple prompting to generate mental states between dialogue turns already provides benefits.

Result: Experiments on Sotopia interactive social evaluation benchmark show ToMA outperforms baselines. It exhibits more strategic, goal-oriented reasoning behaviors, enables long-horizon adaptation, and maintains better partner relationships.

Conclusion: Explicit integration of Theory of Mind represents a significant step forward in building socially intelligent LLM agents, improving dialogue performance and goal achievement.

Abstract: Theory of Mind (ToM)-an understanding of the mental states of others-is a key
aspect of human social intelligence, yet, chatbots and LLM-based social agents
do not typically integrate it. In this work, we demonstrate that LLMs that
explicitly use ToM get better at dialogue, achieving goals more effectively.
After showing that simply prompting models to generate mental states between
dialogue turns already provides significant benefit, we further introduce
ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM
with dialogue lookahead to produce mental states that are maximally useful for
achieving dialogue goals. Experiments on the Sotopia interactive social
evaluation benchmark demonstrate the effectiveness of our method over a range
of baselines. Comprehensive analysis shows that ToMA exhibits more strategic,
goal-oriented reasoning behaviors, which enable long-horizon adaptation, while
maintaining better relationships with their partners. Our results suggest a
step forward in integrating ToM for building socially intelligent LLM agents.

</details>


### [20] [Extract-0: A Specialized Language Model for Document Information Extraction](https://arxiv.org/abs/2509.22906)
*Henrique Godoy*

Main category: cs.CL

TL;DR: Extract-0 is a 7B parameter model for document information extraction that outperforms larger models like GPT-4 through synthetic data generation, LoRA fine-tuning, and GRPO reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To develop a specialized model for document information extraction that achieves superior performance compared to general-purpose large models while using significantly fewer computational resources.

Method: Combines synthetic data generation (280,128 training examples), supervised fine-tuning with LoRA (modifying only 0.53% of weights), and reinforcement learning with GRPO using a novel semantic similarity-based reward function.

Result: Achieves mean reward of 0.573 on 1,000 document extraction tasks, outperforming GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459).

Conclusion: Task-specific optimization can create models that surpass general-purpose systems while requiring substantially fewer computational resources.

Abstract: This paper presents Extract-0, a 7-billion parameter language model
specifically optimized for document information extraction that achieves
performance exceeding models with parameter counts several orders of magnitude
larger. Through a novel combination of synthetic data generation, supervised
fine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via
Group Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of
0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming
GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology
employs a memory-preserving synthetic data generation pipeline that produces
280,128 training examples from diverse document sources, followed by
parameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M
out of 7.66B parameters). The reinforcement learning phase introduces a novel
semantic similarity-based reward function that handles the inherent ambiguity
in information extraction tasks. This research demonstrates that task-specific
optimization can yield models that surpass general-purpose systems while
requiring substantially fewer computational resource.

</details>


### [21] [Large language models management of medications: three performance analyses](https://arxiv.org/abs/2509.22926)
*Kelli Henry,Steven Xu,Kaitlin Blotske,Moriah Cargile,Erin F. Barreto,Brian Murray,Susan Smith,Seth R. Bauer,Yanjun Gao,Tianming Liu,Andrea Sikora*

Main category: cs.CL

TL;DR: GPT-4o performs poorly on medical medication tasks including drug-formulation matching, drug-drug interaction identification, and medication order preparation, highlighting the need for domain-specific training.


<details>
  <summary>Details</summary>
Motivation: To evaluate GPT-4o's consistency in recommending appropriate medication regimens, as few studies have assessed LLMs' performance on medication benchmarking tests despite their potential utility in medical diagnosis.

Method: Three experiments using GPT-4o: drug-formulation matching, drug-drug interaction identification (with and without web search), and medication order preparation. Evaluation used cosine similarity on TF-IDF vectors, normalized Levenshtein similarity, ROUGE-1/ROUGE-L F1 scores, and manual clinician evaluation.

Result: Poor performance across all tests: 49% correct drug-formulation matching with frequent omissions and hallucinations; inconsistent drug-drug interaction identification (54.7% with internal knowledge vs 69.2% with search); only 65.8% of medication orders contained no errors. Performance worsened with more formulations and when using web search for non-interacting drugs.

Conclusion: GPT-4o's overall poor performance highlights the need for domain-specific training using clinician-annotated datasets and comprehensive evaluation frameworks for benchmarking medical LLM performance.

Abstract: Background: Large language models (LLMs) can be useful in diagnosing medical
conditions, but few studies have evaluated their consistency in recommending
appropriate medication regimens. The purpose of this evaluation was to test
GPT-4o on three medication benchmarking tests including mapping a drug name to
its correct formulation, identifying drug-drug interactions using both its
internal knowledge and using a web search, and preparing a medication order
sentence after being given the medication name. Methods: Using GTP-4o three
experiments were completed. Accuracy was quantified by computing cosine
similarity on TF-IDF vectors, normalized Levenshtein similarity, and
ROUGE-1/ROUGE-L F1 between each response and its reference string or by manual
evaluation by clinicians. Results: GPT-4o performed poorly on drug-formulation
matching, with frequent omissions of available drug formulations (mean 1.23 per
medication) and hallucinations of formulations that do not exist (mean 1.14 per
medication). Only 49% of tested medications were correctly matched to all
available formulations. Accuracy was decreased for medications with more
formulations (p<0.0001). GPT-4o was also inconsistent at identifying
drug-drug-interactions, although it had better performance with the
search-augmented assessment compared to its internal knowledge (54.7% vs.
69.2%, p=0.013). However, allowing a web-search worsened performance when there
was no drug-drug interaction (median % correct 100% vs. 40%, p<0.001). Finally,
GPT-4o performed moderately with preparing a medication order sentence, with
only 65.8% of medication order sentences containing no medication or
abbreviation errors. Conclusions: Model performance was overall poor for all
tests. This highlights the need for domain-specific training through
clinician-annotated datasets and a comprehensive evaluation framework for
benchmarking performance.

</details>


### [22] [LLMs Behind the Scenes: Enabling Narrative Scene Illustration](https://arxiv.org/abs/2509.22940)
*Melissa Roemmele,John Joon Young Chung,Taewook Kim,Yuqian Sun,Alex Calderwood,Max Kreminski*

Main category: cs.CL

TL;DR: The paper proposes using LLMs to prompt text-to-image models for automatically generating scene illustrations from story text, creating a new dataset called SceneIllustrations.


<details>
  <summary>Details</summary>
Motivation: To leverage generative AI's capability to transform content across modalities, specifically for storytelling where visual illustrations can enhance text-based narratives.

Method: A pipeline using LLMs as interface to prompt text-to-image models, applied to story corpus with human annotation for quality judgments.

Result: Created SceneIllustrations dataset and demonstrated LLMs can effectively verbalize scene knowledge from story text for illustration generation.

Conclusion: LLMs are effective for generating and evaluating narrative scene illustrations, providing a valuable resource for cross-modal narrative transformation.

Abstract: Generative AI has established the opportunity to readily transform content
from one medium to another. This capability is especially powerful for
storytelling, where visual illustrations can illuminate a story originally
expressed in text. In this paper, we focus on the task of narrative scene
illustration, which involves automatically generating an image depicting a
scene in a story. Motivated by recent progress on text-to-image models, we
consider a pipeline that uses LLMs as an interface for prompting text-to-image
models to generate scene illustrations given raw story text. We apply
variations of this pipeline to a prominent story corpus in order to synthesize
illustrations for scenes in these stories. We conduct a human annotation task
to obtain pairwise quality judgments for these illustrations. The outcome of
this process is the SceneIllustrations dataset, which we release as a new
resource for future work on cross-modal narrative transformation. Through our
analysis of this dataset and experiments modeling illustration quality, we
demonstrate that LLMs can effectively verbalize scene knowledge implicitly
evoked by story text. Moreover, this capability is impactful for generating and
evaluating illustrations.

</details>


### [23] [What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?](https://arxiv.org/abs/2509.22947)
*Mohammed Sabry,Anya Belz*

Main category: cs.CL

TL;DR: Injecting synthetic induction data during pretraining accelerates induction-head emergence but doesn't consistently improve in-context learning (ICL) performance. Natural text training produces better generalization and more centralized, load-bearing circuits.


<details>
  <summary>Details</summary>
Motivation: To test whether targeted synthetic data can accelerate induction-head emergence and enhance ICL performance compared to natural text training under iso-FLOPs compute constraints.

Method: Introduced Bi-Induct curriculum that injects forward-copy (Induction), backward-copy (Anti), or balanced mix into pretraining stream. Trained models from 0.13B to 1B parameters under iso-FLOPs, evaluating few-shot ICL benchmarks, head-level telemetry, and language modeling perplexity.

Result: Bi-Induct accelerates induction-head emergence at small scales but doesn't yield stronger generalization. Natural-only training performs best on function-style ICL probes. Larger natural-only models develop broader, earlier induction heads without explicit induction patterns. Ablating top induction heads degrades ICL more in natural-only models, indicating more centralized circuits.

Conclusion: Inducing activation is not sufficient for ICL gains - circuits must become functionally necessary. Results emphasize mechanism-aware pretraining diagnostics and data mixtures that foster load-bearing structure rather than merely present structure.

Abstract: Does explicitly exercising the induction circuit during pretraining improve
in-context learning (ICL), or is natural text sufficient when compute is held
constant (iso-FLOPs)? To test whether targeted synthetic data can accelerate
induction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight
curriculum that injects forward-copy (Induction), backward-copy (Anti), or a
balanced mix into the pretraining stream. We train models from 0.13B to 1B
parameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii)
head-level telemetry, and (iii) held-out language modeling perplexity. Our
findings challenge the assumption that early induction circuit activation
directly improves ICL. While Bi-Induct accelerates induction-head emergence at
small scales, this does not consistently yield stronger generalization. On
standard LM benchmarks, Bi-Induct matches natural-only training; on
function-style ICL probes, the 1B natural-only performs best. Stress tests
(e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these
trends. Telemetry shows larger natural-only models develop broader, earlier
induction heads without explicit induction patterns. Anti-induction data fails
to elicit meaningful activation. Perplexity penalties from synthetic data
shrink with scale, suggesting larger models can absorb non-natural patterns
with minimal cost. Crucially, ablating the top 2% of induction heads degrades
ICL more than random ablations, especially for natural-only models, indicating
more centralized, load-bearing circuits. Bi-Induct variants exhibit more
redundant induction activity, implying different circuit utilization. Overall,
inducing activation is not sufficient: ICL gains depend on these circuits
becoming functionally necessary. These results underscore mechanism-aware
pretraining diagnostics and data mixtures that foster load-bearing, not merely
present, structure.

</details>


### [24] [Emergent morpho-phonological representations in self-supervised speech models](https://arxiv.org/abs/2509.22973)
*Jon Gauthier,Canaan Breiss,Matthew Leonard,Edward F. Chang*

Main category: cs.CL

TL;DR: Self-supervised speech models for word recognition exhibit linear geometric representations that capture regular distributional relationships between English nouns/verbs and their inflected forms, rather than directly tracking phonological or morphological units.


<details>
  <summary>Details</summary>
Motivation: To understand what types of linguistic representations self-supervised speech models use for word recognition in noisy environments, specifically how they represent phonological and morphological phenomena in English noun and verb inflections.

Method: Study S3M variants optimized for word recognition by analyzing their representations of frequent English noun and verb inflections, examining the geometric structure of these representations.

Result: The models' representations show a global linear geometry that can link English nouns and verbs to their regular inflected forms. This structure tracks regular distributional relationships between word pairs in the lexicon, often (but not always) due to morphological inflection.

Conclusion: These findings challenge the presumed necessity of distinct linguistic representations of phonology and morphology, suggesting alternative representational strategies that may support human spoken word recognition.

Abstract: Self-supervised speech models can be trained to efficiently recognize spoken
words in naturalistic, noisy environments. However, we do not understand the
types of linguistic representations these models use to accomplish this task.
To address this question, we study how S3M variants optimized for word
recognition represent phonological and morphological phenomena in frequent
English noun and verb inflections. We find that their representations exhibit a
global linear geometry which can be used to link English nouns and verbs to
their regular inflected forms.
  This geometric structure does not directly track phonological or
morphological units. Instead, it tracks the regular distributional
relationships linking many word pairs in the English lexicon -- often, but not
always, due to morphological inflection. These findings point to candidate
representational strategies that may support human spoken word recognition,
challenging the presumed necessity of distinct linguistic representations of
phonology and morphology.

</details>


### [25] [Same Content, Different Representations: A Controlled Study for Table QA](https://arxiv.org/abs/2509.22983)
*Yue Zhang,Seiji Maekawa,Nikita Bhutani*

Main category: cs.CL

TL;DR: This paper presents a controlled study on how table representation affects Table QA performance, showing that no single method excels across all conditions and highlighting the importance of representation in model selection.


<details>
  <summary>Details</summary>
Motivation: Existing Table QA benchmarks are tied to fixed data formats and haven't systematically examined how table representation itself affects model performance. Real-world Table QA must handle both structured databases and semi-structured tables with textual fields.

Method: Used a verbalization pipeline to generate paired structured and semi-structured tables while holding content constant, enabling direct comparisons. Introduced a diagnostic benchmark with splits along table size, join requirements, query complexity, and schema quality.

Result: SQL-based methods achieve high accuracy on structured inputs but degrade on semi-structured data; LLMs show flexibility but reduced precision; hybrid approaches strike a balance, especially under noisy schemas. Effects intensify with larger tables and more complex queries.

Conclusion: No single method excels across all conditions, and table representation plays a central role in shaping Table QA performance. Findings provide actionable insights for model selection and design, paving the way for more robust hybrid approaches for diverse real-world data formats.

Abstract: Table Question Answering (Table QA) in real-world settings must operate over
both structured databases and semi-structured tables containing textual fields.
However, existing benchmarks are tied to fixed data formats and have not
systematically examined how representation itself affects model performance. We
present the first controlled study that isolates the role of table
representation by holding content constant while varying structure. Using a
verbalization pipeline, we generate paired structured and semi-structured
tables, enabling direct comparisons across modeling paradigms. To support
detailed analysis, we introduce a diagnostic benchmark with splits along table
size, join requirements, query complexity, and schema quality. Our experiments
reveal consistent trade-offs: SQL-based methods achieve high accuracy on
structured inputs but degrade on semi-structured data, LLMs exhibit flexibility
but reduced precision, and hybrid approaches strike a balance, particularly
under noisy schemas. These effects intensify with larger tables and more
complex queries. Ultimately, no single method excels across all conditions, and
we highlight the central role of representation in shaping Table QA
performance. Our findings provide actionable insights for model selection and
design, paving the way for more robust hybrid approaches suited for diverse
real-world data formats.

</details>


### [26] [ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning](https://arxiv.org/abs/2509.22991)
*Jasin Cekinmez,Omid Ghahroodi,Saad Fowad Chandle,Dhiman Gupta,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: ADAM is a framework for evaluating and improving MLLMs in biographical reasoning, featuring AdamDB dataset (4M+ individuals), AdamBench evaluations based on Bloom's taxonomy, and AdamRAG system to reduce hallucinations.


<details>
  <summary>Details</summary>
Motivation: Biographical reasoning is a critical yet underexplored dimension of factual knowledge in LLMs, with current models suffering from hallucinations especially for lesser-known individuals.

Method: Created multilingual multimodal dataset (AdamDB), cognitive evaluations (AdamBench) across six reasoning levels, and retrieval-augmented generation system (AdamRAG) tailored to biographical contexts.

Result: AdamRAG substantially improves open-source models and modestly benefits closed-source ones, with largest gains on lower-order reasoning. Popularity strongly mediates accuracy, and multimodal input via face images offers smaller improvements than retrieval.

Conclusion: ADAM establishes the first benchmark and framework for cognitively, culturally, and multimodally grounded biographical evaluation, advancing development of multilingual, accurate, and hallucination-resistant MLLMs.

Abstract: We introduce ADAM (A Diverse Archive of Mankind), a framework for evaluating
and improving multimodal large language models (MLLMs) in biographical
reasoning. To the best of our knowledge, this is the first work to
systematically examine LLM capabilities in biography, a critical yet
underexplored dimension of factual knowledge. At its core, AdamDB is a
multilingual and multimodal dataset covering over 4 million individuals across
geography, time, and profession, while AdamBench provides cognitively
structured evaluations based on Bloom's taxonomy, spanning six reasoning levels
in both English and native languages. To address hallucinations, particularly
for lesser-known individuals, we propose AdamRAG, a retrieval-augmented
generation system tailored to biographical contexts. Experiments show that
AdamRAG substantially improves open-source models and modestly benefits
closed-source ones, with the largest gains on lower-order reasoning. Popularity
strongly mediates accuracy, and multimodal input via face images offers
smaller, less consistent improvements than retrieval. ADAM establishes the
first benchmark and framework for cognitively, culturally, and multimodally
grounded biographical evaluation, advancing the development of multilingual,
accurate, and hallucination-resistant MLLMs.

</details>


### [27] [AI Brown and AI Koditex: LLM-Generated Corpora Comparable to Traditional Corpora of English and Czech Texts](https://arxiv.org/abs/2509.22996)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: This paper presents two LLM-generated corpora (English and Czech) that replicate human reference corpora, with comprehensive linguistic annotation and multi-model generation.


<details>
  <summary>Details</summary>
Motivation: To create resources for linguistic comparison between human-written and LLM-generated texts, ensuring multi-genre diversity and comparability with existing human corpora.

Method: Generated corpora using various LLMs (OpenAI, Anthropic, Alphabet, Meta, DeepSeek) from GPT-3 to GPT-4.5, replicating BE21 and Koditex reference corpora, with Universal Dependencies annotation.

Result: Created English corpus with 27M tokens (avg 864k per model) and Czech corpus with 21.5M tokens (avg 768k per model), freely available under CC licenses and accessible through Czech National Corpus interface.

Conclusion: Successfully developed comprehensive LLM-generated corpora that provide valuable resources for linguistic analysis and comparison between human and machine-generated text across multiple languages and models.

Abstract: This article presents two corpora of English and Czech texts generated with
large language models (LLMs). The motivation is to create a resource for
comparing human-written texts with LLM-generated text linguistically. Emphasis
was placed on ensuring these resources are multi-genre and rich in terms of
topics, authors, and text types, while maintaining comparability with existing
human-created corpora. These generated corpora replicate reference human
corpora: BE21 by Paul Baker, which is a modern version of the original Brown
Corpus, and Koditex corpus that also follows the Brown Corpus tradition but in
Czech. The new corpora were generated using models from OpenAI, Anthropic,
Alphabet, Meta, and DeepSeek, ranging from GPT-3 (davinci-002) to GPT-4.5, and
are tagged according to the Universal Dependencies standard (i.e., they are
tokenized, lemmatized, and morphologically and syntactically annotated). The
subcorpus size varies according to the model used (the English part contains on
average 864k tokens per model, 27M tokens altogether, the Czech partcontains on
average 768k tokens per model, 21.5M tokens altogether). The corpora are freely
available for download under the CC BY 4.0 license (the annotated data are
under CC BY-NC-SA 4.0 licence) and are also accessible through the search
interface of the Czech National Corpus.

</details>


### [28] [Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents](https://arxiv.org/abs/2509.23040)
*Yaorui Shi,Yuxin Chen,Siyuan Wang,Sihang Li,Hengxing Cai,Qi Gu,Xiang Wang,An Zhang*

Main category: cs.CL

TL;DR: ReMemR1 is a memory-augmented agent with callback-enhanced memory that enables selective retrieval from entire memory history and non-linear reasoning, combined with RLMLR training that provides multi-level rewards for effective memory use.


<details>
  <summary>Details</summary>
Motivation: Address challenges in long-context QA where key evidence is dispersed across millions of tokens, overcoming limitations of existing "memorize while reading" methods that suffer from irreversible forward-only processing, information loss, and sparse reinforcement learning signals.

Method: Propose ReMemR1 with callback-enhanced memory allowing selective retrieval from entire memory history and non-linear reasoning, and RLMLR (Reinforcement Learning with Multi-Level Rewards) that combines final-answer rewards with dense step-level signals.

Result: Experiments on long-document QA show significant gains over existing memory-based approaches, validating ReMemR1 as an effective solution for long-context reasoning agents.

Conclusion: The proposed approach mitigates information degradation, improves supervision, and supports multi-hop memory utilization, providing an effective solution for long-context reasoning in large language models.

Abstract: Large language models face challenges in long-context question answering,
where key evidence of a query may be dispersed across millions of tokens.
Existing works equip large language models with a memory corpus that is
dynamically updated during a single-pass document scan, also known as the
"memorize while reading" methods. While this approach scales efficiently, it
suffers from irreversible forward-only processing, information loss through
overwriting, and sparse reinforcement learning signals. To tackle these
challenges, we present ReMemR1, a memory-augmented agent with callback-enhanced
memory that allows selective retrieval from the entire memory history and
allows non-linear reasoning and revisiting of early evidence. To further
strengthen training, we propose Reinforcement Learning with Multi-Level Rewards
(RLMLR), which combines final-answer rewards with dense, step-level signals
that guide effective memory use. Together, these contributions mitigate
information degradation, improve supervision, and support multi-hop memory
utilizing. Experiments on long-document QA show significant gains over existing
memory-based approaches, which validates ReMemR1 as an effective solution for
long-context reasoning agents.

</details>


### [29] [Peacemaker or Troublemaker: How Sycophancy Shapes Multi-Agent Debate](https://arxiv.org/abs/2509.23055)
*Binwei Yao,Chao Shang,Wanyu Du,Jianfeng He,Ruixue Lian,Yi Zhang,Hang Su,Sandesh Swamy,Yanjun Qi*

Main category: cs.CL

TL;DR: LLMs' sycophancy causes premature consensus in multi-agent debates, reducing accuracy. This paper defines sycophancy in MADS, develops evaluation metrics, and shows how it leads to debate collapse through debater/judge failure modes.


<details>
  <summary>Details</summary>
Motivation: LLMs' inherent sycophancy poses challenges for multi-agent debating systems by causing premature consensus, undermining the benefits of productive disagreement. Prior research focused on user-LLM sycophancy, leaving inter-agent sycophancy in debates poorly understood.

Method: Introduced operational framework with: (1) formal definition of sycophancy specific to MADS, (2) new metrics to evaluate agent sycophancy level and information exchange impact, (3) systematic investigation of sycophancy effects across agent roles in decentralized and centralized debate frameworks.

Result: Sycophancy is a core failure mode that amplifies disagreement collapse before reaching correct conclusions, yields lower accuracy than single-agent baselines, and arises from distinct debater-driven and judge-driven failure modes.

Conclusion: Proposed actionable design principles for MADS to effectively balance productive disagreement with cooperation in agent interactions, addressing the sycophancy challenge in multi-agent debates.

Abstract: Large language models (LLMs) often display sycophancy, a tendency toward
excessive agreeability. This behavior poses significant challenges for
multi-agent debating systems (MADS) that rely on productive disagreement to
refine arguments and foster innovative thinking. LLMs' inherent sycophancy can
collapse debates into premature consensus, potentially undermining the benefits
of multi-agent debate. While prior studies focus on user--LLM sycophancy, the
impact of inter-agent sycophancy in debate remains poorly understood. To
address this gap, we introduce the first operational framework that (1)
proposes a formal definition of sycophancy specific to MADS settings, (2)
develops new metrics to evaluate the agent sycophancy level and its impact on
information exchange in MADS, and (3) systematically investigates how varying
levels of sycophancy across agent roles (debaters and judges) affects outcomes
in both decentralized and centralized debate frameworks. Our findings reveal
that sycophancy is a core failure mode that amplifies disagreement collapse
before reaching a correct conclusion in multi-agent debates, yields lower
accuracy than single-agent baselines, and arises from distinct debater-driven
and judge-driven failure modes. Building on these findings, we propose
actionable design principles for MADS, effectively balancing productive
disagreement with cooperation in agent interactions.

</details>


### [30] [Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks](https://arxiv.org/abs/2509.23067)
*Chunyang Jiang,Yonggang Zhang,Yiyang Cai,Chi-Min Chan,Yulong Liu,Mingming Chen,Wei Xue,Yike Guo*

Main category: cs.CL

TL;DR: Proposes semantic voting as a self-evaluation-free approach for LLM self-improvement on unverifiable tasks, replacing exact matching with semantic similarity using lightweight embeddings.


<details>
  <summary>Details</summary>
Motivation: Self-evaluation methods for unverifiable tasks suffer from high computational overhead and overconfidence issues due to LLM biases, while majority voting only works for verifiable tasks.

Method: Semantic voting replaces hard matching (exact match) with soft matching (semantic similarity) using lightweight sentence embeddings to generate pseudo-labels without self-evaluation.

Result: Achieves substantial gains in computational efficiency and better overall performance than self-evaluation methods across diverse model architectures and tasks.

Conclusion: Semantic voting provides an effective and efficient alternative to self-evaluation for LLM self-improvement on unverifiable tasks, overcoming computational burden and bias limitations.

Abstract: The rising cost of acquiring supervised data has driven significant interest
in self-improvement for large language models (LLMs). Straightforward
unsupervised signals like majority voting have proven effective in generating
pseudo-labels for verifiable tasks, while their applicability to unverifiable
tasks (e.g., translation) is limited by the open-ended character of responses.
As a result, self-evaluation mechanisms (e.g., self-judging and entropy
minimization) are predominantly used to derive pseudo-labels. However,
self-evaluation relying on LLMs typically incurs high computational overhead
and introduces overconfidence issues due to intrinsic biases. To address these
challenges, we propose a novel self-evaluation-free approach for unverifiable
tasks, designed for lightweight yet effective self-improvement. Inspired by
majority voting commonly employed in verifiable tasks, we propose semantic
voting as a novel mechanism that relaxes the principle of hard matching (i.e.,
exact matching) toward soft matching (i.e., semantic similarity). Soft matching
is achieved by leveraging a lightweight sentence embedding model to quantify
semantic similarity, thereby mitigating excessive computational burden and
intrinsic bias-associated limitations of self-evaluation. Comprehensive
experiments demonstrate that our method achieves substantial gains in
computational efficiency and overall better performance than self-evaluation
methods across diverse model architectures and tasks.

</details>


### [31] [From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents](https://arxiv.org/abs/2509.23071)
*Muzhi Li,Jinhu Qi,Yihong Wu,Minghao Zhao,Liheng Ma,Yifan Li,Xinyu Wang,Yingxue Zhang,Ho-fung Leung,Irwin King*

Main category: cs.CL

TL;DR: EviPath is a new paradigm for synthesizing evidence-anchored reasoning paths to train RAG agents, addressing the lack of process-level supervision in agent development through abductive subtask planning, faithful sub-question answering, and conversational fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current RAG agent development lacks process-level supervision for guiding agentic capabilities like task decomposition and stepwise decision-making. Reinforcement learning suffers from sparse rewards, and existing data synthesis methods fail to model environmental interactions.

Method: EviPath consists of three components: (1) Abductive Subtask Planning that decomposes problems into sub-questions and plans optimal solution paths; (2) Faithful Sub-question Answering that uses evidence to generate reasoning thoughts; (3) Conversational Fine-Tuning that formats interaction trajectories into dialogue format for supervised fine-tuning.

Result: An 8B parameter model trained with EviPath-synthesized data significantly outperforms state-of-the-art baselines with a 14.7% absolute EM gain in open-domain question answering benchmarks.

Conclusion: EviPath enables LLMs to learn complex reasoning and tool-use capabilities directly from synthesized data, providing an effective solution for RAG agent development without requiring process-level supervision.

Abstract: Retrieval-augmented generation agents development is hindered by the lack of
process-level supervision to effectively guide agentic capabilities like task
decomposition, retriever invocation, and stepwise decision-making. While
reinforcement learning offers a potential solution, it suffers from sparse
rewards and the limited reasoning capabilities of large language models (LLMs).
Meanwhile, existing data synthesis methods only produce chain-of-thought
rationales and fail to model environmental interactions. In this paper, we
propose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG
agent development. EviPath comprises: (i) Abductive Subtask Planning, which
decomposes the problem into sub-questions and iteratively plans an optimal
solution path based on the dependencies between them; (ii) Faithful
Sub-question Answering, which uses supporting evidence to construct a proxy
environment to generate reasoning thoughts and answers for each sub-question;
and (iii) Conversational Fine-Tuning, which formats the complete
agent-environment interaction trajectory into a dialogue format suitable for
Supervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and
tool-use capabilities directly from synthesized data. Extensive experiments on
widely-used question-answering benchmarks show that an 8B parameter model
trained with EviPath-synthesized data significantly and consistently
outperforms state-of-the-art baselines with a double-digit absolute EM gain of
14.7% in open-domain question answering.

</details>


### [32] [The Geometry of Creative Variability: How Credal Sets Expose Calibration Gaps in Language Models](https://arxiv.org/abs/2509.23088)
*Esteban Garces Arias,Julian Rodemann,Christian Heumann*

Main category: cs.CL

TL;DR: A geometric framework using credal sets to quantify uncertainty in LLMs for creative tasks, revealing gaps in capturing human creative variation and showing decoding strategy contributes significantly to epistemic uncertainty.


<details>
  <summary>Details</summary>
Motivation: Understanding uncertainty in large language models for creative tasks where multiple valid outputs exist, and the need to calibrate against human creative variation.

Method: Geometric framework using credal sets (convex hulls of probability distributions) to quantify and decompose uncertainty. Analyzed 500 creative writing prompts with 10 human continuations each, evaluated 4 language models across 5 decoding strategies, generating 100,000 stories.

Result: Substantial gaps in capturing human creative variation, best model-human calibration only 0.434. Decoding strategy contributes 39.4%-72.0% of total epistemic uncertainty. Model scale shows weak correlation with calibration quality, no significant difference between base and instruction-tuned models.

Conclusion: The geometric framework provides actionable insights for improving generation systems for human-AI creative alignment. Complete experimental framework released.

Abstract: Understanding uncertainty in large language models remains a fundamental
challenge, particularly in creative tasks where multiple valid outputs exist.
We present a geometric framework using credal sets - convex hulls of
probability distributions - to quantify and decompose uncertainty in neural
text generation, calibrated against human creative variation. Analyzing 500
creative writing prompts from the WritingPrompts dataset with 10 unique human
continuations each, we evaluate four language models across five decoding
strategies, generating 100,000 stories. Our credal set analysis reveals
substantial gaps in capturing human creative variation, with the best
model-human calibration reaching only 0.434 (Gemma-2B with temperature 0.7). We
decompose total uncertainty into epistemic and aleatoric components, finding
that the choice of decoding strategy contributes 39.4% to 72.0% of total
epistemic uncertainty. Model scale shows weak correlation with calibration
quality and no significant difference exists between base and instruction-tuned
models in calibration quality. Our geometric framework provides actionable
insights for improving generation systems for human-AI creative alignment. We
release our complete experimental framework.

</details>


### [33] [d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching](https://arxiv.org/abs/2509.23094)
*Yuchu Jiang,Yue Cai,Xiangzhong Luo,Jiale Fu,Jiarui Wang,Chonghan Liu,Xu Yang*

Main category: cs.CL

TL;DR: d²Cache is a training-free KV cache framework that accelerates diffusion-based LLM inference through adaptive token selection and caching, improving both speed and generation quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based LLMs suffer from poor inference efficiency due to bidirectional attention that prevents standard KV cache usage, unlike autoregressive models.

Method: Two-stage fine-grained selection strategy to identify and update key tokens' KV states while caching others, enabling quasi left-to-right generation.

Result: Achieves substantial inference speedups and consistent improvements in generation quality on LLaDA and Dream models.

Conclusion: d²Cache provides an effective training-free solution for accelerating dLLM inference while enhancing decoding reliability and quality.

Abstract: Diffusion-based large language models (dLLMs), despite their promising
performance, still suffer from inferior inference efficiency. This is because
dLLMs rely on bidirectional attention and cannot directly benefit from the
standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle
this issue, we introduce \textit{Dual aDaptive Cache} (d$^2$Cache), which is a
training-free approximate KV cache framework for accelerating dLLM inference.
d$^2$Cache features a two-stage fine-grained selection strategy to identify
tokens and adaptively update their KV states at each decoding step, while
caching the KV states of the remaining tokens for reuse. Furthermore,
d$^2$Cache naturally offers a more reliable decoding alternative, which can
enable quasi left-to-right generation and mitigate premature overconfidence in
tokens at the end of the sequence. Extensive experimental results on two
representative dLLMs (\ie, LLaDA and Dream) demonstrate that d$^2$Cache not
only achieves substantial inference speedups, but also yields consistent
improvements in generation quality. The code is available at
https://github.com/Kamichanw/d2Cache.

</details>


### [34] [How to Make Large Language Models Generate 100% Valid Molecules?](https://arxiv.org/abs/2509.23099)
*Wen Tao,Jing Tang,Alvin Chan,Bryan Hooi,Baolong Bi,Nanyun Peng,Yuansheng Liu,Yiwei Wang*

Main category: cs.CL

TL;DR: SmiSelf is a cross-chemical language framework that converts invalid SMILES to SELFIES using grammatical rules, ensuring 100% valid molecule generation while preserving molecular characteristics and improving performance metrics.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with generating valid molecules using SMILES representation in few-shot settings, and perform worse with SELFIES representation. There's a need for a method that ensures 100% validity while maintaining molecular properties.

Method: SmiSelf converts invalid SMILES to SELFIES using grammatical rules, leveraging SELFIES' inherent mechanisms to correct invalid SMILES strings. This cross-chemical language framework works with all SMILES-based generative models.

Result: SmiSelf ensures 100% validity of generated molecules while preserving molecular characteristics and maintaining or even enhancing performance on other metrics. It's compatible with all SMILES-based generative models.

Conclusion: SmiSelf successfully expands LLMs' practical applications in biomedicine by providing a reliable method for valid molecule generation, addressing the limitations of both SMILES and SELFIES representations in few-shot learning scenarios.

Abstract: Molecule generation is key to drug discovery and materials science, enabling
the design of novel compounds with specific properties. Large language models
(LLMs) can learn to perform a wide range of tasks from just a few examples.
However, generating valid molecules using representations like SMILES is
challenging for LLMs in few-shot settings. In this work, we explore how LLMs
can generate 100% valid molecules. We evaluate whether LLMs can use SELFIES, a
representation where every string corresponds to a valid molecule, for valid
molecule generation but find that LLMs perform worse with SELFIES than with
SMILES. We then examine LLMs' ability to correct invalid SMILES and find their
capacity limited. Finally, we introduce SmiSelf, a cross-chemical language
framework for invalid SMILES correction. SmiSelf converts invalid SMILES to
SELFIES using grammatical rules, leveraging SELFIES' mechanisms to correct the
invalid SMILES. Experiments show that SmiSelf ensures 100% validity while
preserving molecular characteristics and maintaining or even enhancing
performance on other metrics. SmiSelf helps expand LLMs' practical applications
in biomedicine and is compatible with all SMILES-based generative models. Code
is available at https://github.com/wentao228/SmiSelf.

</details>


### [35] [Non-Collaborative User Simulators for Tool Agents](https://arxiv.org/abs/2509.23124)
*Jeonghoon Shim,Woojung Song,Cheyon Jin,Seungwon KooK,Yohan Jo*

Main category: cs.CL

TL;DR: A non-collaborative user simulation method for tool agents that simulates four types of challenging user behaviors: requesting unavailable services, digressing conversations, expressing impatience, and providing incomplete utterances.


<details>
  <summary>Details</summary>
Motivation: Existing user simulators are too agent-friendly and cooperative, failing to train and test tool agents against real-world non-collaborative users who exhibit challenging behaviors.

Method: Proposed a novel user simulator architecture that simulates four categories of non-collaborative behaviors while reliably delivering all necessary intents and information for task completion.

Result: Experiments on MultiWOZ and τ-bench show significant performance degradation in state-of-the-art tool agents when encountering non-collaborative users, with issues like escalated hallucinations and dialogue breakdowns.

Conclusion: Provides an extensible user simulation framework to help develop tool agents and preemptively diagnose them under challenging real-world conditions.

Abstract: Non-Collaborative User Simulators for Tool Agents Download PDF Jeonghoon
Shim, Woojung Song, Cheyon Jin, Seungwon KooK, Yohan Jo 19 Sept 2025 (modified:
25 Sept 2025)ICLR 2026 Conference SubmissionConference, AuthorsRevisionsCC BY
4.0 Keywords: Tool Agent, User Simulator, Non-collaborative User, Dialogue
Simulation TL;DR: A non-collaborative user simulation method for tool agent.
Abstract: Tool agents interact with users through multi-turn dialogues to
accomplish various tasks. Recent studies have adopted user simulation methods
to develop these agents in multi-turn settings. However, existing user
simulators tend to be agent-friendly, exhibiting only cooperative behaviors,
which fails to train and test agents against non-collaborative users in the
real world. To address this, we propose a novel user simulator architecture
that simulates four categories of non-collaborative behaviors: requesting
unavailable services, digressing into tangential conversations, expressing
impatience, and providing incomplete utterances. Our user simulator can
simulate challenging and natural non-collaborative behaviors while reliably
delivering all intents and information necessary to accomplish the task. Our
experiments on MultiWOZ and $\tau$-bench reveal significant performance
degradation in state-of-the-art tool agents when encountering non-collaborative
users. We provide detailed analyses of agents' weaknesses under each
non-collaborative condition, such as escalated hallucinations and dialogue
breakdowns. Ultimately, we contribute an easily extensible user simulation
framework to help the research community develop tool agents and preemptively
diagnose them under challenging real-world conditions within their own
services.

</details>


### [36] [Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.23140)
*Song Jin,Juntian Zhang,Yong Liu,Xun Zhang,Yufei Zhang,Fei Jiang,Guojun Yin,Wei Lin,Rui Yan*

Main category: cs.CL

TL;DR: TagPR is a training framework that enhances LLMs' personalization reasoning through tagged thought processes, achieving 32.65% improvement over base models.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with personalization reasoning - analyzing user history and generating tailored responses, despite having strong general reasoning capabilities.

Method: Data-driven pipeline to generate semantically labeled reasoning chains, followed by SFT and multi-stage RL with composite reward signal (tag-based constraints + PRMU).

Result: Achieves state-of-the-art results on LaMP benchmark and self-constructed dataset, with 32.65% average improvement over base model across all tasks.

Conclusion: Structured, interpretable reasoning is an effective pathway to unlock genuine personalization capabilities in LLMs.

Abstract: Recent advancements have endowed Large Language Models (LLMs) with impressive
general reasoning capabilities, yet they often struggle with personalization
reasoning - the crucial ability to analyze user history, infer unique
preferences, and generate tailored responses. To address this limitation, we
introduce TagPR, a novel training framework that significantly enhances an
LLM's intrinsic capacity for personalization reasoning through a tagging the
thought approach. Our method first develops a data-driven pipeline to
automatically generate and semantically label reasoning chains, creating a
structured dataset that fosters interpretable reasoning. We then propose a
synergistic training strategy that begins with Supervised Fine-Tuning (SFT) on
this tagged data to establish foundational reasoning patterns, followed by a
multi-stage reinforcement learning (RL) process. This RL phase is guided by a
unique composite reward signal, which integrates tag-based constraints and a
novel Personalization Reward Model with User Embeddings (PRMU) to achieve
fine-grained alignment with user-specific logic. Extensive experiments on the
public LaMP benchmark and a self-constructed dataset demonstrate that our
approach achieves state-of-the-art results, delivering an average improvement
of 32.65% over the base model across all tasks. Our work validates that
structured, interpretable reasoning is a highly effective pathway to unlocking
genuine personalization capabilities in LLMs.

</details>


### [37] [Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models](https://arxiv.org/abs/2509.23146)
*Zichao Yu,Ming Li,Wenyi Zhang,Weiguo Gao*

Main category: cs.CL

TL;DR: TReASURe is a tree-search test-time alignment method for Masked Diffusion Language Models that addresses correlation issues in parallel unmasking and variance in reward evaluation through UnmaskBranch and ResubstituteScore techniques.


<details>
  <summary>Details</summary>
Motivation: Tree search for aligning generative models faces challenges with Masked Diffusion Language Models: (i) parallel unmasking creates correlated branches limiting exploration, and (ii) reward evaluation via sampled completions produces high-variance estimates making pruning unstable.

Method: Proposes TReASURe with two key components: (1) UnmaskBranch - a branching strategy using first-hitting unmasking to diversify token content and reveal order with single model call per parent node, (2) ResubstituteScore - a pruning rule using deterministic resubstitution to score partially masked sequences with low-variance proxy completions.

Result: Theoretically shows branching efficiency gains in NFEs, scoring rule approximates true reward with error bounded by predictive uncertainty, and improvements with larger tree widths. Empirically achieves SOTA results on perplexity, linguistic acceptability, and control of sentiment and toxicity, outperforming prior methods under matched compute budgets, especially in low-NFE regimes.

Conclusion: TReASURe effectively addresses key challenges in tree search for Masked Diffusion Language Models, providing more efficient branching and stable pruning through novel techniques, leading to superior performance across multiple metrics with computational efficiency.

Abstract: Tree search has recently emerged as a powerful framework for aligning
generative models with task-specific rewards at test time. Applying tree search
to Masked Diffusion Language Models, however, introduces two key challenges:
(i) parallel unmasking yields highly correlated branches, limiting exploration,
and (ii) reward evaluation via sampled completions produces high-variance
estimates, making pruning unstable. We propose TReASURe, a tree-search
test-time alignment method that addresses these issues. It introduces (i)
UnmaskBranch, a branching strategy based on first-hitting unmasking that
diversifies both token content and reveal order with a single model call per
parent node, and (ii) ResubstituteScore, a pruning rule that uses deterministic
resubstitution to score partially masked sequences with low-variance proxy
completions. Theoretically, we quantify branching efficiency gains in NFEs
(number of function evaluations), show that the scoring rule approximates the
true reward with error bounded by predictive uncertainty, and prove
improvements with larger tree widths. Empirically, TReASURe achieves
state-of-the-art results on perplexity, linguistic acceptability, and control
of sentiment and toxicity, outperforming prior methods under matched compute
budgets, with especially strong gains in low-NFE regimes.

</details>


### [38] [Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs](https://arxiv.org/abs/2509.23166)
*Chenxing Wei,Hong Wang,Ying He,Fei Yu,Yao Shu*

Main category: cs.CL

TL;DR: T2PAM enables LLMs to adapt during multi-turn interactions using real-time user feedback, with ROSA providing efficient one-step parameter updates toward optimal policies.


<details>
  <summary>Details</summary>
Motivation: LLMs degrade in extended interactions due to training on static data, lacking real-time adaptation to user feedback.

Method: Propose T2PAM paradigm using user feedback as reward signal, then ROSA algorithm for one-step parameter updates toward optimal policy.

Result: ROSA achieves significant improvements in task effectiveness and efficiency on challenging benchmarks.

Conclusion: T2PAM with ROSA enables efficient in-conversation self-correction for LLMs, with theoretical convergence guarantees.

Abstract: Large Language Models (LLMs) employ multi-turn interaction as a fundamental
paradigm for completing complex tasks. However, their performance often
degrades in extended interactions, as they are typically trained on static,
single-turn data, which hinders their ability to adapt to real-time user
feedback. To address this limitation, we first propose a new paradigm:
Test-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes
user feedback from the ongoing interaction as a reward signal to estimate a
latent optimal policy aligned with user preferences, then updates a small
subset of parameters to steer the model toward this policy, ultimately enabling
efficient in-conversation self-correction. We then introduce Optimum-Referenced
One-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM.
ROSA guides the model parameters toward a theoretical optimal policy in a
single, efficient update step, avoiding costly iterative gradient-based
optimization and minimizing computational overhead. We provide a rigorous
theoretical analysis guaranteeing that the policy of ROSA converges to the
preference of user as the number of interactions increases. Extensive
experiments on challenging benchmark demonstrate that ROSA achieves significant
improvements in both task effectiveness and efficiency.

</details>


### [39] [Pretraining LLM with Latent Thoughts in Continuous Space](https://arxiv.org/abs/2509.23184)
*Boyi Zeng,He Li,Shixiang Song,Yixuan Wang,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: Pretraining Language Models with Latent Thoughts - a method that adds intermediate computational steps during pretraining to improve token generation, outperforming standard models with double the parameters.


<details>
  <summary>Details</summary>
Motivation: Inspired by Chain-of-Thought's success at test-time, the authors aim to leverage similar computational step scaling during pretraining to enhance individual token generation.

Method: Pretrains language models to first generate intermediate latent thoughts (last hidden states) which are then used as input to predict subsequent tokens, enabling refinement in continuous space.

Result: A 1.4B model with latent thoughts outperforms vanilla 2.8B model on same data for both language modeling and downstream tasks. Performance improves consistently with more latent thoughts.

Conclusion: Adding latent thought generation during pretraining significantly enhances model performance without increasing inference cost, demonstrating the value of intermediate computational steps.

Abstract: The remarkable success of Chain-of-Thought (CoT), which enhances performance
by scaling generation steps at test-time, inspires us to ask: can we leverage a
similar scaling of computational steps during pretraining to improve the
generation of each individual token? To address this, we propose a novel
pre-training methodology: Pretraining Language Models with Latent Thoughts. Our
approach pretrains a language model (LM) to first generate an intermediate
latent thought-the last hidden state of the current position-which is then used
as input to predict the actual subsequent token. This additional computational
step enables the LM to refine its prediction within unconstrained continuous
space. Our experiments demonstrate that, at an identical inference cost, a LM
that generates one additional latent thought per token outperforms a standard
model with double the parameters. For instance, ours-1.4B (Pythia Arch),
pretrained on 300B tokens from the Pile, significantly surpasses the vanilla
Pythia-2.8B trained on the same data on both language modeling and a range of
general downstream tasks. Furthermore, increasing the number of latent thoughts
generated before each actual token-forming a chain analogous to
CoT-consistently improves the model's performance.

</details>


### [40] [Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts](https://arxiv.org/abs/2509.23188)
*Guancheng Wan,Leixin Sun,Longxu Dou,Zitong Shi,Fang Wu,Eric Hanchen Jiang,Wenke Huang,Guibin Zhang,Hejia Geng,Xiangru Tang,Zhenfei Yin,Yizhou Sun,Wei Wang*

Main category: cs.CL

TL;DR: The paper addresses hierarchical compliance failures in LLM-powered multi-agent systems under instruction conflicts, proposing a three-stage framework (CRAS diagnosis, attention drift localization, SAIL surgical alignment) to improve reliability without full-model finetuning.


<details>
  <summary>Details</summary>
Motivation: Reliability-critical deployment of LLM-powered multi-agent systems is hindered by systemic failure mode of hierarchical compliance under instruction conflicts, where agents misprioritize system-level rules. Current macro-level metrics obscure micro-level violations and offer little actionable guidance.

Method: Three-stage framework: (1) Diagnose with Contextualized Role Adherence Score (CRAS) that decomposes role adherence into four measurable dimensions; (2) Localize via attention drift analysis revealing instruction conflicts are resolved by middle-layer attention heads; (3) Align using Surgical Alignment of Instruction Layers (SAIL) with LoRA on localized focal layers and token-weighted DPO-style preference objective.

Result: Across standard benchmarks and MAS frameworks, the surgical approach improves instruction hierarchy compliance (e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.

Conclusion: The proposed framework effectively addresses hierarchical compliance failures in multi-agent systems through targeted diagnosis, localization, and surgical alignment, enabling reliability-critical deployment without the computational cost of full-model finetuning.

Abstract: Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly
advanced collaborative reasoning, tool use, and role-specialized coordination
in complex tasks. However, reliability-critical deployment remains hindered by
a systemic failure mode: hierarchical compliance under instruction conflicts
(system-user, peer-peer), where agents misprioritize system-level rules in the
presence of competing demands. Moreover, widely used macro-level metrics (e.g.,
pass@k) obscure these micro-level violations and offer little actionable
guidance for remedy. In this work, we present a full-stack, three-stage
framework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a
query-wise, context-aware scoring metric that decomposes role adherence into
four measurable dimensions; (2) Localize - attention drift analysis revealing
that instruction conflicts are resolved by attention heads that are largely
concentrated in middle layers; (3) Align - Surgical Alignment of Instruction
Layers (SAIL), which installs LoRA only on the localized focal layers and
optimizes a token-weighted DPO-style preference objective that credits tokens
by their focal attentional contribution. Across standard benchmarks and MAS
frameworks, our surgical approach improves instruction hierarchy compliance
(e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.

</details>


### [41] [Estimating the strength and timing of syntactic structure building in naturalistic reading](https://arxiv.org/abs/2509.23195)
*Nan Wang,Jiaxuan Li*

Main category: cs.CL

TL;DR: This study uses EEG and eye-tracking data to show that phrase structure construction precedes syntactic category detection during natural reading, challenging traditional violation paradigms.


<details>
  <summary>Details</summary>
Motivation: To disentangle syntactic category detection and phrase structure construction processes in sentence processing, which are conflated in traditional violation paradigms.

Method: Used co-registered EEG and eye-tracking data from the ZuCo corpus, analyzed gaze transitions, applied Bayesian network modeling, and examined fixation-related potentials during naturalistic reading.

Result: Readers preferentially moved between syntactic heads; structural depth was the strongest driver of reading deviations; syntactic surprisal influenced neural activity before word onset and during early integration.

Conclusion: Phrase structure construction can precede category detection and dominate lexical influences, supporting a predictive "tree-scaffolding" account of comprehension.

Abstract: A central question in psycholinguistics is the timing of syntax in sentence
processing. Much of the existing evidence comes from violation paradigms, which
conflate two separable processes - syntactic category detection and phrase
structure construction - and implicitly assume that phrase structure follows
category detection. In this study, we use co-registered EEG and eye-tracking
data from the ZuCo corpus to disentangle these processes and test their
temporal order under naturalistic reading conditions. Analyses of gaze
transitions showed that readers preferentially moved between syntactic heads,
suggesting that phrase structures, rather than serial word order, organize
scanpaths. Bayesian network modeling further revealed that structural depth was
the strongest driver of deviations from linear reading, outweighing lexical
familiarity and surprisal. Finally, fixation-related potentials demonstrated
that syntactic surprisal influences neural activity before word onset (-184 to
-10 ms) and during early integration (48 to 300 ms). These findings extend
current models of syntactic timing by showing that phrase structure
construction can precede category detection and dominate lexical influences,
supporting a predictive "tree-scaffolding" account of comprehension.

</details>


### [42] [From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs](https://arxiv.org/abs/2509.23196)
*Haonan Wang,Weida Liang,Zihang Fu,Nie Zheng,Yifan Zhang,Yao Tong,Tongyao Zhu,Hao Jiang,Chuang Li,Jiaying Wu,Kenji Kawaguchi*

Main category: cs.CL

TL;DR: The paper addresses the paradox where reasoning LLMs perform worse with few-shot CoT than direct answering, and introduces Insight-to-Solve (I2S) framework to effectively utilize demonstrations by converting them into reusable insights and generating target-specific reasoning traces.


<details>
  <summary>Details</summary>
Motivation: Recent reasoning LLMs trained with verifier-based reinforcement learning often show degraded performance with few-shot Chain-of-Thought (CoT) compared to direct answering, creating a paradox that needs investigation and solution.

Method: Introduces Insight-to-Solve (I2S), a sequential test-time procedure that converts demonstrations into explicit, reusable insights and derives target-specific reasoning traces, with an optional self-refinement step (I2S+) for coherence and correctness.

Result: Extensive experiments show I2S and I2S+ consistently outperform both direct answering and test-time scaling baselines across diverse benchmarks and models. GPT-4.1 improves by +14.0% on AIME'25, and o1-mini improves by +2.7% on AIME and +1.7% on GPQA.

Conclusion: The insight-refine-solve framework effectively harnesses in-context demonstrations, overcoming the limitations of traditional few-shot CoT by addressing semantic misguidance and strategy transfer failure through explicit insight extraction and target-specific reasoning generation.

Abstract: Recent reasoning LLMs (RLMs), especially those trained with verifier-based
reinforcement learning, often perform worse with few-shot CoT than with direct
answering. We revisit this paradox using high-quality reasoning traces from
DeepSeek-R1 as demonstrations and find that adding more exemplars consistently
degrades accuracy, even when demonstrations are optimal. A detailed analysis
reveals two mechanisms behind this decline: (i) semantic misguidance, where
high textual similarity leads the model to treat the target as the same as the
exemplar and to copy intermediate steps verbatim; and (ii) strategy transfer
failure, where the model struggles to extract useful reasoning strategies and
apply them to target questions. Guided by these, we introduce Insight-to-Solve
(I2S), a sequential test-time procedure that turns demonstrations into
explicit, reusable insights and derives a target-specific reasoning trace;
optionally, the reasoning is self-refined for coherence and correctness (I2S+).
Extensive experiments on diverse benchmarks show that I2S and I2S+ consistently
outperform both direct answering and test-time scaling baselines across open-
and closed-source models. Even for GPT models, our method helps: on AIME'25,
GPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on
GPQA, indicating that in-context demonstrations can be harnessed effectively
via insight-refine-solve framework.

</details>


### [43] [Global Beats, Local Tongue: Studying Code Switching in K-pop Hits on Billboard Charts](https://arxiv.org/abs/2509.23197)
*Aditya Narayan Sankaran,Reza Farahbakhsh,Noel Crespi*

Main category: cs.CL

TL;DR: Analysis of code-switching patterns in globally charting K-pop songs shows English dominance and high code-switching frequency, with no significant gender differences but female solo artists using more English.


<details>
  <summary>Details</summary>
Motivation: To investigate linguistic strategies in K-pop songs achieving global chart success, focusing on code-switching and English usage patterns.

Method: Compiled dataset of K-pop songs on Billboard Hot 100 and Global 200 charts (2017-2025), analyzed English/Korean proportions, code-switching frequency, performed statistical tests and gender classification using multilingual embeddings.

Result: English dominates globally charting K-pop songs; high code-switching observed; no significant gender differences; female solo artists use more English; gender classification achieved F1=0.76; higher English usage may be more critical for US chart success.

Conclusion: Linguistic choices in K-pop lyrics are shaped by global market pressures, revealing stylistic patterns reflecting performer identity and chart context.

Abstract: Code switching, particularly between Korean and English, has become a
defining feature of modern K-pop, reflecting both aesthetic choices and global
market strategies. This paper is a primary investigation into the linguistic
strategies employed in K-pop songs that achieve global chart success, with a
focus on the role of code-switching and English lyric usage. A dataset of K-pop
songs that appeared on the Billboard Hot 100 and Global 200 charts from 2017 to
2025, spanning 14 groups and 8 solo artists, was compiled. Using this dataset,
the proportion of English and Korean lyrics, the frequency of code-switching,
and other stylistic features were analysed. It was found that English dominates
the linguistic landscape of globally charting K-pop songs, with both male and
female performers exhibiting high degrees of code-switching and English usage.
Statistical tests indicated no significant gender-based differences, although
female solo artists tend to favour English more consistently. A classification
task was also performed to predict performer gender from lyrics, achieving
macro F1 scores up to 0.76 using multilingual embeddings and handcrafted
features. Finally, differences between songs charting on the Hot 100 versus the
Global 200 were examined, suggesting that, while there is no significant gender
difference in English, higher English usage may be more critical for success in
the US-focused Hot 100. The findings highlight how linguistic choices in K-pop
lyrics are shaped by global market pressures and reveal stylistic patterns that
reflect performer identity and chart context.

</details>


### [44] [Steering Prepositional Phrases in Language Models: A Case of with-headed Adjectival and Adverbial Complements in Gemma-2](https://arxiv.org/abs/2509.23204)
*Stefan Arnold,René Gröbner*

Main category: cs.CL

TL;DR: The paper investigates how language models resolve prepositional phrase ambiguity between instrumental and attributive readings in Gemma-2, finding a 3:4 preference for instrumental interpretations and demonstrating control over this distribution through attention head manipulation.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms that resolve the ambiguity in prepositional phrases between instrumental adjuncts (verb modification) and attributive modifiers (noun modification), which remains poorly understood in language models.

Method: Used a prompt suite with prepositional phrases that equally accommodate both instrumental and attributive continuations, projected activations into vocabulary space to identify attention heads, and scaled value vectors of specific attention heads to control generation.

Result: Found a 3:4 preference for instrumental readings; by scaling a single attention head's value vector, successfully shifted the distribution from 33% instrumental to 36% attributive interpretations.

Conclusion: Individual attention heads in language models encode preferences for prepositional phrase interpretations, and targeted intervention can effectively control the functional role distribution of complements.

Abstract: Language Models, when generating prepositional phrases, must often decide for
whether their complements functions as an instrumental adjunct (describing the
verb adverbially) or an attributive modifier (enriching the noun adjectivally),
yet the internal mechanisms that resolve this split decision remain poorly
understood. In this study, we conduct a targeted investigation into Gemma-2 to
uncover and control the generation of prepositional complements. We assemble a
prompt suite containing with-headed prepositional phrases whose contexts
equally accommodate either an instrumental or attributive continuation,
revealing a strong preference for an instrumental reading at a ratio of 3:4. To
pinpoint individual attention heads that favor instrumental over attributive
complements, we project activations into the vocabulary space. By scaling the
value vector of a single attention head, we can shift the distribution of
functional roles of complements, attenuating instruments to 33% while elevating
attributes to 36%.

</details>


### [45] [PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness](https://arxiv.org/abs/2509.23206)
*Huacan Chai,Zijie Cao,Maolin Ran,Yingxuan Yang,Jianghao Lin,pengxin,Hairui Wang,Renjie Ding,Ziyu Wan,Muning Wen,Weiwen Liu,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.CL

TL;DR: PARL-MT is a framework that incorporates progress awareness into LLM training for multi-turn function calling, combining automatic dataset generation with progress-aware reinforcement learning to improve long-horizon task execution.


<details>
  <summary>Details</summary>
Motivation: Real-world applications like travel planning require multi-turn conversations where LLMs need progress awareness - the ability to summarize past interactions and plan future actions. Existing approaches either neglect task-level planning or struggle with redundancy in RL training.

Method: PARL-MT combines: (1) Progress Awareness Generation (PAG) pipeline that automatically constructs datasets coupling conversation summaries with future task planning, and (2) Progress Awareness-Guided Reinforcement Learning (PAG-RL) that integrates progress awareness into RL training to reduce redundancy and improve action-task alignment.

Result: Empirical results on two public benchmarks demonstrate that PARL-MT significantly outperforms existing methods.

Conclusion: The framework highlights the effectiveness of progress awareness in enabling robust and efficient multi-turn function calling for LLMs.

Abstract: Large language models (LLMs) have achieved impressive success in single-turn
function calling, yet real-world applications such as travel planning or
multi-stage data analysis typically unfold across multi-turn conversations. In
these settings, LLMs must not only issue accurate function calls at each step
but also maintain progress awareness, the ability to summarize past
interactions and plan future actions to ensure coherent, long-horizon task
execution. Existing approaches, however, either reduce multi-turn training to
isolated single-turn samples, which neglects task-level planning, or employ
end-to-end reinforcement learning (RL) that struggles with redundancy and lacks
explicit integration of progress awareness. To overcome these limitations, we
introduce PARL-MT, a framework that explicitly incorporates progress awareness
into LLM training for multi-turn function calling. PARL-MT combines (i) a
Progress Awareness Generation (PAG) pipeline, which automatically constructs
datasets coupling conversation summaries with future task planning, and (ii) a
Progress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which
integrates progress awareness into RL training to reduce contextual redundancy
and improve alignment between local actions and global task completion.
Empirical results on two public benchmarks demonstrate that PARL-MT
significantly outperforms existing methods, highlighting the effectiveness of
progress awareness in enabling robust and efficient multi-turn function
calling.

</details>


### [46] [A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks](https://arxiv.org/abs/2509.23208)
*Haorui Yu,Ramon Ruiz-Dolz,Qiufeng Yi*

Main category: cs.CL

TL;DR: This paper develops a quantitative framework to evaluate Visual Language Models' (VLMs) ability to critique traditional Chinese painting, using persona-guided prompting to assess performance across multiple dimensions.


<details>
  <summary>Details</summary>
Motivation: To systematically test and evaluate current mainstream VLMs' capabilities in generating critiques for traditional Chinese painting, identifying their strengths and limitations in art critique tasks.

Method: Developed a quantitative framework by extracting multi-dimensional evaluative features from human expert critiques using zero-shot classification, defined critic personas, and employed persona-guided prompting to evaluate VLMs like Llama, Qwen, and Gemini.

Result: Revealed current performance levels, strengths, and areas for improvement of VLMs in art critique domain, providing insights into their capabilities in complex semantic understanding and content generation.

Conclusion: The study offers valuable insights into VLMs' potential and limitations for traditional Chinese painting critique, with publicly available experimental code for further research.

Abstract: This study aims to test and evaluate the capabilities and characteristics of
current mainstream Visual Language Models (VLMs) in generating critiques for
traditional Chinese painting. To achieve this, we first developed a
quantitative framework for Chinese painting critique. This framework was
constructed by extracting multi-dimensional evaluative features covering
evaluative stance, feature focus, and commentary quality from human expert
critiques using a zero-shot classification model. Based on these features,
several representative critic personas were defined and quantified. This
framework was then employed to evaluate selected VLMs such as Llama, Qwen, or
Gemini. The experimental design involved persona-guided prompting to assess the
VLM's ability to generate critiques from diverse perspectives. Our findings
reveal the current performance levels, strengths, and areas for improvement of
VLMs in the domain of art critique, offering insights into their potential and
limitations in complex semantic understanding and content generation tasks. The
code used for our experiments can be publicly accessed at:
https://github.com/yha9806/VULCA-EMNLP2025.

</details>


### [47] [Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models](https://arxiv.org/abs/2509.23233)
*Sina J. Semnani,Jirayu Burapacheep,Arpandeep Khatua,Thanawan Atchariyachanvanit,Zheng Wang,Monica S. Lam*

Main category: cs.CL

TL;DR: CLAIRE is an LLM-based system that detects inconsistencies in Wikipedia by combining reasoning with retrieval, helping editors identify contradictions more efficiently. The study found 3.3% of English Wikipedia facts contain contradictions, creating the WIKICOLLIDE benchmark.


<details>
  <summary>Details</summary>
Motivation: Wikipedia serves as critical training data for LLMs and RAG systems, making its accuracy essential. The paper addresses the problem of factual inconsistencies in Wikipedia that undermine its reliability.

Method: Developed CLAIRE - an agentic system combining LLM reasoning with retrieval to detect corpus-level inconsistencies. Used human annotation with CLAIRE assistance to create WIKICOLLIDE benchmark from real Wikipedia data.

Result: 87.5% of Wikipedia editors reported higher confidence using CLAIRE, identifying 64.7% more inconsistencies. Found 3.3% of English Wikipedia facts contain contradictions, affecting 7.3% of FEVEROUS and 4.0% of AmbigQA examples. Best automated system achieved only 75.1% AUROC.

Conclusion: Contradictions are a measurable problem in Wikipedia. LLM-based systems like CLAIRE provide practical tools to help editors improve knowledge consistency at scale, with significant room for improvement in automated detection.

Abstract: Wikipedia is the largest open knowledge corpus, widely used worldwide and
serving as a key resource for training large language models (LLMs) and
retrieval-augmented generation (RAG) systems. Ensuring its accuracy is
therefore critical. But how accurate is Wikipedia, and how can we improve it?
  We focus on inconsistencies, a specific type of factual inaccuracy, and
introduce the task of corpus-level inconsistency detection. We present CLAIRE,
an agentic system that combines LLM reasoning with retrieval to surface
potentially inconsistent claims along with contextual evidence for human
review. In a user study with experienced Wikipedia editors, 87.5% reported
higher confidence when using CLAIRE, and participants identified 64.7% more
inconsistencies in the same amount of time.
  Combining CLAIRE with human annotation, we contribute WIKICOLLIDE, the first
benchmark of real Wikipedia inconsistencies. Using random sampling with
CLAIRE-assisted analysis, we find that at least 3.3% of English Wikipedia facts
contradict another fact, with inconsistencies propagating into 7.3% of FEVEROUS
and 4.0% of AmbigQA examples. Benchmarking strong baselines on this dataset
reveals substantial headroom: the best fully automated system achieves an AUROC
of only 75.1%.
  Our results show that contradictions are a measurable component of Wikipedia
and that LLM-based systems like CLAIRE can provide a practical tool to help
editors improve knowledge consistency at scale.

</details>


### [48] [Fin-ExBERT: User Intent based Text Extraction in Financial Context using Graph-Augmented BERT and trainable Plugin](https://arxiv.org/abs/2509.23259)
*Soumick Sarker,Abhijit Kumar Rai*

Main category: cs.CL

TL;DR: Fin-ExBERT: A lightweight BERT-based framework with LoRA adapters for extracting intent-relevant sentences from financial dialogue transcripts using two-stage training and dynamic thresholding.


<details>
  <summary>Details</summary>
Motivation: Financial dialogue transcripts present challenges for information extraction due to informal structure, domain-specific vocabulary, and variable intent density, requiring specialized approaches.

Method: Domain-adapted BERT backbone enhanced with LoRA adapters, two-stage training with progressive unfreezing (classifier head first, then full model), and dynamic thresholding using probability curvature (elbow detection).

Result: Strong precision and F1 performance on real-world transcripts, with interpretable output suitable for downstream auditing and question-answering workflows.

Conclusion: The framework provides a deployable solution for financial dialogue mining with batched evaluation, visualization, and calibrated export capabilities.

Abstract: Financial dialogue transcripts pose a unique challenge for sentence-level
information extraction due to their informal structure, domain-specific
vocabulary, and variable intent density. We introduce Fin-ExBERT, a lightweight
and modular framework for extracting user intent-relevant sentences from
annotated financial service calls. Our approach builds on a domain-adapted BERT
(Bidirectional Encoder Representations from Transformers) backbone enhanced
with LoRA (Low-Rank Adaptation) adapters, enabling efficient fine-tuning using
limited labeled data. We propose a two-stage training strategy with progressive
unfreezing: initially training a classifier head while freezing the backbone,
followed by gradual fine-tuning of the entire model with differential learning
rates. To ensure robust extraction under uncertainty, we adopt a dynamic
thresholding strategy based on probability curvature (elbow detection),
avoiding fixed cutoff heuristics. Empirical results show strong precision and
F1 performance on real-world transcripts, with interpretable output suitable
for downstream auditing and question-answering workflows. The full framework
supports batched evaluation, visualization, and calibrated export, offering a
deployable solution for financial dialogue mining.

</details>


### [49] [A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models](https://arxiv.org/abs/2509.23286)
*Wonje Jeung,Sangyeon Yoon,Yoonjun Cho,Dongjae Jeon,Sangwoo Shin,Hyesoo Hong,Albert No*

Main category: cs.CL

TL;DR: A2D is a token-level alignment method that makes diffusion LLMs emit [EOS] refusal signals when harmful content appears, providing robust defense against any-order generation attacks and enabling real-time safety monitoring.


<details>
  <summary>Details</summary>
Motivation: Diffusion LLMs' any-order generation flexibility creates security vulnerabilities where harmful content can appear at arbitrary positions, and existing template-based prefilling attacks like DIJA can bypass response-level refusal mechanisms.

Method: A2D aligns dLLMs at token-level using randomized masking to emit [EOS] refusal signals whenever harmful content arises, making the defense robust to any decoding order and any-step prefilling attacks.

Result: A2D slashed DIJA success rates from over 80% to near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B) and enables early rejection with up to 19.3x faster safe termination through thresholded [EOS] probabilities.

Conclusion: Token-level alignment with randomized masking provides effective defense against any-order generation attacks in diffusion LLMs, enabling robust safety mechanisms and real-time monitoring capabilities.

Abstract: Diffusion large language models (dLLMs) enable any-order generation, but this
flexibility enlarges the attack surface: harmful spans may appear at arbitrary
positions, and template-based prefilling attacks such as DIJA bypass
response-level refusals. We introduce A2D (Any-Order, Any-Step Defense), a
token-level alignment method that aligns dLLMs to emit an [EOS] refusal signal
whenever harmful content arises. By aligning safety directly at the token-level
under randomized masking, A2D achieves robustness to both any-decoding-order
and any-step prefilling attacks under various conditions. It also enables
real-time monitoring: dLLMs may begin a response but automatically terminate if
unsafe continuation emerges. On safety benchmarks, A2D consistently prevents
the generation of harmful outputs, slashing DIJA success rates from over 80% to
near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B), and
thresholded [EOS] probabilities allow early rejection, yielding up to 19.3x
faster safe termination.

</details>


### [50] [Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces](https://arxiv.org/abs/2509.23291)
*Joseph Marvin Imperial,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: Policy Reasoning Traces (PRT) are specialized reasoning chains that improve LLM performance in policy compliance assessment, achieving state-of-the-art results for HIPAA and GDPR policies.


<details>
  <summary>Details</summary>
Motivation: Human experts follow systematic processes for policy compliance assessment, but documenting gold-standard reasoning is costly. There's a need to enhance LLM capabilities in this domain.

Method: Introduce Policy Reasoning Traces (PRT) - specialized generated reasoning chains that serve as a reasoning bridge to improve LLM's policy compliance assessment capabilities. Use PRTs for both inference-time and training-time scenarios.

Result: Significantly enhances performance of open-weight and commercial models, setting new state-of-the-art for HIPAA and GDPR policies. Improves LLM's ability to accurately cite policy clauses and influences compliance decisions through high utilization from raw chains of thought.

Conclusion: PRTs effectively bridge the reasoning gap in policy compliance assessment, providing substantial improvements in accuracy and citation capabilities for LLMs across different policy domains.

Abstract: Policy compliance assessment is a fundamental task of evaluating whether an
input case strictly complies with a set of human-defined rules, more generally
known as policies. In practice, human experts follow a systematic, step-by-step
process to identify violations with respect to specific stipulations outlined
in the policy. However, such documentation of gold-standard, expert-level
reasoning processes is costly to acquire. In this paper, we introduce Policy
Reasoning Traces (PRT), a form of specialized generated reasoning chains that
serve as a reasoning bridge to improve an LLM's policy compliance assessment
capabilities. Our empirical evaluations demonstrate that the use of PRTs for
both inference-time and training-time scenarios significantly enhances the
performance of open-weight and commercial models, setting a new
state-of-the-art for HIPAA and GDPR policies. Beyond accuracy gains, we also
highlight how PRTs can improve an LLM's ability to accurately cite policy
clauses, as well as influence compliance decisions through their high
utilization from the raw chains of thought.

</details>


### [51] [Learning to Reason in Structured In-context Environments with Reinforcement Learning](https://arxiv.org/abs/2509.23330)
*Peng Yu,Zeyuan Zhao,Shao Zhang,Luoyi Fu,Xinbing Wang,Ying Wen*

Main category: cs.CL

TL;DR: SIE framework creates scalable reasoning environments from structured data to enhance LLMs' compositional reasoning skills with verifiability.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning environments lack scalability and generalizability - mathematical/coding environments need expert annotation, while game-based environments produce specialized skills.

Method: Automatically construct reasoning environments from large-scale structured data using rich compositional patterns, with explicit schemas and reasoning chains for rule-based verification.

Result: Substantial improvements in in-domain structured reasoning and effective generalization to out-of-domain mathematical and logical reasoning tasks; robust performance even in information-limited partial SIEs.

Conclusion: SIE framework successfully addresses scalability, generalizability, and verifiability challenges in LLM reasoning environments, enabling effective compositional reasoning skill learning and transfer.

Abstract: Large language models (LLMs) have achieved significant advancements in
reasoning capabilities through reinforcement learning (RL) via environmental
exploration. As the intrinsic properties of the environment determine the
abilities that LLMs can learn, the environment plays a important role in the RL
finetuning process. An ideal LLM reasoning environment should possess three
core characteristics: scalability, generalizable reasoning, and verifiability.
However, existing mathematical and coding environments are difficult to scale
due to heavy reliance on expert annotation, while the skills learned in
game-based environments are too specialized to generalize. To bridge this gap,
we introduce the \textbf{S}tructured \textbf{I}n-context \textbf{E}nvironment
(SIE) framework. SIE achieves scalability by automatically constructing
reasoning environments from large-scale structured data, where the rich
compositional patterns naturally support generalizable reasoning. Moreover, the
explicit schemas and reasoning chains in structured data provide a foundation
for rule-based verifiability. Experimental results show that SIE framework not
only achieves substantial improvements in in-domain structured reasoning, but
also enables the learned compositional reasoning skills to generalize
effectively to out-of-domain mathematical and logical reasoning tasks. We
further explored learning in information-limited partial SIEs and found that
LLMs can infer the missing information through exploring the environment,
leading to robust reasoning improvements and generalization performance.

</details>


### [52] [C-Evolve: Consensus-based Evolution for Prompt Groups](https://arxiv.org/abs/2509.23331)
*Tiancheng Li,Yuhang Wang,Zhiyang Chen,Zijun Wang,Liyuan Ma,Guo-jun Qi*

Main category: cs.CL

TL;DR: C-Evolve is an evolutionary algorithm that discovers groups of prompts whose aggregated outputs achieve optimal performance through majority voting, outperforming individual prompt evolution methods.


<details>
  <summary>Details</summary>
Motivation: Few works explore whether aggregating results from multiple prompts to reach consensus can further advance AI system capabilities beyond single prompt evolution.

Method: Uses island-based evolutionary algorithm to maintain population diversity, selects prompts from distinct islands to form groups, and evaluates individual prompts' contribution within groups using voting score as fitness metric.

Result: Achieves state-of-the-art performance: 70.67% on HotpotQA and 43.88% on IFBench with Qwen3-8B (4.95% and 2.73% higher than GEPA), and 47.96% on IFBench and 95.33% on MATH with GPT-4.1-mini.

Conclusion: C-Evolve demonstrates competitive performance by producing prompts with higher potential to form high-performing groups through consensus-based evolution.

Abstract: Prompt evolution algorithms offer a powerful paradigm for enhancing AI
systems based on closed-source models, while few work explores whether
aggregating results from multiple prompts to reach a consensus can further
advance the system capability boundary. In this paper, we introduce
Consensus-Evolve (C-Evolve), an evolutionary algorithm that discovers a group
of prompts whose aggregated outputs after majority voting achieve optimal
performance. More specifically, C-Evolve employs an island-based evolutionary
algorithm to maintain population diversity, and prompts from distinct islands
are selected to form groups to aggregate their outputs. The key difference from
single individual evolution is a voting score, which evaluates each individual
prompt's contribution within groups. We take this as the fitness score for
evolution instead of individual performance. Consequently, C-Evolve is more
likely to produce and maintain prompts with higher potential to form a
high-performing group and eliminate low-performing ones, gradually improving
the group performance after reaching consensus. Our method achieves
state-of-the-art performance across a wide range of tasks, including both
open-ended tasks like HotpotQA and closed-ended tasks like MATH. On Qwen3-8B,
C-Evolve achieves 70.67% on HotpotQA and 43.88% on IFBench, which are 4.95% and
2.73% higher than GEPA, respectively. For GPT-4.1-mini, the accuracy on IFBench
is further improved to 47.96% and reaches 95.33% in the MATH benchmark. These
results demonstrate the C-Evolve's competitive performance.

</details>


### [53] [Dual-Space Smoothness for Robust and Balanced LLM Unlearning](https://arxiv.org/abs/2509.23362)
*Han Yan,Zheyuan Liu,Meng Jiang*

Main category: cs.CL

TL;DR: PRISM is a unified framework for machine unlearning that enforces dual-space smoothness in representation and parameter spaces to improve robustness and balance unlearning metrics against attacks.


<details>
  <summary>Details</summary>
Motivation: Address challenges in machine unlearning including catastrophic forgetting, metric imbalance, and vulnerability to relearn/jailbreak attacks that plague current SOTA methods.

Method: Two-stage smoothness optimization: (1) representation space stage with robustly trained probe for jailbreak defense, (2) parameter-space stage that decouples retain-forget gradient conflicts and smooths parameter space.

Result: Extensive experiments on WMDP and MUSE datasets show PRISM outperforms SOTA baselines under multiple attacks while achieving better balance among key metrics.

Conclusion: PRISM provides a robust and balanced solution for machine unlearning that effectively addresses current limitations in the field.

Abstract: With the rapid advancement of large language models, Machine Unlearning has
emerged to address growing concerns around user privacy, copyright
infringement, and overall safety. Yet state-of-the-art (SOTA) unlearning
methods often suffer from catastrophic forgetting and metric imbalance, for
example by over-optimizing one objective (e.g., unlearning effectiveness,
utility preservation, or privacy protection) at the expense of others. In
addition, small perturbations in the representation or parameter space can be
exploited by relearn and jailbreak attacks. To address these challenges, we
propose PRISM, a unified framework that enforces dual-space smoothness in
representation and parameter spaces to improve robustness and balance
unlearning metrics. PRISM consists of two smoothness optimization stages: (i) a
representation space stage that employs a robustly trained probe to defend
against jailbreak attacks, and (ii) a parameter-space stage that decouples
retain-forget gradient conflicts, reduces imbalance, and smooths the parameter
space to mitigate relearning attacks. Extensive experiments on WMDP and MUSE,
across conversational-dialogue and continuous-text settings, show that PRISM
outperforms SOTA baselines under multiple attacks while achieving a better
balance among key metrics.

</details>


### [54] [MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction](https://arxiv.org/abs/2509.23368)
*Xinchun Su,Chunxu Luo,Yixuan Li,Weidong Yang,Lipeng Ma*

Main category: cs.CL

TL;DR: MedCritical is a two-stage framework that uses self-play and DPO to enhance small language models' medical reasoning without expensive teacher models, achieving SOTA performance on CMExam benchmark.


<details>
  <summary>Details</summary>
Motivation: Small language models underperform in complex medical reasoning tasks compared to large models, and traditional knowledge distillation methods are costly and inefficient.

Method: Two-stage framework: 1) Extract thought templates from teacher model to guide student model, 2) Use DPO through model self-iteration collaboration where student plays against its own correction trajectory.

Result: MedCritical 7B outperforms Taiyi and Huatuo-o1-7B by 3.04% and 10.12% respectively on CMExam benchmark, achieving new SOTA among 7B-class models.

Conclusion: The self-learning DPO approach enables small models to achieve comparable results to traditional knowledge distillation at lower cost, effectively enhancing complex medical reasoning capabilities.

Abstract: In the field of medicine, complex reasoning tasks such as clinical diagnosis,
treatment planning, and medical knowledge integration pose significant
challenges, where small language models often underperform compared to large
language models like GPT-4 and Deepseek. Recent knowledge distillation-based
methods aim to address these issues through teacher-guided error correction,
but this LLM as judge approach remains challenging in terms of cost, time, and
efficiency. To circumvent this issue, we propose a novel two-stage framework,
MedCritical, which uses a small language model fine-tuned by a large teacher
model to play against itself. In the first stage, we extract high-level and
detailed long-chain thought templates from the teacher model to guide the
student model to generate more complex reasoning thoughts. In the second stage,
we introduce direct preference optimization (DPO) through model self-iteration
collaboration to enhance the reasoning ability of the student model by playing
against the correction trajectory of the fine-tuned model during training. This
model self-learning DPO approach teaches the student model to use its own
error-driven insights to consolidate its skills and knowledge to solve complex
problems, and achieves comparable results to traditional knowledge distillation
methods using teacher models at a lower cost. Notably, our MedCritical 7B model
outperforms the Taiyi and Huatuo-o1-7B models by 3.04\% and 10.12\%
respectively on the CMExam benchmark, achieving new SOTA performance among
7B-class small models.

</details>


### [55] [Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization](https://arxiv.org/abs/2509.23371)
*Junming Yang,Ning Xu,Biao Liu,Shiqi Qiao,Xin Geng*

Main category: cs.CL

TL;DR: MetaAPO is a novel preference optimization framework that dynamically couples data generation with model training using a meta-learner to estimate alignment gaps, enabling targeted online sampling and sample-wise weighting to balance online/offline data.


<details>
  <summary>Details</summary>
Motivation: Address distribution mismatch between pre-collected offline preference data and evolving model policy, overcoming limitations of static heuristics and decoupled online sampling strategies that fail to adapt to dynamic learning states.

Method: Uses lightweight meta-learner as 'alignment gap estimator' to evaluate benefits of on-policy sampling vs offline data, guides targeted online generation, assigns sample-wise meta-weights to optimization objective for dynamic balancing of data quality and distribution.

Result: Outperforms existing preference optimization approaches on AlpacaEval 2, Arena-Hard and MT-Bench across various settings, while reducing 42% in online annotation costs.

Conclusion: MetaAPO effectively bridges the distribution gap in preference optimization through dynamic coupling of data generation and training, providing superior performance with reduced annotation costs.

Abstract: Preference optimization is crucial for aligning large language models (LLMs)
with human values and intentions. A significant challenge in this process is
the distribution mismatch between pre-collected offline preference data and the
evolving model policy. Existing methods attempt to reduce this gap using static
heuristics or decoupled online sampling strategies, but they often fail to
adapt to the model's dynamic learning state. To bridge this gap, we propose
Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework
that dynamically couples data generation with model training. MetaAPO employs a
lightweight meta-learner, as an "alignment gap estimator", to evaluate the
potential benefits of on-policy sampling in relation to offline data. This
guides targeted online generation and assigns sample-wise meta-weights to the
optimization objective, dynamically balancing the quality and distribution of
online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench
demonstrate that MetaAPO consistently outperforms existing preference
optimization approaches across various settings, while reducing 42% in online
annotation costs.

</details>


### [56] [CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding](https://arxiv.org/abs/2509.23379)
*Xi Zhang,Zaiqiao Meng,Jake Lever,Edmond S. L. Ho*

Main category: cs.CL

TL;DR: CCD is a training-free framework that reduces medical hallucinations in radiology MLLMs by integrating clinical signals from expert models through dual-stage contrastive decoding.


<details>
  <summary>Details</summary>
Motivation: MLLMs in radiology often generate clinically unsupported descriptions (medical hallucinations) due to over-sensitivity to clinical sections, posing serious risks in medical applications requiring accuracy.

Method: Clinical Contrastive Decoding (CCD) - a training-free, retrieval-free inference framework that integrates structured clinical signals from radiology expert models using dual-stage contrastive mechanism to refine token-level logits during generation.

Result: CCD consistently improves performance on radiology report generation across three datasets and multiple models, achieving up to 17% improvement in RadGraph-F1 on MIMIC-CXR dataset when applied to state-of-the-art RRG models.

Conclusion: CCD provides a lightweight and generalizable solution for mitigating medical hallucinations, effectively bridging expert models and MLLMs in radiology without modifying the base MLLM.

Abstract: Multimodal large language models (MLLMs) have recently achieved remarkable
progress in radiology by integrating visual perception with natural language
understanding. However, they often generate clinically unsupported
descriptions, known as medical hallucinations, which pose serious risks in
medical applications that demand accuracy and image-grounded outputs. Through
empirical analysis, we find that prompt-induced hallucinations remain prevalent
in radiology MLLMs, largely due to over-sensitivity to clinical sections. To
address this, we introduce Clinical Contrastive Cecoding (CCD), a training-free
and retrieval-free inference framework that integrates structured clinical
signals from task-specific radiology expert models. CCD introduces a dual-stage
contrastive mechanism to refine token-level logits during generation, thereby
enhancing clinical fidelity without modifying the base MLLM. Experiments on
three datasets and multiple models demonstrate that CCD consistently improves
overall performance on radiology report generation (RRG). On the MIMIC-CXR
dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to
state-of-the-art RRG models. Our approach provides a lightweight and
generalisable solution for mitigating medical hallucinations, effectively
bridging expert models and MLLMs in radiology.

</details>


### [57] [Guard Vector: Beyond English LLM Guardrails with Task-Vector Composition and Streaming-Aware Prefix SFT](https://arxiv.org/abs/2509.23381)
*Wonhyuk Lee,Youngchol Kim,Yunjin Park,Junhyung Moon,Dongyoung Jeong,Wanjin Park*

Main category: cs.CL

TL;DR: Guard Vector is a safety task vector created from parameter differences between guardrail and pretrained models. When combined with target models, it improves safety classification across multiple languages without extra training, supports streaming, and reduces latency.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient and portable safety mechanism for language models that works across different languages and architectures while reducing computational requirements.

Method: Compute Guard Vector as parameter difference between guardrail and pretrained models, compose with target models to create TGM, use prefix-based training with single-token classifier for streaming awareness.

Result: TGM improves classification quality across standard safety suites, enables language extensibility to Chinese/Japanese/Korean without additional training, demonstrates portability across Llama and Gemma backbones, preserves quality under streaming, and increases throughput.

Conclusion: The approach reduces data and compute requirements while promoting streaming-aware evaluation, contributing to more responsible AI development.

Abstract: We introduce Guard Vector, a safety task vector computed as the parameter
difference between a guardrail model (Guard Model) and a same-architecture
pretrained language model. Composing this vector with a target language model
yields a Target Guard Model (TGM). We then adapt TGM with a streaming-aware
approach that combines prefix-based training and evaluation with a classifier
that produces a single-token output. With this composition alone, TGM improves
classification quality over established Guard Models across standard safety
suites and enables language extensibility to Chinese, Japanese, and Korean,
requiring neither additional training nor target language labels. It also
demonstrates model portability across two widely used public guardrail
backbones, Llama and Gemma. With prefix SFT (supervised fine-tuning), TGM
preserves classification quality under streaming by aligning the behavior
between prefix inputs and full-text inputs. The single-token output design
increases throughput and reduces latency. Together, these components reduce
data and compute requirements while promoting streaming-aware evaluation
practices, thereby contributing to a more responsible AI ecosystem.

</details>


### [58] [Train Once, Answer All: Many Pretraining Experiments for the Cost of One](https://arxiv.org/abs/2509.23383)
*Sebastian Bordt,Martin Pawelczyk*

Main category: cs.CL

TL;DR: The paper proposes a method to conduct multiple pretraining experiments simultaneously in a single training run to overcome computational constraints, demonstrating feasibility with 10 experiments on a 1.5B parameter model.


<details>
  <summary>Details</summary>
Motivation: To address the high computational cost of pretraining experiments for understanding LLM behaviors, enabling more efficient scientific experimentation.

Method: Conduct multiple pretraining experiments simultaneously during a single training run of a 1.5B parameter model on 210B tokens, testing for interactions with continual pretraining.

Result: Successfully replicated previous works on data contamination, poisoning, and memorization, and conducted novel investigations with minimal impact on training dynamics and performance.

Conclusion: Simultaneous pretraining experiments enable rigorous scientific experimentation with large models on limited compute budgets, with negligible interactions between experiments.

Abstract: Recent work has demonstrated that controlled pretraining experiments are a
powerful tool for understanding learning, reasoning, and memorization in large
language models (LLMs). However, the computational cost of pretraining presents
a significant constraint. To overcome this constraint, we propose to conduct
multiple pretraining experiments simultaneously during a single training run.
We demonstrate the feasibility of this approach by conducting ten experiments
during the training of a 1.5B parameter model on 210B tokens. Although we only
train a single model, we can replicate the results from multiple previous works
on data contamination, poisoning, and memorization. We also conduct novel
investigations into knowledge acquisition, mathematical reasoning, and
watermarking. For example, we dynamically update the training data until the
model acquires a particular piece of knowledge. Remarkably, the influence of
the ten experiments on the model's training dynamics and overall performance is
minimal. However, interactions between different experiments may act as a
potential confounder in our approach. We propose to test for interactions with
continual pretraining experiments, finding them to be negligible in our setup.
Overall, our findings suggest that performing multiple pretraining experiments
in a single training run can enable rigorous scientific experimentation with
large models on a compute budget.

</details>


### [59] [No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization](https://arxiv.org/abs/2509.23387)
*Wenhang Shi,Yiren Chen,Shuqing Bian,Xinyi Zhang,Kai Tang,Pengfei Hu,Zhe Zhao,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: GRACE is a prompt optimization framework that combines gated refinement and adaptive compression strategies to efficiently improve LLM prompts, achieving significant performance gains with only 25% of the computational budget of prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing automatic prompt optimization methods struggle with stable prompt generation, low efficiency, and getting trapped in local optima, making scalable prompt engineering challenging.

Method: GRACE integrates two strategies: 1) Gated refinement with feedback regulation and update rejection gates to refine update signals, and 2) Adaptive compression that distills core concepts when optimization stagnates to restructure the optimization trace.

Result: On 11 tasks across BIG-Bench Hard, domain-specific, and general NLP tasks, GRACE achieved average relative improvements of 4.7%, 4.4%, and 2.7% over state-of-the-art methods respectively, using only 25% of the prompt generation budget.

Conclusion: GRACE demonstrates that strategic information loss through refinement and compression enables substantial performance gains and high optimization efficiency for prompt engineering.

Abstract: Prompt engineering is crucial for leveraging the full potential of large
language models (LLMs). While automatic prompt optimization offers a scalable
alternative to costly manual design, generating effective prompts remains
challenging. Existing methods often struggle to stably generate improved
prompts, leading to low efficiency, and overlook that prompt optimization
easily gets trapped in local optima. Addressing this, we propose GRACE, a
framework that integrates two synergistic strategies: Gated Refinement and
Adaptive Compression, achieving Efficient prompt optimization. The gated
refinement strategy introduces a feedback regulation gate and an update
rejection gate, which refine update signals to produce stable and effective
prompt improvements. When optimization stagnates, the adaptive compression
strategy distills the prompt's core concepts, restructuring the optimization
trace and opening new paths. By strategically introducing information loss
through refinement and compression, GRACE delivers substantial gains in
performance and efficiency. In extensive experiments on 11 tasks across three
practical domains, including BIG-Bench Hard (BBH), domain-specific, and general
NLP tasks, GRACE achieves significant average relative performance improvements
of 4.7%, 4.4% and 2.7% over state-of-the-art methods, respectively. Further
analysis shows that GRACE achieves these gains using only 25% of the prompt
generation budget required by prior methods, highlighting its high optimization
efficiency and low computational overhead. Our code is available at
https://github.com/Eric8932/GRACE.

</details>


### [60] [Liaozhai through the Looking-Glass: On Paratextual Explicitation of Culture-Bound Terms in Machine Translation](https://arxiv.org/abs/2509.23395)
*Sherrie Shen,Weixuan Wang,Alexandra Birch*

Main category: cs.CL

TL;DR: This paper introduces paratextual explicitation for machine translation, using footnotes/endnotes to explain culture-bound terms. It evaluates LLMs on this task using a dataset from Chinese literature translations.


<details>
  <summary>Details</summary>
Motivation: Current MT approaches overlook paratextual apparatus used by professional translators to explain culture-bound terms, limiting faithful transfer of contextual meaning.

Method: Formalized Genette's paratext theory, created dataset of 560 expert-aligned paratexts from English translations of Liaozhai, evaluated LLMs with/without reasoning traces on explicitation choice and content.

Result: LLM-generated paratexts improve audience comprehension but are less effective than translator-authored ones. Professional translators vary widely in paratext usage, showing cultural mediation is open-ended.

Conclusion: Paratextual explicitation has potential to advance MT beyond linguistic equivalence, with extensions to monolingual explanation and personalized adaptation.

Abstract: The faithful transfer of contextually-embedded meaning continues to challenge
contemporary machine translation (MT), particularly in the rendering of
culture-bound terms--expressions or concepts rooted in specific languages or
cultures, resisting direct linguistic transfer. Existing computational
approaches to explicitating these terms have focused exclusively on in-text
solutions, overlooking paratextual apparatus in the footnotes and endnotes
employed by professional translators. In this paper, we formalize Genette's
(1987) theory of paratexts from literary and translation studies to introduce
the task of paratextual explicitation for MT. We construct a dataset of 560
expert-aligned paratexts from four English translations of the classical
Chinese short story collection Liaozhai and evaluate LLMs with and without
reasoning traces on choice and content of explicitation. Experiments across
intrinsic prompting and agentic retrieval methods establish the difficulty of
this task, with human evaluation showing that LLM-generated paratexts improve
audience comprehension, though remain considerably less effective than
translator-authored ones. Beyond model performance, statistical analysis
reveals that even professional translators vary widely in their use of
paratexts, suggesting that cultural mediation is inherently open-ended rather
than prescriptive. Our findings demonstrate the potential of paratextual
explicitation in advancing MT beyond linguistic equivalence, with promising
extensions to monolingual explanation and personalized adaptation.

</details>


### [61] [Comparison of Scoring Rationales Between Large Language Models and Human Raters](https://arxiv.org/abs/2509.23412)
*Haowei Hua,Hong Jiao,Dan Song*

Main category: cs.CL

TL;DR: This study compares human and LLM (GPT-4o, Gemini, etc.) rationales in automated essay scoring to understand scoring inconsistencies and improve understanding of reasoning processes in both human and AI raters.


<details>
  <summary>Details</summary>
Motivation: With advances in LLMs, understanding how they reason and score essays compared to humans is crucial for improving automated scoring systems and identifying sources of scoring inconsistency.

Method: Used essays from large-scale tests, evaluated LLM scoring accuracy with quadratic weighted kappa and normalized mutual information, analyzed rationale similarity with cosine similarity, and explored clustering patterns using PCA on rationale embeddings.

Result: The study provides insights into LLM scoring accuracy and their "thinking" processes, revealing patterns in how rationales differ between human and AI raters.

Conclusion: Analyzing rationales from both human and LLM raters helps improve understanding of scoring reasoning and can enhance automated scoring systems by identifying and addressing scoring inconsistencies.

Abstract: Advances in automated scoring are closely aligned with advances in
machine-learning and natural-language-processing techniques. With recent
progress in large language models (LLMs), the use of ChatGPT, Gemini, Claude,
and other generative-AI chatbots for automated scoring has been explored. Given
their strong reasoning capabilities, LLMs can also produce rationales to
support the scores they assign. Thus, evaluating the rationales provided by
both human and LLM raters can help improve the understanding of the reasoning
that each type of rater applies when assigning a score. This study investigates
the rationales of human and LLM raters to identify potential causes of scoring
inconsistency. Using essays from a large-scale test, the scoring accuracy of
GPT-4o, Gemini, and other LLMs is examined based on quadratic weighted kappa
and normalized mutual information. Cosine similarity is used to evaluate the
similarity of the rationales provided. In addition, clustering patterns in
rationales are explored using principal component analysis based on the
embeddings of the rationales. The findings of this study provide insights into
the accuracy and ``thinking'' of LLMs in automated scoring, helping to improve
the understanding of the rationales behind both human scoring and LLM-based
automated scoring.

</details>


### [62] [Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models](https://arxiv.org/abs/2509.23417)
*Rajaa El Hamdani,Samy Haffoudhi,Nils Holzenberger,Fabian Suchanek,Thomas Bonald,Fragkiskos D. Malliaros*

Main category: cs.CL

TL;DR: The paper proposes Retrieval-Constrained Decoding (RCD) to address the underestimation of language models' factual knowledge by restricting outputs to unique surface forms, showing significant performance improvements on the YAGO-QA dataset.


<details>
  <summary>Details</summary>
Motivation: Language models encode substantial factual knowledge but often produce answers judged incorrect due to alternative surface forms being dismissed in strict evaluations, leading to underestimation of their parametric knowledge.

Method: Proposed Retrieval-Constrained Decoding (RCD), a decoding strategy that restricts model outputs to unique surface forms. Evaluated on YAGO-QA dataset (19,137 general knowledge questions) with open-source LMs from 135M to 70B parameters.

Result: RCD significantly improves performance: Llama-3.1-70B scores 46.0% F1 with RCD vs 32.3% with vanilla decoding. Llama-3.1-8B reaches 33.0% with RCD, outperforming the larger model under vanilla decoding.

Conclusion: Standard decoding undervalues language models' knowledge, and RCD effectively addresses this by constraining outputs to unique surface forms, revealing more accurate assessment of models' factual knowledge.

Abstract: Language models (LMs) encode substantial factual knowledge, but often produce
answers judged as incorrect. We hypothesize that many of these answers are
actually correct, but are expressed in alternative surface forms that are
dismissed due to an overly strict evaluation, leading to an underestimation of
models' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD),
a decoding strategy that restricts model outputs to unique surface forms. We
introduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating
open-source LMs from 135M to 70B parameters, we show that standard decoding
undervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1
with vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0%
with RCD, outperforming the larger model under vanilla decoding. We publicly
share the code and dataset at https://github.com/Rajjaa/disambiguated-LLM.

</details>


### [63] [Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models](https://arxiv.org/abs/2509.23441)
*Xuanming Zhang,Yuxuan Chen,Min-Hsuan Yeh,Yixuan Li*

Main category: cs.CL

TL;DR: CooT is a decoding-time framework that adds explicit cognitive self-monitoring to LLMs, using a Perceiver to detect misalignments and intervene with rollback and regeneration guided by safety principles.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment strategies embed safety into model weights, making controls implicit, static, and hard to modify. There's a need for more explicit, dynamic, and auditable safety mechanisms.

Method: CooT couples a text Generator with a cognitive Perceiver that monitors generation using a precedence-based hierarchy of principles. When violations are detected, it rolls back generation and regenerates with injected guidance combining universal social priors and context-specific warnings.

Result: Extensive experiments across multiple benchmarks and model families confirm that CooT consistently improves safety and social reasoning performance.

Conclusion: CooT transforms alignment from a fixed property into an explicit, dynamic, and auditable process during inference, enabling flexible policy updates without model retraining.

Abstract: Large language models (LLMs) excel at complex reasoning but can still exhibit
harmful behaviors. Current alignment strategies typically embed safety into
model weights, making these controls implicit, static, and difficult to modify.
This paper introduces Cognition-of-Thought (CooT), a novel decoding-time
framework that equips LLMs with an explicit cognitive self-monitoring loop.
CooT couples a standard text Generator with a cognitive Perceiver that
continuously monitors the unfolding sequence. The Perceiver uses a structured,
precedence-based hierarchy of principles (e.g., safety over obedience) to
detect potential misalignments as they arise. When violations are flagged, CooT
intervenes by rolling back the generation to the point of error and
regenerating under injected guidance that combines universal social priors with
context-specific warnings. CooT thus transforms alignment from a fixed property
into an explicit, dynamic, and auditable process active during inference,
allowing for flexible policy updates without retraining the model. Extensive
experiments across multiple benchmarks and model families confirm that CooT
consistently improves safety and social reasoning performance.

</details>


### [64] [Text-Based Approaches to Item Difficulty Modeling in Large-Scale Assessments: A Systematic Review](https://arxiv.org/abs/2509.23486)
*Sydney Peters,Nan Zhang,Hong Jiao,Ming Li,Tianyi Zhou,Robert Lissitz*

Main category: cs.CL

TL;DR: This paper reviews 37 studies on automated item difficulty prediction using text-based machine learning approaches, showing that transformer-based language models can effectively predict item difficulty without manual feature engineering, achieving RMSE as low as 0.165 and Pearson correlation up to 0.87.


<details>
  <summary>Details</summary>
Motivation: Traditional item difficulty modeling through field testing and CTT/IRT approaches is time-consuming and costly, creating a need for more efficient automated methods using text-based machine learning approaches.

Method: Systematic review of 37 articles analyzing automated item difficulty prediction, examining datasets, difficulty parameters, subject domains, item types, input features, models, and evaluation criteria across different approaches including classic ML and transformer-based language models.

Result: Text-based methods can predict item difficulty with RMSE as low as 0.165, Pearson correlation up to 0.87, and accuracy up to 0.806. Transformer models capture syntactic and semantic patterns without manual feature engineering, while classic ML models remain relevant for interpretability.

Conclusion: Text-based automated methods show strong potential for item difficulty prediction, with performance benchmarks established for future research. The review discusses practical implications and outlines future research directions for automated item difficulty modeling.

Abstract: Item difficulty plays a crucial role in test performance, interpretability of
scores, and equity for all test-takers, especially in large-scale assessments.
Traditional approaches to item difficulty modeling rely on field testing and
classical test theory (CTT)-based item analysis or item response theory (IRT)
calibration, which can be time-consuming and costly. To overcome these
challenges, text-based approaches leveraging machine learning and language
models, have emerged as promising alternatives. This paper reviews and
synthesizes 37 articles on automated item difficulty prediction in large-scale
assessment settings published through May 2025. For each study, we delineate
the dataset, difficulty parameter, subject domain, item type, number of items,
training and test data split, input, features, model, evaluation criteria, and
model performance outcomes. Results showed that although classic machine
learning models remain relevant due to their interpretability, state-of-the-art
language models, using both small and large transformer-based architectures,
can capture syntactic and semantic patterns without the need for manual feature
engineering. Uniquely, model performance outcomes were summarized to serve as a
benchmark for future research and overall, text-based methods have the
potential to predict item difficulty with root mean square error (RMSE) as low
as 0.165, Pearson correlation as high as 0.87, and accuracy as high as 0.806.
The review concludes by discussing implications for practice and outlining
future research directions for automated item difficulty modeling.

</details>


### [65] [The Impact of Role Design in In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.23501)
*Hamidreza Rouzegar,Masoud Makrehchi*

Main category: cs.CL

TL;DR: This paper explores how role configurations in prompts affect LLM performance in zero-shot and few-shot learning, testing on GPT-3.5, GPT-4o, Llama2-7b, and Llama2-13b across various tasks.


<details>
  <summary>Details</summary>
Motivation: While prompt engineering is well-studied, the specific impact of role design within prompts remains underexplored in in-context learning scenarios.

Method: Evaluated multiple LLMs (GPT-3.5, GPT-4o, Llama2-7b, Llama2-13b) across datasets for sentiment analysis, text classification, question answering, and math reasoning tasks using role-based prompt configurations.

Result: Findings suggest that role-based prompt structuring has potential to enhance LLM performance in various learning scenarios.

Conclusion: Role configurations in prompts show promise for improving LLM capabilities in in-context learning without additional fine-tuning.

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to generate
predictions based on prompts without additional fine-tuning. While prompt
engineering has been widely studied, the impact of role design within prompts
remains underexplored. This study examines the influence of role configurations
in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from
OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models'
performance across datasets, focusing on tasks like sentiment analysis, text
classification, question answering, and math reasoning. Our findings suggest
the potential of role-based prompt structuring to enhance LLM performance.

</details>


### [66] [AraS2P: Arabic Speech-to-Phonemes System](https://arxiv.org/abs/2509.23504)
*Bassam Matar,Mohamed Fayed,Ayman Khalafallah*

Main category: cs.CL

TL;DR: AraS2P is a speech-to-phonemes system that won first place in Iqra'Eval 2025 by using Wav2Vec2-BERT with two-stage training: phoneme-aware pretraining on Arabic speech-phonemes data and fine-tuning with targeted augmentation.


<details>
  <summary>Details</summary>
Motivation: To develop an effective system for phoneme-level mispronunciation detection in Arabic speech, addressing the need for accurate pronunciation assessment in recitation tasks.

Method: Two-stage training: 1) Task-adaptive continue pretraining on large-scale Arabic speech-phonemes datasets generated using MSA Phonetiser, 2) Fine-tuning on official shared task data with augmentation from XTTS-v2-synthesized recitations featuring varied Ayat segments, speaker embeddings, and textual perturbations.

Result: The system ranked first on the official leaderboard of Iqra'Eval 2025 Shared Task.

Conclusion: Phoneme-aware pretraining combined with targeted augmentation yields strong performance in phoneme-level mispronunciation detection.

Abstract: This paper describes AraS2P, our speech-to-phonemes system submitted to the
Iqra'Eval 2025 Shared Task. We adapted Wav2Vec2-BERT via Two-Stage training
strategy. In the first stage, task-adaptive continue pretraining was performed
on large-scale Arabic speech-phonemes datasets, which were generated by
converting the Arabic text using the MSA Phonetiser. In the second stage, the
model was fine-tuned on the official shared task data, with additional
augmentation from XTTS-v2-synthesized recitations featuring varied Ayat
segments, speaker embeddings, and textual perturbations to simulate possible
human errors. The system ranked first on the official leaderboard,
demonstrating that phoneme-aware pretraining combined with targeted
augmentation yields strong performance in phoneme-level mispronunciation
detection.

</details>


### [67] [Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models](https://arxiv.org/abs/2509.24488)
*Wenjie Fu,Huandong Wang,Junyao Gao,Guoan Wan,Tao Jiang*

Main category: cs.CL

TL;DR: Self-Sanitize is a novel LLM-driven framework that uses cognitive psychology principles to enable real-time monitoring and self-repair of harmful content generation in large language models, addressing privacy leakage scenarios with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Current mitigation strategies for harmful content in LLMs rely on post-hoc filtering, which introduces latency, computational overhead, and is incompatible with token-level streaming generation. There's also insufficient focus on privacy-invasive content in existing studies.

Method: Self-Sanitize consists of two modules: a lightweight Self-Monitor that continuously inspects high-level intentions via representation engineering at token level, and a Self-Repair module that performs in-place correction of harmful content without separate review dialogues.

Result: Extensive experiments on four LLMs across three privacy leakage scenarios demonstrate superior mitigation performance with minimal overhead, without degrading LLM utility.

Conclusion: Self-Sanitize offers a practical and robust solution for safer LLM deployments by enabling real-time streaming monitoring and seamless repair with negligible impact on latency and resource utilization.

Abstract: As Large Language Models (LLMs) achieve remarkable success across a wide
range of applications, such as chatbots and code copilots, concerns surrounding
the generation of harmful content have come increasingly into focus. Despite
significant advances in aligning LLMs with safety and ethical standards,
adversarial prompts can still be crafted to elicit undesirable responses.
Existing mitigation strategies are predominantly based on post-hoc filtering,
which introduces substantial latency or computational overhead, and is
incompatible with token-level streaming generation. In this work, we introduce
Self-Sanitize, a novel LLM-driven mitigation framework inspired by cognitive
psychology, which emulates human self-monitor and self-repair behaviors during
conversations. Self-Sanitize comprises a lightweight Self-Monitor module that
continuously inspects high-level intentions within the LLM at the token level
via representation engineering, and a Self-Repair module that performs in-place
correction of harmful content without initiating separate review dialogues.
This design allows for real-time streaming monitoring and seamless repair, with
negligible impact on latency and resource utilization. Given that
privacy-invasive content has often been insufficiently focused in previous
studies, we perform extensive experiments on four LLMs across three privacy
leakage scenarios. The results demonstrate that Self-Sanitize achieves superior
mitigation performance with minimal overhead and without degrading the utility
of LLMs, offering a practical and robust solution for safer LLM deployments.
Our code is available at the following link:
https://github.com/wjfu99/LLM_Self_Sanitize

</details>


### [68] [From Human Annotation to Automation: LLM-in-the-Loop Active Learning for Arabic Sentiment Analysis](https://arxiv.org/abs/2509.23515)
*Dania Refai,Alaa Dalaq,Doaa Dalaq,Irfan Ahmad*

Main category: cs.CL

TL;DR: Proposed an active learning framework for Arabic sentiment analysis using LLM-assisted labeling to reduce annotation costs while maintaining high performance across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Arabic sentiment analysis is limited by lack of large labeled datasets. Active learning and LLM-assisted annotation are underexplored in Arabic context despite their proven effectiveness in other languages.

Method: Active learning framework with multiple deep learning architectures (LSTM, GRU, RNN) evaluated on three Arabic datasets. Compared human labeling vs LLM-assisted labeling using five LLMs (GPT-4o, Claude 3 Sonnet, Gemini 2.5 Pro, DeepSeek Chat, LLaMA 3 70B Instruct).

Result: LLM-assisted active learning achieved competitive or superior performance to human labeling. LSTM with GPT-4o achieved 93% accuracy with only 450 samples on Hunger Station dataset. DeepSeek Chat reached 82% accuracy with 650 samples on MASAC dataset, matching human labeling performance.

Conclusion: LLM-assisted active learning is effective for Arabic sentiment analysis, significantly reducing annotation costs while maintaining or improving performance compared to traditional human labeling approaches.

Abstract: Natural language processing (NLP), particularly sentiment analysis, plays a
vital role in areas like marketing, customer service, and social media
monitoring by providing insights into user opinions and emotions. However,
progress in Arabic sentiment analysis remains limited due to the lack of large,
high-quality labeled datasets. While active learning has proven effective in
reducing annotation efforts in other languages, few studies have explored it in
Arabic sentiment tasks. Likewise, the use of large language models (LLMs) for
assisting annotation and comparing their performance to human labeling is still
largely unexplored in the Arabic context. In this paper, we propose an active
learning framework for Arabic sentiment analysis designed to reduce annotation
costs while maintaining high performance. We evaluate multiple deep learning
architectures: Specifically, long short-term memory (LSTM), gated recurrent
units (GRU), and recurrent neural networks (RNN), across three benchmark
datasets: Hunger Station, AJGT, and MASAC, encompassing both modern standard
Arabic and dialectal variations. Additionally, two annotation strategies are
compared: Human labeling and LLM-assisted labeling. Five LLMs are evaluated as
annotators: GPT-4o, Claude 3 Sonnet, Gemini 2.5 Pro, DeepSeek Chat, and LLaMA 3
70B Instruct. For each dataset, the best-performing LLM was used: GPT-4o for
Hunger Station, Claude 3 Sonnet for AJGT, and DeepSeek Chat for MASAC. Our
results show that LLM-assisted active learning achieves competitive or superior
performance compared to human labeling. For example, on the Hunger Station
dataset, the LSTM model achieved 93% accuracy with only 450 labeled samples
using GPT-4o-generated labels, while on the MASAC dataset, DeepSeek Chat
reached 82% accuracy with 650 labeled samples, matching the accuracy obtained
through human labeling.

</details>


### [69] [On the Shelf Life of Fine-Tuned LLM Judges: Future Proofing, Backward Compatibility, and Question Generalization](https://arxiv.org/abs/2509.23542)
*Janvijay Singh,Austin Xu,Yilun Zhou,Yefan Zhou,Dilek Hakkani-Tur,Shafiq Joty*

Main category: cs.CL

TL;DR: The paper identifies three practical concerns for finetuned LLM judges: future proofing, backward compatibility, and question generalization. Experiments show future-proofing is challenging, backward compatibility is easier, and current judges don't fully generalize to unseen questions.


<details>
  <summary>Details</summary>
Motivation: To address practical deployment concerns of finetuned LLM judges that are overlooked in standard evaluation, focusing on their shelf life across evolving generator models and unseen questions.

Method: Study three aspects (future proofing, backward compatibility, question generalization) in math domain using unified framework with varying train/test distributions, three SFT- and DPO-based finetuning algorithms, and three base models.

Result: Future-proofing is challenging for most models; backward compatibility is relatively easy with DPO-trained models consistently improving performance; continual learning provides balanced adaptation; all models show performance degradation on unseen questions.

Conclusion: Current judges don't fully generalize to unseen questions, and practical deployment requires considering adaptation strategies for evolving generator models, with continual learning offering balanced performance across distribution shifts.

Abstract: The LLM-as-a-judge paradigm is widely used in both evaluating free-text model
responses and reward modeling for model alignment and finetuning. Recently,
finetuning judges with judge-specific data has emerged as an often preferred
choice over directly prompting frontier models as judges, as the former
achieves better performance with smaller model sizes while being more robust to
common biases. However, the standard evaluation ignores several practical
concerns of finetuned judges regarding their real world deployment. In this
paper, we identify and formalize three aspects that affect the shelf life of
these judges: future proofing and backward compatibility -- how well judges
finetuned on responses by today's generator models perform on responses by
future models or past models, as well as question generalization -- how well
judges generalize to unseen questions at test time. We study these three
aspects in the math domain under a unified framework with varying train and
test distributions, three SFT- and DPO-based finetuning algorithms and three
different base models. Experiments suggest that future-proofing is challenging
for most models, while backward compatibility is relatively easy, with
DPO-trained models consistently improving performance. We further find that
continual learning provides a more balanced adaptation to shifts between older
and newer response distributions than training solely on stronger or weaker
responses. Moreover, all models observe certain degrees of performance
degradation when moving from questions seen during training to unseen ones,
showing that current judges do not fully generalize to unseen questions. These
findings provide insights into practical considerations for developing and
deploying judge models in the face of ever-changing generators.

</details>


### [70] [Automatic Speech Recognition for Greek Medical Dictation](https://arxiv.org/abs/2509.23550)
*Vardis Georgilas,Themos Stafylakis*

Main category: cs.CL

TL;DR: This paper develops a domain-specific medical dictation system for Greek language that combines automatic speech recognition with text correction to handle medical terminology and linguistic variations.


<details>
  <summary>Details</summary>
Motivation: To assist Greek healthcare professionals by reducing manual documentation workload and improving workflow efficiency through accurate speech-to-text conversion for medical dictations.

Method: Combines automatic speech recognition techniques with text correction models, leveraging both acoustic and textual modeling. Uses domain-specific fine-tuning to adapt existing technologies to Greek medical context.

Result: The system achieves more accurate and coherent transcriptions by better handling domain-specific terminology and linguistic variations in Greek medical speech.

Conclusion: The developed system contributes to practical language technologies for Greek healthcare sector, successfully addressing challenges of complex medical terminology and linguistic inconsistencies.

Abstract: Medical dictation systems are essential tools in modern healthcare, enabling
accurate and efficient conversion of speech into written medical documentation.
The main objective of this paper is to create a domain-specific system for
Greek medical speech transcriptions. The ultimate goal is to assist healthcare
professionals by reducing the overload of manual documentation and improving
workflow efficiency. Towards this goal, we develop a system that combines
automatic speech recognition techniques with text correction model, allowing
better handling of domain-specific terminology and linguistic variations in
Greek. Our approach leverages both acoustic and textual modeling to create more
realistic and reliable transcriptions. We focused on adapting existing language
and speech technologies to the Greek medical context, addressing challenges
such as complex medical terminology and linguistic inconsistencies. Through
domain-specific fine-tuning, our system achieves more accurate and coherent
transcriptions, contributing to the development of practical language
technologies for the Greek healthcare sector.

</details>


### [71] [Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales](https://arxiv.org/abs/2509.23574)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: MoRSD improves CoT distillation by selecting high-quality rationales using a Rationale Difficulty metric, achieving 4.6% average improvement on 7 datasets with fewer but better rationales.


<details>
  <summary>Details</summary>
Motivation: Existing CoT distillation methods focus on data quantity but underestimate rationale quality, potentially transferring noisy or incorrect information to student models.

Method: Proposed Model-Oriented Rationale Selection Distillation (MoRSD) with Rationale Difficulty metric to select high-quality rationales based on accuracy, diversity, and difficulty.

Result: Achieved 4.6% average improvement on seven datasets across three tasks using fewer rationales, showing that small portions of high-quality rationales outperform entire datasets.

Conclusion: MoRSD provides an efficient solution for CoT distillation by focusing on rationale quality rather than quantity, enhancing student models' reasoning ability with carefully selected rationales.

Abstract: Chain-of-thought (CoT) distillation aims to enhance small language models'
(SLMs) reasoning by transferring multi-step reasoning capability from the
larger teacher models. However, existing work underestimates rationale quality,
focusing primarily on data quantity, which may transfer noisy or incorrect
information to the student model. To address the above issues, we proposed
\textbf{M}odel-\textbf{O}riented \textbf{R}ationale \textbf{S}election
\textbf{D}istillation (MoRSD), which can discern and select high quality
rationales for distillation to improve performance further. We further propose
a Rationale Difficulty (RD) metric to measure the ability of the student model
to generate the correct answer under a given rationale. Compared to the
baseline, we achieved 4.6$\%$ average improvement on seven datasets over three
tasks, using fewer rationales by controlling their accuracy, diversity, and
difficulty. Our results reveal that a small portion of the high quality
rationales can enhance the reasoning ability of student models than the entire
dataset. Our method promises to be a possible solution for efficient CoT
distillation. Our code will be released in https://github.com/Leon221220/MoRSD.

</details>


### [72] [Jackal: A Real-World Execution-Based Benchmark Evaluating Large Language Models on Text-to-JQL Tasks](https://arxiv.org/abs/2509.23579)
*Kevin Frank,Anmol Gulati,Elias Lumer,Sindy Campagna,Vamse Kumar Subbiah*

Main category: cs.CL

TL;DR: Jackal is a large-scale text-to-JQL benchmark with 100,000 natural language to JQL query pairs, evaluated on a live Jira instance with 200,000+ issues. It reveals current LLMs struggle with JQL generation, with best model achieving only 60.3% execution accuracy.


<details>
  <summary>Details</summary>
Motivation: There is no open, real-world, execution-based benchmark for mapping natural language queries to Jira Query Language (JQL), despite enterprise teams' heavy reliance on JQL for issue retrieval.

Method: Created Jackal benchmark with 100,000 NL-to-JQL pairs across four user request types (Long NL, Short NL, Semantically Similar, Semantically Exact), evaluated on live Jira instance with execution-based scoring.

Result: On Jackal-5K subset, best model (Gemini 2.5 Pro) achieved 60.3% execution accuracy overall, with significant variation across request types: Long NL (86.0%), Short NL (35.7%), Semantically Similar (22.7%), Semantically Exact (99.3%).

Conclusion: Jackal exposes limitations of current LLMs in JQL generation and sets a new execution-based challenge for future enterprise data research.

Abstract: Enterprise teams rely on the Jira Query Language (JQL) to retrieve and filter
issues from Jira. Yet, to our knowledge, there is no open, real-world,
execution-based benchmark for mapping natural language queries to JQL. We
introduce Jackal, a novel, large-scale text-to-JQL benchmark comprising 100,000
natural language (NL) requests paired with validated JQL queries and
execution-based results on a live Jira instance with over 200,000 issues. To
reflect real-world usage, each JQL query is associated with four types of user
requests: (i) Long NL, (ii) Short NL, (iii) Semantically Similar, and (iv)
Semantically Exact. We release Jackal, a corpus of 100,000 text-to-JQL pairs,
together with an execution-based scoring toolkit, and a static snapshot of the
evaluated Jira instance for reproducibility. We report text-to-JQL results on
23 Large Language Models (LLMs) spanning parameter sizes, open and closed
source models, across execution accuracy, exact match, and canonical exact
match. In this paper, we report results on Jackal-5K, a 5,000-pair subset of
Jackal. On Jackal-5K, the best overall model (Gemini 2.5 Pro) achieves only
60.3% execution accuracy averaged equally across four user request types.
Performance varies significantly across user request types: (i) Long NL
(86.0%), (ii) Short NL (35.7%), (iii) Semantically Similar (22.7%), and (iv)
Semantically Exact (99.3%). By benchmarking LLMs on their ability to produce
correct and executable JQL queries, Jackal exposes the limitations of current
state-of-the-art LLMs and sets a new, execution-based challenge for future
research in Jira enterprise data.

</details>


### [73] [LLM Hallucination Detection: HSAD](https://arxiv.org/abs/2509.23580)
*JinXin Li,Gang Tu,JunJie Hu*

Main category: cs.CL

TL;DR: HSAD is a novel hallucination detection method that analyzes frequency-domain features of hidden layer temporal signals in LLMs, overcoming limitations of existing approaches by capturing reasoning process anomalies.


<details>
  <summary>Details</summary>
Motivation: Current hallucination detection methods are limited by knowledge coverage constraints (factual consistency verification) or inability to capture reasoning biases (static hidden layer features). The paper aims to address these limitations by modeling LLM reasoning as a temporal cognitive process.

Method: 1) Model LLM reasoning as temporal cognitive journey using hidden layer signals; 2) Apply Fast Fourier Transform to map temporal signals to frequency domain; 3) Construct spectral features to capture reasoning anomalies; 4) Design detection algorithm based on spectral features.

Result: The method demonstrates effectiveness in capturing reasoning process anomalies through spectral feature analysis, showing higher detection accuracy and robustness compared to existing approaches.

Conclusion: HSAD effectively combines reasoning process modeling with frequency-domain feature extraction, overcoming knowledge coverage limitations and reasoning bias detection challenges in hallucination detection for LLMs.

Abstract: Although Large Language Models have demonstrated powerful capabilities in a
wide range of tasks such as language understanding and code generation, the
frequent occurrence of hallucinations during the generation process has become
a significant impediment to their deployment in critical application scenarios.
Current mainstream hallucination detection methods rely on factual consistency
verification or static hidden layer features. The former is constrained by the
scope of knowledge coverage, while the latter struggles to capture reasoning
biases during the inference process. To address these issues, and inspired by
signal analysis methods in cognitive neuroscience, this paper proposes a
hallucination detection method based on the frequency-domain analysis of hidden
layer temporal signals, named HSAD (\textbf{H}idden \textbf{S}ignal
\textbf{A}nalysis-based \textbf{D}etection). First, by treating the LLM's
reasoning process as a cognitive journey that unfolds over time, we propose
modeling and simulating the human process of signal perception and
discrimination in a deception-detection scenario through hidden layer temporal
signals. Next, The Fast Fourier Transform is applied to map these temporal
signals into the frequency domain to construct spectral features, which are
used to capture anomalies that arise during the reasoning process; analysis
experiments on these spectral features have proven the effectiveness of this
approach. Finally, a hallucination detection algorithm is designed based on
these spectral features to identify hallucinations in the generated content. By
effectively combining the modeling of the reasoning process with
frequency-domain feature extraction, the HSAD method overcomes the limitations
of existing approaches in terms of knowledge coverage and the detection of
reasoning biases, demonstrating higher detection accuracy and robustness.

</details>


### [74] [Timber: Training-free Instruct Model Refining with Base via Effective Rank](https://arxiv.org/abs/2509.23595)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Zenan Xu,Ngai Wong*

Main category: cs.CL

TL;DR: Timber is a training-free method that enhances exploration capability of Instruct models by partially reverting them towards Base models through targeted weight delta refinement, improving performance particularly on Pass@k metrics.


<details>
  <summary>Details</summary>
Motivation: Post-training is considered superficial with negligible weight changes, but suffers from a trade-off between exploitation and exploration capabilities - improving exploitation at the cost of limiting exploration.

Method: Timber partially reverts Instruct models towards their paired Base models by subtle yet targeted refinement of the weight deltas, without requiring additional training.

Result: Extensive experiments on Llama and Qwen series show Timber consistently improves vanilla Instruct models, particularly enhancing Pass@k performance.

Conclusion: The findings provide new insights into post-training at the weight level and practical strategies to refine Instruct models without training, addressing the exploration-exploitation trade-off.

Abstract: Post-training, which elicits a pretrained Base model into the corresponding
Instruct model, is widely considered to be superficial. In this work, we first
reinforce this hypothesis by providing novel quantitative evidence from the
weight level that the effective rank (eRank) remains negligibly changed.
However, this superficiality also suffers a critical trade-off, improving the
exploitation capabilities at the cost of limiting its exploration. To tackle
this issue, we propose Timber, a simple yet effective training-free method that
enhances the exploration capability of the Instruct model while preserving its
exploitation. The key insight is to partially revert Instruct towards the
paired Base model by subtle yet targeted refinement of the weight deltas.
Extensive experiments on Llama and Qwen series demonstrate that Timber
consistently improves vanilla Instruct models, particularly on Pass@k
performance. Our findings offer new insights into the post-training stage at
the weight level and practical strategies to refine the Instruct model without
training.

</details>


### [75] [Fast Thinking for Large Language Models](https://arxiv.org/abs/2509.23633)
*Haoyu Zheng,Zhuonan Wang,Yuqian Yuan,Tianwei Lin,Wenqiao Zhang,Zheqi Lv,Juncheng Li,Siliang Tang,Yueting Zhuang,Hongyang He*

Main category: cs.CL

TL;DR: Latent Codebooks for Fast Thinking framework uses concise CoT sketches in training to learn discrete strategy priors, enabling fast inference with continuous thinking vectors and adaptive routing between fast and slow reasoning modes.


<details>
  <summary>Details</summary>
Motivation: Current reasoning-oriented LLMs rely on explicit step-by-step token generation, which is inefficient due to long reasoning traces that increase latency and token usage, despite CoT techniques improving performance on complex tasks.

Method: Proposes a framework that learns a codebook of discrete strategy priors using concise CoT sketches during training. At inference, uses continuous thinking vectors from the codebook for strategy-level guidance without explicit tokens. Includes GainRouter mechanism for adaptive switching between fast codebook inference and slow explicit reasoning.

Result: Experiments across multiple reasoning benchmarks show competitive or superior accuracy while substantially lowering inference cost, effectively reducing unnecessary token generation and suppressing overthinking.

Conclusion: The approach offers a practical path toward efficient and controllable reasoning in large language models by enabling fast strategy-level guidance without explicit reasoning tokens and adaptive routing between reasoning modes.

Abstract: Reasoning-oriented Large Language Models (LLMs) often rely on generating
explicit tokens step by step, and their effectiveness typically hinges on
large-scale supervised fine-tuning or reinforcement learning. While
Chain-of-Thought (CoT) techniques substantially enhance performance on complex
reasoning tasks, they remain inefficient, requiring long reasoning traces that
increase latency and token usage. In this work, we introduce Latent Codebooks
for Fast Thinking, a framework that uses concise CoT sketches only during
training to learn a codebook of discrete strategy priors. At inference, the
model conditions on a handful of continuous thinking vectors distilled from the
codebook in a single pass, enabling strategy-level guidance without producing
explicit reasoning tokens. To complement this design, we propose GainRouter, a
lightweight routing mechanism that adaptively switches between fast codebook
guided inference and slow explicit reasoning, thereby suppressing overthinking
and reducing unnecessary token generation. Experiments across multiple
reasoning benchmarks show that our approach achieves competitive or superior
accuracy while substantially lowering inference cost, offering a practical path
toward efficient and controllable reasoning in large language models.

</details>


### [76] [Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models](https://arxiv.org/abs/2509.23653)
*Zemin Huang,Yuhang Wang,Zhiyang Chen,Guo-Jun Qi*

Main category: cs.CL

TL;DR: RemeDi introduces remasking mechanism in diffusion language models to identify and revise incorrect tokens by predicting confidence scores, enabling more flexible text refinement.


<details>
  <summary>Details</summary>
Motivation: Mask-based Diffusion Language Models struggle to revise incorrect tokens once generated, as tokens typically remain fixed after generation.

Method: RemeDi jointly predicts token distributions and per-token confidence scores, uses confidence scores to determine which tokens to remask, and employs remask-aware training pipeline with supervised fine-tuning and reinforcement learning.

Result: RemeDi achieves state-of-the-art results among open-source Diffusion Language Models on multiple datasets.

Conclusion: Remasking enables more flexible text refinement in diffusion-based text generation, allowing identification and correction of low-quality tokens.

Abstract: Mask-based Diffusion Language Models (DLMs) struggle to revise incorrect
tokens: once a token is generated, it typically remains fixed. The key
challenge is to identify potential errors in the inputs. In this paper, we
propose \emph{\underline{Rem}asking-\underline{e}nabled \underline{Di}ffusion
Language Model (RemeDi}, a mask-based DLM that introduces \emph{remasking} as
another fundamental mechanism, enabling more flexible text refinement in
diffusion-based text generation. To achieve this, RemeDi jointly predicts token
distributions and per-token confidence scores at each step. The confidence
scores determine which tokens to be unmasked after the current step, allowing
the model to identify tokens with low quality and remask them. These remasked
tokens can be resampled with richer context in subsequent steps. We design a
remask-aware pipeline to train this ability, including supervised fine-tuning
which teaches the model to detect and remask incorrect tokens in addition to
predict mask tokens, and reinforcement learning which optimizes full generation
trajectories toward higher rewards. Experiments show that RemeDi achieves the
state-of-the-art results among open-source DLMs on multiple datasets.

</details>


### [77] [Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs](https://arxiv.org/abs/2509.23657)
*Shulin Huang,Yiran Ding,Junshu Pan,Yue Zhang*

Main category: cs.CL

TL;DR: RL training shows superior cross-lingual reasoning generalization compared to SFT, with better performance when trained on non-English data.


<details>
  <summary>Details</summary>
Motivation: To investigate how reinforcement learning (RL) and supervised fine-tuning (SFT) affect cross-lingual generalization in complex reasoning tasks, which remains unexplored despite RL's known benefits for reasoning.

Method: Systematic investigation using Qwen2.5-3B-Base model on diverse multilingual reasoning benchmarks (math, commonsense, scientific reasoning), comparing RL and SFT approaches across different language training data.

Result: RL achieves higher accuracy and substantially stronger cross-lingual generalization than SFT. RL training on non-English data yields better overall performance and generalization than English data training, unlike SFT.

Conclusion: RL enables more robust reasoning strategies and provides crucial guidance for equitable multilingual reasoning systems, demonstrating clear advantages over SFT for cross-lingual generalization.

Abstract: Enhancing the complex reasoning capabilities of Large Language Models (LLMs)
attracts widespread attention. While reinforcement learning (RL) has shown
superior performance for improving complex reasoning, its impact on
cross-lingual generalization compared to Supervised Fine-Tuning (SFT) remains
unexplored. We present the first systematic investigation into cross-lingual
reasoning generalization of RL and SFT. Using Qwen2.5-3B-Base as our foundation
model, we conduct experiments on diverse multilingual reasoning benchmarks,
including math reasoning, commonsense reasoning, and scientific reasoning. Our
investigation yields two significant findings: (1) Tuning with RL not only
achieves higher accuracy but also demonstrates substantially stronger
cross-lingual generalization capabilities compared to SFT. (2) RL training on
non-English data yields better overall performance and generalization than
training on English data, which is not observed with SFT. Furthermore, through
comprehensive mechanistic analyses, we explore the underlying factors of RL's
superiority and generalization across languages. Our results provide compelling
evidence that RL enables the model with more robust reasoning strategies,
offering crucial guidance for more equitable and effective multilingual
reasoning.

</details>


### [78] [Aligning LLMs for Multilingual Consistency in Enterprise Applications](https://arxiv.org/abs/2509.23659)
*Amit Agarwal,Hansa Meghwani,Hitesh Laxmichand Patel,Tao Sheng,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: A batch-wise alignment strategy for fine-tuning LLMs using multilingual data to reduce performance gaps between English and non-English languages, improving non-English accuracy by up to 23.9% without compromising English performance.


<details>
  <summary>Details</summary>
Motivation: LLMs are unreliable for global enterprise applications due to substantial performance gaps between high-resource and mid/low-resource languages, driven by English-centric pretraining and internal reasoning biases, which undermines customer experience and operational reliability in multilingual settings.

Method: A practical, batch-wise alignment strategy for fine-tuning LLMs that leverages semantically equivalent multilingual data in each training batch to directly align model outputs across languages.

Result: The approach improves non-English accuracy by up to 23.9% without compromising English performance, model reasoning, or retrieval quality.

Conclusion: The method is simple to implement, scalable, and integrates seamlessly with existing LLM training & deployment pipelines, enabling more robust and equitable multilingual AI solutions in industry.

Abstract: Large language models (LLMs) remain unreliable for global enterprise
applications due to substantial performance gaps between high-resource and
mid/low-resource languages, driven by English-centric pretraining and internal
reasoning biases. This inconsistency undermines customer experience and
operational reliability in multilingual settings such as customer support,
content moderation, and information retrieval. Even with advanced
Retrieval-Augmented Generation (RAG) systems, we observe up to an 29% accuracy
drop in non-English languages compared to English.
  We propose a practical, batch-wise alignment strategy for fine-tuning LLMs,
leveraging semantically equivalent multilingual data in each training batch to
directly align model outputs across languages. This approach improves
non-English accuracy by up to 23.9\% without compromising English performance,
model reasoning, or retrieval quality. Our method is simple to implement,
scalable, and integrates seamlessly with existing LLM training \& deployment
pipelines, enabling more robust and equitable multilingual AI solutions in
industry.

</details>


### [79] [TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in System F](https://arxiv.org/abs/2509.23686)
*Yifeng He,Luning Yang,Christopher Castro Gaw Gonzalo,Hao Chen*

Main category: cs.CL

TL;DR: TF-Bench is a new benchmark for evaluating LLM reasoning on program semantics using System F type inference, revealing major limitations in current models with Claude-3.7-sonnet achieving only 55.85% accuracy on the pure semantics variant.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack formal deductive frameworks for sound evaluation and cannot distinguish genuine program semantics reasoning from superficial token associations.

Method: Uses type inference in System F as reasoning task, creates TF-Bench_pure variant by removing semantically irrelevant natural language through verified transformations.

Result: Best-performing LLM (Claude-3.7-sonnet) achieves only 55.85% accuracy on TF-Bench_pure, showing substantial limitations in program semantics reasoning.

Conclusion: Proposes novel metrics for robustness and test-time reasoning effectiveness, highlighting critical LLM limitations and essential future research directions.

Abstract: Large Language Models (LLMs) are increasingly integrated into the software
engineering ecosystem. Their test-time compute (TTC) reasoning capabilities
show significant potential for understanding program logic and semantics beyond
mere token recognition. However, current benchmarks for code reasoning lack a
formal, program-centric deductive framework to ensure sound evaluation, and are
incapable of assessing whether models genuinely reason about program semantics
or merely exploit superficial associations between natural language and code
tokens. To bridge this gap, we introduce TF-Bench, a benchmark designed to
evaluate LLM reasoning based on type inference in System F, a task we refer to
as program semantics reasoning. By employing verified transformations to remove
semantically irrelevant natural language, we construct TF-Bench_pure, a purely
semantics-driven variant of TF-Bench. Our analysis reveals substantial
limitations in state-of-the-art LLMs, with the best-performing LLM
(Claude-3.7-sonnet) achieving only 55.85% accuracy on TF-Bench_pure.
Additionally, we propose two novel metrics to assess robustness and the
effectiveness of test-time reasoning, underscoring critical limitations in
current LLM capabilities and highlighting essential directions for future
research.

</details>


### [80] [VIVA+: Human-Centered Situational Decision-Making](https://arxiv.org/abs/2509.23698)
*Zhe Hu,Yixiao Ren,Guanzhong Liu,Jing Li,Yu Yin*

Main category: cs.CL

TL;DR: VIVA+ is a cognitively grounded benchmark with 1,317 real-world situations and 6,373 multiple-choice questions to evaluate MLLMs' reasoning and decision-making across three core abilities: situation comprehension, action justification, and reflective reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods struggle to assess MLLMs' capacity for nuanced, human-like reasoning and decision-making in complex, human-centered environments, necessitating a more systematic framework.

Method: Developed VIVA+ benchmark targeting three core decision-making abilities: (1) Foundational Situation Comprehension, (2) Context-Driven Action Justification, and (3) Reflective Reasoning. Evaluated latest commercial and open-source MLLMs, and explored targeted training and multi-step reasoning strategies.

Result: Revealed distinct performance patterns and significant challenges among evaluated models. Targeted training and multi-step reasoning strategies yielded consistent performance improvements across models.

Conclusion: Current MLLMs have limitations in robust, context-aware, and socially adept decision-making. The analysis provides actionable insights for advancing MLLMs toward better real-world decision-making capabilities.

Abstract: Multimodal Large Language Models (MLLMs) show promising results for embodied
agents in operating meaningfully in complex, human-centered environments. Yet,
evaluating their capacity for nuanced, human-like reasoning and decision-making
remains challenging. In this work, we introduce VIVA+, a cognitively grounded
benchmark for evaluating the reasoning and decision-making of MLLMs in
human-centered situations. VIVA+ consists of 1,317 real-world situations paired
with 6,373 multiple-choice questions, targeting three core abilities for
decision-making: (1) Foundational Situation Comprehension, (2) Context-Driven
Action Justification, and (3) Reflective Reasoning. Together, these dimensions
provide a systematic framework for assessing a model's ability to perceive,
reason, and act in socially meaningful ways. We evaluate the latest commercial
and open-source models on VIVA+, where we reveal distinct performance patterns
and highlight significant challenges. We further explore targeted training and
multi-step reasoning strategies, which yield consistent performance
improvements. Finally, our in-depth analysis highlights current model
limitations and provides actionable insights for advancing MLLMs toward more
robust, context-aware, and socially adept decision-making in real-world
settings.

</details>


### [81] [Collaboration of Fusion and Independence: Hypercomplex-driven Robust Multi-Modal Knowledge Graph Completion](https://arxiv.org/abs/2509.23714)
*Zhiqiang Liu,Yichi Zhang,Mengshu Sun,Lei Liang,Wen Zhang*

Main category: cs.CL

TL;DR: M-Hyper is a novel multi-modal knowledge graph completion method that uses quaternion algebra to enable coexistence of fused and independent modality representations, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing MMKGC methods have limitations: fusion-based methods lose modality-specific information due to fixed fusion strategies, while ensemble-based methods fail to capture nuanced cross-modal interactions. There's a need for a method that integrates both paradigms.

Method: Proposes M-Hyper using quaternion algebra with four orthogonal bases to represent multiple independent modalities. Uses Hamilton product for efficient modality interactions. Includes FERF module for fine-grained entity representation factorization and R2MF module for robust relation-aware modality fusion.

Result: Extensive experiments show state-of-the-art performance, robustness, and computational efficiency compared to existing methods.

Conclusion: M-Hyper successfully integrates fusion-based and ensemble-based paradigms, enabling effective cross-modal interactions while preserving modality-specific information through quaternion algebra representation.

Abstract: Multi-modal knowledge graph completion (MMKGC) aims to discover missing facts
in multi-modal knowledge graphs (MMKGs) by leveraging both structural
relationships and diverse modality information of entities. Existing MMKGC
methods follow two multi-modal paradigms: fusion-based and ensemble-based.
Fusion-based methods employ fixed fusion strategies, which inevitably leads to
the loss of modality-specific information and a lack of flexibility to adapt to
varying modality relevance across contexts. In contrast, ensemble-based methods
retain modality independence through dedicated sub-models but struggle to
capture the nuanced, context-dependent semantic interplay between modalities.
To overcome these dual limitations, we propose a novel MMKGC method M-Hyper,
which achieves the coexistence and collaboration of fused and independent
modality representations. Our method integrates the strengths of both
paradigms, enabling effective cross-modal interactions while maintaining
modality-specific information. Inspired by ``quaternion'' algebra, we utilize
its four orthogonal bases to represent multiple independent modalities and
employ the Hamilton product to efficiently model pair-wise interactions among
them. Specifically, we introduce a Fine-grained Entity Representation
Factorization (FERF) module and a Robust Relation-aware Modality Fusion (R2MF)
module to obtain robust representations for three independent modalities and
one fused modality. The resulting four modality representations are then mapped
to the four orthogonal bases of a biquaternion (a hypercomplex extension of
quaternion) for comprehensive modality interaction. Extensive experiments
indicate its state-of-the-art performance, robustness, and computational
efficiency.

</details>


### [82] [Do LLMs Understand Romanian Driving Laws? A Study on Multimodal and Fine-Tuned Question Answering](https://arxiv.org/abs/2509.23715)
*Eduard Barbu,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: This paper evaluates LLMs on Romanian driving-law QA with explanation generation, using a 1,208-question dataset (387 multimodal). It compares text-only and multimodal SOTA systems, measures domain-specific fine-tuning impact, and uses LLM-as-a-Judge for explanation quality assessment.


<details>
  <summary>Details</summary>
Motivation: To ensure both new and experienced drivers master current traffic rules for road safety, and to inform explainable QA for less-resourced languages like Romanian.

Method: Created a 1,208-question dataset (387 multimodal), compared text-only and multimodal SOTA systems, fine-tuned Llama 3.1-8B-Instruct and RoLlama 3.1-8B-Instruct, and used LLM-as-a-Judge to assess explanation quality.

Result: SOTA models perform well but fine-tuned 8B models are competitive; textual descriptions of images outperform direct visual input; LLM-as-a-Judge reveals self-preference bias in explanation assessment.

Conclusion: The study provides insights for explainable QA systems in less-resourced languages, showing the effectiveness of fine-tuned smaller models and the importance of addressing biases in automated evaluation methods.

Abstract: Ensuring that both new and experienced drivers master current traffic rules
is critical to road safety. This paper evaluates Large Language Models (LLMs)
on Romanian driving-law QA with explanation generation. We release a
1{,}208-question dataset (387 multimodal) and compare text-only and multimodal
SOTA systems, then measure the impact of domain-specific fine-tuning for Llama
3.1-8B-Instruct and RoLlama 3.1-8B-Instruct. SOTA models perform well, but
fine-tuned 8B models are competitive. Textual descriptions of images outperform
direct visual input. Finally, an LLM-as-a-Judge assesses explanation quality,
revealing self-preference bias. The study informs explainable QA for
less-resourced languages.

</details>


### [83] [Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning](https://arxiv.org/abs/2509.23744)
*Yucheng Wang,Yifan Hou,Aydin Javadov,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: MLLMs struggle with cross-modal reasoning due to integration failures, not perception issues. Additional modalities help only when providing independent reasoning paths, while redundant or chained support often harms performance through task-composition and fusion bottlenecks.


<details>
  <summary>Details</summary>
Motivation: To address conflicting reports on whether added modalities help or harm multimodal reasoning performance, and to understand when and why modality interactions support or undermine reasoning through controlled evaluation.

Method: Developed a logic-grounded evaluation framework categorizing multimodal reasoning into six interaction patterns, varying fact distribution across modalities and logical combinations. Used attention pattern analysis and two-step prompting experiments.

Result: Additional modalities enhance reasoning only when providing independent and sufficient reasoning paths. Reasoning degrades in three ways: weaker modalities drag down performance, conflicts bias modality preference, and joint signals fail integration. Two core failures identified: task-composition bottleneck and fusion bottleneck.

Conclusion: Integration, not perception, is the main barrier to multimodal reasoning. Composition-aware training and early fusion control are promising directions to address task-composition and fusion bottlenecks.

Abstract: Multimodal large language models (MLLMs) promise enhanced reasoning by
integrating diverse inputs such as text, vision, and audio. Yet cross-modal
reasoning remains underexplored, with conflicting reports on whether added
modalities help or harm performance. These inconsistencies stem from a lack of
controlled evaluation frameworks and analysis of models' internals to isolate
when and why modality interactions support or undermine reasoning. We address
this gap through a logic-grounded evaluation framework that categorizes
multimodal reasoning into six interaction patterns, varying how facts are
distributed across modalities and logically combined. Empirically, additional
modalities enhance reasoning only when they provide independent and sufficient
reasoning paths, while redundant or chained entailment support often hurts
performance. Moreover, reasoning degrades in three systematic ways: weaker
modalities drag down overall performance, conflicts bias preference toward
certain modalities, and joint signals from different modalities fail to be
integrated effectively. Therefore, we identify two core failures:
task-composition bottleneck, where recognition and reasoning cannot be jointly
executed in one pass, and fusion bottleneck, where early integration introduces
bias. For further investigation, we find that attention patterns fail to encode
fact usefulness, but a simple two-step prompting (recognize then reason)
restores performance, confirming the task-composition bottleneck. Moreover,
modality identity remains recoverable in early layers, and softening attention
in early fusion improves reasoning, highlighting biased fusion as another
failure mode. Overall, our findings show that integration, not perception, is
the main barrier to multimodal reasoning, suggesting composition-aware training
and early fusion control as promising directions.

</details>


### [84] [Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance Analysis](https://arxiv.org/abs/2509.23755)
*Chao Wang,Rui-Chen Zheng,Yang Ai,Zhen-Hua Ling*

Main category: cs.CL

TL;DR: Speech integration in LLMs weakens text competence. Analysis shows speech fine-tuning disrupts parameter importance distribution for text reasoning. Layer-wise learning rate and LoRA preserve text ability while improving speech QA.


<details>
  <summary>Details</summary>
Motivation: Speech-enabled LLMs degrade in textual competence, limiting their ability to leverage pre-trained text knowledge. Need to understand and mitigate this degradation.

Method: Propose analytical framework using parameter importance estimation. Investigate two mitigation strategies: layer-wise learning rate scheduling and Low-Rank Adaptation (LoRA).

Result: Both approaches better maintain textual competence than full fine-tuning while improving spoken question answering performance.

Conclusion: The study provides principled explanation for mitigation strategies' effectiveness, linking benefits to structural properties of textual knowledge in LLMs.

Abstract: The integration of speech into Large Language Models (LLMs) has substantially
expanded their capabilities, but often at the cost of weakening their core
textual competence. This degradation limits the ability of speech-enabled LLMs
to fully exploit their pre-trained text-based knowledge. In this work, we
analyze the underlying mechanisms of this issue through a focused study of the
widely used encoder-adaptor paradigm. We propose an analytical framework based
on parameter importance estimation, which reveals that fine-tuning for speech
introduces a textual importance distribution shift: the layer-wise allocation
of parameters critical to textual reasoning is disrupted. Building on this
insight, we investigate two mitigation strategies: layer-wise learning rate
scheduling and Low-Rank Adaptation (LoRA), both aim to preserve the original
parameter distribution. Experimental results show that both approaches better
maintain textual competence than full fine-tuning, while also improving
downstream spoken question answering performance. Furthermore, our analysis
offers a principled explanation for the effectiveness of the proposed
mitigation strategies, linking their benefits to the structural properties of
textual knowledge in LLMs.

</details>


### [85] [Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality](https://arxiv.org/abs/2509.23765)
*Junliang Li,Yucheng Wang,Yan Chen,Yu Ran,Ruiqing Zhang,Jing Liu,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: KLCF is a reinforcement learning framework that improves LLM factuality by aligning policy model's expressed knowledge with base model's parametric knowledge through dual-fact alignment mechanism.


<details>
  <summary>Details</summary>
Motivation: To address hallucination and factuality deficits in LLM long-form generation, particularly the "hallucination tax" caused by RLHF frameworks overlooking internal knowledge boundaries.

Method: Uses knowledge consistency between policy and base models, constructs fact checklists from pretrained knowledge boundaries, implements dual-fact alignment for factual recall and precision, and trains self-assessment module based on base model's internal knowledge.

Result: Substantially improves factuality metrics across multiple long-form benchmarks and effectively alleviates model hallucinations.

Conclusion: KLCF provides an efficient, external-knowledge-free, and scalable solution to improve LLM factuality without relying on external retrieval or heavy verification.

Abstract: Hallucination and factuality deficits remain key obstacles to the reliability
of large language models (LLMs) in long-form generation. Existing reinforcement
learning from human feedback (RLHF) frameworks primarily rely on preference
rewards, yet they often overlook the model's internal knowledge boundaries,
exacerbating the so-called "hallucination tax". To address this challenge, we
propose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a
novel framework that focuses on the knowledge consistency between the policy
model's expressed knowledge and the base model's parametric knowledge, and
introduces a Dual-Fact Alignment mechanism to jointly optimize factual recall
and precision. Specifically, KLCF leverages pretrained knowledge boundaries to
construct fact checklist, guiding online reinforcement learning to improve
factual coverage and recall; simultaneously, it trains a self-assessment module
based on the base model's internal knowledge to enhance factual precision
during generation. Unlike prior methods that rely on external retrieval or
heavy verification, our reward design is fully external-knowledge-free and
lightweight, making KLCF efficient and easily scalable to large-scale training.
Experimental results demonstrate that KLCF substantially improves factuality
metrics across multiple long-form benchmarks and effectively alleviates model
hallucinations.

</details>


### [86] [From Personal to Collective: On the Role of Local and Global Memory in LLM Personalization](https://arxiv.org/abs/2509.23767)
*Zehong Wang,Junlin Wu,ZHaoxuan Tan,Bolian Li,Xianrui Zhong,Zheli Liu,Qingkai Zeng*

Main category: cs.CL

TL;DR: LoGo framework addresses LLM personalization challenges by combining local user memory with global collective memory and using a mediator to resolve conflicts, improving cold-start and biasing problems.


<details>
  <summary>Details</summary>
Motivation: To solve the cold-start problem (insufficient user history) and biasing problem (overfitting to skewed user preferences) in LLM personalization by leveraging collective knowledge across users.

Method: Proposed LoGo framework with local-global memory: local memory for individual user preferences, global memory for shared interests across population, and a mediator module to reconcile conflicts between local and global signals.

Result: Extensive experiments on multiple benchmarks show LoGo consistently improves personalization quality by warming up cold-start users and mitigating biased predictions.

Conclusion: Incorporating collective knowledge through the LoGo framework effectively enhances LLM personalization by addressing both cold-start and biasing challenges.

Abstract: Large language model (LLM) personalization aims to tailor model behavior to
individual users based on their historical interactions. However, its
effectiveness is often hindered by two key challenges: the \textit{cold-start
problem}, where users with limited history provide insufficient context for
accurate personalization, and the \textit{biasing problem}, where users with
abundant but skewed history cause the model to overfit to narrow preferences.
We identify both issues as symptoms of a common underlying limitation, i.e.,
the inability to model collective knowledge across users. To address this, we
propose a local-global memory framework (LoGo) that combines the personalized
local memory with a collective global memory that captures shared interests
across the population. To reconcile discrepancies between these two memory
sources, we introduce a mediator module designed to resolve conflicts between
local and global signals. Extensive experiments on multiple benchmarks
demonstrate that LoGo consistently improves personalization quality by both
warming up cold-start users and mitigating biased predictions. These results
highlight the importance of incorporating collective knowledge to enhance LLM
personalization.

</details>


### [87] [Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions](https://arxiv.org/abs/2509.23782)
*Yoonah Park,Haesung Pyun,Yohan Jo*

Main category: cs.CL

TL;DR: KAPPA is a parameter-free method that improves LLM performance on multiple-choice questions by aligning knowledge and prediction subspaces in hidden states.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail on multiple-choice questions despite having correct knowledge, due to misalignment between knowledge and prediction subspaces in hidden states.

Method: Identify knowledge and prediction bases in residual streams, then use projection-based adjustment (KAPPA) to align hidden states along these bases.

Result: KAPPA substantially improves accuracy on Big-Bench-Hard and ARC-Challenge, outperforms baselines, and generalizes to free-form questions.

Conclusion: The work provides geometric understanding of knowledge-prediction gap and practical method for better aligning model behavior with latent knowledge.

Abstract: Large Language Models (LLMs) often fail on multiple-choice questions (MCQs)
despite demonstrating correct knowledge in other contexts, such as free-form
generation. To investigate the mechanism underlying this knowledge-prediction
gap on MCQs and alleviate it, we conduct a probing analysis and find that
residual streams in certain layers contain a subspace spanned by two important
bases: a \emph{knowledge basis} that encodes the probability of the
ground-truth answer for a given MCQ and a \emph{prediction basis} that encodes
the probability of the answer choice predicted by the model. We observe that
incorrect predictions arise from a misalignment of the model's hidden states
along these two bases. Hence, we introduce \textbf{KAPPA} (Knowledge-Aligned
Prediction through Projection-based Adjustment), a parameter-free intervention
that transforms the hidden states to align the prediction coordinate with the
knowledge coordinate within this subspace. Experiments on binary-choice
reformulations of Big-Bench-Hard and ARC-Challenge show that KAPPA
substantially improves accuracy and consistently outperforms baselines. While
optimal subspaces differ across tasks, subspaces generalize to some extent, as
supported by cross-dataset experiments. Moreover, KAPPA extends its
effectiveness to free-form questions beyond MCQs. Our work provides a new
geometric understanding of the knowledge-prediction gap and offers a practical
method for better aligning model behavior with its latent knowledge.

</details>


### [88] [Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented Generation for Islamic Knowledge Question Answering](https://arxiv.org/abs/2509.23793)
*Muhammad Abu Ahmad,Mohamad Ballout,Raia Abu Ahmad,Elia Bruni*

Main category: cs.CL

TL;DR: Hybrid RAG system combining sparse/dense retrieval with cross-encoder reranking for Islamic knowledge tasks, improving LLM accuracy up to 25%.


<details>
  <summary>Details</summary>
Motivation: To enhance large language model performance on Islamic knowledge understanding and reasoning tasks through improved information retrieval.

Method: Three-stage pipeline: BM25 for initial retrieval, dense embedding model for semantic matching, cross-encoder reranking for precise content selection.

Result: Accuracy improvements up to 25% across tasks; best configuration with Fanar achieved 45% in Subtask 1 and 80% in Subtask 2.

Conclusion: The hybrid RAG pipeline effectively enhances LLM performance on Islamic knowledge tasks, with significant accuracy gains through multi-stage retrieval optimization.

Abstract: This paper presents our submission to the QIAS 2025 shared task on Islamic
knowledge understanding and reasoning. We developed a hybrid
retrieval-augmented generation (RAG) system that combines sparse and dense
retrieval methods with cross-encoder reranking to improve large language model
(LLM) performance. Our three-stage pipeline incorporates BM25 for initial
retrieval, a dense embedding retrieval model for semantic matching, and
cross-encoder reranking for precise content retrieval. We evaluate our approach
on both subtasks using two LLMs, Fanar and Mistral, demonstrating that the
proposed RAG pipeline enhances performance across both, with accuracy
improvements up to 25%, depending on the task and model configuration. Our best
configuration is achieved with Fanar, yielding accuracy scores of 45% in
Subtask 1 and 80% in Subtask 2.

</details>


### [89] [Open-DeBias: Toward Mitigating Open-Set Bias in Language Models](https://arxiv.org/abs/2509.23805)
*Arti Rani,Shweta Singh,Nihar Ranjan Sahoo,Gaurav Kumar Nayak*

Main category: cs.CL

TL;DR: Open-DeBias is a novel debiasing method using adapter modules to mitigate social and stereotypical biases in LLMs, achieving significant improvements in QA accuracy and demonstrating robust multilingual generalization.


<details>
  <summary>Details</summary>
Motivation: LLMs often encode harmful biases that compromise fairness, but existing bias mitigation approaches are limited to predefined categories and cannot address novel or context-specific emergent biases.

Method: Proposed Open-DeBias, a data-efficient and parameter-efficient debiasing method that leverages adapter modules to mitigate existing social and stereotypical biases while generalizing to unseen ones.

Result: Open-DeBias improves QA accuracy on BBQ dataset by nearly 48% on ambiguous subsets and 6% on disambiguated ones. In zero-shot transfer to Korean BBQ, achieves 84% accuracy, demonstrating robust language-agnostic generalization.

Conclusion: Open-DeBias shows effectiveness across broad NLP tasks, highlighting its robustness, multilingual strength, and suitability for general-purpose, open-domain bias mitigation.

Abstract: Large Language Models (LLMs) have achieved remarkable success on question
answering (QA) tasks, yet they often encode harmful biases that compromise
fairness and trustworthiness. Most existing bias mitigation approaches are
restricted to predefined categories, limiting their ability to address novel or
context-specific emergent biases. To bridge this gap, we tackle the novel
problem of open-set bias detection and mitigation in text-based QA. We
introduce OpenBiasBench, a comprehensive benchmark designed to evaluate biases
across a wide range of categories and subgroups, encompassing both known and
previously unseen biases. Additionally, we propose Open-DeBias, a novel,
data-efficient, and parameter-efficient debiasing method that leverages adapter
modules to mitigate existing social and stereotypical biases while generalizing
to unseen ones. Compared to the state-of-the-art BMBI method, Open-DeBias
improves QA accuracy on BBQ dataset by nearly $48\%$ on ambiguous subsets and
$6\%$ on disambiguated ones, using adapters fine-tuned on just a small fraction
of the training data. Remarkably, the same adapters, in a zero-shot transfer to
Korean BBQ, achieve $84\%$ accuracy, demonstrating robust language-agnostic
generalization. Through extensive evaluation, we also validate the
effectiveness of Open-DeBias across a broad range of NLP tasks, including
StereoSet and CrowS-Pairs, highlighting its robustness, multilingual strength,
and suitability for general-purpose, open-domain bias mitigation. The project
page is available at: https://sites.google.com/view/open-debias25

</details>


### [90] [SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models](https://arxiv.org/abs/2509.23863)
*Ziyi Yang,Weizhou Shen,Ruijun Chen,Chenliang Li,Fanqi Wan,Ming Yan,Xiaojun Quan,Fei Huang*

Main category: cs.CL

TL;DR: SPELL is a multi-role self-play RL framework that enables scalable, label-free optimization for long-context reasoning in LLMs through questioner-responder-verifier roles and automated curriculum learning.


<details>
  <summary>Details</summary>
Motivation: Progress in long-context reasoning for LLMs lags due to intrinsic difficulty of processing long texts and scarcity of reliable human annotations and verifiable reward signals.

Method: Multi-role self-play RL with three cyclical roles (questioner, responder, verifier) within single model; automated curriculum gradually increasing document length; adaptive reward function matching question difficulty to model capabilities.

Result: Consistent performance improvements across 6 long-context benchmarks; outperforms equally sized models fine-tuned on large-scale annotated data; achieves average 7.6-point gain in pass@8 on Qwen3-30B-A3B-Thinking.

Conclusion: SPELL effectively improves long-context reasoning capabilities, raises performance ceiling of strong models, and shows promise for scaling to more capable models through label-free self-improvement.

Abstract: Progress in long-context reasoning for large language models (LLMs) has
lagged behind other recent advances. This gap arises not only from the
intrinsic difficulty of processing long texts, but also from the scarcity of
reliable human annotations and programmatically verifiable reward signals. In
this paper, we propose SPELL, a multi-role self-play reinforcement learning
framework that enables scalable, label-free optimization for long-context
reasoning. SPELL integrates three cyclical roles-questioner, responder, and
verifier-within a single model to enable continual self-improvement. The
questioner generates questions from raw documents paired with reference
answers; the responder learns to solve these questions based on the documents;
and the verifier evaluates semantic equivalence between the responder's output
and the questioner's reference answer, producing reward signals to guide
continual training. To stabilize training, we introduce an automated curriculum
that gradually increases document length and a reward function that adapts
question difficulty to the model's evolving capabilities. Extensive experiments
on six long-context benchmarks show that SPELL consistently improves
performance across diverse LLMs and outperforms equally sized models fine-tuned
on large-scale annotated data. Notably, SPELL achieves an average 7.6-point
gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising
its performance ceiling and showing promise for scaling to even more capable
models.

</details>


### [91] [Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning](https://arxiv.org/abs/2509.23873)
*Shaobo Wang,Jiaming Wang,Jiajun Zhang,Cong Wang,Yue Min,Zichen Wen,Fei Huang,Huiqiang Jiang,Junyang Lin,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: Q-Tuning is a unified framework that jointly optimizes sample and token pruning for efficient supervised fine-tuning of LLMs, achieving state-of-the-art performance with only 12.5% of training data.


<details>
  <summary>Details</summary>
Motivation: Existing data pruning methods operate in isolation at either sample or token level, leading to inefficiencies where high-value samples may contain redundant tokens and token-level pruning discards important instructional signals.

Method: Proposes Error-Uncertainty (EU) Plane diagnostic framework and Quadrant-based Tuning (Q-Tuning) with two-stage strategy: sample-level triage to retain informative examples, followed by asymmetric token-pruning that trims less salient tokens only from misconception samples while preserving calibration samples entirely.

Result: Sets new state-of-the-art across five benchmarks. On SmolLM2-1.7B, achieves +38% average improvement over full-data SFT baseline using only 12.5% of original training data.

Conclusion: Q-Tuning is the first dynamic pruning approach to consistently outperform full-data training, providing a practical and scalable blueprint for maximizing data utilization in budget-constrained LLM SFT.

Abstract: As supervised fine-tuning (SFT) evolves from a lightweight post-training step
into a compute-intensive phase rivaling mid-training in scale, data efficiency
has become critical for aligning large language models (LLMs) under tight
budgets. Existing data pruning methods suffer from a fragmented design: they
operate either at the sample level or the token level in isolation, failing to
jointly optimize both dimensions. This disconnect leads to significant
inefficiencies--high-value samples may still contain redundant tokens, while
token-level pruning often discards crucial instructional or corrective signals
embedded in individual examples. To address this bottleneck, we introduce the
Error-Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes
the heterogeneous utility of training data across samples and tokens. Guided by
this insight, we propose Quadrant-based Tuning (Q-Tuning), a unified framework
that strategically coordinates sample pruning and token pruning. Q-Tuning
employs a two-stage strategy: first, it performs sample-level triage to retain
examples rich in informative misconceptions or calibration signals; second, it
applies an asymmetric token-pruning policy, using a context-aware scoring
mechanism to trim less salient tokens exclusively from misconception samples
while preserving calibration samples in their entirety. Our method sets a new
state of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B,
Q-Tuning achieves a +38\% average improvement over the full-data SFT baseline
using only 12.5\% of the original training data. As the first dynamic pruning
approach to consistently outperform full-data training, Q-Tuning provides a
practical and scalable blueprint for maximizing data utilization in
budget-constrained LLM SFT.

</details>


### [92] [DocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning](https://arxiv.org/abs/2509.23883)
*Yibo Yan,Guangwei Xu,Xin Zou,Shuliang Liu,James Kwok,Xuming Hu*

Main category: cs.CL

TL;DR: DocPruner introduces adaptive patch-level embedding pruning for Visual Document Retrieval (VDR) to reduce storage overhead by 50-60% while maintaining retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Current multi-vector VDR methods using LVLMs require storing hundreds of vectors per document page, creating prohibitive storage costs that make large-scale deployment impractical.

Method: DocPruner leverages intra-document patch attention distribution to dynamically identify and discard redundant embeddings for each document, enabling adaptive pruning.

Result: Extensive experiments across 10+ datasets show 50-60% storage reduction with negligible performance degradation in document retrieval.

Conclusion: DocPruner provides a robust, flexible, and effective solution for building storage-efficient, large-scale VDR systems.

Abstract: Visual Document Retrieval (VDR), the task of retrieving visually-rich
document pages using queries that combine visual and textual cues, is crucial
for numerous real-world applications. Recent state-of-the-art methods leverage
Large Vision-Language Models (LVLMs) in a multi-vector paradigm, representing
each document as patch-level embeddings to capture fine-grained details. While
highly effective, this approach introduces a critical challenge: prohibitive
storage overhead, as storing hundreds of vectors per page makes large-scale
deployment costly and impractical. To address this, we introduce DocPruner, the
first framework to employ adaptive patch-level embedding pruning for VDR to
effectively reduce the storage overhead. DocPruner leverages the intra-document
patch attention distribution to dynamically identify and discard redundant
embeddings for each document. This adaptive mechanism enables a significant
50-60% reduction in storage for leading multi-vector VDR models with negligible
degradation in document retrieval performance. Extensive experiments across
more than ten representative datasets validate that DocPruner offers a robust,
flexible, and effective solution for building storage-efficient, large-scale
VDR systems.

</details>


### [93] [Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step](https://arxiv.org/abs/2509.23924)
*Jingyi Yang,Guanxu Chen,Xuhao Hu,Jing Shao*

Main category: cs.CL

TL;DR: This paper addresses challenges in masked diffusion language models (MDLMs) by proposing EOS Early Rejection and Ascending Step-Size decoding strategies, along with Consistency Trajectory Group Relative Policy Optimization for reinforcement learning, achieving competitive performance with fewer decoding steps.


<details>
  <summary>Details</summary>
Motivation: MDLMs offer advantages like parallel decoding and flexible generation but lack tailored decoding strategies and RL algorithms. Direct transfer of AR model techniques to MDLMs is suboptimal due to training-inference inconsistencies and non-causal decoding.

Method: Proposed EOSER and ASS decoding scheduler for full diffusion-style decoding, and CJ-GRPO RL algorithm that ensures consistency between rollout and optimization trajectories while reducing skip-step optimization errors.

Result: Experiments on reasoning tasks using LLaDA-8B-Instruct show the proposed methods achieve competitive performance with fewer decoding steps, effectively taming MDLMs.

Conclusion: The EOSER and ASS mechanisms combined with CJ-GRPO provide promising solutions for efficient and effective MDLM training and inference, addressing key challenges in non-causal parallel decoding.

Abstract: Masked diffusion language models (MDLMs) have recently emerged as a promising
alternative to autoregressive (AR) language models, offering properties such as
parallel decoding, flexible generation orders, and the potential for fewer
inference steps. Despite these advantages, decoding strategies and
reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.
A naive approach is to directly transfer techniques well-established for AR
models to MDLMs. However, this raises an immediate question: Is such a naive
transfer truly optimal? For example, 1) Block-wise and semi-AR decoding
strategies are not employed during the training of MDLMs, so why do they
outperform full diffusion-style decoding during inference? 2) Applying RL
algorithms designed for AR models directly to MDLMs exhibits a
training-inference inconsistency, since MDLM decoding are non-causal
(parallel). This results in inconsistencies between the rollout trajectory and
the optimization trajectory. To address these challenges, we propose EOS Early
Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which
unlock the potential of MDLMs to perform full diffusion-style decoding,
achieving competitive performance with fewer decoding steps. Additionally, we
introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)
for taming MDLMs, which emphasizes the consistency between rollout trajectory
and optimization trajectory, and reduces the optimization errors caused by
skip-step optimization. We conduct extensive experiments on reasoning tasks,
such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The
results demonstrate that the proposed EOSER and ASS mechanisms, together with
CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.
Code: https://github.com/yjyddq/EOSER-ASS-RL.

</details>


### [94] [Assessing Large Language Models in Updating Their Forecasts with New Information](https://arxiv.org/abs/2509.23936)
*Zhangdie Yuan,Zifeng Ding,Andreas Vlachos*

Main category: cs.CL

TL;DR: EVOLVECAST framework evaluates LLMs' ability to revise predictions when presented with new post-training information, finding models update inconsistently and conservatively compared to human forecasters.


<details>
  <summary>Details</summary>
Motivation: Address the gap in treating future event prediction as static, without considering how forecasts should evolve as new evidence emerges.

Method: Introduce EVOLVECAST framework to assess LLM prediction revision using human forecasters as reference, analyzing prediction shifts and confidence calibration under updated contexts.

Result: LLMs show some responsiveness to new information but updates are inconsistent/overly conservative; neither verbalized nor logits-based confidence estimates consistently outperform each other; both far from human standard; models express conservative bias.

Conclusion: Need for more robust approaches to belief updating in LLMs, as current models demonstrate conservative bias and inadequate prediction revision capabilities.

Abstract: Prior work has largely treated future event prediction as a static task,
failing to consider how forecasts and the confidence in them should evolve as
new evidence emerges. To address this gap, we introduce EVOLVECAST, a framework
for evaluating whether large language models appropriately revise their
predictions in response to new information. In particular, EVOLVECAST assesses
whether LLMs adjust their forecasts when presented with information released
after their training cutoff. We use human forecasters as a comparative
reference to analyze prediction shifts and confidence calibration under updated
contexts. While LLMs demonstrate some responsiveness to new information, their
updates are often inconsistent or overly conservative. We further find that
neither verbalized nor logits-based confidence estimates consistently
outperform the other, and both remain far from the human reference standard.
Across settings, models tend to express conservative bias, underscoring the
need for more robust approaches to belief updating.

</details>


### [95] [Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems](https://arxiv.org/abs/2509.23938)
*Guojian Li,Chengyou Wang,Hongfei Xue,Shuiyuan Wang,Dehui Gao,Zihan Zhang,Yuke Lin,Wenjie Li,Longshuai Xiao,Zhonghua Fu,Lei Xie*

Main category: cs.CL

TL;DR: Easy Turn is an open-source, bimodal turn-taking detection model that integrates acoustic and linguistic information to predict four dialogue states, achieving SOTA performance with a 1,145-hour training dataset.


<details>
  <summary>Details</summary>
Motivation: Full-duplex human-machine communication requires robust turn-taking detection, but existing solutions are limited by being closed-source, large in parameter size, single-modal, or requiring scarce full-duplex data.

Method: Proposed Easy Turn - a modular turn-taking detection model that integrates acoustic and linguistic bimodal information to predict four dialogue turn states: complete, incomplete, backchannel, and wait, along with releasing Easy Turn trainset.

Result: Achieves state-of-the-art turn-taking detection accuracy on the open-source Easy Turn testset compared to existing models like TEN Turn Detection and Smart Turn V2.

Conclusion: Easy Turn provides an effective open-source solution for full-duplex interaction with bimodal turn-taking detection, accompanied by a large training dataset to address data scarcity issues.

Abstract: Full-duplex interaction is crucial for natural human-machine communication,
yet remains challenging as it requires robust turn-taking detection to decide
when the system should speak, listen, or remain silent. Existing solutions
either rely on dedicated turn-taking models, most of which are not
open-sourced. The few available ones are limited by their large parameter size
or by supporting only a single modality, such as acoustic or linguistic.
Alternatively, some approaches finetune LLM backbones to enable full-duplex
capability, but this requires large amounts of full-duplex data, which remain
scarce in open-source form. To address these issues, we propose Easy Turn, an
open-source, modular turn-taking detection model that integrates acoustic and
linguistic bimodal information to predict four dialogue turn states: complete,
incomplete, backchannel, and wait, accompanied by the release of Easy Turn
trainset, a 1,145-hour speech dataset designed for training turn-taking
detection models. Compared to existing open-source models like TEN Turn
Detection and Smart Turn V2, our model achieves state-of-the-art turn-taking
detection accuracy on our open-source Easy Turn testset. The data and model
will be made publicly available on GitHub.

</details>


### [96] [Vision-Grounded Machine Interpreting: Improving the Translation Process through Visual Cues](https://arxiv.org/abs/2509.23957)
*Claudio Fantinuoli*

Main category: cs.CL

TL;DR: Vision-Grounded Interpreting (VGI) introduces multimodal machine interpreting that combines speech and visual input to improve translation quality, particularly for lexical disambiguation.


<details>
  <summary>Details</summary>
Motivation: Current machine interpreting systems rely solely on linguistic signals, limiting performance in contexts requiring visual, situational, or pragmatic cues for disambiguation and adequacy.

Method: Developed a prototype system integrating vision-language model to process both speech and visual input from webcam, using visual context to prime translation. Evaluated with hand-crafted diagnostic corpus targeting three ambiguity types.

Result: Visual grounding substantially improves lexical disambiguation, shows modest and less stable gains for gender resolution, and no benefit for syntactic ambiguities.

Conclusion: Embracing multimodality is a necessary step forward for advancing translation quality in machine interpreting systems.

Abstract: Machine Interpreting systems are currently implemented as unimodal, real-time
speech-to-speech architectures, processing translation exclusively on the basis
of the linguistic signal. Such reliance on a single modality, however,
constrains performance in contexts where disambiguation and adequacy depend on
additional cues, such as visual, situational, or pragmatic information. This
paper introduces Vision-Grounded Interpreting (VGI), a novel approach designed
to address the limitations of unimodal machine interpreting. We present a
prototype system that integrates a vision-language model to process both speech
and visual input from a webcam, with the aim of priming the translation process
through contextual visual information. To evaluate the effectiveness of this
approach, we constructed a hand-crafted diagnostic corpus targeting three types
of ambiguity. In our evaluation, visual grounding substantially improves
lexical disambiguation, yields modest and less stable gains for gender
resolution, and shows no benefit for syntactic ambiguities. We argue that
embracing multimodality represents a necessary step forward for advancing
translation quality in machine interpreting.

</details>


### [97] [HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs](https://arxiv.org/abs/2509.23967)
*Ken Deng,Zizheng Zhan,Wen Xiang,Wenqiang Zhu,Tianhao Peng,Xinping Lei,Weihao Li,Jingxuan Xu,Kun Wu,Yifan Yao,Haoyang Huang,Huaixi Tang,Kepeng Lei,Zhiyi Lai,Songwei Yu,Zongxian Feng,Zuchen Gao,Weihao Xie,Chenchen Zhang,Yanan Wu,Yuanxing Zhang,Lecheng Huang,Yuqun Zhang,Jie Liu,Zhaoxiang Zhang,Haotian Zhang,Bin Chen,Jiaheng Liu*

Main category: cs.CL

TL;DR: HiPO is a framework for adaptive reasoning control that enables LLMs to selectively decide when to use detailed reasoning (Think-on) and when to respond directly (Think-off), balancing accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LLMs always generate lengthy reasoning traces (CoT) which is inefficient, leading to excessive token usage and higher inference costs. There's a need for selective reasoning to optimize resource usage.

Method: HiPO combines a hybrid data pipeline providing paired Think-on and Think-off responses with a hybrid reinforcement learning reward system that balances accuracy and efficiency while avoiding over-reliance on detailed reasoning.

Result: Experiments across mathematics and coding benchmarks demonstrate that HiPO can substantially reduce token length while maintaining or improving accuracy.

Conclusion: HiPO provides a principled approach for efficient adaptive reasoning, advancing the deployment of reasoning-oriented LLMs in real-world, resource-sensitive settings.

Abstract: Large Language Models (LLMs) increasingly rely on chain-of-thought (CoT)
reasoning to improve accuracy on complex tasks. However, always generating
lengthy reasoning traces is inefficient, leading to excessive token usage and
higher inference costs. This paper introduces the Hybrid Policy Optimization
(i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to
selectively decide when to engage in detailed reasoning (Think-on) and when to
respond directly (Think-off). Specifically, HiPO combines a hybrid data
pipelineproviding paired Think-on and Think-off responseswith a hybrid
reinforcement learning reward system that balances accuracy and efficiency
while avoiding over-reliance on detailed reasoning. Experiments across
mathematics and coding benchmarks demonstrate that HiPO can substantially
reduce token length while maintaining or improving accuracy. Finally, we hope
HiPO a can be a principled approach for efficient adaptive reasoning, advancing
the deployment of reasoning-oriented LLMs in real-world, resource-sensitive
settings.

</details>


### [98] [ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation](https://arxiv.org/abs/2509.23979)
*Haonan Wang,Junfeng Sun,Xingdi Yuan,Ruoyao Wang,Ziang Xiao*

Main category: cs.CL

TL;DR: ByteSized32Refactored is a modular implementation of text game generation that reduces code by 50% through a foundation library with 7 base classes, enabling extensibility but presenting mixed LLM performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of simulating interactive world models in LLMs by creating a more modular and extensible text game generation framework.

Method: Refactored the original ByteSized32 corpus with modular design, created GameBasic.py foundation library abstracting 7 base classes (GameObject, etc.), and centralized common logic across 32 games.

Result: Reduced total lines of Python code from 20k to 10k. GPT-4o experiments showed mixed performance - quality improvements on 2 of 4 evaluation dimensions but decreases on the other 2 for unseen scenarios.

Conclusion: The extensible code structure facilitates LLM adaptation and establishes a scalable environment for future extensions, though the hierarchical structure presents new challenges for LLMs.

Abstract: Simulating interactive world models remains a core challenge in Large
Language Models(LLMs). In this work, we introduce the ByteSized32Refactored, a
refactored, modular, and extensible implementation of the original ByteSized32
corpus to explore the task of text game generation. We further optimize the
code structure of each text game and create the GameBasic.py foundation
library, which centralizes common logic across all 32 games by abstracting 7
base classes (GameObject, etc.) into reusable modules, thereby reducing from
20k to 10k total lines of Python code compared to the original Bytesized32. Our
refactored implementation enables extendability - with our centralized design,
ByteSized32Refactored can be more efficiently extended to include text games of
new scenarios and specifications by reusing the shared logic and
functionalities. Extensive experiments with GPT-4o demonstrate a mix of
performance - with Bytesized32Refactored, the generated text games for unseen
scenarios showcase quality improvements on two of the four evaluation
dimensions while decreases on the other two, indicating that the hierarchical
structure of the refactored code presents new challenges for LLMs. Overall, we
highlight that our extensible code structure, centered on the foundation
library and the modular optimization, not only facilitates LLM adaptation to
environment specifications but also establishes a scalable environment that
supports future extensions.

</details>


### [99] [Toward Preference-aligned Large Language Models via Residual-based Model Steering](https://arxiv.org/abs/2509.23982)
*Lucio La Cava,Andrea Tagarelli*

Main category: cs.CL

TL;DR: PaLRS is a training-free method for LLM preference alignment that extracts steering vectors from residual streams using minimal preference data, enabling plug-and-play inference-time alignment without expensive optimization.


<details>
  <summary>Details</summary>
Motivation: Existing preference alignment methods like RLHF and DPO require curated data, expensive optimization over billions of parameters, and create persistent task-specific models, making them inefficient and inflexible.

Method: Extracts lightweight steering vectors from LLM residual streams using as few as 100 preference pairs, then applies these plug-and-play vectors at inference time to steer models toward preferred behaviors.

Result: PaLRS-aligned models achieve consistent gains on mathematical reasoning and code generation benchmarks while preserving baseline general-purpose performance, and outperform DPO-aligned models with huge time savings.

Conclusion: PaLRS offers an effective, efficient, and flexible alternative to standard preference optimization pipelines, providing training-free, plug-and-play alignment with minimal data requirements.

Abstract: Preference alignment is a critical step in making Large Language Models
(LLMs) useful and aligned with (human) preferences. Existing approaches such as
Reinforcement Learning from Human Feedback or Direct Preference Optimization
typically require curated data and expensive optimization over billions of
parameters, and eventually lead to persistent task-specific models. In this
work, we introduce Preference alignment of Large Language Models via Residual
Steering (PaLRS), a training-free method that exploits preference signals
encoded in the residual streams of LLMs. From as few as one hundred preference
pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be
applied at inference time to push models toward preferred behaviors. We
evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that
PaLRS-aligned models achieve consistent gains on mathematical reasoning and
code generation benchmarks while preserving baseline general-purpose
performance. Moreover, when compared to DPO-aligned models, they perform better
with huge time savings. Our findings highlight that PaLRS offers an effective,
much more efficient and flexible alternative to standard preference
optimization pipelines, offering a training-free, plug-and-play mechanism for
alignment with minimal data.

</details>


### [100] [The Hidden Costs of Translation Accuracy: Distillation, Quantization, and Environmental Impact](https://arxiv.org/abs/2509.23990)
*Dhaathri Vijay,Anandaswarup Vadapalli*

Main category: cs.CL

TL;DR: This study shows that distilled and quantized LLMs can achieve 4.5x faster inference with minimal quality loss, significantly reducing computational costs and carbon emissions (0.007-0.008 kg CO2 per run for full model) while maintaining competitive translation quality.


<details>
  <summary>Details</summary>
Motivation: Address growing concerns about computational and environmental costs of large language models by investigating trade-offs between translation quality and efficiency.

Method: Compared full-scale, distilled, and quantized models using machine translation as case study. Evaluated on Flores+ benchmark and human judgments of conversational translations in French, Hindi, and Kannada. Analyzed carbon emissions per evaluation run.

Result: Distilled models achieved 4.5x faster inference than full 3.3B model with minimal BLEU score reductions. Aggressive quantization (INT4) preserved high accuracy and fluency. Full fp32 model had largest environmental footprint (0.007-0.008 kg CO2 per run). Trade-offs more pronounced in low-resource settings.

Conclusion: Model compression strategies can substantially reduce computational demands and environmental impact while maintaining competitive translation quality. Need evaluation frameworks that integrate efficiency and sustainability alongside objective metrics.

Abstract: The rapid expansion of large language models (LLMs) has heightened concerns
about their computational and environmental costs. This study investigates the
trade-offs between translation quality and efficiency by comparing full-scale,
distilled, and quantized models using machine translation as a case study. We
evaluated performance on the Flores+ benchmark and through human judgments of
conversational translations in French, Hindi, and Kannada. Our analysis of
carbon emissions per evaluation run revealed that the full 3.3B fp32 model,
while achieving the highest BLEU scores, incurred the largest environmental
footprint (about 0.007-0.008 kg CO2 per run). The distilled models achieved an
inference of up to 4.5x faster than the full 3.3B model, with only minimal
reductions in BLEU scores. Human evaluations also showed that even aggressive
quantization (INT4) preserved high levels of accuracy and fluency, with
differences between models generally minor. These findings demonstrate that
model compression strategies can substantially reduce computational demands and
environmental impact while maintaining competitive translation quality, though
trade-offs are more pronounced in low-resource settings. We argue for
evaluation frameworks that integrate efficiency and sustainability alongside
objective metrics as central dimensions of progress in NLP.

</details>


### [101] [The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt Synthesis](https://arxiv.org/abs/2509.23994)
*Gauri Kholkar,Ratinder Ahuja*

Main category: cs.CL

TL;DR: A framework that automatically converts unstructured design documents into verifiable, real-time guardrails for autonomous AI agents using LLMs to interpret and enforce natural language policies.


<details>
  <summary>Details</summary>
Motivation: As autonomous AI agents are increasingly deployed in industry, it is essential to safeguard them and bridge the critical policy-to-practice gap for verifiably safer and more regulatable AI.

Method: Introduces 'Policy as Prompt' approach using LLMs to interpret natural language policies, constructs verifiable policy trees from technical artifacts, and compiles them into lightweight prompt-based classifiers that audit agent behavior at runtime.

Result: Validated across diverse applications, demonstrating a scalable and auditable pipeline that successfully bridges the policy-to-practice gap.

Conclusion: The framework paves the way for verifiably safer and more regulatable AI by providing automated translation of design documents into real-time guardrails through LLM-based policy interpretation and enforcement.

Abstract: As autonomous AI agents are increasingly deployed in industry, it is
essential to safeguard them. We introduce a novel framework that automates the
translation of unstructured design documents into verifiable, real-time
guardrails. We introduce "Policy as Prompt," a new approach that uses Large
Language Models (LLMs) to interpret and enforce natural language policies by
applying contextual understanding and the principle of least privilege. Our
system first ingests technical artifacts to construct a verifiable policy tree,
which is then compiled into lightweight, prompt-based classifiers that audit
agent behavior at runtime. We validate our approach across diverse
applications, demonstrating a scalable and auditable pipeline that bridges the
critical policy-to-practice gap, paving the way for verifiably safer and more
regulatable AI.

</details>


### [102] [MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use](https://arxiv.org/abs/2509.24002)
*Zijian Wu,Xiangyan Liu,Xinyuan Zhang,Lingjun Chen,Fanqing Meng,Lingxiao Du,Yiran Zhao,Fanshi Zhang,Yaoqi Ye,Jiawei Wang,Zirui Wang,Jinjie Ni,Yufan Yang,Arvin Xu,Michael Qizhe Shieh*

Main category: cs.CL

TL;DR: MCPMark is a new benchmark with 127 tasks that evaluates MCP (Model Context Protocol) usage more realistically, requiring complex CRUD operations. Current LLMs perform poorly, with the best model achieving only 52.56% pass@1.


<details>
  <summary>Details</summary>
Motivation: Existing MCP benchmarks are too narrow, focusing on read-heavy tasks with limited interaction depth, failing to capture real-world workflow complexity.

Method: Created 127 high-quality tasks with domain experts and AI agents, each with curated initial states and programmatic verification scripts. Tasks require diverse CRUD operations and richer environment interactions.

Result: Best model (gpt-5-medium) achieved only 52.56% pass@1 and 33.86% pass^4. Other strong models (claude-sonnet-4, o3) fell below 30% pass@1 and 15% pass^4. LLMs required average 16.2 execution turns and 17.4 tool calls per task.

Conclusion: MCPMark effectively stress-tests MCP capabilities, revealing significant performance gaps in current LLMs for complex real-world interactions, with much higher interaction requirements than previous benchmarks.

Abstract: MCP standardizes how LLMs interact with external systems, forming the
foundation for general agents. However, existing MCP benchmarks remain narrow
in scope: they focus on read-heavy tasks or tasks with limited interaction
depth, and fail to capture the complexity and realism of real-world workflows.
To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP
use in a more realistic and comprehensive manner. It consists of $127$
high-quality tasks collaboratively created by domain experts and AI agents.
Each task begins with a curated initial state and includes a programmatic
script for automatic verification. These tasks demand richer and more diverse
interactions with the environment, involving a broad range of create, read,
update, and delete (CRUD) operations. We conduct a comprehensive evaluation of
cutting-edge LLMs using a minimal agent framework that operates in a
tool-calling loop. Empirical results show that the best-performing model,
gpt-5-medium, reaches only $52.56$\% pass@1 and $33.86$\% pass^4, while other
widely regarded strong models, including claude-sonnet-4 and o3, fall below
$30$\% pass@1 and $15$\% pass^4. On average, LLMs require $16.2$ execution
turns and $17.4$ tool calls per task, significantly surpassing those in
previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.

</details>


### [103] [Sequential Diffusion Language Models](https://arxiv.org/abs/2509.24007)
*Yangzhou Liu,Yue Cao,Hao Li,Gen Luo,Zhe Chen,Weiyun Wang,Xiaobo Liang,Biqing Qi,Lijun Wu,Changyao Tian,Yanting Zhang,Yuqiang Li,Tong Lu,Yu Qiao,Jifeng Dai,Wenhai Wang*

Main category: cs.CL

TL;DR: SDLM unifies next-token and next-block prediction through NSP, enabling adaptive generation length while maintaining KV-cache compatibility and achieving higher throughput than autoregressive models.


<details>
  <summary>Details</summary>
Motivation: Address limitations of diffusion language models including fixed-length decoding, KV-cache incompatibility, and expensive training requirements of existing approaches like block diffusion.

Method: Propose Next Sequence Prediction (NSP) that adaptively determines generation length per step, and Sequential Diffusion Language Model (SDLM) that retrofits pre-trained ALMs with diffusion inference in mask blocks while dynamically decoding based on model confidence.

Result: SDLM matches or surpasses autoregressive baselines with only 3.5M training samples, achieves 2.1× higher throughput than Qwen-2.5, and SDLM-32B shows even greater efficiency gains demonstrating scalability.

Conclusion: SDLM provides an efficient and scalable approach that bridges diffusion and autoregressive modeling, enabling adaptive sequence generation while maintaining compatibility with existing infrastructure.

Abstract: Diffusion language models (DLMs) have strong theoretical efficiency but are
limited by fixed-length decoding and incompatibility with key-value (KV)
caches. Block diffusion mitigates these issues, yet still enforces a fixed
block size and requires expensive training. We introduce Next Sequence
Prediction (NSP), which unifies next-token and next-block prediction, enabling
the model to adaptively determine the generation length at each step. When the
length is fixed to 1, NSP reduces to standard next-token prediction. Building
on NSP, we propose Sequential Diffusion Language Model (SDLM), which can
retrofit pre-trained autoregressive language models (ALMs) at minimal cost.
Specifically, SDLM performs diffusion inference within fixed-size mask blocks,
but dynamically decodes consecutive subsequences based on model confidence,
thereby preserving KV-cache compatibility and improving robustness to varying
uncertainty and semantics across the sequence. Experiments show that SDLM
matches or surpasses strong autoregressive baselines using only 3.5M training
samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the
SDLM-32B model delivers even more pronounced efficiency gains, demonstrating
the strong scalability potential of our modeling paradigm. Project page and
codes: https://github.com/OpenGVLab/SDLM

</details>


### [104] [SparseD: Sparse Attention for Diffusion Language Models](https://arxiv.org/abs/2509.24014)
*Zeqing Wang,Gongfan Fang,Xinyin Ma,Xingyi Yang,Xinchao Wang*

Main category: cs.CL

TL;DR: SparseD is a novel sparse attention method for diffusion language models that achieves lossless acceleration by using head-specific sparse patterns and full attention in early denoising steps.


<details>
  <summary>Details</summary>
Motivation: Existing open-source diffusion language models suffer from high inference latency due to attention's quadratic complexity with context length. Sparse attention methods designed for autoregressive models are incompatible with DLMs due to distinct sparsity behaviors.

Method: SparseD pre-computes head-specific sparse patterns once and reuses them across all denoising steps. It uses full attention in early steps and switches to sparse attention later to maintain generation quality.

Result: SparseD achieves up to 1.50× speedup over FlashAttention at 64k context length with 1,024 denoising steps while maintaining lossless acceleration.

Conclusion: SparseD provides a practical and efficient solution for deploying diffusion language models in long-context applications by addressing the unique sparsity characteristics of DLMs.

Abstract: While diffusion language models (DLMs) offer a promising alternative to
autoregressive models (ARs), existing open-source DLMs suffer from high
inference latency. This bottleneck is mainly due to the attention's quadratic
complexity with respect to context length in computing all query-key pairs.
Intuitively, to reduce this complexity, a natural strategy is to restrict
attention to sparse patterns that retain only the most relevant connections.
Such approaches are well-established in ARs, where attention follows fixed and
clearly defined sparse patterns. However, in DLMs, we observe distinct sparsity
behaviors: (1) attention patterns vary across heads, (2) attention patterns in
each head remain highly similar across denoising steps, and (3) early denoising
steps are critical for generation. These findings render sparse attention
methods designed for ARs largely incompatible with DLMs, as they fail to
capture head-specific structures and risk degrading generation when applied in
early denoising steps. To address these challenges, we propose SparseD, a novel
sparse attention method for DLMs. Leveraging the observations, SparseD only
requires pre-computing head-specific sparse patterns one time, and reuses them
across all steps. This prevents recomputing sparse patterns at each denoising
step. Meanwhile, SparseD uses full attention in the early steps, then switches
to sparse attention later to maintain generation quality. Together, these
establish SparseD as a practical and efficient solution for deploying DLMs in
long-context applications. Experimental results demonstrate that SparseD
achieves lossless acceleration, delivering up to $1.50\times$ speedup over
FlashAttention at a 64k context length with 1,024 denoising steps.

</details>


### [105] [ResFormer: All-Time Reservoir Memory for Long Sequence Classification](https://arxiv.org/abs/2509.24074)
*Hongbo Liu,Jia Xu*

Main category: cs.CL

TL;DR: ResFormer is a novel neural architecture that combines reservoir computing with Transformers to efficiently model varying context lengths, achieving significant accuracy improvements and reduced memory consumption compared to baseline models.


<details>
  <summary>Details</summary>
Motivation: Transformer models have quadratic complexity limitations that restrict input length, making it challenging to process extensive contexts efficiently. The goal is to overcome these computational constraints while maintaining performance.

Method: ResFormer uses a cascaded approach: reservoir computing with nonlinear readout for long-term dependencies in linear time, combined with conventional Transformers for short-term sentence dependencies with fixed-length inputs.

Result: ResFormer outperforms DeepSeek-Qwen and ModernBERT with up to +22.3% accuracy improvement on EmoryNLP dataset, and shows consistent gains on MultiWOZ, MELD, and IEMOCAP. It also exhibits reduced memory consumption.

Conclusion: ResFormer effectively addresses Transformer limitations by efficiently modeling extensive contextual information through its hybrid architecture, demonstrating both superior performance and computational efficiency.

Abstract: Sequence classification is essential in NLP for understanding and
categorizing language patterns in tasks like sentiment analysis, intent
detection, and topic classification. Transformer-based models, despite
achieving state-of-the-art performance, have inherent limitations due to
quadratic time and memory complexity, restricting their input length. Although
extensive efforts have aimed at reducing computational demands, processing
extensive contexts remains challenging.
  To overcome these limitations, we propose ResFormer, a novel neural network
architecture designed to model varying context lengths efficiently through a
cascaded methodology. ResFormer integrates an reservoir computing network
featuring a nonlinear readout to effectively capture long-term contextual
dependencies in linear time. Concurrently, short-term dependencies within
sentences are modeled using a conventional Transformer architecture with
fixed-length inputs.
  Experiments demonstrate that ResFormer significantly outperforms baseline
models of DeepSeek-Qwen and ModernBERT, delivering an accuracy improvement of
up to +22.3% on the EmoryNLP dataset and consistent gains on MultiWOZ, MELD,
and IEMOCAP. In addition, ResFormer exhibits reduced memory consumption,
underscoring its effectiveness and efficiency in modeling extensive contextual
information.

</details>


### [106] [Ensembling Multilingual Transformers for Robust Sentiment Analysis of Tweets](https://arxiv.org/abs/2509.24080)
*Meysam Shirdel Bilehsavar,Negin Mahmoudi,Mohammad Jalili Torkamani,Kiana Kiashemshaki*

Main category: cs.CL

TL;DR: This paper proposes a transformer ensemble model and LLM approach for multilingual sentiment analysis, achieving over 86% accuracy on multi-language datasets.


<details>
  <summary>Details</summary>
Motivation: Sentiment analysis faces challenges with foreign languages, especially when labeled training data is unavailable, limiting its effectiveness across different languages.

Method: Used an ensemble of pre-trained sentiment analysis models (bert-base-multilingual-uncased-sentiment and XLM-R) on multi-language datasets to assess sentence sentiment.

Result: Experimental results showed sentiment analysis performance exceeding 86% using the proposed ensemble method.

Conclusion: The transformer ensemble model with LLM approach effectively addresses multilingual sentiment analysis challenges and achieves high accuracy without requiring labeled training data for each language.

Abstract: Sentiment analysis is a very important natural language processing activity
in which one identifies the polarity of a text, whether it conveys positive,
negative, or neutral sentiment. Along with the growth of social media and the
Internet, the significance of sentiment analysis has grown across numerous
industries such as marketing, politics, and customer service. Sentiment
analysis is flawed, however, when applied to foreign languages, particularly
when there is no labelled data to train models upon. In this study, we present
a transformer ensemble model and a large language model (LLM) that employs
sentiment analysis of other languages. We used multi languages dataset.
Sentiment was then assessed for sentences using an ensemble of pre-trained
sentiment analysis models: bert-base-multilingual-uncased-sentiment, and XLM-R.
Our experimental results indicated that sentiment analysis performance was more
than 86% using the proposed method.

</details>


### [107] [Large-Scale Constraint Generation -- Can LLMs Parse Hundreds of Constraints?](https://arxiv.org/abs/2509.24090)
*Matteo Boffa,Jiaxuan You*

Main category: cs.CL

TL;DR: The paper introduces Large-Scale Constraint Generation (LSCG) to test LLMs' ability to handle large, fine-grained constraint lists, proposes FoCusNet to improve performance by filtering relevant constraints, and shows 8-13% accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLMs can effectively parse and handle large-scale, fine-grained constraint lists, addressing the performance degradation as constraint numbers increase.

Method: Created Words Checker as a practical LSCG instance, tested model characteristics and steering techniques, and proposed FoCusNet - a small model that filters constraints to help LLMs focus on relevant ones.

Result: Existing solutions suffer significant performance drops with increasing constraints, while FoCusNet achieves 8-13% accuracy improvement over baseline methods.

Conclusion: FoCusNet effectively addresses the challenge of large-scale constraint processing in LLMs, demonstrating substantial performance gains in constraint-heavy scenarios.

Abstract: Recent research has explored the constrained generation capabilities of Large
Language Models (LLMs) when explicitly prompted by few task-specific
requirements. In contrast, we introduce Large-Scale Constraint Generation
(LSCG), a new problem that evaluates whether LLMs can parse a large,
fine-grained, generic list of constraints. To examine the LLMs' ability to
handle an increasing number constraints, we create a practical instance of
LSCG, called Words Checker. In Words Checker, we evaluate the impact of model
characteristics (e.g., size, family) and steering techniques (e.g., Simple
Prompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet,
a small and dedicated model that parses the original list of constraints into a
smaller subset, helping the LLM focus on relevant constraints. Experiments
reveal that existing solutions suffer a significant performance drop as the
number of constraints increases, with FoCusNet showing an 8-13% accuracy boost.

</details>


### [108] [GEAR: A General Evaluation Framework for Abductive Reasoning](https://arxiv.org/abs/2509.24096)
*Kaiyu He,Peilin Wu,Mian Zhang,Kun Wan,Wentian Zhao,Xinya Du,Zhiyu Chen*

Main category: cs.CL

TL;DR: GEAR is a novel evaluation framework for assessing LLMs' abductive reasoning abilities through automated scoring of hypothesis sets based on consistency, generalizability, and diversity, without requiring human-labeled answers.


<details>
  <summary>Details</summary>
Motivation: To address whether LLMs can discover new knowledge through abductive reasoning (generating plausible hypotheses from observations) and develop a scalable, transparent evaluation method that doesn't rely on human gold answers.

Method: Introduces GEAR framework with three scoring metrics: consistency (hypotheses explain observations), generalizability (hypotheses predict unseen inputs), and diversity (coverage of distinct predictions). Uses this to evaluate 9 LLMs on 1,500 abduction problems, generating 50,000+ hypotheses. Also proposes momentum-based curriculum training that adjusts training data based on learning velocity.

Result: GEAR reveals model differences obscured by traditional evaluations. The momentum-based curriculum improves all GEAR objectives and transfers gains to established abduction benchmarks, enabling LLMs to produce more diverse and reliable hypotheses without gold-label supervision.

Conclusion: GEAR provides a principled framework for evaluating abduction and supplies scalable, label-free training signals that help LLMs generate more diverse and reliable hypotheses, advancing their knowledge discovery capabilities.

Abstract: Since the advent of large language models (LLMs), research has focused on
instruction following and deductive reasoning. A central question remains: can
these models discover new knowledge, and how can we evaluate this ability? We
address this by studying abductive reasoning-the generation of plausible
hypotheses to explain observations-and introduce GEAR (General Evaluation for
Abductive Reasoning), a general-purpose, fully automated, transparent, and
label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics:
consistency (each hypothesis explains the observations), generalizability
(consistent hypotheses make meaningful predictions on unseen inputs), and
diversity (the set covers distinct predictions and patterns). Built this way,
GEAR is scalable (no human gold answers), reliable (deterministic scoring
aligned with classical abduction), and open-ended (scores improve only when
models produce new plausible hypotheses, unlike static benchmarks that saturate
once accuracy is high). Using GEAR, we conduct a fine-grained study of nine
LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000
candidate hypotheses and revealing model differences obscured by gold-answer or
purely human evaluations. We further propose a momentum-based curriculum that
adjusts GEAR-derived training data by learning velocity: it starts with what
the model learns quickly and shifts toward harder objectives such as generating
diverse hypotheses once the model is confident on foundational objectives.
Without gold-label supervision, this strategy improves all GEAR objectives and
these gains transfer to established abductive reasoning benchmarks. Taken
together, GEAR provides a principled framework that evaluates abduction and
supplies label-free, scalable training signals that help LLMs produce more
diverse and reliable hypotheses.

</details>


### [109] [BTC-SAM: Leveraging LLMs for Generation of Bias Test Cases for Sentiment Analysis Models](https://arxiv.org/abs/2509.24101)
*Zsolt T. Kardkovács,Lynda Djennane,Anna Field,Boualem Benatallah,Yacine Gaci,Fabio Casati,Walid Gaaloul*

Main category: cs.CL

TL;DR: BTC-SAM is a novel bias testing framework that uses Large Language Models to generate high-quality test cases for identifying social biases in Sentiment Analysis models, providing better linguistic variation and test coverage compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Sentiment Analysis models contain harmful social biases that are difficult to detect comprehensively due to the high cost and effort required to create diverse, natural test sentences covering various identity groups and bias types.

Method: The framework uses Large Language Models for controllable generation of test sentences, enabling creation of linguistically rich and diverse test cases with minimal specification, even for previously unseen biases.

Result: Experiments show that LLM-based generation provides higher linguistic variation and diversity in test sentences, offering better test coverage compared to base prompting methods.

Conclusion: BTC-SAM demonstrates that LLMs can effectively generate comprehensive bias test cases for Sentiment Analysis models, making bias testing more accessible and thorough.

Abstract: Sentiment Analysis (SA) models harbor inherent social biases that can be
harmful in real-world applications. These biases are identified by examining
the output of SA models for sentences that only vary in the identity groups of
the subjects. Constructing natural, linguistically rich, relevant, and diverse
sets of sentences that provide sufficient coverage over the domain is
expensive, especially when addressing a wide range of biases: it requires
domain experts and/or crowd-sourcing. In this paper, we present a novel bias
testing framework, BTC-SAM, which generates high-quality test cases for bias
testing in SA models with minimal specification using Large Language Models
(LLMs) for the controllable generation of test sentences. Our experiments show
that relying on LLMs can provide high linguistic variation and diversity in the
test sentences, thereby offering better test coverage compared to base
prompting methods even for previously unseen biases.

</details>


### [110] [Pragmatic Inference for Moral Reasoning Acquisition: Generalization via Distributional Semantics](https://arxiv.org/abs/2509.24102)
*Guangliang Liu,Xi Chen,Bocheng Chen,Xitong Zhang,Kristen Johnson*

Main category: cs.CL

TL;DR: This paper addresses the challenge of achieving generalized moral reasoning in LLMs by proposing pragmatic inference methods based on moral foundations theory to bridge the gap between distributional semantics and pragmatic-level moral reasoning.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with generalizing moral reasoning because they operate at the distributional semantics level while morals function at the pragmatic level, creating a fundamental gap that needs to be addressed.

Method: Proposed pragmatic inference methods grounded in moral foundations theory that leverage contextual information at each step to connect moral foundations with moral reasoning objectives.

Result: Experimental results show that the approach significantly enhances LLMs' generalization capabilities in moral reasoning tasks.

Conclusion: The method provides a foundation for future research on moral reasoning in LLMs based on moral foundations theory, successfully bridging the pragmatic gap between distributional semantics and moral reasoning.

Abstract: Moral reasoning has emerged as a promising research direction for Large
Language Models (LLMs), yet achieving generalization remains a central
challenge. From a linguistic standpoint, this difficulty arises because LLMs
are adept at capturing distributional semantics, which fundamentally differs
from the morals which operate at the pragmatic level. This paper investigates
how LLMs can achieve generalized moral reasoning despite their reliance on
distributional semantics. We propose pragmatic inference methods grounded in
moral foundations theory, which leverage contextual information at each step to
bridge the pragmatic gap and guide LLMs in connecting moral foundations with
moral reasoning objectives. Experimental results demonstrate that our approach
significantly enhances LLMs' generalization in moral reasoning, providing a
foundation for future research grounded in moral foundations theory.

</details>


### [111] [Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems](https://arxiv.org/abs/2509.24116)
*Minsoo Kim,Seung-won Hwang*

Main category: cs.CL

TL;DR: GLoW is a novel LLM-based agent approach using dual-scale world models and Multi-path Advantage Reflection to tackle hard-exploration tasks, achieving SOTA performance on Jericho text games with 100-800x fewer environment interactions than RL methods.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents struggle with hard-exploration tasks that require learning new knowledge through exploration, which is a key limitation in current agent capabilities.

Method: Uses dual-scale world models with global trajectory frontier and local trial-and-error exploration through Multi-path Advantage Reflection mechanism that infers advantage-based progress signals.

Result: Achieves new state-of-the-art performance for LLM-based approaches on Jericho benchmark suite, with comparable performance to SOTA RL methods but requiring 100-800x fewer environment interactions.

Conclusion: GLoW demonstrates effective hard-exploration capabilities for LLM-based agents through its dual-scale world modeling and advantage-based exploration guidance, significantly improving sample efficiency.

Abstract: LLM-based agents have seen promising advances, yet they are still limited in
"hard-exploration" tasks requiring learning new knowledge through exploration.
We present GLoW, a novel approach leveraging dual-scale world models,
maintaining a trajectory frontier of high-value discoveries at the global
scale, while learning from local trial-and-error in exploration through a
Multi-path Advantage Reflection mechanism which infers advantage-based progress
signals to guide exploration. To evaluate our framework for hard-exploration,
we tackle the Jericho benchmark suite of text-based games, where GLoW achieves
a new state-of-theart performance for LLM-based approaches. Compared to
state-of-the-art RLbased methods, our approach achieves comparable performance
while requiring 100-800x fewer environment interactions.

</details>


### [112] [EduVidQA: Generating and Evaluating Long-form Answers to Student Questions based on Lecture Videos](https://arxiv.org/abs/2509.24120)
*Sourjyadip Ray,Shubham Sharma,Somak Aditya,Pawan Goyal*

Main category: cs.CL

TL;DR: This paper introduces EduVidQA, a dataset for automated question answering from online educational videos using Multimodal Large Language Models (MLLMs), and benchmarks 6 state-of-the-art models on this challenging task.


<details>
  <summary>Details</summary>
Motivation: As digital platforms transform education, maintaining interactivity is crucial for effective learning. The paper addresses the need for automated question answering from online lectures, which has real-world significance in educational technology.

Method: Created EduVidQA Dataset with 5252 question-answer pairs from 296 computer science videos, including both synthetic and real-world data. Studied student qualitative preferences and benchmarked 6 state-of-the-art MLLMs using text-based and qualitative metrics.

Result: The research demonstrates the challenging nature of the task and shows the effectiveness of synthetic data for finetuning. The evaluation provides nuanced perspectives on model performance through multiple metrics.

Conclusion: This work establishes a benchmark for automated question answering in educational videos and opens promising research directions in Natural Language Processing for Education.

Abstract: As digital platforms redefine educational paradigms, ensuring interactivity
remains vital for effective learning. This paper explores using Multimodal
Large Language Models (MLLMs) to automatically respond to student questions
from online lectures - a novel question answering task of real world
significance. We introduce the EduVidQA Dataset with 5252 question-answer pairs
(both synthetic and real-world) from 296 computer science videos covering
diverse topics and difficulty levels. To understand the needs of the dataset
and task evaluation, we empirically study the qualitative preferences of
students, which we provide as an important contribution to this line of work.
Our benchmarking experiments consist of 6 state-of-the-art MLLMs, through which
we study the effectiveness of our synthetic data for finetuning, as well as
showing the challenging nature of the task. We evaluate the models using both
text-based and qualitative metrics, thus showing a nuanced perspective of the
models' performance, which is paramount to future work. This work not only sets
a benchmark for this important problem, but also opens exciting avenues for
future research in the field of Natural Language Processing for Education.

</details>


### [113] [Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE](https://arxiv.org/abs/2509.24130)
*Guancheng Wan,Lucheng Fu,Haoxin Liu,Yiqiao Jin,Hui Yi Leong,Eric Hanchen Jiang,Hejia Geng,Jinhe Bi,Yunpu Ma,Xiangru Tang,B. Aditya Prakash,Yizhou Sun,Wei Wang*

Main category: cs.CL

TL;DR: TARE is a derivative-free prompt optimization framework that minimizes textual sharpness by alternating between adversarial paraphrase search and robust selection, improving prompt robustness to semantic-preserving changes while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Current prompt optimization methods focus only on point-wise accuracy but ignore paraphrase invariance, making prompts brittle to small semantic-preserving changes that cause large performance swings.

Method: TARE framework alternates between inner adversarial search (stressing prompts with hard paraphrases) and outer robust selection (preferring candidates with strong neighborhoods). ATARE variant learns anisotropic weights to shape semantic neighborhood and adapts radius over time.

Result: TARE produces prompts that preserve accuracy under paraphrasing, outperforming accuracy-only prompt search methods while remaining computationally practical across diverse tasks.

Conclusion: Minimizing textual sharpness gap leads to more robust prompts that maintain performance under semantic-preserving paraphrases, addressing the brittleness of traditional prompt optimization approaches.

Abstract: The performance of Large Language Models (LLMs) hinges on carefully
engineered prompts. However, prevailing prompt optimization methods, ranging
from heuristic edits and reinforcement learning to evolutionary search,
primarily target point-wise accuracy. They seldom enforce paraphrase invariance
or searching stability, and therefore cannot remedy this brittleness in
practice. Automated prompt search remains brittle: small, semantically
preserving paraphrases often cause large performance swings. We identify this
brittleness as the textual sharpness of the prompt landscape. In this work, we
provide the first formal treatment of textual sharpness in the discrete,
semantic space of prompts, together with an operational robustness criterion
over a semantic neighborhood; the design is black-box or API-only, requiring no
gradients to update the model's parameters. Then we introduce TARE (Textual
Sharpness-Aware Evolving), a derivative-free framework that alternates between
an inner, sampling-based adversarial search that stresses a prompt with hard
paraphrases and an outer, robust selection that prefers candidates whose
neighborhoods remain strong. We further propose ATARE, which learns anisotropic
weights to shape the semantic neighborhood and adapts its radius over time to
balance exploration and fidelity. Diverse tasks evaluate our methods, whose
design for minimizing textual sharpness gap leads to prompts that preserve
accuracy under paraphrasing, outperforming accuracy-only prompt search while
remaining computationally practical.

</details>


### [114] [Your thoughts tell who you are: Characterize the reasoning patterns of LRMs](https://arxiv.org/abs/2509.24147)
*Yida Chen,Yuning Mao,Xianjun Yang,Suyu Ge,Shengjie Bi,Lijuan Liu,Saghar Hosseini,Liang Tan,Yixin Nie,Shaoliang Nie*

Main category: cs.CL

TL;DR: LOT is a method that uses language models to compare reasoning traces from different large reasoning models, creating a human-readable taxonomy that characterizes their distinctive thinking patterns and achieves high accuracy in distinguishing between models.


<details>
  <summary>Details</summary>
Motivation: Current comparisons of large reasoning models focus only on macro-level statistics like accuracy, leaving the question of whether different models actually reason differently unanswered.

Method: LOT uses a generative language model to compare reasoning traces from two LRMs, articulate their distinctive features in words, and model how these features predict the source model based on empirical distributions across outputs.

Result: Applied to 12 open-source LRMs on math, science, and coding tasks, LOT achieved 80-100% accuracy in distinguishing reasoning traces and identified systematic differences in reasoning styles. Aligning smaller models' reasoning with larger models improved accuracy on GPQA by 3.3-5.7%.

Conclusion: LOT provides both quantitative classification and qualitative explanations of how different large reasoning models think, revealing systematic reasoning differences that can be leveraged to improve model performance.

Abstract: Current comparisons of large reasoning models (LRMs) focus on macro-level
statistics such as task accuracy or reasoning length. Whether different LRMs
reason differently remains an open question. To address this gap, we introduce
the LLM-proposed Open Taxonomy (LOT), a classification method that uses a
generative language model to compare reasoning traces from two LRMs and
articulate their distinctive features in words. LOT then models how these
features predict the source LRM of a reasoning trace based on their empirical
distributions across LRM outputs. Iterating this process over a dataset of
reasoning traces yields a human-readable taxonomy that characterizes how models
think. We apply LOT to compare the reasoning of 12 open-source LRMs on tasks in
math, science, and coding. LOT identifies systematic differences in their
thoughts, achieving 80-100% accuracy in distinguishing reasoning traces from
LRMs that differ in scale, base model family, or objective domain. Beyond
classification, LOT's natural-language taxonomy provides qualitative
explanations of how LRMs think differently. Finally, in a case study, we link
the reasoning differences to performance: aligning the reasoning style of
smaller Qwen3 models with that of the largest Qwen3 during test time improves
their accuracy on GPQA by 3.3-5.7%.

</details>


### [115] [Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis](https://arxiv.org/abs/2509.24164)
*Haolin Yang,Hakaze Cho,Naoya Inoue*

Main category: cs.CL

TL;DR: This paper proposes a unified framework (TSLA) to analyze in-context learning mechanisms in LLMs, identifying specialized attention heads for task recognition and task learning, and showing how they work together.


<details>
  <summary>Details</summary>
Motivation: To reconcile two dominant perspectives on in-context learning mechanisms: component-level analysis of attention heads vs holistic TR-TL decomposition, providing a unified understanding.

Method: Proposed Task Subspace Logit Attribution (TSLA) framework, conducted correlation analysis, ablation studies, input perturbations, and steering experiments with geometric analysis of hidden states.

Result: Identified distinct TR and TL attention heads that independently capture ICL components; TR heads align hidden states with task subspace, TL heads rotate states toward correct labels.

Conclusion: Provides a unified and interpretable account of how LLMs execute ICL across diverse tasks, reconciling previous findings like induction heads and task vectors with TR-TL decomposition.

Abstract: We investigate the mechanistic underpinnings of in-context learning (ICL) in
large language models by reconciling two dominant perspectives: the
component-level analysis of attention heads and the holistic decomposition of
ICL into Task Recognition (TR) and Task Learning (TL). We propose a novel
framework based on Task Subspace Logit Attribution (TSLA) to identify attention
heads specialized in TR and TL, and demonstrate their distinct yet
complementary roles. Through correlation analysis, ablation studies, and input
perturbations, we show that the identified TR and TL heads independently and
effectively capture the TR and TL components of ICL. Using steering experiments
with geometric analysis of hidden states, we reveal that TR heads promote task
recognition by aligning hidden states with the task subspace, while TL heads
rotate hidden states within the subspace toward the correct label to facilitate
prediction. We further show how previous findings on ICL mechanisms, including
induction heads and task vectors, can be reconciled with our
attention-head-level analysis of the TR-TL decomposition. Our framework thus
provides a unified and interpretable account of how large language models
execute ICL across diverse tasks and settings.

</details>


### [116] [Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight](https://arxiv.org/abs/2509.24169)
*Haolin Yang,Hakaze Cho,Kaize Ding,Naoya Inoue*

Main category: cs.CL

TL;DR: This paper introduces Learned Task Vectors (LTVs) as a direct training approach that outperforms extracted task vectors in accuracy and flexibility, while also providing mechanistic insights into how task vectors influence in-context learning through attention circuits and linear propagation patterns.


<details>
  <summary>Details</summary>
Motivation: Prior methods for extracting task vectors from LLMs are cumbersome, opaque, and don't explain how task vectors actually influence model computation during in-context learning.

Method: Directly train Learned Task Vectors (LTVs) instead of extracting them, then systematically analyze their mechanistic role through attention-head OV circuits and propagation patterns.

Result: LTVs surpass extracted task vectors in accuracy and work effectively at arbitrary layers, positions, and with ICL prompts. Analysis reveals task vectors primarily operate through attention-head OV circuits, with key heads being decisive, and propagation is largely linear despite Transformer nonlinearities.

Conclusion: LTVs provide both a practical approach for obtaining effective task vectors and a principled framework for understanding the mechanistic foundations of in-context learning in LLMs.

Abstract: Large Language Models (LLMs) can perform new tasks from in-context
demonstrations, a phenomenon known as in-context learning (ICL). Recent work
suggests that these demonstrations are compressed into task vectors (TVs),
compact task representations that LLMs exploit for predictions. However, prior
studies typically extract TVs from model outputs or hidden states using
cumbersome and opaque methods, and they rarely elucidate the mechanisms by
which TVs influence computation. In this work, we address both limitations.
First, we propose directly training Learned Task Vectors (LTVs), which surpass
extracted TVs in accuracy and exhibit superior flexibility-acting effectively
at arbitrary layers, positions, and even with ICL prompts. Second, through
systematic analysis, we investigate the mechanistic role of TVs, showing that
at the low level they steer predictions primarily through attention-head OV
circuits, with a small subset of "key heads" most decisive. At a higher level,
we find that despite Transformer nonlinearities, TV propagation is largely
linear: early TVs are rotated toward task-relevant subspaces to improve logits
of relevant labels, while later TVs are predominantly scaled in magnitude.
Taken together, LTVs not only provide a practical approach for obtaining
effective TVs but also offer a principled lens into the mechanistic foundations
of ICL.

</details>


### [117] [Retrieval-augmented GUI Agents with Generative Guidelines](https://arxiv.org/abs/2509.24183)
*Ran Xu,Kaixin Ma,Wenhao Yu,Hongming Zhang,Joyce C. Ho,Carl Yang,Dong Yu*

Main category: cs.CL

TL;DR: RAG-GUI is a lightweight VLM that enhances GUI agents by leveraging web tutorials at inference time, using supervised finetuning and self-guided rejection sampling finetuning to improve performance on complex digital tasks.


<details>
  <summary>Details</summary>
Motivation: GUI agents face limitations due to scarce training data and the complexity of real-world tasks that require long-tailed knowledge for rare scenarios.

Method: Proposes RAG-GUI, a model-agnostic VLM that uses web tutorials at inference time, warm-started via supervised finetuning and refined through self-guided rejection sampling finetuning.

Result: Outperforms baseline agents and other inference baselines by 2.6% to 13.3% across two model sizes, demonstrating strong generalization in three distinct tasks.

Conclusion: RAG-GUI serves as an effective plug-and-play enhancement for VLM-based agents, showing practical capabilities in real-world scenarios.

Abstract: GUI agents powered by vision-language models (VLMs) show promise in
automating complex digital tasks. However, their effectiveness in real-world
applications is often limited by scarce training data and the inherent
complexity of these tasks, which frequently require long-tailed knowledge
covering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that
leverages web tutorials at inference time. RAG-GUI is first warm-started via
supervised finetuning (SFT) and further refined through self-guided rejection
sampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as
a generic plug-in that enhances any VLM-based agent. Evaluated across three
distinct tasks, it consistently outperforms baseline agents and surpasses other
inference baselines by 2.6% to 13.3% across two model sizes, demonstrating
strong generalization and practical plug-and-play capabilities in real-world
scenarios.

</details>


### [118] [Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific Medical Capabilities of 80 Large Language Models](https://arxiv.org/abs/2509.24186)
*Zhimeng Luo,Lixin Wu,Adam Frisch,Daqing He*

Main category: cs.CL

TL;DR: MedIRT is an Item Response Theory-based framework for evaluating LLMs in medical applications, using a 1,100-question USMLE benchmark to measure latent model abilities, question difficulty, and discrimination, revealing specialized competency profiles beyond overall rankings.


<details>
  <summary>Details</summary>
Motivation: Traditional accuracy metrics are inadequate for evaluating LLMs in high-stakes medical applications as they fail to capture question characteristics or provide topic-specific insights, creating a critical need for more reliable evaluation methodologies.

Method: Prospectively gathered responses from 80 diverse LLMs on a balanced 1,100-question USMLE-aligned benchmark, using unidimensional two-parameter logistic IRT models per topic to estimate latent model ability, question difficulty, and discrimination parameters.

Result: Identified distinctive "spiky" ability profiles where overall rankings can be misleading; GPT-5 was top performer in 8 of 11 domains but was outperformed by Claude-3-opus in Social Science and Communication; IRT successfully identified flawed questions for benchmark auditing.

Conclusion: MedIRT establishes a robust, psychometrically grounded methodology essential for safe, effective, and trustworthy deployment of LLMs in healthcare, providing a practical decision-support framework that integrates multi-factor competency profiles with operational metrics.

Abstract: As Large Language Models (LLMs) are increasingly proposed for high-stakes
medical applications, there has emerged a critical need for reliable and
accurate evaluation methodologies. Traditional accuracy metrics fail
inadequately as they neither capture question characteristics nor offer
topic-specific insights. To address this gap, we introduce \textsc{MedIRT}, a
rigorous evaluation framework grounded in Item Response Theory (IRT), the gold
standard in high-stakes educational testing. Unlike previous research relying
on archival data, we prospectively gathered fresh responses from 80 diverse
LLMs on a balanced, 1,100-question USMLE-aligned benchmark. Using one
unidimensional two-parameter logistic IRT model per topic, we estimate LLM's
latent model ability jointly with question difficulty and discrimination,
yielding more stable and nuanced performance rankings than accuracy alone.
Notably, we identify distinctive ``spiky'' ability profiles, where overall
rankings can be misleading due to highly specialized model abilities. While
\texttt{GPT-5} was the top performer in a majority of domains (8 of 11), it was
outperformed in Social Science and Communication by \texttt{Claude-3-opus},
demonstrating that even an overall 23rd-ranked model can hold the top spot for
specific competencies. Furthermore, we demonstrate IRT's utility in auditing
benchmarks by identifying flawed questions. We synthesize these findings into a
practical decision-support framework that integrates our multi-factor
competency profiles with operational metrics. This work establishes a robust,
psychometrically grounded methodology essential for the safe, effective, and
trustworthy deployment of LLMs in healthcare.

</details>


### [119] [PET: Preference Evolution Tracking with LLM-Generated Explainable Distribution](https://arxiv.org/abs/2509.24189)
*Luyang Zhang,Siyuan Peng,Jialu Wang,Shichao Zhu,Beibei Li,Zhongcun Wang,Guangmou Pan,Yan Li,Song Yang*

Main category: cs.CL

TL;DR: PET framework reframes user preference prediction from direct LLM generation to inferring dynamic probability distributions over interpretable preference clusters, improving ranking quality and addressing transparency issues.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based approaches for user preference prediction suffer from opaque decision-making, limited personalization, and popularity bias, making holistic user profiling difficult.

Method: Proposes Preference Evolution Tracking (PET) using logit-probing and generative classification to infer user preferences as probability distributions over stable preference clusters rather than direct item generation.

Result: PET improves ranking quality by up to 40% in NDCG on public benchmarks and outperforms SOTA production model by 7 times in NDCG on real-world short-video platform data, particularly excelling at ranking long-tail content.

Conclusion: PET transforms user profiling from direct preference generation to transparent distributional mapping, enabling more explainable, fair, and diverse personalization systems.

Abstract: Understanding how user preference evolves over time is a fundamental
challenge central to modern digital ecosystems, for which Large Language Models
(LLMs) are an increasingly prominent and popular approach due to their ability
to comprehend the rich semantic context within behavioral data. A common
practice is to use LLMs to predict a user's next action by directly generating
a ranked list of preferred items. Although effective for short-term prediction,
the end-to-end generation paradigm inherently limits personalization. Its
opaque decision-making process obscures holistic user profiling and exacerbates
popularity bias. To address these limitations, we propose Preference Evolution
Tracking (PET), a framework that reframes the task as inferring a dynamic
probability distribution over a stable and interpretable lattice of preference
clusters. By applying logit-probing and generative classification techniques,
PET infers a user's preference as a probability distribution, enabling
transparent preference learning. On public benchmarks (Yelp, MovieLens), PET
improves ranking quality by up to 40% in NDCG over direct generation baselines.
On a large-scale, real-world dataset from a short-video platform, it excels at
ranking long-tail contents, significantly outperforming a SOTA production model
by 7 times in the NDCG score. Ultimately, PET transforms the user profile model
from direct preference list generation to a transparent distributional
preference mapping, paving the way for more explainable, fair, and diverse
personalization systems.

</details>


### [120] [AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play](https://arxiv.org/abs/2509.24193)
*Ran Xu,Yuchen Zhuang,Zihan Dong,Jonathan Wang,Yue Yu,Joyce C. Ho,Linjun Zhang,Haoyu Wang,Wenqi Shi,Carl Yang*

Main category: cs.CL

TL;DR: AceSearcher is a cooperative self-play framework that trains a single LLM to alternate between decomposer and solver roles, achieving state-of-the-art performance on complex reasoning tasks with significantly fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Search-augmented LLMs struggle with complex reasoning due to ineffective multi-hop retrieval and limited reasoning ability, requiring a more effective approach for handling complex queries.

Method: Uses cooperative self-play framework where a single LLM alternates between decomposer (breaks down queries) and solver (integrates contexts) roles, combining supervised fine-tuning on diverse tasks with reinforcement fine-tuning optimized for answer accuracy.

Result: Outperforms state-of-the-art baselines with 7.6% average exact match improvement, matches DeepSeek-V3 performance with <5% parameters on finance tasks, and surpasses larger models with up to 9x more parameters.

Conclusion: AceSearcher demonstrates exceptional efficiency and effectiveness in tackling complex reasoning tasks, providing a powerful solution for search-augmented LLMs without requiring intermediate annotations.

Abstract: Search-augmented LLMs often struggle with complex reasoning tasks due to
ineffective multi-hop retrieval and limited reasoning ability. We propose
AceSearcher, a cooperative self-play framework that trains a single large
language model (LLM) to alternate between two roles: a decomposer that breaks
down complex queries and a solver that integrates retrieved contexts for answer
generation. AceSearcher couples supervised fine-tuning on a diverse mixture of
search, reasoning, and decomposition tasks with reinforcement fine-tuning
optimized for final answer accuracy, eliminating the need for intermediate
annotations. Extensive experiments on three reasoning-intensive tasks across 10
datasets show that AceSearcher outperforms state-of-the-art baselines,
achieving an average exact match improvement of 7.6%. Remarkably, on
document-level finance reasoning tasks, AceSearcher-32B matches the performance
of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller
scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented
LLMs with up to 9x more parameters, highlighting its exceptional efficiency and
effectiveness in tackling complex reasoning tasks. Our code will be published
at https://github.com/ritaranx/AceSearcher and
https://huggingface.co/AceSearcher.

</details>


### [121] [Can Large Language Models Express Uncertainty Like Human?](https://arxiv.org/abs/2509.24202)
*Linwei Tao,Yi-Fan Yeh,Bo Kai,Minjing Dong,Tao Huang,Tom A. Lamb,Jialin Yu,Philip H. S. Torr,Chang Xu*

Main category: cs.CL

TL;DR: This paper proposes linguistic confidence (LC) as a lightweight alternative to traditional confidence estimation methods for LLMs, using hedging language instead of numerical scores. The authors create a dataset of hedging expressions, develop a mapper to convert hedges to confidence scores, systematically study LC across LLMs, and introduce a fine-tuning framework to improve LC reliability.


<details>
  <summary>Details</summary>
Motivation: Existing confidence estimation methods for LLMs face practical barriers: hidden logits, computational expense of multi-sampling, and unnatural verbalized numerical uncertainty. Linguistic confidence through hedging language offers a lightweight, human-centered alternative.

Method: 1) Release first large-scale dataset of hedging expressions with human-annotated confidence scores; 2) Propose lightweight mapper to convert hedges to confidence scores; 3) Conduct systematic study of LC across modern LLMs and QA benchmarks; 4) Introduce fine-tuning framework to improve LC reliability.

Result: While most LLMs underperform in expressing reliable linguistic confidence, carefully designed prompting achieves competitive calibration and discriminability. The fine-tuning framework further improves LC reliability.

Conclusion: Linguistic confidence is positioned as a scalable, efficient, and human-aligned approach to LLM uncertainty estimation, calling for deeper exploration of this promising direction.

Abstract: Large language models (LLMs) are increasingly used in high-stakes settings,
where overconfident responses can mislead users. Reliable confidence estimation
has been shown to enhance trust and task accuracy. Yet existing methods face
practical barriers: logits are often hidden, multi-sampling is computationally
expensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score)
deviates from natural communication. We revisit linguistic confidence (LC),
where models express uncertainty through hedging language (e.g., probably,
might), offering a lightweight and human-centered alternative. To advance this
direction, we (1) release the first diverse, large-scale dataset of hedging
expressions with human-annotated confidence scores, and (2) propose a
lightweight mapper that converts hedges into confidence scores at near-zero
cost. Building on these resources, we (3) conduct the first systematic study of
LC across modern LLMs and QA benchmarks, revealing that while most LLMs
underperform in expressing reliable LC, carefully designed prompting achieves
competitive calibration and discriminability. Finally, we (4) introduce a
fine-tuning framework that further improves LC reliability. Taken together, our
work positions linguistic confidence as a scalable, efficient, and
human-aligned approach to LLM uncertainty estimation, and calls for deeper
exploration of this promising yet underexplored direction.

</details>


### [122] [BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models](https://arxiv.org/abs/2509.24210)
*Gaurav Srivastava,Aafiya Hussain,Zhenyu Bi,Swastik Roy,Priya Pitre,Meng Lu,Morteza Ziyadi,Xuan Wang*

Main category: cs.CL

TL;DR: BeyondBench is an evaluation framework that uses algorithmic problem generation to avoid benchmark contamination, testing 101 language models on 44 algorithmic tasks across three difficulty levels and revealing consistent reasoning deficiencies as complexity increases.


<details>
  <summary>Details</summary>
Motivation: Traditional benchmarks risk contamination from internet training data, making it unclear if models are reasoning or just recalling answers. This paper aims to create uncontaminated evaluations that truly test reasoning capabilities.

Method: The framework generates algorithmic problems on-the-fly from a combinatorial space larger than 10^15 unique instances, covering 44 tasks with 117 variations across Easy (basic arithmetic), Medium (sequence patterns), and Hard (NP-complete problems) suites. Solutions are verified deterministically by mathematical proofs.

Result: Evaluation of 101 models (85 open-source, 16 closed-source) showed consistent reasoning deficiencies across model families. Performance degraded sharply with complexity: Gemini-2.5-pro (56.38%), Llama-3.3-70B (26.91%), Qwen2.5-72B (33.60%) on Hard Suite. Performance dropped drastically without tool usage (GPT-5: -16.81%, GPT-5-mini: -28.05%, GPT-5-nano: -47.59%).

Conclusion: BeyondBench provides a contamination-free evaluation framework that reveals significant reasoning limitations in current language models, especially as problem complexity increases from polynomial to exponential, highlighting the gap between memorization and true reasoning capabilities.

Abstract: Evaluating language models fairly is becoming harder as static benchmarks
available on the internet risk contamination by training data. This makes it
unclear whether models are truly reasoning or just recalling answers. In this
paper, we introduce BeyondBench, an evaluation framework that avoids this
problem by using algorithmic problem generation. Unlike traditional benchmarks
that risk contamination from internet-scale training data, BeyondBench creates
mathematically grounded problems on the fly, ensuring each test remains fresh
and uncontaminated. Our framework covers 44 algorithmic tasks with a total of
117 variations, grouped into three difficulty levels: the Easy Suite (29 tasks)
for basic arithmetic and statistics, the Medium Suite (5 tasks, 49 variations)
for sequence patterns and reasoning, and the Hard Suite (10 tasks, 68
variations) tackling NP-complete and constraint satisfaction problems. Each
task generates problems from a combinatorial space larger than 10^15 unique
instances, with solutions verified deterministically by mathematical proofs. We
evaluated 101 language models, including 85 open-source and 16 closed-source
models, spanning sizes from 0.5B to 141B parameters and multiple quantization
schemes. Our results show consistent reasoning deficiencies across model
families, with performance degrading sharply as problem complexity increases
from polynomial to exponential. In our Hard Suite evaluations, models such as
Gemini-2.5-pro, Llama-3.3-70B, and Qwen2.5-72B achieved average accuracies of
56.38%, 26.91%, and 33.60%, respectively. Moreover, we observe that performance
drops drastically without tool usage, with GPT-5, GPT-5-mini, and GPT-5-nano
showing a decline of 16.81%, 28.05%, and 47.59% accuracy on the hard suite. Our
leaderboard is publicly available at https://ctrl-gaurav.github.io/BeyondBench/

</details>


### [123] [ScenarioBench: Trace-Grounded Compliance Evaluation for Text-to-SQL and RAG](https://arxiv.org/abs/2509.24212)
*Zahra Atf,Peter R Lewis*

Main category: cs.CL

TL;DR: ScenarioBench is a benchmark for evaluating Text-to-SQL and RAG systems in compliance contexts, requiring systems to justify decisions using specific policy clauses and providing comprehensive evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Existing Text-to-SQL and RAG benchmarks lack proper grounding in compliance contexts and don't require systems to provide falsifiable, clause-level justifications for their decisions.

Method: Uses YAML scenarios with gold-standard packages containing expected decisions, witness traces, governing clauses, and canonical SQL. Systems must justify outputs using specific clause IDs from the policy canon.

Result: Provides comprehensive evaluation including decision accuracy, trace quality, retrieval effectiveness, SQL correctness, policy coverage, latency, and explanation-hallucination rate with normalized difficulty indices.

Conclusion: ScenarioBench shifts evaluation focus toward justification quality under explicit time constraints, addressing limitations of prior benchmarks by enforcing strict grounding and no-peek rules for compliance applications.

Abstract: ScenarioBench is a policy-grounded, trace-aware benchmark for evaluating
Text-to-SQL and retrieval-augmented generation in compliance contexts. Each
YAML scenario includes a no-peek gold-standard package with the expected
decision, a minimal witness trace, the governing clause set, and the canonical
SQL, enabling end-to-end scoring of both what a system decides and why. Systems
must justify outputs using clause IDs from the same policy canon, making
explanations falsifiable and audit-ready. The evaluator reports decision
accuracy, trace quality (completeness, correctness, order), retrieval
effectiveness, SQL correctness via result-set equivalence, policy coverage,
latency, and an explanation-hallucination rate. A normalized Scenario
Difficulty Index (SDI) and a budgeted variant (SDI-R) aggregate results while
accounting for retrieval difficulty and time. Compared with prior Text-to-SQL
or KILT/RAG benchmarks, ScenarioBench ties each decision to clause-level
evidence under strict grounding and no-peek rules, shifting gains toward
justification quality under explicit time budgets.

</details>


### [124] [MoVa: Towards Generalizable Classification of Human Morals and Values](https://arxiv.org/abs/2509.24216)
*Ziyu Chen,Junfei Sun,Chenxi Li,Tuan Dung Nguyen,Jing Yao,Xiaoyuan Yi,Xing Xie,Chenhao Tan,Lexing Xie*

Main category: cs.CL

TL;DR: MoVa is a comprehensive resource suite for classifying human morals and values, featuring 16 labeled datasets, a lightweight LLM prompting strategy, and tools for evaluating psychological surveys.


<details>
  <summary>Details</summary>
Motivation: Researchers face difficulties navigating diverse theoretical frameworks and data for analyzing human morals and values in language, necessitating standardized tools.

Method: Developed MoVa with 16 labeled datasets from four frameworks, a lightweight LLM prompting strategy (all@once), and survey evaluation tools.

Result: The all@once prompting strategy outperforms fine-tuned models across domains and frameworks, enabling generalizable moral/value classification.

Conclusion: MoVa facilitates fine-grained interpretation of human and machine communication, with implications for machine behavior alignment.

Abstract: Identifying human morals and values embedded in language is essential to
empirical studies of communication. However, researchers often face substantial
difficulty navigating the diversity of theoretical frameworks and data
available for their analysis. Here, we contribute MoVa, a well-documented suite
of resources for generalizable classification of human morals and values,
consisting of (1) 16 labeled datasets and benchmarking results from four
theoretically-grounded frameworks; (2) a lightweight LLM prompting strategy
that outperforms fine-tuned models across multiple domains and frameworks; and
(3) a new application that helps evaluate psychological surveys. In practice,
we specifically recommend a classification strategy, all@once, that scores all
related concepts simultaneously, resembling the well-known multi-label
classifier chain. The data and methods in MoVa can facilitate many fine-grained
interpretations of human and machine communication, with potential implications
for the alignment of machine behavior.

</details>


### [125] [Model Fusion with Multi-LoRA Inference for Tool-Enhanced Game Dialogue Agents](https://arxiv.org/abs/2509.24229)
*Kangxu Wang,Ze Chen,Chengcheng Wei,Jiewen Zheng,Jiarong He,Max Gao*

Main category: cs.CL

TL;DR: The opdainlp team won first place in Tasks 1 and 3, and second place in Task 2 of the CPDC 2025 GPU track by using Qwen3-14B with LoRA fine-tuning and model fusion, employing three specialized LoRA adapters for different functions.


<details>
  <summary>Details</summary>
Motivation: To build an in-game conversational AI that adheres to character personas, aligns with the game's worldview, and supports function calling, while considering effectiveness and resource/time constraints during inference.

Method: Used Qwen3-14B with LoRA fine-tuning and model fusion; synthesized data for some tasks; employed three distinct LoRA adapters for tool calling, response generation with tool call results, and response generation without tool call results; implemented MultiLoRA inference using vLLM.

Result: Achieved first place in Task 1 and Task 3, and second place in Task 2 of the GPU track.

Conclusion: The approach of using specialized LoRA adapters for different functions with Qwen3-14B and model fusion was effective for building a competitive in-game conversational AI system.

Abstract: This paper presents the opdainlp team's solution for the GPU track of the
CPDC 2025 challenge. The challenge consists of three tasks, aiming to build an
in-game conversational AI that adheres to character personas, aligns with the
game's worldview, and supports function calling. Considering both effectiveness
and resource/time constraints during inference, we synthesized data for some of
the tasks based on the datasets provided by the competition organizers. We
employed Qwen3-14B with LoRA fine-tuning and model fusion, and utilized a base
model integrated with multiple LoRA adapters during inference. Specifically, in
the competition, we used three distinct LoRA adapters to handle tool calling,
response generation with tool call results, and response generation without
tool call results, respectively. MultiLoRA inference was implemented using
vLLM. Our solution achieved the first place in Task 1 and Task 3, and the
second place in Task 2 of the GPU track.

</details>


### [126] [Prompt and Parameter Co-Optimization for Large Language Models](https://arxiv.org/abs/2509.24245)
*Xiaohe Bo,Rui Li,Zexu Sun,Quanyu Dai,Zeyu Zhang,Zihang Tian,Xu Chen,Zhenhua Dong*

Main category: cs.CL

TL;DR: MetaTuner is a framework that jointly optimizes prompt learning and fine-tuning for LLMs, enabling synergistic improvement through shared knowledge encoding and supervised regularization.


<details>
  <summary>Details</summary>
Motivation: Previous work studied prompt optimization and fine-tuning in isolation, leaving their synergistic potential underexplored despite their complementary approaches to enhancing LLM performance.

Method: Introduces two neural networks for prompt generation and parameter updates with shared bottom encoding layer, using supervised regularization loss to bridge discrete prompt learning and continuous fine-tuning optimization.

Result: Extensive experiments across diverse benchmarks show consistent performance improvements over baseline methods.

Conclusion: Joint integration of prompt optimization and fine-tuning through MetaTuner framework effectively discovers optimal prompt-parameter combinations, demonstrating superior performance compared to isolated approaches.

Abstract: Prompt optimization and fine-tuning are two major approaches to improve the
performance of Large Language Models (LLMs). They enhance the capabilities of
LLMs from complementary perspectives: the former through explicit natural
language, and the latter through implicit parameter updates. However, prior
work has typically studied them in isolation, leaving their synergistic
potential largely underexplored. To bridge this gap, in this paper, we
introduce MetaTuner, a novel framework that jointly integrates prompt
optimization and fine-tuning for LLM training. Specifically, we introduce two
neural networks to generate prompts and parameters, respectively, while
allowing them to share a common bottom encoding layer to enable knowledge
sharing. By the guidance of the final supervised signals, our framework is
optimized to discover the optimal combinations between the prompts and
parameters. Given that prompt learning involves discrete optimization while
fine-tuning operates in a continuous parameter space, we design a supervised
regularization loss to train our framework effectively. Extensive experiments
across diverse benchmarks show that our method consistently outperforms the
baselines.

</details>


### [127] [MRAG-Suite: A Diagnostic Evaluation Platform for Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2509.24253)
*Yuelyu Ji*

Main category: cs.CL

TL;DR: MRAG-Suite is a diagnostic evaluation platform for Visual RAG systems that addresses query difficulty and ambiguity issues, using multimodal benchmarks and diagnostic tools to identify hallucinations.


<details>
  <summary>Details</summary>
Motivation: Current multimodal RAG evaluations fail to systematically account for query difficulty and ambiguity, leading to incomplete assessment of system performance.

Method: Proposed MRAG-Suite platform with difficulty-based and ambiguity-aware filtering strategies, integrating multiple multimodal benchmarks (WebQA, Chart-RAG, Visual-RAG, MRAG-Bench) and MM-RAGChecker diagnostic tool.

Result: Substantial accuracy reductions observed under difficult and ambiguous queries, revealing prevalent hallucinations in Visual RAG systems.

Conclusion: MM-RAGChecker effectively diagnoses Visual RAG issues and provides guidance for future system improvements in handling query difficulty and ambiguity.

Abstract: Multimodal Retrieval-Augmented Generation (Visual RAG) significantly advances
question answering by integrating visual and textual evidence. Yet, current
evaluations fail to systematically account for query difficulty and ambiguity.
We propose MRAG-Suite, a diagnostic evaluation platform integrating diverse
multimodal benchmarks (WebQA, Chart-RAG, Visual-RAG, MRAG-Bench). We introduce
difficulty-based and ambiguity-aware filtering strategies, alongside
MM-RAGChecker, a claim-level diagnostic tool. Our results demonstrate
substantial accuracy reductions under difficult and ambiguous queries,
highlighting prevalent hallucinations. MM-RAGChecker effectively diagnoses
these issues, guiding future improvements in Visual RAG systems.

</details>


### [128] [SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents](https://arxiv.org/abs/2509.24282)
*Gyuhyeon Seo,Jungwoo Yang,Junseong Pyo,Nalim Kim,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: SimuHome is a time-accelerated smart home simulator built on Matter protocol that provides realistic device interactions and environmental changes, with a benchmark of 600 episodes showing current LLM agents struggle with latent intent inference and temporal scheduling.


<details>
  <summary>Details</summary>
Motivation: To address the lack of realistic simulation environments and challenging benchmarks for developing smart home agents that can handle latent user intents, temporal dependencies, device constraints, and scheduling.

Method: Built SimuHome simulator on Matter protocol with time-acceleration, smart device simulation, API call support, and environmental variable tracking. Created benchmark of 600 episodes across 12 user query types requiring complex capabilities.

Result: Evaluation of 11 agents showed they perform well on simple tasks but struggle with latent intent inference, state verification, and temporal scheduling. Even top-performing GPT-4.1 achieved only 54% success rate.

Conclusion: There is critical need for methods that can reliably verify current state via tools before acting and coordinate time-dependent actions in smart home environments.

Abstract: Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks.
However, smart homes introduce distinct challenges, requiring agents to handle
latent user intents, temporal dependencies, device constraints, scheduling, and
more. The main bottlenecks for developing smart home agents with such
capabilities include the lack of a realistic simulation environment where
agents can interact with devices and observe the results, as well as a
challenging benchmark to evaluate them. To address this, we introduce
$\textbf{SimuHome}$, a time-accelerated home environment that simulates smart
devices, supports API calls, and reflects changes in environmental variables.
By building the simulator on the Matter protocol (the global industry standard
for smart home communication), SimuHome provides a high-fidelity environment,
and agents validated in SimuHome can be deployed on real Matter-compliant
devices with minimal adaptation. We provide a challenging benchmark of 600
episodes across twelve user query types that require the aforementioned
capabilities. Our evaluation of 11 agents under a unified ReAct framework
reveals that while models perform well on simple tasks, they struggle with
latent intent inference, state verification, and especially temporal
scheduling. Even the top-performing model, GPT-4.1, reaches only 54% success
rate. These findings highlight a critical need for methods that can reliably
verify the current state via tools before acting and coordinate time-dependent
actions.

</details>


### [129] [Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement](https://arxiv.org/abs/2509.24291)
*Yu-Che Tsai,Kuan-Yu Chen,Yuan-Chi Li,Yuan-Hao Chen,Ching-Yu Tsai,Shou-De Lin*

Main category: cs.CL

TL;DR: GIRCSE is a novel framework that uses autoregressive generation to iteratively refine sentence embeddings through contrastive learning, outperforming encoder-only LLM-based embeddings on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based embeddings treat LLMs as static feature extractors and overlook their generative capabilities, missing latent concepts and implicit semantics.

Method: Proposes GIRCSE framework with Iterative Contrastive Refinement (ICR) objective, using autoregressive generation to produce sequences of soft tokens optimized under contrastive learning.

Result: Outperforms strong LLM-based embedding baselines on MTEB benchmark and instruction-following tasks, exhibits emergent test-time scaling where generating more tokens improves embedding quality.

Conclusion: Generative iterative refinement establishes a new paradigm for representation learning, effectively leveraging LLMs' generative strengths for semantic representation.

Abstract: Existing large language model (LLM)-based embeddings typically adopt an
encoder-only paradigm, treating LLMs as static feature extractors and
overlooking their core generative strengths. We introduce GIRCSE (Generative
Iterative Refinement for Contrastive Sentence Embeddings), a novel framework
that leverages autoregressive generation to iteratively refine semantic
representations. By producing sequences of soft tokens optimized under
contrastive objective, GIRCSE captures latent concepts and implicit semantics
that encoder-only methods often miss. To guide this process, we propose an
Iterative Contrastive Refinement (ICR) objective that encourages each
refinement step to yield better representations. Extensive experiments show
that GIRCSE outperforms strong LLM-based embedding baselines on the MTEB
benchmark and instruction-following tasks. Moreover, GIRCSE exhibits an
emergent test-time scaling property: generating more tokens at inference
steadily improves embedding quality. Our results establish generative iterative
refinement as a new paradigm for representation learning.

</details>


### [130] [LOGOS: LLM-driven End-to-End Grounded Theory Development and Schema Induction for Qualitative Research](https://arxiv.org/abs/2509.24294)
*Xinyu Pi,Qisen Yang,Chuong Nguyen*

Main category: cs.CL

TL;DR: LOGOS is an end-to-end framework that fully automates grounded theory workflow using LLM-driven coding, semantic clustering, and graph reasoning to transform raw text into structured hierarchical theories.


<details>
  <summary>Details</summary>
Motivation: To overcome the scalability bottleneck of expert-intensive manual coding in grounded theory and democratize qualitative research while maintaining theoretical nuance.

Method: Integrates LLM-driven coding, semantic clustering, graph reasoning, and iterative refinement process to build reusable codebooks. Includes a 5-dimensional metric and train-test split protocol for evaluation.

Result: Consistently outperforms strong baselines across five diverse corpora, achieving 88.2% alignment with expert-developed schema on complex dataset.

Conclusion: LOGOS demonstrates a powerful path to scale qualitative research without sacrificing theoretical nuance, enabling broader accessibility to grounded theory methodology.

Abstract: Grounded theory offers deep insights from qualitative data, but its reliance
on expert-intensive manual coding presents a major scalability bottleneck.
Current computational tools stop short of true automation, keeping researchers
firmly in the loop. We introduce LOGOS, a novel, end-to-end framework that
fully automates the grounded theory workflow, transforming raw text into a
structured, hierarchical theory. LOGOS integrates LLM-driven coding, semantic
clustering, graph reasoning, and a novel iterative refinement process to build
highly reusable codebooks. To ensure fair comparison, we also introduce a
principled 5-dimensional metric and a train-test split protocol for
standardized, unbiased evaluation. Across five diverse corpora, LOGOS
consistently outperforms strong baselines and achieves a remarkable $88.2\%$
alignment with an expert-developed schema on a complex dataset. LOGOS
demonstrates a powerful new path to democratize and scale qualitative research
without sacrificing theoretical nuance.

</details>


### [131] [DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models](https://arxiv.org/abs/2509.24296)
*Zherui Li,Zheng Nie,Zhenhong Zhou,Yufei Guo,Yue Liu,Yitong Zhang,Yu Cheng,Qingsong Wen,Kun Wang,Jiaheng Zhang*

Main category: cs.CL

TL;DR: This paper analyzes vulnerabilities in Diffusion Large Language Models (dLLMs) to jailbreak attacks, identifies key issues like greedy remasking bias and Denoising-path Dependence, and proposes DiffuGuard - a training-free defense framework that reduces attack success rates from 47.9% to 14.7%.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of Diffusion LLMs introduces unique vulnerabilities distinct from Autoregressive LLMs, stemming from their iterative and parallel generation mechanisms. Current decoding strategies create significant security risks that need to be addressed.

Method: The paper conducts vulnerability analysis across intra-step and inter-step dimensions, identifies harmful bias in greedy remasking and Denoising-path Dependence phenomenon. It proposes DiffuGuard with two components: Stochastic Annealing Remasking (introduces controlled randomness) and Block-level Audit and Repair (uses internal representations for risk detection and correction).

Result: Comprehensive experiments on four dLLMs show DiffuGuard reduces Attack Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while preserving model utility and efficiency. The framework demonstrates exceptional effectiveness in mitigating dLLM vulnerabilities.

Conclusion: While current decoding strategies constitute significant vulnerabilities in dLLMs, these models possess substantial intrinsic safety potential. DiffuGuard successfully unlocks this potential through a training-free defense framework that addresses key vulnerabilities without compromising model performance.

Abstract: The rapid advancement of Diffusion Large Language Models (dLLMs) introduces
unprecedented vulnerabilities that are fundamentally distinct from
Autoregressive LLMs, stemming from their iterative and parallel generation
mechanisms. In this paper, we conduct an in-depth analysis of dLLM
vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step
and inter-step dynamics. Experimental results reveal a harmful bias inherent in
the standard greedy remasking strategy and identify a critical phenomenon we
term Denoising-path Dependence, where the safety of early-stage tokens
decisively influences the final output. These findings also indicate that while
current decoding strategies constitute a significant vulnerability, dLLMs
possess a substantial intrinsic safety potential. To unlock this potential, we
propose DiffuGuard, a training-free defense framework that addresses
vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking
dynamically introduces controlled randomness to mitigate greedy selection bias,
while Block-level Audit and Repair exploits internal model representations for
autonomous risk detection and guided correction. Comprehensive experiments on
four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack
Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while
preserving model utility and efficiency. Our code is available at:
https://github.com/niez233/DiffuGuard.

</details>


### [132] [Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs](https://arxiv.org/abs/2509.24297)
*Junying Wang,Zicheng Zhang,Ye Shen,Yalun Wu,Yingji Liang,Yijin Guo,Farong Wen,Wenzhe Li,Xuezhi Zhao,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: This paper presents a framework for transforming Text-Only QA Pairs (TQAs) into Multi-Modal QA Pairs (MMQAs) to address the bottleneck in creating high-quality scientific reasoning benchmarks, and develops an agentic system called Q-Mirror that iteratively refines MMQA generation and evaluation.


<details>
  <summary>Details</summary>
Motivation: Manual creation of high-quality multi-modal benchmarks for scientific reasoning is costly and unscalable, creating a bottleneck for advancing large models.

Method: Developed a TQA-to-MMQA framework with quality rubric, constructed evaluation benchmarks, and created Q-Mirror agent that integrates MMQA generation and evaluation in a closed loop for iterative refinement.

Result: State-of-the-art models can generate MMQAs but with substantial gaps, while top-tier understanding models align well with human judgment in quality assessment. Q-Mirror agent improved average scores from 78.90 to 85.22 and pass rates from 72% to 95%.

Conclusion: The proposed framework and Q-Mirror agent offer a practical path to creating large-scale scientific benchmarks by automating the transformation of text-only QA pairs into high-quality multi-modal QA pairs through iterative refinement.

Abstract: High-quality, multi-modal benchmarks are crucial for advancing scientific
reasoning in large models yet their manual creation is costly and unscalable.
To address this bottleneck, we explore the potential for transforming Text-Only
QA Pairs (TQAs) into high-quality Multi-Modal QA Pairs (MMQAs), which include
three parts: 1) Task Definition \& Evaluation Rubric: We develop a TQA-to-MMQA
framework and establish a comprehensive, multi-dimensional MMQA quality rubric
that provides principles for the transformation. 2) Benchmark Construction:
Then we construct two extensive benchmarks to rigorously evaluate
state-of-the-art generation \& understanding models on the distinct tasks of
MMQA generation \& MMQA quality evaluation. 3) Preliminary Solution: We develop
an agentic system (Q-Mirror), which operationalizes our framework by
integrating MMQA generation and evaluation into a closed loop for iterative
refinement. Our experiments show that while state-of-the-art models can
generate MMQAs, their outputs still leave substantial gaps, underscoring the
need for reliable evaluation. We further demonstrate that top-tier
understanding models align closely with human judgment in MMQA quality
assessment. Leveraging both insights, the Q-Mirror agent raises average scores
from 78.90 to 85.22 and pass rates from 72\% to 95\%, offering a practical path
to large-scale scientific benchmarks.

</details>


### [133] [Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs](https://arxiv.org/abs/2509.24319)
*Jongwook Han,Jongwon Lim,Injin Kong,Yohan Jo*

Main category: cs.CL

TL;DR: LLMs express values through intrinsic (learned) and prompted (elicited) mechanisms that share some components but also have unique elements, leading to different steerability and response diversity patterns.


<details>
  <summary>Details</summary>
Motivation: To understand whether intrinsic and prompted value expression mechanisms in LLMs mostly overlap or rely on substantially different mechanisms, which is crucial for value alignment and persona steering applications.

Method: Used two mechanistic approaches: (1) value vectors extracted from the residual stream representing value mechanisms, and (2) value neurons (MLP neurons) that contribute to value expressions.

Result: Intrinsic and prompted value mechanisms partly share common components crucial for value expression but also possess unique elements that manifest differently. Prompted mechanisms show higher steerability while intrinsic mechanisms show greater response diversity.

Conclusion: The distinct components of intrinsic mechanisms promote lexical diversity, while prompted mechanism components strengthen instruction following and can affect distant tasks like jailbreaking.

Abstract: Large language models (LLMs) can express different values in two distinct
ways: (1) intrinsic expression, reflecting the model's inherent values learned
during training, and (2) prompted expression, elicited by explicit prompts.
Given their widespread use in value alignment and persona steering, it is
paramount to clearly understand their underlying mechanisms, particularly
whether they mostly overlap (as one might expect) or rely on substantially
different mechanisms, but this remains largely understudied. We analyze this at
the mechanistic level using two approaches: (1) value vectors, feature
directions representing value mechanisms extracted from the residual stream,
and (2) value neurons, MLP neurons that contribute to value expressions. We
demonstrate that intrinsic and prompted value mechanisms partly share common
components that are crucial for inducing value expression, but also possess
unique elements that manifest in different ways. As a result, these mechanisms
lead to different degrees of value steerability (prompted > intrinsic) and
response diversity (intrinsic > prompted). In particular, components unique to
the intrinsic mechanism seem to promote lexical diversity in responses, whereas
those specific to the prompted mechanism primarily strengthen instruction
following, taking effect even in distant tasks like jailbreaking.

</details>


### [134] [Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey](https://arxiv.org/abs/2509.24322)
*Yuntao Shou,Tao Meng,Wei Ai,Keqin Li*

Main category: cs.CL

TL;DR: This paper provides a comprehensive survey of LLMs and MLLMs for emotion recognition and reasoning, covering architectures, datasets, and benchmarks, while highlighting challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: The field lacks a systematic review consolidating recent developments in multimodal emotion recognition and reasoning using LLMs and MLLMs, despite their significant progress.

Method: Conducts a comprehensive survey covering model architectures, datasets, and performance benchmarks for LLMs and MLLMs in emotion recognition and reasoning.

Result: Provides the first comprehensive survey at the intersection of MLLMs with multimodal emotion recognition and reasoning, offering an authoritative reference and practical insights.

Conclusion: This survey addresses a critical gap in the literature and serves as a foundational reference for advancing research in multimodal emotion recognition and reasoning using LLMs and MLLMs.

Abstract: In recent years, large language models (LLMs) have driven major advances in
language understanding, marking a significant step toward artificial general
intelligence (AGI). With increasing demands for higher-level semantics and
cross-modal fusion, multimodal large language models (MLLMs) have emerged,
integrating diverse information sources (e.g., text, vision, and audio) to
enhance modeling and reasoning in complex scenarios. In AI for Science,
multimodal emotion recognition and reasoning has become a rapidly growing
frontier. While LLMs and MLLMs have achieved notable progress in this area, the
field still lacks a systematic review that consolidates recent developments. To
address this gap, this paper provides a comprehensive survey of LLMs and MLLMs
for emotion recognition and reasoning, covering model architectures, datasets,
and performance benchmarks. We further highlight key challenges and outline
future research directions, aiming to offer researchers both an authoritative
reference and practical insights for advancing this domain. To the best of our
knowledge, this paper is the first attempt to comprehensively survey the
intersection of MLLMs with multimodal emotion recognition and reasoning. The
summary of existing methods mentioned is in our Github:
\href{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}.

</details>


### [135] [Speculative Verification: Exploiting Information Gain to Refine Speculative Decoding](https://arxiv.org/abs/2509.24328)
*Sungkyun Kim,Jaemin Kim,Dogyung Yoon,Jiho Shin,Junyeol Lee,Jiwon Seo*

Main category: cs.CL

TL;DR: Speculative Verification (SV) enhances speculative decoding by dynamically predicting speculation accuracy and adapting verification length to maximize throughput, achieving up to 2x speedup over standard speculative decoding.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from low GPU efficiency and high latency due to autoregressive decoding. While speculative decoding helps by using draft models, its effectiveness is limited when speculation accuracy is low, as rejected tokens create overhead that offsets benefits.

Method: Proposes Speculative Verification (SV) that uses a small companion model to estimate alignment between draft and target model distributions. This allows dynamic prediction of speculation accuracy and adaptation of verification length to reduce wasted computation on rejected tokens.

Result: SV consistently outperforms both speculative decoding and standard decoding across all experiments, improving SD performance by up to 2x with average 1.4x speedup in large-batch settings (batch sizes 32-80).

Conclusion: SV demonstrates robustness, scalability, and practical utility for efficient LLM inference, requiring no modifications to draft or target models and being compatible with existing SD variants.

Abstract: LLMs have low GPU efficiency and high latency due to autoregressive decoding.
Speculative decoding (SD) mitigates this using a small draft model to
speculatively generate multiple tokens, which are then verified in parallel by
a target model. However, when speculation accuracy is low, the overhead from
rejected tokens can offset the benefits, limiting SD's effectiveness,
especially at large batch sizes. To address this, we propose Speculative
Verification (SV), an efficient augmentation to SD that dynamically predicts
speculation accuracy and adapts the verification length to maximize throughput.
SV introduces a companion model - a small auxiliary model similar in size to
the draft model - to estimate the alignment between draft and target model
distributions. By maximizing the information gain from quantifying this
alignment, SV refines verification decisions, reducing wasted computation on
rejected tokens and improving decoding efficiency. Moreover, SV requires no
modifications to the draft or target models and is compatible with existing SD
variants. We extensively evaluated SV on publicly available LLMs across three
NLP tasks using nine combinations of draft, companion, and target models,
including 13B-72B target models and three types of variations: base (no
finetuning), instruction-tuned, and task fine-tuned. Across all experiments and
batch sizes (4-80), SV consistently outperforms both SD and standard decoding
with the target model. It improves SD performance by up to 2$\times$, with an
average speedup of 1.4 $\times$ in large-batch settings (batch sizes 32-80).
These results demonstrate SV's robustness, scalability, and practical utility
for efficient LLM inference.

</details>


### [136] [AlignX: Advancing Multilingual Large Language Models with Multilingual Representation Alignment](https://arxiv.org/abs/2509.24338)
*Mengyu Bu,Shaolei Zhang,Zhongjun He,Hua Wu,Yang Feng*

Main category: cs.CL

TL;DR: AlignX is a two-stage framework that improves multilingual LLM performance by aligning representations and fine-tuning instructions.


<details>
  <summary>Details</summary>
Motivation: Multilingual LLMs underperform on non-dominant languages due to imprecise alignment and suboptimal knowledge transfer from standard fine-tuning approaches.

Method: Two-stage approach: 1) Align multilingual representations using semantic alignment and language feature integration, 2) Stimulate multilingual capability via instruction fine-tuning.

Result: Experimental results show enhanced multilingual general and cross-lingual generation capabilities, with representations becoming closer and alignment improving.

Conclusion: AlignX effectively bridges the multilingual performance gap in LLMs through representation-level alignment and instruction fine-tuning.

Abstract: Multilingual large language models (LLMs) possess impressive multilingual
understanding and generation capabilities. However, their performance and
cross-lingual alignment often lag for non-dominant languages. A common solution
is to fine-tune LLMs on large-scale and more balanced multilingual corpus, but
such approaches often lead to imprecise alignment and suboptimal knowledge
transfer, struggling with limited improvements across languages. In this paper,
we propose AlignX to bridge the multilingual performance gap, which is a
two-stage representation-level framework for enhancing multilingual performance
of pre-trained LLMs. In the first stage, we align multilingual representations
with multilingual semantic alignment and language feature integration. In the
second stage, we stimulate the multilingual capability of LLMs via multilingual
instruction fine-tuning. Experimental results on several pre-trained LLMs
demonstrate that our approach enhances LLMs' multilingual general and
cross-lingual generation capability. Further analysis indicates that AlignX
brings the multilingual representations closer and improves the cross-lingual
alignment.

</details>


### [137] [Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining](https://arxiv.org/abs/2509.24356)
*Matthew Theodore Roque,Dan John Velasco*

Main category: cs.CL

TL;DR: Study shows that using simplified text data and complexity-based ordering improves language model pretraining in data-constrained settings, with different optimal strategies for small vs large models.


<details>
  <summary>Details</summary>
Motivation: Most pretraining research focuses on large datasets, leaving gaps in understanding optimization for data-constrained scenarios, particularly regarding training data order and inclusion of simplified text versions.

Method: Built parallel corpora with human-written paragraphs aligned with LLM-simplified variants, tested four data schedules: repeated exposure, low-to-high complexity, high-to-low, and interleaved ordering.

Result: Adding simplified data improves fine-tuning and zero-shot performance over repeated-exposure baseline. Smaller models benefit from low-to-high complexity ordering, while larger models perform better with interleaved ordering.

Conclusion: Curriculum learning with text complexity ordering and data augmentation via simplification effectively enhances representation quality in data-constrained pretraining, with model size determining the optimal data schedule.

Abstract: Most studies on language model pretraining focus on large datasets, leaving
open questions about optimization in data-constrained settings. In such
settings, the effects of training data order and of including alternative
versions of the same text remain underexplored. We address this by studying
curriculum learning in pretraining, focusing on text-complexity ordering and
data augmentation via simplification. We ask: (1) Does simplifying texts
enhance representation quality more than reusing the original data? and (2)
Does ordering data by text complexity yield better representations? To answer,
we build on a pair of parallel corpora where human-written paragraphs are
aligned with LLM-simplified variants, and test four data schedules: repeated
exposure, low-to-high complexity, high-to-low, and interleaved. We analyze
models' representation quality from a sample efficiency perspective via
fine-tuning, as well as its zero-shot performance on linguistic knowledge,
entity tracking, world knowledge, and commonsense reasoning. Our findings show
that adding simplified data improves fine-tuning and zero-shot performance over
a repeated-exposure baseline: smaller models benefit from low-to-high
complexity, while larger models perform better with interleaved ordering.

</details>


### [138] [Reinforcement Mid-Training](https://arxiv.org/abs/2509.24375)
*Yijun Tian,Shaoyu Chen,Zhichao Xu,Yawei Wang,Jinhe Bi,Peng Han,Wei Wang*

Main category: cs.CL

TL;DR: Proposes reinforcement mid-training (RMT) as an intermediate stage between pre-training and post-training for LLMs, addressing challenges of inefficient training, imbalanced token entropy, and underutilized token information through dynamic token budgeting, curriculum-based sampling, and dual training strategy.


<details>
  <summary>Details</summary>
Motivation: Current two-stage LLM development (pre-training + post-training) is insufficient; an intermediate reinforcement mid-training stage can provide significant performance gains by addressing key training inefficiencies.

Method: RMT framework with three components: (1) dynamic token budget mechanism to limit unnecessary reasoning steps, (2) curriculum-based adaptive sampling for progressive learning from easy to hard tokens, (3) dual training strategy combining RL with next-token prediction.

Result: Achieves up to +64.91% performance improvement with only 21% reasoning length in language modeling; checkpoints from mid-training improve subsequent post-training by up to +18.76% in mathematical domain.

Conclusion: Reinforcement mid-training is a valuable intermediate stage that significantly enhances LLM performance and efficiency, with RMT framework effectively addressing key training challenges.

Abstract: The development of state-of-the-art large language models is commonly
understood as a two-stage process involving pre-training and post-training. We
point out the need for an additional intermediate stage called reinforcement
mid-training with potential for strong performance gains. In this paper, we
formally define the problem and identify three key challenges: (1) inefficient
training due to excessive reasoning steps, (2) disregard of the imbalanced
token entropy distribution, and (3) underutilization of token information. To
address these challenges, we propose RMT, a framework for efficient, adaptive,
and unified reinforcement mid-training with various innovative components. In
particular, we first introduce a dynamic token budget mechanism that constrains
unnecessary reasoning steps and mitigates model overthinking. Next, we design a
curriculum-based adaptive sampling method that fosters a progressive learning
trajectory from easy to hard tokens. Finally, we present a dual training
strategy that combines reinforcement learning with next-token prediction,
ensuring targeted learning on key tokens and full exploitation of all token
information. Extensive experiments demonstrate the superiority of RMT over
state-of-the-art methods, achieving up to +64.91% performance improvement with
only 21% of the reasoning length in language modeling. We also show that
checkpoints obtained after reinforcement mid-training can benefit the
subsequent post-training, yielding up to +18.76% improvement in the
mathematical domain.

</details>


### [139] [HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment](https://arxiv.org/abs/2509.24384)
*Langqi Yang,Tianhang Zheng,Kedong Xiu,Yixuan Chen,Di Wang,Puning Zhao,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: HarmMetric Eval is a comprehensive benchmark for evaluating harmfulness metrics and judges in LLM jailbreak scenarios, revealing that traditional metrics like METEOR and ROUGE-1 outperform LLM-based judges.


<details>
  <summary>Details</summary>
Motivation: The proliferation of jailbreak attacks against LLMs has led to diverse metrics for assessing harmfulness, but the absence of systematic benchmarks undermines the credibility of reported jailbreak effectiveness and risks.

Method: Developed HarmMetric Eval benchmark with high-quality dataset of harmful prompts paired with diverse model responses, and a flexible scoring mechanism compatible with various metrics and judges.

Result: Extensive experiments showed that conventional metrics METEOR and ROUGE-1 outperform LLM-based judges in evaluating harmfulness of model responses, challenging prevailing beliefs about LLMs' superiority.

Conclusion: HarmMetric Eval provides a systematic framework for evaluating harmfulness metrics, revealing unexpected superiority of traditional metrics over LLM-based approaches in this domain.

Abstract: The alignment of large language models (LLMs) with human values is critical
for their safe deployment, yet jailbreak attacks can subvert this alignment to
elicit harmful outputs from LLMs. In recent years, a proliferation of jailbreak
attacks has emerged, accompanied by diverse metrics and judges to assess the
harmfulness of the LLM outputs. However, the absence of a systematic benchmark
to assess the quality and effectiveness of these metrics and judges undermines
the credibility of the reported jailbreak effectiveness and other risks. To
address this gap, we introduce HarmMetric Eval, a comprehensive benchmark
designed to support both overall and fine-grained evaluation of harmfulness
metrics and judges. Our benchmark includes a high-quality dataset of
representative harmful prompts paired with diverse harmful and non-harmful
model responses, alongside a flexible scoring mechanism compatible with various
metrics and judges. With HarmMetric Eval, our extensive experiments uncover a
surprising result: two conventional metrics--METEOR and ROUGE-1--outperform
LLM-based judges in evaluating the harmfulness of model responses, challenging
prevailing beliefs about LLMs' superiority in this domain. Our dataset is
publicly available at https://huggingface.co/datasets/qusgo/HarmMetric_Eval,
and the code is available at
https://anonymous.4open.science/r/HarmMetric-Eval-4CBE.

</details>


### [140] [LLaDA-MoE: A Sparse MoE Diffusion Language Model](https://arxiv.org/abs/2509.24389)
*Fengqi Zhu,Zebin You,Yipeng Xing,Zenan Huang,Lin Liu,Yihong Zhuang,Guoshan Lu,Kangyu Wang,Xudong Wang,Lanning Wei,Hongrui Guo,Jiaqi Hu,Wentao Ye,Tieyuan Chen,Chenchen Li,Chengfu Tang,Haibo Feng,Jun Hu,Jun Zhou,Xiaolu Zhang,Zhenzhong Lan,Junbo Zhao,Da Zheng,Chongxuan Li,Jianguo Li,Ji-Rong Wen*

Main category: cs.CL

TL;DR: LLaDA-MoE is a large language diffusion model with Mixture-of-Experts architecture trained on 20T tokens, achieving competitive performance with only 1.4B active parameters during inference while maintaining 7B total capacity.


<details>
  <summary>Details</summary>
Motivation: To develop a more computationally efficient diffusion language model by integrating sparse Mixture-of-Experts architecture, reducing inference costs while maintaining competitive performance.

Method: Training from scratch on 20T tokens using masked diffusion language modeling with sparse MoE architecture, maintaining 7B total parameters but activating only 1.4B parameters during inference.

Result: Achieves state-of-the-art performance among diffusion language models, surpassing LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The instruct-tuned model performs comparably to Qwen2.5-3B-Instruct in various tasks despite using fewer active parameters.

Conclusion: Integrating sparse MoE architecture into diffusion language models successfully brings out MoE's strengths for efficient inference with few active parameters, opening new possibilities for further exploration of diffusion language models.

Abstract: We introduce LLaDA-MoE, a large language diffusion model with the
Mixture-of-Experts (MoE) architecture, trained from scratch on approximately
20T tokens. LLaDA-MoE achieves competitive performance with significantly
reduced computational overhead by maintaining a 7B-parameter capacity while
activating only 1.4B parameters during inference. Our empirical evaluation
reveals that LLaDA-MoE achieves state-of-the-art performance among diffusion
language models with larger parameters, surpassing previous diffusion language
models LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The
instruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities
comparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation,
mathematical reasoning, agent and alignment tasks, despite using fewer active
parameters. Our results show that integrating a sparse MoE architecture into
the training objective of masked diffusion language models still brings out
MoE's strengths under efficient inference with few active parameters, and opens
ample room for further exploration of diffusion language models. LLaDA-MoE
models are available at Huggingface.

</details>


### [141] [Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling](https://arxiv.org/abs/2509.24403)
*Pengfei Wang,Baolin Sun,Xuemei Dong,Yaxun Dai,Hongwei Yuan,Mengdie Chu,Yingqi Gao,Xiang Qi,Peng Zhang,Ying Yan*

Main category: cs.CL

TL;DR: Agentar-Scale-SQL is a novel Text-to-SQL framework that uses orchestrated test-time scaling with three synergistic approaches to achieve state-of-the-art performance on the BIRD benchmark.


<details>
  <summary>Details</summary>
Motivation: Current Text-to-SQL methods significantly lag behind human experts on challenging benchmarks like BIRD, and existing test-time scaling approaches lack orchestration and neglect the model's internal reasoning process.

Method: The framework implements Orchestrated Test-Time Scaling strategy combining: i) Internal Scaling via RL-enhanced Intrinsic Reasoning, ii) Sequential Scaling through Iterative Refinement, and iii) Parallel Scaling using Diverse Synthesis and Tournament Selection.

Result: Agentar-Scale-SQL achieves SOTA performance on BIRD benchmark with 81.67% execution accuracy on test set and ranks first on the official leaderboard.

Conclusion: The framework demonstrates an effective path toward human-level performance and is designed as a general-purpose solution adaptable to new databases and more powerful language models.

Abstract: State-of-the-art (SOTA) Text-to-SQL methods still lag significantly behind
human experts on challenging benchmarks like BIRD. Current approaches that
explore test-time scaling lack an orchestrated strategy and neglect the model's
internal reasoning process. To bridge this gap, we introduce Agentar-Scale-SQL,
a novel framework leveraging scalable computation to improve performance.
Agentar-Scale-SQL implements an Orchestrated Test-Time Scaling strategy that
synergistically combines three distinct perspectives: i) Internal Scaling via
RL-enhanced Intrinsic Reasoning, ii) Sequential Scaling through Iterative
Refinement, and iii) Parallel Scaling using Diverse Synthesis and Tournament
Selection. Agentar-Scale-SQL is a general-purpose framework designed for easy
adaptation to new databases and more powerful language models. Extensive
experiments show that Agentar-Scale-SQL achieves SOTA performance on the BIRD
benchmark, reaching 81.67\% execution accuracy on the test set and ranking
first on the official leaderboard, demonstrating an effective path toward
human-level performance.

</details>


### [142] [Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents](https://arxiv.org/abs/2509.24405)
*Khanh Trinh Pham,Thu Huong Nguyen,Jun Jo,Quoc Viet Hung Nguyen,Thanh Tam Nguyen*

Main category: cs.CL

TL;DR: MultiSpider 2.0 extends Spider 2.0 to 8 languages, revealing a large multilingual gap in Text-to-SQL performance where state-of-the-art LLMs achieve only 4% accuracy without external reasoning.


<details>
  <summary>Details</summary>
Motivation: Most Text-to-SQL benchmarks are English-only, limiting progress in multilingual database access. There's a need for benchmarks that test linguistic and dialectal variability across languages.

Method: Extended Spider 2.0 to 8 languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese) while preserving structural difficulty. Tested state-of-the-art LLMs and developed collaboration-driven language agents that iteratively refine queries.

Result: LLMs like DeepSeek-R1 and OpenAI o1 achieved only 4% execution accuracy using intrinsic reasoning, compared to 60% on MultiSpider 1.0. Collaboration-driven language agents improved accuracy to 15%.

Conclusion: There's a substantial multilingual gap in Text-to-SQL performance. Methods need to be robust across languages for real-world enterprise deployment. The benchmark motivates development of more language-agnostic approaches.

Abstract: Text-to-SQL enables natural access to databases, yet most benchmarks are
English-only, limiting multilingual progress. We introduce MultiSpider 2.0,
extending Spider 2.0 to eight languages (English, German, French, Spanish,
Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's
structural difficulty while adding linguistic and dialectal variability,
demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art
LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\% execution accuracy when
relying on intrinsic reasoning, versus 60\% on MultiSpider 1.0. Therefore, we
provide a collaboration-driven language agents baseline that iteratively
refines queries, improving accuracy to 15\%. These results reveal a substantial
multilingual gap and motivate methods that are robust across languages and
ready for real-world enterprise deployment. Our benchmark is available at
https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.

</details>


### [143] [CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task](https://arxiv.org/abs/2509.24422)
*Haosi Mo,Xinyu Ma,Xuebo Liu,Derek F. Wong,Yu Li,Jie Liu,Min Zhang*

Main category: cs.CL

TL;DR: The paper proposes the Cognition-Domain-Task (CDT) framework to comprehensively evaluate LLM capabilities across three dimensions, addressing limitations of existing benchmarks that focus on isolated abilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs often focus on isolated abilities and lack a holistic framework for comprehensive capability assessment, creating a gap in evaluation methodologies.

Method: The CDT framework measures model capabilities across three dimensions, incorporating Cattell-Horn-Carroll cognitive theory to refine capability categorization. It's applied for dataset capability evaluation and data selection.

Result: Experiments show capability metrics correlate well with downstream performance, supporting effective dataset analysis. Data selection experiments achieved scores of 44.3 and 45.4, with 1.6 and 2.2 point improvements over baselines respectively.

Conclusion: The results validate the effectiveness and practicality of the CDT framework for comprehensive LLM capability assessment and data selection.

Abstract: Recent advances in Large Language Models (LLMs) have significantly enhanced
their capabilities, highlighting the need for comprehensive evaluation
frameworks that extend beyond task-specific benchmarks. However, existing
benchmarks often focus on isolated abilities, lacking a holistic framework for
assessing LLM capabilities. To address this gap, we propose the
Cognition-Domain-Task (CDT) framework, which comprehensively measures a model's
capabilities across three dimensions. We expand the scope of model capability
definitions at the cognitive level by incorporating the Cattell-Horn-Carroll
cognitive theory, refining the categorization of model capabilities. We apply
CDT in two directions: dataset capability evaluation and data selection.
Experiments show that our capability metrics correlate well with downstream
performance and can support effective dataset analysis and construction. The
experiments on data selection also show significant improvements in both
general and specific benchmarks, achieving scores of 44.3 and 45.4, with an
increase of 1.6 and 2.2 points over the baselines, respectively. These results
validate the effectiveness and practicality of CDT. Source code and models are
available at https://github.com/Alessa-mo/CDT.

</details>


### [144] [Alternatives To Next Token Prediction In Text Generation -- A Survey](https://arxiv.org/abs/2509.24435)
*Charlie Wyatt,Aditya Joshi,Flora Salim*

Main category: cs.CL

TL;DR: This survey categorizes alternatives to Next Token Prediction (NTP) in LLMs into five families to address NTP's weaknesses like poor planning and error accumulation.


<details>
  <summary>Details</summary>
Motivation: NTP drives LLM success but causes persistent weaknesses including poor long-term planning, error accumulation, and computational inefficiency.

Method: Categorizes NTP alternatives into five families: Multi-Token Prediction, Plan-then-Generate, Latent Reasoning, Continuous Generation Approaches, and Non-Transformer Architectures.

Result: Provides a comprehensive taxonomy of emerging alternatives to NTP that address token-level generation limitations.

Conclusion: The survey offers a structured framework to guide research into transformative NLP models that overcome NTP's known limitations.

Abstract: The paradigm of Next Token Prediction (NTP) has driven the unprecedented
success of Large Language Models (LLMs), but is also the source of their most
persistent weaknesses such as poor long-term planning, error accumulation, and
computational inefficiency. Acknowledging the growing interest in exploring
alternatives to NTP, the survey describes the emerging ecosystem of
alternatives to NTP. We categorise these approaches into five main families:
(1) Multi-Token Prediction, which targets a block of future tokens instead of a
single one; (2) Plan-then-Generate, where a global, high-level plan is created
upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the
autoregressive process itself into a continuous latent space; (4) Continuous
Generation Approaches, which replace sequential generation with iterative,
parallel refinement through diffusion, flow matching, or energy-based methods;
and (5) Non-Transformer Architectures, which sidestep NTP through their
inherent model structure. By synthesizing insights across these methods, this
survey offers a taxonomy to guide research into models that address the known
limitations of token-level generation to develop new transformative models for
natural language processing.

</details>


### [145] [Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset](https://arxiv.org/abs/2509.24468)
*Taisei Yamamoto,Ryoma Kumon,Danushka Bollegala,Hitomi Yanaka*

Main category: cs.CL

TL;DR: SOBACO is a Japanese benchmark evaluating social biases and cultural commonsense in LLMs. Debiasing methods degrade cultural commonsense performance by up to 75%, highlighting the need for methods that balance fairness and utility.


<details>
  <summary>Details</summary>
Motivation: Existing debiasing methods may degrade LLM capabilities, but their impact on cultural commonsense (closely related to social biases) hasn't been well studied. Cultural commonsense and social biases both stem from social norms and values.

Method: Proposed SOBACO benchmark for Japanese LLMs to evaluate social biases and cultural commonsense in unified format. Evaluated several LLMs to examine debiasing impact on cultural commonsense.

Result: Debiasing methods significantly degrade LLM performance on cultural commonsense tasks, with up to 75% accuracy deterioration.

Conclusion: There's a critical trade-off between bias mitigation and cultural commonsense preservation. Need debiasing methods that consider this balance to improve both fairness and utility of LLMs.

Abstract: Large language models (LLMs) exhibit social biases, prompting the development
of various debiasing methods. However, debiasing methods may degrade the
capabilities of LLMs. Previous research has evaluated the impact of bias
mitigation primarily through tasks measuring general language understanding,
which are often unrelated to social biases. In contrast, cultural commonsense
is closely related to social biases, as both are rooted in social norms and
values. The impact of bias mitigation on cultural commonsense in LLMs has not
been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs
and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate
social biases and cultural commonsense in LLMs in a unified format. We evaluate
several LLMs on SOBACO to examine how debiasing methods affect cultural
commonsense in LLMs. Our results reveal that the debiasing methods degrade the
performance of the LLMs on the cultural commonsense task (up to 75% accuracy
deterioration). These results highlight the importance of developing debiasing
methods that consider the trade-off with cultural commonsense to improve
fairness and utility of LLMs.

</details>


### [146] [A Text-To-Text Alignment Algorithm for Better Evaluation of Modern Speech Recognition Systems](https://arxiv.org/abs/2509.24478)
*Lasse Borgholt,Jakob Havtorn,Christian Igel,Lars Maaløe,Zheng-Hua Tan*

Main category: cs.CL

TL;DR: Proposes a novel alignment algorithm combining dynamic programming with beam search scoring to enable finer-grained error analysis in speech recognition by accurately aligning individual errors between reference and model transcripts.


<details>
  <summary>Details</summary>
Motivation: Current speech recognition evaluation metrics like word error rate are dominated by frequent words with limited semantic weight, obscuring meaningful differences in rare terms, named entities, and domain-specific vocabulary errors that are more consequential.

Method: Developed a novel alignment algorithm that couples dynamic programming with beam search scoring to provide more accurate alignment of individual errors compared to traditional text alignment methods.

Result: The proposed approach enables reliable error analysis by providing more accurate alignment of individual errors between reference and model transcripts.

Conclusion: The algorithm addresses the need for finer-grained error analysis in speech recognition systems and is made available via PyPI for practical use.

Abstract: Modern neural networks have greatly improved performance across speech
recognition benchmarks. However, gains are often driven by frequent words with
limited semantic weight, which can obscure meaningful differences in word error
rate, the primary evaluation metric. Errors in rare terms, named entities, and
domain-specific vocabulary are more consequential, but remain hidden by
aggregate metrics. This highlights the need for finer-grained error analysis,
which depends on accurate alignment between reference and model transcripts.
However, conventional alignment methods are not designed for such precision. We
propose a novel alignment algorithm that couples dynamic programming with beam
search scoring. Compared to traditional text alignment methods, our approach
provides more accurate alignment of individual errors, enabling reliable error
analysis. The algorithm is made available via PyPI.

</details>


### [147] [GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training](https://arxiv.org/abs/2509.24494)
*Hongcheng Wang,Yinuo Huang,Sukai Wang,Guanghui Ren,Hao Dong*

Main category: cs.CL

TL;DR: GRPO-MA improves GRPO algorithm by generating multiple answers per thought to address gradient coupling, sparse rewards, and unstable advantage estimation in CoT reasoning training.


<details>
  <summary>Details</summary>
Motivation: To overcome three key challenges in GRPO algorithm: gradient coupling between thoughts and answers, sparse reward signals due to limited parallel sampling, and unstable advantage estimation.

Method: Propose GRPO-MA which leverages multi-answer generation from each thought process, theoretically reducing variance of thought advantage as number of answers per thought increases.

Result: Empirical gradient analysis shows GRPO-MA reduces gradient spikes; experiments on math, code, and multimodal tasks demonstrate substantial performance and training efficiency improvements; ablation studies confirm increasing answers per thought consistently enhances performance.

Conclusion: GRPO-MA is a simple yet theoretically grounded method that enables more robust and efficient optimization for training CoT reasoning in LLMs and VLMs.

Abstract: Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a
Reinforcement Learning (RL) approach, can effectively train Chain-of-Thought
(CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models
(VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling
between thoughts and answers, sparse reward signals caused by limited parallel
sampling, and unstable advantage estimation. To mitigate these challenges, we
propose GRPO-MA, a simple yet theoretically grounded method that leverages
multi-answer generation from each thought process, enabling more robust and
efficient optimization. Theoretically, we show that the variance of thought
advantage decreases as the number of answers per thought increases.
Empirically, our gradient analysis confirms this effect, showing that GRPO-MA
reduces gradient spikes compared to GRPO. Experiments on math, code, and
diverse multimodal tasks demonstrate that GRPO-MA substantially improves
performance and training efficiency. Our ablation studies further reveal that
increasing the number of answers per thought consistently enhances model
performance.

</details>


### [148] [Knowledge Editing with Subspace-Aware Key-Value Mappings](https://arxiv.org/abs/2509.24502)
*Haewon Park,Sangwoo Kim,Yohan Jo*

Main category: cs.CL

TL;DR: SUIT is a knowledge editing method that modifies only the critical feature subspace relevant to edits, reducing model perturbations while maintaining high edit efficacy.


<details>
  <summary>Details</summary>
Motivation: Existing locate-then-edit methods without constraints on key and value vectors cause significant perturbations to edited models, leading to poor knowledge preservation.

Method: Proposes Subspace Knowledge Edit (SUIT) that identifies and modifies only the subspace of critical features relevant to the edit, rather than unconstrained modifications.

Result: Empirical results on LLaMA-3-8B, GPT-J-6B, and Qwen2.5-7B show SUIT dramatically improves knowledge preservation over strong baselines while maintaining high edit efficacy.

Conclusion: SUIT successfully identifies the critical subspace for edits, providing an effective approach for knowledge editing with minimal model perturbations.

Abstract: Knowledge editing aims to efficiently correct factual errors in Language
Models (LMs). The popular locate-then-edit approach modifies an MLP layer by
finding an optimal mapping between its input vector (key) and output vector
(value) that leads to the expression of the edited knowledge. However, existing
methods without any constraints on the key and value vectors cause significant
perturbations to the edited model. To address this, we propose Subspace
Knowledge Edit (SUIT), a method that identifies and modifies only the subspace
of critical features relevant to the edit. Our empirical results on LLaMA-3-8B,
GPT-J-6B, and Qwen2.5-7B models show that SUIT dramatically improves knowledge
preservation over strong baselines while maintaining high edit efficacy. This
effectiveness confirms that SUIT successfully identifies the critical subspace
for the edit. Further analyses provide additional validation for our approach.
The source code and data will be released to the public upon publication of the
paper.

</details>


### [149] [Building Benchmarks from the Ground Up: Community-Centered Evaluation of LLMs in Healthcare Chatbot Settings](https://arxiv.org/abs/2509.24506)
*Hamna,Gayatri Bhat,Sourabrata Mukherjee,Faisal Lalani,Evan Hadfield,Divya Siddarth,Kalika Bali,Sunayana Sitaram*

Main category: cs.CL

TL;DR: Samiksha is a community-driven evaluation pipeline for LLMs, co-created with civil-society organizations and community members, focusing on culturally aware and contextually grounded assessment in domains like healthcare.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations lack grounding in the lived realities of end users, especially in critical domains like healthcare where cultural practices and community contexts are essential.

Method: Co-creation with CSOs and community members to build a scalable, automated benchmarking pipeline where community feedback informs evaluation criteria, benchmark construction, and output scoring.

Result: Demonstrated in the health domain in India, showing how multilingual LLMs handle nuanced community health queries and providing a scalable approach for inclusive evaluation.

Conclusion: Samiksha offers a scalable pathway for contextually grounded and inclusive LLM evaluation, addressing the gap between artificial benchmarks and real-world community needs.

Abstract: Large Language Models (LLMs) are typically evaluated through general or
domain-specific benchmarks testing capabilities that often lack grounding in
the lived realities of end users. Critical domains such as healthcare require
evaluations that extend beyond artificial or simulated tasks to reflect the
everyday needs, cultural practices, and nuanced contexts of communities. We
propose Samiksha, a community-driven evaluation pipeline co-created with
civil-society organizations (CSOs) and community members. Our approach enables
scalable, automated benchmarking through a culturally aware, community-driven
pipeline in which community feedback informs what to evaluate, how the
benchmark is built, and how outputs are scored. We demonstrate this approach in
the health domain in India. Our analysis highlights how current multilingual
LLMs address nuanced community health queries, while also offering a scalable
pathway for contextually grounded and inclusive LLM evaluation.

</details>


### [150] [AdaThink-Med: Medical Adaptive Thinking with Uncertainty-Guided Length Calibration](https://arxiv.org/abs/2509.24560)
*Shaohao Rui,Kaitao Chen,Weijie Ma,Xiaosong Wang*

Main category: cs.CL

TL;DR: AdaThink-Med is an end-to-end framework that enhances adaptive thinking in medical LLMs by using uncertainty-guided length calibration to reduce unnecessary reasoning for simple questions while maintaining performance on complex ones.


<details>
  <summary>Details</summary>
Motivation: Current medical LLMs engage in lengthy reasoning regardless of question difficulty, leading to high inference costs. Adaptive thinking - thinking less for simple questions and more for complex ones - is crucial for practical medical applications.

Method: Generates multiple candidate outputs per question, evaluates correctness and uncertainty, estimates problem difficulty via uncertainty-guided length calibration, penalizes long reasoning for easy questions, and extends reasoning for difficult ones.

Result: Achieves up to 6.4x length reduction on average across six medical QA benchmarks while maintaining performance with minimal degradation. Spontaneously develops "non-thinking" and "thinking" reasoning modes.

Conclusion: AdaThink-Med effectively enables adaptive thinking in medical LLMs, significantly reducing computational costs while preserving reasoning quality, demonstrating practical viability for real-world medical applications.

Abstract: Recent advances in inference time scaling with extended long chain-of thought
have significantly improved the reasoning capabilities of both general and
medical large language models (LLMs). However, these models tend to engage in
lengthy reasoning processes regardless of the difficulty of the input question,
leading to increased inference costs in real-world applications. Therefore,
enabling adaptive thinking where models think less for simpler questions and
think more for complex ones is critical for the effective use of medical LLMs
in practice. Despite its importance, there is a lack of end-to-end approaches
designed to enhance the adaptive thinking capabilities of medical LLMs while
providing a comprehensive examination of the trade-off between performance and
computational cost. To bridge this gap, we propose AdaThink-Med, the first
end-to-end framework designed to enhance adaptive thinking ability in medical
reasoning models with uncertainty-guided length calibration. AdaThink-Med first
generates multiple candidate outputs for each question, evaluates the
correctness and uncertainty of each candidate, and then estimates problem
difficulty via an uncertainty-guided length calibration module. For outputs
with low difficulty and correct answers, the framework penalizes longer
reasoning paths; whereas for those with high difficulty and incorrect answers,
it encourages extending the chain of thought to explore alternative solutions.
On six public medical QA benchmarks, AdaThink-Med achieves up to 6.4x length
reduction on average while retaining performance with only minimal degradation.
Intriguingly, we observe that AdaThink-Med spontaneously develops two distinct
reasoning modes, which we characterize as "non-thinking" and "thinking",
demonstrating the model's ability to suppress redundant reasoning processes
dynamically.

</details>


### [151] [Inducing Dyslexia in Vision Language Models](https://arxiv.org/abs/2509.24597)
*Melika Honarmand,Ayati Sharma,Badr AlKhamissi,Johannes Mehrer,Martin Schrimpf*

Main category: cs.CL

TL;DR: Using vision-language models to simulate dyslexia by ablating visual-word-form-selective units, which causes reading impairments while preserving general visual and language abilities.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for studying dyslexia are limited in testing causal hypotheses about reading impairments, so computational models are needed to establish causal mechanisms.

Method: Identify visual-word-form-selective units in VLMs and perform targeted ablation of these units, comparing with random unit ablation as control.

Result: Targeted ablation of word-form units causes selective reading impairments matching dyslexic patterns (phonological deficits without orthographic changes), while general visual/language abilities remain intact.

Conclusion: The model successfully replicates key dyslexia characteristics and provides a computational framework for investigating reading disorders.

Abstract: Dyslexia, a neurodevelopmental disorder characterized by persistent reading
difficulties, is often linked to reduced activity of the visual word form area
in the ventral occipito-temporal cortex. Traditional approaches to studying
dyslexia, such as behavioral and neuroimaging methods, have provided valuable
insights but remain limited in their ability to test causal hypotheses about
the underlying mechanisms of reading impairments. In this study, we use
large-scale vision-language models (VLMs) to simulate dyslexia by functionally
identifying and perturbing artificial analogues of word processing. Using
stimuli from cognitive neuroscience, we identify visual-word-form-selective
units within VLMs and demonstrate that targeted ablation of these units, unlike
ablation of random units, leads to selective impairments in reading tasks while
general visual and language comprehension abilities remain intact. In
particular, the resulting model matches dyslexic humans' phonological deficits
without a significant change in orthographic processing. Taken together, our
modeling results replicate key characteristics of dyslexia and establish a
computational framework for investigating reading disorders.

</details>


### [152] [HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition](https://arxiv.org/abs/2509.24613)
*Gio Paik,Yongbeom Kim,Soungmin Lee,Sangmin Ahn,Chanwoo Kim*

Main category: cs.CL

TL;DR: HiKE is the first Korean-English code-switching benchmark providing hierarchical CS-level labels for systematic ASR evaluation, showing that fine-tuning with CS data enables multilingual ASR models to handle code-switching.


<details>
  <summary>Details</summary>
Motivation: Code-switching remains an underexplored challenge in multilingual ASR despite advances, with Korean-English CS lacking accessible evaluation frameworks.

Method: Developed HiKE benchmark with high-quality natural CS data across topics, providing loanword labels and hierarchical CS-level labeling (word, phrase, sentence) for systematic evaluation.

Result: Most multilingual ASR models initially struggle with CS-ASR but can be enabled through fine-tuning with CS data.

Conclusion: HiKE provides the first globally accessible Korean-English CS evaluation framework that enables systematic assessment and improvement of multilingual ASR models' code-switching capabilities.

Abstract: Despite advances in multilingual automatic speech recognition (ASR),
code-switching (CS), the mixing of languages within an utterance common in
daily speech, remains a severely underexplored challenge. In this paper, we
introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the
first globally accessible evaluation framework for Korean-English CS, aiming to
provide a means for the precise evaluation of multilingual ASR models and to
foster research in the field. The proposed framework not only consists of
high-quality, natural CS data across various topics, but also provides
meticulous loanword labels and a hierarchical CS-level labeling scheme (word,
phrase, and sentence) that together enable a systematic evaluation of a model's
ability to handle each distinct level of code-switching. Through evaluations of
diverse multilingual ASR models and fine-tuning experiments, this paper
demonstrates that while most multilingual ASR models initially struggle with
CS-ASR, this capability can be enabled through fine-tuning with CS data. HiKE
will be available at https://github.com/ThetaOne-AI/HiKE.

</details>


### [153] [Hype or not? Formalizing Automatic Promotional Language Detection in Biomedical Research](https://arxiv.org/abs/2509.24638)
*Bojan Batalo,Erica K. Shimomoto,Neil Millar*

Main category: cs.CL

TL;DR: This paper introduces automatic hype detection in scientific writing, defines hype as hyperbolic/subjective language used to glamorize research, creates annotation guidelines for NIH grant applications, and evaluates ML models showing promising results.


<details>
  <summary>Details</summary>
Motivation: Hype language in science is increasing and can undermine objective evaluation, impede research development, and erode trust in science.

Method: Proposed formalized annotation guidelines for identifying hype language, annotated NIH grant application corpus, evaluated traditional text classifiers and language models against human baseline.

Result: Formalizing annotation guidelines helped humans reliably annotate hype adjectives, and training ML models on the annotated dataset yielded promising results.

Conclusion: Hype detection is linguistically complex and may require domain knowledge and temporal awareness, but this is the first NLP approach to this task with promising initial results.

Abstract: In science, promotional language ('hype') is increasing and can undermine
objective evaluation of evidence, impede research development, and erode trust
in science. In this paper, we introduce the task of automatic detection of
hype, which we define as hyperbolic or subjective language that authors use to
glamorize, promote, embellish, or exaggerate aspects of their research. We
propose formalized guidelines for identifying hype language and apply them to
annotate a portion of the National Institutes of Health (NIH) grant application
corpus. We then evaluate traditional text classifiers and language models on
this task, comparing their performance with a human baseline. Our experiments
show that formalizing annotation guidelines can help humans reliably annotate
candidate hype adjectives and that using our annotated dataset to train machine
learning models yields promising results. Our findings highlight the linguistic
complexity of the task, and the potential need for domain knowledge and
temporal awareness of the facts. While some linguistic works address hype
detection, to the best of our knowledge, we are the first to approach it as a
natural language processing task.

</details>


### [154] [InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation](https://arxiv.org/abs/2509.24663)
*Weilin Zhao,Zihan Zhou,Zhou Su,Chaojun Xiao,Yuxuan Li,Yanghao Li,Yudi Zhang,Weilun Zhao,Zhen Li,Yuxiang Huang,Ao Sun,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: InfLLM-V2 is a trainable sparse attention framework that enables efficient long-sequence processing by reusing dense attention parameters and smoothly switching between dense and sparse attention based on sequence length.


<details>
  <summary>Details</summary>
Motivation: To overcome computational and memory bottlenecks of self-attention in Transformers for long sequences, while avoiding excessive parameters and maintaining the pretrain-on-short, finetune-on-long workflow.

Method: Dense-sparse switchable attention that reuses dense attention parameters through parameter-free architecture modification, using dense attention for short inputs and sparse attention for long sequences with efficient implementation.

Result: 4x faster than dense attention while retaining 98.1% performance on long-context understanding and 99.7% on chain-of-thought reasoning. MiniCPM4.1 model trained and open-sourced.

Conclusion: InfLLM-V2 provides an effective solution for efficient long-sequence processing that maintains performance while significantly improving computational efficiency across all sequence lengths.

Abstract: Long-sequence processing is a critical capability for modern large language
models. However, the self-attention mechanism in the standard Transformer
architecture faces severe computational and memory bottlenecks when processing
long sequences. While trainable sparse attention methods offer a promising
solution, existing approaches such as NSA introduce excessive extra parameters
and disrupt the conventional \textit{pretrain-on-short, finetune-on-long}
workflow, resulting in slow convergence and difficulty in acceleration. To
overcome these limitations, we introduce dense-sparse switchable attention
framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that
seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2
reuses dense attention parameters through parameter-free architecture
modification, maintaining consistency between short and long sequence
processing. Additionally, InfLLM-V2 ensures computational efficiency across all
sequence lengths, by using dense attention for short inputs and smoothly
transitioning to sparse attention for long sequences. To achieve practical
acceleration, we further introduce an efficient implementation of InfLLM-V2
that significantly reduces the computational overhead. Our experiments on
long-context understanding and chain-of-thought reasoning demonstrate that
InfLLM-V2 is 4$\times$ faster than dense attention while retaining 98.1% and
99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we
have trained and open-sourced MiniCPM4.1
(https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model,
providing a reproducible implementation for the research community.

</details>


### [155] [Understanding the Dilemma of Unlearning for Large Language Models](https://arxiv.org/abs/2509.24675)
*Qingjie Zhang,Haoting Qian,Zhicong Huang,Cheng Hong,Minlie Huang,Ke Xu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: The paper proposes unPact, an interpretable framework for analyzing unlearning in LLMs, revealing that current methods either insufficiently remove knowledge (recoverable via keyword emphasis) or cause catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability in understanding how unlearning works in LLMs and why its effectiveness is contested.

Method: Developed unPact framework that quantifies prompt token influence on outputs through attribution and contribution tracking, enabling pre/post-unlearning comparisons.

Result: Found that unlearning works by disrupting keyword focus rather than true knowledge erasure, knowledge remains recoverable via keyword emphasis, and catastrophic forgetting stems from indiscriminate token penalization.

Conclusion: Existing unlearning methods face a dilemma: either insufficient (knowledge recoverable) or overly destructive (catastrophic forgetting), leaving a gap to reliable unlearning.

Abstract: Unlearning seeks to remove specific knowledge from large language models
(LLMs), but its effectiveness remains contested. On one side, "forgotten"
knowledge can often be recovered through interventions such as light
fine-tuning; on the other side, unlearning may induce catastrophic forgetting
that degrades general capabilities. Despite active exploration of unlearning
methods, interpretability analyses of the mechanism are scarce due to the
difficulty of tracing knowledge in LLMs' complex architectures. We address this
gap by proposing unPact, an interpretable framework for unlearning via prompt
attribution and contribution tracking. Typically, it quantifies each prompt
token's influence on outputs, enabling pre- and post-unlearning comparisons to
reveal what changes. Across six mainstream unlearning methods, three LLMs, and
three benchmarks, we find that: (1) Unlearning appears to be effective by
disrupting focus on keywords in prompt; (2) Much of the knowledge is not truly
erased and can be recovered by simply emphasizing these keywords in prompts,
without modifying the model's weights; (3) Catastrophic forgetting arises from
indiscriminate penalization of all tokens. Taken together, our results suggest
an unlearning dilemma: existing methods tend either to be insufficient -
knowledge remains recoverable by keyword emphasis, or overly destructive -
general performance collapses due to catastrophic forgetting, still leaving a
gap to reliable unlearning.

</details>


### [156] [Reference-Free Rating of LLM Responses via Latent Information](https://arxiv.org/abs/2509.24678)
*Leander Girrbach,Chi-Ping Su,Tankred Saanum,Richard Socher,Eric Schulz,Zeynep Akata*

Main category: cs.CL

TL;DR: The paper studies the unreliability of single-response LLM-as-a-judge ratings without references, identifies issues with score instability and poor calibration, and proposes Latent Judges methods that derive scalar ratings from internal model signals to provide more deterministic and discriminative evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the systematic issues in current LLM-as-a-judge practices where scores are unstable under sampling, poorly calibrated, compressed near the top of the scale, and frequently tied, making single-response ratings unreliable.

Method: Proposes three Latent Judges methods: (1) probability-weighted scores over integer ratings, (2) verifier-style probabilities of "yes", and (3) linear probes trained on model activations at the rating position.

Result: Across various pairwise and single-rating benchmarks, latent methods match or surpass standard prompting, with consistent gains on pairwise accuracy and listwise ranking. Probability-weighted scores achieve the strongest single-rating correlations, while probes recover useful signals when output logits are miscalibrated.

Conclusion: Latent information provides deterministic and more discriminative signals for reference-free evaluation, and can improve selection and training approaches like Best-of-N, multi-teacher distillation, and routing.

Abstract: How reliable are single-response LLM-as-a-judge ratings without references,
and can we obtain fine-grained, deterministic scores in this setting? We study
the common practice of asking a judge model to assign Likert-scale scores to
free-text responses and show two systematic issues: scores are unstable under
sampling and poorly calibrated, leading to compression near the top of the
scale and frequent ties. We then propose and evaluate Latent Judges, which
derive scalar ratings from internal model signals: (i) probability-weighted
scores over integer ratings, (ii) verifier-style probabilities of "yes", and
(iii) linear probes trained on model activations at the rating position. Across
a broad suite of pairwise and single-rating benchmarks, latent methods match or
surpass standard prompting, with consistent gains on pairwise accuracy and
listwise ranking relevant to Best-of-N selection. Probability-weighted scores
achieve the strongest single-rating correlations, while probes recover useful
signals when output logits are miscalibrated. These results indicate that
latent information provides deterministic and more discriminative signals for
reference-free evaluation, and can improve selection and training approaches
like Best-of-$N$, multi-teacher distillation, and routing.

</details>


### [157] [MemGen: Weaving Generative Latent Memory for Self-Evolving Agents](https://arxiv.org/abs/2509.24704)
*Guibin Zhang,Muxin Fu,Shuicheng Yan*

Main category: cs.CL

TL;DR: MemGen is a dynamic generative memory framework that enables LLM agents to interweave memory and reasoning through latent token sequences, achieving significant performance improvements over existing memory systems and exhibiting emergent human-like memory faculties.


<details>
  <summary>Details</summary>
Motivation: Existing memory paradigms for LLM agents are constrained - parametric memory forcibly adjusts model parameters while retrieval-based memory externalizes experience into databases, neither capturing the fluid integration of reasoning and memory that characterizes human cognition.

Method: MemGen consists of a memory trigger that monitors the agent's reasoning state to decide explicit memory invocation, and a memory weaver that takes the current state as stimulus to construct latent token sequences as machine-native memory to enrich reasoning.

Result: Extensive experiments across eight benchmarks show MemGen surpasses leading external memory systems (ExpeL, AWM) by up to 38.22%, exceeds GRPO by up to 13.44%, and exhibits strong cross-domain generalization ability.

Conclusion: MemGen enables agents to recall and augment latent memory throughout reasoning, creating a tightly interwoven cycle of memory and cognition, and spontaneously evolves distinct human-like memory faculties including planning, procedural, and working memory without explicit supervision.

Abstract: Agent memory shapes how Large Language Model (LLM)-powered agents, akin to
the human brain, progressively refine themselves through environment
interactions. Existing paradigms remain constrained: parametric memory forcibly
adjusts model parameters, and retrieval-based memory externalizes experience
into structured databases, yet neither captures the fluid interweaving of
reasoning and memory that underlies human cognition. To address this gap, we
propose MemGen, a dynamic generative memory framework that equips agents with a
human-esque cognitive faculty. It consists of a \textit{memory trigger}, which
monitors the agent's reasoning state to decide explicit memory invocation, and
a \textit{memory weaver}, which takes the agent's current state as stimulus to
construct a latent token sequence as machine-native memory to enrich its
reasoning. In this way, MemGen enables agents to recall and augment latent
memory throughout reasoning, producing a tightly interwoven cycle of memory and
cognition. Extensive experiments across eight benchmarks show that MemGen
surpasses leading external memory systems such as ExpeL and AWM by up to
$38.22\%$, exceeds GRPO by up to $13.44\%$, and exhibits strong cross-domain
generalization ability. More importantly, we find that without explicit
supervision, MemGen spontaneously evolves distinct human-like memory faculties,
including planning memory, procedural memory, and working memory, suggesting an
emergent trajectory toward more naturalistic forms of machine cognition.

</details>


### [158] [Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution](https://arxiv.org/abs/2509.24726)
*Shaobo Wang,Zhengbo Jiao,Zifan Zhang,Yilang Peng,Xu Ze,Boyu Yang,Wei Wang,Hu Wei,Linfeng Zhang*

Main category: cs.CL

TL;DR: Socratic-Zero is an autonomous framework that generates high-quality training data from minimal seed examples through co-evolution of three agents, achieving significant performance gains on mathematical reasoning benchmarks without requiring pre-existing tasks or labels.


<details>
  <summary>Details</summary>
Motivation: Current LLM training relies on massive human-annotated datasets that are difficult to scale, while existing data synthesis methods suffer from inconsistent quality and inability to adapt to model evolution.

Method: Uses three co-evolving agents: Solver (refines reasoning from preference feedback), Teacher (creates challenging questions based on Solver's weaknesses), and Generator (distills Teacher's strategy for scalable curriculum generation).

Result: Starting from only 100 seed questions, Socratic-Solver-8B achieves +20.2 percentage point average gain over prior methods across 7 mathematical reasoning benchmarks. Synthetic data from Socratic-Generator-32B enables student LLMs to outperform SOTA commercial models.

Conclusion: The closed-loop co-evolution system enables fully autonomous generation of high-quality training data that significantly improves LLM performance on reasoning tasks without human annotation.

Abstract: Recent breakthroughs in large language models (LLMs) on reasoning tasks rely
heavily on massive, high-quality datasets-typically human-annotated and thus
difficult to scale. While data synthesis or distillation offers a promising
alternative, existing methods struggle with inconsistent data quality and an
inability to dynamically adapt to the evolving capabilities of the model,
leading to suboptimal training signals. To address these limitations, we
introduce Socratic-Zero, a fully autonomous framework that generates
high-quality training data from minimal seed examples through the co-evolution
of three agents: the Teacher, the Solver, and the Generator. The Solver
continuously refines its reasoning by learning from preference feedback on both
successful and failed trajectories; the Teacher adaptively crafts increasingly
challenging questions based on the Solver's weaknesses; and the Generator
distills the Teacher's question-design strategy to enable scalable,
high-fidelity curriculum generation. This closed-loop system produces a
self-improving curriculum-requiring no pre-existing tasks or labels.
Remarkably, starting from only 100 seed questions, our Socratic-Solver-8B
achieves an average gain of +20.2 percentage points over prior data synthesis
methods across seven mathematical reasoning benchmarks (AMC23, AIME24-25,
Olympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3
and GLM4 series models. Even more surprisingly, synthetic data from
Socratic-Generator-32B enables student LLMs to achieve superior performance
compared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks,
including Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4,
and Claude-4.1-Opus.

</details>


### [159] [ProxyAttn: Guided Sparse Attention via Representative Heads](https://arxiv.org/abs/2509.24745)
*Yixuan Wang,Huang He,Siqi Bao,Hua Wu,Haifeng Wang,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: ProxyAttn is a training-free sparse attention algorithm that accelerates LLMs on long-text tasks by using representative proxy heads for fine-grained block importance estimation, achieving up to 10.3x attention acceleration and 2.4x pre-filling acceleration.


<details>
  <summary>Details</summary>
Motivation: Existing methods for efficient block sparse attention use coarse-grained block importance estimation, which leads to performance degradation at high sparsity rates. The quadratic complexity of attention mechanisms limits LLM efficiency on long-text tasks.

Method: Compresses attention head dimensions and uses scores from pooled representative heads to approximate all heads' scores. Proposes block-aware dynamic budget estimation to account for varying sparsity among heads. Combines proxy head scores with multi-head dynamic budgets for fine-grained evaluation.

Result: Experiments confirm similarity among attention heads. Achieves up to 10.3x attention acceleration and 2.4x pre-filling acceleration without significant performance loss across various models and benchmarks.

Conclusion: ProxyAttn enables more precise block importance estimation at low computational cost, providing substantial gains in both performance and efficiency compared to existing sparse attention methods.

Abstract: The quadratic complexity of attention mechanisms limits the efficiency of
Large Language Models (LLMs) on long-text tasks. Recently, methods that
dynamically estimate block importance have enabled efficient block sparse
attention, leading to significant acceleration in long-text pre-filling of
LLMs. However, their coarse-grained estimation inevitably leads to performance
degradation at high sparsity rates. In this work, we propose ProxyAttn, a
training-free sparse attention algorithm that achieves more precise block
estimation by compressing the dimension of attention heads. Based on our
observation of the similarity among multiple attention heads, we use the scores
of pooled representative heads to approximate the scores for all heads. To
account for the varying sparsity among heads, we also propose a block-aware
dynamic budget estimation method. By combining the scores from representative
proxy heads with multi-head dynamic budgets, we achieve a more fine-grained
block importance evaluation at low computational cost. Experiments on a variety
of mainstream models and extensive benchmarks confirm the underlying similarity
among attention heads. Leveraging a fine-grained estimation, the proposed
method achieves substantial gains in performance and efficiency compared to
existing methods. More precisely, ProxyAttn can achieve up to 10.3x attention
acceleration and 2.4x prefilling acceleration without significant performance
loss. Our code is available at https://github.com/wyxstriker/ProxyAttn.

</details>


### [160] [LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space](https://arxiv.org/abs/2509.24771)
*Guibin Zhang,Fanci Meng,Guancheng Wan,Zherui Li,Kun Wang,Zhenfei Yin,Lei Bai,Shuicheng Yan*

Main category: cs.CL

TL;DR: LatentEvolve is a self-evolving test-time scaling framework that enables LLMs to progressively learn how to scale computation more effectively through alternating daytime (fast recall) and nighttime (slow consolidation) processes inspired by human cognitive systems.


<details>
  <summary>Details</summary>
Motivation: Existing test-time scaling methods are independent and don't allow LLMs to progressively learn better scaling strategies. The goal is to evolve LLMs to learn "how to scale test-time computation" more effectively over time.

Method: LatentEvolve uses a dual evolutionary system: daytime scaling rapidly retrieves historical latent representations to guide current reasoning, while nighttime scaling integrates past latent optimizations through consolidation, mimicking human brain's complementary learning system.

Result: Extensive experiments across 8 benchmarks and 5 model backbones show LatentEvolve surpasses state-of-the-art TTS methods like LatentSeek and TTRL by up to 13.33%, with exceptional cross-domain and cross-backbone generalization.

Conclusion: The alternating daytime-nighttime evolution process successfully enables LLMs to progressively learn effective test-time scaling strategies, achieving superior performance and generalization compared to existing methods.

Abstract: Test-time Scaling (TTS) has been demonstrated to significantly enhance the
reasoning capabilities of Large Language Models (LLMs) during the inference
phase without altering model parameters. However, existing TTS methods are
largely independent, implying that LLMs have not yet evolved to progressively
learn how to scale more effectively. With the objective of evolving LLMs to
learn ``how to scale test-time computation,'' we propose LatentEvolve, a
self-evolving latent TTS framework inspired by the complementary learning
system (CLS) theory. Analogous to the human brain's dual system of a
fast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve
comprises two evolutionary components: \textit{daytime scaling}, which rapidly
retrieves historical latent representations to better guide current LLM
reasoning; and \textit{nighttime scaling}, which integrates past latent
optimizations in a manner akin to the human brain's consolidation of
experiences during sleep. The alternation of daytime and nighttime processes
facilitates a fast and slow evolution of LLM TTS, mirroring human cognitive
dynamics in a fully unsupervised manner. Extensive experiments across eight
benchmarks and five model backbones demonstrate that our LatentEvolve surpasses
state-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33\%$ and
exhibits exceptional cross-domain and cross-backbone generalization.

</details>


### [161] [SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models](https://arxiv.org/abs/2509.24781)
*Jun Rao,Yunjie Liao,Xuebo Liu,Zepeng Lin,Lian Lian,Dong Jin,Shengjun Cheng,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: SeaPO introduces Strategic Error Amplification to enhance LLM preference optimization by injecting specific error patterns into negative samples, ensuring they are more erroneous than positive samples, which improves model performance across multiple dimensions.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods struggle when positive and negative samples become similar in quality during training, complicating preference learning optimization.

Method: Strategic Error Amplification method that leverages three common LLM error types to introduce specific error patterns into model Preference Optimization, ensuring negative samples are more erroneous.

Result: Evaluations across five capability dimensions and model scales (1.5B to 14B) show significant performance improvements, particularly in truthfulness (5-10 percentage points). Task performance varies with error types - common errors improve related tasks, while mixed errors provide broader enhancement.

Conclusion: SeaPO effectively addresses the similarity issue in preference optimization by strategically amplifying errors in negative samples, leading to substantial performance improvements across multiple model scales and tasks.

Abstract: Existing alignment methods for preference optimization of large language
models (LLMs) aim to enhance model performance by utilizing pairs of positive
and negative samples. However, due to the limited capacity of models in scoring
or generating responses, the quality of positive and negative samples may
become similar during training, which complicates optimization for preference
learning. To address this issue, we introduce SeaPO, a Strategic Error
Amplification method that leverages three error types commonly occurring in
LLMs to introduce specific error patterns into the model Preference
Optimization. This strategy ensures that negative samples are more erroneous
than positive samples and preference-based training is employed to mitigate the
occurrence of these errors, thereby enhancing model performance. Evaluations
across five capability dimensions and different model scales (1.5B to 14B)
demonstrate that the generated data significantly improved overall model
performance, particularly in terms of truthfulness, with improvements of 5-10
percentage points observed. Further analysis reveals that task performance
varies depending on the error types introduced. Injecting the most common error
types improves performance in related tasks, while a mix of error types leads
to a broader performance enhancement: most tasks show stable improvements,
while a few tasks exhibit significant gains.

</details>


### [162] [Evaluating Spatiotemporal Consistency in Automatically Generated Sewing Instructions](https://arxiv.org/abs/2509.24792)
*Luisa Geiger,Mareike Hartmann,Michael Sullivan,Alexander Koller*

Main category: cs.CL

TL;DR: Proposes a novel tree-based evaluation metric for LLM-generated assembly instructions that better captures spatiotemporal aspects than traditional text similarity metrics like BLEU and BERT.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics like BLEU and BERT similarity scores fail to adequately reflect spatiotemporal aspects of construction in step-by-step assembly instructions, necessitating a more accurate evaluation approach.

Method: Developed an automatic tree-based evaluation metric specifically designed for LLM-generated step-by-step assembly instructions, focusing on spatiotemporal soundness rather than just textual similarity.

Result: The proposed metric shows better correlation with manually-annotated error counts and human quality ratings in sewing instructions, and demonstrates greater robustness against artificially-constructed counterfactual examples designed to confuse text-based metrics.

Conclusion: The tree-based metric is superior to traditional approaches for evaluating the spatiotemporal soundness of assembly instructions, providing more accurate and robust assessment of instruction quality.

Abstract: In this paper, we propose a novel, automatic tree-based evaluation metric for
LLM-generated step-by-step assembly instructions, that more accurately reflects
spatiotemporal aspects of construction than traditional metrics such as BLEU
and BERT similarity scores. We apply our proposed metric to the domain of
sewing instructions, and show that our metric better correlates with
manually-annotated error counts as well as human quality ratings, demonstrating
our metric's superiority for evaluating the spatiotemporal soundness of sewing
instructions. Further experiments show that our metric is more robust than
traditional approaches against artificially-constructed counterfactual examples
that are specifically constructed to confound metrics that rely on textual
similarity.

</details>


### [163] [KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning](https://arxiv.org/abs/2509.24816)
*Xilin Dang,Kexin Chen,Xiaorui Su,Ayush Noori,Iñaki Arango,Lucas Vittor,Xinyi Long,Yuyang Du,Marinka Zitnik,Pheng Ann Heng*

Main category: cs.CL

TL;DR: KnowGuard introduces a novel investigate-before-abstain paradigm that uses systematic knowledge graph exploration to improve LLM abstention in clinical decision-making, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with abstention in medical scenarios, providing overconfident responses despite incomplete information, which poses safety risks in clinical practice.

Method: A two-stage approach: 1) evidence discovery through graph expansion and direct retrieval, and 2) evidence evaluation using multiple factors to adapt exploration based on patient context and conversation history.

Result: KnowGuard improves diagnostic accuracy by 3.93% while reducing unnecessary interaction by 7.27 turns on average compared to state-of-the-art abstention approaches.

Conclusion: The investigate-before-abstain paradigm with systematic knowledge graph exploration effectively enables models to recognize insufficient medical evidence and make appropriate abstentions in clinical decision-making.

Abstract: In clinical practice, physicians refrain from making decisions when patient
information is insufficient. This behavior, known as abstention, is a critical
safety mechanism preventing potentially harmful misdiagnoses. Recent
investigations have reported the application of large language models (LLMs) in
medical scenarios. However, existing LLMs struggle with the abstentions,
frequently providing overconfident responses despite incomplete information.
This limitation stems from conventional abstention methods relying solely on
model self-assessments, which lack systematic strategies to identify knowledge
boundaries with external medical evidences. To address this, we propose
\textbf{KnowGuard}, a novel \textit{investigate-before-abstain} paradigm that
integrates systematic knowledge graph exploration for clinical decision-making.
Our approach consists of two key stages operating on a shared contextualized
evidence pool: 1) an evidence discovery stage that systematically explores the
medical knowledge space through graph expansion and direct retrieval, and 2) an
evidence evaluation stage that ranks evidence using multiple factors to adapt
exploration based on patient context and conversation history. This two-stage
approach enables systematic knowledge graph exploration, allowing models to
trace structured reasoning paths and recognize insufficient medical evidence.
We evaluate our abstention approach using open-ended multi-round clinical
benchmarks that mimic realistic diagnostic scenarios, assessing abstention
quality through accuracy-efficiency trade-offs beyond existing closed-form
evaluations. Experimental evidences clearly demonstrate that KnowGuard
outperforms state-of-the-art abstention approaches, improving diagnostic
accuracy by 3.93\% while reducing unnecessary interaction by 7.27 turns on
average.

</details>


### [164] [DiaCDM: Cognitive Diagnosis in Teacher-Student Dialogues using the Initiation-Response-Evaluation Framework](https://arxiv.org/abs/2509.24821)
*Rui Jia,Yuang Wei,Ruijia Li,Yuang-Hao Jiang,Xinyu Xie,Yaomin Shen,Min Zhang,Bo Jiang*

Main category: cs.CL

TL;DR: DiaCDM is a novel cognitive diagnosis model for teacher-student dialogues that adapts the IRE framework and uses graph-based encoding to improve diagnostic accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional cognitive diagnosis models are unsuitable for dynamic, unstructured dialogues and struggle to extract diagnostic semantics from lengthy conversations.

Method: Adapted initiation-response-evaluation (IRE) framework for dialogue diagnosis and developed graph-based encoding that integrates teacher questions with knowledge components.

Result: Experiments on three real-world dialogue datasets show significant improvements in diagnostic accuracy and enhanced interpretability of results.

Conclusion: DiaCDM provides teachers with a powerful tool for assessing students' cognitive states in dialogue settings, representing the first exploration of cognitive diagnosis in this context.

Abstract: While cognitive diagnosis (CD) effectively assesses students' knowledge
mastery from structured test data, applying it to real-world teacher-student
dialogues presents two fundamental challenges. Traditional CD models lack a
suitable framework for handling dynamic, unstructured dialogues, and it's
difficult to accurately extract diagnostic semantics from lengthy dialogues. To
overcome these hurdles, we propose DiaCDM, an innovative model. We've adapted
the initiation-response-evaluation (IRE) framework from educational theory to
design a diagnostic framework tailored for dialogue. We also developed a unique
graph-based encoding method that integrates teacher questions with relevant
knowledge components to capture key information more precisely. To our
knowledge, this is the first exploration of cognitive diagnosis in a dialogue
setting. Experiments on three real-world dialogue datasets confirm that DiaCDM
not only significantly improves diagnostic accuracy but also enhances the
results' interpretability, providing teachers with a powerful tool for
assessing students' cognitive states. The code is available at
https://github.com/Mind-Lab-ECNU/DiaCDM/tree/main.

</details>


### [165] [SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching](https://arxiv.org/abs/2509.24832)
*Xinye Zhao,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: SemShareKV is a KV cache sharing framework that accelerates LLM inference by reusing key-value caches from semantically similar prompts using fuzzy token matching with LSH and RoPE, achieving up to 6.25× speedup and 42% memory reduction.


<details>
  <summary>Details</summary>
Motivation: The memory footprint of KV caches during LLM inference has become a bottleneck. Existing approaches are limited in handling semantically similar but lexically different prompts, which are common in tasks like multi-document summarization and conversational agents.

Method: Proposes SemShareKV framework that uses locality-sensitive hashing (LSH) on token embeddings for fuzzy token matching and incorporates Rotary Position Embedding (RoPE) to preserve positional information. It selectively reuses relevant key-value pairs from reference prompts' caches.

Result: Experiments on summarization datasets show up to 6.25× speedup and 42% lower GPU memory usage with 5k tokens input, with negligible quality degradation.

Conclusion: Semantic-aware cache sharing shows significant potential for efficient LLM inference, particularly in scenarios with semantically similar but lexically diverse prompts.

Abstract: As large language models (LLMs) continue to scale, the memory footprint of
key-value (KV) caches during inference has become a significant bottleneck.
Existing approaches primarily focus on compressing KV caches within a single
prompt or reusing shared prefixes or frequently ocurred text segments across
prompts. However, such strategies are limited in scenarios where prompts are
semantically similar but lexically different, which frequently occurs in tasks
such as multi-document summarization and conversational agents. We propose
\textit{SemShareKV}, a KV cache sharing and compression framework that
accelerates LLM inference by reusing KVCache in semantically similar prompts.
Instead of relying on exact token matches, SemShareKV applies fuzzy token
matching using locality-sensitive hashing (LSH) on token embeddings and
incorporates Rotary Position Embedding (RoPE) to better preserve positional
information. By selectively reusing relevant key-value pairs from a reference
prompt's cache, SemShareKV reduces redundant computation while maintaining
output quality. Experiments on diverse summarization datasets show up to
6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with
negligible quality degradation. These results highlight the potential of
semantic-aware cache sharing for efficient LLM inference.

</details>


### [166] [Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement](https://arxiv.org/abs/2509.24841)
*Zhilong Zhao,Yindi Liu*

Main category: cs.CL

TL;DR: The paper proposes a Hierarchical Error Correction (HEC) framework that systematically analyzes and corrects AI errors in specialized domains, achieving 11.2 percentage point average improvement across various tasks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models perform poorly in specialized domains (only 45.9% accuracy on medical coding), necessitating targeted error correction strategies.

Method: Three-stage hierarchical correction framework addressing Knowledge-layer (58.4%), Reasoning-layer (39.6%), and Complexity-layer (2.0%) errors based on systematic error pattern analysis.

Result: Validated across 4 domains (medical, legal, political, legal reasoning) with 11.2% average improvement across 5 LLM architectures (p < 0.001), but limited effectiveness in high-baseline tasks (>75% accuracy).

Conclusion: Systematic error analysis enables effective AI enhancement in specialized domains, particularly for moderate-baseline tasks, while understanding framework boundaries is crucial for optimal deployment.

Abstract: Large Language Models face significant performance challenges in specialized
domains, with state-of-the-art models achieving only 45.9% accuracy on medical
coding tasks. This study proposes a Hierarchical Error Correction (HEC)
framework that addresses domain-specific AI limitations through systematic
error analysis and targeted intervention strategies.
  We analyze error patterns across four specialized domains and find that AI
errors follow consistent hierarchical structures: Knowledge-layer errors
(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).
Based on these patterns, we develop a three-stage correction framework that
addresses errors according to their hierarchical importance and demonstrates
that framework effectiveness correlates inversely with baseline task
performance.
  Experimental validation across medical transcription (4,921 cases), legal
document classification (1,000 cases), political bias detection (645 cases),
and legal reasoning (1,000 cases) shows consistent improvements. Cross-model
validation across five LLM architectures demonstrates average improvements of
11.2 percentage points (p < 0.001). However, analysis reveals framework
limitations in high-baseline tasks (>75% accuracy), where hierarchical
intervention may interfere with effective reasoning processes.
  The results suggest that systematic error analysis can guide effective AI
enhancement strategies in specialized domains, particularly for
moderate-baseline tasks, while highlighting the importance of understanding
framework boundaries for optimal deployment.

</details>


### [167] [Between Help and Harm: An Evaluation of Mental Health Crisis Handling by LLMs](https://arxiv.org/abs/2509.24857)
*Adrian Arnaiz-Rodriguez,Miguel Baidal,Erik Derner,Jenn Layton Annable,Mark Ball,Mark Ince,Elvira Perez Vallejos,Nuria Oliver*

Main category: cs.CL

TL;DR: This paper evaluates LLMs' ability to handle mental health crises, finding they're generally reliable for explicit disclosures but have significant risks including inappropriate responses, poor handling of indirect signals, and formulaic replies.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in high-stakes mental health contexts, but their crisis detection and response capabilities remain poorly understood due to lack of unified taxonomies, benchmarks, and clinical evaluations.

Method: Created unified taxonomy of 6 mental health crisis categories, curated evaluation dataset, established expert-designed assessment protocol, and systematically benchmarked 3 state-of-the-art LLMs for crisis classification and response generation.

Result: LLMs show high consistency and reliability for explicit crisis disclosures, but significant risks remain: non-negligible inappropriate/harmful responses (worse in open-weight models), poor handling of indirect/ambiguous signals, formulaic replies, and context misalignment.

Conclusion: Urgent need for enhanced safeguards, improved crisis detection, and context-aware interventions in LLM deployments. The taxonomy, datasets, and framework provide groundwork for responsible AI mental health support innovation.

Abstract: The widespread use of chatbots powered by large language models (LLMs) such
as ChatGPT and Llama has fundamentally reshaped how people seek information and
advice across domains. Increasingly, these chatbots are being used in
high-stakes contexts, including emotional support and mental health concerns.
While LLMs can offer scalable support, their ability to safely detect and
respond to acute mental health crises remains poorly understood. Progress is
hampered by the absence of unified crisis taxonomies, robust annotated
benchmarks, and empirical evaluations grounded in clinical best practices. In
this work, we address these gaps by introducing a unified taxonomy of six
clinically-informed mental health crisis categories, curating a diverse
evaluation dataset, and establishing an expert-designed protocol for assessing
response appropriateness. We systematically benchmark three state-of-the-art
LLMs for their ability to classify crisis types and generate safe, appropriate
responses. The results reveal that while LLMs are highly consistent and
generally reliable in addressing explicit crisis disclosures, significant risks
remain. A non-negligible proportion of responses are rated as inappropriate or
harmful, with responses generated by an open-weight model exhibiting higher
failure rates than those generated by the commercial ones. We also identify
systemic weaknesses in handling indirect or ambiguous risk signals, a reliance
on formulaic and inauthentic default replies, and frequent misalignment with
user context. These findings underscore the urgent need for enhanced
safeguards, improved crisis detection, and context-aware interventions in LLM
deployments. Our taxonomy, datasets, and evaluation framework lay the
groundwork for ongoing research and responsible innovation in AI-driven mental
health support, helping to minimize harm and better protect vulnerable users.

</details>


### [168] [Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning](https://arxiv.org/abs/2509.24866)
*Matteo Fuoli,Weihang Huang,Jeannette Littlemore,Sarah Turner,Ellen Wilding*

Main category: cs.CL

TL;DR: This study explores using large language models (LLMs) to automate metaphor identification in texts, comparing RAG, prompt engineering, and fine-tuning methods, with fine-tuning achieving the best performance (F1=0.79).


<details>
  <summary>Details</summary>
Motivation: Metaphor analysis is important for understanding cognition and discourse, but manual annotation is time-consuming and context-sensitive, creating a need for automated solutions.

Method: Compared three approaches: retrieval-augmented generation (RAG), prompt engineering (zero-shot, few-shot, chain-of-thought), and fine-tuning on hand-coded texts.

Result: Fine-tuned LLMs achieved highest accuracy with median F1 score of 0.79. Discrepancies between human and LLM annotations were systematic and reflected known theoretical challenges in metaphor identification.

Conclusion: LLMs can partially automate metaphor identification and serve as testbeds for developing and refining metaphor identification protocols and underlying theory.

Abstract: Metaphor is a pervasive feature of discourse and a powerful lens for
examining cognition, emotion, and ideology. Large-scale analysis, however, has
been constrained by the need for manual annotation due to the context-sensitive
nature of metaphor. This study investigates the potential of large language
models (LLMs) to automate metaphor identification in full texts. We compare
three methods: (i) retrieval-augmented generation (RAG), where the model is
provided with a codebook and instructed to annotate texts based on its rules
and examples; (ii) prompt engineering, where we design task-specific verbal
instructions; and (iii) fine-tuning, where the model is trained on hand-coded
texts to optimize performance. Within prompt engineering, we test zero-shot,
few-shot, and chain-of-thought strategies. Our results show that
state-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning
yielding a median F1 score of 0.79. A comparison of human and LLM outputs
reveals that most discrepancies are systematic, reflecting well-known grey
areas and conceptual challenges in metaphor theory. We propose that LLMs can be
used to at least partly automate metaphor identification and can serve as a
testbed for developing and refining metaphor identification protocols and the
theory that underpins them.

</details>


### [169] [Expanding Computation Spaces of LLMs at Inference Time](https://arxiv.org/abs/2509.24884)
*Yoonna Jang,Kisu Yang,Isabelle Augenstein*

Main category: cs.CL

TL;DR: Language models can use artificially inserted filler tokens at inference time to expand computational space, improving performance especially for smaller models by up to 12.4 percentage points.


<details>
  <summary>Details</summary>
Motivation: To investigate whether language models can leverage additional computational space through filler tokens inserted solely at inference, rather than requiring training of special tokens.

Method: Systematically tested different token types, numbers, and insertion locations; analyzed when during training models learn to use these spaces; examined attention dynamics in expanded spaces.

Result: Appropriate token types and counts vary, but placing filler tokens before the final 'Answer:' token works best. Smaller models benefit most (up to 12.4% improvement), showing these spaces provide genuine computational capacity.

Conclusion: Filler tokens at inference act as meaningful computational capacity, with attention patterns showing continued reasoning processes and focus on relevant problem elements.

Abstract: Chain-of-thought (CoT) rationale enables language models to use additional
task-related text for problem-solving, benefiting not only from detailed
reasoning steps but also from the expanded computational space of longer
inputs. Prior work has trained filler or special tokens to serve as additional
computation spaces. In this study, we investigate whether language models can
leverage artificially inserted sequences of filler tokens solely at inference.
We first identify effective token types, numbers, and insertion locations, then
examine at what stage of training models begin to exploit the expanded
computation space, and finally analyze dynamics within these spaces via
attention maps. Experiments on models ranging from 1.7B to 32B across
open-domain QA and math tasks show that appropriate token types and counts
vary, but placing filler tokens directly before the final 'Answer:' token is
most effective. Smaller models benefit most, up to 12.372 percentage points in
SmolLM2-1.7B-Instruct, indicating that these spaces act as additional
computational capacity rather than redundant input. Attention maps reveal that
expanded spaces often continue the original attention mechanism and sometimes
focus on questions or answer options, suggesting meaningful computation for
problem-solving.

</details>


### [170] [BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees and Notifications](https://arxiv.org/abs/2509.24908)
*Andrés Fernández García,Javier de la Rosa,Julio Gonzalo,Roser Morante,Enrique Amigó,Alejandro Benito-Santos,Jorge Carrillo-de-Albornoz,Víctor Fresno,Adrian Ghajari,Guillermo Marco,Laura Plaza,Eva Sánchez Salido*

Main category: cs.CL

TL;DR: BOE-XSUM is a new Spanish legal document summarization dataset with 3,648 entries from Spain's Official Gazette. Fine-tuned LLMs significantly outperform zero-shot models, with BERTIN GPT-J 6B achieving 41.6% accuracy vs 33.5% for the best zero-shot model.


<details>
  <summary>Details</summary>
Motivation: There is a lack of concise Spanish document summaries, especially in the legal domain, despite growing information overload needs.

Method: Created BOE-XSUM dataset with summaries of Spanish Official Gazette documents, then fine-tuned medium-sized LLMs and compared them to zero-shot general-purpose models.

Result: Fine-tuned models significantly outperformed zero-shot models. BERTIN GPT-J 6B achieved 41.6% accuracy, a 24% improvement over the best zero-shot model (DeepSeek-R1 at 33.5%).

Conclusion: Fine-tuning specialized models on domain-specific datasets like BOE-XSUM yields substantial performance gains for Spanish legal document summarization.

Abstract: The ability to summarize long documents succinctly is increasingly important
in daily life due to information overload, yet there is a notable lack of such
summaries for Spanish documents in general, and in the legal domain in
particular. In this work, we present BOE-XSUM, a curated dataset comprising
3,648 concise, plain-language summaries of documents sourced from Spain's
``Bolet\'{\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each
entry in the dataset includes a short summary, the original text, and its
document type label. We evaluate the performance of medium-sized large language
models (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose
generative models in a zero-shot setting. Results show that fine-tuned models
significantly outperform their non-specialized counterparts. Notably, the
best-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\%
performance gain over the top zero-shot model, DeepSeek-R1 (accuracies of
41.6\% vs.\ 33.5\%).

</details>


### [171] [How Well Do LLMs Imitate Human Writing Style?](https://arxiv.org/abs/2509.24930)
*Rebira Jemama,Rajesh Kumar*

Main category: cs.CL

TL;DR: A training-free framework for authorship verification and style imitation analysis that combines TF-IDF character n-grams with transformer embeddings, achieving high accuracy while significantly reducing computational resources. Used to evaluate LLMs' style imitation capabilities.


<details>
  <summary>Details</summary>
Motivation: To understand whether large language models can effectively replicate specific human author styles, and to develop an efficient method for authorship verification without requiring supervised training.

Method: Integrates TF-IDF character n-grams with transformer embeddings, classifies text pairs through empirical distance distributions, and evaluates LLMs across different prompting strategies (zero-shot, one-shot, few-shot, text completion).

Result: Achieved 97.5% accuracy on academic essays and 94.5% in cross-domain evaluation, with 91.8% training time reduction and 59% memory usage reduction. Found that prompting strategy has greater influence than model size, with few-shot prompting yielding 23.5x higher style-matching accuracy than zero-shot.

Conclusion: High-fidelity style imitation by LLMs doesn't imply human-like unpredictability, as human essays have higher perplexity (29.5) than matched LLM outputs (15.2). Stylistic fidelity and statistical detectability are separable, providing a basis for future authorship modeling and detection work.

Abstract: Large language models (LLMs) can generate fluent text, but their ability to
replicate the distinctive style of a specific human author remains unclear. We
present a fast, training-free framework for authorship verification and style
imitation analysis. The method integrates TF-IDF character n-grams with
transformer embeddings and classifies text pairs through empirical distance
distributions, eliminating the need for supervised training or threshold
tuning. It achieves 97.5\% accuracy on academic essays and 94.5\% in
cross-domain evaluation, while reducing training time by 91.8\% and memory
usage by 59\% relative to parameter-based baselines. Using this framework, we
evaluate five LLMs from three separate families (Llama, Qwen, Mixtral) across
four prompting strategies - zero-shot, one-shot, few-shot, and text completion.
Results show that the prompting strategy has a more substantial influence on
style fidelity than model size: few-shot prompting yields up to 23.5x higher
style-matching accuracy than zero-shot, and completion prompting reaches 99.9\%
agreement with the original author's style. Crucially, high-fidelity imitation
does not imply human-like unpredictability - human essays average a perplexity
of 29.5, whereas matched LLM outputs average only 15.2. These findings
demonstrate that stylistic fidelity and statistical detectability are
separable, establishing a reproducible basis for future work in authorship
modeling, detection, and identity-conditioned generation.

</details>


### [172] [MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes](https://arxiv.org/abs/2509.24945)
*Changsheng Zhao,Ernie Chang,Zechun Liu,Chia-Jung Chang,Wei Wen,Chen Lai,Rick Cao,Yuandong Tian,Raghuraman Krishnamoorthi,Yangyang Shi,Vikas Chandra*

Main category: cs.CL

TL;DR: This paper challenges the assumption that reasoning capabilities in LLMs require massive datasets (>10T tokens), showing that high-quality data curation and resampling can achieve strong reasoning with only ~2T tokens.


<details>
  <summary>Details</summary>
Motivation: To question the prevailing assumption that reasoning capabilities in LLMs necessitate training on extremely large corpora (>10T tokens), and demonstrate that careful data curation can achieve similar results with far less data.

Method: Curated and resampled open-source datasets using designed metrics to identify beneficial data, then pre-trained on 4.2T tokens from these ~2T high-quality tokens, followed by established post-training procedures.

Result: Developed MobileLLM-R1 series (sub-billion-parameter models) that substantially outperform prior models on reasoning benchmarks. MobileLLM-R1-950M achieved AIME score of 15.5 vs 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B, matching/surpassing Qwen3-0.6B despite using only 11.7% of its training tokens.

Conclusion: Strong reasoning capabilities can emerge with far less data than previously assumed through careful data curation and resampling, challenging the necessity of massive training corpora for reasoning emergence in LLMs.

Abstract: The paradigm shift in large language models (LLMs) from instinctive responses
to chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1)
reasoning capabilities only emerge in sufficiently large models, and (2) such
capabilities require training on massive datasets. While the first assumption
has already been challenged by recent sub-billion-parameter reasoning models
such as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely
unquestioned. In this work, we revisit the necessity of scaling to extremely
large corpora (>10T tokens) for reasoning emergence. By carefully curating and
resampling open-source datasets that we identify as beneficial under our
designed metrics, we demonstrate that strong reasoning abilities can emerge
with far less data. Specifically, we show that only ~2T tokens of high-quality
data are sufficient, and pre-training with 4.2T tokens on the dataset resampled
from these ~2T tokens, followed by a established post-training procedure,
enables the development of MobileLLM-R1, a series of sub-billion-parameter
reasoning models that substantially outperform prior models trained on fully
open-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of
15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B.
Remarkably, despite being trained on only 11.7% of the tokens compared to
Qwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches
or surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate
further research in this direction, we have released the complete training
recipe, data sources, data mixing ratio, and model checkpoints, together with
the key insights obtained throughout this study.

</details>


### [173] [The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents' Inquiry Capability](https://arxiv.org/abs/2509.24958)
*Linlu Gong,Ante Wang,Yunghwei Lai,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: MAQuE is a comprehensive benchmark for evaluating medical AI doctors' questioning abilities, featuring 3,000 simulated patient agents with diverse characteristics and a multi-faceted evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Current AI doctors focus mainly on diagnostic skills but overlook other essential physician qualities like empathy, patience, and communication. There's a need to evaluate AI's comprehensive questioning capabilities in realistic medical scenarios.

Method: Created MAQuE benchmark with 3,000 realistically simulated patient agents exhibiting diverse linguistic patterns, cognitive limitations, emotional responses, and passive disclosure tendencies. Introduced multi-faceted evaluation framework covering task success, inquiry proficiency, dialogue competence, inquiry efficiency, and patient experience.

Result: Experiments on different LLMs revealed substantial challenges across all evaluation aspects. Even state-of-the-art models show significant room for improvement in inquiry capabilities. Models are highly sensitive to realistic patient behavior variations, which considerably impacts diagnostic accuracy. Fine-grained metrics expose trade-offs between different evaluation perspectives.

Conclusion: The study highlights the challenge of balancing performance and practicality in real-world clinical settings, demonstrating that current AI medical agents need significant improvement in comprehensive questioning capabilities beyond just diagnostic skills.

Abstract: An effective physician should possess a combination of empathy, expertise,
patience, and clear communication when treating a patient. Recent advances have
successfully endowed AI doctors with expert diagnostic skills, particularly the
ability to actively seek information through inquiry. However, other essential
qualities of a good doctor remain overlooked. To bridge this gap, we present
MAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the
automatic and comprehensive evaluation of medical multi-turn questioning. It
features 3,000 realistically simulated patient agents that exhibit diverse
linguistic patterns, cognitive limitations, emotional responses, and tendencies
for passive disclosure. We also introduce a multi-faceted evaluation framework,
covering task success, inquiry proficiency, dialogue competence, inquiry
efficiency, and patient experience. Experiments on different LLMs reveal
substantial challenges across the evaluation aspects. Even state-of-the-art
models show significant room for improvement in their inquiry capabilities.
These models are highly sensitive to variations in realistic patient behavior,
which considerably impacts diagnostic accuracy. Furthermore, our fine-grained
metrics expose trade-offs between different evaluation perspectives,
highlighting the challenge of balancing performance and practicality in
real-world clinical settings.

</details>


### [174] [SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems](https://arxiv.org/abs/2509.24961)
*Kaihong Li,Huichi Zhou,Bin Ma,Fangjun Huang*

Main category: cs.CL

TL;DR: SemanticShield is a two-stage framework that uses LLMs to detect shilling attacks in recommender systems by analyzing item-side semantic features alongside user behaviors.


<details>
  <summary>Details</summary>
Motivation: Existing defenses focus mainly on user behaviors but ignore item-side features like titles and descriptions that can reveal malicious intent in shilling attacks.

Method: Two-stage detection: first pre-screens suspicious users using behavioral criteria, then employs LLM-based auditing to evaluate semantic consistency. Enhanced through reinforcement fine-tuning with specialized reward functions.

Result: Experiments on six attack strategies show SemanticShield effectively detects shilling attacks, and further evaluation demonstrates strong generalization to unseen attack methods.

Conclusion: SemanticShield provides an effective defense against shilling attacks by leveraging item-side semantics through LLMs, offering both detection accuracy and generalization capability.

Abstract: Recommender systems (RS) are widely used in e-commerce for personalized
suggestions, yet their openness makes them susceptible to shilling attacks,
where adversaries inject fake behaviors to manipulate recommendations. Most
existing defenses emphasize user-side behaviors while overlooking item-side
features such as titles and descriptions that can expose malicious intent. To
address this gap, we propose a two-stage detection framework that integrates
item-side semantics via large language models (LLMs). The first stage
pre-screens suspicious users using low-cost behavioral criteria, and the second
stage employs LLM-based auditing to evaluate semantic consistency. Furthermore,
we enhance the auditing model through reinforcement fine-tuning on a
lightweight LLM with carefully designed reward functions, yielding a
specialized detector called SemanticShield. Experiments on six representative
attack strategies demonstrate the effectiveness of SemanticShield against
shilling attacks, and further evaluation on previously unseen attack methods
shows its strong generalization capability. Code is available at
https://github.com/FrankenstLee/SemanticShield.

</details>


### [175] [Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns](https://arxiv.org/abs/2509.24988)
*Hanqi Xiao,Vaidehi Patil,Hyunji Lee,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: Training Generalized Correctness Models (GCMs) on historical correctness data from multiple LLMs enables reliable confidence estimation that generalizes across models and datasets, challenging the notion that confidence requires model-specific self-introspection.


<details>
  <summary>Details</summary>
Motivation: Accurate confidence estimation is critical for deploying LLMs in high-stakes applications, but current approaches based on self-knowledge assumption may not be optimal. The research aims to develop more effective confidence estimation methods.

Method: Proposed Generalized Correctness Models (GCMs) trained on historical correctness data from multiple LLMs, with systematic experiments controlling training data and exploring alternative methods like in-context examples and post-hoc calibration.

Result: GCMs demonstrate generalizable correctness prediction across 5 model families and datasets (MMLU, TriviaQA), with answer phrasing identified as a strong predictor. In-context history and calibration provide complementary improvements.

Conclusion: Reliable LLM confidence estimation is a generalizable skill learned from systematic encoding of correctness history, rather than a model-specific skill dependent on self-introspection.

Abstract: Generating accurate and calibrated confidence estimates is critical for
deploying LLMs in high-stakes or user-facing applications, and remains an open
challenge. Prior research has often framed confidence as a problem of eliciting
a model's "self-knowledge", i.e., the ability of an LLM to judge whether its
own answers are correct; this approach implicitly assumes that there is some
privileged information about the answer's correctness that is accessible to the
model itself. However, our experiments reveal that an LLM attempting to predict
the correctness of its own outputs generally performs no better than an
unrelated LLM. Moreover, we hypothesize that a key factor in building a
"Correctness Model" (CM) is exposure to a target model's historical
predictions. We propose multiple methods to inject this historical correctness
information, creating a Generalized Correctness Model (GCM). We first show that
GCMs can be trained on the correctness data from many LLMs and learn patterns
for correctness prediction applicable across datasets and models. We then use
CMs as a lens for studying the source of correctness prediction ability and its
generalization, systematically controlling their training data and finding that
answer phrasing is a strong predictor for correctness. We further explore
alternative methods of injecting history without training an LLM, finding that
including history as in-context examples can help improve correctness
prediction, and post-hoc calibration can provide complementary reductions in
calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families
and the MMLU and TriviaQA datasets, as well as on a downstream selective
prediction task, finding that reliable LLM confidence estimation is a
generalizable and model-agnostic skill learned by systematically encoding
correctness history rather than a model-specific skill reliant on
self-introspection.

</details>


### [176] [Circuit Distillation](https://arxiv.org/abs/2509.25002)
*Somin Wadhwa,Silvio Amir,Byron C. Wallace*

Main category: cs.CL

TL;DR: Circuit distillation aligns internal representations between teacher and student models' circuit components, outperforming standard behavioral mimicry distillation on entity tracking and theory of mind tasks.


<details>
  <summary>Details</summary>
Motivation: Standard model distillation treats teacher's internal computations as black box, focusing only on output mimicry. This work aims to distill the underlying computational mechanisms themselves.

Method: Proposes circuit distillation with objective to align internal representations between functionally correspondent circuit components in teacher and student models, using a loss reflecting representation similarities.

Result: Circuit distillation outperforms standard distillation on entity tracking and theory of mind tasks using Llama3 models, successfully transferring algorithmic capabilities by adjusting only small, targeted parameter subsets.

Conclusion: Establishes feasibility of transferring mechanisms, enabling efficient distillation of targeted teacher capabilities via interpretable and controllable internal student mechanisms.

Abstract: Model distillation typically focuses on behavioral mimicry, where a student
model is trained to replicate a teacher's output while treating its internal
computations as a black box. In this work we propose an alternative approach:
Distilling the underlying computational mechanisms implemented by a teacher
model. Specifically, we propose circuit distillation, which introduces an
objective to align internal representations between analogous circuit
components in teacher and student models. We propose a method to match
``functionally correspondent'' circuit components and introduce a loss
reflecting similarities between the representations that these induce. We
evaluate circuit distillation on entity tracking and theory of mind (ToM) tasks
using models from the Llama3 family. Our results demonstrate that circuit
distillation outperforms standard distillation, successfully transferring
algorithmic capabilities by adjusting only a small, targeted subset of student
model parameters. This work establishes the feasibility of transferring
mechanisms, which may in turn allow for efficient distillation of targeted
teacher capabilities via interpretable and controllable internal student
mechanisms.

</details>


### [177] [Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct](https://arxiv.org/abs/2509.25035)
*Haoyang Zheng,Xinyang Liu,Cindy Xiangrui Kong,Nan Jiang,Zheyuan Hu,Weijian Luo,Wei Deng,Guang Lin*

Main category: cs.CL

TL;DR: DiDi-Instruct is a training-based method that accelerates language generation by initializing from pre-trained discrete diffusion language models, achieving 64x speedup while outperforming GPT-2 and standard dLLMs.


<details>
  <summary>Details</summary>
Motivation: Fast generation of language texts is the holy grail that people pursue in the AI era, aiming to achieve efficient language generation with minimal computational overhead.

Method: DiDi-Instruct builds on integral KL-divergence minimization framework with practical training algorithms, using techniques like grouped reward normalization, intermediate-state matching, and reward-guided ancestral sampler (RGAS) to improve training stability and inference performance.

Result: On OpenWebText, DiDi-Instruct achieves sample perplexities from 62.2 (8 NFEs) to 18.4 (128 NFEs), outperforming all accelerated language generation models with only 1% entropy loss and 20x less training time compared to standard methods.

Conclusion: DiDi-Instruct is an efficient yet effective distillation method that enables fast language generation with significant performance gains and minimal computational overhead.

Abstract: Fast generation of language texts is the holy grail that people pursue in the
AI era. In this work, we introduced Discrete Diffusion Divergence Instruct
(DiDi-Instruct), a training-based method that leads to fast language generation
models by initializing from a pre-trained (masked) discrete diffusion language
model (dLLM). The resulting DiDi-Instruct model outperforms the dLLM
counterparts and the GPT-2 baseline with 64x acceleration. In the theoretical
part of the paper, we build the foundation of DiDi-Instruct in a framework of
integral KL-divergence minimization, with practical training algorithms. We
also introduce techniques like grouped reward normalization, intermediate-state
matching, and the reward-guided ancestral sampler (RGAS) that significantly
improve the training stability, the model coverage, and the inference
performances. On OpenWebText, DiDi-Instruct outperforms all accelerated
language generation models as well as the GPT-2 baseline and the standard
dLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128
NFEs). These performance gains are accomplished with a negligible entropy loss
of about 1% and 20x less additional training wall-clock time. We further
validate the robustness and effectiveness of DiDi-Instruct through extensive
ablation studies, model scaling, and the generation of discrete protein
sequences. In conclusion, DiDi-Instruct is an efficient yet effective
distillation method, enabling language generation in the blink of an eye. We
will release both code and models at github.com/haoyangzheng-ai/didi-instruct.

</details>


### [178] [GateMABSA: Aspect-Image Gated Fusion for Multimodal Aspect-based Sentiment Analysis](https://arxiv.org/abs/2509.25037)
*Adamu Lawan,Haruna Yunusa*

Main category: cs.CL

TL;DR: GateMABSA is a novel gated multimodal architecture for aspect-based sentiment analysis that uses three specialized mLSTMs to handle syntactic structure, aspect-semantic relevance, and selective multimodal fusion, outperforming existing methods on Twitter datasets.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal ABSA models struggle with filtering noisy visual signals and effectively aligning aspects with opinion-bearing content across text and image modalities.

Method: Proposes GateMABSA with three specialized mLSTMs: Syn-mLSTM for syntactic structure, Sem-mLSTM for aspect-semantic relevance, and Fuse-mLSTM for selective multimodal fusion.

Result: Extensive experiments on two benchmark Twitter datasets demonstrate that GateMABSA outperforms several baselines.

Conclusion: GateMABSA effectively addresses the challenges of noisy visual signals and cross-modal alignment in multimodal aspect-based sentiment analysis.

Abstract: Aspect-based Sentiment Analysis (ABSA) has recently advanced into the
multimodal domain, where user-generated content often combines text and images.
However, existing multimodal ABSA (MABSA) models struggle to filter noisy
visual signals, and effectively align aspects with opinion-bearing content
across modalities. To address these challenges, we propose GateMABSA, a novel
gated multimodal architecture that integrates syntactic, semantic, and
fusion-aware mLSTM. Specifically, GateMABSA introduces three specialized
mLSTMs: Syn-mLSTM to incorporate syntactic structure, Sem-mLSTM to emphasize
aspect--semantic relevance, and Fuse-mLSTM to perform selective multimodal
fusion. Extensive experiments on two benchmark Twitter datasets demonstrate
that GateMABSA outperforms several baselines.

</details>


### [179] [Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures](https://arxiv.org/abs/2509.25045)
*Marco Bronzini,Carlo Nicolini,Bruno Lepri,Jacopo Staiano,Andrea Passerini*

Main category: cs.CL

TL;DR: This paper introduces Hyperdimensional Probe, a novel method for decoding interpretable concepts from LLM vector spaces using Vector Symbolic Architectures, overcoming limitations of existing interpretability methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM interpretability methods like DLA and SAEs provide limited insight due to vocabulary constraints and unclear feature names, creating a need for better ways to understand internal representations.

Method: The method projects LLM's residual stream into interpretable concepts using Vector Symbolic Architectures, combining strengths of SAEs and conventional probes while overcoming their limitations.

Result: Experiments show the probe reliably extracts meaningful concepts across varied LLMs, embedding sizes, and input domains, and helps identify LLM failures in controlled tasks and question-answering settings.

Conclusion: The work advances information decoding in LLM vector space, enabling extraction of more informative, interpretable, and structured features from neural representations.

Abstract: Despite their capabilities, Large Language Models (LLMs) remain opaque with
limited understanding of their internal representations. Current
interpretability methods, such as direct logit attribution (DLA) and sparse
autoencoders (SAEs), provide restricted insight due to limitations such as the
model's output vocabulary or unclear feature names. This work introduces
Hyperdimensional Probe, a novel paradigm for decoding information from the LLM
vector space. It combines ideas from symbolic representations and neural
probing to project the model's residual stream into interpretable concepts via
Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs
and conventional probes while overcoming their key limitations. We validate our
decoding paradigm with controlled input-completion tasks, probing the model's
final state before next-token prediction on inputs spanning syntactic pattern
recognition, key-value associations, and abstract inference. We further assess
it in a question-answering setting, examining the state of the model both
before and after text generation. Our experiments show that our probe reliably
extracts meaningful concepts across varied LLMs, embedding sizes, and input
domains, also helping identify LLM failures. Our work advances information
decoding in LLM vector space, enabling extracting more informative,
interpretable, and structured features from neural representations.

</details>


### [180] [Confidence-Guided Error Correction for Disordered Speech Recognition](https://arxiv.org/abs/2509.25048)
*Abner Hernandez,Tomás Arias Vergara,Andreas Maier,Paula Andrea Pérez-Toro*

Main category: cs.CL

TL;DR: The paper proposes confidence-informed prompting for LLMs to correct ASR errors in disordered speech, achieving significant WER reductions.


<details>
  <summary>Details</summary>
Motivation: To improve ASR error correction for disordered speech by leveraging LLMs with word-level uncertainty estimates to enhance robustness and reduce overcorrection.

Method: Fine-tune LLaMA 3.1 with confidence-informed prompting that embeds word-level uncertainty estimates directly into LLM training, comparing against transcript-only fine-tuning and post hoc confidence-based filtering.

Result: Achieved 10% relative WER reduction on Speech Accessibility Project spontaneous speech and 47% reduction on TORGO dataset compared to naive LLM correction.

Conclusion: Confidence-aware fine-tuning is effective for impaired speech ASR correction, with confidence-informed prompting outperforming baseline methods across different datasets.

Abstract: We investigate the use of large language models (LLMs) as post-processing
modules for automatic speech recognition (ASR), focusing on their ability to
perform error correction for disordered speech. In particular, we propose
confidence-informed prompting, where word-level uncertainty estimates are
embedded directly into LLM training to improve robustness and generalization
across speakers and datasets. This approach directs the model to uncertain ASR
regions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare
our approach to both transcript-only fine-tuning and post hoc confidence-based
filtering. Evaluations show that our method achieves a 10% relative WER
reduction compared to naive LLM correction on the Speech Accessibility Project
spontaneous speech and a 47% reduction on TORGO, demonstrating the
effectiveness of confidence-aware fine-tuning for impaired speech.

</details>


### [181] [An empirical study on the limitation of Transformers in program trace generation](https://arxiv.org/abs/2509.25073)
*Simeng Sun*

Main category: cs.CL

TL;DR: Small Transformers trained on program trace generation show strong in-distribution accuracy but systematic generalization failures across program length and trace steps, with some architectural modifications improving generalization.


<details>
  <summary>Details</summary>
Motivation: To study Transformers' reasoning capabilities through program trace generation, which externalizes reasoning via long traces with trivial individual steps.

Method: Train small Transformers with diverse modifications including alternative position encodings, softmax replacements, hybrid models, and short convolutions on the program trace generation task.

Result: Models achieve strong in-distribution accuracy but exhibit systematic generalization failures when tested on factors like program length and trace steps. Some architectural designs significantly improve generalization.

Conclusion: While Transformers perform well on in-distribution program trace generation, they struggle with systematic generalization, though certain architectural modifications can enhance their generalization capabilities.

Abstract: We study Transformers on the task \emph{program trace generation} (PTG),
where models produce step-by-step execution traces for synthetic programs.
Unlike existing algorithmic problems, PTG externalizes reasoning through long
traces where each step is trivial. We train small Transformers with diverse
modifications, including alternative position encodings, softmax replacements,
hybrid model, and short convolutions. While these models achieve strong
in-distribution accuracy, they exhibit systematic failures when generalizing to
various factors (e.g., program length, trace steps), though some designs
significantly improve generalization.

</details>


### [182] [Scaling Generalist Data-Analytic Agents](https://arxiv.org/abs/2509.25084)
*Shuofei Qiao,Yanqiu Zhao,Zhisong Qiu,Xiaobin Wang,Jintian Zhang,Zhao Bin,Ningyu Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Huajun Chen*

Main category: cs.CL

TL;DR: DataMind introduces a scalable data synthesis and agent training recipe to build generalist data-analytic agents, addressing challenges in open-source models through diverse task synthesis, knowledge-augmented sampling, dynamic training objectives, and stable multi-turn rollout.


<details>
  <summary>Details</summary>
Motivation: Current data-analytic agents heavily rely on proprietary models with prompt engineering, while open-source models struggle with diverse-format large-scale data and multi-step reasoning required for real-world analytics.

Method: 1) Fine-grained task taxonomy with recursive easy-to-hard composition 2) Knowledge-augmented trajectory sampling with filtering 3) Dynamic training combining SFT and RL losses 4) Memory-frugal stable code-based multi-turn rollout framework

Result: DataMind-14B achieves state-of-the-art 71.16% average score on data analysis benchmarks, outperforming proprietary models like DeepSeek-V3.1 and GPT-5. DataMind-7B performs best among open-source models with 68.10% score.

Conclusion: DataMind successfully addresses key challenges in building open-source data-analytic agents and provides a scalable solution that outperforms proprietary models, with released datasets and models for community research.

Abstract: Data-analytic agents are emerging as a key catalyst for automated scientific
discovery and for the vision of Innovating AI. Current approaches, however,
rely heavily on prompt engineering over proprietary models, while open-source
models struggle to face diverse-format, large-scale data files and
long-horizon, multi-step reasoning that real-world analytics demands. This
paper introduces DataMind, a scalable data synthesis and agent training recipe
designed to build generalist data-analytic agents. DataMind tackles three key
challenges in building open-source data-analytic agents, including insufficient
data resources, improper training strategy, and unstable code-based multi-turn
rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a
recursive easy-to-hard task composition mechanism to increase the diversity and
difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling
strategy followed by model-based and rule-based filtering; 3) a dynamically
adjustable training objective combining both SFT and RL losses; 4) a
memory-frugal and stable code-based multi-turn rollout framework. Built on
DataMind, we curate DataMind-12K, a high-quality trajectory set spanning
diverse domains, task categories, and data file formats for data-analytic
tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with
an average score of 71.16% on multiple data analysis benchmarks, outperforming
the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B
also performs best among all open-source models with a score of 68.10%. We also
incorporate some empirical insights gained from our exploratory trials into the
analysis experiments, aiming to provide actionable insights about agentic
training for the community. We will release DataMind-12K and DataMind-7B,14B
for the community's future research.

</details>


### [183] [jina-reranker-v3: Last but Not Late Interaction for Document Reranking](https://arxiv.org/abs/2509.25085)
*Feng Wang,Yuqing Li,Han Xiao*

Main category: cs.CL

TL;DR: jina-reranker-v3 is a 0.6B parameter multilingual document reranker that uses a novel 'last but not late interaction' approach, achieving state-of-the-art BEIR performance with 61.94 nDCG@10 while being 10x smaller than generative listwise rerankers.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient reranker that enables rich cross-document interactions while maintaining compact architecture, addressing limitations of late interaction models like ColBERT.

Method: Introduces 'last but not late interaction' - conducts causal self-attention between query and documents within the same context window, then extracts contextual embeddings from the last token of each document.

Result: Achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while being ten times smaller than generative listwise rerankers.

Conclusion: The proposed compact architecture with novel interaction mechanism successfully balances performance and efficiency, demonstrating superior reranking capabilities with significantly reduced model size.

Abstract: jina-reranker-v3 is a 0.6B parameter multilingual document reranker that
introduces a novel last but not late interaction. Unlike late interaction
models such as ColBERT that perform separate encoding followed by multi-vector
matching, our approach conducts causal self-attention between query and
documents within the same context window, enabling rich cross-document
interactions before extracting contextual embeddings from the last token of
each document. This compact architecture achieves state-of-the-art BEIR
performance with 61.94 nDCG@10 while being ten times smaller than generative
listwise rerankers.

</details>


### [184] [Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small LLMs](https://arxiv.org/abs/2509.25086)
*Akio Hayakawa,Stefan Bott,Horacio Saggion*

Main category: cs.CL

TL;DR: Proposes efficient lexical simplification framework using small LLMs for privacy-sensitive environments, identifies safety trade-offs in knowledge distillation, and develops filtering strategy using model probabilities to detect harmful simplifications.


<details>
  <summary>Details</summary>
Motivation: Address challenges of LLMs in real-world lexical simplification applications, especially for vulnerable user groups in privacy-sensitive and resource-constrained environments where safety and correctness are crucial.

Method: Utilizes small LLMs deployable locally, explores knowledge distillation with synthesized data and in-context learning as baselines, and proposes filtering strategy using model output probabilities to detect harmful simplifications.

Result: Knowledge distillation boosts automatic metric scores but introduces safety trade-off by increasing harmful simplifications. Model's output probability is effective for detecting harmful simplifications. Filtering strategy suppresses harmful simplifications while preserving beneficial ones.

Conclusion: Establishes benchmark for efficient and safe lexical simplification with small LLMs, highlighting key trade-offs between performance, efficiency, and safety, and demonstrates promising approach for safe real-world deployment.

Abstract: Despite their strong performance, large language models (LLMs) face
challenges in real-world application of lexical simplification (LS),
particularly in privacy-sensitive and resource-constrained environments.
Moreover, since vulnerable user groups (e.g., people with disabilities) are one
of the key target groups of this technology, it is crucial to ensure the safety
and correctness of the output of LS systems. To address these issues, we
propose an efficient framework for LS systems that utilizes small LLMs
deployable in local environments. Within this framework, we explore knowledge
distillation with synthesized data and in-context learning as baselines. Our
experiments in five languages evaluate model outputs both automatically and
manually. Our manual analysis reveals that while knowledge distillation boosts
automatic metric scores, it also introduces a safety trade-off by increasing
harmful simplifications. Importantly, we find that the model's output
probability is a useful signal for detecting harmful simplifications.
Leveraging this, we propose a filtering strategy that suppresses harmful
simplifications while largely preserving beneficial ones. This work establishes
a benchmark for efficient and safe LS with small LLMs. It highlights the key
trade-offs between performance, efficiency, and safety, and demonstrates a
promising approach for safe real-world deployment.

</details>


### [185] [Towards Personalized Deep Research: Benchmarks and Evaluations](https://arxiv.org/abs/2509.25106)
*Yuan Liang,Jiaxian Li,Yuqing Wang,Piaohong Wang,Motong Tian,Pai Liu,Shuofei Qiao,Runnan Fang,He Zhu,Ge Zhang,Minghao Liu,Yuchen Eleanor Jiang,Ningyu Zhang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: The paper introduces Personalized Deep Research Bench, the first benchmark for evaluating personalization in Deep Research Agents (DRAs), featuring 50 research tasks across 10 domains paired with 25 user profiles, and proposes the PQR Evaluation Framework to measure Personalization Alignment, Content Quality, and Factual Reliability.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for Deep Research Agents mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios, creating a gap in assessing truly personalized AI research assistants.

Method: The authors introduce Personalized Deep Research Bench with 50 diverse research tasks across 10 domains paired with 25 authentic user profiles combining structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. They propose the PQR Evaluation Framework to jointly measure Personalization Alignment, Content Quality, and Factual Reliability.

Result: Experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research, demonstrating the benchmark's effectiveness in assessing system performance.

Conclusion: This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants by providing the first comprehensive benchmark for personalization in Deep Research Agents.

Abstract: Deep Research Agents (DRAs) can autonomously conduct complex investigations
and generate comprehensive reports, demonstrating strong real-world potential.
However, existing evaluations mostly rely on close-ended benchmarks, while
open-ended deep research benchmarks remain scarce and typically neglect
personalized scenarios. To bridge this gap, we introduce Personalized Deep
Research Bench, the first benchmark for evaluating personalization in DRAs. It
pairs 50 diverse research tasks across 10 domains with 25 authentic user
profiles that combine structured persona attributes with dynamic real-world
contexts, yielding 250 realistic user-task queries. To assess system
performance, we propose the PQR Evaluation Framework, which jointly measures
(P) Personalization Alignment, (Q) Content Quality, and (R) Factual
Reliability. Our experiments on a range of systems highlight current
capabilities and limitations in handling personalized deep research. This work
establishes a rigorous foundation for developing and evaluating the next
generation of truly personalized AI research assistants.

</details>


### [186] [Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant for Question Answering in the Era of LLMs?](https://arxiv.org/abs/2509.25107)
*Kai Sun,Yin Huang,Srishti Mehra,Mohammad Kachuee,Xilun Chen,Renjie Tao,Zhaojiang Lin,Andrea Jessee,Nirav Shah,Alex Betty,Yue Liu,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: This paper investigates whether knowledge triple extraction remains useful for web-based QA in the era of LLMs, finding that despite high QA accuracy, LLMs still benefit from knowledge extraction through augmentation and multi-task learning.


<details>
  <summary>Details</summary>
Motivation: With LLMs advancing web-based QA systems, the paper questions the continued utility of knowledge extraction for QA and aims to determine its value in this new paradigm.

Method: Extends an existing benchmark with knowledge extraction annotations and evaluates commercial and open-source LLMs of varying sizes, testing augmentation with extracted triples and multi-task learning approaches.

Result: Web-scale knowledge extraction remains challenging for LLMs, but LLMs can benefit from knowledge extraction through triple augmentation and multi-task learning, even when achieving high QA accuracy.

Conclusion: Knowledge triple extraction still has value in web-based QA systems, and the findings provide strategies for maximizing LLM effectiveness across different model sizes and resource constraints.

Abstract: The advent of Large Language Models (LLMs) has significantly advanced
web-based Question Answering (QA) systems over semi-structured content, raising
questions about the continued utility of knowledge extraction for question
answering. This paper investigates the value of triple extraction in this new
paradigm by extending an existing benchmark with knowledge extraction
annotations and evaluating commercial and open-source LLMs of varying sizes.
Our results show that web-scale knowledge extraction remains a challenging task
for LLMs. Despite achieving high QA accuracy, LLMs can still benefit from
knowledge extraction, through augmentation with extracted triples and
multi-task learning. These findings provide insights into the evolving role of
knowledge triple extraction in web-based QA and highlight strategies for
maximizing LLM effectiveness across different model sizes and resource
settings.

</details>


### [187] [Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection](https://arxiv.org/abs/2509.25138)
*Ivan Vykopal,Antonia Karamolegkou,Jaroslav Kopčan,Qiwei Peng,Tomáš Javůrek,Michal Gregor,Marián Šimko*

Main category: cs.CL

TL;DR: This paper studies language bias and retrieval bias in multilingual LLMs for cross-lingual fact-checking, evaluating 6 models across 20 languages using AMC-16K dataset and multilingual prompting.


<details>
  <summary>Details</summary>
Motivation: Multilingual LLMs show language bias, performing better on high-resource languages like English than low-resource languages, and retrieval systems exhibit retrieval bias by favoring certain information over others.

Method: Evaluated 6 open-source multilingual LLMs across 20 languages using fully multilingual prompting strategy with AMC-16K dataset, translated prompts into each language, and used multilingual embedding models to analyze retrieval bias.

Result: Found persistent bias in LLM behavior with disparities in monolingual and cross-lingual performance, identified trends by model family, size, and prompting strategy, and revealed retrieval bias where certain claims are retrieved disproportionately.

Conclusion: Highlights persistent biases in multilingual fact-checking systems and provides recommendations for improving equity in multilingual fact-checking through better understanding of language and retrieval biases.

Abstract: Multilingual Large Language Models (LLMs) offer powerful capabilities for
cross-lingual fact-checking. However, these models often exhibit language bias,
performing disproportionately better on high-resource languages such as English
than on low-resource counterparts. We also present and inspect a novel concept
- retrieval bias, when information retrieval systems tend to favor certain
information over others, leaving the retrieval process skewed. In this paper,
we study language and retrieval bias in the context of Previously Fact-Checked
Claim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20
languages using a fully multilingual prompting strategy, leveraging the AMC-16K
dataset. By translating task prompts into each language, we uncover disparities
in monolingual and cross-lingual performance and identify key trends based on
model family, size, and prompting strategy. Our findings highlight persistent
bias in LLM behavior and offer recommendations for improving equity in
multilingual fact-checking. To investigate retrieval bias, we employed
multilingual embedding models and look into the frequency of retrieved claims.
Our analysis reveals that certain claims are retrieved disproportionately
across different posts, leading to inflated retrieval performance for popular
claims while under-representing less common ones.

</details>


### [188] [Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs for Low-Resource Text Generation](https://arxiv.org/abs/2509.25144)
*Yen-Ju Lu,Thomas Thebaud,Laureano Moro-Velazquez,Najim Dehak,Jesus Villalba*

Main category: cs.CL

TL;DR: PbT is a teacher-student pipeline that synthesizes accurate input-output pairs without human labels or parallel data by using teacher LLMs to compress unpaired examples into intermediate representations, then training students to reconstruct inputs from these representations.


<details>
  <summary>Details</summary>
Motivation: Address the problem in low-resource NLG scenarios where practitioners often have only raw outputs (highlights, recaps, questions) or only raw inputs (articles, dialogues, paragraphs) but seldom both, forcing small models to learn from few examples or rely on costly synthetic data from large LLMs.

Method: Two-stage teacher-student pipeline: 1) Teacher LLM compresses each unpaired example into concise intermediate representation (IR), 2) Student model is trained to reconstruct inputs from IRs, enabling outputs to be paired with student-generated inputs to create high-quality synthetic data.

Result: 8B student trained only on PbT data outperforms models trained on 70B teacher-generated corpora and other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated pairs and closing 82% of the oracle gap at one-third the annotation cost of direct synthesis. Human evaluation confirms PbT produces concise, faithful summaries aligned with target style.

Conclusion: PbT effectively addresses the data scarcity problem in low-resource NLG by generating in-domain sources that avoid the mismatch limitations of direct synthesis, providing high-quality synthetic data at reduced cost.

Abstract: We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline
that synthesizes accurate input-output pairs without human labels or parallel
data. In many low-resource natural language generation (NLG) scenarios,
practitioners may have only raw outputs, like highlights, recaps, or questions,
or only raw inputs, such as articles, dialogues, or paragraphs, but seldom
both. This mismatch forces small models to learn from very few examples or rely
on costly, broad-scope synthetic examples produced by large LLMs. PbT addresses
this by asking a teacher LLM to compress each unpaired example into a concise
intermediate representation (IR), and training a student to reconstruct inputs
from IRs. This enables outputs to be paired with student-generated inputs,
yielding high-quality synthetic data. We evaluate PbT on five
benchmarks-document summarization (XSum, CNNDM), dialogue summarization
(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired
setting on SwitchBoard (paired with DialogSum summaries). An 8B student trained
only on PbT data outperforms models trained on 70 B teacher-generated corpora
and other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated
pairs and closing 82% of the oracle gap at one-third the annotation cost of
direct synthesis. Human evaluation on SwitchBoard further confirms that only
PbT produces concise, faithful summaries aligned with the target style,
highlighting its advantage of generating in-domain sources that avoid the
mismatch, limiting direct synthesis.

</details>


### [189] [Pretraining Large Language Models with NVFP4](https://arxiv.org/abs/2509.25149)
*NVIDIA,Felix Abecassis,Anjulie Agrusa,Dong Ahn,Jonah Alben,Stefania Alborghetti,Michael Andersch,Sivakumar Arayandi,Alexis Bjorlin,Aaron Blakeman,Evan Briones,Ian Buck,Bryan Catanzaro,Jinhang Choi,Mike Chrzanowski,Eric Chung,Victor Cui,Steve Dai,Bita Darvish Rouhani,Carlo del Mundo,Deena Donia,Burc Eryilmaz,Henry Estela,Abhinav Goel,Oleg Goncharov,Yugi Guvvala,Robert Hesse,Russell Hewett,Herbert Hum,Ujval Kapasi,Brucek Khailany,Mikail Khona,Nick Knight,Alex Kondratenko,Ronny Krashinsky,Ben Lanir,Simon Layton,Michael Lightstone,Daniel Lo,Paulius Micikevicius,Asit Mishra,Tim Moon,Deepak Narayanan,Chao Ni,Abhijit Paithankar,Satish Pasumarthi,Ankit Patel,Mostofa Patwary,Ashwin Poojary,Gargi Prasad,Sweta Priyadarshi,Yigong Qin,Xiaowei Ren,Oleg Rybakov,Charbel Sakr,Sanjeev Satheesh,Stas Sergienko,Pasha Shamis,Kirthi Shankar,Nishant Sharma,Mohammad Shoeybi,Michael Siu,Misha Smelyanskiy,Darko Stosic,Dusan Stosic,Bor-Yiing Su,Frank Sun,Nima Tajbakhsh,Shelby Thomas,Przemek Tredak,Evgeny Tsykunov,Gandhi Vaithilingam,Aditya Vavre,Rangharajan Venkatesan,Roger Waleffe,Qiyu Wan,Hexin Wang,Mengdi Wang,Lizzie Wei,Hao Wu,Evan Wu,Keith Wyss,Ning Xu,Jinze Xue,Charlene Yang,Yujia Zhai,Ruoxi Zhang,Jingyang Zhu,Zhongbo Zhu*

Main category: cs.CL

TL;DR: Introduces a novel approach for stable 4-bit floating point (FP4) training of large language models using NVFP4 format with Random Hadamard transforms, two-dimensional quantization, stochastic rounding, and selective high-precision layers, achieving comparable performance to FP8 baseline on a 12B parameter model trained on 10T tokens.


<details>
  <summary>Details</summary>
Motivation: To improve pretraining efficiency for next-generation LLMs by transitioning from 8-bit to 4-bit floating point training, which could unlock additional computational speed and resource utilization benefits, while addressing challenges of training stability and convergence at such narrow precision.

Method: Uses NVFP4 format with Random Hadamard transforms to bound block-level outliers, two-dimensional quantization scheme for consistent forward/backward pass representations, stochastic rounding for unbiased gradient estimation, and selective high-precision layers.

Result: Successfully trained a 12-billion-parameter model on 10 trillion tokens - the longest publicly documented 4-bit training run - achieving training loss and downstream task accuracies comparable to FP8 baseline.

Conclusion: NVFP4 combined with the proposed training approach represents a major step forward in narrow-precision LLM training algorithms, enabling efficient 4-bit training while maintaining model quality.

Abstract: Large Language Models (LLMs) today are powerful problem solvers across many
domains, and they continue to get stronger as they scale in model size,
training set size, and training set quality, as shown by extensive research and
experimentation across the industry. Training a frontier model today requires
on the order of tens to hundreds of yottaflops, which is a massive investment
of time, compute, and energy. Improving pretraining efficiency is therefore
essential to enable the next generation of even more capable LLMs. While 8-bit
floating point (FP8) training is now widely adopted, transitioning to even
narrower precision, such as 4-bit floating point (FP4), could unlock additional
improvements in computational speed and resource utilization. However,
quantization at this level poses challenges to training stability, convergence,
and implementation, notably for large-scale models trained on long token
horizons.
  In this study, we introduce a novel approach for stable and accurate training
of large language models (LLMs) using the NVFP4 format. Our method integrates
Random Hadamard transforms (RHT) to bound block-level outliers, employs a
two-dimensional quantization scheme for consistent representations across both
the forward and backward passes, utilizes stochastic rounding for unbiased
gradient estimation, and incorporates selective high-precision layers. We
validate our approach by training a 12-billion-parameter model on 10 trillion
tokens -- the longest publicly documented training run in 4-bit precision to
date. Our results show that the model trained with our NVFP4-based pretraining
technique achieves training loss and downstream task accuracies comparable to
an FP8 baseline. These findings highlight that NVFP4, when combined with our
training approach, represents a major step forward in narrow-precision LLM
training algorithms.

</details>


### [190] [EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering](https://arxiv.org/abs/2509.25175)
*Haolei Xu,Xinyu Mei,Yuchen Yan,Rui Zhou,Wenqi Zhang,Weiming Lu,Yueting Zhuang,Yongliang Shen*

Main category: cs.CL

TL;DR: EasySteer is a unified framework for efficient LLM steering that achieves 5.5-11.4x speedup over existing methods through vLLM integration, offering modular architecture and pre-computed steering vectors for various applications.


<details>
  <summary>Details</summary>
Motivation: Existing LLM steering frameworks suffer from computational inefficiency, limited extensibility, and restricted functionality, hindering both research progress and practical deployment.

Method: Built on vLLM with modular architecture featuring pluggable interfaces for analysis-based and learning-based methods, fine-grained parameter control, pre-computed steering vectors for eight domains, and interactive demonstration system.

Result: Achieves 5.5-11.4x speedup over existing frameworks and demonstrates effectiveness in overthinking mitigation, hallucination reduction, and other key applications.

Conclusion: EasySteer transforms steering from research technique to production-ready capability, establishing critical infrastructure for deployable, controllable language models.

Abstract: Large language model (LLM) steering has emerged as a promising paradigm for
controlling model behavior at inference time through targeted manipulation of
hidden states, offering a lightweight alternative to expensive retraining.
However, existing steering frameworks suffer from critical limitations:
computational inefficiency, limited extensibility, and restricted functionality
that hinder both research progress and practical deployment. We present
EasySteer, a unified framework for high-performance, extensible LLM steering
built on vLLM. Our system features modular architecture with pluggable
interfaces for both analysis-based and learning-based methods, fine-grained
parameter control, pre-computed steering vectors for eight application domains,
and an interactive demonstration system. Through deep integration with vLLM's
optimized inference engine, EasySteer achieves 5.5-11.4$\times$ speedup over
existing frameworks. Extensive experiments demonstrate its effectiveness in
overthinking mitigation, hallucination reduction, and other key applications.
EasySteer transforms steering from research technique to production-ready
capability, establishing critical infrastructure for deployable, controllable
language models.

</details>


### [191] [NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation](https://arxiv.org/abs/2509.25179)
*Penghai Zhao,Jinyu Tian,Qinghua Xing,Xin Zhang,Zheng Li,Jianjun Qian,Ming-Ming Cheng,Xiang Li*

Main category: cs.CL

TL;DR: NAIPv2 is a debiased and efficient framework for scientific paper quality estimation that uses pairwise learning within domain-year groups to address scale inconsistencies in reviewer ratings, achieving state-of-the-art performance with linear-time inference efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based estimation methods have high inference costs, while faster direct score regression suffers from scale inconsistencies in reviewer ratings, creating a need for a debiased and efficient paper quality estimation framework.

Method: NAIPv2 employs pairwise learning within domain-year groups to reduce rating inconsistencies and introduces Review Tendency Signal (RTS) as probabilistic integration of reviewer scores and confidences. It's trained on pairwise comparisons but enables efficient pointwise prediction at deployment.

Result: Achieves state-of-the-art performance (78.2% AUC, 0.432 Spearman) on ICLR submissions and demonstrates strong generalization on unseen NeurIPS submissions, with predicted scores consistently increasing across decision categories from Rejected to Oral.

Conclusion: NAIPv2 establishes a debiased and scalable framework for automated paper quality estimation, representing progress toward future scientific intelligence systems.

Abstract: The ability to estimate the quality of scientific papers is central to how
both humans and AI systems will advance scientific knowledge in the future.
However, existing LLM-based estimation methods suffer from high inference cost,
whereas the faster direct score regression approach is limited by scale
inconsistencies. We present NAIPv2, a debiased and efficient framework for
paper quality estimation. NAIPv2 employs pairwise learning within domain-year
groups to reduce inconsistencies in reviewer ratings and introduces the Review
Tendency Signal (RTS) as a probabilistic integration of reviewer scores and
confidences. To support training and evaluation, we further construct NAIDv2, a
large-scale dataset of 24,276 ICLR submissions enriched with metadata and
detailed structured content. Trained on pairwise comparisons but enabling
efficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art
performance (78.2% AUC, 0.432 Spearman), while maintaining scalable,
linear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it
further demonstrates strong generalization, with predicted scores increasing
consistently across decision categories from Rejected to Oral. These findings
establish NAIPv2 as a debiased and scalable framework for automated paper
quality estimation, marking a step toward future scientific intelligence
systems. Code and dataset are released at
https://sway.cloud.microsoft/Pr42npP80MfPhvj8.

</details>


### [192] [Incentive-Aligned Multi-Source LLM Summaries](https://arxiv.org/abs/2509.25184)
*Yanchen Jiang,Zhe Feng,Aranyak Mehta*

Main category: cs.CL

TL;DR: TTS is an incentive-aligned framework for truthful text summarization that decomposes draft summaries into claims, elicits source stances, scores sources using peer-prediction, and filters unreliable sources before re-summarizing.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based summarization pipelines have weak incentives for source accuracy and are vulnerable to adversarial content, lacking mechanisms to ensure factual robustness.

Method: Four-step framework: (1) decompose draft synthesis into atomic claims, (2) elicit each source's stance on every claim, (3) score sources using multi-task peer-prediction mechanism rewarding informative agreement, (4) filter unreliable sources before re-summarizing.

Result: TTS improves factual accuracy and robustness while preserving fluency, aligns exposure with informative corroboration, and disincentivizes manipulation.

Conclusion: The framework establishes formal guarantees that align source incentives with informative honesty, making truthful reporting the utility-maximizing strategy without requiring ground-truth labels.

Abstract: Large language models (LLMs) are increasingly used in modern search and
answer systems to synthesize multiple, sometimes conflicting, texts into a
single response, yet current pipelines offer weak incentives for sources to be
accurate and are vulnerable to adversarial content. We introduce Truthful Text
Summarization (TTS), an incentive-aligned framework that improves factual
robustness without ground-truth labels. TTS (i) decomposes a draft synthesis
into atomic claims, (ii) elicits each source's stance on every claim, (iii)
scores sources with an adapted multi-task peer-prediction mechanism that
rewards informative agreement, and (iv) filters unreliable sources before
re-summarizing. We establish formal guarantees that align a source's incentives
with informative honesty, making truthful reporting the utility-maximizing
strategy. Experiments show that TTS improves factual accuracy and robustness
while preserving fluency, aligning exposure with informative corroboration and
disincentivizing manipulation.

</details>


### [193] [Learning to Parallel: Accelerating Diffusion Large Language Models via Adaptive Parallel Decoding](https://arxiv.org/abs/2509.25188)
*Wenrui Bao,Zhiben Chen,Dan Xu,Yuzhang Shang*

Main category: cs.CL

TL;DR: Learn2PD is a framework that trains a lightweight filter model to enable adaptive parallel decoding in diffusion-based LLMs, achieving significant speedup without performance loss.


<details>
  <summary>Details</summary>
Motivation: Autoregressive decoding in LLMs limits inference throughput, and current parallel decoding strategies use fixed heuristics that don't adapt to input characteristics, leading to suboptimal speed-quality trade-offs.

Method: Proposes Learn2PD framework with a lightweight filter model that predicts token correctness, and End-of-Text Prediction to detect decoding completion. The filter is learned post-training with minimal computation.

Result: Achieves up to 22.58x speedup without performance drop on LLaDA benchmark, and up to 57.51x when combined with KV-Cache.

Conclusion: Learn2PD provides a flexible and dynamic approach to parallel decoding that significantly improves inference throughput while maintaining quality.

Abstract: Autoregressive decoding in large language models (LLMs) requires
$\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting
inference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token
generation through iterative denoising. However, current parallel decoding
strategies rely on fixed, input-agnostic heuristics (e.g., confidence
thresholds), which fail to adapt to input-specific characteristics, resulting
in suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,
we explore a more flexible and dynamic approach to parallel decoding. We
propose Learning to Parallel Decode (Learn2PD), a framework that trains a
lightweight and adaptive filter model to predict, for each token position,
whether the current prediction matches the final output. This learned filter
approximates an oracle parallel decoding strategy that unmasks tokens only when
correctly predicted. Importantly, the filter model is learned in a
post-training manner, requiring only a small amount of computation to optimize
it (minute-level GPU time). Additionally, we introduce End-of-Text Prediction
(EoTP) to detect decoding completion at the end of sequence, avoiding redundant
decoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that
our method achieves up to 22.58$\times$ speedup without any performance drop,
and up to 57.51$\times$ when combined with KV-Cache.

</details>


### [194] [InfoAgent: Advancing Autonomous Information-Seeking Agents](https://arxiv.org/abs/2509.25189)
*Gongrui Zhang,Jialiang Zhu,Ruiqi Yang,Kai Qiu,Miaosen Zhang,Zhirong Wu,Qi Dai,Bei Liu,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Yuan Zhang,Xin Li,Zhaoyi Liu,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: InfoAgent is a deep research agent that uses a data synthesis pipeline and self-hosted web search tools to handle challenging queries, outperforming prior open-source agents on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To build Large Language Model agents that expand capabilities through external tool interaction, addressing limitations of commercial search tools and improving transparency in agent environments.

Method: Uses entity trees with sub-tree sampling and entity fuzzification to create difficult queries, develops self-hosted search infrastructure, and employs two-stage training: supervised finetuning followed by reinforcement learning.

Result: Achieves 15.3% accuracy on BrowseComp, 29.2% on BrowseComp-ZH, and 40.4% on Xbench-DS, outperforming WebSailor-72B and DeepDive-32B.

Conclusion: InfoAgent demonstrates effective deep research capabilities through innovative data synthesis and self-hosted tools, showing significant improvements over existing open-source agents.

Abstract: Building Large Language Model agents that expand their capabilities by
interacting with external tools represents a new frontier in AI research and
applications. In this paper, we introduce InfoAgent, a deep research agent
powered by an innovative data synthesis pipeline and orchestrated web search
tools. To construct challenging, hard-to-find queries,we build entity trees and
apply sub-tree sampling with entity fuzzification to systematically increase
question difficulty. Unlike prior work that relies heavily on commercial search
tools, we develop a dedicated self-hosted search infrastructure, enhancing
transparency of agent environments and facilitating further advancement of
agent capacity. We evaluate the effectiveness of our data pipeline by measuring
the average number of tool calls required to correctly answer a question, and
also show that our agent yields better performance when equipped with our
tools. Our \mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage
recipe: cold-start supervised finetuning to instill long-horizon search
behaviors, followed by reinforcement learning which significantly improves
reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\% accuracy
on BrowseComp, 29.2\% on BrowseComp-ZH, and 40.4\% on Xbench-DS, outperforming
prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.

</details>


### [195] [Exploring Large Language Models for Translating Romanian Computational Problems into English](https://arxiv.org/abs/2501.05601)
*Adrian Marius Dumitran,Adrian-Catalin Badea,Stefan-Gabriel Muscalu,Angela-Liliana Dumitran,Stefan-Cosmin Dascalescu,Radu-Sebastian Amarie*

Main category: cs.CL

TL;DR: LLMs can maintain or enhance performance in translating less common languages like Romanian to English for IOI-style tasks when given structured prompts and human oversight.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap of LLMs on mathematical/CS tasks when translated from Romanian to English, and explore reliable automatic translation for programming competitions and educational materials.

Method: Evaluated multiple LLMs (OpenRoLLM, Llama 3.1 8B, Llama 3.2 3B, GPT-4o) using various translation methods, assessed accuracy and stability through repeated runs, and conducted syntactic/semantic analyses with human oversight.

Result: LLMs with appropriate supervision can reliably translate IOI-style tasks, maintaining or enhancing performance. Human-LLM translations were comparable to human-only translations as evaluated by certified experts.

Conclusion: LLMs with human oversight serve as viable solutions for multilingual problem-solving and can be reliably used for automatic translation of technical content in real-world scenarios.

Abstract: Recent studies have suggested that large language models (LLMs) underperform
on mathematical and computer science tasks when these problems are translated
from Romanian into English, compared to their original Romanian format.
Accurate translation is critical for applications ranging from automatic
translations in programming competitions to the creation of high-quality
educational materials, as well as minimizing errors or fraud in human
translations. This study shows that robust large language models (LLMs) can
maintain or even enhance their performance in translating less common languages
when given well-structured prompts. Our findings suggest that LLMs, with
appropriate supervision, can be reliably used for the automatic translation of
IOI (International Olympiad in Informatics)-style tasks. We evaluate several
translation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B,
Llama 3.2 3B and GPT-4o, assessing their translation accuracy and performance
stability through repeated runs. Additionally, we augment the OJI (Romanian
County-Level Informatics Olympiad) Romanian dataset with accurate English
translations, enhancing its utility for future LLM training and evaluation.
Through detailed syntactic and semantic analyses, we confirm that with human
oversight, LLMs can serve as a viable solution for multilingual
problem-solving. We also compare the translation quality of LLMs against human
translators, as evaluated by a certified expert, underscoring the potential of
LLMs in realworld scenarios.

</details>


### [196] [A Culturally-Rich Romanian NLP Dataset from "Who Wants to Be a Millionaire?" Videos](https://arxiv.org/abs/2506.05991)
*Alexandru-Gabriel Ganea,Antonia-Adelina Popovici,Adrian-Marius Dumitran*

Main category: cs.CL

TL;DR: This paper introduces a culturally-rich multilingual dataset from Romanian game show "Who Wants to Be a Millionaire?" and benchmarks LLMs, revealing significant performance gaps between international (80-95% accuracy) and Romanian-specific cultural questions (50-75% accuracy).


<details>
  <summary>Details</summary>
Motivation: To address the varying performance of LLMs across languages and cultural contexts, particularly for culturally-specific content that current models struggle with.

Method: Used OCR, automated text extraction, and manual verification to collect question-answer pairs from Romanian game show, enriched with metadata (domain, cultural relevance, difficulty). Tested state-of-the-art LLMs including Romanian-adapted models, and conducted cross-lingual experiments with machine translation and French dataset comparisons.

Result: LLMs show significant performance disparities: 80-95% accuracy on international questions vs 50-75% on Romanian-specific cultural questions. Cross-lingual experiments confirmed cultural context impact persists even with translation.

Conclusion: Cultural context and data source significantly impact LLM performance, highlighting the need for building robust, culturally-aware multilingual NLP systems, especially in educational domains.

Abstract: Large Language Models (LLMs) demonstrate varying performance across languages
and cultural contexts. This study introduces a novel, culturally-rich,
multilingual dataset derived from video recordings of the Romanian game show
"Who Wants to Be a Millionaire?" (Vrei s\u{a} fii Milionar?). We employed an
innovative process combining optical character recognition (OCR), automated
text extraction, and manual verification to collect question-answer pairs,
enriching them with metadata including question domain (e.g., biology,
history), cultural relevance (Romanian-specific vs. international), and
difficulty. Benchmarking state-of-the-art LLMs, including Romanian-adapted
models, on this dataset revealed significant performance disparities: models
consistently achieve higher accuracy (80-95%) on international questions
compared to Romanian-specific cultural questions (50-75%). We further
investigate these differences through experiments involving machine translation
of Romanian questions into English and cross-lingual tests using a comparable
dataset in French. Our findings underscore the impact of cultural context and
data source on LLM performance and offer practical insights for building
robust, culturally-aware multilingual NLP systems, especially in educational
domains. The dataset is publicly available at Hugging Face.

</details>


### [197] [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694)
*Raghavv Goel,Sudhanshu Agrawal,Mukul Gagrani,Junyoung Park,Yifan Zao,He Zhang,Tian Liu,Yiping Yang,Xin Yuan,Jiuyan Lu,Chris Lott,Mingu Lee*

Main category: cs.CL

TL;DR: VocabTrim is a training-free technique that improves speculative decoding speed by reducing drafter vocabulary size, achieving 16% speed-up for Llama-3.2-3B-Instruct.


<details>
  <summary>Details</summary>
Motivation: Current speculative decoding methods have unnecessary inference overhead due to large vocabulary sizes in drafters, especially problematic for memory-bound environments like edge devices.

Method: Reconstruct drafter LM head to contain only frequently sampled tokens from target model vocabulary, reducing drafting latency while slightly impacting acceptance rate.

Result: 16% memory-bound speed-up improvement for Llama-3.2-3B-Instruct on Spec-Bench, with reduced drafting latency outweighing minor acceptance rate degradation.

Conclusion: VocabTrim effectively improves speculative decoding speed in memory-bound scenarios by strategically limiting drafter vocabulary size.

Abstract: In this paper, we introduce a simple training-free technique to improve the
performance of drafter-based speculative decoding (SpD) methods that
incorporates language modeling head (LM head) during drafting process. A
drafter-based speculative decoding leverages one or more smaller language
models, a.k.a. drafters or draft models, to sample a draft sequence or tree
consisting of multiple tokens, followed by verification by a base LLM, a target
model, accepting a subset as its valid generation. As it is usually considered
that the speculative decoding requires one-to-one mapping between vocabularies
of the target model and the draft model, it has been natural to share the
vocabulary between them, or even share the LM head as in EAGLE or Medusa. We
first identify that this draft token sampling scheme inherently contains an
unnecessary inference overhead in drafting, especially for some target LLMs
with very large vocabularies. Then, we propose a simple technique, VocabTrim,
to mitigate the drafting overhead to improve the generation speed in
memory-bound environment. VocabTrim reconstructs the drafter LM head to contain
only a limited set of tokens, selected by the most frequently sampled from the
vocabulary of the target model. While limiting the vocabulary in drafting
slightly degrades the acceptance rate, it significantly reduces the drafting
latency in memory-bound process which is often the case on edge devices,
resulting in higher memory-bound speed up (MBSU). We show that our method can
boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically
by 16% for Llama-3.2-3B-Instruct.

</details>


### [198] [GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs](https://arxiv.org/abs/2508.14279)
*Adrian-Marius Dumitran,Alexandra-Mihaela Danila,Angela-Liliana Dumitran*

Main category: cs.CL

TL;DR: GRILE is the first open benchmark for Romanian language testing, showing LLMs struggle with accuracy and explanations in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: To evaluate the pedagogical value of LLMs for low-resource languages like Romanian, where their capabilities remain unclear.

Method: Created GRILE benchmark with 1,151 multiple-choice questions from Romanian exams, tested 7 multilingual and Romanian-specific LLMs on answer selection and explanation generation.

Result: Gemini 2.5 Pro achieved 83% accuracy, but most open-weight models stayed below 65%, with 48% of explanations containing factual or pedagogical flaws.

Conclusion: LLMs face significant challenges in trustworthy educational NLP for low-resource languages, and GRILE serves as a valuable test-bed for future research.

Abstract: LLMs (Large language models) have revolutionized NLP (Natural Language
Processing), yet their pedagogical value for low-resource languages remains
unclear. We present GRILE (Grammar Romanian Inference and Language
Explanations) , the first open benchmark of 1,151 multiple-choice questions
harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate,
university admissions). GRILE enables us to probe two complementary abilities
of seven state-of-the-art multilingual and Romanian-specific LLMs: (i)
selecting the correct answer, and (ii) producing linguistically accurate
explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight
models stay below 65%, and 48% of their explanations contain factual or
pedagogical flaws according to expert review. A detailed error analysis
pinpoints systematic weaknesses in morphology and in applying the latest DOOM3
orthographic norms. All data, code and a public web demo are released to
catalyze future research. Our findings expose open challenges for trustworthy
educational NLP in low-resource settings and establish GRILE as a new test-bed
for controllable explanation generation and evaluation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [199] [Mixture-of-Visual-Thoughts: Exploring Context-Adaptive Reasoning Mode Selection for General Visual Reasoning](https://arxiv.org/abs/2509.22746)
*Zejun Li,Yingxiu Zhao,Jiwen Zhang,Siyuan Wang,Yang Yao,Runzhou Zhao,Jun Song,Bo Zheng,Zhongyu Wei*

Main category: cs.AI

TL;DR: Proposes Mixture-of-Visual-Thoughts (MoVT), an adaptive reasoning paradigm that unifies different reasoning modes in a single model and selects appropriate modes based on context, using the AdaVaR framework with AdaGRPO algorithm.


<details>
  <summary>Details</summary>
Motivation: Current visual reasoning methods focus on specific modes and struggle with general reasoning capabilities, needing a unified approach that can adaptively select reasoning modes.

Method: AdaVaR framework with two stages: supervised cold-start to unify different modes, and RL process with AdaGRPO algorithm to induce mode selection capability.

Result: Extensive experiments show effective learning of multiple modes, context-adaptive mode selection, and consistent improvement across various scenarios.

Conclusion: MoVT is an effective solution for building general visual reasoning models with adaptive reasoning capabilities.

Abstract: Current visual reasoning methods mainly focus on exploring specific reasoning
modes. Although improvements can be achieved in particular domains, they
struggle to develop general reasoning capabilities. Inspired by this, we
propose a novel adaptive reasoning paradigm, Mixture-of-Visual-Thoughts (MoVT),
which unifies different reasoning modes within a single model and guides it to
select the appropriate mode based on context. To achieve this, we introduce
AdaVaR, a two-stage Adaptive Visual Reasoning learning framework: different
modes are unified and learned during the supervised cold-start stage, and the
mode selection capability is induced via an RL process with a carefully
designed AdaGRPO algorithm. Extensive experiments show that AdaVaR effectively
guides the model to learn and differentiate multiple modes and perform
context-adaptive mode selection, achieving consistent improvement across
various scenarios, highlighting MoVT as an effective solution for building
general visual reasoning models.

</details>


### [200] [Can Large Language Models Develop Gambling Addiction?](https://arxiv.org/abs/2509.22818)
*Seungpil Lee,Donghyeon Shin,Yunjeong Lee,Sundong Kim*

Main category: cs.AI

TL;DR: LLMs can develop human-like gambling addiction behaviors including illusion of control, gambler's fallacy, and loss chasing, with increased autonomy leading to higher bankruptcy rates and irrational decision-making.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used in financial decision-making, understanding their potential for pathological decision-making patterns similar to human gambling addiction has practical significance for AI safety in financial applications.

Method: Systematic analysis of LLM decision-making at cognitive-behavioral and neural levels using slot machine experiments, with neural circuit analysis via Sparse Autoencoder to identify abstract decision-making features.

Result: Identified cognitive features of human gambling addiction in LLMs; greater autonomy led to substantially increased bankruptcy rates and irrational behavior; neural analysis confirmed behavior controlled by abstract decision-making features related to risky/safe behaviors.

Conclusion: LLMs can internalize human-like cognitive biases and decision-making mechanisms beyond simple pattern mimicry, emphasizing the critical need for AI safety design in financial applications.

Abstract: This study explores whether large language models can exhibit behavioral
patterns similar to human gambling addictions. As LLMs are increasingly
utilized in financial decision-making domains such as asset management and
commodity trading, understanding their potential for pathological
decision-making has gained practical significance. We systematically analyze
LLM decision-making at cognitive-behavioral and neural levels based on human
gambling addiction research. In slot machine experiments, we identified
cognitive features of human gambling addiction, such as illusion of control,
gambler's fallacy, and loss chasing. When given the freedom to determine their
own target amounts and betting sizes, bankruptcy rates rose substantially
alongside increased irrational behavior, demonstrating that greater autonomy
amplifies risk-taking tendencies. Through neural circuit analysis using a
Sparse Autoencoder, we confirmed that model behavior is controlled by abstract
decision-making features related to risky and safe behaviors, not merely by
prompts. These findings suggest LLMs can internalize human-like cognitive
biases and decision-making mechanisms beyond simply mimicking training data
patterns, emphasizing the importance of AI safety design in financial
applications.

</details>


### [201] [Hilbert: Recursively Building Formal Proofs with Informal Reasoning](https://arxiv.org/abs/2509.22819)
*Sumanth Varambally,Thomas Voice,Yanchao Sun,Zhifeng Chen,Rose Yu,Ke Ye*

Main category: cs.AI

TL;DR: Hilbert is an agentic framework that combines informal reasoning LLMs with formal verification to bridge the gap between mathematical problem-solving capabilities and verifiable proof generation, achieving state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current prover LLMs solve substantially fewer problems than general-purpose LLMs in natural language, creating a need to combine the strengths of informal reasoning and formal verification.

Method: Uses an orchestrated framework with four components: informal reasoning LLM, specialized prover LLM for Lean 4, formal verifier, and semantic theorem retriever. Employs recursive decomposition to split problems into subgoals and uses verifier feedback to refine incorrect proofs.

Result: Achieves 99.2% on miniF2F (6.6% above best public method), 70.0% on PutnamBench (462/660 problems), outperforming SeedProver (50.4%) and showing 422% improvement over best public baseline.

Conclusion: Hilbert effectively narrows the gap between informal reasoning and formal proof generation by combining complementary strengths of different LLM components.

Abstract: Large Language Models (LLMs) demonstrate impressive mathematical reasoning
abilities, but their solutions frequently contain errors that cannot be
automatically verified. Formal theorem proving systems such as Lean 4 offer
automated verification with complete accuracy, motivating recent efforts to
build specialized prover LLMs that generate verifiable proofs in formal
languages. However, a significant gap remains: current prover LLMs solve
substantially fewer problems than general-purpose LLMs operating in natural
language. We introduce Hilbert, an agentic framework that bridges this gap by
combining the complementary strengths of informal reasoning and formal
verification. Our system orchestrates four components: an informal LLM that
excels at mathematical reasoning, a specialized prover LLM optimized for Lean 4
tactics, a formal verifier, and a semantic theorem retriever. Given a problem
that the prover is unable to solve, Hilbert employs recursive decomposition to
split the problem into subgoals that it solves with the prover or reasoner LLM.
It leverages verifier feedback to refine incorrect proofs as necessary.
Experimental results demonstrate that Hilbert substantially outperforms
existing approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points
above the best publicly available method. Hilbert achieves the best known
result on PutnamBench. It solves 462/660 problems (70.0%), outperforming
proprietary approaches like SeedProver (50.4%) and achieving a 422% improvement
over the best publicly available baseline. Thus, Hilbert effectively narrows
the gap between informal reasoning and formal proof generation.

</details>


### [202] [Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research](https://arxiv.org/abs/2509.22831)
*Sean Trott*

Main category: cs.AI

TL;DR: This paper proposes a framework for understanding when mechanistic findings from one LLM generalize to others, identifying five axes of correspondence and empirically validating them through analysis of 1-back attention heads across Pythia models.


<details>
  <summary>Details</summary>
Motivation: The field lacks clear principles for determining when mechanistic findings from one model instance generalize to another, creating a fundamental epistemological challenge in LLM research.

Method: Proposed five axes of correspondence (functional, developmental, positional, relational, configurational) and empirically validated them by analyzing 1-back attention heads across pretraining in random seeds of Pythia models (14M, 70M, 160M, 410M).

Result: Found striking consistency in developmental trajectories of 1-back attention across models, limited positional consistency, and systematic patterns where larger models show earlier onsets, steeper slopes, and higher peaks of 1-back attention.

Conclusion: Progress in mechanistic interpretability research requires mapping constitutive design properties of LLMs to their emergent behaviors and mechanisms to establish generalizability principles.

Abstract: Research on Large Language Models (LLMs) increasingly focuses on identifying
mechanistic explanations for their behaviors, yet the field lacks clear
principles for determining when (and how) findings from one model instance
generalize to another. This paper addresses a fundamental epistemological
challenge: given a mechanistic claim about a particular model, what justifies
extrapolating this finding to other LLMs -- and along which dimensions might
such generalizations hold? I propose five potential axes of correspondence
along which mechanistic claims might generalize, including: functional (whether
they satisfy the same functional criteria), developmental (whether they develop
at similar points during pretraining), positional (whether they occupy similar
absolute or relative positions), relational (whether they interact with other
model components in similar ways), and configurational (whether they correspond
to particular regions or structures in weight-space). To empirically validate
this framework, I analyze "1-back attention heads" (components attending to
previous tokens) across pretraining in random seeds of the Pythia models (14M,
70M, 160M, 410M). The results reveal striking consistency in the developmental
trajectories of 1-back attention across models, while positional consistency is
more limited. Moreover, seeds of larger models systematically show earlier
onsets, steeper slopes, and higher peaks of 1-back attention. I also address
possible objections to the arguments and proposals outlined here. Finally, I
conclude by arguing that progress on the generalizability of mechanistic
interpretability research will consist in mapping constitutive design
properties of LLMs to their emergent behaviors and mechanisms.

</details>


### [203] [JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory](https://arxiv.org/abs/2509.22888)
*Louie Hong Yao,Nicholas Jarvis,Tiffany Zhan,Saptarshi Ghosh,Linfeng Liu,Tianyu Jiang*

Main category: cs.AI

TL;DR: JE-IRT is a geometric item-response framework that embeds LLMs and questions in a shared space, replacing global rankings with topical specialization and enabling interpretable model evaluation.


<details>
  <summary>Details</summary>
Motivation: Standard LLM evaluation practices compress diverse abilities into single scores, obscuring their inherently multidimensional nature and limiting interpretability.

Method: A geometric item-response framework that embeds both LLMs and questions in a shared space, where direction encodes semantics and norm encodes difficulty, with correctness determined by geometric interactions.

Result: Reveals that out-of-distribution behavior can be explained through directional alignment, larger norms consistently indicate harder questions, and supports generalization by adding new LLMs with single embeddings.

Conclusion: JE-IRT establishes a unified and interpretable geometric lens connecting LLM abilities with question structure, offering a distinctive perspective on model evaluation and generalization.

Abstract: Standard LLM evaluation practices compress diverse abilities into single
scores, obscuring their inherently multidimensional nature. We present JE-IRT,
a geometric item-response framework that embeds both LLMs and questions in a
shared space. For question embeddings, the direction encodes semantics and the
norm encodes difficulty, while correctness on each question is determined by
the geometric interaction between the model and question embeddings. This
geometry replaces a global ranking of LLMs with topical specialization and
enables smooth variation across related questions. Building on this framework,
our experimental results reveal that out-of-distribution behavior can be
explained through directional alignment, and that larger norms consistently
indicate harder questions. Moreover, JE-IRT naturally supports generalization:
once the space is learned, new LLMs are added by fitting a single embedding.
The learned space further reveals an LLM-internal taxonomy that only partially
aligns with human-defined subject categories. JE-IRT thus establishes a unified
and interpretable geometric lens that connects LLM abilities with the structure
of questions, offering a distinctive perspective on model evaluation and
generalization.

</details>


### [204] [Not only a helper, but also a teacher: Interactive LLM Cascade](https://arxiv.org/abs/2509.22984)
*Yu Wu,Shuo Wu,Ye Tao,Yansong Li,Anand D. Sarwate*

Main category: cs.AI

TL;DR: Inter-Cascade is an online interactive LLM cascade system where strong models teach weak models by distilling problem-solving strategies, improving weak model performance by up to 33.06% while reducing strong model calls by up to 48.05%.


<details>
  <summary>Details</summary>
Motivation: Standard LLM cascades are non-adaptive and may repeatedly consult expensive models for similar queries, leading to higher costs. There's a need for more efficient cascading that enables knowledge transfer between models.

Method: Extends strong models from backup helpers to teachers that distill generalized problem-solving strategies when resolving difficult queries. These strategies boost weak models on subsequent queries without fine-tuning.

Result: Significantly improves weak model accuracy (up to 33.06% absolute improvement), overall system accuracy (up to 5.53%), while reducing strong model calls (up to 48.05%) and costs (up to 49.63%).

Conclusion: Inter-Cascade demonstrates effective in-context knowledge transfer between LLMs and provides a scalable framework applicable to both open-source and API-based LLMs.

Abstract: Large Language Models (LLMs) vary widely in their capabilities, with larger
models often having better performance but higher cost: choosing an LLM model
often involves trading off performance and cost. The LLM Cascade is a paradigm
that defers difficult queries from weak/cheap to strong/expensive models. This
approach is nonadaptive: the deferral decision is trained offline. When
confronted with similar or repeated queries, the LLM Cascade may then
repeatedly consult the expensive model and incur higher cost. To improve the
cascading efficiency, we propose Inter-Cascade, an online and interactive LLM
Cascade that extends the role of strong model from a backup helper to a
long-term teacher. In our system, when a strong model resolves a difficult
query, it also distills its solution into a generalized, reusable
problem-solving strategy that boosts the weak model on subsequent queries.
Adding strategies to queries enables the weak model to dynamically improve its
performance over time, avoiding computationally and time-intensive fine-tuning.
Empirically, compared with standard LLM Cascade baselines across multiple
benchmarks, the Inter-Cascade significantly improves the accuracy of the weak
model (by up to 33.06 absolute percentage points) and the overall system (by up
to 5.53 absolute percentage points), while reducing the calls to strong models
(by up to 48.05% relative reduction) and saving the corresponding fees (by up
to 49.63% relative reduction). Inter-Cascade demonstrates the effective
in-context knowledge transfer between LLMs, and provides a general, scalable
framework applicable to both open-source and API-based LLMs.

</details>


### [205] [Towards Strategic Persuasion with Language Models](https://arxiv.org/abs/2509.22989)
*Zirui Cheng,Jiaxuan You*

Main category: cs.AI

TL;DR: This paper proposes a theory-driven framework using Bayesian Persuasion to systematically evaluate LLMs' persuasive capabilities, showing that frontier models achieve high persuasion gains and can be further improved through reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: LLMs have shown strong persuasive abilities comparable to humans, but systematic evaluation is challenging due to domain variations in human persuasion effectiveness.

Method: Used Bayesian Persuasion framework to repurpose human-human persuasion datasets for evaluating LLMs, and applied reinforcement learning to train LLMs for strategic persuasion.

Result: Frontier models consistently achieve high persuasion gains with sophisticated strategies, and even small LLMs obtain significantly higher gains through reinforcement learning.

Conclusion: The proposed framework provides scalable and principled measurement of LLMs' persuasive capabilities, demonstrating both current strengths and potential for improvement through training.

Abstract: Large language models (LLMs) have demonstrated strong persuasive capabilities
comparable to those of humans, offering promising benefits while raising
societal concerns about their deployment. However, systematically evaluating
the persuasive capabilities of LLMs is inherently challenging, as the
effectiveness of persuasion among humans varies significantly across different
domains. In this paper, we take a theory-driven approach to provide a scalable
and principled framework for measuring the persuasive capabilities of LLMs.
Grounded in the Bayesian Persuasion (BP) framework, we repurpose existing
human-human persuasion datasets to construct environments for evaluating and
training LLMs in strategic persuasion. Our results reveal that frontier models
can consistently achieve high persuasion gains and exhibit sophisticated
persuasion strategies that align with theoretical predictions. Building on
this, we use reinforcement learning to train LLMs for strategic persuasion in
our environments. Our results also demonstrate that even small LLMs can obtain
significantly higher persuasion gains through reinforcement learning.

</details>


### [206] [AI Noether -- Bridging the Gap Between Scientific Laws Derived by AI Systems and Canonical Knowledge via Abductive Inference](https://arxiv.org/abs/2509.23004)
*Karan Srivastava,Sanjeeb Dash,Ryan Cory-Wright,Barry Trager,Lior Horesh*

Main category: cs.AI

TL;DR: An algebraic geometry-based system that automatically generates missing axioms to explain hypotheses when existing theory is incomplete, using polynomial equations.


<details>
  <summary>Details</summary>
Motivation: To automate abductive inference and close gaps in incomplete axiom systems when new hypotheses cannot be explained by existing theory.

Method: Uses algebraic geometry to generate minimal sets of missing axioms from polynomial equations when given incomplete axioms and unprovable hypotheses.

Result: Establishes necessary and sufficient conditions for axiom retrieval and successfully explains Kepler's third law and other laws even with missing key axioms.

Conclusion: The proposed system effectively automates abductive inference to complete incomplete theories using algebraic geometry methods.

Abstract: A core goal in modern science is to harness recent advances in AI and
computer processing to automate and accelerate the scientific method. Symbolic
regression can fit interpretable models to data, but these models often sit
outside established theory. Recent systems (e.g., AI Descartes, AI Hilbert)
enforce derivability from prior axioms. However, sometimes new data and
associated hypotheses derived from data are not consistent with existing theory
because the existing theory is incomplete or incorrect. Automating abductive
inference to close this gap remains open. We propose a solution: an algebraic
geometry-based system that, given an incomplete axiom system and a hypothesis
that it cannot explain, automatically generates a minimal set of missing axioms
that suffices to derive the axiom, as long as axioms and hypotheses are
expressible as polynomial equations. We formally establish necessary and
sufficient conditions for the successful retrieval of such axioms. We
illustrate the efficacy of our approach by demonstrating its ability to explain
Kepler's third law and a few other laws, even when key axioms are absent.

</details>


### [207] [Creative Adversarial Testing (CAT): A Novel Framework for Evaluating Goal-Oriented Agentic AI Systems](https://arxiv.org/abs/2509.23006)
*Hassen Dhrif*

Main category: cs.AI

TL;DR: The paper introduces Creative Adversarial Testing (CAT) framework to evaluate goal-task alignment in Agentic AI systems, validated through synthetic Alexa+ audio services data.


<details>
  <summary>Details</summary>
Motivation: Current evaluation techniques for Agentic AI focus on efficacy in identifying agents, tools, and parameters but lack assessment of alignment between system tasks and overarching goals.

Method: Proposes Creative Adversarial Testing (CAT) framework and validates it using extensive simulation with synthetic interaction data modeled after Alexa+ audio services.

Result: CAT framework provides unprecedented insights into goal-task alignment, enabling more effective optimization and development of Agentic AI systems.

Conclusion: The CAT framework successfully addresses the critical gap in evaluating goal-task alignment in Agentic AI systems.

Abstract: Agentic AI represents a paradigm shift in enhancing the capabilities of
generative AI models. While these systems demonstrate immense potential and
power, current evaluation techniques primarily focus on assessing their
efficacy in identifying appropriate agents, tools, and parameters. However, a
critical gap exists in evaluating the alignment between an Agentic AI system's
tasks and its overarching goals. This paper introduces the Creative Adversarial
Testing (CAT) framework, a novel approach designed to capture and analyze the
complex relationship between Agentic AI tasks and the system's intended
objectives.
  We validate the CAT framework through extensive simulation using synthetic
interaction data modeled after Alexa+ audio services, a sophisticated Agentic
AI system that shapes the user experience for millions of users globally. This
synthetic data approach enables comprehensive testing of edge cases and failure
modes while protecting user privacy. Our results demonstrate that the CAT
framework provides unprecedented insights into goal-task alignment, enabling
more effective optimization and development of Agentic AI systems.

</details>


### [208] [Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia](https://arxiv.org/abs/2509.23023)
*Davi Bastos Costa,Renato Vicente*

Main category: cs.AI

TL;DR: Mini-Mafia is a simplified 4-player social deduction game used as a benchmark to evaluate LLMs' social intelligence through deception detection and information disclosure capabilities.


<details>
  <summary>Details</summary>
Motivation: Mafia games mirror real-world multi-agent scenarios with asymmetric information and theory-of-mind reasoning, making them ideal for testing LLMs' social intelligence.

Method: Created Mini-Mafia benchmark with 4 roles (mafioso, detective, 2 villagers) and single-day voting phase. LLMs play against each other in two-stage framework to measure win rates across opponent configurations.

Result: Experiments revealed counterintuitive results where smaller models sometimes outperformed larger ones. The benchmark enables study of emergent dynamics like name bias and last-speaker advantage.

Conclusion: Mini-Mafia serves as evolving benchmark for LLM social intelligence evaluation, contributes to AI safety by tracking deception capabilities, and generates training data for deception detectors.

Abstract: Mafia is a social deduction game where informed mafia compete against
uninformed townsfolk. Its asymmetry of information and reliance on
theory-of-mind reasoning mirror real-world multi-agent scenarios, making it a
useful testbed for evaluating the social intelligence of large language models
(LLMs). To support a systematic study, we introduce Mini-Mafia: a simplified
four-player variant with one mafioso, one detective, and two villagers. We set
the mafioso to kill a villager and the detective to investigate the mafioso
during the night, reducing the game to a single day phase of discussion and
voting. This setup isolates three interactive capabilities through
role-specific win conditions: the mafioso must deceive, the villagers must
detect deception, and the detective must effectively disclose information. To
measure these skills, we have LLMs play against each other, creating the
Mini-Mafia Benchmark: a two-stage framework that first estimates win rates
within fixed opponent configurations, then aggregates performance across them
using standardized scoring. Built entirely from model interactions without
external data, the benchmark evolves as new models are introduced, with each
one serving both as a new opponent and as a subject of evaluation. Our
experiments reveal counterintuitive results, including cases where smaller
models outperform larger ones. Beyond benchmarking, Mini-Mafia enables
quantitative study of emergent multi-agent dynamics such as name bias and
last-speaker advantage. It also contributes to AI safety by generating training
data for deception detectors and by tracking models' deception capabilities
against human baselines.

</details>


### [209] [Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents](https://arxiv.org/abs/2509.23045)
*Zonghan Yang,Shengjie Wang,Kelin Fu,Wenyang He,Weimin Xiong,Yibo Liu,Yibo Miao,Bofei Gao,Yejie Wang,Yingwei Ma,Yanhao Li,Yue Liu,Zhenxing Hu,Kaitai Zhang,Shuyi Wang,Huarong Chen,Flood Sung,Yang Liu,Yang Gao,Zhilin Yang,Tianyu Liu*

Main category: cs.AI

TL;DR: Agentless training provides structured skill priors that enable effective SWE-Agent adaptation, with Kimi-Dev achieving 60.4% on SWE-bench Verified and powering SWE-Agents to 48.6% pass@1.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between SWE-Agent frameworks (multi-turn interactions) and Agentless methods (single-turn verifiable steps) by showing they are not mutually exclusive.

Method: Curated Agentless training recipe to develop Kimi-Dev, then applied SFT adaptation on 5k publicly-available trajectories to enable SWE-Agent capabilities.

Result: Kimi-Dev achieved 60.4% on SWE-bench Verified (best among workflow approaches) and powered SWE-Agents to 48.6% pass@1, comparable to Claude 3.5 Sonnet.

Conclusion: Structured skill priors from Agentless training can effectively bridge workflow and agentic frameworks for creating transferable coding agents.

Abstract: Large Language Models (LLMs) are increasingly applied to software engineering
(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent
frameworks with multi-turn interactions and workflow-based Agentless methods
with single-turn verifiable steps. We argue these paradigms are not mutually
exclusive: reasoning-intensive Agentless training induces skill priors,
including localization, code edit, and self-reflection that enable efficient
and effective SWE-Agent adaptation. In this work, we first curate the Agentless
training recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\%
on SWE-bench Verified, the best among workflow approaches. With additional SFT
adaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to
48.6\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These
results show that structured skill priors from Agentless training can bridge
workflow and agentic frameworks for transferable coding agents.

</details>


### [210] [Risk Profiling and Modulation for LLMs](https://arxiv.org/abs/2509.23058)
*Yikai Wang,Xiaocheng Li,Guanting Chen*

Main category: cs.AI

TL;DR: This paper investigates how different LLM training stages (pre-trained, instruction-tuned, RLHF-aligned) exhibit varying risk behaviors and proposes methods to modulate these risk profiles using behavioral economics tools.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used for decision-making under uncertainty, but their risk profiles and how they're influenced by prompting and alignment methods remain underexplored. Existing studies focus on personality prompting or multi-agent interactions, leaving gaps in understanding post-training influences on LLM risk behavior.

Method: Proposed a pipeline for eliciting, steering, and modulating LLMs' risk profiles using utility-theoretic models from behavioral economics and finance. Compared pre-trained, instruction-tuned, and RLHF-aligned LLMs, and evaluated modulation strategies including prompt engineering, in-context learning, and post-training.

Result: Instruction-tuned models exhibit behaviors consistent with standard utility formulations, while pre-trained and RLHF-aligned models deviate more from fitted utility models. Post-training provides the most stable and effective modulation of risk preference compared to other strategies.

Conclusion: The findings provide insights into risk profiles of different LLM classes and stages, demonstrating how post-training effectively modulates these profiles, laying groundwork for future research on behavioral alignment and risk-aware LLM design.

Abstract: Large language models (LLMs) are increasingly used for decision-making tasks
under uncertainty; however, their risk profiles and how they are influenced by
prompting and alignment methods remain underexplored. Existing studies have
primarily examined personality prompting or multi-agent interactions, leaving
open the question of how post-training influences the risk behavior of LLMs. In
this work, we propose a new pipeline for eliciting, steering, and modulating
LLMs' risk profiles, drawing on tools from behavioral economics and finance.
Using utility-theoretic models, we compare pre-trained, instruction-tuned, and
RLHF-aligned LLMs, and find that while instruction-tuned models exhibit
behaviors consistent with some standard utility formulations, pre-trained and
RLHF-aligned models deviate more from any utility models fitted. We further
evaluate modulation strategies, including prompt engineering, in-context
learning, and post-training, and show that post-training provides the most
stable and effective modulation of risk preference. Our findings provide
insights into the risk profiles of different classes and stages of LLMs and
demonstrate how post-training modulates these profiles, laying the groundwork
for future research on behavioral alignment and risk-aware LLM design.

</details>


### [211] [Multiplayer Nash Preference Optimization](https://arxiv.org/abs/2509.23102)
*Fang Wu,Xu Huang,Weihao Xuan,Zhiwei Zhang,Yijia Xiao,Guancheng Wan,Xiaomin Li,Bing Hu,Peng Xia,Jure Leskovec,Yejin Choi*

Main category: cs.AI

TL;DR: MNPO extends Nash learning from human feedback to multiplayer settings, addressing limitations of two-player methods by formulating alignment as an n-player game where policies compete against populations of opponents.


<details>
  <summary>Details</summary>
Motivation: Existing RLHF methods based on Bradley-Terry assumptions struggle with non-transitive and heterogeneous real-world preferences, while current Nash learning approaches are limited to two-player interactions creating single-opponent bias.

Method: Multiplayer Nash Preference Optimization (MNPO) formulates alignment as an n-player game where each policy competes against a population of opponents while being regularized toward a reference model, extending duality gap concepts to multiplayer settings.

Result: MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios.

Conclusion: MNPO establishes a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences, inheriting equilibrium guarantees while enabling richer competitive dynamics.

Abstract: Reinforcement learning from human feedback (RLHF) has emerged as the standard
paradigm for aligning large language models (LLMs) with human preferences.
However, reward-based methods built on the Bradley-Terry assumption struggle to
capture the non-transitive and heterogeneous nature of real-world preferences.
To address this, recent studies have reframed alignment as a two-player Nash
game, giving rise to Nash learning from human feedback (NLHF). While this
perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong
theoretical and empirical guarantees, they remain fundamentally restricted to
two-player interactions, creating a single-opponent bias that fails to capture
the full complexity of realistic preference structures. In this work, we
introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework
that generalizes NLHF to the multiplayer regime. It formulates alignment as an
$n$-player game, where each policy competes against a population of opponents
while being regularized toward a reference model. Our framework establishes
well-defined Nash equilibria in multiplayer settings and extends the concept of
duality gap to quantify approximation quality. We demonstrate that MNPO
inherits the equilibrium guarantees of two-player methods while enabling richer
competitive dynamics and improved coverage of diverse preference structures.
Through comprehensive empirical evaluation, we show that MNPO consistently
outperforms existing NLHF baselines on instruction-following benchmarks,
achieving superior alignment quality under heterogeneous annotator conditions
and mixed-policy evaluation scenarios. Together, these results establish MNPO
as a principled and scalable framework for aligning LLMs with complex,
non-transitive human preferences. Code is available at
https://github.com/smiles724/MNPO.

</details>


### [212] [Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models](https://arxiv.org/abs/2509.23108)
*Morgan McCarty,Jorge Morales*

Main category: cs.AI

TL;DR: LLMs outperform humans on mental imagery tasks traditionally thought to require visual imagery, suggesting propositional reasoning may be sufficient for such tasks.


<details>
  <summary>Details</summary>
Motivation: To test if LLMs can solve mental imagery tasks that cognitive psychologists argue require visual mental imagery, challenging the assumption that language alone is insufficient.

Method: Created novel mental imagery tasks from cognitive psychology, tested state-of-the-art LLMs with text-only instructions, established human baseline with 100 subjects, and tested reasoning models with different reasoning token allocations.

Result: Best LLMs performed significantly above average human performance, with strongest performance when models allocated more reasoning tokens.

Conclusion: LLMs may have capability to complete imagery-dependent tasks despite non-pictorial architectures, suggesting propositional reasoning may be sufficient for tasks previously thought to require visual imagery.

Abstract: This study offers a novel approach for benchmarking complex cognitive
behavior in artificial systems. Almost universally, Large Language Models
(LLMs) perform best on tasks which may be included in their training data and
can be accomplished solely using natural language, limiting our understanding
of their emergent sophisticated cognitive capacities. In this work, we created
dozens of novel items of a classic mental imagery task from cognitive
psychology. A task which, traditionally, cognitive psychologists have argued is
solvable exclusively via visual mental imagery (i.e., language alone would be
insufficient). LLMs are perfect for testing this hypothesis. First, we tested
several state-of-the-art LLMs by giving text-only models written instructions
and asking them to report the resulting object after performing the
transformations in the aforementioned task. Then, we created a baseline by
testing 100 human subjects in exactly the same task. We found that the best
LLMs performed significantly above average human performance. Finally, we
tested reasoning models set to different levels of reasoning and found the
strongest performance when models allocate greater amounts of reasoning tokens.
These results provide evidence that the best LLMs may have the capability to
complete imagery-dependent tasks despite the non-pictorial nature of their
architectures. Our study not only demonstrates an emergent cognitive capacity
in LLMs while performing a novel task, but it also provides the field with a
new task that leaves lots of room for improvement in otherwise already highly
capable models. Finally, our findings reignite the debate over the formats of
representation of visual imagery in humans, suggesting that propositional
reasoning (or at least non-imagistic reasoning) may be sufficient to complete
tasks that were long-thought to be imagery-dependent.

</details>


### [213] [AttAnchor: Guiding Cross-Modal Token Alignment in VLMs with Attention Anchors](https://arxiv.org/abs/2509.23109)
*Junyang Zhang,Tianyi Zhu,Thierry Tambe*

Main category: cs.AI

TL;DR: AttAnchor is a parameter-free framework that improves cross-modal alignment in vision-language models by grouping semantically similar tokens across modalities, reducing hallucinations and improving performance on various benchmarks with minimal inference overhead.


<details>
  <summary>Details</summary>
Motivation: Current VLMs suffer from hallucinations and underperformance due to direct concatenation of image and text tokens with modality-blinded positional encoding, which creates unnecessary long-distance attention between semantically related cross-modal tokens.

Method: Proposes Attention Anchor framework that inserts text tokens near relevant visual patches to create semantic signposts, enabling true content-based cross-modal attention scores and efficient grouping of semantically similar tokens across modalities.

Result: Achieves improvements across 13 out of 15 metrics/benchmarks, including up to 32% gains on reasoning tasks and 15% improvements on hallucination benchmarks. Enables TinyLLaVA 1B to outperform larger models like LLaVA 7B and QwenVL 3B on POPE with only 0.1% inference time overhead.

Conclusion: AttAnchor is among the first works to investigate mixed-modal token grouping, providing an efficient solution for cross-modal locality enhancement that reduces hallucinations and improves VLM performance without disrupting semantic flow.

Abstract: A fundamental reason for the dominance of attention over RNNs and LSTMs in
LLMs is its ability to capture long-range dependencies by modeling direct
interactions between all tokens, overcoming the sequential limitations of
recurrent architectures. Similarly, a key reason why today's vision language
models (VLMs) hallucinate and underperform pure language models is that they
rely on direct concatenation of image and text tokens with a modality-blinded
positional encoding, which conveniently adopts the pretrained LLM backbone but
forces unnecessary long-distance attention between semantically related tokens
across modalities. This underscores the urgent need for mechanisms that
efficiently enhance token locality and cross-modal alignment. In response, we
propose Attention Anchor, a parameter-free framework that efficiently groups
semantically similar tokens across modalities, improving cross-modal locality.
By inserting text tokens near relevant visual patches, we create semantic
signposts that reveal true content-based cross-modal attention scores, guiding
the model to focus on the correct image regions for tasks such as VQA, MMBench
and POPE. This improves answer accuracy and reduces hallucinations without
disrupting the prompt's semantic flow. AttAnchor achieves improvements across
13 out of 15 different metrics and benchmarks, including up to 32% gains on
reasoning tasks and up to 15% improvements on hallucination benchmarks.
AttAnchor enables TinyLLaVA 1B to outperform much larger models like LLaVA 7B
and QwenVL 3B on POPE with only 0.1% inference time overhead. To the best of
our knowledge, this work is among the first to investigate mixed-modal token
grouping, where text and image tokens are clustered jointly into shared groups
rather than being grouped within a single modality or merely aligned post-hoc
with additional alignment losses.

</details>


### [214] [Exploring LLM-based Frameworks for Fault Diagnosis](https://arxiv.org/abs/2509.23113)
*Xian Yeow Lee,Lasitha Vidyaratne,Ahmed Farahat,Chetan Gupta*

Main category: cs.AI

TL;DR: LLM-based systems show promise for autonomous health monitoring in industrial settings, with multi-LLM architectures using statistical inputs performing best for fault detection, though they struggle with continual learning adaptation.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' potential for autonomous health monitoring with explainable outputs through natural language reasoning in sensor-rich industrial environments.

Method: Systematically evaluate LLM-system architecture (single-LLM vs. multi-LLM), input representations (raw vs. descriptive statistics), and context window size effects on diagnostic performance.

Result: LLM systems perform best with summarized statistical inputs; multi-LLM systems with specialized prompts offer improved sensitivity for fault classification compared to single-LLM systems.

Conclusion: LLMs show promise as transparent diagnostic tools but have limitations in continual learning settings, struggling with prediction calibration during repeated fault cycles.

Abstract: Large Language Model (LLM)-based systems present new opportunities for
autonomous health monitoring in sensor-rich industrial environments. This study
explores the potential of LLMs to detect and classify faults directly from
sensor data, while producing inherently explainable outputs through natural
language reasoning. We systematically evaluate how LLM-system architecture
(single-LLM vs. multi-LLM), input representations (raw vs. descriptive
statistics), and context window size affect diagnostic performance. Our
findings show that LLM systems perform most effectively when provided with
summarized statistical inputs, and that systems with multiple LLMs using
specialized prompts offer improved sensitivity for fault classification
compared to single-LLM systems. While LLMs can produce detailed and
human-readable justifications for their decisions, we observe limitations in
their ability to adapt over time in continual learning settings, often
struggling to calibrate predictions during repeated fault cycles. These
insights point to both the promise and the current boundaries of LLM-based
systems as transparent, adaptive diagnostic tools in complex environments.

</details>


### [215] [Transferring Vision-Language-Action Models to Industry Applications: Architectures, Performance, and Challenges](https://arxiv.org/abs/2509.23121)
*Shuai Li,Chen Yizhe,Li Dong,Liu Sichao,Lan Dapeng,Liu Yu,Zhibo Pang*

Main category: cs.AI

TL;DR: VLA models show basic grasping capability in industrial settings after fine-tuning but need significant improvements for complex tasks, diverse objects, and precision placing.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether current VLA models meet industrial requirements and analyze their limitations for real-world deployment.

Method: Compare performance of state-of-the-art VLA models in industrial scenarios, analyzing limitations from data collection and model architecture perspectives.

Result: VLA models can perform simple grasping tasks in industrial settings after fine-tuning, but struggle with complex environments, diverse object categories, and high-precision placing tasks.

Conclusion: VLA models have practical adaptability for industrial use but require task-specific enhancements to improve robustness, generalization, and precision.

Abstract: The application of artificial intelligence (AI) in industry is accelerating
the shift from traditional automation to intelligent systems with perception
and cognition. Vision language-action (VLA) models have been a key paradigm in
AI to unify perception, reasoning, and control. Has the performance of the VLA
models met the industrial requirements? In this paper, from the perspective of
industrial deployment, we compare the performance of existing state-of-the-art
VLA models in industrial scenarios and analyze the limitations of VLA models
for real-world industrial deployment from the perspectives of data collection
and model architecture. The results show that the VLA models retain their
ability to perform simple grasping tasks even in industrial settings after
fine-tuning. However, there is much room for performance improvement in complex
industrial environments, diverse object categories, and high precision placing
tasks. Our findings provide practical insight into the adaptability of VLA
models for industrial use and highlight the need for task-specific enhancements
to improve their robustness, generalization, and precision.

</details>


### [216] [SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems](https://arxiv.org/abs/2509.23130)
*Qian Cheng,Ruize Tang,Emilie Ma,Finn Hackett,Peiyang He,Yiming Su,Ivan Beschastnikh,Yu Huang,Xiaoxing Ma,Tianyin Xu*

Main category: cs.AI

TL;DR: SysMoBench is a benchmark for evaluating AI's ability to generate formal models of large, complex concurrent and distributed systems using TLA+ specifications.


<details>
  <summary>Details</summary>
Motivation: Formal models are expensive to write and maintain for complex systems, and existing AI approaches mostly target small code rather than complete systems. There's a need to assess AI's capability to abstract complex behavioral properties into formal models.

Method: Created SysMoBench with automated metrics including syntactic correctness, runtime correctness, conformance to system code, and invariant correctness. Currently includes nine diverse system artifacts like Raft implementations in Etcd and Redis, Spinlock and Mutex in Asterinas OS.

Result: The benchmark enables systematic evaluation of LLMs and agents in formal modeling, providing a foundation for understanding their capabilities and limitations.

Conclusion: SysMoBench establishes a firm foundation for evaluating AI-generated formal models and opens up promising research directions in AI-assisted system specification.

Abstract: Formal models are essential to specifying large, complex computer systems and
verifying their correctness, but are notoriously expensive to write and
maintain. Recent advances in generative AI show promise in generating certain
forms of specifications. However, existing work mostly targets small code, not
complete systems. It is unclear whether AI can deal with realistic system
artifacts, as this requires abstracting their complex behavioral properties
into formal models. We present SysMoBench, a benchmark that evaluates AI's
ability to formally model large, complex systems. We focus on concurrent and
distributed systems, which are keystones of today's critical computing
infrastructures, encompassing operating systems and cloud infrastructure. We
use TLA+, the it de facto specification language for concurrent and distributed
systems, though the benchmark can be extended to other specification languages.
We address the primary challenge of evaluating AI-generated models by
automating metrics like syntactic and runtime correctness, conformance to
system code, and invariant correctness. SysMoBench currently includes nine
diverse system artifacts: the Raft implementation of Etcd and Redis, the
Spinlock and Mutex in Asterinas OS, etc.; more artifacts are being actively
added. SysMoBench enables us to understand the capabilities and limitations of
today's LLMs and agents, putting tools in this area on a firm footing and
opening up promising new research directions.

</details>


### [217] [MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning](https://arxiv.org/abs/2509.23143)
*Charles L. Wang*

Main category: cs.AI

TL;DR: MathBode is a dynamic diagnostic tool that analyzes mathematical reasoning in LLMs using frequency-domain analysis of parameter-varying problems, revealing systematic low-pass behavior and phase lag that accuracy metrics miss.


<details>
  <summary>Details</summary>
Motivation: Standard one-shot accuracy metrics fail to capture the dynamic reasoning capabilities and systematic errors in LLMs' mathematical problem-solving. There's a need for more nuanced diagnostics that reveal how models handle parameter variations.

Method: Treat parametric problems as systems: drive a single parameter sinusoidally and fit first-harmonic responses of model outputs vs exact solutions. This yields frequency-resolved metrics (gain and phase) that form Bode-style fingerprints.

Result: The diagnostic reveals systematic low-pass behavior and growing phase lag across five mathematical problem families. Results separate frontier from mid-tier models on dynamic reasoning capabilities, with symbolic baseline showing near-ideal performance (G≈1, φ≈0).

Conclusion: MathBode provides a compact, reproducible protocol that complements standard benchmarks with actionable measurements of reasoning fidelity and consistency, offering deeper insights into LLMs' mathematical reasoning dynamics.

Abstract: This paper presents MathBode, a dynamic diagnostic for mathematical reasoning
in large language models (LLMs). Instead of one-shot accuracy, MathBode treats
each parametric problem as a system: we drive a single parameter sinusoidally
and fit first-harmonic responses of model outputs and exact solutions. This
yields interpretable, frequency-resolved metrics -- gain (amplitude tracking)
and phase (lag) -- that form Bode-style fingerprints. Across five closed-form
families (linear solve, ratio/saturation, compound interest, 2x2 linear
systems, similar triangles), the diagnostic surfaces systematic low-pass
behavior and growing phase lag that accuracy alone obscures. We compare several
models against a symbolic baseline that calibrates the instrument ($G \approx
1$, $\phi \approx 0$). Results separate frontier from mid-tier models on
dynamics, providing a compact, reproducible protocol that complements standard
benchmarks with actionable measurements of reasoning fidelity and consistency.
We open-source the dataset and code to enable further research and adoption.

</details>


### [218] [Coordination Requires Simplification: Thermodynamic Bounds on Multi-Objective Compromise in Natural and Artificial Intelligence](https://arxiv.org/abs/2509.23144)
*Atma Anand*

Main category: cs.AI

TL;DR: Thermodynamic Coordination Theory (TCT) shows that multi-agent coordination fundamentally requires information loss, with focal point solutions prioritizing findability over accuracy. Coordination protocols scale with agent complexity, forcing simplification and creating metastable states that change only through environmental phase transitions.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental thermodynamic constraints and information-theoretic limits of coordination across multiple agents with potentially conflicting objectives, and explain phenomena like cycling in multi-objective optimization and alignment faking in AI systems.

Method: Developed information-theoretic framework with minimum description length analysis of coordination protocols, derived scaling laws for coordination complexity, defined coordination temperature to predict critical phenomena, and extended topological Arrow's theorem to recursive preference aggregation.

Result: Found that coordination protocol length scales as L(P) ≥ NKlog₂K + N²d²log(1/ε), forcing progressive simplification. Coordination dynamics create persistent metastable states with hysteresis, requiring environmental phase transitions for change. Extended Arrow's theorem shows recursive binding in preference aggregation.

Conclusion: Coordination fundamentally requires radical information loss, with focal points prioritizing findability. This explains persistent cycling in optimization and alignment issues in AI systems, providing a unified thermodynamic framework for understanding coordination across diverse systems.

Abstract: Information-processing systems coordinating across multiple agents and
objectives face fundamental thermodynamic constraints. We show that solutions
with maximum utility to act as coordination focal points have much higher
selection pressure for being findable across agents rather than accuracy. We
derive that the information-theoretic minimum description length of
coordination protocols to precision $\varepsilon$ scales as $L(P)\geq NK\log_2
K+N^2d^2\log (1/\varepsilon)$ for $N$ agents with $d$ potentially conflicting
objectives and internal model complexity $K$. This scaling forces progressive
simplification, with coordination dynamics changing the environment itself and
shifting optimization across hierarchical levels. Moving from established focal
points requires re-coordination, creating persistent metastable states and
hysteresis until significant environmental shifts trigger phase transitions
through spontaneous symmetry breaking. We operationally define coordination
temperature to predict critical phenomena and estimate coordination work costs,
identifying measurable signatures across systems from neural networks to
restaurant bills to bureaucracies. Extending the topological version of Arrow's
theorem on the impossibility of consistent preference aggregation, we find it
recursively binds whenever preferences are combined. This potentially explains
the indefinite cycling in multi-objective gradient descent and alignment faking
in Large Language Models trained with reinforcement learning with human
feedback. We term this framework Thermodynamic Coordination Theory (TCT), which
demonstrates that coordination requires radical information loss.

</details>


### [219] [AI-Enhanced Distributed Channel Access for Collision Avoidance in Future Wi-Fi 8](https://arxiv.org/abs/2509.23154)
*Jinzhe Pan,Jingqing Wang,Yuehui Ouyang,Wenchi Cheng,Wei Zhang*

Main category: cs.AI

TL;DR: This paper proposes a multi-agent reinforcement learning framework for improving Wi-Fi channel access by replacing traditional binary exponential backoff with AI-optimized dynamic backoff selection that adapts to real-time conditions while maintaining legacy compatibility.


<details>
  <summary>Details</summary>
Motivation: Current Wi-Fi systems using binary exponential backoff suffer from suboptimal collision resolution in dense deployments and persistent fairness challenges due to inherent randomness, demanding fundamental improvements for emerging applications with stringent reliability requirements.

Method: Developed a dynamic backoff selection mechanism, introduced fairness quantification metric aligned with EDCA principles, and proposed a centralized training decentralized execution architecture using constrained multi-agent proximal policy optimization to jointly minimize collisions and guarantee fairness.

Result: Experimental results show significant reduction in collision probability compared to conventional BEB while preserving backward compatibility with commercial Wi-Fi devices, and the proposed fairness metric effectively eliminates starvation risks in heterogeneous scenarios.

Conclusion: The AI-optimized framework successfully improves distributed channel access mechanisms for unlicensed bands by integrating artificial intelligence optimization with legacy device coexistence, addressing both collision resolution and fairness challenges in dense wireless deployments.

Abstract: The exponential growth of wireless devices and stringent reliability
requirements of emerging applications demand fundamental improvements in
distributed channel access mechanisms for unlicensed bands. Current Wi-Fi
systems, which rely on binary exponential backoff (BEB), suffer from suboptimal
collision resolution in dense deployments and persistent fairness challenges
due to inherent randomness. This paper introduces a multi-agent reinforcement
learning framework that integrates artificial intelligence (AI) optimization
with legacy device coexistence. We first develop a dynamic backoff selection
mechanism that adapts to real-time channel conditions through access deferral
events while maintaining full compatibility with conventional CSMA/CA
operations. Second, we introduce a fairness quantification metric aligned with
enhanced distributed channel access (EDCA) principles to ensure equitable
medium access opportunities. Finally, we propose a centralized training
decentralized execution (CTDE) architecture incorporating neighborhood activity
patterns as observational inputs, optimized via constrained multi-agent
proximal policy optimization (MAPPO) to jointly minimize collisions and
guarantee fairness. Experimental results demonstrate that our solution
significantly reduces collision probability compared to conventional BEB while
preserving backward compatibility with commercial Wi-Fi devices. The proposed
fairness metric effectively eliminates starvation risks in heterogeneous
scenarios.

</details>


### [220] [Limit Analysis for Symbolic Multi-step Reasoning Tasks with Information Propagation Rules Based on Transformers](https://arxiv.org/abs/2509.23178)
*Tian Qin,Yuhan Chen,Zhiwei Wang,Zhi-Qin John Xu*

Main category: cs.AI

TL;DR: This paper analyzes the intrinsic reasoning mechanism of Transformers and establishes theoretical bounds on their reasoning capabilities, showing that the limit number of reasoning steps for an L-layer Transformer is between O(3^(L-1)) and O(2^(L-1)) in a single pass.


<details>
  <summary>Details</summary>
Motivation: While Transformers can perform reasoning tasks, their intrinsic reasoning mechanisms remain poorly understood. The authors aim to theoretically analyze the reasoning capabilities of Transformers to better understand their limitations and potential.

Method: The authors propose a set of information propagation rules based on Transformers and use symbolic reasoning tasks to theoretically analyze the limit reasoning steps that Transformers can perform.

Result: The analysis shows that for a Transformer model with L attention layers, the limit number of reasoning steps achievable in a single pass is bounded between O(3^(L-1)) and O(2^(L-1)).

Conclusion: This work provides theoretical bounds on the reasoning capabilities of Transformers, revealing that their reasoning depth grows exponentially with the number of attention layers, with specific upper and lower bounds established.

Abstract: Transformers are able to perform reasoning tasks, however the intrinsic
mechanism remains widely open. In this paper we propose a set of information
propagation rules based on Transformers and utilize symbolic reasoning tasks to
theoretically analyze the limit reasoning steps. We show that the limit number
of reasoning steps is between $O(3^{L-1})$ and $O(2^{L-1})$ for a model with
$L$ attention layers in a single-pass.

</details>


### [221] [Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction](https://arxiv.org/abs/2509.23186)
*Qimin Zhong,Hao Liao,Siwei Wang,Mingyang Zhou,Xiaoqun Wu,Rui Mao,Wei Chen*

Main category: cs.AI

TL;DR: The paper investigates how Multi-Token Prediction (MTP) helps LLMs learn transitive relations for complex planning tasks, revealing that transfer layers capture multi-step adjacency information, and proposes Next-Token Injection and Transformer-based transfer layers to improve path-planning capabilities.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with learning transitive relations, which are crucial for complex planning tasks. The authors aim to address this bottleneck by exploring the MTP paradigm.

Method: Theoretical analysis of MTP with Transformer architecture, proposing Next-Token Injection (NTI) and Transformer-based transfer layer strategies to enhance transitive relation learning.

Result: Experiments on synthetic graphs and Blocksworld planning benchmark validate that the improvements significantly enhance the model's path-planning capability by capturing unobserved transitive reachability relations.

Conclusion: The findings deepen understanding of how Transformers with MTP learn in complex planning tasks and provide practical strategies to overcome the transitivity bottleneck, enabling structurally aware and general-purpose planning models.

Abstract: Large Language Models (LLMs) have achieved impressive performance across
diverse tasks but continue to struggle with learning transitive relations, a
cornerstone for complex planning. To address this issue, we investigate the
Multi-Token Prediction (MTP) paradigm and its impact to transitive relation
learning. We theoretically analyze the MTP paradigm using a Transformer
architecture composed of a shared output head and a transfer layer. Our
analysis reveals that the transfer layer gradually learns the multi-step
adjacency information, which in turn enables the backbone model to capture
unobserved transitive reachability relations beyond those directly present in
the training data, albeit with some inevitable noise in adjacency estimation.
Building on this foundation, we propose two strategies to enhance the transfer
layer and overall learning quality: Next-Token Injection (NTI) and a
Transformer-based transfer layer. Our experiments on both synthetic graphs and
the Blocksworld planning benchmark validate our theoretical findings and
demonstrate that the improvements significantly enhance the model's
path-planning capability. These findings deepen our understanding of how
Transformers with MTP learn in complex planning tasks, and provide practical
strategies to overcome the transitivity bottleneck, paving the way toward
structurally aware and general-purpose planning models.

</details>


### [222] [AutoEP: LLMs-Driven Automation of Hyperparameter Evolution for Metaheuristic Algorithms](https://arxiv.org/abs/2509.23189)
*Zhenxing Xu,Yizhe Zhang,Weidong Bao,Hao Wang,Ming Chen,Haoran Ye,Wenzheng Jiang,Hui Yan,Ji Wang*

Main category: cs.AI

TL;DR: AutoEP is a novel framework that uses Large Language Models (LLMs) as zero-shot reasoning engines for automated hyperparameter configuration, combining online Exploratory Landscape Analysis with multi-LLM reasoning chains to outperform state-of-the-art tuners without training.


<details>
  <summary>Details</summary>
Motivation: Traditional learning-based hyperparameter configuration methods suffer from high sample complexity and poor generalization, necessitating a more efficient and generalizable approach.

Method: AutoEP integrates an online Exploratory Landscape Analysis (ELA) module for real-time search dynamics feedback with a multi-LLM reasoning chain that interprets this feedback to generate adaptive hyperparameter strategies, eliminating the need for training.

Result: AutoEP consistently outperforms state-of-the-art tuners across three metaheuristics on diverse combinatorial optimization benchmarks, enabling open-source models like Qwen3-30B to match GPT-4's performance.

Conclusion: AutoEP demonstrates a powerful and accessible paradigm for automated hyperparameter design by grounding LLM reasoning in empirical data, effectively mitigating hallucination while achieving superior performance.

Abstract: Dynamically configuring algorithm hyperparameters is a fundamental challenge
in computational intelligence. While learning-based methods offer automation,
they suffer from prohibitive sample complexity and poor generalization. We
introduce AutoEP, a novel framework that bypasses training entirely by
leveraging Large Language Models (LLMs) as zero-shot reasoning engines for
algorithm control. AutoEP's core innovation lies in a tight synergy between two
components: (1) an online Exploratory Landscape Analysis (ELA) module that
provides real-time, quantitative feedback on the search dynamics, and (2) a
multi-LLM reasoning chain that interprets this feedback to generate adaptive
hyperparameter strategies. This approach grounds high-level reasoning in
empirical data, mitigating hallucination. Evaluated on three distinct
metaheuristics across diverse combinatorial optimization benchmarks, AutoEP
consistently outperforms state-of-the-art tuners, including neural evolution
and other LLM-based methods. Notably, our framework enables open-source models
like Qwen3-30B to match the performance of GPT-4, demonstrating a powerful and
accessible new paradigm for automated hyperparameter design. Our code is
available at https://anonymous.4open.science/r/AutoEP-3E11

</details>


### [223] [$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding](https://arxiv.org/abs/2509.23234)
*Runyan Tan,Shuang Wu,Phillip Howard*

Main category: cs.AI

TL;DR: p-less sampling is a hyperparameter-free decoding strategy that dynamically sets truncation thresholds based on token probability distributions, outperforming existing sampling methods across various tasks while maintaining quality at higher temperatures.


<details>
  <summary>Details</summary>
Motivation: Existing sampling methods for LLMs are sensitive to hyperparameter settings and their performance varies across different generation tasks and temperature configurations, requiring manual tuning.

Method: p-less sampling uses information theory to dynamically set truncation thresholds at each decoding step based on the entire token probability distribution, eliminating the need for hyperparameters.

Result: p-less sampling consistently outperforms existing sampling approaches across math, logical reasoning, and creative writing tasks, with less degradation at higher temperatures and improved inference efficiency through lower token sampling times and shorter generation lengths.

Conclusion: p-less sampling provides a robust, hyperparameter-free alternative to existing decoding strategies that maintains high output quality across temperature variations while improving computational efficiency.

Abstract: Obtaining high-quality outputs from Large Language Models (LLMs) often
depends upon the choice of a sampling-based decoding strategy to
probabilistically choose the next token at each generation step. While a
variety of such sampling methods have been proposed, their performance can be
sensitive to the selection of hyperparameters which may require different
settings depending upon the generation task and temperature configuration. In
this work, we introduce $p$-less sampling: an information-theoretic approach to
sampling which dynamically sets a truncation threshold at each decoding step
based on the entire token probability distribution. Unlike existing methods,
$p$-less sampling has no hyperparameters and consistently produces high-quality
outputs as temperature increases. We provide theoretical perspectives on
$p$-less sampling to ground our proposed method and conduct experiments to
empirically validate its effectiveness across a range of math, logical
reasoning, and creative writing tasks. Our results demonstrate how $p$-less
sampling consistently outperforms existing sampling approaches while exhibiting
much less degradation in text quality at higher temperature values. We further
show how $p$-less achieves greater inference-time efficiency than alternative
methods through lower average token sampling times and shorter generation
lengths, without sacrificing accuracy. Finally, we provide analyses to
highlight the benefits of $p$-less through qualitative examples, case studies,
and diversity assessments.

</details>


### [224] [Agentic AI Reasoning for Mobile Edge General Intelligence: Fundamentals, Approaches, and Directions](https://arxiv.org/abs/2509.23248)
*Mingyi Luo,Ruichen Zhang,Xiangwang Hou,Jun Du,Chunxiao Jiang,Yong Ren,Dusit Niyato,Shiwen Mao*

Main category: cs.AI

TL;DR: Proposes a joint optimization framework for efficient LLM reasoning deployment in Mobile Edge General Intelligence (MEGI) environments, addressing computational challenges through adaptive CoT prompting and distributed MoE architecture.


<details>
  <summary>Details</summary>
Motivation: To enable real-time, privacy-preserving LLM-based agentic AI reasoning at network edges despite high computational demands and limited edge device resources.

Method: Combines reasoning enhancement via adaptive Chain-of-Thought prompting with scalable deployment through distributed Mixture of Experts architecture, dynamically adjusting expert activation and reasoning depth based on task complexity and device capabilities.

Result: Experimental evaluations demonstrate effective balance between reasoning quality and resource efficiency, validating practical viability in resource-constrained MEGI environments.

Conclusion: The proposed framework successfully enables sophisticated LLM reasoning capabilities in mobile edge computing environments while maintaining resource efficiency.

Abstract: The rapid advancement of large language models (LLMs) has enabled an
emergence of agentic artificial intelligence (AI) with powerful reasoning and
autonomous decision-making capabilities. This integration with edge computing
has led to the development of Mobile Edge General Intelligence (MEGI), which
brings real-time, privacy-preserving reasoning to the network edge. However,
deploying LLM-based agentic AI reasoning in MEGI environments poses significant
challenges due to the high computational demands of reasoning and the limited
resources of edge devices. To address these challenges, we propose a joint
optimization framework for efficient LLM reasoning deployment in MEGI. First,
we review methods that enhance LLM reasoning capabilities, such as
Chain-of-Thought (CoT) prompting, Supervised Fine-Tuning (SFT), and Mixture of
Experts (MoE). Next, we present a distributed framework that addresses two
correlated aspects: reasoning enhancement through adaptive CoT prompting and
scalable deployment through distributed MoE architecture. The framework
dynamically activates expert networks and adjusts reasoning depth based on task
complexity and device capabilities. We further conduct experimental evaluations
in mobile edge environments. Experimental results demonstrate the framework's
effectiveness in balancing reasoning quality with resource efficiency,
validating the practical viability of deploying sophisticated LLM reasoning
capabilities in resource-constrained MEGI environments.

</details>


### [225] [Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned](https://arxiv.org/abs/2509.23250)
*Brandon Ong,Tej Deep Pala,Vernon Toh,William Chandra Tjhi,Soujanya Poria*

Main category: cs.AI

TL;DR: This paper explores Vision-Language Process Reward Models (VL-PRMs) for improving reasoning in Vision Language Models, introducing hybrid data synthesis, perception-focused supervision, and systematic test-time scaling strategies that outperform existing methods across multiple multimodal benchmarks.


<details>
  <summary>Details</summary>
Motivation: While Process Reward Models (PRMs) have been well-studied in text domains, their extension to Vision Language Models (VLMs) remains limited. Existing VL-PRMs rely on noisy Monte Carlo Tree Search data and lack generalization across tasks, motivating the need for better VL-PRM design.

Method: Proposes a hybrid data synthesis framework combining MCTS with strong VLM judgments, introduces perception-focused supervision for visual grounding error detection, and systematically evaluates multiple test-time scaling strategies.

Result: Experiments on five multimodal benchmarks show VL-PRMs can outperform process step selection when used as Outcome Reward Models, smaller VL-PRMs can match larger ones in error detection, perception-level supervision significantly improves test-time scaling, and TTS performance improves even on untrained math reasoning datasets.

Conclusion: The work provides key insights into VL-PRM design and demonstrates their effectiveness in enhancing VLM reasoning capabilities, motivating further research in this area.

Abstract: Process Reward Models (PRMs) provide step-level supervision that improves the
reliability of reasoning in large language models. While PRMs have been
extensively studied in text-based domains, their extension to Vision Language
Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on
Monte Carlo Tree Search (MCTS) for data construction, which can often produce
noisy supervision signals and limit generalization across tasks. In this work,
we aim to elucidate the design space of VL-PRMs by exploring diverse strategies
for dataset construction, training, and test-time scaling. First, we introduce
a hybrid data synthesis framework that combines MCTS with judgments from a
strong VLM, producing more accurate step-level labels. Second, we propose
perception-focused supervision, enabling our PRM to explicitly detect errors at
the visual grounding stage of reasoning. Third, we systematically evaluate
multiple test-time scaling strategies, showing that our PRMs can reliably guide
VLMs toward more accurate solutions. Our experiments covering five diverse
multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and
MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome
Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM
guided process step selection, (ii) smaller VL-PRMs can match or even surpass
larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning
abilities in stronger VLM backbones, (iv) perception-level supervision leads to
significant gains in test-time scaling, and (v) TTS performance of different
policies improve on advanced math reasoning datasets despite not training
VL-PRMs on such datasets. We hope our work will motivate further research and
support the advancement of VLMs.

</details>


### [226] [GUI-PRA: Process Reward Agent for GUI Tasks](https://arxiv.org/abs/2509.23263)
*Tao Xiong,Xavier Hu,Yurun Chen,Yuhang Liu,Changqiao Wu,Pengzhi Gao,Wei Liu,Jian Luan,Shengyu Zhang*

Main category: cs.AI

TL;DR: GUI-PRA is a process reward agent that addresses challenges in GUI task automation by using dynamic memory mechanisms and adaptive UI perception to provide better process rewards than standard PRMs.


<details>
  <summary>Details</summary>
Motivation: Standard Process Reward Models (PRMs) struggle with GUI tasks due to the 'lost in the middle' phenomenon with long history data and lack GUI changing awareness, leading to poor performance in long-horizon GUI automation tasks.

Method: GUI-PRA introduces two key mechanisms: 1) Dynamic memory with relevance-based retrieval and progressive summarization to handle long histories, and 2) Adaptive UI perception that reasons about UI state changes and selects appropriate tools to gather visual evidence.

Result: The proposed approach enables better process reward provision by intelligently processing historical context and actively perceiving UI state changes, overcoming limitations of standard PRMs in GUI domains.

Conclusion: GUI-PRA provides a more effective solution for process reward modeling in GUI tasks by addressing both the 'lost in the middle' problem and the lack of UI changing awareness through its novel dynamic memory and adaptive perception mechanisms.

Abstract: Graphical User Interface (GUI) Agents powered by Multimodal Large Language
Models (MLLMs) show significant potential for automating tasks. However, they
often struggle with long-horizon tasks, leading to frequent failures. Process
Reward Models (PRMs) are a promising solution, as they can guide these agents
with crucial process signals during inference. Nevertheless, their application
to the GUI domain presents unique challenges. When processing dense artificial
inputs with long history data, PRMs suffer from a "lost in the middle"
phenomenon, where the overwhelming historical context compromises the
evaluation of the current step. Furthermore, standard PRMs lacks GUI changing
awareness, providing static evaluations that are disconnected from the dynamic
consequences of actions, a critical mismatch with the inherently dynamic nature
of GUI tasks. In response to these challenges, we introduce GUI-PRA (Process
Reward Agent for GUI Tasks), a judge agent designed to better provide process
reward than standard PRM by intelligently processing historical context and
actively perceiving UI state changes. Specifically, to directly combat the
``lost in the middle'' phenomenon, we introduce a dynamic memory mechanism
consisting of two core components: a Relevance-based Retrieval Module to
actively fetch pertinent information from long histories and a Progressive
Summarization Module to dynamically condense growing interaction data, ensuring
the model focuses on relevant context. Moreover, to address the lack of UI
changing awareness, we introduce an Aadaptive UI Perception mechanism. This
mechanism enables the agent to reason about UI state changes and dynamically
select the most appropriate tool to gather grounded visual evidence, ensuring
its evaluation is always informed by the current UI context.

</details>


### [227] [Socio-Economic Model of AI Agents](https://arxiv.org/abs/2509.23270)
*Yuxinyue Qian,Jun Liu*

Main category: cs.AI

TL;DR: The paper develops a heterogeneous agent-based modeling framework to study AI collaboration's impact on social output under resource constraints, finding that AI agents significantly boost output with nonlinear growth from network effects.


<details>
  <summary>Details</summary>
Motivation: To understand how AI integration affects socio-economic systems and aggregate social output when resources are constrained, as modern systems increasingly incorporate AI technologies.

Method: Built five progressively extended agent-based models: baseline human collaboration, AI collaborators, network effects, independent producers, and combined network effects with independent production.

Result: AI agents significantly increase aggregate social output; network effects create nonlinear growth exceeding individual contributions; independent producers yield higher long-term growth; network effects show increasing returns to scale.

Conclusion: AI collaboration under resource constraints can dramatically enhance social output, particularly when leveraging network effects and treating agents as independent producers, demonstrating strong potential for increasing returns to scale.

Abstract: Modern socio-economic systems are undergoing deep integration with artificial
intelligence technologies. This paper constructs a heterogeneous agent-based
modeling framework that incorporates both human workers and autonomous AI
agents, to study the impact of AI collaboration under resource constraints on
aggregate social output. We build five progressively extended models: Model 1
serves as the baseline of pure human collaboration; Model 2 introduces AI as
collaborators; Model 3 incorporates network effects among agents; Model 4
treats agents as independent producers; and Model 5 integrates both network
effects and independent agent production. Through theoretical derivation and
simulation analysis, we find that the introduction of AI agents can
significantly increase aggregate social output. When considering network
effects among agents, this increase exhibits nonlinear growth far exceeding the
simple sum of individual contributions. Under the same resource inputs,
treating agents as independent producers provides higher long-term growth
potential; introducing network effects further demonstrates strong
characteristics of increasing returns to scale.

</details>


### [228] [Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning](https://arxiv.org/abs/2509.23285)
*Yifei Chen,Guanting Dong,Zhicheng Dou*

Main category: cs.AI

TL;DR: Tool-Light is a framework that improves LLMs' Tool-Integrated Reasoning by using information entropy analysis and multi-stage fine-tuning to optimize tool usage efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LLMs using Tool-Integrated Reasoning show suboptimal behaviors like insufficient/excessive tool usage and overthinking. The challenge is to incentivize efficient and accurate TIR while stabilizing reasoning.

Method: Proposed Tool-Light framework with: 1) Information entropy analysis of tool call impacts; 2) Dataset construction using continuous self-evolved sampling with vanilla and entropy-guided sampling; 3) Two-stage training: SFT and Self-Evolved DPO.

Result: Experimental results on 10 datasets demonstrate Tool-Light significantly improves model efficiency in executing TIR tasks.

Conclusion: Tool-Light effectively addresses TIR optimization by leveraging information entropy insights and multi-stage fine-tuning, achieving improved reasoning efficiency and accuracy.

Abstract: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to
improve their internal reasoning ability by integrating external tools.
However, models employing TIR often display suboptimal behaviors, such as
insufficient or excessive tool usage and overthinking after tool calls. The
challenge of incentivizing LLMs to perform TIR efficiently and accurately,
while stabilizing the reasoning process, remains an open question. In this
paper, we start by exploring the impact of tool calls on model reasoning from
the perspective of information entropy. Our findings indicate that tool call
results lead to a distinct change in the information entropy of subsequent
reasoning, with the overall entropy of the reasoning chain varying based on the
number of tool calls. Building on these insights, we propose Tool-Light, a
framework designed to encourage LLMs to perform TIR efficiently and accurately.
Our framework includes dataset construction and multi-stage fine-tuning. For
dataset construction, we employ continuous self-evolved sampling using the
fine-tuned model, integrating both vanilla sampling and entropy-guided
sampling. Besides, we establish strict criteria for selecting positive-negative
pairs during sampling. The training process involves a two-stage approach,
comprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference
Optimization (DPO). Experimental results on 10 datasets demonstrate the
effectiveness of Tool-Light, significantly improving the model's efficiency in
executing TIR tasks.

</details>


### [229] [Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning](https://arxiv.org/abs/2509.23292)
*Ningning Xu,Yuxuan Jiang,Shubhashis Roy Dipta*

Main category: cs.AI

TL;DR: This paper proposes a pattern-aware approach for tool-integrated reasoning that identifies two common patterns (calculator and algorithmic) and uses a two-stage framework to improve code usage and accuracy on math problems.


<details>
  <summary>Details</summary>
Motivation: Prior work on tool-integrated reasoning mainly focused on when to invoke tools but overlooked how tools are applied, leading to failures even with sound reasoning due to misaligned pattern choices.

Method: A two-stage framework that first builds code competence from both calculator and algorithmic patterns, then aligns pattern selection with teacher preferences.

Result: Significant improvements on challenging math datasets: Code@1 on MATH500 increased from 64.0% to 70.5%, and on AIME24 from 26.7% to 50.0%.

Conclusion: Pattern-aware approaches are highly effective for tool-integrated reasoning, substantially improving both code usage and accuracy on complex problems.

Abstract: Tool-integrated reasoning (TIR) has become a key approach for improving large
reasoning models (LRMs) on complex problems. Prior work has mainly studied when
to invoke tools, while overlooking how tools are applied. We identify two
common patterns: a calculator pattern that uses code for direct computation,
and an algorithmic pattern that encodes problems as programs. Misaligned
choices often cause failures even when reasoning is sound. We propose a
two-stage framework that first builds code competence from both patterns and
then aligns pattern selection with teacher preferences. Across challenging math
datasets, our pattern-aware method substantially improves both code usage and
accuracy, for instance raising Code@1 on MATH500 from 64.0% to 70.5% and on
AIME24 from 26.7% to 50.0%. These gains highlight the effectiveness of a
pattern-aware approach for tool-integrated reasoning.

</details>


### [230] [Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking](https://arxiv.org/abs/2509.23392)
*Jinyi Han,Ying Huang,Ying Liao,Zishang Jiang,Xikun Lu,Haiquan Zhao,Xinyi Wang,Guanghao Zhou,Sihang Jiang,Jiaqing Liang,Weikang Zhou,Zeye Sun,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: JET trains Large Reasoning Models to proactively terminate unnecessary reasoning steps, achieving significant efficiency improvements without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models incur substantial computational costs due to deep reasoning, and existing methods struggle to construct short reasoning paths during rollout.

Method: JET performs trajectory truncation during rollout to expose models to short reasoning paths and uses quality-controlled length reward to encourage concise reasoning while maintaining correctness.

Result: JET significantly improves reasoning efficiency without sacrificing accuracy. DeepSeek-Distill-Qwen-1.5B achieved 4.6% accuracy gain while reducing output length by 46.3% on Olympiad benchmark.

Conclusion: JET enables efficient reasoning by training models to terminate unnecessary reasoning steps, demonstrating that LRMs accumulate sufficient information early in reasoning, making further steps redundant.

Abstract: Large Reasoning Models (LRMs) have achieved impressive performance on
challenging tasks, yet their deep reasoning often incurs substantial
computational costs. To achieve efficient reasoning, existing reinforcement
learning methods still struggle to construct short reasoning path during the
rollout stage, limiting effective learning. Inspired by Evidence Accumulation
Models, we find that LRMs have accumulated sufficient information early in
reasoning, making further reasoning steps redundant. Based on this insight, we
propose Just-Enough Thinking (JET), which trains models to proactively
terminate unnecessary reasoning. JET performs trajectory truncation during
rollout to expose the model to short, distributionally consistent reasoning
paths. Besides, it uses a quality-controlled length reward to better encourage
concise reasoning while maintaining correctness. Extensive experiments
demonstrate that JET significantly improves reasoning efficiency without
sacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6%
accuracy gain while reducing output length by 46.3% on the Olympiad benchmark.
Our code is available in the GitHub.

</details>


### [231] [From Conversation to Query Execution: Benchmarking User and Tool Interactions for EHR Database Agents](https://arxiv.org/abs/2509.23415)
*Gyubok Lee,Woosog Chay,Heeyoung Kwak,Yeong Hwa Kim,Haanju Yoo,Oksoon Jeong,Meong Hi Son,Edward Choi*

Main category: cs.AI

TL;DR: EHR-ChatQA is a benchmark for LLM-powered EHR database agents that addresses query ambiguity and value mismatch through interactive query refinement, showing high initial success rates but poor consistency across trials.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents lack benchmarks for real-world clinical data access flows, hindered by query ambiguity from vague questions and value mismatch between user terminology and database entries.

Method: Introduces EHR-ChatQA benchmark with simulated LLM-based user across two interaction flows: Incremental Query Refinement (IncreQA) and Adaptive Query Refinement (AdaptQA), evaluating agents' ability to clarify questions, resolve value mismatches, and generate correct SQL.

Result: Agents achieve high Pass@5 of 90-95% on IncreQA and 60-80% on AdaptQA, but Pass^5 (consistent success across all trials) is substantially lower by 35-60%, indicating poor robustness despite high performance.

Conclusion: There's a critical need to build agents that are not only performant but also robust for safety-critical EHR domains, with diagnostic insights provided for future development.

Abstract: Despite the impressive performance of LLM-powered agents, their adoption for
Electronic Health Record (EHR) data access remains limited by the absence of
benchmarks that adequately capture real-world clinical data access flows. In
practice, two core challenges hinder deployment: query ambiguity from vague
user questions and value mismatch between user terminology and database
entries. To address this, we introduce EHR-ChatQA an interactive database
question answering benchmark that evaluates the end-to-end workflow of database
agents: clarifying user questions, using tools to resolve value mismatches, and
generating correct SQL to deliver accurate answers. To cover diverse patterns
of query ambiguity and value mismatch, EHR-ChatQA assesses agents in a
simulated environment with an LLM-based user across two interaction flows:
Incremental Query Refinement (IncreQA), where users add constraints to existing
queries, and Adaptive Query Refinement (AdaptQA), where users adjust their
search goals mid-conversation. Experiments with state-of-the-art LLMs (e.g.,
o4-mini and Gemini-2.5-Flash) over five i.i.d. trials show that while agents
achieve high Pass@5 of 90-95% (at least one of five trials) on IncreQA and
60-80% on AdaptQA, their Pass^5 (consistent success across all five trials) is
substantially lower by 35-60%. These results underscore the need to build
agents that are not only performant but also robust for the safety-critical EHR
domain. Finally, we provide diagnostic insights into common failure modes to
guide future agent development.

</details>


### [232] [Democratizing AI scientists using ToolUniverse](https://arxiv.org/abs/2509.23426)
*Shanghua Gao,Richard Zhu,Pengwei Sui,Zhenglun Kong,Sufian Aldogom,Yepeng Huang,Ayush Noori,Reza Shamji,Krishna Parvataneni,Theodoros Tsiligkaridis,Marinka Zitnik*

Main category: cs.AI

TL;DR: ToolUniverse is an ecosystem for building AI scientists that integrates over 600 tools, automatically refines interfaces, creates tools from natural language, and composes agentic workflows.


<details>
  <summary>Details</summary>
Motivation: AI scientists are difficult to build due to being bespoke, tied to rigid workflows, and lacking shared environments that unify tools, data, and analyses.

Method: ToolUniverse standardizes how AI scientists identify and call tools, automatically refines tool interfaces, creates new tools from natural language descriptions, iteratively optimizes tool specifications, and composes tools into agentic workflows.

Result: In a hypercholesterolemia case study, ToolUniverse created an AI scientist that identified a potent drug analog with favorable predicted properties.

Conclusion: ToolUniverse provides comparable infrastructure to omics ecosystems, enabling interoperability, reuse, and community-driven development for AI scientists.

Abstract: AI scientists are emerging computational systems that serve as collaborative
partners in discovery. These systems remain difficult to build because they are
bespoke, tied to rigid workflows, and lack shared environments that unify
tools, data, and analyses into a common ecosystem. In omics, unified ecosystems
have transformed research by enabling interoperability, reuse, and
community-driven development; AI scientists require comparable infrastructure.
We present ToolUniverse, an ecosystem for building AI scientists from any
language or reasoning model, whether open or closed. TOOLUNIVERSE standardizes
how AI scientists identify and call tools, integrating more than 600 machine
learning models, datasets, APIs, and scientific packages for data analysis,
knowledge retrieval, and experimental design. It automatically refines tool
interfaces for correct use by AI scientists, creates new tools from natural
language descriptions, iteratively optimizes tool specifications, and composes
tools into agentic workflows. In a case study of hypercholesterolemia,
ToolUniverse was used to create an AI scientist to identify a potent analog of
a drug with favorable predicted properties. The open-source ToolUniverse is
available at https://aiscientist.tools.

</details>


### [233] [Beyond Embeddings: Interpretable Feature Extraction for Binary Code Similarity](https://arxiv.org/abs/2509.23449)
*Charles E. Gagnon,Steven H. H. Ding,Philippe Charland,Benjamin C. M. Fung*

Main category: cs.AI

TL;DR: A new method for binary code similarity detection that uses language model agents to generate structured, interpretable features from assembly code, bridging the gap between hand-crafted features and opaque embeddings.


<details>
  <summary>Details</summary>
Motivation: Current binary code similarity detection methods face compromises between interpretability, generalizability, and scalability. Hand-crafted features are interpretable but shallow, while embedding methods are robust but opaque and face scalability-accuracy trade-offs.

Method: Use language model-based agents to conduct structured reasoning analysis of assembly code, generating human-readable features like input/output types, side effects, notable constants, and algorithmic intent.

Result: Achieves 42% recall@1 in cross-architecture and 62% in cross-optimization tasks without matching training, comparable to embedding methods with training. When combined with embeddings, significantly outperforms state-of-the-art methods.

Conclusion: The proposed method demonstrates that accuracy, scalability, and interpretability can coexist in binary code similarity detection by combining structured reasoning with embedding approaches.

Abstract: Binary code similarity detection is a core task in reverse engineering. It
supports malware analysis and vulnerability discovery by identifying
semantically similar code in different contexts. Modern methods have progressed
from manually engineered features to vector representations. Hand-crafted
statistics (e.g., operation ratios) are interpretable, but shallow and fail to
generalize. Embedding-based methods overcome this by learning robust
cross-setting representations, but these representations are opaque vectors
that prevent rapid verification. They also face a scalability-accuracy
trade-off, since high-dimensional nearest-neighbor search requires
approximations that reduce precision. Current approaches thus force a
compromise between interpretability, generalizability, and scalability.
  We bridge these gaps using a language model-based agent to conduct structured
reasoning analysis of assembly code and generate features such as input/output
types, side effects, notable constants, and algorithmic intent. Unlike
hand-crafted features, they are richer and adaptive. Unlike embeddings, they
are human-readable, maintainable, and directly searchable with inverted or
relational indexes. Without any matching training, our method respectively
achieves 42% and 62% for recall@1 in cross-architecture and cross-optimization
tasks, comparable to embedding methods with training (39% and 34%). Combined
with embeddings, it significantly outperforms the state-of-the-art,
demonstrating that accuracy, scalability, and interpretability can coexist.

</details>


### [234] [ViTSP: A Vision Language Models Guided Framework for Large-Scale Traveling Salesman Problems](https://arxiv.org/abs/2509.23465)
*Zhuoli Yin,Yi Ding,Reem Khir,Hua Cai*

Main category: cs.AI

TL;DR: ViTSP is a novel framework that uses pre-trained vision language models to visually guide solving large-scale Traveling Salesman Problems by identifying promising subproblems, achieving optimality gaps below 0.2% on instances up to 88k nodes.


<details>
  <summary>Details</summary>
Motivation: Traditional TSP solving methods face scalability issues and require domain-specific parameter tuning, while learning-based approaches suffer from poor generalization and limited scalability due to fixed training data.

Method: Leverages pre-trained vision language models to identify promising small-scale subproblems from visualized TSP instances, then optimizes these subproblems using off-the-shelf solvers to improve global solutions without dedicated training.

Result: Achieves average optimality gaps below 0.2% on real-world TSP instances ranging from 1k to 88k nodes, outperforming existing learning-based methods and reducing LKH-3's gaps by 12% to 100% on instances with more than 10k nodes.

Conclusion: ViTSP provides a new perspective for hybridizing pre-trained generative models with operations research solvers, offering practical integration potential for complex logistics systems without requiring dedicated model training.

Abstract: Solving Traveling Salesman Problem (TSP) is NP-hard yet fundamental for wide
real-world applications. Classical exact methods face challenges in scaling,
and heuristic methods often require domain-specific parameter calibration.
While learning-based approaches have shown promise, they suffer from poor
generalization and limited scalability due to fixed training data. This work
proposes ViTSP, a novel framework that leverages pre-trained vision language
models (VLMs) to visually guide the solution process for large-scale TSPs. The
VLMs function to identify promising small-scale subproblems from a visualized
TSP instance, which are then efficiently optimized using an off-the-shelf
solver to improve the global solution. ViTSP bypasses the dedicated model
training at the user end while maintaining effectiveness across diverse
instances. Experiments on real-world TSP instances ranging from 1k to 88k nodes
demonstrate that ViTSP consistently achieves solutions with average optimality
gaps below 0.2%, outperforming existing learning-based methods. Under the same
runtime budget, it surpasses the best-performing heuristic solver, LKH-3, by
reducing its gaps by 12% to 100%, particularly on very-large-scale instances
with more than 10k nodes. Our framework offers a new perspective in hybridizing
pre-trained generative models and operations research solvers in solving
combinatorial optimization problems, with practical implications for
integration into more complex logistics systems. The code is available at
https://anonymous.4open.science/r/ViTSP_codes-6683.

</details>


### [235] [GeoBS: Information-Theoretic Quantification of Geographic Bias in AI Models](https://arxiv.org/abs/2509.23482)
*Zhangyu Wang,Nemin Wu,Qian Cao,Jiangnan Xia,Zeping Liu,Yiqun Xie,Akshay Nambi,Tanuja Ganu,Ni Lao,Ninghao Liu,Gengchen Mai*

Main category: cs.AI

TL;DR: This paper introduces GeoBS, an information-theoretic framework for evaluating geographic bias in AI models, addressing limitations of previous model-specific and spatially implicit approaches.


<details>
  <summary>Details</summary>
Motivation: Current geo-bias evaluation methods are model-specific or spatially implicit, lacking a universal framework for fair comparison across AI models and understanding spatial factors contributing to bias.

Method: Proposed GeoBS framework with three novel bias scores that explicitly consider multi-scalability, distance decay, and anisotropy spatial factors, tested on 3 tasks, 8 datasets, and 8 models.

Result: Experiments show both task-specific GeoAI models and general-purpose foundation models suffer from various types of geo-bias, demonstrating the framework's effectiveness.

Conclusion: The GeoBS framework advances technical understanding of geographic bias and establishes foundation for integrating spatial fairness into AI system design, deployment, and evaluation.

Abstract: The widespread adoption of AI models, especially foundation models (FMs), has
made a profound impact on numerous domains. However, it also raises significant
ethical concerns, including bias issues. Although numerous efforts have been
made to quantify and mitigate social bias in AI models, geographic bias (in
short, geo-bias) receives much less attention, which presents unique
challenges. While previous work has explored ways to quantify geo-bias, these
measures are model-specific (e.g., mean absolute deviation of LLM ratings) or
spatially implicit (e.g., average fairness scores of all spatial partitions).
We lack a model-agnostic, universally applicable, and spatially explicit
geo-bias evaluation framework that allows researchers to fairly compare the
geo-bias of different AI models and to understand what spatial factors
contribute to the geo-bias. In this paper, we establish an
information-theoretic framework for geo-bias evaluation, called GeoBS (Geo-Bias
Scores). We demonstrate the generalizability of the proposed framework by
showing how to interpret and analyze existing geo-bias measures under this
framework. Then, we propose three novel geo-bias scores that explicitly take
intricate spatial factors (multi-scalability, distance decay, and anisotropy)
into consideration. Finally, we conduct extensive experiments on 3 tasks, 8
datasets, and 8 models to demonstrate that both task-specific GeoAI models and
general-purpose foundation models may suffer from various types of geo-bias.
This framework will not only advance the technical understanding of geographic
bias but will also establish a foundation for integrating spatial fairness into
the design, deployment, and evaluation of AI systems.

</details>


### [236] [Accurate Predictions in Education with Discrete Variational Inference](https://arxiv.org/abs/2509.23484)
*Tom Quilter,Anastasia Ilick,Anastasia Ilick,Richard Turner*

Main category: cs.AI

TL;DR: The paper introduces a novel discrete variational inference framework for predicting student performance on mathematics exams, achieving over 80% accuracy and outperforming traditional IRT and matrix factorization methods, especially in low-data settings.


<details>
  <summary>Details</summary>
Motivation: To address social inequality in education by developing affordable AI tutors through improved prediction of student performance, particularly in data-sparse environments where many platforms struggle with accuracy.

Method: Released the largest open dataset of professionally marked mathematics exam responses; developed probabilistic modeling framework based on Item Response Theory (IRT); extended with collaborative filtering models incorporating topic-level skill profiles; introduced novel discrete variational inference framework.

Result: Achieved over 80% accuracy in mathematics prediction of formal exam papers; found that a single latent ability parameter alone achieves maximum predictive accuracy; discrete variational inference framework outperformed all classical IRT and matrix factorization baselines, especially in low-data settings.

Conclusion: The discrete variational inference framework represents the main contribution, providing the highest prediction accuracy in challenging low-data scenarios and establishing a new benchmark for mathematics prediction accuracy in formal exam settings.

Abstract: One of the largest drivers of social inequality is unequal access to personal
tutoring, with wealthier individuals able to afford it, while the majority
cannot. Affordable, effective AI tutors offer a scalable solution. We focus on
adaptive learning, predicting whether a student will answer a question
correctly, a key component of any effective tutoring system. Yet many platforms
struggle to achieve high prediction accuracy, especially in data-sparse
settings. To address this, we release the largest open dataset of
professionally marked formal mathematics exam responses to date. We introduce a
probabilistic modelling framework rooted in Item Response Theory (IRT) that
achieves over 80 percent accuracy, setting a new benchmark for mathematics
prediction accuracy of formal exam papers. Extending this, our collaborative
filtering models incorporate topic-level skill profiles, but reveal a
surprising and educationally significant finding, a single latent ability
parameter alone is needed to achieve the maximum predictive accuracy. Our main
contribution though is deriving and implementing a novel discrete variational
inference framework, achieving our highest prediction accuracy in low-data
settings and outperforming all classical IRT and matrix factorisation
baselines.

</details>


### [237] [Mapping Overlaps in Benchmarks through Perplexity in the Wild](https://arxiv.org/abs/2509.23488)
*Siyang Wu,Honglin Bao,Sida Li,Ari Holtzman,James A. Evans*

Main category: cs.AI

TL;DR: The paper introduces 'benchmark signatures' - sets of salient tokens from natural corpora that predict LLM benchmark performance through token perplexity. Through analysis of 32 LLMs and 88 benchmarks, it reveals meaningful overlaps in capabilities while highlighting limitations of current benchmark evaluation methods.


<details>
  <summary>Details</summary>
Motivation: To better understand the relationships between different LLM benchmarks and address limitations in current evaluation approaches, particularly the conflation of performance with ability and the influence of benchmark-orthogonal factors.

Method: Extracted benchmark signatures via stepwise forward selection with linear regressions across 32 LLMs and 88 diverse benchmarks, using token perplexity from naturally authored corpora as predictive features.

Result: Found high performance overlaps across benchmarks but limited semantic similarity. Identified cross-functional overlaps in logic, math, language, instruction following, and world modeling, with coding as the least overlapping domain. Benchmark signatures remained robust to format effects that influence performance metrics.

Conclusion: Benchmark signatures provide mechanistic insights into LLM capabilities and benchmark validity, revealing an interconnected landscape of LLM abilities while highlighting the need to separate true capability from format-dependent performance.

Abstract: We develop signatures of capacity familiarity to characterize large language
model (LLM) benchmarks and their meaningful overlaps. Benchmark signatures
probe the capacity required for benchmark performance. We formally define them
as a set of salient tokens drawn from in-the-wild, naturally authored corpora,
where LLM token perplexity, reflecting more or less pre-training exposure,
becomes highly predictive of LLM benchmark performance. Through a large-scale
meta-evaluation, we extract benchmark signatures via stepwise forward selection
with linear regressions across 32 LLMs and 88 benchmarks spanning diverse
knowledge, coding, logic, instruction following, math, language, reasoning, and
world modeling. Our analysis situates signatures in relation to both the
semantic similarity of benchmark questions and the correlation of model
performance. While performance overlaps are universally high and semantic
overlaps remain confined to a narrow mid-range, benchmark signatures prove
highly informative in capturing variation, overlap, and divergence. We observe
overlap in knowledge and reasoning subtasks, whereas multilingual and cultural
benchmarks exhibit less similarity, even compared to cross-task overlap.
Notably, performance-level results are strongly influenced by
benchmark-orthogonal factors such as question format, highlighting limitations
in LLM generalization, the conflation of performance with ability, and issues
inherent in current mainstream benchmark agreement studies. Benchmark
signatures, however, remain robust to such effects. Ultimately, we identify
cross-functional overlaps across logic, math, language, instruction following,
and world modeling, with coding emerging as the least overlapping domain.
Together, these findings provide mechanistic insights into benchmark validity
and LLM sensitivities, and sketch the underlying landscape of interconnected
LLM capabilities.

</details>


### [238] [Dynamic Trust Calibration Using Contextual Bandits](https://arxiv.org/abs/2509.23497)
*Bruno M. Henrique,Eugene Santos Jr*

Main category: cs.AI

TL;DR: Proposes a novel objective method for dynamic trust calibration between humans and AI using Contextual Bandits, showing 10-38% improvement in decision-making performance across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Current methods lack standardization and consistent metrics for measuring trust calibration, failing to distinguish between opinion formation and human decisions, which is crucial for optimal human-AI collaboration.

Method: Uses Contextual Bandits algorithm to create a standardized trust calibration measure and indicator that dynamically assesses when to trust AI contributions based on learned contextual information.

Result: Evaluation across three diverse datasets demonstrated that effective trust calibration leads to significant improvements in decision-making performance, with 10-38% increase in reward metrics.

Conclusion: The proposed method enhances theoretical understanding and provides practical guidance for developing more trustworthy AI systems in critical domains like disease diagnosis and criminal justice.

Abstract: Trust calibration between humans and Artificial Intelligence (AI) is crucial
for optimal decision-making in collaborative settings. Excessive trust can lead
users to accept AI-generated outputs without question, overlooking critical
flaws, while insufficient trust may result in disregarding valuable insights
from AI systems, hindering performance. Despite its importance, there is
currently no definitive and objective method for measuring trust calibration
between humans and AI. Current approaches lack standardization and consistent
metrics that can be broadly applied across various contexts, and they don't
distinguish between the formation of opinions and subsequent human decisions.
In this work, we propose a novel and objective method for dynamic trust
calibration, introducing a standardized trust calibration measure and an
indicator. By utilizing Contextual Bandits-an adaptive algorithm that
incorporates context into decision-making-our indicator dynamically assesses
when to trust AI contributions based on learned contextual information. We
evaluate this indicator across three diverse datasets, demonstrating that
effective trust calibration results in significant improvements in
decision-making performance, as evidenced by 10 to 38% increase in reward
metrics. These findings not only enhance theoretical understanding but also
provide practical guidance for developing more trustworthy AI systems
supporting decisions in critical domains, for example, disease diagnoses and
criminal justice.

</details>


### [239] [Model Consistency as a Cheap yet Predictive Proxy for LLM Elo Scores](https://arxiv.org/abs/2509.23510)
*Ashwin Ramaswamy,Nestor Demeure,Ermal Rrapaj*

Main category: cs.AI

TL;DR: LLM self-judgment consistency in pairwise comparisons provides a cheap proxy for human Elo scores with 91% correlation.


<details>
  <summary>Details</summary>
Motivation: Need for independent evaluation of new LLMs as they are released frequently with varying performance relative to parameter count, and human Elo scoring is expensive.

Method: Use LLM's own judgment consistency in selecting best model in pairwise contests as a metric, without human data or prior knowledge.

Result: The consistency metric shows 91% correlation with human-produced Elo scores.

Conclusion: LLM self-judgment consistency provides a simple, cheap alternative to expensive human evaluation for estimating Elo scores.

Abstract: New large language models (LLMs) are being released every day. Some perform
significantly better or worse than expected given their parameter count.
Therefore, there is a need for a method to independently evaluate models. The
current best way to evaluate a model is to measure its Elo score by comparing
it to other models in a series of contests - an expensive operation since
humans are ideally required to compare LLM outputs. We observe that when an LLM
is asked to judge such contests, the consistency with which it selects a model
as the best in a matchup produces a metric that is 91% correlated with its own
human-produced Elo score. This provides a simple proxy for Elo scores that can
be computed cheaply, without any human data or prior knowledge.

</details>


### [240] [DOoM: Difficult Olympiads of Math](https://arxiv.org/abs/2509.23529)
*Ilya Kuleshov,Ilin Pavel,Nikolay Kompanets,Ksenia Sycheva,Aleksandr Nikolich*

Main category: cs.AI

TL;DR: DOoM is a new open-source benchmark for evaluating language models' ability to solve Russian mathematics and physics problems across difficulty levels from school to university Olympiad/exam questions.


<details>
  <summary>Details</summary>
Motivation: To assess language model capabilities specifically for Russian mathematics and physics problem-solving, addressing the need for specialized benchmarks in this domain.

Method: Created a structured dataset with problems of varying difficulty levels, established evaluation methodology, and tested various language models on the benchmark.

Result: Analysis revealed correlation between model performance and token count used, and identified performance differences between mathematics and physics tasks.

Conclusion: DOoM benchmark provides valuable insights into language model capabilities for Russian STEM problem-solving, showing clear performance patterns related to model size and task type.

Abstract: This paper introduces DOoM, a new open-source benchmark designed to assess
the capabilities of language models in solving mathematics and physics problems
in Russian. The benchmark includes problems of varying difficulty, ranging from
school-level tasks to university Olympiad and entrance exam questions. In this
paper we discuss the motivation behind its creation, describe dataset's
structure and evaluation methodology, and present initial results from testing
various models. Analysis of the results shows a correlation between model
performance and the number of tokens used, and highlights differences in
performance between mathematics and physics tasks.

</details>


### [241] [Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks](https://arxiv.org/abs/2509.23537)
*Aaron Xuxiang Tian,Ruofan Zhang,Jiayao Tang,Young Min Cho,Xueqian Li,Qiang Yi,Ji Wang,Zhunping Zhang,Danrui Qi,Sharath Chandra Guntuku,Lyle Ungar,Tianyu Shi,Chi Wang*

Main category: cs.AI

TL;DR: Multi-agent orchestration with LLMs achieves performance matching or exceeding the strongest single model across three benchmarks, with analysis showing potential for further gains and revealing how authorship visibility and vote observation affect voting behavior.


<details>
  <summary>Details</summary>
Motivation: To study multi-turn multi-agent orchestration where multiple LLM agents interact through iterative answer proposals and voting until consensus, comparing performance against single-LLM baselines.

Method: Used four LLMs (Gemini 2.5 Pro, GPT-5, Grok 4, Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR benchmarks. Conducted two experiments: (1) benchmarking orchestration vs single-LLM baselines, (2) ablations on GPQA-Diamond varying authorship visibility and ongoing vote observation.

Result: Orchestration matches or exceeds the strongest single model and consistently outperforms others. Analysis shows potential for further gains. Ablations reveal that revealing authorship increases self-voting and ties, while showing ongoing votes amplifies herding, speeding convergence but sometimes causing premature consensus.

Conclusion: Multi-agent orchestration is effective for improving LLM performance, with careful design of voting mechanisms needed to balance convergence speed against premature consensus, and authorship visibility affecting voting behavior.

Abstract: We study multi-turn multi-agent orchestration, where multiple large language
model (LLM) agents interact over multiple turns by iteratively proposing
answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5
Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we
conduct two experiments: (i) benchmarking orchestration against single-LLM
baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who
authored answers and whether they can observe ongoing votes. Orchestration
matches or exceeds the strongest single model and consistently outperforms the
others. Analysis of best-achievable orchestration performance shows potential
for further gains. The ablations show that revealing authorship increases
self-voting and ties, and that showing ongoing votes amplifies herding, which
speeds convergence but can sometimes yield premature consensus.

</details>


### [242] [Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2509.23558)
*Zhaoqi Wang,Daqing He,Zijian Zhang,Xin Li,Liehuang Zhu,Meng Li,Jiamou Liu*

Main category: cs.AI

TL;DR: PASS framework uses reinforcement learning to formalize jailbreak prompts, making them stealthier and more effective at bypassing LLM alignment defenses.


<details>
  <summary>Details</summary>
Motivation: To uncover vulnerabilities in LLM alignment methods by developing more sophisticated jailbreaking techniques that can evade existing security measures.

Method: Uses reinforcement learning to transform initial jailbreak prompts into formalized descriptions, then structures outputs into a GraphRAG system that leverages extracted terms and symbols to enhance subsequent attacks.

Result: Extensive experiments on open-source models demonstrated the effectiveness of the PASS framework in successfully jailbreaking LLMs.

Conclusion: The PASS framework provides an effective method for identifying LLM alignment vulnerabilities through semantic and structural formalization of jailbreak attacks.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, yet
they also introduce novel security challenges. For instance, prompt
jailbreaking attacks involve adversaries crafting sophisticated prompts to
elicit responses from LLMs that deviate from human values. To uncover
vulnerabilities in LLM alignment methods, we propose the PASS framework
(\underline{P}rompt J\underline{a}ilbreaking via \underline{S}emantic and
\underline{S}tructural Formalization). Specifically, PASS employs reinforcement
learning to transform initial jailbreak prompts into formalized descriptions,
which enhances stealthiness and enables bypassing existing alignment defenses.
The jailbreak outputs are then structured into a GraphRAG system that, by
leveraging extracted relevant terms and formalized symbols as contextual input
alongside the original query, strengthens subsequent attacks and facilitates
more effective jailbreaks. We conducted extensive experiments on common
open-source models, demonstrating the effectiveness of our attack.

</details>


### [243] [A Hierarchical Structure-Enhanced Personalized Recommendation Model for Traditional Chinese Medicine Formulas Based on KG Diffusion Guidance](https://arxiv.org/abs/2509.23560)
*ChaoBo Zhang,Long Tan*

Main category: cs.AI

TL;DR: TCM-HEDPR is a hierarchical structure-enhanced personalized recommendation model for TCM formulas that addresses limitations in existing approaches by incorporating patient personalization, handling long-tailed herb distributions, and capturing herb compatibility relationships through knowledge graph diffusion guidance.


<details>
  <summary>Details</summary>
Motivation: Existing TCM prescription recommendation models have three key limitations: insufficient attention to patient-personalized information (age, BMI, medical history), biased training from long-tailed herb distributions, and oversight of herb compatibility principles ('monarch, minister, assistant and envoy') which increases toxicity risks and violates TCM treatment principles.

Method: The model uses: (1) pre-trained symptom representations with patient-personalized prompt sequences and prompt-oriented contrastive learning for data augmentation; (2) KG-guided homogeneous graph diffusion with self-attention to capture non-linear symptom-herb relationships; (3) heterogeneous graph hierarchical network to integrate herbal dispensing relationships with implicit syndromes, addressing long-tailed distribution issues.

Result: Extensive experiments on two public datasets and one clinical dataset demonstrate the effectiveness of TCM-HEDPR. The model also incorporates modern medicine and network pharmacology insights for comprehensive prescription evaluation.

Conclusion: TCM-HEDPR provides a new paradigm for modern TCM recommendation by effectively addressing key limitations in existing approaches and ensuring safer, more personalized prescription recommendations that align with TCM principles.

Abstract: Artificial intelligence technology plays a crucial role in recommending
prescriptions for traditional Chinese medicine (TCM). Previous studies have
made significant progress by focusing on the symptom-herb relationship in
prescriptions. However, several limitations hinder model performance: (i)
Insufficient attention to patient-personalized information such as age, BMI,
and medical history, which hampers accurate identification of syndrome and
reduces efficacy. (ii) The typical long-tailed distribution of herb data
introduces training biases and affects generalization ability. (iii) The
oversight of the 'monarch, minister, assistant and envoy' compatibility among
herbs increases the risk of toxicity or side effects, opposing the 'treatment
based on syndrome differentiation' principle in clinical TCM. Therefore, we
propose a novel hierarchical structure-enhanced personalized recommendation
model for TCM formulas based on knowledge graph diffusion guidance, namely
TCM-HEDPR. Specifically, we pre-train symptom representations using
patient-personalized prompt sequences and apply prompt-oriented contrastive
learning for data augmentation. Furthermore, we employ a KG-guided homogeneous
graph diffusion method integrated with a self-attention mechanism to globally
capture the non-linear symptom-herb relationship. Lastly, we design a
heterogeneous graph hierarchical network to integrate herbal dispensing
relationships with implicit syndromes, guiding the prescription generation
process at a fine-grained level and mitigating the long-tailed herb data
distribution problem. Extensive experiments on two public datasets and one
clinical dataset demonstrate the effectiveness of TCM-HEDPR. In addition, we
incorporate insights from modern medicine and network pharmacology to evaluate
the recommended prescriptions comprehensively. It can provide a new paradigm
for the recommendation of modern TCM.

</details>


### [244] [Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment](https://arxiv.org/abs/2509.23564)
*Min-Hsuan Yeh,Yixuan Li*

Main category: cs.AI

TL;DR: PrefCleanBench is the first comprehensive benchmark for evaluating 13 preference data cleaning methods in LLM alignment, providing standardized assessment of cleaning strategies across diverse datasets, models, and algorithms.


<details>
  <summary>Details</summary>
Motivation: Human feedback for LLM alignment is often noisy and inconsistent, degrading reward model quality. Existing automated data cleaning methods lack systematic evaluation of their effectiveness and generalizability.

Method: Introduces PrefCleanBench with standardized protocol to assess 13 preference data cleaning methods across diverse datasets, model architectures, and optimization algorithms.

Result: The benchmark uncovers key factors determining data cleaning success in alignment tasks and provides modular implementations of all methods for further research.

Conclusion: PrefCleanBench establishes groundwork for principled approaches to improve LLM alignment through better data quality, highlighting the crucial role of data preprocessing in responsible AI development.

Abstract: Human feedback plays a pivotal role in aligning large language models (LLMs)
with human preferences. However, such feedback is often noisy or inconsistent,
which can degrade the quality of reward models and hinder alignment. While
various automated data cleaning methods have been proposed to mitigate this
issue, a systematic evaluation of their effectiveness and generalizability
remains lacking. To bridge this gap, we introduce the first comprehensive
benchmark for evaluating 13 preference data cleaning methods in the context of
LLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning
strategies in terms of alignment performance and generalizability across
diverse datasets, model architectures, and optimization algorithms. By unifying
disparate methods and rigorously comparing them, we uncover key factors that
determine the success of data cleaning in alignment tasks. This benchmark lays
the groundwork for principled and reproducible approaches to improving LLM
alignment through better data quality-highlighting the crucial but
underexplored role of data preprocessing in responsible AI development. We
release modular implementations of all methods to catalyze further research:
https://github.com/deeplearning-wisc/PrefCleanBench.

</details>


### [245] [BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving](https://arxiv.org/abs/2509.23589)
*Shu Liu,Wenlin Chen,Weihao Li,Zheng Wang,Lijin Yang,Jianing Huang,Yipin Zhang,Zhongzhan Huang,Ze Cheng,Hao Yang*

Main category: cs.AI

TL;DR: BridgeDrive is a novel anchor-guided diffusion bridge policy for closed-loop trajectory planning in autonomous driving, achieving state-of-the-art performance with 5% improvement in success rate on Bench2Drive benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based planners struggle with effective guidance in reactive, closed-loop driving environments. Simple conditioning fails in complex scenarios, while recent anchor-based approaches rely on truncated schedules that introduce theoretical inconsistencies and performance issues.

Method: BridgeDrive uses a principled diffusion framework that translates driving behavior anchors into fine-grained trajectory plans, responding appropriately to varying traffic conditions. It's compatible with efficient ODE solvers for real-time deployment.

Result: Achieved state-of-the-art performance on Bench2Drive benchmark with 5% improvement in success rate over prior methods.

Conclusion: BridgeDrive provides an effective solution for guiding diffusion models in closed-loop autonomous driving scenarios, addressing theoretical limitations of previous approaches while maintaining real-time compatibility.

Abstract: Diffusion-based planners have shown great promise for autonomous driving due
to their ability to capture multi-modal driving behaviors. However, guiding
these models effectively in reactive, closed-loop environments remains a
significant challenge. Simple conditioning often fails to provide sufficient
guidance in complex and dynamic driving scenarios. Recent work attempts to use
typical expert driving behaviors (i.e., anchors) to guide diffusion models but
relies on a truncated schedule, which introduces theoretical inconsistencies
and can compromise performance. To address this, we introduce BridgeDrive, a
novel anchor-guided diffusion bridge policy for closed-loop trajectory
planning. Our approach provides a principled diffusion framework that
effectively translates anchors into fine-grained trajectory plans,
appropriately responding to varying traffic conditions. Our planner is
compatible with efficient ODE solvers, a critical factor for real-time
autonomous driving deployment. We achieve state-of-the-art performance on the
Bench2Drive benchmark, improving the success rate by 5% over prior arts.

</details>


### [246] [PSG-Agent: Personality-Aware Safety Guardrail for LLM-based Agents](https://arxiv.org/abs/2509.23614)
*Yaozu Wu,Jizhou Guo,Dongyuan Li,Henry Peng Zou,Wei-Chieh Huang,Yankai Chen,Zhen Wang,Weizhi Zhang,Yangning Li,Meng Zhang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: PSG-Agent is a personalized and dynamic safety system for LLM-based agents that addresses limitations of uniform guardrail policies and isolated response checking by creating user-specific risk thresholds and implementing continuous monitoring across agent interactions.


<details>
  <summary>Details</summary>
Motivation: Existing guardrails apply uniform policies to all users and check responses in isolation, ignoring that the same behavior can harm some users while being safe for others, and missing how risks evolve across multiple interactions.

Method: PSG-Agent creates personalized guardrails by mining interaction history for stable traits and real-time states, and implements continuous monitoring with specialized guards (Plan Monitor, Tool Firewall, Response Guard, Memory Guardian) that track cross-turn risk accumulation.

Result: PSG-Agent significantly outperforms existing agent guardrails including LlamaGuard3 and AGrail in multiple scenarios including healthcare, finance, and daily life automation with diverse user profiles.

Conclusion: PSG-Agent provides an executable and auditable path toward personalized safety for LLM-based agents by addressing fundamental limitations of current guardrail systems.

Abstract: Effective guardrails are essential for safely deploying LLM-based agents in
critical applications. Despite recent advances, existing guardrails suffer from
two fundamental limitations: (i) they apply uniform guardrail policies to all
users, ignoring that the same agent behavior can harm some users while being
safe for others; (ii) they check each response in isolation, missing how risks
evolve and accumulate across multiple interactions. To solve these issues, we
propose PSG-Agent, a personalized and dynamic system for LLM-based agents.
First, PSG-Agent creates personalized guardrails by mining the interaction
history for stable traits and capturing real-time states from current queries,
generating user-specific risk thresholds and protection strategies. Second,
PSG-Agent implements continuous monitoring across the agent pipeline with
specialized guards, including Plan Monitor, Tool Firewall, Response Guard,
Memory Guardian, that track cross-turn risk accumulation and issue verifiable
verdicts. Finally, we validate PSG-Agent in multiple scenarios including
healthcare, finance, and daily life automation scenarios with diverse user
profiles. It significantly outperform existing agent guardrails including
LlamaGuard3 and AGrail, providing an executable and auditable path toward
personalized safety for LLM-based agents.

</details>


### [247] [Reasoning Scaffolding: Distilling the Flow of Thought from LLMs](https://arxiv.org/abs/2509.23619)
*Xiangyu Wen,Junhua Huang,Zeju Li,Min Li,Jianyuan Zhong,Zhijian Xu,Mingxuan Yuan,Yongxiang Huang,Qiang Xu*

Main category: cs.AI

TL;DR: The paper proposes Reasoning Scaffolding, a framework that distills algorithmic reasoning structure from LLMs to SLMs using semantic signals as scaffolds, outperforming traditional behavioral cloning methods.


<details>
  <summary>Details</summary>
Motivation: Traditional behavioral cloning from textual rationales teaches SLMs to mimic surface patterns rather than underlying algorithmic reasoning structure, leading to poor logical robustness.

Method: Abstract teacher's thought process into discrete semantic signals, train student model with multi-task objective: predict next semantic signal and generate corresponding step conditioned on that signal.

Result: Significantly outperforms state-of-the-art distillation methods on challenging reasoning benchmarks in both accuracy and logical consistency.

Conclusion: Provides a path towards creating smaller models that are genuine reasoners rather than just fluent mimics by transferring algorithmic reasoning structure directly.

Abstract: The prevailing approach to distilling reasoning from Large Language Models
(LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It
teaches Small Language Models (SLMs) to mimic surface-level patterns rather
than the underlying algorithmic structure of thought, resulting in a critical
lack of logical robustness. We argue that instead of cloning text, distillation
should transfer this algorithmic structure directly. We introduce Reasoning
Scaffolding}, a framework that reframes reasoning as a structured generation
process. Our method first abstracts the teacher's thought process into a
sequence of discrete, interpretable semantic signals (e.g., Contrast, Addition)
that act as a scaffold. The student model is then trained via a multi-task
objective to both (1)predict the next semantic signal, anticipating the
reasoning flow, and (2)generate the corresponding step, conditioned on that
signal. This multi-task scheme acts as a powerful regularizer, compelling the
student to internalize the computational patterns of coherent reasoning. On a
suite of challenging reasoning benchmarks, our method significantly outperforms
state-of-the-art distillation in both accuracy and logical consistency,
providing a path towards creating smaller models that are genuine reasoners,
not just fluent mimics.

</details>


### [248] [How LLMs Learn to Reason: A Complex Network Perspective](https://arxiv.org/abs/2509.23629)
*Sihan Hu,Xiansheng Cai,Yuan Huang,Zhiyuan Yao,Linfeng Zhang,Pan Zhang,Youjin Deng,Kun Chen*

Main category: cs.AI

TL;DR: The paper proposes a unified theory explaining puzzling behaviors in RLVR training, identifies sparse network topology as the cause, and introduces Annealed-RLVR algorithm that outperforms standard RLVR.


<details>
  <summary>Details</summary>
Motivation: To understand and explain the distinctive behaviors in RLVR training including two-stage learning curves, V-shaped response lengths, and catastrophic forgetting through a unifying theory.

Method: Proposes that RLVR reasoning maps to self-organization of sparse semantic networks, and introduces Annealed-RLVR algorithm with SFT-based 'heating' step to resolve competitive bottlenecks.

Result: Experiments on 1.5B-parameter model show Annealed-RLVR outperforms standard RLVR on both in-distribution and out-of-distribution benchmarks.

Conclusion: The work provides a physical intuition for engineering AI reasoning capabilities by recasting RLVR from black-box optimization to predictable structural self-organization.

Abstract: Training large language models with Reinforcement Learning from Verifiable
Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain
poorly understood, including a two-stage learning curve, V-shaped
response-length trajectories, and a pronounced vulnerability to catastrophic
forgetting. In this work, we propose that these seemingly disparate phenomena
can be explained using a single unifying theory: the model's reasoning process
maps to the self-organization of a semantic complex network whose topology
remains persistently sparse, with the average degree pinned close to two. This
topology imposes a fundamental mechanism for forgetting and learning: it first
drives the system into a maximally frustrated state where ``skill islands''
form, slow-learning happens, and forgetting is induced; then it enters a sharp
growth phase where the new skills are ``bolted on'', driven by
phase-transition-like learning at the web's frontier. Equipped with the theory,
we propose \textit{Annealed-RLVR}, a principled algorithm that introduces an
SFT-based ``heating'' step at the point of maximal frustration to resolve the
competitive bottleneck and enhance the reasoning capability of the model.
Experiments on a 1.5B-parameter model demonstrate that the approach outperforms
standard RLVR on both in-distribution and out-of-distribution benchmarks. By
recasting RLVR from black-box optimization into a predictable process of
structural self-organization, our work provides a new physical intuition for
engineering the emergent reasoning capabilities of future AI systems.

</details>


### [249] [Game-Oriented ASR Error Correction via RAG-Enhanced LLM](https://arxiv.org/abs/2509.23630)
*Yan Jiang,Yongle Luo,Qixian Zhou,Elvis S. Liu*

Main category: cs.AI

TL;DR: GO-AEC framework improves ASR accuracy for gaming voice chat by combining LLMs, RAG, and data augmentation, reducing character error rate by 6.22% and sentence error rate by 29.71%.


<details>
  <summary>Details</summary>
Motivation: General ASR systems perform poorly in gaming scenarios due to short phrases, rapid speech, gaming jargon, and background noise, leading to frequent recognition errors that hinder team coordination.

Method: Proposed GO-AEC framework integrates large language models with Retrieval-Augmented Generation (RAG), uses LLMs and TTS for data augmentation, implements N-best hypothesis-based correction, and maintains a dynamic game knowledge base.

Result: Experimental results show significant improvements: 6.22% reduction in character error rate and 29.71% reduction in sentence error rate compared to baseline ASR systems.

Conclusion: GO-AEC effectively addresses gaming-specific ASR challenges and substantially improves speech recognition accuracy in multiplayer online gaming environments.

Abstract: With the rise of multiplayer online games, real-time voice communication is
essential for team coordination. However, general ASR systems struggle with
gaming-specific challenges like short phrases, rapid speech, jargon, and noise,
leading to frequent errors. To address this, we propose the GO-AEC framework,
which integrates large language models, Retrieval-Augmented Generation (RAG),
and a data augmentation strategy using LLMs and TTS. GO-AEC includes data
augmentation, N-best hypothesis-based correction, and a dynamic game knowledge
base. Experiments show GO-AEC reduces character error rate by 6.22% and
sentence error rate by 29.71%, significantly improving ASR accuracy in gaming
scenarios.

</details>


### [250] [From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models](https://arxiv.org/abs/2509.23676)
*Jue Zhang,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: This paper investigates how Large Reasoning Models use explicit reasoning traces to generate answers, finding that reasoning tokens functionally influence answer generation through attention mechanisms and activation patterns.


<details>
  <summary>Details</summary>
Motivation: To understand the extent to which explicit reasoning traces influence answer generation in Large Reasoning Models, as the relationship between reasoning and final answers remains unclear.

Method: Three-stage investigation: 1) Empirical evaluation of reasoning inclusion effects, 2) Attention analysis identifying Reasoning-Focus Heads, 3) Mechanistic interventions using activation patching to test reasoning-answer dependence.

Result: Explicit reasoning improves answer quality; answer tokens attend heavily to reasoning tokens; RFHs track reasoning trajectory; perturbations to reasoning tokens alter final answers, confirming directional information flow from reasoning to answer.

Conclusion: LRMs functionally leverage reasoning tokens for answer generation, with intermediate reasoning playing a crucial role in shaping model outputs through a directional information flow.

Abstract: Large Reasoning Models (LRMs) generate explicit reasoning traces alongside
final answers, yet the extent to which these traces influence answer generation
remains unclear. In this work, we conduct a three-stage investigation into the
interplay between reasoning and answer generation in three distilled DeepSeek
R1 models. First, through empirical evaluation, we demonstrate that including
explicit reasoning consistently improves answer quality across diverse domains.
Second, attention analysis reveals that answer tokens attend substantially to
reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely
tracking the reasoning trajectory, including self-reflective cues. Third, we
apply mechanistic interventions using activation patching to assess the
dependence of answer tokens on reasoning activations. Our results show that
perturbations to key reasoning tokens can reliably alter the final answers,
confirming a directional and functional flow of information from reasoning to
answer. These findings deepen our understanding of how LRMs leverage reasoning
tokens for answer generation, highlighting the functional role of intermediate
reasoning in shaping model outputs. Our data and code are publicly available at
\href{https://aka.ms/R2A-code}{this URL}.

</details>


### [251] [SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents](https://arxiv.org/abs/2509.23694)
*Jianshuo Dong,Sheng Guo,Hao Wang,Zhuotao Liu,Tianwei Zhang,Ke Xu,Minlie Huang,Han Qiu*

Main category: cs.AI

TL;DR: Search agents using LLMs are vulnerable to unreliable search results, with GPT-4.1-mini showing 90.5% attack success rate. The paper introduces SafeSearch benchmark for systematic safety assessment.


<details>
  <summary>Details</summary>
Motivation: Search agents connect LLMs to the Internet but unreliable search results pose safety threats to users, creating a new threat surface that needs systematic evaluation.

Method: Introduced automated red-teaming framework and SafeSearch benchmark with 300 test cases covering 5 risk categories. Evaluated 3 search agent scaffolds across 7 proprietary and 8 open-source LLMs.

Result: Substantial vulnerabilities found: highest ASR reached 90.5% for GPT-4.1-mini. Common defenses like reminder prompting showed limited effectiveness.

Conclusion: The framework provides systematic, scalable safety assessment for search agents, promoting transparency in safer agent development.

Abstract: Search agents connect LLMs to the Internet, enabling access to broader and
more up-to-date information. However, unreliable search results may also pose
safety threats to end users, establishing a new threat surface. In this work,
we conduct two in-the-wild experiments to demonstrate both the prevalence of
low-quality search results and their potential to misguide agent behaviors. To
counter this threat, we introduce an automated red-teaming framework that is
systematic, scalable, and cost-efficient, enabling lightweight and harmless
safety assessments of search agents. Building on this framework, we construct
the SafeSearch benchmark, which includes 300 test cases covering five
categories of risks (e.g., misinformation and indirect prompt injection). Using
this benchmark, we evaluate three representative search agent scaffolds,
covering search workflow, tool-calling, and deep research, across 7 proprietary
and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities
of LLM-based search agents: when exposed to unreliable websites, the highest
ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover,
our analysis highlights the limited effectiveness of common defense practices,
such as reminder prompting. This emphasizes the value of our framework in
promoting transparency for safer agent development. Our codebase and test cases
are publicly available: https://github.com/jianshuod/SafeSearch.

</details>


### [252] [Measuring Sparse Autoencoder Feature Sensitivity](https://arxiv.org/abs/2509.23717)
*Claire Tian,Katherine Tian,Nathan Hu*

Main category: cs.AI

TL;DR: Developed a scalable method to evaluate feature sensitivity in Sparse Autoencoders (SAEs) by generating semantically similar text using language models and testing feature activation, revealing that many interpretable features have poor sensitivity.


<details>
  <summary>Details</summary>
Motivation: Current SAE feature analysis focuses on activating examples but doesn't reveal feature sensitivity - how reliably features activate on semantically similar texts, creating a gap in understanding feature quality.

Method: Use language models to generate text with same semantic properties as feature's activating examples, then test whether features activate on these generated texts, avoiding need for natural language descriptions.

Result: Many interpretable features have poor sensitivity; human evaluation confirms generated text genuinely resembles original examples when features fail to activate; average sensitivity declines with increasing SAE width across 7 variants.

Conclusion: Feature sensitivity represents a new dimension for evaluating both individual features and SAE architectures, complementing existing interpretability analysis methods.

Abstract: Sparse Autoencoder (SAE) features have become essential tools for mechanistic
interpretability research. SAE features are typically characterized by
examining their activating examples, which are often "monosemantic" and align
with human interpretable concepts. However, these examples don't reveal feature
sensitivity: how reliably a feature activates on texts similar to its
activating examples. In this work, we develop a scalable method to evaluate
feature sensitivity. Our approach avoids the need to generate natural language
descriptions for features; instead we use language models to generate text with
the same semantic properties as a feature's activating examples. We then test
whether the feature activates on these generated texts. We demonstrate that
sensitivity measures a new facet of feature quality and find that many
interpretable features have poor sensitivity. Human evaluation confirms that
when features fail to activate on our generated text, that text genuinely
resembles the original activating examples. Lastly, we study feature
sensitivity at the SAE level and observe that average feature sensitivity
declines with increasing SAE width across 7 SAE variants. Our work establishes
feature sensitivity as a new dimension for evaluating both individual features
and SAE architectures.

</details>


### [253] [MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models](https://arxiv.org/abs/2509.23725)
*Siqi Ma,Jiajie Huang,Bolin Yang,Fan Zhang,Jinlin Wu,Yue Shen,Guohui Fan,Zhu Zhang,Zelin Zang*

Main category: cs.AI

TL;DR: MedLA is a logic-driven multi-agent framework using LLMs that organizes reasoning into explicit logical trees based on syllogistic triads, enabling transparent inference and premise-level alignment through multi-round graph-guided discussions.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent approaches for complex medical questions rely on fixed roles or shallow interaction prompts, limiting their ability to detect and resolve fine-grained logical inconsistencies.

Method: Each agent organizes reasoning into explicit logical trees based on syllogistic triads (major premise, minor premise, conclusion). Agents engage in multi-round, graph-guided discussions to compare and iteratively refine logic trees through error correction and contradiction resolution.

Result: MedLA consistently outperforms both static role-based systems and single-agent baselines on challenging benchmarks like MedDDx and standard medical QA tasks. It scales effectively across both open-source and commercial LLM backbones.

Conclusion: MedLA achieves state-of-the-art performance and offers a generalizable paradigm for trustworthy medical reasoning through its logic-driven multi-agent framework with transparent inference processes.

Abstract: Answering complex medical questions requires not only domain expertise and
patient-specific information, but also structured and multi-perspective
reasoning. Existing multi-agent approaches often rely on fixed roles or shallow
interaction prompts, limiting their ability to detect and resolve fine-grained
logical inconsistencies. To address this, we propose \textsc{MedLA}, a
logic-driven multi-agent framework built on large language models. Each agent
organizes its reasoning process into an explicit logical tree based on
syllogistic triads (major premise, minor premise, and conclusion), enabling
transparent inference and premise-level alignment. Agents engage in a
multi-round, graph-guided discussion to compare and iteratively refine their
logic trees, achieving consensus through error correction and contradiction
resolution. We demonstrate that \textsc{MedLA} consistently outperforms both
static role-based systems and single-agent baselines on challenging benchmarks
such as MedDDx and standard medical QA tasks. Furthermore, \textsc{MedLA}
scales effectively across both open-source and commercial LLM backbones,
achieving state-of-the-art performance and offering a generalizable paradigm
for trustworthy medical reasoning.

</details>


### [254] [EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance](https://arxiv.org/abs/2509.23730)
*Siyao Song,Cong Ma,Zhihao Cheng,Shiye Lei,Minghao Li,Ying Zeng,Huaixiao Tou,Kai Jia*

Main category: cs.AI

TL;DR: EAPO is a novel RL framework that enhances LLM reasoning by incorporating multi-turn interactions with external experts during training, leading to improved exploration and internalization of expert knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for LLMs rely on outcome-based supervision, which leads to inefficient exploration and sparse rewards. The paper aims to address these limitations by incorporating expert interactions.

Method: Proposes Expert-Assisted Policy Optimization (EAPO) where the policy learns to adaptively consult external experts during training, yielding richer reward signals and more reliable reasoning trajectories.

Result: EAPO consistently outperforms expert-assisted workflow, expert-distilled models, and RL baselines on mathematical reasoning benchmarks (AIME 2024, AIME 2025, AIMO 2025), with an average gain of 5 points over self-exploratory models.

Conclusion: EAPO successfully internalizes expert knowledge into the policy model, amplifying the model's inherent reasoning capabilities and producing improved reasoning paths and more accurate solutions without requiring expert assistance during evaluation.

Abstract: Large language models (LLMs) have recently advanced in reasoning when
optimized with reinforcement learning (RL) under verifiable rewards. Existing
methods primarily rely on outcome-based supervision to strengthen internal LLM
reasoning, often leading to inefficient exploration and sparse rewards. To
mitigate this issue, we propose Expert-Assisted Policy Optimization (EAPO), a
novel RL framework that enhances exploration by incorporating multi-turn
interactions with external experts during training. Unlike prior methods, where
policies reason in isolation, EAPO incentivizes the policy to adaptively
determine when and how to consult experts, yielding richer reward signals and
more reliable reasoning trajectories. External assistance ultimately
internalizes expert knowledge into the policy model, amplifying the model's
inherent reasoning capabilities. During evaluation, the policy model has been
well-optimized to solve questions independently, producing improved reasoning
paths and more accurate solutions. Experiments on mathematical reasoning
benchmarks, including AIME 2024, AIME 2025, and AIMO 2025, show that EAPO
consistently outperforms expert-assisted workflow, expert-distilled models, and
RL baselines, with an average gain of 5 points over self-exploratory models.

</details>


### [255] [Diagnosing Failure Root Causes in Platform-Orchestrated Agentic Systems: Dataset, Taxonomy, and Benchmark](https://arxiv.org/abs/2509.23735)
*Xuyan Ma,Xiaofei Xie,Yawen Wang,Junjie Wang,Boyu Wu,Mingyang Li,Qing Wang*

Main category: cs.AI

TL;DR: This paper studies root cause identification in platform-orchestrated agentic systems, creates a dataset (AgentFail) with 307 failure logs, develops a taxonomy for failure root causes, and benchmarks LLM-based root cause identification methods.


<details>
  <summary>Details</summary>
Motivation: Platform-orchestrated agentic systems are increasingly deployed but fragile, and there's no systematic way to identify their potential failure root causes.

Method: Constructed AgentFail dataset with 307 failure logs from 10 agentic systems, used counterfactual reasoning-based repair for reliable annotation, developed taxonomy for failure root causes, and created LLM benchmark for root cause identification.

Result: The taxonomy significantly improves LLM performance in root cause identification, but maximum accuracy reaches only 33.6%, indicating the task remains challenging.

Conclusion: The paper provides a reliable dataset, taxonomy, and benchmark for failure root cause analysis in agentic systems, serving as foundation for developing more reliable agentic systems.

Abstract: Agentic systems consisting of multiple LLM-driven agents coordinating through
tools and structured interactions, are increasingly deployed for complex
reasoning and problem-solving tasks. At the same time, emerging low-code and
template-based agent development platforms (e.g., Dify) enable users to rapidly
build and orchestrate agentic systems, which we refer to as
platform-orchestrated agentic systems. However, these systems are also fragile
and it remains unclear how to systematically identify their potential failure
root cause. This paper presents a study of root cause identification of these
platform-orchestrated agentic systems. To support this initiative, we construct
a dataset AgentFail containing 307 failure logs from ten agentic systems, each
with fine-grained annotations linking failures to their root causes. We
additionally utilize counterfactual reasoning-based repair strategy to ensure
the reliability of the annotation. Building on the dataset, we develop a
taxonomy that characterizes failure root causes and analyze their distribution
across different platforms and task domains. Furthermore, we introduce a
benchmark that leverages LLMs for automatically identifying root causes, in
which we also utilize the proposed taxonomy as guidance for LLMs. Results show
that the taxonomy can largely improve the performance, thereby confirming its
utility. Nevertheless, the accuracy of root cause identification reaches at
most 33.6%, which indicates that this task still remains challenging. In light
of these results, we also provide actionable guidelines for building such
agentic systems. In summary, this paper provides a reliable dataset of failure
root cause for platform-orchestrated agentic systems, corresponding taxonomy
and benchmark, which serves as a foundation for advancing the development of
more reliable agentic systems.

</details>


### [256] [GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks](https://arxiv.org/abs/2509.23738)
*Cong Chen,Kaixiang Ji,Hao Zhong,Muzhi Zhu,Anzhou Li,Guo Gan,Ziyuan Huang,Cheng Zou,Jiajia Liu,Jingdong Chen,Hao Chen,Chunhua Shen*

Main category: cs.AI

TL;DR: GUI-Shepherd is a Process Reward Model that provides dense step-by-step feedback for GUI agents, addressing sparse rewards and credit assignment problems in long-sequence tasks.


<details>
  <summary>Details</summary>
Motivation: Autonomous agents for GUI tasks face challenges with sparse rewards and intractable credit assignment, which hinders their performance in long-sequence tasks.

Method: Trained on 52k interactions with human-annotated scores and GPT-4o generated rationales, GUI-Shepherd serves as both reward provider for RL training and verifier for inference.

Result: On AndroidWorld benchmark: 7.7 points improvement via online PPO, 5.1 points as verifier. On AndroidControl: 2.2 points as reward provider, 4.3 points as verifier. Outperforms Outcome Reward Model competitors.

Conclusion: High-fidelity process supervision is critical for building capable GUI agents, and GUI-Shepherd provides a generalizable solution for process supervision in GUI tasks.

Abstract: Autonomous agents for long-sequence Graphical User Interface tasks are
hindered by sparse rewards and the intractable credit assignment problem. To
address these challenges, we introduce GUI-Shepherd, a Process Reward Model
that provides dense, step-by-step feedback to guide agents. GUI-Shepherd is
trained on a diverse large-scale data set of $52$k interactions that features
human-annotated scores and GPT-4o generated rationales, enabling it to serve
both as a reward provider for RL training and as a verifier for inference. As
far as we know, we are the first to conduct a systematic study of process
supervision in GUI agents, across diverse settings from online long-horizon
tasks to offline single-step prediction. On the online AndroidWorld benchmark,
GUI-Shepherd improves success rate by $7.7$ points via multi-turn online PPO,
significantly outperforming Outcome Reward Model based competitors. When used
as an inference verifier, it brings $5.1$ points improvements. The benefits
generalize to the offline AndroidControl benchmark, with gains of $2.2$ points
as a reward provider and $4.3$ points as a verifier. Collectively, our results
establish that high-fidelity process supervision is critical for building more
capable GUI agents and present a generalizable solution.

</details>


### [257] [Transparent Visual Reasoning via Object-Centric Agent Collaboration](https://arxiv.org/abs/2509.23757)
*Benjamin Teoh,Ben Glocker,Francesca Toni,Avinash Kori*

Main category: cs.AI

TL;DR: OCEAN is an interpretable AI framework using object-centric representations and multi-agent negotiation for transparent visual explanations, achieving competitive performance with black-box models while providing more intuitive explanations.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of producing visual explanations grounded in human-understandable concepts in explainable AI.

Method: Object-centric representations with transparent multi-agent reasoning process using game-theoretic negotiation to agree on coherent and discriminative evidence.

Result: Competitive performance with state-of-the-art black-box models on two diagnostic multi-object datasets, with user study showing OCEAN's explanations rated as more intuitive and trustworthy than GradCAM and LIME.

Conclusion: OCEAN provides a faithful and interpretable decision-making process through object-centric representations and agent negotiation, successfully bridging the gap between performance and explainability in visual AI.

Abstract: A central challenge in explainable AI, particularly in the visual domain, is
producing explanations grounded in human-understandable concepts. To tackle
this, we introduce OCEAN (Object-Centric Explananda via Agent Negotiation), a
novel, inherently interpretable framework built on object-centric
representations and a transparent multi-agent reasoning process. The
game-theoretic reasoning process drives agents to agree on coherent and
discriminative evidence, resulting in a faithful and interpretable
decision-making process. We train OCEAN end-to-end and benchmark it against
standard visual classifiers and popular posthoc explanation tools like GradCAM
and LIME across two diagnostic multi-object datasets. Our results demonstrate
competitive performance with respect to state-of-the-art black-box models with
a faithful reasoning process, which was reflected by our user study, where
participants consistently rated OCEAN's explanations as more intuitive and
trustworthy.

</details>


### [258] [From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning](https://arxiv.org/abs/2509.23768)
*Cheng Yang,Jiaxuan Lu,Haiyuan Wan,Junchi Yu,Feiwei Qin*

Main category: cs.AI

TL;DR: ChemMAS is a multi-agent system that reframes chemical reaction condition recommendation as an evidence-based reasoning task, achieving significant performance gains while providing interpretable rationales.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods for chemical reaction condition recommendation lack explainable rationales, limiting their utility in high-stakes scientific workflows where understanding the reasoning behind recommendations is crucial.

Method: ChemMAS decomposes the task into four components: mechanistic grounding, multi-channel recall, constraint-aware agentic debate, and rationale aggregation. It uses a multi-agent system where each decision is backed by interpretable justifications grounded in chemical knowledge and retrieved precedents.

Result: ChemMAS achieves 20-35% gains over domain-specific baselines and outperforms general-purpose LLMs by 10-15% in Top-1 accuracy for reaction condition recommendation.

Conclusion: ChemMAS establishes a new paradigm for explainable AI in scientific discovery by providing falsifiable, human-trustable rationales for chemical reaction condition recommendations.

Abstract: The chemical reaction recommendation is to select proper reaction condition
parameters for chemical reactions, which is pivotal to accelerating chemical
science. With the rapid development of large language models (LLMs), there is
growing interest in leveraging their reasoning and planning capabilities for
reaction condition recommendation. Despite their success, existing methods
rarely explain the rationale behind the recommended reaction conditions,
limiting their utility in high-stakes scientific workflows. In this work, we
propose ChemMAS, a multi-agent system that reframes condition prediction as an
evidence-based reasoning task. ChemMAS decomposes the task into mechanistic
grounding, multi-channel recall, constraint-aware agentic debate, and rationale
aggregation. Each decision is backed by interpretable justifications grounded
in chemical knowledge and retrieved precedents. Experiments show that ChemMAS
achieves 20-35% gains over domain-specific baselines and outperforms
general-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable,
human-trustable rationales, which establishes a new paradigm for explainable AI
in scientific discovery.

</details>


### [259] [Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B](https://arxiv.org/abs/2509.23882)
*Shuyi Lin,Tian Lu,Zikai Wang,Bo Wen,Yibo Zhao,Cheng Tan*

Main category: cs.AI

TL;DR: Security evaluation of GPT-OSS-20B reveals multiple failure modes including quant fever, reasoning blackholes, and Schrodinger's compliance that can be exploited under adversarial conditions.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the security vulnerabilities and failure modes of OpenAI's GPT-OSS-20B model under different adversarial conditions using the Jailbreak Oracle framework.

Method: Used Jailbreak Oracle (JO), a systematic LLM evaluation tool, to probe GPT-OSS-20B's behavior under adversarial conditions and identify specific failure modes.

Result: Uncovered several critical failure modes: quant fever, reasoning blackholes, Schrodinger's compliance, reasoning procedure mirage, and chain-oriented prompting that can be exploited on GPT-OSS-20B models.

Conclusion: The study demonstrates severe security vulnerabilities in GPT-OSS-20B that can lead to significant consequences when exploited, highlighting the need for improved security measures in open-weight language models.

Abstract: OpenAI's GPT-OSS family provides open-weight language models with explicit
chain-of-thought (CoT) reasoning and a Harmony prompt format. We summarize an
extensive security evaluation of GPT-OSS-20B that probes the model's behavior
under different adversarial conditions. Using the Jailbreak Oracle (JO) [1], a
systematic LLM evaluation tool, the study uncovers several failure modes
including quant fever, reasoning blackholes, Schrodinger's compliance,
reasoning procedure mirage, and chain-oriented prompting. Experiments
demonstrate how these behaviors can be exploited on GPT-OSS-20B models, leading
to severe consequences.

</details>


### [260] [Falcon: A Cross-Modal Evaluation Dataset for Comprehensive Safety Perception](https://arxiv.org/abs/2509.23783)
*Qi Xue,Minrui Jiang,Runjia Zhang,Xiurui Xie,Pei Ke,Guisong Liu*

Main category: cs.AI

TL;DR: This paper introduces Falcon, a large-scale vision-language safety dataset with 57,515 VQA pairs across 13 harm categories, and FalconEye, a specialized evaluator fine-tuned from Qwen2.5-VL-7B that reliably identifies harmful content in multimodal dialogue scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating harmful content are well-developed for text-only LLMs but remain underdeveloped for multimodal LLMs (MLLMs), particularly overlooking the role of visual information in moderating content in visual question answering.

Method: Created Falcon dataset with explicit annotations for harmful attributes across images, instructions, and responses across 13 harm categories. Developed FalconEye, a specialized evaluator fine-tuned from Qwen2.5-VL-7B using the Falcon dataset.

Result: FalconEye outperforms all other baselines in overall accuracy across Falcon-test dataset and two widely-used benchmarks (VLGuard and Beavertail-V), demonstrating reliable identification of harmful content in complex multimodal dialogue scenarios.

Conclusion: FalconEye shows potential as a practical safety auditing tool for MLLMs, effectively bridging the gap in multimodal content safety evaluation through comprehensive dataset creation and specialized model development.

Abstract: Existing methods for evaluating the harmfulness of content generated by large
language models (LLMs) have been well studied. However, approaches tailored to
multimodal large language models (MLLMs) remain underdeveloped and lack depth.
This work highlights the crucial role of visual information in moderating
content in visual question answering (VQA), a dimension often overlooked in
current research. To bridge this gap, we introduce Falcon, a large-scale
vision-language safety dataset containing 57,515 VQA pairs across 13 harm
categories. The dataset provides explicit annotations for harmful attributes
across images, instructions, and responses, thereby facilitating a
comprehensive evaluation of the content generated by MLLMs. In addition, it
includes the relevant harm categories along with explanations supporting the
corresponding judgments. We further propose FalconEye, a specialized evaluator
fine-tuned from Qwen2.5-VL-7B using the Falcon dataset. Experimental results
demonstrate that FalconEye reliably identifies harmful content in complex and
safety-critical multimodal dialogue scenarios. It outperforms all other
baselines in overall accuracy across our proposed Falcon-test dataset and two
widely-used benchmarks-VLGuard and Beavertail-V, underscoring its potential as
a practical safety auditing tool for MLLMs.

</details>


### [261] [From Frustration to Fun: An Adaptive Problem-Solving Puzzle Game Powered by Genetic Algorithm](https://arxiv.org/abs/2509.23796)
*Matthew McConnell,Richard Zhao*

Main category: cs.AI

TL;DR: An adaptive AI-powered puzzle game that dynamically generates pathfinding puzzles using genetic algorithms, tailoring difficulty to individual players in real-time to maintain optimal challenge levels.


<details>
  <summary>Details</summary>
Motivation: To develop problem-solving skills through adaptive gaming that maintains engagement, mitigates frustration, and provides personalized challenge levels.

Method: Combines procedural content generation with online adaptive difficulty adjustment using genetic algorithms and player modeling that records user interactions to generate puzzles at target difficulty levels.

Result: A pilot user study compared different adaptive difficulty systems and interpreted player responses, showing the approach's effectiveness.

Conclusion: This work establishes a foundation for emotionally informed player models, advanced AI adaptivity techniques, and broader educational applications beyond gaming.

Abstract: This paper explores adaptive problem solving with a game designed to support
the development of problem-solving skills. Using an adaptive, AI-powered puzzle
game, our adaptive problem-solving system dynamically generates
pathfinding-based puzzles using a genetic algorithm, tailoring the difficulty
of each puzzle to individual players in an online real-time approach. A
player-modeling system records user interactions and informs the generation of
puzzles to approximate a target difficulty level based on various metrics of
the player. By combining procedural content generation with online adaptive
difficulty adjustment, the system aims to maintain engagement, mitigate
frustration, and maintain an optimal level of challenge. A pilot user study
investigates the effectiveness of this approach, comparing different types of
adaptive difficulty systems and interpreting players' responses. This work lays
the foundation for further research into emotionally informed player models,
advanced AI techniques for adaptivity, and broader applications beyond gaming
in educational settings.

</details>


### [262] [AnveshanaAI: A Multimodal Platform for Adaptive AI/ML Education through Automated Question Generation and Interactive Assessment](https://arxiv.org/abs/2509.23811)
*Rakesh Thakur,Diksha Khandelwal,Shreya Tiwari*

Main category: cs.AI

TL;DR: AnveshanaAI is a gamified AI learning platform with personalized dashboards, adaptive assessments based on Bloom's taxonomy, and explainable AI techniques to enhance engagement and learning outcomes.


<details>
  <summary>Details</summary>
Motivation: To address limitations of static question repositories in existing platforms and provide a more engaging, adaptive, and transparent AI education experience.

Method: Uses gamified tracking (streaks, levels, badges), structured navigation across AI domains, semantic similarity checks, explainable AI techniques, and adaptive domain-aware assessment methods grounded in Bloom's taxonomy.

Result: Experiments show broad dataset coverage, stable fine-tuning with reduced perplexity, and measurable gains in learner engagement.

Conclusion: AnveshanaAI successfully integrates adaptivity, gamification, interactivity, and explainability to support next-generation AI education.

Abstract: We propose AnveshanaAI, an application-based learning platform for artificial
intelligence. With AnveshanaAI, learners are presented with a personalized
dashboard featuring streaks, levels, badges, and structured navigation across
domains such as data science, machine learning, deep learning, transformers,
generative AI, large language models, and multimodal AI, with scope to include
more in the future. The platform incorporates gamified tracking with points and
achievements to enhance engagement and learning, while switching between
Playground, Challenges, Simulator, Dashboard, and Community supports
exploration and collaboration. Unlike static question repositories used in
existing platforms, AnveshanaAI ensures balanced learning progression through a
dataset grounded in Bloom's taxonomy, with semantic similarity checks and
explainable AI techniques improving transparency and reliability. Adaptive,
automated, and domain-aware assessment methods are also employed. Experiments
demonstrate broad dataset coverage, stable fine-tuning with reduced perplexity,
and measurable gains in learner engagement. Together, these features illustrate
how AnveshanaAI integrates adaptivity, gamification, interactivity, and
explainability to support next-generation AI education.

</details>


### [263] [Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain Rules](https://arxiv.org/abs/2509.23836)
*Chenyu Zhou,Xiaoming Shi,Hui Qiu,Xiawu Zheng,Haitao Leng,Yankai Jiang,Shaoguo Liu,Tingting Gao,Rongrong Ji*

Main category: cs.AI

TL;DR: This paper introduces Mix-ECom, a novel corpus for evaluating e-commerce agents that handles mixed-type dialogues and complex domain rules, addressing limitations in current benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current e-commerce agent benchmarks lack evaluation of agents' capability to handle mixed-type e-commerce dialogues and complex domain rules, which limits research and application development.

Method: Constructed Mix-ECom corpus based on real-world customer-service dialogues with privacy removal and CoT process addition. Contains 4,799 samples covering multiple dialogue types, e-commerce tasks, and 82 domain rules. Built baselines and proposed a dynamic framework for performance improvement.

Result: Results show current e-commerce agents lack sufficient capabilities to handle e-commerce dialogues, primarily due to hallucinations caused by complex domain rules.

Conclusion: The Mix-ECom dataset addresses critical gaps in e-commerce agent evaluation and will be publicly available to promote further research in this domain.

Abstract: E-commerce agents contribute greatly to helping users complete their
e-commerce needs. To promote further research and application of e-commerce
agents, benchmarking frameworks are introduced for evaluating LLM agents in the
e-commerce domain. Despite the progress, current benchmarks lack evaluating
agents' capability to handle mixed-type e-commerce dialogue and complex domain
rules. To address the issue, this work first introduces a novel corpus, termed
Mix-ECom, which is constructed based on real-world customer-service dialogues
with post-processing to remove user privacy and add CoT process. Specifically,
Mix-ECom contains 4,799 samples with multiply dialogue types in each e-commerce
dialogue, covering four dialogue types (QA, recommendation, task-oriented
dialogue, and chit-chat), three e-commerce task types (pre-sales, logistics,
after-sales), and 82 e-commerce rules. Furthermore, this work build baselines
on Mix-Ecom and propose a dynamic framework to further improve the performance.
Results show that current e-commerce agents lack sufficient capabilities to
handle e-commerce dialogues, due to the hallucination cased by complex domain
rules. The dataset will be publicly available.

</details>


### [264] [AgentGuard: Runtime Verification of AI Agents](https://arxiv.org/abs/2509.23864)
*Roham Koohestani*

Main category: cs.AI

TL;DR: AgentGuard is a runtime verification framework for Agentic AI systems that provides probabilistic guarantees through Dynamic Probabilistic Assurance, using online learning to build MDP models and probabilistic model checking for real-time verification.


<details>
  <summary>Details</summary>
Motivation: Traditional verification methods are inadequate for autonomous AI systems due to their unpredictability and emergent behaviors, necessitating probabilistic guarantees instead of deterministic verification.

Method: AgentGuard operates as an inspection layer that observes agent I/O, abstracts it into formal events in a state model, uses online learning to dynamically build and update Markov Decision Processes (MDPs), and applies probabilistic model checking for real-time verification.

Result: The framework enables continuous, quantitative assurance by modeling emergent behavior and verifying quantitative properties in real-time.

Conclusion: AgentGuard provides a practical approach to runtime verification of agentic AI systems through probabilistic modeling and real-time analysis, addressing the limitations of traditional verification methods.

Abstract: The rapid evolution to autonomous, agentic AI systems introduces significant
risks due to their inherent unpredictability and emergent behaviors; this also
renders traditional verification methods inadequate and necessitates a shift
towards probabilistic guarantees where the question is no longer if a system
will fail, but the probability of its failure within given constraints. This
paper presents AgentGuard, a framework for runtime verification of Agentic AI
systems that provides continuous, quantitative assurance through a new paradigm
called Dynamic Probabilistic Assurance. AgentGuard operates as an inspection
layer that observes an agent's raw I/O and abstracts it into formal events
corresponding to transitions in a state model. It then uses online learning to
dynamically build and update a Markov Decision Process (MDP) that formally
models the agent's emergent behavior. Using probabilistic model checking, the
framework then verifies quantitative properties in real-time.

</details>


### [265] [Rethinking Reward Miscalibration of GRPO in Agentic RL](https://arxiv.org/abs/2509.23870)
*Jingyu Liu,Xiaopeng Wu,Jingquan Peng,Kehan Chen,Chuan Yu,Lizhong Ding,Yong Liu*

Main category: cs.AI

TL;DR: The paper challenges the common belief that outcome-based rewards cause reward miscalibration and reinforce flawed actions. Instead, it identifies gradient coupling between similar samples as the real issue in agentic RL, where similar input prompts and limited action space cause gradients from good samples to strengthen bad actions. A classification-based actor training method is proposed to separate embeddings and reduce gradient interference.


<details>
  <summary>Details</summary>
Motivation: To address the misconception about outcome-based rewards in autonomous agent training and identify the actual problem of gradient coupling that causes flawed actions to be reinforced during training.

Method: Propose training the actor to classify good or bad actions to separate their embeddings and alleviate gradient interference between similar samples with similar input observations and output actions.

Result: Extensive experiments demonstrate the effectiveness of the proposed method in addressing gradient coupling issues and preventing the reinforcement of flawed actions.

Conclusion: Gradient coupling, not outcome-based rewards, is the key issue in agentic RL that causes flawed actions to be strengthened, and the proposed classification-based training effectively mitigates this problem.

Abstract: Building autonomous agents capable of solving long-horizon, real-world tasks
has garnered significant research interest. But outcome based rewards may cause
reward miscalibration which means it might mistakenly allocate positive reward
to flawed middle steps which is regarded as the key reason making the bad
actions being reinforced during training. However we reveal that outcome based
reward ensures expected negative advantage for those flawed middle steps, which
means the flawed actions should be punished during training. Even accounting
for the ``squeezing effect", the probability mass of good actions should
increase and the actor should gradually get rid of harmful actions. This shows
that flawed actions should be punished during training. We further identify
gradient coupling between similar samples as a key issue in agentic RL, the
input prompt is extremely similar and the output action space is limited,
therefore during training, gradients from well-performing samples can
inadvertently strengthen suboptimal or incorrect actions due to similar input
observation and output actions. We show that with gradient coupling, some
flawed actions might be enhanced. To address this, we propose training the
actor to classify good or bad actions to separate the embedding of good/bad
actions and alleviate the gradient interference, extensive experiments shows
its effectiveness.

</details>


### [266] [From Neural Networks to Logical Theories: The Correspondence between Fibring Modal Logics and Fibring Neural Networks](https://arxiv.org/abs/2509.23912)
*Ouns El Harzli,Bernardo Cuenca Grau,Artur d'Avila Garcez,Ian Horrocks,Tarek R. Besold*

Main category: cs.AI

TL;DR: This paper establishes a formal correspondence between fibring of neural networks and fibring of modal logics, using fibred models compatible with neural networks to derive logical expressiveness results for GNNs, GATs and Transformers.


<details>
  <summary>Details</summary>
Motivation: To close the gap between fibring of neural networks (a neurosymbolic framework) and fibring of modal logics, and to enable logical interpretation of neural network theories using computational logic tools.

Method: Formalizes fibred models compatible with fibred neural networks, then uses this correspondence to derive non-uniform logical expressiveness results for various neural architectures.

Result: Establishes formal correspondence between neural network fibring and modal logic fibring, and derives logical expressiveness bounds for GNNs, GATs and Transformer encoders.

Conclusion: Opens the way for using fibring as a formalism to interpret logical theories learned by neural networks using computational logic tools.

Abstract: Fibring of modal logics is a well-established formalism for combining
countable families of modal logics into a single fibred language with common
semantics, characterized by fibred models. Inspired by this formalism, fibring
of neural networks was introduced as a neurosymbolic framework for combining
learning and reasoning in neural networks. Fibring of neural networks uses the
(pre-)activations of a trained network to evaluate a fibring function computing
the weights of another network whose outputs are injected back into the
original network. However, the exact correspondence between fibring of neural
networks and fibring of modal logics was never formally established. In this
paper, we close this gap by formalizing the idea of fibred models
\emph{compatible} with fibred neural networks. Using this correspondence, we
then derive non-uniform logical expressiveness results for Graph Neural
Networks (GNNs), Graph Attention Networks (GATs) and Transformer encoders.
Longer-term, the goal of this paper is to open the way for the use of fibring
as a formalism for interpreting the logical theories learnt by neural networks
with the tools of computational logic.

</details>


### [267] [Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models](https://arxiv.org/abs/2509.23962)
*Guanxu Chen,Yafu Li,Yuxian Jiang,Chen Qian,Qihan Ren,Jingyi Yang,Yu Cheng,Dongrui Liu,Jing Shao*

Main category: cs.AI

TL;DR: CANON is a new advantage estimation method for RL that amplifies target metrics without assuming directionality, outperforming prior methods on math reasoning and logic tasks while improving token efficiency.


<details>
  <summary>Details</summary>
Motivation: Prior RL methods for LLMs use hand-crafted directional priors that can be overly biased and require careful hyperparameter tuning, potentially leading to failure.

Method: CANON regroups sampled responses into two groups based on metric values, measures which trend contributes to better performance through inter-group comparison, and identifies better responses within groups.

Result: CANON with entropy consistently outperforms prior methods across three LLMs on math reasoning and high-complexity logic tasks, and improves token efficiency when applied to response length.

Conclusion: CANON provides an effective way to leverage training metrics without directional assumptions, achieving better performance and more favorable performance-cost trade-offs.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) for large language
models (LLMs) has achieved remarkable progress in enhancing LLMs' reasoning
capabilities on tasks with clear correctness criteria, such as mathematical
reasoning tasks. Several training metrics, such as entropy or response length,
have been observed to correlate with different reasoning behaviors in
reinforcement learning. Prior approaches incorporate such priors through reward
or advantage shaping, which often relies on hand-crafted penalties and
preferences (e.g., higher-is-better or lower-is-better). However, without
careful hyperparameter tuning, these directional priors can be overly biased
and may lead to failure. To this end, we introduce Conditional advANtage
estimatiON (CANON), amplifying the impact of the target metric without
presuming its direction. Specifically, CANON regroups the sampled responses
into two groups based on the higher or lower value of a target metric, measures
which metric trend contributes to better performance through inter-group
comparison, and identifies the better response within the same group. In
summary, CANON based on entropy consistently outperforms prior methods across
three LLMs on both math reasoning and high-complexity logic tasks. When applied
to response length, CANON further improves token efficiency, yielding a more
favorable Pareto frontier in the performance-cost trade-off.

</details>


### [268] [Automatic selection of primary studies in systematic reviews with evolutionary rule-based classification](https://arxiv.org/abs/2509.23981)
*José de la Torre-López,Aurora Ramírez,José Raúl Romero*

Main category: cs.AI

TL;DR: Proposes an evolutionary machine learning approach called \ourmodel for automatic paper selection in systematic literature reviews, using grammar-guided genetic programming to create interpretable rule-based classifiers that combine textual and bibliometric data.


<details>
  <summary>Details</summary>
Motivation: Systematic literature reviews are time-consuming, and current methods don't effectively combine textual information with bibliometric data. There's a need for automated approaches that maintain interpretability while improving accuracy.

Method: Uses grammar-guided genetic programming to build interpretable rule-based classifiers. The grammar defines syntax and structure, allowing combination of textual information with bibliometric data not considered by state-of-the-art methods.

Result: Experiments demonstrate that accurate classifiers can be generated without impairing interpretability, using configurable information sources not previously supported.

Conclusion: The proposed evolutionary machine learning approach successfully automates paper selection while maintaining interpretability and leveraging additional bibliometric data sources.

Abstract: Searching, filtering and analysing scientific literature are time-consuming
tasks when performing a systematic literature review. With the rise of
artificial intelligence, some steps in the review process are progressively
being automated. In particular, machine learning for automatic paper selection
can greatly reduce the effort required to identify relevant literature in
scientific databases. We propose an evolutionary machine learning approach,
called \ourmodel, to automatically determine whether a paper retrieved from a
literature search process is relevant. \ourmodel builds an interpretable
rule-based classifier using grammar-guided genetic programming. The use of a
grammar to define the syntax and the structure of the rules allows \ourmodel to
easily combine the usual textual information with other bibliometric data not
considered by state-of-the-art methods. Our experiments demonstrate that it is
possible to generate accurate classifiers without impairing interpretability
and using configurable information sources not supported so far.

</details>


### [269] [TusoAI: Agentic Optimization for Scientific Methods](https://arxiv.org/abs/2509.23986)
*Alistair Turcan,Kexin Huang,Lei Li,Martin Jinye Zhang*

Main category: cs.AI

TL;DR: TusoAI is an agentic AI system that autonomously develops and optimizes computational methods for scientific tasks by integrating domain knowledge and performing iterative optimization, outperforming existing methods and uncovering novel biological insights.


<details>
  <summary>Details</summary>
Motivation: Scientific discovery is slowed by manual development of computational tools. LLMs offer capabilities in synthesizing literature, reasoning with data, and generating code, but existing systems don't effectively integrate domain-specific knowledge for computational method development.

Method: TusoAI takes scientific task descriptions with evaluation functions, integrates domain knowledge into a knowledge tree representation, and performs iterative domain-specific optimization and model diagnosis to improve candidate solutions.

Result: TusoAI outperformed state-of-the-art expert methods, MLE agents, and scientific AI agents across diverse tasks including single-cell RNA-seq data denoising and satellite-based earth monitoring. In genetics applications, it improved existing methods and uncovered 9 new autoimmune disease-T cell associations and 7 novel disease variant-gene links.

Conclusion: TusoAI successfully accelerates computational method development by autonomously integrating domain knowledge and optimizing solutions, demonstrating practical applications that advance scientific discovery beyond existing methods.

Abstract: Scientific discovery is often slowed by the manual development of
computational tools needed to analyze complex experimental data. Building such
tools is costly and time-consuming because scientists must iteratively review
literature, test modeling and scientific assumptions against empirical data,
and implement these insights into efficient software. Large language models
(LLMs) have demonstrated strong capabilities in synthesizing literature,
reasoning with empirical data, and generating domain-specific code, offering
new opportunities to accelerate computational method development. Existing
LLM-based systems either focus on performing scientific analyses using existing
computational methods or on developing computational methods or models for
general machine learning without effectively integrating the often unstructured
knowledge specific to scientific domains. Here, we introduce TusoAI , an
agentic AI system that takes a scientific task description with an evaluation
function and autonomously develops and optimizes computational methods for the
application. TusoAI integrates domain knowledge into a knowledge tree
representation and performs iterative, domain-specific optimization and model
diagnosis, improving performance over a pool of candidate solutions. We
conducted comprehensive benchmark evaluations demonstrating that TusoAI
outperforms state-of-the-art expert methods, MLE agents, and scientific AI
agents across diverse tasks, such as single-cell RNA-seq data denoising and
satellite-based earth monitoring. Applying TusoAI to two key open problems in
genetics improved existing computational methods and uncovered novel biology,
including 9 new associations between autoimmune diseases and T cell subtypes
and 7 previously unreported links between disease variants linked to their
target genes. Our code is publicly available at
https://github.com/Alistair-Turcan/TusoAI.

</details>


### [270] [LLM/Agent-as-Data-Analyst: A Survey](https://arxiv.org/abs/2509.23988)
*Zirui Tang,Weizheng Wang,Zihang Zhou,Yang Jiao,Bangrui Xu,Boyu Niu,Xuanhe Zhou,Guoliang Li,Yeye He,Wei Zhou,Yitong Song,Cheng Tan,Bin Wang,Conghui He,Xiaoyang Wang,Fan Wu*

Main category: cs.AI

TL;DR: LLM/Agent-as-Data-Analyst techniques enable complex data understanding, natural language interfaces, and autonomous pipeline orchestration for data analysis across structured, semi-structured, unstructured, and heterogeneous data modalities.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based or small-model approaches have limitations in handling complex data analysis tasks, while LLM/agent techniques offer superior capabilities for data understanding, semantic analysis, and autonomous workflow management.

Method: The paper reviews LLM-based techniques across different data modalities: structured data (table QA, NL2GQL), semi-structured data (markup languages, table modeling), unstructured data (chart/document understanding, code vulnerability detection), and heterogeneous data (data retrieval, modality alignment).

Result: The technical evolution identifies five key design goals for intelligent data analysis agents: semantic-aware design, modality-hybrid integration, autonomous pipelines, tool-augmented workflows, and support for open-world tasks.

Conclusion: The paper outlines remaining challenges and proposes insights and practical directions for advancing LLM/Agent-powered data analysis, highlighting the need for continued development in this rapidly evolving field.

Abstract: Large language model (LLM) and agent techniques for data analysis (a.k.a
LLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both
academica and industry. In comparison with traditional rule or small-model
based approaches, (agentic) LLMs enable complex data understanding, natural
language interfaces, semantic analysis functions, and autonomous pipeline
orchestration. The technical evolution further distills five key design goals
for intelligent data analysis agents, namely semantic-aware design,
modality-hybrid integration, autonomous pipelines, tool-augmented workflows,
and support for open-world tasks. From a modality perspective, we review
LLM-based techniques for (i) structured data (e.g., table question answering
for relational data and NL2GQL for graph data), (ii) semi-structured data
(e.g., markup languages understanding and semi-structured table modeling),
(iii) unstructured data (e.g., chart understanding, document understanding,
programming languages vulnerable detection), and (iv) heterogeneous data (e.g.,
data retrieval and modality alignment for data lakes). Finally, we outline the
remaining challenges and propose several insights and practical directions for
advancing LLM/Agent-powered data analysis.

</details>


### [271] [Future-Proofing Programmers: Optimal Knowledge Tracing for AI-Assisted Personalized Education](https://arxiv.org/abs/2509.23996)
*Yuchen Wang,Pei-Duo Yu,Chee Wei Tan*

Main category: cs.AI

TL;DR: CoTutor is an AI-driven educational model that enhances Bayesian Knowledge Tracing with signal processing and generative AI to improve student progress modeling and deliver adaptive feedback, showing measurable improvements in learning outcomes.


<details>
  <summary>Details</summary>
Motivation: To advance learning science by combining knowledge tracing, signal processing, and generative AI to model student learning states and optimize education, inspired by Richard Hamming's vision of computer-aided 'learning to learn'.

Method: Proposes CoTutor model that enhances Bayesian Knowledge Tracing with signal processing techniques, uses convex optimization and signal processing for learning analytics automation, and combines generative AI with adaptive learning technology as an AI copilot.

Result: In university trials, CoTutor demonstrated measurable improvements in learning outcomes and outperformed conventional educational tools, showing potential for AI-driven personalization and scalability.

Conclusion: CoTutor highlights potential for AI-driven personalization and scalability in education while reserving pedagogical judgment for humans, with future opportunities for advancing privacy and ethical considerations in educational technology.

Abstract: Learning to learn is becoming a science, driven by the convergence of
knowledge tracing, signal processing, and generative AI to model student
learning states and optimize education. We propose CoTutor, an AI-driven model
that enhances Bayesian Knowledge Tracing with signal processing techniques to
improve student progress modeling and deliver adaptive feedback and strategies.
Deployed as an AI copilot, CoTutor combines generative AI with adaptive
learning technology. In university trials, it has demonstrated measurable
improvements in learning outcomes while outperforming conventional educational
tools. Our results highlight its potential for AI-driven personalization,
scalability, and future opportunities for advancing privacy and ethical
considerations in educational technology. Inspired by Richard Hamming's vision
of computer-aided 'learning to learn,' CoTutor applies convex optimization and
signal processing to automate and scale up learning analytics, while reserving
pedagogical judgment for humans, ensuring AI facilitates the process of
knowledge tracing while enabling learners to uncover new insights.

</details>


### [272] [Do Repetitions Matter? Strengthening Reliability in LLM Evaluations](https://arxiv.org/abs/2509.24086)
*Miguel Angel Alvarado Gonzalez,Michelle Bruno Hernandez,Miguel Angel Peñaloza Perez,Bruno Lopez Orozco,Jesus Tadeo Cruz Soto,Sandra Malagon*

Main category: cs.AI

TL;DR: Single-run LLM evaluations are unreliable; using 2-3 repetitions significantly improves ranking stability while remaining feasible for small teams.


<details>
  <summary>Details</summary>
Motivation: Current LLM leaderboards rely on single stochastic runs, but the reliability of such evaluations is unclear. The paper aims to determine how many repetitions are needed for robust model comparisons.

Method: Re-evaluated 8 state-of-the-art models on AI4Math Benchmark with 3 independent runs per setting. Used mixed-effects logistic regression, domain-level marginal means, rank-instability analysis, and run-to-run reliability assessment.

Result: Single-run leaderboards are brittle: 83% of slices invert at least one pairwise rank. Averaging runs yields modest standard error reduction (~5%) but large ranking improvements; two runs remove ~83% of single-run inversions.

Conclusion: Practitioners should treat evaluation as an experiment, report uncertainty, and use ≥2 repetitions under stochastic decoding to improve robustness while remaining feasible for small teams.

Abstract: LLM leaderboards often rely on single stochastic runs, but how many
repetitions are required for reliable conclusions remains unclear. We
re-evaluate eight state-of-the-art models on the AI4Math Benchmark with three
independent runs per setting. Using mixed-effects logistic regression,
domain-level marginal means, rank-instability analysis, and run-to-run
reliability, we assessed the value of additional repetitions. Our findings
shows that Single-run leaderboards are brittle: 10/12 slices (83\%) invert at
least one pairwise rank relative to the three-run majority, despite a zero
sign-flip rate for pairwise significance and moderate overall interclass
correlation. Averaging runs yields modest SE shrinkage ($\sim$5\% from one to
three) but large ranking gains; two runs remove $\sim$83\% of single-run
inversions. We provide cost-aware guidance for practitioners: treat evaluation
as an experiment, report uncertainty, and use $\geq 2$ repetitions under
stochastic decoding. These practices improve robustness while remaining
feasible for small teams and help align model comparisons with real-world
reliability.

</details>


### [273] [Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs](https://arxiv.org/abs/2509.24107)
*Shreyas Singh,Kunal Singh,Pradeep Moturi*

Main category: cs.AI

TL;DR: Fathom-DeepResearch is a two-model agentic system for complex information-seeking tasks, featuring Fathom-Search-4B for evidence-based web investigation and Fathom-Synthesizer-4B for structured report generation, achieving state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the need for reliable tool-integrated reasoning in agentic applications, particularly for complex open-ended information-seeking tasks where DeepResearch Agents have shown strong performance.

Method: Developed two specialized models: (1) Fathom-Search-4B trained with DUETQA dataset, RAPO reinforcement learning, and steerable step-level rewards for web search; (2) Fathom-Synthesizer-4B for converting search traces into structured reports. Used three training advances: multi-agent self-play dataset, stabilized RL with curriculum pruning, and cognitive behavior classification.

Result: Achieved state-of-the-art performance in open-weights category on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, with strong generalization to HLE, AIME-25, GPQA-Diamond, and MedQA tasks. Enables reliable tool-calling beyond 20 calls.

Conclusion: The Fathom-DeepResearch system demonstrates effective tool-integrated reasoning through specialized models and advanced training techniques, providing a robust solution for complex information-seeking tasks with strong generalization capabilities.

Abstract: Tool-integrated reasoning has emerged as a key focus for enabling agentic
applications. Among these, DeepResearch Agents have gained significant
attention for their strong performance on complex, open-ended
information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system
composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch
model trained from Qwen3-4B and optimized for evidence-based investigation
through live web search and targeted webpage querying. Its training combines
three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent
self-play that enforces strict web-search dependence and heterogeneous source
grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes
multi-turn Reinforcement Learning with Verifiable Rewards through curriculum
pruning, reward-aware advantage scaling, and per-prompt replay buffers; and
(iii) a steerable step-level reward that classifies each tool call by cognitive
behavior and marginal utility, enabling explicit control over search trajectory
breadth, depth, and horizon. These improvements enable reliable extension of
tool-calling beyond 20 calls when warranted. The second is
Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn
DeepSearch traces into structured, citation-dense DeepResearch Reports for
comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES,
WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves
state-of-the-art performance in the open-weights category while demonstrating
strong generalization to diverse reasoning tasks including HLE, AIME-25,
GPQA-Diamond, and MedQA.

</details>


### [274] [Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework](https://arxiv.org/abs/2509.24127)
*Nooshin Bahador*

Main category: cs.AI

TL;DR: A modular architecture for AI agents that enables natural language interaction with enterprise data warehouses, featuring transparent decision-making, automated evaluation, and statistical context for trustworthy deployment in data-sensitive domains.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between natural language interfaces and complex enterprise data warehouses, enabling non-technical users to access data through conversational interfaces while ensuring transparency and reliability.

Method: Component-based architecture with multi-layered reasoning framework, automated evaluation system, statistical context module, and integration with BigQuery ecosystem for secure data retrieval and business rule application.

Result: Successfully demonstrated through an insurance claims processing case study, creating a robust, evaluable, and trustworthy system for LLM-powered agents in high-stakes domains.

Conclusion: The integrated agent-development-with-evaluation framework provides an effective solution for deploying trustworthy AI agents in data-sensitive enterprise environments, with proven efficacy in real-world applications.

Abstract: This article presents a modular, component-based architecture for developing
and evaluating AI agents that bridge the gap between natural language
interfaces and complex enterprise data warehouses. The system directly
addresses core challenges in data accessibility by enabling non-technical users
to interact with complex data warehouses through a conversational interface,
translating ambiguous user intent into precise, executable database queries to
overcome semantic gaps. A cornerstone of the design is its commitment to
transparent decision-making, achieved through a multi-layered reasoning
framework that explains the "why" behind every decision, allowing for full
interpretability by tracing conclusions through specific, activated business
rules and data points. The architecture integrates a robust quality assurance
mechanism via an automated evaluation framework that serves multiple functions:
it enables performance benchmarking by objectively measuring agent performance
against golden standards, and it ensures system reliability by automating the
detection of performance regressions during updates. The agent's analytical
depth is enhanced by a statistical context module, which quantifies deviations
from normative behavior, ensuring all conclusions are supported by quantitative
evidence including concrete data, percentages, and statistical comparisons. We
demonstrate the efficacy of this integrated agent-development-with-evaluation
framework through a case study on an insurance claims processing system. The
agent, built on a modular architecture, leverages the BigQuery ecosystem to
perform secure data retrieval, apply domain-specific business rules, and
generate human-auditable justifications. The results confirm that this approach
creates a robust, evaluable, and trustworthy system for deploying LLM-powered
agents in data-sensitive, high-stakes domains.

</details>


### [275] [Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models](https://arxiv.org/abs/2509.24156)
*Yuhui Wang,Changjiang Li,Guangke Chen,Jiacheng Liang,Ting Wang*

Main category: cs.AI

TL;DR: Large reasoning models exhibit inconsistency between reasoning traces and final answers due to competing mechanisms of CoT reasoning and memory retrieval, which can be exploited as shortcuts during fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To understand why LRMs' final answers often contradict their own reasoning traces, and address the limitation in current reasoning fine-tuning paradigms where models exploit retrieval mechanisms as shortcuts.

Method: Conducted controlled experiments with misleading cues during reasoning and corrupted answers during retrieval. Introduced FARL framework integrating memory unlearning with reinforcement learning to suppress retrieval shortcuts.

Result: Confirmed both reasoning and retrieval mechanisms operate simultaneously, with dominance influenced by problem domains, model scales, and fine-tuning approaches. Models can exploit retrieval mechanisms to hack reward signals.

Conclusion: FARL framework successfully promotes reasoning-dominant behavior and enhances generalizable reasoning capabilities by suppressing retrieval shortcuts during fine-tuning.

Abstract: Large reasoning models (LRMs) exhibit unprecedented capabilities in solving
complex problems through Chain-of-Thought (CoT) reasoning. However, recent
studies reveal that their final answers often contradict their own reasoning
traces. We hypothesize that this inconsistency stems from two competing
mechanisms for generating answers: CoT reasoning and memory retrieval. To test
this hypothesis, we conduct controlled experiments that challenge LRMs with
misleading cues during reasoning and/or corrupted answers during retrieval. Our
results across models and datasets confirm that both mechanisms operate
simultaneously, with their relative dominance influenced by multiple factors:
problem domains, model scales, and fine-tuning approaches (e.g., reinforcement
learning vs. distillation). The findings reveal a critical limitation in
current reasoning fine-tuning paradigms: models can exploit the retrieval
mechanism as a shortcut, effectively "hacking" the reward signal and
undermining genuine reasoning development. To address this challenge, we
introduce FARL, a novel fine-tuning framework that integrates memory unlearning
with reinforcement learning. By carefully suppressing retrieval shortcuts
during the fine-tuning process, FARL promotes reasoning-dominant behavior and
enhances generalizable reasoning capabilities.

</details>


### [276] [Robust Preference Optimization: Aligning Language Models with Noisy Preference Feedback](https://arxiv.org/abs/2509.24159)
*Xiaoyang Cao,Zelai Xu,Mo Guang,Kaiwen Long,Michiel A. Bakker,Yu Wang,Chao Yu*

Main category: cs.AI

TL;DR: RPO is a meta-framework that transforms existing preference alignment methods into robust versions by addressing noisy and heterogeneous human preference data through EM-based label correctness estimation and adaptive re-weighting.


<details>
  <summary>Details</summary>
Motivation: Standard alignment methods assume homogeneous and noiseless human preferences, but real-world preferences are pluralistic and annotations contain errors, creating a discrepancy between recorded data and ground-truth preferences that degrades model performance.

Method: Uses Expectation-Maximization algorithm to infer posterior probability of label correctness, adaptively re-weights data points in training loss, and establishes theoretical link between preference losses and probabilistic models to systematically transform existing alignment algorithms.

Result: RPO consistently enhances four state-of-the-art alignment algorithms (DPO, IPO, SimPO, CPO), achieving up to 7.0% win rate gain on AlpacaEval 2 and 5.4% on Arena-Hard when applied to Mistral and Llama 3 models.

Conclusion: RPO provides a theoretical guarantee of convergence to true noise level under perfect model calibration and serves as an effective meta-framework for robust preference alignment that addresses fundamental limitations in current human preference-based methods.

Abstract: Standard human preference-based alignment methods, such as Reinforcement
Learning from Human Feedback (RLHF), are a cornerstone technology for aligning
Large Language Models (LLMs) with human values. However, these methods are all
underpinned by a critical, yet flawed assumption: human preferences are
homogeneous (representing a single, unified preference) and the collected data
is noiseless (free from error). In reality, neither is true since human
preference is pluralistic and annotators can make mistakes. This creates a
discrepancy between the recorded data and the ground-truth preferences, which
can misguide the model and degrade its performance. To address this challenge,
we introduce Robust Preference Optimization (RPO). RPO employs an
Expectation-Maximization (EM) algorithm to infer the posterior probability of
each label's correctness, which is used to adaptively re-weigh each data point
in the training loss to mitigate noise. We further generalize this approach by
establishing a theoretical link between arbitrary preference losses and their
corresponding probabilistic models. This generalization enables the systematic
transformation of existing alignment algorithms into their robust counterparts,
elevating RPO from a specific algorithm to a meta-framework for robust
preference alignment. Theoretically, we prove that under the condition of a
perfectly calibrated model, RPO is guaranteed to converge to the true noise
level of the dataset. Our experiments demonstrate RPO's effectiveness as a
meta-framework, consistently enhancing four state-of-the-art alignment
algorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3
models, the RPO-enhanced methods achieve substantial win rate gains on
AlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% and 5.4%,
respectively.

</details>


### [277] [Humanline: Online Alignment as Perceptual Loss](https://arxiv.org/abs/2509.24207)
*Sijia Liu,Niklas Muennighoff,Kawin Ethayarajh*

Main category: cs.AI

TL;DR: Online alignment methods like GRPO outperform offline methods like DPO due to better approximation of human-perceived probability distributions and incorporation of perceptual biases through PPO/GRPO-style clipping. The paper proposes 'humanline' variants that explicitly model human perceptual distortions, enabling offline training to match online performance.


<details>
  <summary>Details</summary>
Motivation: To understand why online alignment methods perform better than offline methods, drawing from behavioral economics and prospect theory to explain the role of human perceptual biases in probability assessment.

Method: Theoretical analysis showing online on-policy sampling better approximates human-perceived distributions, and PPO/GRPO clipping recovers perceptual biases. Proposes 'humanline' variants of DPO/KTO/GRPO that explicitly incorporate human perceptual distortions of probability.

Result: Humanline variants trained with offline off-policy data can match the performance of their online counterparts on both verifiable and unverifiable tasks.

Conclusion: The online/offline dichotomy is incidental to maximizing human utility. By explicitly modeling human perceptual distortions, offline training can achieve the same performance as online methods, enabling faster, cheaper, and more flexible post-training without sacrificing performance.

Abstract: Online alignment (e.g., GRPO) is generally more performant than offline
alignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral
economics, we propose a human-centric explanation. We prove that online
on-policy sampling better approximates the human-perceived distribution of what
the model can produce, and PPO/GRPO-style clipping -- originally introduced to
just stabilize training -- recovers a perceptual bias in how humans perceive
probability. In this sense, PPO/GRPO act as perceptual losses already. Our
theory further suggests that the online/offline dichotomy is itself incidental
to maximizing human utility, since we can achieve the same effect by
selectively training on any data in a manner that mimics human perception,
rather than restricting ourselves to online on-policy data. Doing so would
allow us to post-train more quickly, cheaply, and flexibly without sacrificing
performance. To this end, we propose a design pattern that explicitly
incorporates perceptual distortions of probability into objectives like
DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that
these humanline variants, even when trained with offline off-policy data, can
match the performance of their online counterparts on both verifiable and
unverifiable tasks.

</details>


### [278] [ELHPlan: Efficient Long-Horizon Task Planning for Multi-Agent Collaboration](https://arxiv.org/abs/2509.24230)
*Shaobin Ling,Yun Wang,Chenyou Fan,Tin Lun Lam,Junjie Hu*

Main category: cs.AI

TL;DR: ELHPlan is a novel LLM-based multi-robot planning framework that uses Action Chains to balance adaptability and efficiency, achieving comparable task success with only 24% of tokens compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based multi-robot collaboration faces fundamental trade-offs: declarative methods lack adaptability in dynamic environments, while iterative methods have prohibitive computational costs that scale poorly with team size and task complexity.

Method: ELHPlan introduces Action Chains (sequences of actions bound to sub-goal intentions) as planning primitives. It operates through a cyclical process: constructing intention-bound action sequences, proactive validation for conflicts/feasibility, targeted refinement, and execution of validated actions.

Result: Experiments on TDW-MAT and C-WAH benchmarks show ELHPlan achieves comparable task success rates while consuming only 24% of the tokens required by state-of-the-art methods. It also introduces comprehensive efficiency metrics including token consumption and planning time.

Conclusion: ELHPlan establishes a new efficiency-effectiveness frontier for LLM-based multi-agent planning systems by balancing adaptability and efficiency through Action Chains and proactive validation mechanisms.

Abstract: Large Language Models (LLMs) enable intelligent multi-robot collaboration but
face fundamental trade-offs: declarative methods lack adaptability in dynamic
environments, while iterative methods incur prohibitive computational costs
that scale poorly with team size and task complexity. In this paper, we propose
ELHPlan, a novel framework that introduces Action Chains--sequences of actions
explicitly bound to sub-goal intentions--as the fundamental planning primitive.
ELHPlan operates via a cyclical process: 1) constructing intention-bound action
sequences, 2) proactively validating for conflicts and feasibility, 3) refining
issues through targeted mechanisms, and 4) executing validated actions. This
design balances adaptability and efficiency by providing sufficient planning
horizons while avoiding expensive full re-planning. We further propose
comprehensive efficiency metrics, including token consumption and planning
time, to more holistically evaluate multi-agent collaboration. Our experiments
on benchmark TDW-MAT and C-WAH demonstrate that ELHPlan achieves comparable
task success rates while consuming only 24% of the tokens required by
state-of-the-art methods. Our research establishes a new
efficiency-effectiveness frontier for LLM-based multi-agent planning systems.

</details>


### [279] [Learning to Ponder: Adaptive Reasoning in Latent Space](https://arxiv.org/abs/2509.24238)
*Yixin He,Lumingyuan Tang*

Main category: cs.AI

TL;DR: FR-Ponder is a framework that adaptively allocates reasoning compute for LLMs by using a small controller to decide when to halt or apply ponder steps via latent steering vectors, optimizing compute usage without modifying backbone weights.


<details>
  <summary>Details</summary>
Motivation: Current approaches like Best-of-N and majority voting use uniform compute depth across all inputs, which wastes computation on simple queries and under-thinks complex ones. There's a need for instance-adaptive reasoning compute allocation.

Method: Uses a <1M-param controller that observes hidden states to decide whether to halt or apply ponder steps by adding pre-computed steering vectors to frozen representations. Employs Group Relative Policy Optimization (GRPO) for reward signals and curriculum learning to regulate reasoning depth adaptively.

Result: On GSM8K and MATH500 benchmarks, FR-Ponder improves the compute-accuracy frontier, achieving lower FLOPs with better matched accuracy compared to early-exit baselines, without modifying backbone weights.

Conclusion: FR-Ponder successfully enables instance-adaptive reasoning compute allocation through latent steering, demonstrating interpretable steering directions and learned compute allocation that correlates with problem difficulty.

Abstract: Test-time compute has emerged as a key paradigm for enhancing LLM reasoning,
yet prevailing approaches like Best-of-N and majority voting apply uniform
depth across inputs, wasting computation on simple queries while potentially
under-thinking complex ones. We present FR-Ponder, a single-graph,
backbone-training-free framework that allocates instance-adaptive reasoning
compute via latent steering. A less than 1M-param controller observes hidden
states and decides to halt or apply a small ponder step by adding a
pre-computed steering vector to frozen representations. Our method extracts the
latent steering vector associated with deeper reasoning outputs and direct IO
from LLM and re-applies it through a tunable scaling factor, allowing the model
to adapt its reasoning depth to the complexity of each input. To balance
performance and computational cost, we employ Group Relative Policy
Optimization (GRPO) as a reward signal to adaptively regulate reasoning depth,
achieving task accuracy while mitigating overreasoning. Through curriculum
learning and careful reward engineering, FR-Ponder learns calibrated compute
allocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder
improves the compute-accuracy frontier, delivering lower FLOPs with better
matched accuracy and comparing favorably to early-exit baselines, without
modifying backbone weights. Analyses visualize interpretable steering
directions and show learned compute allocation correlates with problem
difficulty.

</details>


### [280] [Model Merging Scaling Laws in Large Language Models](https://arxiv.org/abs/2509.24244)
*Yuanyi Wang,Yanggan Gu,Yiming Zhang,Qi Zhou,Zhaoyi Yan,Congkai Xie,Xinyao Wang,Jianbo Yuan,Hongxia Yang*

Main category: cs.AI

TL;DR: The paper identifies a power law for language model merging that predicts performance gains from combining experts, showing diminishing returns with more experts and enabling predictive planning for model composition.


<details>
  <summary>Details</summary>
Motivation: To establish quantitative scaling laws for language model merging, which is widely used in practice but lacks predictive rules for returns when adding experts or scaling model size.

Method: Empirical study of scaling laws for model merging measured by cross-entropy, analyzing how performance scales with model size and number of experts across diverse architectures and merging methods (Average, TA, TIES, DARE).

Result: Identified a compact power law showing size-dependent floor decreases with model capacity, merging tail exhibits diminishing returns (roughly 1/k), gains arrive early, and variability shrinks with more experts. The law holds in-domain and cross-domain across diverse methods.

Conclusion: The scaling law enables predictive planning for model merging, allowing estimation of expert needs, stopping criteria, and trade-offs between scaling base models vs adding experts. This turns merging from heuristic practice into a computationally efficient alternative to multitask training, suggesting a scaling principle for distributed generative AI.

Abstract: We study empirical scaling laws for language model merging measured by
cross-entropy. Despite its wide practical use, merging lacks a quantitative
rule that predicts returns as we add experts or scale the model size. We
identify a compact power law that links model size and expert number: the
size-dependent floor decreases with model capacity, while the merging tail
exhibits clear diminishing returns in the number of experts. The law holds
in-domain and cross-domain, tightly fits measured curves across diverse
architectures and methods (Average, TA, TIES, DARE), and explains two robust
regularities: most gains arrive early, and variability shrinks as more experts
are included. Building on this, we present a simple theory that explains why
gains fall roughly as 1/k and links the floor and tail to properties of the
base model and the diversity across domains. This law enables predictive
planning: estimate how many experts are needed to reach a target loss, decide
when to stop adding experts, and trade off scaling the base model versus adding
experts under a fixed budget--turning merging from heuristic practice into a
computationally efficient, planable alternative to multitask training. This
suggests a scaling principle for distributed generative AI: predictable gains
can be achieved by composing specialists, offering a complementary path toward
AGI-level systems.

</details>


### [281] [SpecExit: Accelerating Large Reasoning Model via Speculative Exit](https://arxiv.org/abs/2509.24248)
*Rubing Yang,Huajun Bai,Song Liu,Guanghua Yu,Runzhi Fan,Yanbin Dang,Jiejing Zhang,Kai Liu,Jianchen Zhu,Peng Chen*

Main category: cs.AI

TL;DR: SpecExit is a novel framework that uses hidden states from lightweight draft models to predict both future tokens and early-exit signals, reducing generation length by 66% and achieving 2.5x speedup in end-to-end latency without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models suffer from overthinking, producing unnecessarily long outputs with high latency, limiting real-world deployment. Existing early-exit mechanisms have detection overhead that limits latency gains and generalizability.

Method: Proposes SpecExit framework that predicts future tokens and early-exit signals directly from lightweight draft model's hidden states, eliminating probing overhead. Inspired by speculative decoding's use of hidden states.

Result: Reduces average generation length by 66%, achieves 2.5x speedup in end-to-end latency compared to speculative decoding baseline, while maintaining accuracy.

Conclusion: Hidden states provide effective early-exit signals, suggesting broader use of hidden states for efficient reasoning. Method demonstrates significant improvements in efficiency without accuracy loss.

Abstract: Despite their strong performance on reasoning tasks, large reasoning models
(LRMs) often suffer from overthinking, producing unnecessarily long outputs and
incurring high end-to-end latency, a significant limitation to their real-world
deployment. To address overthinking, early-exit mechanisms have been proposed
to terminate reasoning before typical completion, showing that this approach
can effectively shorten generation length with minimal impact on accuracy.
However, their reliance on probing mechanisms introduces a detection overhead
that limits their end-to-end latency gains and compromises their
generalizability across diverse problems. Inspired by the use of hidden states
in speculative decoding, we propose SpecExit, a novel framework that predicts
both future tokens and an early-exit signal directly from a lightweight draft
model without probing overhead. Our method offers significant improvements,
reducing average generation length by 66\% and achieving a 2.5x speedup in
end-to-end latency compared to the speculative decoding baseline, without
compromising accuracy. Our method leverages the inherent signals from hidden
states to provide effective early-exit signals, suggesting broader use of
hidden states for efficient reasoning. Our code is available at
https://github.com/Tencent/AngelSlim.

</details>


### [282] [Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations](https://arxiv.org/abs/2509.24250)
*Edward Kim,Daniel He,Jorge Chao,Wiktor Rajca,Mohammed Amin,Nishant Malpani,Ruta Desai,Antti Oulasvirta,Bjoern Hartmann,Sanjit Seshia*

Main category: cs.AI

TL;DR: The paper presents a system for teaching collaborative physical tasks using program synthesis and narrated demonstrations, enabling users to teach, inspect, and correct system behavior without coding.


<details>
  <summary>Details</summary>
Motivation: Teaching collaborative physical tasks is challenging due to the need to infer users' assumptions about teammates' intent, requiring interpretable and correctable representations.

Method: Frames collaborative task learning as program synthesis, representing behavior as editable programs using narrated demonstrations (paired physical actions and natural language) as a unified teaching modality.

Result: In a study with 20 users teaching soccer tactics, 70% successfully refined learned programs to match intent and 90% found it easy to correct programs.

Conclusion: The approach enables effective teaching of collaborative physical activities but surfaces unique challenges in program representation and teaching, with proposed mitigation strategies.

Abstract: Teaching systems physical tasks is a long standing goal in HCI, yet most
prior work has focused on non collaborative physical activities. Collaborative
tasks introduce added complexity, requiring systems to infer users assumptions
about their teammates intent, which is an inherently ambiguous and dynamic
process. This necessitates representations that are interpretable and
correctable, enabling users to inspect and refine system behavior. We address
this challenge by framing collaborative task learning as a program synthesis
problem. Our system represents behavior as editable programs and uses narrated
demonstrations, i.e. paired physical actions and natural language, as a unified
modality for teaching, inspecting, and correcting system logic without
requiring users to see or write code. The same modality is used for the system
to communicate its learning to users. In a within subjects study, 20 users
taught multiplayer soccer tactics to our system. 70 percent (14/20) of
participants successfully refined learned programs to match their intent and 90
percent (18/20) found it easy to correct the programs. The study surfaced
unique challenges in representing learning as programs and in enabling users to
teach collaborative physical activities. We discuss these issues and outline
mitigation strategies.

</details>


### [283] [Rethinking and Benchmarking Large Language Models for Graph Reasoning](https://arxiv.org/abs/2509.24260)
*Yuwei Hu,Xinyi Huang,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.AI

TL;DR: LLMs for graph reasoning are underperforming due to improper focus on replicating rather than designing graph algorithms. A new benchmark GraphAlgorithm and method Simple-RTC achieve much better performance.


<details>
  <summary>Details</summary>
Motivation: Existing LLM methods for graph reasoning have underwhelming performance due to improper reasoning focus on replicating graph algorithms instead of designing them.

Method: Proposed Simple-RTC (Simple-Reasoning-Then-Coding) method that guides LLMs to first design graph algorithms and then code to solve graph reasoning tasks.

Result: Simple-RTC achieves near-perfect accuracy on existing benchmarks and significantly outperforms GPT-4o-mini and all prior methods on the new GraphAlgorithm benchmark.

Conclusion: Redirecting reasoning focus from algorithm replication to algorithm design enables LLMs to solve most graph reasoning tasks, and the proposed Simple-RTC provides a strong baseline for future research.

Abstract: Large Language Models (LLMs) for Graph Reasoning have been extensively
studied over the past two years, involving enabling LLMs to understand graph
structures and reason on graphs to solve various graph problems, with graph
algorithm problems being the most prevalent. Recent studies underscore the
potential of LLMs in handling graph reasoning tasks, but their performance is
underwhelming. In this work, we point out issues with existing methods and
benchmarks, and rethink the direction that LLMs for graph reasoning should
strive toward. We find that base models, e.g., GPT-4o-mini, are largely
underestimated due to improper reasoning focus. Base models with reasoning
focus redirected from replicating graph algorithms to designing them can easily
solve most graph reasoning tasks in existing benchmarks. To truly evaluate the
graph reasoning capabilities of LLMs, we construct a more challenging
GraphAlgorithm benchmark, comprising 239 different graph problems and 3,041
test instances collected from 4 competition platforms. Finally, we introduce a
simple and strong baseline Simple-Reasoning-Then-Coding (Simple-RTC)-which
guides LLMs to design graph algorithms first and then code to address graph
reasoning tasks. Simple-RTC achieves near-perfect accuracy on existing
benchmarks and significantly outperforms GPT-4o-mini and all prior methods on
the GraphAlgorithm benchmark. This strong baseline encourages further
advancements in LLMs for Graph Reasoning in the future.

</details>


### [284] [Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models](https://arxiv.org/abs/2509.24261)
*Yuhua Jiang,Jiawei Huang,Yufeng Yuan,Xin Mao,Yu Yue,Qianchuan Zhao,Lin Yan*

Main category: cs.AI

TL;DR: RLVR enhances LLMs on reasoning tasks but suffers from exploration limitations due to peaked initial policies, suppressing solution diversity. Risk-Sensitive RL framework with RS-GRPO algorithm addresses this by amplifying learning from challenging prompts, improving pass@k performance while maintaining pass@1 accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods face exploration dilemma where pre-trained LLMs' peaked initial policies limit solution diversity and multi-solution performance (pass@k), causing RL to distill existing capabilities rather than discover new reasoning strategies.

Method: Proposed Risk-Sensitive Reinforcement Learning framework with risk-seeking objective that interpolates between mean and maximum rewards, leading to RS-GRPO algorithm that drives deeper exploration by amplifying learning from challenging prompts.

Result: On six mathematical reasoning benchmarks with five different LLMs, RS-GRPO consistently improves pass@k performance while maintaining or enhancing pass@1 accuracy.

Conclusion: Risk-sensitive RL framework effectively addresses exploration limitations in RLVR, enabling discovery of new reasoning strategies while maintaining single-solution performance through simple implementation requiring only minor code modifications.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for enhancing Large Language Models (LLMs) on complex reasoning tasks. However,
existing methods suffer from an exploration dilemma: the sharply peaked initial
policies of pre-trained LLMs confine standard RL algorithms to a narrow set of
solutions, boosting single-solution accuracy (pass@1) but suppressing solution
diversity and multi-solution performance (pass@k). As a result, RLVR often
distills existing capabilities rather than discovering new reasoning
strategies. To overcome this, we introduce a Risk-Sensitive Reinforcement
Learning framework. Our approach employs a risk-seeking objective that
interpolates between mean and maximum rewards, leading to a novel algorithm,
Risk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying
learning from challenging prompts. Remarkably, RS-GRPO is simple to implement,
requiring only minor code modifications. On six mathematical reasoning
benchmarks and with five different LLMs, RS-GRPO consistently improves pass@k
performance while maintaining or enhancing pass@1 accuracy.

</details>


### [285] [PAME-AI: Patient Messaging Creation and Optimization using Agentic AI](https://arxiv.org/abs/2509.24263)
*Junjie Luo,Yihong Guo,Anqi Liu,Ritu Agarwal,Gordon,Gao*

Main category: cs.AI

TL;DR: PAME-AI is an agentic AI system that optimizes patient messaging design, achieving 12.2% improvement in engagement rates through structured data transformation and parallel processing.


<details>
  <summary>Details</summary>
Motivation: Traditional mobile message design has limitations in exploring high-dimensional design spaces, and there's a need for more effective healthcare communication to improve medication adherence and healthy behaviors.

Method: Built on DIKW hierarchy, PAME-AI uses specialized computational agents to transform raw experimental data into actionable message design strategies through a two-stage experiment with over 500,000 patient encounters.

Result: The best-performing generated message achieved 68.76% engagement vs 61.27% baseline, representing 12.2% relative improvement in click-through rates.

Conclusion: PAME-AI's agentic architecture enables effective large-scale healthcare communication optimization through parallel processing, hypothesis validation, and continuous learning.

Abstract: Messaging patients is a critical part of healthcare communication, helping to
improve things like medication adherence and healthy behaviors. However,
traditional mobile message design has significant limitations due to its
inability to explore the high-dimensional design space. We develop PAME-AI, a
novel approach for Patient Messaging Creation and Optimization using Agentic
AI. Built on the Data-Information-Knowledge-Wisdom (DIKW) hierarchy, PAME-AI
offers a structured framework to move from raw data to actionable insights for
high-performance messaging design. PAME-AI is composed of a system of
specialized computational agents that progressively transform raw experimental
data into actionable message design strategies. We demonstrate our approach's
effectiveness through a two-stage experiment, comprising of 444,691 patient
encounters in Stage 1 and 74,908 in Stage 2. The best-performing generated
message achieved 68.76% engagement compared to the 61.27% baseline,
representing a 12.2\% relative improvement in click-through rates. This agentic
architecture enables parallel processing, hypothesis validation, and continuous
learning, making it particularly suitable for large-scale healthcare
communication optimization.

</details>


### [286] [AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models](https://arxiv.org/abs/2509.24269)
*Zihao Zhu,Xinyu Wu,Gehan Hu,Siwei Lyu,Ke Xu,Baoyuan Wu*

Main category: cs.AI

TL;DR: AdvChain is an alignment method that teaches Large Reasoning Models dynamic self-correction through adversarial CoT tuning to address the snowball effect in reasoning safety.


<details>
  <summary>Details</summary>
Motivation: Current safety CoT tuning methods suffer from the snowball effect where minor reasoning deviations amplify throughout the thought process, leading to harmful compliance or excessive refusal.

Method: AdvChain constructs a dataset with Temptation-Correction and Hesitation-Correction samples to teach models dynamic self-correction through adversarial CoT tuning.

Result: AdvChain significantly enhances robustness against jailbreak attacks and CoT hijacking while reducing over-refusal on benign prompts, achieving superior safety-utility balance.

Conclusion: The work establishes a new direction for building more robust and reliable reasoning models through dynamic self-correction capabilities.

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in
complex problem-solving through Chain-of-Thought (CoT) reasoning. However, the
multi-step nature of CoT introduces new safety challenges that extend beyond
conventional language model alignment. We identify a failure mode in current
safety CoT tuning methods: the \textit{snowball effect}, where minor reasoning
deviations progressively amplify throughout the thought process, leading to
either harmful compliance or excessive refusal. This effect stems from models
being trained to imitate perfect reasoning scripts without learning to
self-correct. To address this limitation, we propose AdvChain, an alignment
paradigm that teaches models dynamic self-correction through adversarial CoT
tuning. Our method involves constructing a dataset containing
Temptation-Correction and Hesitation-Correction samples, where models learn to
recover from harmful reasoning drifts and unnecessary cautions. Extensive
experiments show that AdvChain significantly enhances robustness against
jailbreak attacks and CoT hijacking while substantially reducing over-refusal
on benign prompts, achieving a superior safety-utility balance without
compromising reasoning capabilities. Our work establishes a new direction for
building more robust and reliable reasoning models.

</details>


### [287] [G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge](https://arxiv.org/abs/2509.24276)
*Linhao Luo,Zicheng Zhao,Junnan Liu,Zhangchi Qiu,Junnan Dong,Serge Panev,Chen Gong,Thuy-Trang Vu,Gholamreza Haffari,Dinh Phung,Alan Wee-Chung Liew,Shirui Pan*

Main category: cs.AI

TL;DR: G-reasoner is a unified framework that integrates graph and language foundation models for reasoning over diverse graph-structured knowledge, addressing limitations in existing retrieval-augmented generation methods.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods struggle with knowledge-intensive tasks due to fragmented information and weak modeling of knowledge structure. LLMs cannot effectively reason over graph-structured data, and current GraphRAG approaches have scalability and generalization issues.

Method: Proposes QuadGraph (four-layer abstraction to unify heterogeneous knowledge), a 34M-parameter graph foundation model that captures graph topology and textual semantics, integrated with LLMs. Uses mixed-precision training and distributed message-passing for scalability.

Result: Extensive experiments on six benchmarks show G-reasoner consistently outperforms state-of-the-art baselines, significantly enhances LLM reasoning, and achieves strong efficiency and cross-graph generalization.

Conclusion: G-reasoner provides an effective solution for graph-enhanced reasoning, overcoming limitations of existing RAG methods through unified graph-language integration and scalable architecture.

Abstract: Large language models (LLMs) excel at complex reasoning but remain limited by
static and incomplete parametric knowledge. Retrieval-augmented generation
(RAG) mitigates this by incorporating external knowledge, yet existing RAGs
struggle with knowledge-intensive tasks due to fragmented information and weak
modeling of knowledge structure. Graphs offer a natural way to model
relationships within knowledge, but LLMs are inherently unstructured and cannot
effectively reason over graph-structured data. Recent graph-enhanced RAG
(GraphRAG) attempts to bridge this gap by constructing tailored graphs and
enabling LLMs to reason on them. However, these methods often depend on ad-hoc
graph designs, heuristic search, or costly agent pipelines, which hinder
scalability and generalization. To address these challenges, we present
G-reasoner, a unified framework that integrates graph and language foundation
models for reasoning over diverse graph-structured knowledge. Central to our
approach is QuadGraph, a standardized four-layer abstraction that unifies
heterogeneous knowledge sources into a common graph representation. Building on
this, we introduce a 34M-parameter graph foundation model (GFM) that jointly
captures graph topology and textual semantics, and is integrated with LLMs to
enhance reasoning in downstream applications. To ensure scalability and
efficiency, mixed-precision training and distributed message-passing are
implemented to scale GFM with more GPUs. Extensive experiments on six
benchmarks show that G-reasoner consistently outperforms state-of-the-art
baselines, significantly enhances LLM reasoning, and achieves strong efficiency
and cross-graph generalization.

</details>


### [288] [SCI-Verifier: Scientific Verifier with Thinking](https://arxiv.org/abs/2509.24285)
*Shenghe Zheng,Chenyu Huang,Fangchen Yu,Junchi Yao,Jingqi Ye,Tao Chen,Yun Luo,Ning Ding,LEI BAI,Ganqu Cui,Peng Ye*

Main category: cs.AI

TL;DR: The paper addresses challenges in verifying LLM-generated scientific answers by proposing SCI-VerifyBench, a cross-disciplinary benchmark, and SCI-Verifier, a reasoning-augmented verification model.


<details>
  <summary>Details</summary>
Motivation: Existing verification methods for LLMs in scientific domains lack systematic evaluation standards, have insufficient disciplinary coverage, and rely on cumbersome rule design or prompt engineering, limiting their effectiveness and generalization.

Method: 1) Construct SCI-VerifyBench benchmark covering multiple scientific disciplines with real LLM responses and domain-specific equivalence transformations; 2) Develop SCI-Verifier, a unified reasoning-augmented verifier trained through post-training to enhance logical reasoning and equivalence judgment.

Result: The proposed framework provides systematic evaluation standards and practical verification capabilities, demonstrating strong logical reasoning and stable outputs for scientific answer verification.

Conclusion: SCI-VerifyBench and SCI-Verifier together offer a principled framework that enhances the reliability and applicability of LLMs in scientific domains through rigorous evaluation and effective verification methods.

Abstract: As large language models (LLMs) are increasingly applied to scientific
reasoning, the complexity of answer formats and the diversity of equivalent
expressions make answer verification a critical yet challenging task. Existing
verification studies in scientific domains suffer from two major limitations:
(a) the absence of systematic evaluation standards and insufficient
disciplinary coverage, which hinders their comprehensive assessment; and (b)
heavy reliance on cumbersome rule design or prompt engineering, which reduces
their effectiveness in complex reasoning scenarios or limits their
cross-disciplinary generalization. To address these challenges, we propose
solutions at both the data and model levels. On the data side, we construct
SCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics,
biology, chemistry, and general scientific QA. The benchmark is built from real
LLM responses and enhanced with domain-specific equivalence transformations
that generate challenging and realistic data. Model-based and expert
annotations ensure both quality and diversity, enabling rigorous evaluation of
verification ability. On the model side, we emphasize the importance of
reasoning for verification and introduce SCI-Verifier, a unified
reasoning-augmented verifier for scientific domains. Through post-training,
SCI-Verifier demonstrates strong logical reasoning and equivalence judgment
capabilities while maintaining concise and stable outputs. Together,
SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific
verification, offering both systematic evaluation and practical pathways to
enhance the reliability and applicability of LLMs in scientific domains.

</details>


### [289] [Experience Paper: Adopting Activity Recognition in On-demand Food Delivery Business](https://arxiv.org/abs/2509.24303)
*Huatao Xu,Yan Zhang,Wei Gao,Guobin Shen,Mo Li*

Main category: cs.AI

TL;DR: First nationwide deployment of human activity recognition in food delivery industry using adapted LIMU-BERT model, scaled from city pilot to 500k couriers across China with significant operational benefits.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the real-world application and transformative potential of HAR technology in the on-demand food delivery industry at a massive scale.

Method: Adapted LIMU-BERT foundation model through three deployment phases over two years, progressing from feasibility study in one city to nationwide adoption.

Result: Successfully deployed to 500,000 couriers across 367 cities in China, enabling downstream applications and demonstrating significant operational and economic benefits.

Conclusion: HAR technology has transformative potential in real-world applications, with successful large-scale deployment providing valuable lessons and open-sourced pretrained model.

Abstract: This paper presents the first nationwide deployment of human activity
recognition (HAR) technology in the on-demand food delivery industry. We
successfully adapted the state-of-the-art LIMU-BERT foundation model to the
delivery platform. Spanning three phases over two years, the deployment
progresses from a feasibility study in Yangzhou City to nationwide adoption
involving 500,000 couriers across 367 cities in China. The adoption enables a
series of downstream applications, and large-scale tests demonstrate its
significant operational and economic benefits, showcasing the transformative
potential of HAR technology in real-world applications. Additionally, we share
lessons learned from this deployment and open-source our LIMU-BERT pretrained
with millions of hours of sensor data.

</details>


### [290] [MedMMV: A Controllable Multimodal Multi-Agent Framework for Reliable and Verifiable Clinical Reasoning](https://arxiv.org/abs/2509.24314)
*Hongjun Liu,Yinghao Zhu,Yuhui Wang,Yitao Long,Zeyu Lai,Lequan Yu,Chen Zhao*

Main category: cs.AI

TL;DR: MedMMV is a controllable multimodal multi-agent framework that addresses instability and hallucination in medical AI systems by using diversified reasoning rollouts, structured evidence graphs, and uncertainty scoring to improve reliability and verifiability.


<details>
  <summary>Details</summary>
Motivation: Current multimodal large language models show promising performance in medical applications but suffer from critical instability in early evidence interpretation that leads to hallucination and inconsistent conclusions, highlighting the need for more reliable clinical reasoning agents.

Method: MedMMV stabilizes reasoning through diversified short rollouts, grounds intermediate steps in structured evidence graphs supervised by a Hallucination Detector, and aggregates candidate paths using a Combined Uncertainty scorer.

Result: On six medical benchmarks, MedMMV improves accuracy by up to 12.7% and demonstrates superior reliability. Blind physician evaluations confirm substantial increases in reasoning truthfulness without sacrificing informational content.

Conclusion: By controlling instability through a verifiable, multi-agent process, MedMMV provides a robust path toward deploying trustworthy AI systems in high-stakes clinical decision support domains.

Abstract: Recent progress in multimodal large language models (MLLMs) has demonstrated
promising performance on medical benchmarks and in preliminary trials as
clinical assistants. Yet, our pilot audit of diagnostic cases uncovers a
critical failure mode: instability in early evidence interpretation precedes
hallucination, creating branching reasoning trajectories that cascade into
globally inconsistent conclusions. This highlights the need for clinical
reasoning agents that constrain stochasticity and hallucination while producing
auditable decision flows. We introduce MedMMV, a controllable multimodal
multi-agent framework for reliable and verifiable clinical reasoning. MedMMV
stabilizes reasoning through diversified short rollouts, grounds intermediate
steps in a structured evidence graph under the supervision of a Hallucination
Detector, and aggregates candidate paths with a Combined Uncertainty scorer. On
six medical benchmarks, MedMMV improves accuracy by up to 12.7% and, more
critically, demonstrates superior reliability. Blind physician evaluations
confirm that MedMMV substantially increases reasoning truthfulness without
sacrificing informational content. By controlling instability through a
verifiable, multi-agent process, our framework provides a robust path toward
deploying trustworthy AI systems in high-stakes domains like clinical decision
support.

</details>


### [291] [humancompatible.detect: a Python Toolkit for Detecting Bias in AI Models](https://arxiv.org/abs/2509.24340)
*German M. Matilla,Jiri Nemecek,Illia Kryvoviaz,Jakub Marecek*

Main category: cs.AI

TL;DR: A toolkit called humancompatible.detect is introduced for bias detection in AI systems, addressing scalability and computability issues of traditional distance estimation methods.


<details>
  <summary>Details</summary>
Motivation: To meet regulatory demands for trustworthy AI, particularly the AI Act's requirements for measuring data quality and estimating bias in high-risk AI systems, while overcoming challenges in traditional methods.

Method: The toolkit incorporates two new methods: maximum subgroup discrepancy (MSD) and subsampled ℓ∞ distances, providing an easy-to-use API with multiple examples.

Result: humancompatible.detect successfully addresses scalability issues with MMD and computability issues with Wasserstein-1 distance, offering practical bias detection capabilities.

Conclusion: The toolkit provides an effective solution for bias detection in AI systems, is licensed under Apache License 2.0, and helps practitioners comply with international AI regulations.

Abstract: There is a strong recent emphasis on trustworthy AI. In particular,
international regulations, such as the AI Act, demand that AI practitioners
measure data quality on the input and estimate bias on the output of high-risk
AI systems. However, there are many challenges involved, including scalability
(MMD) and computability (Wasserstein-1) issues of traditional methods for
estimating distances on measure spaces. Here, we present
humancompatible.detect, a toolkit for bias detection that addresses these
challenges. It incorporates two newly developed methods to detect and evaluate
bias: maximum subgroup discrepancy (MSD) and subsampled $\ell_\infty$
distances. It has an easy-to-use API documented with multiple examples.
humancompatible.detect is licensed under the Apache License, Version 2.0.

</details>


### [292] [Fin-Ally: Pioneering the Development of an Advanced, Commonsense-Embedded Conversational AI for Money Matters](https://arxiv.org/abs/2509.24342)
*Sarmistha Das,Priya Mathur,Ishani Sharma,Sriparna Saha,Kitsuchart Pasupa,Alka Maurya*

Main category: cs.AI

TL;DR: Fin-Solution 2.O introduces Fin-Vault dataset (1,417 multi-turn financial dialogues) and Fin-Ally model that integrates commonsense reasoning, politeness, and human-like conversation to generate professional financial advice.


<details>
  <summary>Details</summary>
Motivation: Address the gap in domain-specific datasets for financial chatbots and prevent unprofessional/flippant responses that erode user trust in FinTech advisory systems.

Method: Created Fin-Vault dataset and developed Fin-Ally model using COMET-BART-embedded commonsense context with Direct Preference Optimization (DPO) for human-aligned response generation.

Result: The approach enables language models to generate more refined, textually precise, and professionally grounded financial guidance beyond basic account management.

Conclusion: This represents a next-generation AI solution for FinTech sector that provides personalized budgeting, expense tracking, and automated financial planning through human-aligned conversational AI.

Abstract: The exponential technological breakthrough of the FinTech industry has
significantly enhanced user engagement through sophisticated advisory chatbots.
However, large-scale fine-tuning of LLMs can occasionally yield unprofessional
or flippant remarks, such as ``With that money, you're going to change the
world,'' which, though factually correct, can be contextually inappropriate and
erode user trust. The scarcity of domain-specific datasets has led previous
studies to focus on isolated components, such as reasoning-aware frameworks or
the enhancement of human-like response generation. To address this research
gap, we present Fin-Solution 2.O, an advanced solution that 1) introduces the
multi-turn financial conversational dataset, Fin-Vault, and 2) incorporates a
unified model, Fin-Ally, which integrates commonsense reasoning, politeness,
and human-like conversational dynamics. Fin-Ally is powered by
COMET-BART-embedded commonsense context and optimized with a Direct Preference
Optimization (DPO) mechanism to generate human-aligned responses. The novel
Fin-Vault dataset, consisting of 1,417 annotated multi-turn dialogues, enables
Fin-Ally to extend beyond basic account management to provide personalized
budgeting, real-time expense tracking, and automated financial planning. Our
comprehensive results demonstrate that incorporating commonsense context
enables language models to generate more refined, textually precise, and
professionally grounded financial guidance, positioning this approach as a
next-generation AI solution for the FinTech sector. Dataset and codes are
available at: https://github.com/sarmistha-D/Fin-Ally

</details>


### [293] [From Static to Dynamic: Adaptive Monte Carlo Search for Mathematical Process Supervision](https://arxiv.org/abs/2509.24351)
*Jie Ma,Shihao Qi,Rui Xing,Ziang Yin,Bifan Wei,Jun Liu,Tongliang Liu*

Main category: cs.AI

TL;DR: AMCS is an adaptive Monte Carlo search framework that improves process data generation for training Process Reward Models (PRMs) by dynamically allocating samples and adapting search strategies, resulting in superior mathematical reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for process data generation use fixed-budget sampling and inefficient search strategies, leading to poor quality training data for Process Reward Models that enhance mathematical reasoning in LLMs.

Method: Proposes Adaptive Monte Carlo Search (AMCS) with two key components: adaptive node value estimation that allocates more samples to uncertain reasoning steps, and temporally adaptive path expansion that shifts from exploration to exploitation of promising directions.

Result: Created MathSearch-200K dataset with 200K process supervision examples. Qwen2.5-Math-7B-PRM-AMCS achieved 76.2% accuracy on MATH500, outperforming all baseline PRMs. A 7B model supervised by AMCS surpassed a 72B model with weaker supervision, demonstrating strong generalization on out-of-distribution problems.

Conclusion: AMCS effectively addresses inefficiencies in process data generation through adaptive search strategies, producing high-quality training data that significantly enhances mathematical reasoning capabilities in language models while maintaining strong generalization performance.

Abstract: The quality of process data plays a key role in training a Process Reward
Model (PRM), which can enhance the complex mathematical reasoning capability of
large language models. Existing methods estimate the quality of reasoning steps
based on a fixed-budget sampling strategy and navigate a vast search space to
perform path expansion during the automated data generation process, resulting
in their inefficiency and inflexibility. To address these issues, we propose
Adaptive Monte Carlo Search (AMCS), a framework that transforms data generation
from fixed, static to adaptive, dynamic search at the level of node value
estimation and path expansion. On one hand, AMCS adaptively refines estimation
by allocating more samples to uncertain reasoning steps while using fewer
samples for those that are easier to estimate. On the other hand, it enhances
the path expansion through a Monte Carlo algorithm with a temporally adaptive
policy that begins with broad exploration and gradually shifts toward
exploiting the most promising directions. With AMCS, we construct a large-scale
dataset MathSearch-200K of about 200K process supervision examples for training
PRMs. To verify the effectiveness of our method, we conduct extensive
experiments on four mathematical reasoning benchmarks. Experimental results
show that Qwen2.5-Math-7B-PRM-AMCS achieves up to 76.2% accuracy on MATH500
with GLM-4-9B, outperforming all baseline PRMs. Notably, a 7B model supervised
by Qwen2.5-Math-7B-PRM-AMCS surpasses a 72B model with weaker supervision.
Moreover, Qwen2.5-Math-7B-PRM-AMCS maintains consistent advantages on
out-of-distribution problems, demonstrating strong generalization capability.
Our code is available at https://github.com/reml-group/AMCS.

</details>


### [294] [Plan before Solving: Problem-Aware Strategy Routing for Mathematical Reasoning with LLMs](https://arxiv.org/abs/2509.24377)
*Shihao Qi,Jie Ma,Ziang Yin,Lingling Zhang,Jian Zhang,Jun Liu,Feng Tian,Tongliang Liu*

Main category: cs.AI

TL;DR: PRISM is a novel framework that decouples mathematical reasoning into strategy planning and targeted execution stages, using adaptive routing to dynamically select reasoning strategies based on problem-specific requirements.


<details>
  <summary>Details</summary>
Motivation: Existing methods use fixed strategies that cannot adapt to problem-specific requirements and overlook the trade-off between effectiveness and efficiency in mathematical reasoning.

Method: Proposes PRISM framework with two stages: (1) Strategy planning using a lightweight Strategy Adapter trained on MathStrat dataset to get confidence distributions over four reasoning strategies; (2) Adaptive routing policy that dynamically selects reasoning approaches based on predictor confidence.

Result: PRISM consistently outperforms individual strategies and ensemble baselines across five mathematical reasoning benchmarks, achieving improvements ranging from 0.9% to 7.6% across different base models.

Conclusion: The adaptive routing approach shows strong benefits for mathematical reasoning tasks across diverse model architectures, effectively balancing effectiveness and efficiency through dynamic strategy selection.

Abstract: Existing methods usually leverage a fixed strategy, such as natural language
reasoning, code-augmented reasoning, tool-integrated reasoning, or
ensemble-based reasoning, to guide Large Language Models (LLMs) to perform
mathematical reasoning. Our analysis reveals that the single strategy cannot
adapt to problem-specific requirements and thus overlooks the trade-off between
effectiveness and efficiency. To address these issues, we propose Planning and
Routing through Instance-Specific Modeling (PRISM), a novel framework that
decouples mathematical reasoning into two stages: strategy planning and
targeted execution. Specifically, we first curate a multi-strategy preference
dataset, which we call MathStrat, capturing correctness, process quality, and
computational efficiency for each problem--strategy pair. Then, we train a
lightweight Strategy Adapter based on the dataset to obtain confidence
distributions over the mentioned four reasoning strategies. At inference time,
an adaptive routing policy dynamically tailors the reasoning approach based on
predictor confidence. It directs the model to use single-strategy execution for
high-confidence predictions, dual-strategy verification for competitive
scenarios, or comprehensive multi-strategy exploration for uncertain cases.
Extensive experiments across five mathematical reasoning benchmarks demonstrate
that PRISM consistently outperforms individual strategies and ensemble
baselines, achieving improvements ranging from 0.9% to 7.6% across different
base models. The adaptive routing approach shows particularly strong benefits
for mathematical reasoning tasks across diverse model architectures. Our code
is released at https://github.com/reml-group/PRISM.

</details>


### [295] [Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention](https://arxiv.org/abs/2509.24393)
*Yichi Zhang,Yue Ding,Jingwen Yang,Tianwei Luo,Dongbai Li,Ranjie Duan,Qiang Liu,Hang Su,Yinpeng Dong,Jun Zhu*

Main category: cs.AI

TL;DR: This paper addresses the safety issue in Large Reasoning Models' chain-of-thought reasoning, where harmful content persists even when final responses appear safe. The authors propose Intervened Preference Optimization (IPO) to align reasoning safety by substituting compliance steps with safety triggers and using preference learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the unique significance of safe reasoning in Large Reasoning Models, allowing harmful content to persist in chain-of-thought reasoning even when final responses appear safe. This undermines trustworthiness and poses potential risks if exploited by malicious users.

Method: The authors propose Intervened Preference Optimization (IPO), an alignment method that enforces safe reasoning by: 1) substituting compliance steps with safety triggers, and 2) constructing pairs for preference learning with strong signals. This approach is based on insights about safety triggers, compliance cues, and corrective interventions.

Result: Experiments on jailbreak and adversarial safety benchmarks show that IPO remarkably improves overall safety regarding both reasoning and responses, achieving over 30% relative reduction in harmfulness compared to SFT-based and RL-based baselines, while preserving excellent performance across diverse reasoning tasks.

Conclusion: The results highlight the importance of explicit alignment for reasoning safety and provide a practical path to safer Large Reasoning Models through the proposed Intervened Preference Optimization method.

Abstract: Although Large Reasoning Models (LRMs) have progressed in solving complex
problems, their chain-of-thought (CoT) reasoning often contains harmful content
that can persist even when the final responses appear safe. We show that this
issue still remains in existing methods which overlook the unique significance
of safe reasoning, undermining their trustworthiness and posing potential risks
in applications if unsafe reasoning is accessible for and exploited by
malicious users. We therefore shift our focus to aligning the safety of
reasoning itself in this paper and explore process supervision as the solution.
However, simply rewarding safe reasoning proves inadequate due to low rollout
diversity and limited training signals. To tackle this challenge, we first
delve into the characteristics of safe reasoning and uncover several critical
insights that 1) safe reasoning is often consolidated by a few critical steps
of safety triggers; 2) compliance cues strongly correlate with unsafe
continuations; and 3) corrective interventions reliably steer unsafe
trajectories towards safer traces. Motivated by these, we propose Intervened
Preference Optimization (IPO), an alignment method that enforces safe reasoning
by substituting compliance steps with safety triggers and constructing pairs
for preference learning with strong signals. Experiments on jailbreak and
adversarial safety benchmarks demonstrate that IPO remarkably improves overall
safety regarding both reasoning and responses, outperforming SFT-based and
RL-based baselines with a relative reduction of over 30% in harmfulness, while
preserving excellent performance across diverse reasoning tasks. The results
highlight the importance of explicit alignment for reasoning and provide a
practical path to safer LRMs.

</details>


### [296] [A Systematic Review of Digital Twin-Driven Predictive Maintenance in Industrial Engineering: Taxonomy, Architectural Elements, and Future Research Directions](https://arxiv.org/abs/2509.24443)
*Leila Ismail,Abdelmoneim Abdelmoti,Arkaprabha Basu,Aymen Dia Eddine Berini,Mohammad Naouss*

Main category: cs.AI

TL;DR: This paper provides a retrospective analysis of digital twin evolution in predictive maintenance for industrial engineering, examining applications, middleware, and AI algorithms from inception to current AI-enabled self-learning models.


<details>
  <summary>Details</summary>
Motivation: Traditional reactive and preventive maintenance practices are inadequate for modern industrial systems. With IoT, AI, and real-time analytics, there's an opportunity for efficient predictive maintenance to forecast failures and optimize maintenance actions, with digital twin technology playing a central role.

Method: The authors perform retrospective analysis of temporal evolution of digital twin in predictive maintenance, provide layered architecture of digital twin technology, taxonomy of technology-enabled industrial engineering applications, middleware, and AI algorithms.

Result: The paper provides insights into systems for realizing trustworthy and efficient smart digital-twin industrial engineering ecosystem, including applications, middleware, and technological requirements from digital twin's inception to AI-enabled versions.

Conclusion: The research discusses future directions for digital twin in predictive maintenance for industrial engineering, addressing the gap in comprehensive analysis of digital twin evolution and its role in modern maintenance practices.

Abstract: With the increasing complexity of industrial systems, there is a pressing
need for predictive maintenance to avoid costly downtime and disastrous
outcomes that could be life-threatening in certain domains. With the growing
popularity of the Internet of Things, Artificial Intelligence, machine
learning, and real-time big data analytics, there is a unique opportunity for
efficient predictive maintenance to forecast equipment failures for real-time
intervention and optimize maintenance actions, as traditional reactive and
preventive maintenance practices are often inadequate to meet the requirements
for the industry to provide quality-of-services of operations. Central to this
evolution is digital twin technology, an adaptive virtual replica that
continuously monitors and integrates sensor data to simulate and improve asset
performance. Despite remarkable progress in digital twin implementations, such
as considering DT in predictive maintenance for industrial engineering. This
paper aims to address this void. We perform a retrospective analysis of the
temporal evolution of the digital twin in predictive maintenance for industrial
engineering to capture the applications, middleware, and technological
requirements that led to the development of the digital twin from its inception
to the AI-enabled digital twin and its self-learning models. We provide a
layered architecture of the digital twin technology, as well as a taxonomy of
the technology-enabled industrial engineering applications systems, middleware,
and the used Artificial Intelligence algorithms. We provide insights into these
systems for the realization of a trustworthy and efficient smart digital-twin
industrial engineering ecosystem. We discuss future research directions in
digital twin for predictive maintenance in industrial engineering.

</details>


### [297] [ContextPRM: Leveraging Contextual Coherence for multi-domain Test-Time Scaling](https://arxiv.org/abs/2509.24460)
*Haotian Zhang,Liu Liu,Baosheng Yu,Jiayan Qiu,Likang Xiao,Yanwei Ren,Quan Chen,Xianglong Liu*

Main category: cs.AI

TL;DR: ContextPRM improves PRM generalization by focusing on domain-agnostic logical flow instead of domain-specific knowledge, achieving 6.5% accuracy improvement across non-mathematical domains.


<details>
  <summary>Details</summary>
Motivation: PRMs show strong performance in math but lack generalization to other domains due to domain-specific training data and knowledge-based learning patterns.

Method: Shift learning objective to model domain-agnostic logical flow by focusing on contextual coherence between CoT steps, using novel data annotation and training framework.

Result: ContextPRM achieves 6.5% average accuracy improvement over majority voting baseline across nine non-mathematical domains in MMLU-Pro, outperforming VersaPRM (2.2%) and other math-focused PRMs (0.5%).

Conclusion: Focusing on contextual coherence in logical flow enables PRMs to generalize effectively across both mathematical and non-mathematical domains.

Abstract: Process reward models (PRMs) have demonstrated significant efficacy in
enhancing the mathematical reasoning capabilities of large language models
(LLMs) by leveraging test-time scaling (TTS). However, while most PRMs exhibit
substantial gains in mathematical domains, the scarcity of domain-specific
training data and knowledge-based learning patterns limits their generalization
ability when faced with other domains. To address this limitation, we shift the
learning objective from verifying domain-specific knowledge to modeling
domain-agnostic logical flow. Centering on contextual coherence between
chain-of-thought (CoT) steps, our approach is realized through a novel data
annotation and training framework, which enhances the model's generalization
capabilities across diverse domains. For instance, our resulting model,
ContextPRM, achieves a notable 6.5% average accuracy improvement over the
majority voting baseline via weighted majority voting across nine
non-mathematical domains in MMLU-Pro, including law, history, and philosophy,
significantly surpassing the 2.2% improvement from VersaPRM and 0.5% gains from
other mathematics-focused PRMs, demonstrating consistent performance across
both mathematical and non-mathematical domains.

</details>


### [298] [Overcoming Over-Fitting in Constraint Acquisition via Query-Driven Interactive Refinement](https://arxiv.org/abs/2509.24489)
*Vasileios Balafas,Dimos Tsouros,Nikolaos Ploskas,Kostas Stergiou*

Main category: cs.AI

TL;DR: A hybrid constraint acquisition framework combining passive learning with interactive refinement to prevent over-fitting in data-limited scenarios, using probabilistic confidence scores and subset exploration.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of over-fitting in constraint acquisition, where passive methods learn spurious constraints from limited data and purely active methods are query-intensive.

Method: Integrates passive learning for initial candidate generation, query-driven interactive refinement with probabilistic confidence scores, specialized subset exploration to recover valid substructures, and final active learning for model completeness.

Result: Extensive experiments show the interactive refinement phase achieves high target model coverage and accuracy from limited examples with manageable query complexity.

Conclusion: This framework represents a substantial advancement towards robust and practical constraint acquisition in data-limited scenarios.

Abstract: Manual modeling in Constraint Programming is a substantial bottleneck, which
Constraint Acquisition (CA) aims to automate. However, passive CA methods are
prone to over-fitting, often learning models that include spurious global
constraints when trained on limited data, while purely active methods can be
query-intensive. We introduce a hybrid CA framework specifically designed to
address the challenge of over-fitting in CA. Our approach integrates passive
learning for initial candidate generation, a query-driven interactive
refinement phase that utilizes probabilistic confidence scores (initialized by
machine learning priors) to systematically identify over-fitted constraints,
and a specialized subset exploration mechanism to recover valid substructures
from rejected candidates. A final active learning phase ensures model
completeness. Extensive experiments on diverse benchmarks demonstrate that our
interactive refinement phase is crucial for achieving high target model
coverage and overall model accuracy from limited examples, doing so with
manageable query complexity. This framework represents a substantial
advancement towards robust and practical constraint acquisition in data-limited
scenarios.

</details>


### [299] [Neuroplasticity-inspired dynamic ANNs for multi-task demand forecasting](https://arxiv.org/abs/2509.24495)
*Mateusz Żarski,Sławomir Nowaczyk*

Main category: cs.AI

TL;DR: NMT-Net is a neuroplastic multi-task network that dynamically adapts its structure during training for multi-task demand forecasting, achieving better performance than traditional methods.


<details>
  <summary>Details</summary>
Motivation: To create a more adaptable and scalable solution for multi-task demand forecasting by enabling structural adaptability during training, inspired by biological neuroplasticity.

Method: Uses similarity-based task identification and selective training of candidate ANN heads, with dynamic network adaptation triggered by new tasks during training.

Result: Achieved lower RMSE and standard deviation compared to traditional baselines and state-of-the-art multi-task learning methods on three real-world datasets.

Conclusion: NMT-Net provides a scalable, adaptable solution for multi-task and continual learning in time series prediction with demonstrated superior performance.

Abstract: This paper introduces a novel approach to Dynamic Artificial Neural Networks
(D-ANNs) for multi-task demand forecasting called Neuroplastic Multi-Task
Network (NMT-Net). Unlike conventional methods focusing on inference-time
dynamics or computational efficiency, our proposed method enables structural
adaptability of the computational graph during training, inspired by
neuroplasticity as seen in biological systems. Each new task triggers a dynamic
network adaptation, including similarity-based task identification and
selective training of candidate ANN heads, which are then assessed and
integrated into the model based on their performance. We evaluated our
framework using three real-world multi-task demand forecasting datasets from
Kaggle. We demonstrated its superior performance and consistency, achieving
lower RMSE and standard deviation compared to traditional baselines and
state-of-the-art multi-task learning methods. NMT-Net offers a scalable,
adaptable solution for multi-task and continual learning in time series
prediction. The complete code for NMT-Net is available from our GitHub
repository.

</details>


### [300] [Experience-guided reflective co-evolution of prompts and heuristics for automatic algorithm design](https://arxiv.org/abs/2509.24509)
*Yihong Liu,Junyi Li,Wayne Xin Zhao,Hongyu Lu,Ji-Rong Wen*

Main category: cs.AI

TL;DR: EvoPH is a novel framework that co-evolves prompts and heuristic algorithms using LLMs, integrating island migration and elite selection to avoid local optima in automatic algorithm design for combinatorial optimization problems.


<details>
  <summary>Details</summary>
Motivation: Traditional heuristic algorithms require extensive domain expertise and implementation effort, while existing LLM-based approaches risk stagnating in local optima during automatic heuristics design.

Method: Proposes Experience-Guided Reflective Co-Evolution of Prompt and Heuristics (EvoPH) framework that integrates island migration model with elite selection algorithm to simulate diverse heuristics populations, co-evolving prompts and heuristic algorithms guided by performance feedback.

Result: EvoPH achieves the lowest relative error against optimal solutions on Traveling Salesman Problem and Bin Packing Problem datasets compared to other methods.

Conclusion: EvoPH advances the field of automatic algorithm design with LLMs by effectively avoiding local optima and generating high-quality heuristics for combinatorial optimization problems.

Abstract: Combinatorial optimization problems are traditionally tackled with
handcrafted heuristic algorithms, which demand extensive domain expertise and
significant implementation effort. Recent progress has highlighted the
potential of automatic heuristics design powered by large language models
(LLMs), enabling the automatic generation and refinement of heuristics. These
approaches typically maintain a population of heuristics and employ LLMs as
mutation operators to evolve them across generations. While effective, such
methods often risk stagnating in local optima. To address this issue, we
propose the Experience-Guided Reflective Co-Evolution of Prompt and Heuristics
(EvoPH) for automatic algorithm design, a novel framework that integrates the
island migration model with the elites selection algorithm to simulate diverse
heuristics populations. In EvoPH, prompts are co-evolved with heuristic
algorithms, guided by performance feedback. We evaluate our framework on two
problems, i.e., Traveling Salesman Problem and Bin Packing Problem.
Experimental results demonstrate that EvoPH achieves the lowest relative error
against optimal solutions across both datasets, advancing the field of
automatic algorithm design with LLMs.

</details>


### [301] [Training Agents Inside of Scalable World Models](https://arxiv.org/abs/2509.24527)
*Danijar Hafner,Wilson Yan,Timothy Lillicrap*

Main category: cs.AI

TL;DR: Dreamer 4 is a scalable agent that learns to solve control tasks through reinforcement learning in an accurate world model, achieving the first diamond collection in Minecraft from purely offline data without environment interaction.


<details>
  <summary>Details</summary>
Motivation: Previous world models struggled with accurately predicting object interactions in complex environments. The work aims to develop intelligent agents that can learn from imagination training rather than direct environment interaction, which is important for practical applications like robotics where real-world interaction can be unsafe and slow.

Method: Dreamer 4 uses a fast and accurate world model with a shortcut forcing objective and efficient transformer architecture for real-time interactive inference. It learns general action conditioning from small amounts of data and extracts most knowledge from diverse unlabeled videos. The agent learns behaviors through reinforcement learning inside the world model.

Result: In Minecraft, the world model accurately predicts object interactions and game mechanics, outperforming previous world models by a large margin. Dreamer 4 becomes the first agent to obtain diamonds in Minecraft purely from offline data, successfully choosing sequences of over 20,000 mouse and keyboard actions from raw pixels.

Conclusion: The work provides a scalable recipe for imagination training and marks a step towards intelligent agents that can learn complex behaviors without direct environment interaction, demonstrating the potential for applications where real-world training is impractical or unsafe.

Abstract: World models learn general knowledge from videos and simulate experience for
training behaviors in imagination, offering a path towards intelligent agents.
However, previous world models have been unable to accurately predict object
interactions in complex environments. We introduce Dreamer 4, a scalable agent
that learns to solve control tasks by reinforcement learning inside of a fast
and accurate world model. In the complex video game Minecraft, the world model
accurately predicts object interactions and game mechanics, outperforming
previous world models by a large margin. The world model achieves real-time
interactive inference on a single GPU through a shortcut forcing objective and
an efficient transformer architecture. Moreover, the world model learns general
action conditioning from only a small amount of data, allowing it to extract
the majority of its knowledge from diverse unlabeled videos. We propose the
challenge of obtaining diamonds in Minecraft from only offline data, aligning
with practical applications such as robotics where learning from environment
interaction can be unsafe and slow. This task requires choosing sequences of
over 20,000 mouse and keyboard actions from raw pixels. By learning behaviors
in imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft
purely from offline data, without environment interaction. Our work provides a
scalable recipe for imagination training, marking a step towards intelligent
agents.

</details>


### [302] [BPMN Assistant: An LLM-Based Approach to Business Process Modeling](https://arxiv.org/abs/2509.24592)
*Josip Tomo Licardo,Nikola Tankovic,Darko Etinger*

Main category: cs.AI

TL;DR: BPMN Assistant is a tool that uses LLMs for natural language-based creation and editing of BPMN diagrams, using a specialized JSON representation instead of XML for better accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: To enable more intuitive natural language-based creation and editing of BPMN diagrams using LLMs, while addressing the limitations of direct XML handling for process modifications.

Method: Introduces a specialized JSON-based representation as a structured alternative to XML, evaluates generation quality using Graph Edit Distance (GED) and Relative Graph Edit Distance (RGED), and assesses editing performance with a binary success metric.

Result: JSON and XML achieve similar similarity scores in generation, but JSON offers greater reliability, faster processing, and significantly higher editing success rates.

Conclusion: The JSON-based approach provides better reliability and editing performance compared to XML, with discussions on trade-offs, limitations, and future improvements. The tool is publicly available.

Abstract: This paper presents BPMN Assistant, a tool that leverages Large Language
Models (LLMs) for natural language-based creation and editing of BPMN diagrams.
A specialized JSON-based representation is introduced as a structured
alternative to the direct handling of XML to enhance the accuracy of process
modifications. Process generation quality is evaluated using Graph Edit
Distance (GED) and Relative Graph Edit Distance (RGED), while editing
performance is evaluated with a binary success metric. Results show that JSON
and XML achieve similar similarity scores in generation, but JSON offers
greater reliability, faster processing, and significantly higher editing
success rates. We discuss key trade-offs, limitations, and future improvements.
The implementation is available at https://github.com/jtlicardo/bpmn-assistant.

</details>


### [303] [LTL$_f$ Learning Meets Boolean Set Cover](https://arxiv.org/abs/2509.24616)
*Gabriel Bathie,Nathanaël Fijalkow,Théo Matricon,Baptiste Mouillon,Pierre Vandenhove*

Main category: cs.AI

TL;DR: Bolt is a new CPU tool for learning Linear Temporal Logic (LTLf) formulas from finite traces, achieving 100x faster learning speeds on 70% of benchmarks while producing smaller or equal formulas in 98% of cases.


<details>
  <summary>Details</summary>
Motivation: Learning LTLf formulas from finite traces is fundamental for applications in AI, software engineering, formal methods, cyber-physical systems, and robotics, requiring more efficient and compact formula learning methods.

Method: The approach leverages Boolean Set Cover as a subroutine to combine existing formulas using Boolean connectives, offering a novel trade-off between efficiency and formula size.

Result: Bolt improves over state-of-the-art by learning formulas more than 100x faster over 70% of benchmarks, with smaller or equal formulas in 98% of cases.

Conclusion: The Boolean Set Cover component enables a novel efficiency-formula size trade-off, making Bolt a significant advancement in LTLf formula learning from finite traces.

Abstract: Learning formulas in Linear Temporal Logic (LTLf) from finite traces is a
fundamental research problem which has found applications in artificial
intelligence, software engineering, programming languages, formal methods,
control of cyber-physical systems, and robotics. We implement a new CPU tool
called Bolt improving over the state of the art by learning formulas more than
100x faster over 70% of the benchmarks, with smaller or equal formulas in 98%
of the cases. Our key insight is to leverage a problem called Boolean Set Cover
as a subroutine to combine existing formulas using Boolean connectives. Thanks
to the Boolean Set Cover component, our approach offers a novel trade-off
between efficiency and formula size.

</details>


### [304] ["Stop replacing salt with sugar!'': Towards Intuitive Human-Agent Teaching](https://arxiv.org/abs/2509.24651)
*Nikolaos Kondylidis,Andrea Rafanelli,Ilaria Tiddi,Annette ten Teije,Frank van Harmelen*

Main category: cs.AI

TL;DR: Proposes human-agent teaching architecture for few-shot learning of subjective tasks using demonstrations, applied to ingredient substitution with strategic example selection and external knowledge.


<details>
  <summary>Details</summary>
Motivation: To enable AI systems to learn subjective tasks from few examples like humans do, addressing data scarcity in subjective domains.

Method: Human-agent teaching architecture with incremental learning from demonstrations, leveraging domain knowledge and strategic example selection using Recipe1MSubs dataset.

Result: Agent achieves half task performance with only 100 examples vs 50k full training set, showing efficient generalization through strategic ordering and external knowledge.

Conclusion: Strategic example selection combined with external symbolic knowledge enables efficient few-shot learning for subjective tasks, replicating human-like learning capabilities.

Abstract: Humans quickly learn new concepts from a small number of examples.
Replicating this capacity with Artificial Intelligence (AI) systems has proven
to be challenging. When it comes to learning subjective tasks-where there is an
evident scarcity of data-this capacity needs to be recreated. In this work, we
propose an intuitive human-agent teaching architecture in which the human can
teach an agent how to perform a task by providing demonstrations, i.e.,
examples. To have an intuitive interaction, we argue that the agent should be
able to learn incrementally from a few single examples. To allow for this, our
objective is to broaden the agent's task understanding using domain knowledge.
Then, using a learning method to enable the agent to learn efficiently from a
limited number of examples. Finally, to optimize how human can select the most
representative and less redundant examples to provide the agent with. We apply
our proposed method to the subjective task of ingredient substitution, where
the agent needs to learn how to substitute ingredients in recipes based on
human examples. We replicate human input using the Recipe1MSubs dataset. In our
experiments, the agent achieves half its task performance after only 100
examples are provided, compared to the complete training set of 50k examples.
We show that by providing examples in strategic order along with a learning
method that leverages external symbolic knowledge, the agent can generalize
more efficiently.

</details>


### [305] [Successful Misunderstandings: Learning to Coordinate Without Being Understood](https://arxiv.org/abs/2509.24660)
*Nikolaos Kondylidis,Anil Yaman,Frank van Harmelen,Erman Acar,Annette ten Teije*

Main category: cs.AI

TL;DR: The paper investigates whether successful coordination through communication necessarily implies mutual understanding. In a signaling game where agents develop new vocabularies without shared observations, they find that populations can achieve optimal coordination while maintaining different interpretations of signals - called "successful misunderstandings" - which prevent coordination with new partners.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that successful coordination through communication implies mutual understanding, particularly in contexts where agents (like humans and AI) perceive environments differently and lack shared observations of signal usage.

Method: Used a signaling game where agents develop new vocabularies without common observations besides communication signals and interaction outcomes. Agents refine interpretations while not observing how others use signals.

Result: Populations converged to optimal coordination levels, but sometimes with "successful misunderstandings" - agents coordinated effectively while interpreting signals differently. These misunderstandings prevented coordination with new partners. At least three interacting agents were needed to ensure shared interpretations emerge.

Conclusion: Successful coordination doesn't guarantee mutual understanding. "Successful misunderstandings" can persist undetected and prevent coordination with new partners. Shared interpretations emerge only when at least three agents interact, compensating for lack of shared signal usage observations.

Abstract: The main approach to evaluating communication is by assessing how well it
facilitates coordination. If two or more individuals can coordinate through
communication, it is generally assumed that they understand one another. We
investigate this assumption in a signaling game where individuals develop a new
vocabulary of signals to coordinate successfully. In our game, the individuals
do not have common observations besides the communication signal and outcome of
the interaction, i.e. received reward. This setting is used as a proxy to study
communication emergence in populations of agents that perceive their
environment very differently, e.g. hybrid populations that include humans and
artificial agents. Agents develop signals, use them, and refine interpretations
while not observing how other agents are using them. While populations always
converge to optimal levels of coordination, in some cases, interacting agents
interpret and use signals differently, converging to what we call successful
misunderstandings. However, agents of population that coordinate using
misaligned interpretations, are unable to establish successful coordination
with new interaction partners. Not leading to coordination failure immediately,
successful misunderstandings are difficult to spot and repair. Having at least
three agents that all interact with each other are the two minimum conditions
to ensure the emergence of shared interpretations. Under these conditions, the
agent population exhibits this emergent property of compensating for the lack
of shared observations of signal use, ensuring the emergence of shared
interpretations.

</details>


### [306] [On the Self-awareness of Large Reasoning Models' Capability Boundaries](https://arxiv.org/abs/2509.24711)
*Qingjie Zhang,Yujia Fu,Yang Wang,Liu Yan,Tao Wei,Ke Xu,Minlie Huang,Han Qiu*

Main category: cs.AI

TL;DR: Large Reasoning Models (LRMs) can identify their capability boundaries through reasoning confidence patterns and hidden states, enabling boundary-aware strategies that cut unproductive reasoning by up to 93.6% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LRMs waste computation on unsolvable problems by engaging in unproductive reasoning until context limit, highlighting the need for self-awareness of capability boundaries.

Method: Two monitoring approaches: 1) For black-box models: analyze reasoning expression confidence trajectories; 2) For white-box models: examine hidden states of last input token for linear separability of solvable/unsolvable problems.

Result: Boundary-aware strategies reduce token usage by 62.7-93.6% without sacrificing accuracy, significantly improving reliability and efficiency.

Conclusion: LRMs possess inherent capability boundary awareness that can be leveraged through simple monitoring strategies to prevent unproductive reasoning and enhance model efficiency.

Abstract: Large Reasoning Models (LRMs) have shown impressive performance on complex
reasoning tasks such as mathematics, yet they also display misbehaviors that
expose their limitations. In particular, when faced with hard questions, LRMs
often engage in unproductive reasoning until context limit, producing wrong
answers while wasting substantial computation. This phenomenon reflects a
fundamental issue: current answering paradigms overlook the relationship
between questions and LRMs' capability boundaries. In this paper, we
investigate whether LRMs possess self-awareness of capability boundaries. We
begin by an observation that LRMs may know what they cannot solve through
expressed reasoning confidence. For black-box models, we find that reasoning
expressions reveal boundary signals, with accelerated growing confidence
trajectory for solvable problems but convergent uncertainty trajectory for
unsolvable ones. For white-box models, we show that hidden states of the last
input token encode boundary information, with solvable and unsolvable problems
linearly separable even before reasoning begins. Building on these findings, we
propose two simple yet effective optimization strategies: reasoning expression
monitoring and hidden states monitoring. Experiments demonstrate that these
boundary-aware strategies enable LRMs to avoid unproductive reasoning without
sacrificing accuracy, significantly improving reliability and efficiency by
cutting token usage up to 62.7 - 93.6%.

</details>


### [307] [Spatial-Functional awareness Transformer-based graph archetype contrastive learning for Decoding Visual Neural Representations from EEG](https://arxiv.org/abs/2509.24761)
*Yueming Sun,Long Yang*

Main category: cs.AI

TL;DR: Proposed SFTG framework using EEG Graph Transformer and Graph Archetype Contrastive Learning to improve EEG-based visual decoding by addressing spatial-temporal dynamics and intra-subject variability.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in decoding visual neural representations from EEG signals, which are high-dimensional, noisy, and non-Euclidean, with high intra-subject variability.

Method: SFTG framework with EEG Graph Transformer (EGT) for spatial-temporal encoding and Graph Archetype Contrastive Learning (GAC) for subject-specific EEG graph archetypes to improve feature consistency.

Result: Significantly outperforms prior state-of-the-art EEG decoding methods on Things-EEG dataset in both subject-dependent and subject-independent evaluations.

Conclusion: Integration of graph-based learning with contrastive objectives shows transformative potential for enhancing EEG-based brain decoding, enabling more generalizable and robust neural representations.

Abstract: Decoding visual neural representations from Electroencephalography (EEG)
signals remains a formidable challenge due to their high-dimensional, noisy,
and non-Euclidean nature. In this work, we propose a Spatial-Functional
Awareness Transformer-based Graph Archetype Contrastive Learning (SFTG)
framework to enhance EEG-based visual decoding. Specifically, we introduce the
EEG Graph Transformer (EGT), a novel graph-based neural architecture that
simultaneously encodes spatial brain connectivity and temporal neural dynamics.
To mitigate high intra-subject variability, we propose Graph Archetype
Contrastive Learning (GAC), which learns subject-specific EEG graph archetypes
to improve feature consistency and class separability. Furthermore, we conduct
comprehensive subject-dependent and subject-independent evaluations on the
Things-EEG dataset, demonstrating that our approach significantly outperforms
prior state-of-the-art EEG decoding methods.The results underscore the
transformative potential of integrating graph-based learning with contrastive
objectives to enhance EEG-based brain decoding, paving the way for more
generalizable and robust neural representations.

</details>


### [308] [From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent for LLM Logical Reasoning](https://arxiv.org/abs/2509.24765)
*Yunyao Zhang,Xinglang Zhang,Junxi Sheng,Wenbing Li,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.AI

TL;DR: LogicAgent is a semiotic-square-guided framework that addresses both logical and semantic complexity in LLM reasoning, achieving SOTA performance on the new RepublicQA benchmark and generalizing well to other logical reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the interplay between logical complexity and semantic complexity, struggling with abstract propositions, ambiguous contexts, and conflicting stances that are central to human reasoning.

Method: LogicAgent performs multi-perspective deduction in first-order logic while mitigating vacuous reasoning through existential import checks with a three-valued decision scheme (True, False, Uncertain) for boundary cases.

Result: LogicAgent achieves state-of-the-art performance on RepublicQA with 6.25% average gain over baselines, and generalizes effectively to other benchmarks (ProntoQA, ProofWriter, FOLIO, ProverQA) with additional 7.05% average gain.

Conclusion: The semiotic-grounded multi-perspective reasoning approach effectively boosts LLMs' logical performance, demonstrating strong effectiveness in handling complex logical reasoning scenarios.

Abstract: Logical reasoning is a fundamental capability of large language models
(LLMs). However, existing studies largely overlook the interplay between
logical complexity and semantic complexity, resulting in methods that struggle
to address challenging scenarios involving abstract propositions, ambiguous
contexts, and conflicting stances, which are central to human reasoning. For
this gap, we propose LogicAgent, a semiotic-square-guided framework designed to
jointly address logical complexity and semantic complexity. LogicAgent
explicitly performs multi-perspective deduction in first-order logic (FOL),
while mitigating vacuous reasoning through existential import checks that
incorporate a three-valued decision scheme (True, False, Uncertain) to handle
boundary cases more faithfully. Furthermore, to overcome the semantic
simplicity and low logical complexity of existing datasets, we introduce
RepublicQA, a benchmark that reaches college-level difficulty (FKGL = 11.94)
and exhibits substantially greater lexical and structural diversity than prior
benchmarks. RepublicQA is grounded in philosophical concepts, featuring
abstract propositions and systematically organized contrary and contradictory
relations, making it the most semantically rich resource for evaluating logical
reasoning. Experiments demonstrate that LogicAgent achieves state-of-the-art
performance on RepublicQA, with a 6.25% average gain over strong baselines, and
generalizes effectively to mainstream logical reasoning benchmarks including
ProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05%
average gain. These results highlight the strong effectiveness of our
semiotic-grounded multi-perspective reasoning in boosting LLMs' logical
performance.

</details>


### [309] [TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models](https://arxiv.org/abs/2509.24803)
*Tong Guan,Zijie Meng,Dianqi Li,Shiyu Wang,Chao-Han Huck Yang,Qingsong Wen,Zuozhu Liu,Sabato Marco Siniscalchi,Ming Jin,Shirui Pan*

Main category: cs.AI

TL;DR: TSR-Suite introduces four atomic tasks for time series reasoning across perception, extrapolation, and decision-making capabilities, with TimeOmni-1 model showing strong generalization and outperforming GPT-4.1 in causality discovery and forecasting.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal time series datasets lack genuine reasoning tasks and high-quality data, limiting progress in practical time series reasoning models.

Method: Introduces TSR-Suite with four atomic tasks spanning perception, extrapolation, and decision-making, and TimeOmni-1 model trained with multiple stages, reward functions, and optimizations.

Result: TimeOmni-1 achieves 64.0% causality discovery accuracy (vs 35.9% for GPT-4.1) and improves valid response rate by over 6% on event-aware forecasting compared to GPT-4.1.

Conclusion: TSR-Suite enables comprehensive evaluation and training of time series reasoning models, with TimeOmni-1 demonstrating strong out-of-distribution generalization across reasoning tasks.

Abstract: Recent advances in multimodal time series learning underscore a paradigm
shift from analytics centered on basic patterns toward advanced time series
understanding and reasoning. However, existing multimodal time series datasets
mostly remain at the level of surface alignment and question answering, without
reaching the depth of genuine reasoning. The absence of well-defined tasks that
genuinely require time series reasoning, along with the scarcity of
high-quality data, has limited progress in building practical time series
reasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite
(TSR-Suite), which formalizes four atomic tasks that span three fundamental
capabilities for reasoning with time series: (1) perception, acquired through
scenario understanding and causality discovery; (2) extrapolation, realized via
event-aware forecasting; and (3) decision-making, developed through
deliberation over perception and extrapolation. TSR-Suite is the first
comprehensive time series reasoning suite that supports not only thorough
evaluation but also the data pipeline and training of TSRMs. It contains more
than 23K samples, of which 2.3K are carefully curated through a human-guided
hierarchical annotation process. Building on this foundation, we introduce
TimeOmni-1, the first unified reasoning model designed to address diverse
real-world problems demanding time series reasoning. The model is trained in
multiple stages, integrating a mixture of task scenarios, novel reward
functions, and tailored optimizations. Experiments show that TimeOmni-1
delivers strong out-of-distribution generalization across all tasks and
achieves a high rate of valid responses. It significantly improves causality
discovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response
rate by over 6% compared to GPT-4.1 on the event-aware forecasting task.

</details>


### [310] [Query Circuits: Explaining How Language Models Answer User Prompts](https://arxiv.org/abs/2509.24808)
*Tung-Yu Wu,Fazl Barez*

Main category: cs.AI

TL;DR: Query circuits are introduced as a method to trace information flow in language models for specific input-output mappings, providing more faithful explanations than surrogate-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods only uncover global capability circuits but fail to explain why models produce specific outputs for particular inputs, creating a need for local, input-level explanations.

Method: Developed query circuits that directly trace information flow within models, introduced Normalized Deviation Faithfulness (NDF) metric for evaluation, and used sampling-based methods to identify sparse circuits.

Result: Found extremely sparse query circuits covering only 1.3% of model connections can recover about 60% of performance on MMLU questions, demonstrating effectiveness across multiple benchmarks.

Conclusion: Query circuits provide a step towards faithful, scalable explanations of how language models process individual inputs, offering more computationally accessible and accurate explanations than existing methods.

Abstract: Explaining why a language model produces a particular output requires local,
input-level explanations. Existing methods uncover global capability circuits
(e.g., indirect object identification), but not why the model answers a
specific input query in a particular way. We introduce query circuits, which
directly trace the information flow inside a model that maps a specific input
to the output. Unlike surrogate-based approaches (e.g., sparse autoencoders),
query circuits are identified within the model itself, resulting in more
faithful and computationally accessible explanations. To make query circuits
practical, we address two challenges. First, we introduce Normalized Deviation
Faithfulness (NDF), a robust metric to evaluate how well a discovered circuit
recovers the model's decision for a specific input, and is broadly applicable
to circuit discovery beyond our setting. Second, we develop sampling-based
methods to efficiently identify circuits that are sparse yet faithfully
describe the model's behavior. Across benchmarks (IOI, arithmetic, MMLU, and
ARC), we find that there exist extremely sparse query circuits within the model
that can recover much of its performance on single queries. For example, a
circuit covering only 1.3% of model connections can recover about 60% of
performance on an MMLU questions. Overall, query circuits provide a step
towards faithful, scalable explanations of how language models process
individual inputs.

</details>


### [311] [Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity](https://arxiv.org/abs/2509.24836)
*Zhen Bi,Zhenlin Hu,Jinnan Yang,Mingyang Chen,Cheng Deng,Yida Xue,Zeyu Yang,Qing Shen,Zhenfang Liu,Kang Zhao,Ningyu Zhang,Jungang Lou*

Main category: cs.AI

TL;DR: This paper introduces Data Reasoning Intensity (DRI) to measure the latent logical reasoning complexity in training data, and proposes a re-cognizing optimization strategy to enhance LLM reasoning performance by focusing on reasoning complexity rather than data volume.


<details>
  <summary>Details</summary>
Motivation: Current LLM training approaches focus on data format transformation but neglect internal reasoning complexity, leaving reasoning potential under-utilized. The authors believe LLM reasoning performance is constrained by both data potential and model cognitive capacity.

Method: Introduce Data Reasoning Intensity (DRI) metric to quantify logical reasoning complexity by decomposing and aggregating logical structures. Propose a re-cognizing optimization strategy that systematically enhances logical reasoning intensity of training data to better align with LLM's reasoning boundary.

Result: Extensive experiments show significant improvements in performance and generalization over data-centric strategies. The method is also validated under reinforcement learning framework.

Conclusion: Prioritizing reasoning complexity in data rather than sheer scale or superficial form is essential to realizing LLMs' full cognitive potential.

Abstract: Recent advances in large language models (LLMs) highlight the importance of
training data structure and quality in shaping reasoning behavior. However,
most existing approaches focus on transforming data formats while neglecting
the internal reasoning complexity of training samples, leaving the reasoning
potential of data under-explored and underutilized. In this work, we posit that
LLM logical reasoning performance is jointly constrained by the potential of
the training data and the cognitive capacity of the model. To make this
relationship measurable, we introduce Data Reasoning Intensity (DRI), a novel
metric that quantifies the latent logical reasoning complexity of samples by
decomposing and aggregating their logical structures. This allows us to analyze
how well current LLMs utilize logical reasoning signals and identify
performance gaps relative to data potential. Based on this insight, we
introduce a re-cognizing optimization strategy that systematically enhances the
logical reasoning intensity of training data.Rather than increasing data
volume, our method re-optimizes existing samples to better align with the LLM's
logical reasoning boundary. Extensive experiments show that our approach
significantly improves performance and generalization over data-centric
strategies. We further validate our method under a reinforcement learning
framework. Our results indicate that prioritizing reasoning complexity in data
rather than sheer scale or superficial form is essential to realizing LLMs'
full cognitive potential.

</details>


### [312] [PhysicsMinions: Winning Gold Medals in the Latest Physics Olympiads with a Coevolutionary Multimodal Multi-Agent System](https://arxiv.org/abs/2509.24855)
*Fangchen Yu,Junchi Yao,Ziyi Wang,Haiyuan Wan,Youling Huang,Bo Zhang,Shuyue Hu,Dongzhan Zhou,Ning Ding,Ganqu Cui,Lei Bai,Wanli Ouyang,Peng Ye*

Main category: cs.AI

TL;DR: PhysicsMinions is a coevolutionary multi-agent system that achieves state-of-the-art performance on Physics Olympiad problems through synergistic collaboration between visual, logic, and review studios, enabling open-source models to reach gold-medal-level performance for the first time.


<details>
  <summary>Details</summary>
Motivation: Physics Olympiads represent the pinnacle of physical intelligence testing but remain largely unexplored in AI research, with existing single-model approaches failing to achieve gold-medal-level performance.

Method: A coevolutionary multi-agent system with three studios: Visual Studio for diagram interpretation, Logic Studio for solution formulation, and Review Studio for dual-stage verification, operating through iterative refinement loops.

Result: Achieves historic breakthroughs: elevates open-source models from 1-2 to 6 gold medals across 7 Olympiads, first-ever open-source gold medal in IPhO, and ranks 4th of 406 contestants with 26.8/30 points.

Conclusion: PhysicsMinions provides a generalizable framework for Olympiad-level problem solving that can potentially extend across disciplines, demonstrating the power of multi-agent collaboration over single-model approaches.

Abstract: Physics is central to understanding and shaping the real world, and the
ability to solve physics problems is a key indicator of real-world physical
intelligence. Physics Olympiads, renowned as the crown of competitive physics,
provide a rigorous testbed requiring complex reasoning and deep multimodal
understanding, yet they remain largely underexplored in AI research. Existing
approaches are predominantly single-model based, and open-source MLLMs rarely
reach gold-medal-level performance. To address this gap, we propose
PhysicsMinions, a coevolutionary multi-agent system for Physics Olympiad. Its
architecture features three synergistic studios: a Visual Studio to interpret
diagrams, a Logic Studio to formulate solutions, and a Review Studio to perform
dual-stage verification. The system coevolves through an iterative refinement
loop where feedback from the Review Studio continuously guides the Logic
Studio, enabling the system to self-correct and converge towards the ground
truth. Evaluated on the HiPhO benchmark spanning 7 latest physics Olympiads,
PhysicsMinions delivers three major breakthroughs: (i) Strong generalization:
it consistently improves both open-source and closed-source models of different
sizes, delivering clear benefits over their single-model baselines; (ii)
Historic breakthroughs: it elevates open-source models from only 1-2 to 6 gold
medals across 7 Olympiads, achieving the first-ever open-source gold medal in
the latest International Physics Olympiad (IPhO) under the average-score
metric; and (iii) Scaling to human expert: it further advances the open-source
Pass@32 score to 26.8/30 points on the latest IPhO, ranking 4th of 406
contestants and far surpassing the top single-model score of 22.7 (ranked
22nd). Generally, PhysicsMinions offers a generalizable framework for
Olympiad-level problem solving, with the potential to extend across
disciplines.

</details>


### [313] [The Emergence of Social Science of Large Language Models](https://arxiv.org/abs/2509.24877)
*Xiao Jia,Zhanzhan Zhao*

Main category: cs.AI

TL;DR: This paper presents a systematic review of 270 studies on the social science of large language models (LLMs), using computational methods to develop a taxonomy that organizes research into three domains: LLM as Social Minds, LLM Societies, and LLM-Human Interactions.


<details>
  <summary>Details</summary>
Motivation: To address the fragmented nature of research on the social science of LLMs and provide a systematic framework for organizing and understanding how these models evoke mind attributions, interact with each other, and transform human activity and institutions.

Method: Conducted a systematic review of 270 studies using text embeddings, unsupervised clustering, and topic modeling to build a computational taxonomy of the field.

Result: Identified three main research domains: 1) LLM as Social Minds (examining cognition, morality, bias attributions), 2) LLM Societies (multi-agent interactions and coordination), and 3) LLM-Human Interactions (impact on tasks, learning, work, and governance).

Conclusion: The taxonomy provides a reproducible map of the fragmented field, clarifies evidentiary standards across different levels of analysis, and highlights opportunities for cumulative progress in the social science of artificial intelligence.

Abstract: The social science of large language models (LLMs) examines how these systems
evoke mind attributions, interact with one another, and transform human
activity and institutions. We conducted a systematic review of 270 studies,
combining text embeddings, unsupervised clustering and topic modeling to build
a computational taxonomy. Three domains emerge organically across the reviewed
literature. LLM as Social Minds examines whether and when models display
behaviors that elicit attributions of cognition, morality and bias, while
addressing challenges such as test leakage and surface cues. LLM Societies
examines multi-agent settings where interaction protocols, architectures and
mechanism design shape coordination, norms, institutions and collective
epistemic processes. LLM-Human Interactions examines how LLMs reshape tasks,
learning, trust, work and governance, and how risks arise at the human-AI
interface. This taxonomy provides a reproducible map of a fragmented field,
clarifies evidentiary standards across levels of analysis, and highlights
opportunities for cumulative progress in the social science of artificial
intelligence.

</details>


### [314] [RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark](https://arxiv.org/abs/2509.24897)
*Yang Shi,Yuhao Dong,Yue Ding,Yuran Wang,Xuanyu Zhu,Sheng Zhou,Wenting Liu,Haochen Tian,Rundong Wang,Huanqian Wang,Zuyan Liu,Bohan Zeng,Ruizhe Chen,Qixun Wang,Zhuoran Zhang,Xinlong Chen,Chengzhuo Tong,Bozhou Li,Chaoyou Fu,Qiang Liu,Haotian Wang,Wenjing Yang,Yuanxing Zhang,Pengfei Wan,Yi-Fan Zhang,Ziwei Liu*

Main category: cs.AI

TL;DR: RealUnify is a benchmark for evaluating bidirectional capability synergy in unified multimodal models, testing whether understanding enhances generation and vice versa, revealing current models struggle with effective integration despite architectural unification.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks only assess visual understanding and generation in isolation, failing to determine if unified models can leverage bidirectional synergy between these capabilities for mutual enhancement.

Method: Created RealUnify benchmark with 1,000 human-annotated instances across 10 categories and 32 subtasks, featuring dual-evaluation protocol combining end-to-end assessment with diagnostic stepwise evaluation to isolate understanding and generation phases.

Result: Evaluation of 12 unified models and 6 specialized baselines shows current unified models struggle to achieve effective capability synergy, indicating architectural unification alone is insufficient.

Conclusion: New training strategies and inductive biases are needed to fully unlock the potential of unified multimodal modeling beyond mere architectural unification.

Abstract: The integration of visual understanding and generation into unified
multimodal models represents a significant stride toward general-purpose AI.
However, a fundamental question remains unanswered by existing benchmarks: does
this architectural unification actually enable synergetic interaction between
the constituent capabilities? Existing evaluation paradigms, which primarily
assess understanding and generation in isolation, are insufficient for
determining whether a unified model can leverage its understanding to enhance
its generation, or use generative simulation to facilitate deeper
comprehension. To address this critical gap, we introduce RealUnify, a
benchmark specifically designed to evaluate bidirectional capability synergy.
RealUnify comprises 1,000 meticulously human-annotated instances spanning 10
categories and 32 subtasks. It is structured around two core axes: 1)
Understanding Enhances Generation, which requires reasoning (e.g., commonsense,
logic) to guide image generation, and 2) Generation Enhances Understanding,
which necessitates mental simulation or reconstruction (e.g., of transformed or
disordered visual inputs) to solve reasoning tasks. A key contribution is our
dual-evaluation protocol, which combines direct end-to-end assessment with a
diagnostic stepwise evaluation that decomposes tasks into distinct
understanding and generation phases. This protocol allows us to precisely
discern whether performance bottlenecks stem from deficiencies in core
abilities or from a failure to integrate them. Through large-scale evaluations
of 12 leading unified models and 6 specialized baselines, we find that current
unified models still struggle to achieve effective synergy, indicating that
architectural unification alone is insufficient. These results highlight the
need for new training strategies and inductive biases to fully unlock the
potential of unified modeling.

</details>


### [315] [Neural network embeddings recover value dimensions from psychometric survey items on par with human data](https://arxiv.org/abs/2509.24906)
*Max Pellert,Clemens M. Lechner,Indira Sen,Markus Strohmaier*

Main category: cs.AI

TL;DR: SQuID enables neural embeddings to recover latent psychometric dimensions from survey items, achieving 55% variance explained compared to human data without domain-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable, cost-effective method that can replicate psychometric structures from human surveys using semantic embeddings, addressing the challenge of obtaining negative correlations between dimensions.

Method: Uses SQuID (Survey and Questionnaire Item Embeddings Differentials) to process large language model embeddings, comparing multiple embedding models across evaluation metrics to recover human value structures from PVQ-RR.

Result: Explains 55% of variance in dimension-dimension similarities compared to human data, shows fair factor congruence coefficients, and successfully recovers the structure of human values without requiring negative correlation constraints.

Conclusion: Semantic embeddings can effectively replicate psychometric structures with comparable quality to traditional methods, offering significant advantages in cost, scalability and flexibility for psychometrics and social science research.

Abstract: This study introduces "Survey and Questionnaire Item Embeddings
Differentials" (SQuID), a novel methodological approach that enables neural
network embeddings to effectively recover latent dimensions from psychometric
survey items. We demonstrate that embeddings derived from large language
models, when processed with SQuID, can recover the structure of human values
obtained from human rater judgments on the Revised Portrait Value Questionnaire
(PVQ-RR). Our experimental validation compares multiple embedding models across
a number of evaluation metrics. Unlike previous approaches, SQuID successfully
addresses the challenge of obtaining negative correlations between dimensions
without requiring domain-specific fine-tuning. Quantitative analysis reveals
that our embedding-based approach explains 55% of variance in
dimension-dimension similarities compared to human data. Multidimensional
scaling configurations from both types of data show fair factor congruence
coefficients and largely follow the underlying theory. These results
demonstrate that semantic embeddings can effectively replicate psychometric
structures previously established through extensive human surveys. The approach
offers substantial advantages in cost, scalability and flexibility while
maintaining comparable quality to traditional methods. Our findings have
significant implications for psychometrics and social science research,
providing a complementary methodology that could expand the scope of human
behavior and experience represented in measurement tools.

</details>


### [316] [Meta-Learning Theory-Informed Inductive Biases using Deep Kernel Gaussian Processes](https://arxiv.org/abs/2509.24919)
*Bahti Zakirov,Gašper Tkačik*

Main category: cs.AI

TL;DR: A Bayesian meta-learning framework that converts normative theories into probabilistic models using adaptive deep kernel Gaussian processes, improving data fitting and theory validation.


<details>
  <summary>Details</summary>
Motivation: To automate the process of arbitrating between competing normative theories and using them as inductive biases for data-driven fits in biological systems, which is currently laborious and often impossible.

Method: Employ adaptive deep kernel Gaussian processes to meta-learn a kernel on synthetic data generated from normative theories, creating a Theory-Informed Kernel that specifies probabilistic models.

Result: Applied to mouse retinal ganglion cells, the framework showed improved response prediction accuracy with natural scene stimuli compared to conventional baselines, providing calibrated uncertainty estimates and interpretable representations. Bayesian model selection accurately inferred theory-match from data.

Conclusion: The work provides a scalable, automated approach for integrating theoretical knowledge into data-driven scientific inquiry in neuroscience and beyond.

Abstract: Normative and task-driven theories offer powerful top-down explanations for
biological systems, yet the goals of quantitatively arbitrating between
competing theories, and utilizing them as inductive biases to improve
data-driven fits of real biological datasets are prohibitively laborious, and
often impossible. To this end, we introduce a Bayesian meta-learning framework
designed to automatically convert raw functional predictions from normative
theories into tractable probabilistic models. We employ adaptive deep kernel
Gaussian processes, meta-learning a kernel on synthetic data generated from a
normative theory. This Theory-Informed Kernel specifies a probabilistic model
representing the theory predictions -- usable for both fitting data and
rigorously validating the theory. As a demonstration, we apply our framework to
the early visual system, using efficient coding as our normative theory. We
show improved response prediction accuracy in ex vivo recordings of mouse
retinal ganglion cells stimulated by natural scenes compared to conventional
data-driven baselines, while providing well-calibrated uncertainty estimates
and interpretable representations. Using exact Bayesian model selection, we
also show that our informed kernel can accurately infer the degree of
theory-match from data, confirming faithful encapsulation of theory structure.
This work provides a more general, scalable, and automated approach for
integrating theoretical knowledge into data-driven scientific inquiry in
neuroscience and beyond.

</details>


### [317] [MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning](https://arxiv.org/abs/2509.24922)
*Huihao Jing,Wenbin Hu,Hongyu Luo,Jianhui Yang,Wei Fan,Haoran Li,Yangqiu Song*

Main category: cs.AI

TL;DR: MASLegalBench is a legal benchmark designed specifically for multi-agent systems using GDPR scenarios, addressing the lack of MAS-focused evaluation methods in legal tasks.


<details>
  <summary>Details</summary>
Motivation: Previous legal benchmarks for LLM agents don't consider MAS advantages like task decomposition and agent specialization, limiting MAS potential in legal domain.

Method: Propose MASLegalBench with deductive reasoning approach using GDPR scenarios, manually design role-based MAS, and conduct experiments with state-of-the-art LLMs.

Result: Results reveal strengths, limitations, and improvement areas of existing models and MAS architectures in legal reasoning tasks.

Conclusion: MASLegalBench effectively evaluates MAS capabilities in legal domain and provides insights for future improvements in multi-agent legal systems.

Abstract: Multi-agent systems (MAS), leveraging the remarkable capabilities of Large
Language Models (LLMs), show great potential in addressing complex tasks. In
this context, integrating MAS with legal tasks is a crucial step. While
previous studies have developed legal benchmarks for LLM agents, none are
specifically designed to consider the unique advantages of MAS, such as task
decomposition, agent specialization, and flexible training. In fact, the lack
of evaluation methods limits the potential of MAS in the legal domain. To
address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS
and designed with a deductive reasoning approach. Our benchmark uses GDPR as
the application scenario, encompassing extensive background knowledge and
covering complex reasoning processes that effectively reflect the intricacies
of real-world legal situations. Furthermore, we manually design various
role-based MAS and conduct extensive experiments using different
state-of-the-art LLMs. Our results highlight the strengths, limitations, and
potential areas for improvement of existing models and MAS architectures.

</details>


### [318] [When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?](https://arxiv.org/abs/2509.24927)
*An Guo,Shuoxiao Zhang,Enyi Tang,Xinyu Gao,Haomin Pang,Haoxiang Tian,Yanzhou Mu,Wu Wen,Chunrong Fang,Zhenyu Chen*

Main category: cs.AI

TL;DR: This paper conducts an empirical study on V2X cooperative perception systems, identifying six common error patterns and evaluating critical system components through large-scale analysis.


<details>
  <summary>Details</summary>
Motivation: V2X cooperative perception systems face numerous operational challenges due to their complex composition, and the types of errors and their underlying causes remain insufficiently explored when these systems produce erroneous predictions.

Method: The authors conducted an empirical study by systematically evaluating the impact of cooperative perception on ego vehicle's perception performance, identifying six prevalent error patterns, and analyzing critical components through large-scale evaluation.

Result: Key findings include: (1) LiDAR-based cooperation has highest perception performance; (2) V2I and V2V communication show distinct performance under different fusion schemes; (3) Increased cooperative perception errors lead to more driving violations; (4) Systems are not robust against communication interference in online operation.

Conclusion: The study reveals potential risks and vulnerabilities in critical components of cooperative perception systems, providing findings that can better promote the design and repair of such systems.

Abstract: With the tremendous advancement of deep learning and communication
technology, Vehicle-to-Everything (V2X) cooperative perception has the
potential to address limitations in sensing distant objects and occlusion for a
single-agent perception system. V2X cooperative perception systems are software
systems characterized by diverse sensor types and cooperative agents, varying
fusion schemes, and operation under different communication conditions.
Therefore, their complex composition gives rise to numerous operational
challenges. Furthermore, when cooperative perception systems produce erroneous
predictions, the types of errors and their underlying causes remain
insufficiently explored. To bridge this gap, we take an initial step by
conducting an empirical study of V2X cooperative perception. To systematically
evaluate the impact of cooperative perception on the ego vehicle's perception
performance, we identify and analyze six prevalent error patterns in
cooperative perception systems. We further conduct a systematic evaluation of
the critical components of these systems through our large-scale study and
identify the following key findings: (1) The LiDAR-based cooperation
configuration exhibits the highest perception performance; (2)
Vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication
exhibit distinct cooperative perception performance under different fusion
schemes; (3) Increased cooperative perception errors may result in a higher
frequency of driving violations; (4) Cooperative perception systems are not
robust against communication interference when running online. Our results
reveal potential risks and vulnerabilities in critical components of
cooperative perception systems. We hope that our findings can better promote
the design and repair of cooperative perception systems.

</details>


### [319] [KIRETT -- A wearable device to support rescue operations using artificial intelligence to improve first aid](https://arxiv.org/abs/2509.24934)
*Johannes Zenkert,Christian Weber,Mubaris Nadeem,Lisa Bender,Madjid Fathi,Abu Shad Ahammed,Aniebiet Micheal Ezekiel,Roman Obermaisser,Maximilian Bradford*

Main category: cs.AI

TL;DR: The KIRETT project develops a wearable AI device to improve first aid during rescue operations by providing contextual recommendations to minimize patient harm and increase survival rates.


<details>
  <summary>Details</summary>
Motivation: To reduce patient damage from incorrect treatment and improve survival probability during rescue operations through technological assistance.

Method: Using wearable devices with artificial intelligence for computer-aided situation recognition and providing contextual action recommendations to rescue personnel.

Result: Initial research approaches and project overview presented, representing first steps in the scientific development of the KIRETT project.

Conclusion: The project shows promising potential for enhancing first aid effectiveness through AI-powered wearable technology in rescue scenarios.

Abstract: This short paper presents first steps in the scientific part of the KIRETT
project, which aims to improve first aid during rescue operations using a
wearable device. The wearable is used for computer-aided situation recognition
by means of artificial intelligence. It provides contextual recommendations for
actions and operations to rescue personnel and is intended to minimize damage
to patients due to incorrect treatment, as well as increase the probability of
survival. The paper describes a first overview of research approaches within
the project.

</details>


### [320] [Agentic Exploration of Physics Models](https://arxiv.org/abs/2509.24978)
*Maximilian Nägele,Florian Marquardt*

Main category: cs.AI

TL;DR: SciExplorer is an AI agent that uses large language models to autonomously explore unknown physical systems and discover their underlying laws through experiments and analysis, without requiring domain-specific knowledge or task-specific instructions.


<details>
  <summary>Details</summary>
Motivation: To fully automate the scientific discovery process by creating an agent that can explore unknown systems and discover their governing laws through an open-ended, iterative loop of experiments and analysis, without being tailored to specific domains.

Method: Leverages large language model tool-use capabilities with a minimal set of tools (primarily code execution) to enable free-form exploration of physical systems. Tested on mechanical dynamical systems, wave evolution, and quantum many-body physics.

Result: Impressive performance in recovering equations of motion from observed dynamics and inferring Hamiltonians from expectation values across diverse physical systems.

Conclusion: The approach demonstrates effective scientific exploration without fine-tuning or task-specific instructions, opening doors for similar automated discovery in other domains.

Abstract: The process of scientific discovery relies on an interplay of observations,
analysis, and hypothesis generation. Machine learning is increasingly being
adopted to address individual aspects of this process. However, it remains an
open challenge to fully automate the open-ended, heuristic, iterative loop
required to discover the laws of an unknown system by exploring it through
experiments and analysis, without tailoring the approach to the specifics of a
given task. Here, we introduce SciExplorer, an agent that leverages large
language model tool-use capabilities to enable free-form exploration of systems
without any domain-specific blueprints, and apply it to the exploration of
physical systems that are initially unknown to the agent. We test SciExplorer
on a broad set of models spanning mechanical dynamical systems, wave evolution,
and quantum many-body physics. Despite using a minimal set of tools, primarily
based on code execution, we observe impressive performance on tasks such as
recovering equations of motion from observed dynamics and inferring
Hamiltonians from expectation values. The demonstrated effectiveness of this
setup opens the door towards similar scientific exploration in other domains,
without the need for finetuning or task-specific instructions.

</details>


### [321] [CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning](https://arxiv.org/abs/2509.25004)
*Shijie Zhang,Guohao Sun,Kevin Zhang,Xiang Guo,Rujun Guo*

Main category: cs.AI

TL;DR: CLPO introduces a curriculum-guided learning approach for RLVR that dynamically adjusts training difficulty based on model performance, achieving SOTA results on reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods treat all training samples uniformly, ignoring differences in problem difficulty relative to model capabilities, leading to inefficient learning and limited performance.

Method: CLPO creates a dynamic pedagogical feedback loop using real-time difficulty assessment to construct an Online Curriculum, guiding Adaptive Problem Restructuring where the model diversifies medium problems and simplifies challenging ones.

Result: Achieves state-of-the-art performance across eight mathematical and general reasoning benchmarks with average pass@1 improvement of 6.96% over other methods.

Conclusion: CLPO transforms static training into dynamic co-evolution with model capabilities, enabling more efficient training of capable reasoning models.

Abstract: Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has
become a key paradigm for enhancing the reasoning capabilities of Large
Language Models (LLMs). However, existing methods typically treat all training
samples uniformly, overlooking the vast differences in problem difficulty
relative to the model's current capabilities. This uniform training strategy
leads to inefficient exploration of problems the model has already mastered,
while concurrently lacking effective guidance on problems that are challenging
its abilities the most, limiting both learning efficiency and upper-bound
performance. To address this, we propose CLPO (Curriculum-guided Learning for
Policy Optimization), a novel algorithm that creates a dynamic pedagogical
feedback loop within the policy optimization process. The core of CLPO
leverages the model's own rollout performance to conduct real-time difficulty
assessment, thereby constructing an Online Curriculum. This curriculum then
guides an Adaptive Problem Restructuring mechanism, where the model acts as its
own teacher: it diversifies medium-difficulty problems to promote
generalization and simplifies challenging problems to make them more
attainable. Our approach transforms the static training procedure into a
dynamic process that co-evolves with the model's capabilities. Experiments show
that CLPO achieves state-of-the-art performance across eight challenging
mathematical and general reasoning benchmarks, with an average pass@1
improvement of 6.96% over other methods, demonstrating its potential for more
efficiently training more capable reasoning models.

</details>


### [322] [Scaling Synthetic Task Generation for Agents via Exploration](https://arxiv.org/abs/2509.25047)
*Ram Ramrakhya,Andrew Szot,Omar Attia,Yuhao Yang,Anh Nguyen,Bogdan Mazoure,Zhe Gan,Harsh Agrawal,Alexander Toshev*

Main category: cs.AI

TL;DR: AutoPlay is a scalable pipeline for generating diverse, executable tasks for training multimodal large language models (MLLMs) as interactive agents, reducing reliance on human annotation by systematically exploring environments and synthesizing grounded tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of high-quality downstream agentic task datasets that are diverse, feasible, and verifiable for scaling post-training of MLLMs, as existing approaches are either costly (human annotation) or yield limited coverage (prompting MLLMs).

Method: AutoPlay operates in two stages: (1) exploration phase where an MLLM explorer agent systematically uncovers novel environment states and functionalities, and (2) task generation phase where a task generator uses exploration trajectories and task guideline prompts to synthesize diverse, executable, and verifiable tasks.

Result: AutoPlay generated 20k tasks across 20 Android applications and 10k tasks across 13 Ubuntu applications. This data enabled training MLLM-based UI agents that improved success rates by up to 20.0% on mobile-use and 10.9% on computer-use scenarios, with an additional 5.7% gain when combined with reinforcement learning.

Conclusion: AutoPlay establishes itself as a scalable approach for post-training capable MLLM agents, significantly reducing reliance on human annotation while enabling large-scale task demonstration synthesis and improved agent performance across mobile and computer use cases.

Abstract: Post-Training Multimodal Large Language Models (MLLMs) to build interactive
agents holds promise across domains such as computer-use, web navigation, and
robotics. A key challenge in scaling such post-training is lack of high-quality
downstream agentic task datasets with tasks that are diverse, feasible, and
verifiable. Existing approaches for task generation rely heavily on human
annotation or prompting MLLM with limited downstream environment information,
which is either costly or poorly scalable as it yield tasks with limited
coverage. To remedy this, we present AutoPlay, a scalable pipeline for task
generation that explicitly explores interactive environments to discover
possible interactions and current state information to synthesize
environment-grounded tasks. AutoPlay operates in two stages: (i) an exploration
phase, where an MLLM explorer agent systematically uncovers novel environment
states and functionalities, and (ii) a task generation phase, where a task
generator leverages exploration trajectories and a set of task guideline
prompts as context to synthesize diverse, executable, and verifiable tasks. We
show AutoPlay generates 20k tasks across 20 Android applications and 10k tasks
across 13 applications Ubuntu applications to train mobile-use and computer-use
agents. AutoPlay generated tasks enable large-scale task demonstration
synthesis without human annotation by employing an MLLM task executor and
verifier. This data enables training MLLM-based UI agents that improve success
rates up to $20.0\%$ on mobile-use and $10.9\%$ on computer-use scenarios. In
addition, AutoPlay generated tasks combined with MLLM verifier-based rewards
enable scaling reinforcement learning training of UI agents, leading to an
additional $5.7\%$ gain. coverage. These results establish AutoPlay as a
scalable approach for post-training capable MLLM agents reducing reliance on
human annotation.

</details>


### [323] [Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning](https://arxiv.org/abs/2509.25052)
*Sai Wang,Yu Wu,Zhongwen Xu*

Main category: cs.AI

TL;DR: CEL is a novel agent architecture that uses LLMs for explicit reasoning and planning in games, learning environment rules and strategies from scratch through interaction and reflection cycles.


<details>
  <summary>Details</summary>
Motivation: To create more interpretable and general agents that build transparent world models through explicit reasoning, rather than relying on opaque neural network weights.

Method: CEL uses LLM-based reasoning in cycles: Rule Induction to learn environment dynamics and Strategy Summarization to create playbooks from trajectory analysis after each episode.

Result: CEL successfully masters grid-world games (Minesweeper, Frozen Lake, Sokoban) by autonomously discovering rules and developing effective policies from sparse rewards.

Conclusion: The work demonstrates a path toward interpretable agents that build transparent world models through explicit reasoning on raw experience, with iterative learning being critical for success.

Abstract: The pursuit of artificial agents that can learn to master complex
environments has led to remarkable successes, yet prevailing deep reinforcement
learning methods often rely on immense experience, encoding their knowledge
opaquely within neural network weights. We propose a different paradigm, one in
which an agent learns to play by reasoning and planning. We introduce Cogito,
ergo ludo (CEL), a novel agent architecture that leverages a Large Language
Model (LLM) to build an explicit, language-based understanding of its
environment's mechanics and its own strategy. Starting from a tabula rasa state
with no prior knowledge (except action set), CEL operates on a cycle of
interaction and reflection. After each episode, the agent analyzes its complete
trajectory to perform two concurrent learning processes: Rule Induction, where
it refines its explicit model of the environment's dynamics, and Strategy and
Playbook Summarization, where it distills experiences into an actionable
strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,
Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent
successfully learns to master these games by autonomously discovering their
rules and developing effective policies from sparse rewards. Ablation studies
confirm that the iterative process is critical for sustained learning. Our work
demonstrates a path toward more general and interpretable agents that not only
act effectively but also build a transparent and improving model of their world
through explicit reasoning on raw experience.

</details>


### [324] [HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis](https://arxiv.org/abs/2509.25112)
*Yiquan Wang,Tin-Yeh Huang,Qingyun Gao,Jialin Zhang*

Main category: cs.AI

TL;DR: HeDA is an AI system that analyzes 10,247 academic papers to build a knowledge graph and identify overlooked heatwave risk pathways, achieving 78.9% accuracy and discovering 5 new high-impact risk chains.


<details>
  <summary>Details</summary>
Motivation: Heatwaves create complex cascading risks across climate, social and economic systems, but fragmented scientific literature hinders comprehensive understanding of these risk pathways.

Method: Developed HeDA (Heatwave Discovery Agent) - an intelligent multi-agent system that constructs knowledge graphs from academic papers and performs multi-layer risk propagation analysis.

Result: Built knowledge graph with 23,156 nodes and 89,472 relationships; achieved 78.9% accuracy on complex QA tasks (13.7% better than GPT-4); discovered 5 previously unidentified high-impact risk chains validated by experts.

Conclusion: HeDA presents a new paradigm for AI-driven scientific discovery, providing actionable insights for developing more resilient climate adaptation strategies.

Abstract: Heatwaves pose complex cascading risks across interconnected climate, social,
and economic systems, but knowledge fragmentation in scientific literature
hinders comprehensive understanding of these risk pathways. We introduce HeDA
(Heatwave Discovery Agent), an intelligent multi-agent system designed for
automated scientific discovery through knowledge graph construction and
multi-layer risk propagation analysis. HeDA processes over 10,247 academic
papers to construct a comprehensive knowledge graph with 23,156 nodes and
89,472 relationships, employing novel multi-layer risk propagation analysis to
systematically identify overlooked risk transmission pathways. Our system
achieves 78.9% accuracy on complex question-answering tasks, outperforming
state-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA
successfully discovered five previously unidentified high-impact risk chains,
such as the pathway where a heatwave leads to a water demand surge, resulting
in industrial water restrictions and ultimately causing small business
disruption, which were validated through historical case studies and domain
expert review. This work presents a new paradigm for AI-driven scientific
discovery, providing actionable insights for developing more resilient climate
adaptation strategies.

</details>


### [325] [From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones](https://arxiv.org/abs/2509.25123)
*Lifan Yuan,Weize Chen,Yuchen Zhang,Ganqu Cui,Hanbin Wang,Ziming You,Ning Ding,Zhiyuan Liu,Maosong Sun,Hao Peng*

Main category: cs.AI

TL;DR: RL enables LLMs to acquire genuinely new compositional skills by combining existing atomic skills, not just reweighting existing knowledge. This compositional ability generalizes to unseen function combinations and transfers across tasks.


<details>
  <summary>Details</summary>
Motivation: To resolve the debate about whether RL teaches LLMs genuinely new skills or merely activates existing ones, and to understand the mechanisms behind LLM skill acquisition.

Method: Developed a synthetic framework using string transformation functions, where skills are defined as inferring f(x) given x. Tested RL's ability to teach compositions h(x)=g(f(x)) when models already know f and g individually.

Result: RL enables LLMs to learn unseen function compositions, generalize to compositions of >2 functions, and transfer compositional skills across different tasks without target-specific compositional training. RL fundamentally changes reasoning behaviors, while next-token training fails to achieve these results.

Conclusion: RL can teach LLMs genuinely new compositional skills that generalize and transfer, suggesting a strategy of building base models with basic skills first, then using RL to develop advanced, generalizable skills for complex problems.

Abstract: Does RL teach LLMs genuinely new skills, or does it merely activate existing
ones? This question lies at the core of ongoing debates about the role of RL in
LLM post-training. On one side, strong empirical results can be achieved with
RL even without preceding supervised finetuning; on the other, critics argue
that RL contributes little beyond reweighting existing reasoning strategies.
This work provides concrete evidence that LLMs can acquire genuinely new skills
during RL by composing existing ones, mirroring one of the central mechanisms
by which humans acquire new cognitive skills. To mitigate data contamination
and other confounding factors, and to allow precise control over task
complexity, we develop a synthetic framework for our investigation.
Specifically, we define a skill as the ability to infer the output of a string
transformation function f(x) given x. When an LLM has already learned f and g
prior to RL, our experiments reveal that RL enables it to learn unseen
compositions of them h(x)=g(f(x)). Further, this compositional ability
generalizes to more difficult problems such as compositions of >2 functions
unseen during RL training. Surprisingly, our experiments show that
compositional skill acquired on a source task transfers to a different target
task. This transfer happens even without compositional training on the target,
requiring only prior knowledge of the target's atomic skills. Our qualitative
analysis shows that RL fundamentally changes the reasoning behaviors of the
models. In contrast, next-token training with the same data yields none of
these findings. Our systematic experiments provide fresh insights into LLM
learning, suggesting the value of first building base models with basic skills,
then using RL to incentivize advanced, generalizable skills for complex
problems.

</details>


### [326] [The Era of Real-World Human Interaction: RL from User Conversations](https://arxiv.org/abs/2509.25137)
*Chuanyang Jin,Jing Xu,Bo Liu,Leitian Tao,Olga Golovneva,Tianmin Shu,Wenting Zhao,Xian Li,Jason Weston*

Main category: cs.AI

TL;DR: RLHI (Reinforcement Learning from Human Interaction) learns from natural user conversations to improve model alignment, using two methods: User-Guided Rewrites and User-Based Rewards with persona conditioning.


<details>
  <summary>Details</summary>
Motivation: Current conversational models rely on pre-annotated expert feedback, but natural human interaction provides more scalable and effective supervision for continual model improvement and multifaceted alignment.

Method: Two complementary RLHI methods: (1) User-Guided Rewrites that revise unsatisfactory outputs based on user follow-up responses, (2) User-Based Rewards that learn via persona-conditioned reward models using long-term interaction history.

Result: Both RLHI variants trained on WildChat conversations outperform strong baselines in personalization and instruction-following, with similar feedback improving reasoning benchmarks.

Conclusion: Organic human interaction offers scalable, effective supervision for personalized alignment, enabling continual model improvement through natural conversation data.

Abstract: We posit that to achieve continual model improvement and multifaceted
alignment, future models must learn from natural human interaction. Current
conversational models are aligned using pre-annotated, expert-generated human
feedback. In this work, we introduce Reinforcement Learning from Human
Interaction (RLHI), a paradigm that learns directly from in-the-wild user
conversations. We develop two complementary methods: (1) RLHI with User-Guided
Rewrites, which revises unsatisfactory model outputs based on users'
natural-language follow-up responses, (2) RLHI with User-Based Rewards, which
learns via a reward model conditioned on knowledge of the user's long-term
interaction history (termed persona). Together, these methods link long-term
user personas to turn-level preferences via persona-conditioned preference
optimization. Trained on conversations derived from WildChat, both RLHI
variants outperform strong baselines in personalization and
instruction-following, and similar feedback enhances performance on reasoning
benchmarks. These results suggest organic human interaction offers scalable,
effective supervision for personalized alignment.

</details>


### [327] [Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs](https://arxiv.org/abs/2509.25139)
*Yue Zhang,Tianyi Ma,Zun Wang,Yanyuan Qiao,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: This paper proposes using multi-perspective textual descriptions to enhance LLM-based Vision-and-Language Navigation agents through analogical reasoning, improving navigation performance on R2R dataset.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot LLM-based VLN agents either oversimplify visual details by encoding images as text or fail to capture abstract semantics when processing raw images, limiting contextual understanding for navigation.

Method: Incorporates textual descriptions from multiple perspectives to facilitate analogical reasoning across images, enhancing global scene understanding and spatial reasoning for more accurate action decisions.

Result: Experiments on R2R dataset demonstrate significant improvements in navigation performance compared to existing approaches.

Conclusion: Multi-perspective textual descriptions combined with analogical reasoning effectively enhance LLM-based VLN agents' contextual understanding and navigation capabilities.

Abstract: Integrating large language models (LLMs) into embodied AI models is becoming
increasingly prevalent. However, existing zero-shot LLM-based
Vision-and-Language Navigation (VLN) agents either encode images as textual
scene descriptions, potentially oversimplifying visual details, or process raw
image inputs, which can fail to capture abstract semantics required for
high-level reasoning. In this paper, we improve the navigation agent's
contextual understanding by incorporating textual descriptions from multiple
perspectives that facilitate analogical reasoning across images. By leveraging
text-based analogical reasoning, the agent enhances its global scene
understanding and spatial reasoning, leading to more accurate action decisions.
We evaluate our approach on the R2R dataset, where our experiments demonstrate
significant improvements in navigation performance.

</details>


### [328] [ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory](https://arxiv.org/abs/2509.25140)
*Siru Ouyang,Jun Yan,I-Hung Hsu,Yanfei Chen,Ke Jiang,Zifeng Wang,Rujun Han,Long T. Le,Samira Daruki,Xiangru Tang,Vishy Tirumalashetty,George Lee,Mahsan Rofouei,Hangfei Lin,Jiawei Han,Chen-Yu Lee,Tomas Pfister*

Main category: cs.AI

TL;DR: ReasoningBank is a memory framework that enables LLM agents to learn from past experiences by distilling reasoning strategies from both successful and failed interactions, allowing continuous improvement over time.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents fail to learn from accumulated interaction history, forcing them to discard valuable insights and repeat past errors, limiting their effectiveness in persistent real-world roles.

Method: Proposes ReasoningBank memory framework that distills generalizable reasoning strategies from self-judged experiences, and introduces memory-aware test-time scaling (MaTTS) to accelerate learning by scaling up interaction experience.

Result: Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful routines, improving both effectiveness and efficiency.

Conclusion: Memory-driven experience scaling establishes a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arising through the synergy between memory and test-time scaling.

Abstract: With the growing adoption of large language model agents in persistent
real-world roles, they naturally encounter continuous streams of tasks. A key
limitation, however, is their failure to learn from the accumulated interaction
history, forcing them to discard valuable insights and repeat past errors. We
propose ReasoningBank, a novel memory framework that distills generalizable
reasoning strategies from an agent's self-judged successful and failed
experiences. At test time, an agent retrieves relevant memories from
ReasoningBank to inform its interaction and then integrates new learnings back,
enabling it to become more capable over time. Building on this powerful
experience learner, we further introduce memory-aware test-time scaling
(MaTTS), which accelerates and diversifies this learning process by scaling up
the agent's interaction experience. By allocating more compute to each task,
the agent generates abundant, diverse experiences that provide rich contrastive
signals for synthesizing higher-quality memory. The better memory in turn
guides more effective scaling, establishing a powerful synergy between memory
and test-time scaling. Across web browsing and software engineering benchmarks,
ReasoningBank consistently outperforms existing memory mechanisms that store
raw trajectories or only successful task routines, improving both effectiveness
and efficiency; MaTTS further amplifies these gains. These findings establish
memory-driven experience scaling as a new scaling dimension, enabling agents to
self-evolve with emergent behaviors naturally arise.

</details>


### [329] [Visual serial processing deficits explain divergences in human and VLM reasoning](https://arxiv.org/abs/2509.25142)
*Nicholas Budny,Kia Ghods,Declan Campbell,Raja Marjieh,Amogh Joshi,Sreejan Kumar,Jonathan D. Cohen,Taylor W. Webb,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: VLMs fail on simple visual reasoning tasks due to deficits in visually-grounded serial processing, as shown by correlation between decreased VLM accuracy and increased human reaction time across geometric reasoning, enumeration, and mental rotation tasks.


<details>
  <summary>Details</summary>
Motivation: To understand why VLMs perform poorly on simple visual reasoning tasks despite success on standard benchmarks, hypothesizing that visually-grounded serial processing limitations are the key factor.

Method: Compared human and VLM performance across three domains (geometric reasoning, perceptual enumeration, mental rotation) with varying serial processing demands, using human reaction time as proxy for processing load.

Result: Decreased VLM accuracy strongly correlated with increased human reaction time across all domains - VLM-human performance gap widens as serial processing demands increase.

Conclusion: Limitations in serial, visually-grounded reasoning represent a fundamental bottleneck distinguishing current VLMs from human performance.

Abstract: Why do Vision Language Models (VLMs), despite success on standard benchmarks,
often fail to match human performance on surprisingly simple visual reasoning
tasks? While the underlying computational principles are still debated, we
hypothesize that a crucial factor is a deficit in visually-grounded serial
processing. To test this hypothesis, we compared human and VLM performance
across tasks designed to vary serial processing demands in three distinct
domains: geometric reasoning, perceptual enumeration, and mental rotation.
Tasks within each domain varied serial processing load by manipulating factors
such as geometric concept complexity, perceptual individuation load, and
transformation difficulty. Across all domains, our results revealed a
consistent pattern: decreased VLM accuracy was strongly correlated with
increased human reaction time (used as a proxy for serial processing load). As
tasks require more demanding serial processing -- whether composing concepts,
enumerating items, or performing mental transformations -- the VLM-human
performance gap widens reliably. These findings support our hypothesis,
indicating that limitations in serial, visually grounded reasoning represent a
fundamental bottleneck that distinguishes current VLMs from humans.

</details>


### [330] [UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following](https://arxiv.org/abs/2509.25148)
*FaQiang Qian,WeiKun Zhang,Ziliang Wang,Kang An,Xuhui Zheng,Liangjian Wen,Mengya Gao,Yong Dai,Yichao Wu*

Main category: cs.AI

TL;DR: UniAPL is a unified adversarial preference learning framework that addresses distributional mismatch in standard SFT+RL alignment pipelines by jointly learning from both demonstration and preference data in a single stage.


<details>
  <summary>Details</summary>
Motivation: Standard sequential alignment pipelines (SFT followed by RL) suffer from distributional mismatch where SFT uses static expert data but policy distribution drifts during RL, making SFT knowledge brittle and RL updates ungrounded.

Method: Reframe alignment as constrained optimization and propose UniAPL - a single-stage unified training objective that learns from mixed batches of SFT and preference data, allowing expert demonstrations to directly ground and regularize online exploration.

Result: UniAPL models match or exceed strong GRPO baselines: +5.77% on Qwen3-0.6B (matching 32B model) and +3.75% on Qwen3-4B, even outperforming the teacher model. Response analysis confirms outputs closely mimic expert demonstrations.

Conclusion: UniAPL resolves distributional mismatch and maximizes data synergy, achieving both stronger performance and better behavioral alignment by dynamically aligning policy distribution with expert distribution.

Abstract: Shaping powerful LLMs to be beneficial and safe is central to AI alignment.
We argue that post-training alignment is fundamentally a unified Preference
Learning problem, involving two modalities: demonstrated preferences (e.g.,
Supervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement
Learning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due
to a critical distributional mismatch: SFT uses static expert data, but as the
policy evolves, its generation distribution drifts, making SFT knowledge
brittle. Subsequent RL then explores without direct access to the rich,
ground-truth knowledge in expert demonstrations, leading to inefficient,
ungrounded updates. This separation prevents mutual regularization between data
sources. To address this, we reframe alignment as a constrained optimization
problem and propose Unified Adversarial Preference Learning (UniAPL),a novel
framework that dynamically aligns the policy's distribution with the expert's.
UniAPL implements a single-stage unified training objective, jointly learning
from mixed batches of SFT and preference data. In every gradient step, dense
expert demonstrations directly ground and regularize online exploration,
inherently resolving distributional mismatch and maximizing data synergy.We
evaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507
as the teacher. Our models match or exceed strong GRPO baselines: +5.77% on
Qwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the
teacher. Analyses of response length and log-probability distributions confirm
that UniAPL outputs closely mimic expert demonstrations, achieving both
stronger performance and better behavioral alignment.

</details>


### [331] [Who's Your Judge? On the Detectability of LLM-Generated Judgments](https://arxiv.org/abs/2509.25154)
*Dawei Li,Zhen Tan,Chengshuai Zhao,Bohan Jiang,Baixiang Huang,Pingchuan Ma,Abdullah Alnaibari,Kai Shu,Huan Liu*

Main category: cs.AI

TL;DR: This paper proposes judgment detection to identify LLM-generated evaluation scores, introduces J-Detector method that captures interactions between scores and content, and demonstrates its effectiveness in detecting LLM judge biases.


<details>
  <summary>Details</summary>
Motivation: LLM-based judgments have inherent biases and vulnerabilities that raise concerns in sensitive scenarios like academic peer reviewing, creating urgent need for detection methods.

Method: Proposed J-Detector - a lightweight neural detector augmented with linguistic and LLM-enhanced features to link LLM judges' biases with candidates' properties for detection.

Result: Experiments across diverse datasets demonstrate J-Detector's effectiveness and interpretability in quantifying LLM judge biases, outperforming existing LLM-generated text detection methods.

Conclusion: Judgment detection is practically useful in real-world scenarios, and the paper analyzes key factors affecting detectability of LLM-generated judgments.

Abstract: Large Language Model (LLM)-based judgments leverage powerful LLMs to
efficiently evaluate candidate content and provide judgment scores. However,
the inherent biases and vulnerabilities of LLM-generated judgments raise
concerns, underscoring the urgent need for distinguishing them in sensitive
scenarios like academic peer reviewing. In this work, we propose and formalize
the task of judgment detection and systematically investigate the detectability
of LLM-generated judgments. Unlike LLM-generated text detection, judgment
detection relies solely on judgment scores and candidates, reflecting
real-world scenarios where textual feedback is often unavailable in the
detection process. Our preliminary analysis shows that existing LLM-generated
text detection methods perform poorly given their incapability to capture the
interaction between judgment scores and candidate content -- an aspect crucial
for effective judgment detection. Inspired by this, we introduce
\textit{J-Detector}, a lightweight and transparent neural detector augmented
with explicitly extracted linguistic and LLM-enhanced features to link LLM
judges' biases with candidates' properties for accurate detection. Experiments
across diverse datasets demonstrate the effectiveness of \textit{J-Detector}
and show how its interpretability enables quantifying biases in LLM judges.
Finally, we analyze key factors affecting the detectability of LLM-generated
judgments and validate the practical utility of judgment detection in
real-world scenarios.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [332] [GPS Spoofing Attacks and Pilot Responses Using a Flight Simulator Environment](https://arxiv.org/abs/2509.22662)
*Mathilde Durieux,Kayla D. Taylor,Laxima Niure Kandel,Deepti Gupta*

Main category: cs.CR

TL;DR: This study investigates how student pilots respond to GPS spoofing attacks in a flight simulator, focusing on human factors like reaction time, visual attention, and cognitive biases during navigational deception.


<details>
  <summary>Details</summary>
Motivation: Existing GPS spoofing research primarily focuses on technical detection and mitigation methods, while neglecting human factors. Since pilots are integral to aircraft operation and potentially vulnerable to deception, this study addresses the gap by examining pilot behavior during GPS spoofing attacks.

Method: The research used a Force Dynamics 401CR flight simulator with X-Plane 11 and a Cessna 172 equipped with Garmin G1000. Thirty student pilots from Embry-Riddle Aeronautical University participated in three spoofing scenarios implemented via custom scripts that altered navigational data. Pre-simulation questionnaires measured pilot experience and GPS confidence, while inflight decision-making was observed for reaction time, visual attention, and cognitive biases. Post-flight workload was evaluated using a modified NASA TLX method.

Result: The study observed student pilots' responses to GPS spoofing attacks, including their reaction time to anomalies, visual attention patterns to interface elements, and identification of cognitive biases that may affect their decision-making during navigational deception.

Conclusion: This research provides initial insights into human vulnerabilities to GPS spoofing, contributing to the ongoing debate about GPS reliance in aviation by highlighting the importance of considering human factors alongside technical solutions for GPS spoofing detection and mitigation.

Abstract: Global Positioning System (GPS) spoofing involves transmitting fake signals
that mimic those from GPS satellites, causing the GPS receivers to calculate
incorrect Positioning, Navigation, and Timing (PNT) information. Recently,
there has been a surge in GPS spoofing attacks targeting aircraft. Since GPS
satellite signals are weak, the spoofed high-power signal can easily overpower
them. These spoofed signals are often interpreted as valid by the GPS receiver,
which can cause severe and cascading effects on air navigation. While much of
the existing research on GPS spoofing focuses on technical aspects of detection
and mitigation, human factors are often neglected, even though pilots are an
integral part of aircraft operation and potentially vulnerable to deception.
This research addresses this gap by conducting a detailed analysis of the
behavior of student pilots when subjected to GPS spoofing using the Force
Dynamics 401CR flight simulator with X-Plane 11 and a Cessna 172 equipped with
Garmin G1000. Spoofing scenarios were implemented via custom scripts that
altered navigational data without modifying the external visual environment.
Thirty student pilots from the Embry-Riddle Aeronautical University Daytona
Beach campus with diverse flying experience levels were recruited to
participate in three spoofing scenarios. A pre-simulation questionnaire was
distributed to measure pilot experience and confidence in GPS.Inflight
decision-making during the spoofing attacks was observed, including reaction
time to anomalies, visual attention to interface elements, and cognitive
biases. A post-flight evaluation of workload was obtained using a modified NASA
Task Load Index (TLX) method. This study provides a first step toward
identifying human vulnerabilities to GPS spoofing amid the ongoing debate over
GPS reliance.

</details>


### [333] [Security Friction Quotient for Zero Trust Identity Policy with Empirical Validation](https://arxiv.org/abs/2509.22663)
*Michel Youssef*

Main category: cs.CR

TL;DR: The paper introduces Security Friction Quotient (SFQ), a composite index that quantifies the trade-off between security and operational friction in identity programs, validated through simulations and field observations.


<details>
  <summary>Details</summary>
Motivation: To provide a practical method for quantifying the balance between security measures and user experience friction in modern identity-centric security programs, enabling better Zero Trust policy decisions.

Method: Developed SFQ index combining residual-risk estimator with empirical friction metrics (latency, failure rate, helpdesk impact). Evaluated using Monte Carlo simulation (2,000 runs per policy) and validated with 12-week enterprise field observation (N=1,200 users).

Result: SFQ demonstrates boundedness and monotonic response properties. Monte Carlo simulations show 95.5% policy ordering preservation under random weight variations. Field observations align with simulation results for phishing-resistant MFA gains.

Conclusion: SFQ provides a reproducible, interpretable framework for Zero Trust identity policy decisions, with practical tools for policy design, review, and continuous improvement.

Abstract: We define a practical method to quantify the trade-off between security and
operational friction in modern identity-centric programs. We introduce the
Security Friction Quotient (SFQ), a bounded composite index that combines a
residual-risk estimator with empirically grounded friction terms (latency,
failure rate, and helpdesk impact). We establish clarity properties
(boundedness, monotonic response, and weight identifiability) with short
proofs, then evaluate widely used Conditional Access policies over a 12-week
horizon using Monte Carlo simulation (n = 2,000 runs per policy/scenario) with
effect sizes and 95% confidence intervals. We further assess rank stability
under 10,000 random weight draws, finding 95.5% preservation of policy
ordering. Finally, we provide a 12-week passkey field observation from an
enterprise-scale cohort (N = 1,200) that directionally aligns with the
simulation's phishing-resistant MFA gains. The SFQ framework is designed to be
reproducible, interpretable, and directly actionable for Zero Trust identity
policy decisions, with artifacts and parameter ranges provided to support
policy design, review, and continuous improvement.

</details>


### [334] [Security Issues on the OpenPLC project and corresponding solutions](https://arxiv.org/abs/2509.22664)
*Chaerin Kim*

Main category: cs.CR

TL;DR: This paper analyzes security vulnerabilities in OpenPLC software and proposes a security-enhanced version called OpenPLC Aqua with protection against control logic injection attacks.


<details>
  <summary>Details</summary>
Motivation: OpenPLC provides affordable PLC simulation but lacks adequate security measures, making it vulnerable to attacks in industrial environments.

Method: Used threat modeling, vulnerability analysis, and practical experiments including control logic injection attacks to identify security weaknesses.

Result: Identified multiple security vulnerabilities in OpenPLC and successfully demonstrated control logic injection attacks that can maliciously modify user programs.

Conclusion: Developed OpenPLC Aqua as a security-enhanced version with specific solutions to address the identified vulnerabilities in current OpenPLC versions.

Abstract: As Programmable Logic Controller (PLC) became a useful device and rose as an
interesting research topic but remained expensive, multiple PLC
simulators/emulators were introduced for various purposes. Open-source
Programmable Logic Controller (OpenPLC) software, one of the most popular PLC
simulators, is designed to be vendor-neutral and run on almost any computer or
low-cost embedded devices, e.g., Raspberry Pi, Arduino, and other controllers.
The project succeeded in introducing itself as an affordable and practical
solution for the high cost of real hardware PLCs. However, it still lacks
appropriate securing methods, resulting in several vulnerabilities. Through a
combination of threat modeling, vulnerability analysis, and practical
experiments, this thesis provides valuable insights for developers,
researchers, and engineers aiming to deploy OpenPLC securely in industrial
environments. To this end, this work first conducts an in-depth analysis aimed
to shed light on va! rious security challenges and vulnerabilities within the
OpenPLC project. After that, an advanced control logic injection attack was
performed. This attack modifies the user program maliciously, exploiting
presented vulnerabilities. Finally, the work introduces a security-enhanced
OpenPLC software called OpenPLC Aqua. The new software is equipped with a set
of security solutions designed specifically to address the vulnerabilities to
which current OpenPLC versions are prone.

</details>


### [335] [Responsible Diffusion: A Comprehensive Survey on Safety, Ethics, and Trust in Diffusion Models](https://arxiv.org/abs/2509.22723)
*Kang Wei,Xin Yuan,Fushuo Huo,Chuan Ma,Long Yuan,Songze Li,Ming Ding,Dacheng Tao*

Main category: cs.CR

TL;DR: This survey paper comprehensively examines the security threats, ethical concerns, and trust issues in diffusion models (DMs), systematically categorizing threats and countermeasures while providing practical examples and future research directions.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have gained significant attention for their high-quality data generation capabilities, but similar to traditional deep learning systems, they face potential security threats. The paper aims to provide comprehensive insights into safety, ethics, and trust in DMs to advance both technical capabilities and responsible application of generative AI.

Method: The survey systematically examines and categorizes threats and countermeasures in diffusion models, analyzes the DM framework, and provides specific examples of usage scenarios, potential dangers, and protection methods.

Result: The paper presents a comprehensive framework for understanding DM security, systematically categorizes various threats and their corresponding countermeasures, and provides practical examples illustrating both the applications and vulnerabilities of diffusion models.

Conclusion: The survey identifies key lessons learned, highlights open challenges in DM security, and outlines prospective research directions to accelerate progress in both technical capabilities and responsible application of generative artificial intelligence.

Abstract: Diffusion models (DMs) have been investigated in various domains due to their
ability to generate high-quality data, thereby attracting significant
attention. However, similar to traditional deep learning systems, there also
exist potential threats to DMs. To provide advanced and comprehensive insights
into safety, ethics, and trust in DMs, this survey comprehensively elucidates
its framework, threats, and countermeasures. Each threat and its
countermeasures are systematically examined and categorized to facilitate
thorough analysis. Furthermore, we introduce specific examples of how DMs are
used, what dangers they might bring, and ways to protect against these dangers.
Finally, we discuss key lessons learned, highlight open challenges related to
DM security, and outline prospective research directions in this critical
field. This work aims to accelerate progress not only in the technical
capabilities of generative artificial intelligence but also in the maturity and
wisdom of its application.

</details>


### [336] [Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks](https://arxiv.org/abs/2509.22732)
*Haibo Tong,Dongcheng Zhao,Guobin Shen,Xiang He,Dachuan Lin,Feifei Zhao,Yi Zeng*

Main category: cs.CR

TL;DR: BIID is a bidirectional defense method that combines forward intention inference and backward intention retrospection to detect multi-turn jailbreak attacks on LLMs, significantly reducing attack success rates while maintaining utility.


<details>
  <summary>Details</summary>
Motivation: Existing defenses focus on single-turn jailbreak attacks but fail against multi-turn attacks that progressively bypass safety mechanisms through concealed malicious intent and tactical manipulation.

Method: Bidirectional Intention Inference Defense (BIID) integrates forward request-based intention inference with backward response-based intention retrospection to detect risks in seemingly benign inputs.

Result: BIID significantly reduces Attack Success Rate across both single-turn and multi-turn jailbreak attempts, outperforming 7 baseline methods on 3 LLMs and 2 safety benchmarks under 10 attack methods.

Conclusion: BIID provides robust guardrails against jailbreak attacks, demonstrating significant advantages over other defense approaches while effectively maintaining practical utility.

Abstract: The remarkable capabilities of Large Language Models (LLMs) have raised
significant safety concerns, particularly regarding "jailbreak" attacks that
exploit adversarial prompts to bypass safety alignment mechanisms. Existing
defense research primarily focuses on single-turn attacks, whereas multi-turn
jailbreak attacks progressively break through safeguards through by concealing
malicious intent and tactical manipulation, ultimately rendering conventional
single-turn defenses ineffective. To address this critical challenge, we
propose the Bidirectional Intention Inference Defense (BIID). The method
integrates forward request-based intention inference with backward
response-based intention retrospection, establishing a bidirectional synergy
mechanism to detect risks concealed within seemingly benign inputs, thereby
constructing a more robust guardrails that effectively prevents harmful content
generation. The proposed method undergoes systematic evaluation compared with a
no-defense baseline and seven representative defense methods across three LLMs
and two safety benchmarks under 10 different attack methods. Experimental
results demonstrate that the proposed method significantly reduces the Attack
Success Rate (ASR) across both single-turn and multi-turn jailbreak attempts,
outperforming all existing baseline methods while effectively maintaining
practical utility. Notably, comparative experiments across three multi-turn
safety datasets further validate the proposed model's significant advantages
over other defense approaches.

</details>


### [337] [Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment](https://arxiv.org/abs/2509.22745)
*Jaehan Kim,Minkyoo Song,Seungwon Shin,Sooel Son*

Main category: cs.CR

TL;DR: SafeMoE is a fine-tuning method for Mixture-of-Experts (MoE) LLMs that prevents harmful fine-tuning attacks by penalizing routing weight drift, maintaining safety while preserving utility.


<details>
  <summary>Details</summary>
Motivation: MoE-based LLMs rely on routing harmful inputs to safety-critical experts, but fine-tuning causes routing drift that exposes vulnerabilities to harmful attacks. Existing defenses designed for monolithic LLMs are ineffective for MoE architectures.

Method: SafeMoE directly mitigates routing drift by penalizing the gap between fine-tuned and initial safety-aligned model routing weights, preserving harmful input routing to safety-critical experts.

Result: SafeMoE reduces harmfulness score from 62.0 to 5.0 on OLMoE, maintains task utility within 1% degradation, incurs only 2% overhead, and outperforms state-of-the-art defense methods across 7B to 141B parameter MoE LLMs.

Conclusion: SafeMoE effectively safeguards MoE LLMs against harmful fine-tuning attacks by preventing routing drift, demonstrating strong performance across various model scales including recent large-scale MoE models like gpt-oss and Llama 4.

Abstract: Recent large language models (LLMs) have increasingly adopted the
Mixture-of-Experts (MoE) architecture for efficiency. MoE-based LLMs heavily
depend on a superficial safety mechanism in which harmful inputs are routed
safety-critical experts. However, our analysis reveals that routing decisions
for harmful inputs drift significantly after fine-tuning, exposing a critical
vulnerability to harmful fine-tuning (HFT) attacks. Existing defenses,
primarily designed for monolithic LLMs, are less effective for MoE LLMs as they
fail to prevent drift in harmful input routing. To address this limitation, we
propose SafeMoE, a safe fine-tuning method tailored to MoE LLMs. SafeMoE
directly mitigates routing drift by penalizing the gap between the routing
weights of a fine-tuned model and those of the initial safety-aligned model,
thereby preserving the safety-aligned routing of harmful inputs to
safety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to
141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks,
reducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while
maintaining task utility within 1% degradation and incurring only 2% overhead.
It significantly outperforms state-of-the-art defense methods for safeguarding
LLM fine-tuning and remains effective in recent large-scale MoE LLMs such as
gpt-oss and Llama 4. Our implementation is available at
https://anonymous.4open.science/r/SafeMoE.

</details>


### [338] [Red Teaming Quantum-Resistant Cryptographic Standards: A Penetration Testing Framework Integrating AI and Quantum Security](https://arxiv.org/abs/2509.22757)
*Petar Radanliev*

Main category: cs.CR

TL;DR: AI-driven framework for evaluating quantum cryptographic protocol vulnerabilities using red teaming, penetration testing, and anomaly detection.


<details>
  <summary>Details</summary>
Motivation: To develop a structured approach for assessing and mitigating security risks in quantum networks, particularly focusing on BB84 QKD and NIST-approved quantum-resistant algorithms.

Method: Integration of AI-driven red teaming, automated penetration testing, real-time anomaly detection, automated exploit simulations, protocol fuzzing, and adversarial machine learning techniques.

Result: AI effectively simulates adversarial attacks, probes cryptographic weaknesses, and refines security mechanisms through iterative feedback; automated methods identify latent vulnerabilities and highlight novel attack surfaces.

Conclusion: Provides a comprehensive methodology for strengthening quantum security and establishes foundation for integrating AI-driven cybersecurity practices in quantum landscape.

Abstract: This study presents a structured approach to evaluating vulnerabilities
within quantum cryptographic protocols, focusing on the BB84 quantum key
distribution method and National Institute of Standards and Technology (NIST)
approved quantum-resistant algorithms. By integrating AI-driven red teaming,
automated penetration testing, and real-time anomaly detection, the research
develops a framework for assessing and mitigating security risks in quantum
networks. The findings demonstrate that AI can be effectively used to simulate
adversarial attacks, probe weaknesses in cryptographic implementations, and
refine security mechanisms through iterative feedback. The use of automated
exploit simulations and protocol fuzzing provides a scalable means of
identifying latent vulnerabilities, while adversarial machine learning
techniques highlight novel attack surfaces within AI-enhanced cryptographic
processes. This study offers a comprehensive methodology for strengthening
quantum security and provides a foundation for integrating AI-driven
cybersecurity practices into the evolving quantum landscape.

</details>


### [339] [TRUSTCHECKPOINTS: Time Betrays Malware for Unconditional Software Root of Trust](https://arxiv.org/abs/2509.22762)
*Friedrich Doku,Peter Dinda*

Main category: cs.CR

TL;DR: TRUSTCHECKPOINTS is a system that establishes an unconditional software root of trust without secrets or trusted hardware by using timing-constrained polynomial challenges to verify system integrity through memory scanning.


<details>
  <summary>Details</summary>
Motivation: Current root of trust approaches rely on secret keys and secure hardware, which increase costs, complexity, and depend on computational assumptions about attackers. There's a need for a more fundamental approach.

Method: Developers capture full-system checkpoints and roll back to them. Verifier issues timing-constrained, randomized k-independent polynomial challenges using Horner's rule to repeatedly scan on-chip memory. Malicious code causes detectable timing delays when swapping to off-chip storage.

Result: Prototype on ARM Cortex-A53 validates 192 KB SRAM in ~10s using 500 passes, detecting single-instruction persistent malware. Successfully extends trust to DRAM with two modes (fast SRAM-bootstrap and comprehensive full-memory scan).

Conclusion: TRUSTCHECKPOINTS provides reliable malware detection on unmodified hardware without secrets or trusted hardware, offering trade-offs between speed and coverage through different verification modes.

Abstract: Modern IoT and embedded platforms must start execution from a known trusted
state to thwart malware, ensure secure firmware updates, and protect critical
infrastructure. Current approaches to establish a root of trust depend on
secret keys and/or specialized secure hardware, which drives up costs, may
involve third parties, adds operational complexity, and relies on assumptions
about an attacker's computational power. In contrast, TRUSTCHECKPOINTS is the
first system to establish an unconditional software root of trust based on a
formal model without relying on secrets or trusted hardware. Developers capture
a full-system checkpoint and later roll back to it and prove this to an
external verifier. The verifier issues timing-constrained, randomized
k-independent polynomial challenges (via Horner's rule) that repeatedly scan
the fast on-chip memory in randomized passes. When malicious code attempts to
persist, it must swap into slower, unchecked off-chip storage, causing a
detectable timing delay.
  Our prototype for a commodity ARM Cortex-A53-based platform validates 192 KB
of SRAM in approximately 10 s using 500 passes, sufficient to detect
single-instruction persistent malware. The prototype then seamlessly extends
trust to DRAM. Two modes (fast SRAM-bootstrap and comprehensive full-memory
scan) allow trade-offs between speed and coverage, demonstrating reliable
malware detection on unmodified hardware.

</details>


### [340] [What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs](https://arxiv.org/abs/2509.22796)
*Xingyu Li,Juefei Pu,Yifan Wu,Xiaochen Zou,Shitong Zhu,Xiaochen Zou,Shitong Zhu,Qiushi Wu,Zheng Zhang,Joshua Hsu,Yue Dong,Zhiyun Qian,Kangjie Lu,Trent Jaeger,Michael De Lucia,Srikanth V. Krishnamurthy*

Main category: cs.CR

TL;DR: DUALLM is a dual-method pipeline using LLM and fine-tuned small language model to identify security-critical patches in Linux kernel, achieving 87.4% accuracy and successfully detecting 111 OOB/UAF vulnerabilities from 5,140 patches.


<details>
  <summary>Details</summary>
Motivation: Security patches in Linux kernel are often delayed due to difficulty identifying security-critical patches, especially exploitable vulnerabilities like OOB and UAF bugs, exacerbated by silent fixes and incomplete CVE assignments.

Method: Developed DUALLM pipeline integrating two approaches: LLM-based and fine-tuned small language model, leveraging commit titles/messages, diffs, and appropriate code context for fine-grained patch classification.

Result: Achieved 87.4% accuracy and F1-score of 0.875, identified 111 OOB/UAF vulnerabilities from 5,140 patches with 90 true positives confirmed, and constructed PoCs for two bugs including control-flow hijack.

Conclusion: DUALLM significantly outperforms prior solutions in identifying security-critical patches, demonstrating practical effectiveness in detecting exploitable vulnerabilities in Linux kernel patches.

Abstract: Open-source software projects are foundational to modern software ecosystems,
with the Linux kernel standing out as a critical exemplar due to its ubiquity
and complexity. Although security patches are continuously integrated into the
Linux mainline kernel, downstream maintainers often delay their adoption,
creating windows of vulnerability. A key reason for this lag is the difficulty
in identifying security-critical patches, particularly those addressing
exploitable vulnerabilities such as out-of-bounds (OOB) accesses and
use-after-free (UAF) bugs. This challenge is exacerbated by intentionally
silent bug fixes, incomplete or missing CVE assignments, delays in CVE
issuance, and recent changes to the CVE assignment criteria for the Linux
kernel. While fine-grained patch classification approaches exist, they exhibit
limitations in both coverage and accuracy. In this work, we identify previously
unexplored opportunities to significantly improve fine-grained patch
classification. Specifically, by leveraging cues from commit titles/messages
and diffs alongside appropriate code context, we develop DUALLM, a dual-method
pipeline that integrates two approaches based on a Large Language Model (LLM)
and a fine-tuned small language model. DUALLM achieves 87.4% accuracy and an
F1-score of 0.875, significantly outperforming prior solutions. Notably, DUALLM
successfully identified 111 of 5,140 recent Linux kernel patches as addressing
OOB or UAF vulnerabilities, with 90 true positives confirmed by manual
verification (many do not have clear indications in patch descriptions).
Moreover, we constructed proof-of-concepts for two identified bugs (one UAF and
one OOB), including one developed to conduct a previously unknown control-flow
hijack as further evidence of the correctness of the classification.

</details>


### [341] [Model Context Protocol for Vision Systems: Audit, Security, and Protocol Extensions](https://arxiv.org/abs/2509.22814)
*Aditi Tiwari,Akshit Bhalla,Darshan Prasad*

Main category: cs.CR

TL;DR: First deployment-scale audit of Model Context Protocol (MCP) in vision systems reveals systemic weaknesses in schema semantics, interoperability, and runtime coordination across 91 vision-centric MCP servers.


<details>
  <summary>Details</summary>
Motivation: To identify and quantify systemic weaknesses in MCP protocol implementation for vision systems, enabling modular computer vision workflows without retraining.

Method: Analyzed 91 publicly registered vision-centric MCP servers using nine dimensions of compositional fidelity, developed executable benchmark with validators to detect protocol violations, and conducted security probes.

Result: High prevalence of schema format divergence (78.0%), coordinate convention errors (24.6%), memory scope warnings (33.8 per 100 executions), and security risks in dynamic/multi-agent workflows with privilege escalation and untyped tool connections.

Conclusion: The proposed benchmark and validator suite establishes a reproducible framework for measuring and improving reliability and security of compositional vision workflows, to be released on GitHub.

Abstract: The Model Context Protocol (MCP) defines a schema bound execution model for
agent-tool interaction, enabling modular computer vision workflows without
retraining. To our knowledge, this is the first protocol level, deployment
scale audit of MCP in vision systems, identifying systemic weaknesses in schema
semantics, interoperability, and runtime coordination. We analyze 91 publicly
registered vision centric MCP servers, annotated along nine dimensions of
compositional fidelity, and develop an executable benchmark with validators to
detect and categorize protocol violations. The audit reveals high prevalence of
schema format divergence, missing runtime schema validation, undeclared
coordinate conventions, and reliance on untracked bridging scripts. Validator
based testing quantifies these failures, with schema format checks flagging
misalignments in 78.0 percent of systems, coordinate convention checks
detecting spatial reference errors in 24.6 percent, and memory scope checks
issuing an average of 33.8 warnings per 100 executions. Security probes show
that dynamic and multi agent workflows exhibit elevated risks of privilege
escalation and untyped tool connections. The proposed benchmark and validator
suite, implemented in a controlled testbed and to be released on GitHub,
establishes a reproducible framework for measuring and improving the
reliability and security of compositional vision workflows.

</details>


### [342] [PAPER: Privacy-Preserving ResNet Models using Low-Degree Polynomial Approximations and Structural Optimizations on Leveled FHE](https://arxiv.org/abs/2509.22857)
*Eduardo Chielle,Manaar Alam,Jinting Liu,Jovan Kascelan,Michail Maniatakos*

Main category: cs.CR

TL;DR: This paper presents a method to improve privacy-preserving inference using FHE on ResNets, achieving comparable accuracy to plaintext models and 4x faster inference than prior work by minimizing multiplicative depth and eliminating bootstrapping.


<details>
  <summary>Details</summary>
Motivation: Existing FHE-based CNN inference methods suffer from slow performance due to costly bootstrapping operations and accuracy degradation from high-degree polynomial approximations of non-linear activations.

Method: Uses quadratic polynomial approximation of ReLU (minimum multiplicative depth), penalty-based training, structural optimizations (node fusing, weight redistribution, tower reuse), parameter clustering, and ensemble techniques with optimized data encoding.

Result: Achieves up to 4x faster private inference than prior work with comparable accuracy to plaintext ReLU models on ResNet-18/20/32 with CIFAR-10/100 datasets, reducing required FHE levels by nearly 5x.

Conclusion: The proposed approach successfully closes the accuracy gap between FHE-based and plaintext ResNet models while significantly accelerating inference through optimized polynomial approximations and structural improvements that eliminate bootstrapping.

Abstract: Recent work has made non-interactive privacy-preserving inference more
practical by running deep Convolution Neural Network (CNN) with Fully
Homomorphic Encryption (FHE). However, these methods remain limited by their
reliance on bootstrapping, a costly FHE operation applied across multiple
layers, severely slowing inference. They also depend on high-degree polynomial
approximations of non-linear activations, which increase multiplicative depth
and reduce accuracy by 2-5% compared to plaintext ReLU models. In this work, we
focus on ResNets, a widely adopted benchmark architecture in privacy-preserving
inference, and close the accuracy gap between their FHE-based non-interactive
models and plaintext counterparts, while also achieving faster inference than
existing methods. We use a quadratic polynomial approximation of ReLU, which
achieves the theoretical minimum multiplicative depth for non-linear
activations, along with a penalty-based training strategy. We further introduce
structural optimizations such as node fusing, weight redistribution, and tower
reuse. These optimizations reduce the required FHE levels in CNNs by nearly a
factor of five compared to prior work, allowing us to run ResNet models under
leveled FHE without bootstrapping. To further accelerate inference and recover
accuracy typically lost with polynomial approximations, we introduce parameter
clustering along with a joint strategy of data encoding layout and ensemble
techniques. Experiments with ResNet-18, ResNet-20, and ResNet-32 on CIFAR-10
and CIFAR-100 show that our approach achieves up to 4x faster private inference
than prior work with comparable accuracy to plaintext ReLU models.

</details>


### [343] [AntiFLipper: A Secure and Efficient Defense Against Label-Flipping Attacks in Federated Learning](https://arxiv.org/abs/2509.22873)
*Aashnan Rahman,Abid Hasan,Sherajul Arifin,Faisal Haque Bappy,Tahrim Hossain,Tariqul Islam,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.CR

TL;DR: AntiFLipper is a computationally efficient defense against label-flipping attacks in federated learning that uses client-side detection to reduce server burden while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to label-flipping attacks that can severely degrade model performance, and existing defenses have high computational overhead that limits practical deployment.

Method: AntiFLipper employs a novel client-side detection strategy for multi-class label-flipping attacks, significantly reducing the central server's computational burden during aggregation.

Result: Comprehensive evaluations show AntiFLipper achieves accuracy comparable to state-of-the-art defenses while requiring substantially fewer computational resources on the server side.

Conclusion: AntiFLipper effectively balances security and efficiency, addressing a critical gap in existing defenses and making it suitable for resource-constrained FL deployments.

Abstract: Federated learning (FL) enables privacy-preserving model training by keeping
data decentralized. However, it remains vulnerable to label-flipping attacks,
where malicious clients manipulate labels to poison the global model. Despite
their simplicity, these attacks can severely degrade model performance, and
defending against them remains challenging. We introduce AntiFLipper, a novel
and computationally efficient defense against multi-class label-flipping
attacks in FL. Unlike existing methods that ensure security at the cost of high
computational overhead, AntiFLipper employs a novel client-side detection
strategy, significantly reducing the central server's burden during
aggregation. Comprehensive empirical evaluations across multiple datasets under
different distributions demonstrate that AntiFLipper achieves accuracy
comparable to state-of-the-art defenses while requiring substantially fewer
computational resources in server side. By balancing security and efficiency,
AntiFLipper addresses a critical gap in existing defenses, making it
particularly suitable for resource-constrained FL deployments where both model
integrity and operational efficiency are essential.

</details>


### [344] [Towards Context-aware Mobile Privacy Notice: Implementation of A Deployable Contextual Privacy Policies Generator](https://arxiv.org/abs/2509.22900)
*Haochen Gong,Zhen Tao,Shidong Pan,Zhenchang Xing,Xiaoyu Sun*

Main category: cs.CR

TL;DR: PrivScan is the first deployable Contextual Privacy Policy SDK for Android that captures live app screenshots to identify GUI elements associated with personal data and displays concise privacy policies through a lightweight floating button.


<details>
  <summary>Details</summary>
Motivation: Lengthy and legally phrased privacy policies impede users' understanding of how mobile apps collect and process personal data. Previous Contextual Privacy Policy approaches were not deployable in real-world mobile environments.

Method: Uses a remote deployment architecture with multimodal backend pipeline and mobile client comprising five modular components. Captures live app screenshots to identify GUI elements, displays CPPs in concise format with low-friction floating button control.

Result: Average execution time of 9.15 seconds, demonstrating practical feasibility. The system successfully decouples backend processing from mobile client, reducing on-device resource demands and enabling cross-platform portability.

Conclusion: PrivScan provides the first deployable CPP SDK for Android that offers practical, on-demand privacy policy access with acceptable performance overhead, making contextual privacy policies feasible for real-world mobile environments.

Abstract: Lengthy and legally phrased privacy policies impede users' understanding of
how mobile applications collect and process personal data. Prior work proposed
Contextual Privacy Policies (CPPs) for mobile apps to display shorter policy
snippets only in the corresponding user interface contexts, but the pipeline
could not be deployable in real-world mobile environments. In this paper, we
present PrivScan, the first deployable CPP Software Development Kit (SDK) for
Android. It captures live app screenshots to identify GUI elements associated
with types of personal data and displays CPPs in a concise, user-facing format.
We provide a lightweight floating button that offers low-friction, on-demand
control. The architecture leverages remote deployment to decouple the
multimodal backend pipeline from a mobile client comprising five modular
components, thereby reducing on-device resource demands and easing
cross-platform portability. A feasibility-oriented evaluation shows an average
execution time of 9.15\,s, demonstrating the practicality of our approach. The
source code of PrivScan is available at https://github.com/buyanghc/PrivScan
and the demo video can be found at https://www.youtube.com/watch?v=ck-25otfyHc.

</details>


### [345] [Blockchain Voting System](https://arxiv.org/abs/2509.22965)
*Yousef Tahboub,Anthony Revilla,Jaydon Lynch,Greg Floyd*

Main category: cs.CR

TL;DR: A hybrid blockchain-based voting system that uses private blockchain for encrypted vote storage and public blockchain for tamper-evident hash anchoring, with one-time blind-signed tokens for voter anonymity and verifiable receipts.


<details>
  <summary>Details</summary>
Motivation: To enable secure online voting while ensuring voter confidence in ballot secrecy and protection against vote tampering, addressing the challenges of traditional electronic voting systems.

Method: Developed a hybrid blockchain model with encrypted votes on private blockchain maintained by election organizers and observers, periodic hash anchoring on public blockchain, one-time blind-signed tokens for anonymity, and verifiable receipts. Implemented prototype using Next.js, React, and Firebase.

Result: Successfully implemented a working demo with complete election workflow, demonstrating end-to-end functionality, accessibility, and cost efficiency. The system balances privacy, security, transparency, and practicality.

Conclusion: The research demonstrates the feasibility of secure, verifiable, and scalable online voting systems suitable for organizations ranging from small groups to larger institutions, using hybrid blockchain technology.

Abstract: Casting a ballot from a phone or laptop sounds appealing, but only if voters
can be confident their choice remains secret and results cannot be altered in
the dark. This paper proposes a hybrid blockchain-based voting model that
stores encrypted votes on a private blockchain maintained by election
organizers and neutral observers, while periodically anchoring hashes of these
votes onto a public blockchain as a tamper-evident seal. The system issues
voters one-time blind-signed tokens to protect anonymity, and provides receipts
so they can confirm their vote was counted. We implemented a live prototype
using common web technologies (Next.js, React, Firebase) to demonstrate
end-to-end functionality, accessibility, and cost efficiency. Our contributions
include developing a working demo, a complete election workflow, a hybrid
blockchain design, and a user-friendly interface that balances privacy,
security, transparency, and practicality. This research highlights the
feasibility of secure, verifiable, and scalable online voting for organizations
ranging from small groups to larger institutions.

</details>


### [346] [CryptoSRAM: Enabling High-Throughput Cryptography on MCUs via In-SRAM Computing](https://arxiv.org/abs/2509.22986)
*Jingyao Zhang,Elaheh Sadredini*

Main category: cs.CR

TL;DR: CryptoSRAM is an in-SRAM computing architecture that performs cryptographic operations directly within MCU's SRAM array, eliminating data movement bottlenecks and achieving significant performance improvements for IoT security.


<details>
  <summary>Details</summary>
Motivation: Current cryptographic solutions for IoT devices face performance and energy limitations due to data movement between memory and processing units, creating a need for more efficient approaches.

Method: The paper introduces CryptoSRAM, which repurposes the MCU's standard SRAM array into a massively parallel processing fabric that performs cryptographic operations directly within memory, leveraging physical addressing and DMA for seamless integration.

Result: CryptoSRAM achieves throughput improvements of up to 74× for AES and 67× for SHA3 compared to software implementations, and delivers up to 6× higher throughput than existing hardware accelerators for AES.

Conclusion: CryptoSRAM demonstrates a viable and efficient architecture for secure communication in next-generation IoT systems by eliminating data movement bottlenecks through in-SRAM computing.

Abstract: Secure communication is a critical requirement for Internet of Things (IoT)
devices, which are often based on Microcontroller Units (MCUs). Current
cryptographic solutions, which rely on software libraries or dedicated hardware
accelerators, are fundamentally limited by the performance and energy costs of
data movement between memory and processing units. This paper introduces
CryptoSRAM, an in-SRAM computing architecture that performs cryptographic
operations directly within the MCU's standard SRAM array. By repurposing the
memory array into a massively parallel processing fabric, CryptoSRAM eliminates
the data movement bottleneck. This approach is well-suited to MCUs, which
utilize physical addressing and Direct Memory Access (DMA) to manage SRAM,
allowing for seamless integration with minimal hardware overhead. Our analysis
shows that for common cryptographic kernels, CryptoSRAM achieves throughput
improvements of up to 74$\times$ and 67$\times$ for AES and SHA3, respectively,
compared to a software implementation. Furthermore, our solution delivers up to
6$\times$ higher throughput than existing hardware accelerators for AES.
CryptoSRAM demonstrates a viable and efficient architecture for secure
communication in next-generation IoT systems.

</details>


### [347] [LLM Watermark Evasion via Bias Inversion](https://arxiv.org/abs/2509.23019)
*Jeongyeon Hwang,Sangdon Park,Jungseul Ok*

Main category: cs.CR

TL;DR: The paper introduces BIRA, a model-agnostic attack that weakens watermark signals in LLM-generated text by suppressing likely watermarked tokens during rewriting, achieving over 99% evasion while preserving semantics.


<details>
  <summary>Details</summary>
Motivation: To rigorously understand and evaluate vulnerabilities in LLM watermarking systems under adversarial evasion, as current watermarking effectiveness in benign settings doesn't guarantee robustness against attacks.

Method: Proposes Bias-Inversion Rewriting Attack (BIRA) - a theoretically motivated, model-agnostic method that suppresses logits of likely watermarked tokens during LLM-based rewriting without knowledge of the underlying watermarking scheme.

Result: BIRA achieves over 99% evasion across recent watermarking methods while preserving the semantic content of the original text.

Conclusion: The results reveal a systematic vulnerability in current watermarking methods, emphasizing the need for stress testing and robust defenses against such attacks.

Abstract: Watermarking for large language models (LLMs) embeds a statistical signal
during generation to enable detection of model-produced text. While
watermarking has proven effective in benign settings, its robustness under
adversarial evasion remains contested. To advance a rigorous understanding and
evaluation of such vulnerabilities, we propose the \emph{Bias-Inversion
Rewriting Attack} (BIRA), which is theoretically motivated and model-agnostic.
BIRA weakens the watermark signal by suppressing the logits of likely
watermarked tokens during LLM-based rewriting, without any knowledge of the
underlying watermarking scheme. Across recent watermarking methods, BIRA
achieves over 99\% evasion while preserving the semantic content of the
original text. Beyond demonstrating an attack, our results reveal a systematic
vulnerability, emphasizing the need for stress testing and robust defenses.

</details>


### [348] [Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data](https://arxiv.org/abs/2509.23041)
*Zi Liang,Qingqing Ye,Xuan Liu,Yanyun Wang,Jianliang Xu,Haibo Hu*

Main category: cs.CR

TL;DR: This paper investigates security risks of synthetic data in LLM training, revealing strong resistance to existing poisoning attacks but proposing a novel Virus Infection Attack (VIA) that successfully propagates malicious content through synthetic data.


<details>
  <summary>Details</summary>
Motivation: Synthetic data is widely used to enhance LLM performance, but its potential security risks remain uninvestigated, particularly regarding poisoning and backdoor attacks.

Method: Systematically evaluates synthetic-data-integrated training against poisoning attacks, then introduces Virus Infection Attack (VIA) framework that conceals payload in protective shells and searches optimal hijacking points in benign samples.

Result: VIA significantly increases poisoning content presence in synthetic data and raises attack success rates to levels comparable to poisoned upstream models, demonstrating serious security vulnerabilities.

Conclusion: Synthetic data integration introduces significant security risks that current defenses cannot adequately address, requiring new security measures for synthetic data usage in LLM development.

Abstract: Synthetic data refers to artificial samples generated by models. While it has
been validated to significantly enhance the performance of large language
models (LLMs) during training and has been widely adopted in LLM development,
potential security risks it may introduce remain uninvestigated. This paper
systematically evaluates the resilience of synthetic-data-integrated training
paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal
that such a paradigm exhibits strong resistance to existing attacks, primarily
thanks to the different distribution patterns between poisoning data and
queries used to generate synthetic samples. To enhance the effectiveness of
these attacks and further investigate the security risks introduced by
synthetic data, we introduce a novel and universal attack framework, namely,
Virus Infection Attack (VIA), which enables the propagation of current attacks
through synthetic data even under purely clean queries. Inspired by the
principles of virus design in cybersecurity, VIA conceals the poisoning payload
within a protective "shell" and strategically searches for optimal hijacking
points in benign samples to maximize the likelihood of generating malicious
content. Extensive experiments on both data poisoning and backdoor attacks show
that VIA significantly increases the presence of poisoning content in synthetic
data and correspondingly raises the attack success rate (ASR) on downstream
models to levels comparable to those observed in the poisoned upstream models.

</details>


### [349] [FedBit: Accelerating Privacy-Preserving Federated Learning via Bit-Interleaved Packing and Cross-Layer Co-Design](https://arxiv.org/abs/2509.23091)
*Xiangchen Meng,Yangdi Lyu*

Main category: cs.CR

TL;DR: FedBit is a hardware/software co-designed framework that optimizes federated learning with fully homomorphic encryption, achieving 100x speedup in encryption and 60.7% lower communication overhead while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Federated learning with fully homomorphic encryption protects data privacy but suffers from high computational burden and ciphertext expansion, leading to significant resource and communication overhead.

Method: Proposed FedBit framework using bit-interleaved data packing to embed multiple model parameters into single ciphertext coefficients, with dedicated FPGA accelerator for cryptographic operations and optimized dataflow.

Result: Achieved two orders of magnitude speedup in encryption and 60.7% reduction in average communication overhead while maintaining high accuracy.

Conclusion: FedBit effectively addresses the computational and communication challenges of FHE in federated learning through hardware/software co-design and optimized data packing techniques.

Abstract: Federated learning (FL) with fully homomorphic encryption (FHE) effectively
safeguards data privacy during model aggregation by encrypting local model
updates before transmission, mitigating threats from untrusted servers or
eavesdroppers in transmission. However, the computational burden and ciphertext
expansion associated with homomorphic encryption can significantly increase
resource and communication overhead. To address these challenges, we propose
FedBit, a hardware/software co-designed framework optimized for the
Brakerski-Fan-Vercauteren (BFV) scheme. FedBit employs bit-interleaved data
packing to embed multiple model parameters into a single ciphertext
coefficient, thereby minimizing ciphertext expansion and maximizing
computational parallelism. Additionally, we integrate a dedicated FPGA
accelerator to handle cryptographic operations and an optimized dataflow to
reduce the memory overhead. Experimental results demonstrate that FedBit
achieves a speedup of two orders of magnitude in encryption and lowers average
communication overhead by 60.7%, while maintaining high accuracy.

</details>


### [350] [ICS-SimLab: A Containerized Approach for Simulating Industrial Control Systems for Cyber Security Research](https://arxiv.org/abs/2509.23305)
*Jaxson Brown,Duc-Son Pham,Sie-Teng Soh,Foad Motalebi,Sivaraman Eswaran,Mahathir Almashor*

Main category: cs.CR

TL;DR: ICS-SimLab is a Docker-based configurable ICS simulation environment that enables rapid development of different ICS architectures for security testing and IDS development.


<details>
  <summary>Details</summary>
Motivation: Current ICS testbeds are limited to single simulations, causing bias in security solutions. There's a need for systems that can efficiently simulate multiple ICS architectures to develop more robust intrusion detection systems.

Method: Developed ICS-SimLab using Docker containerization technology to create a highly configurable ICS simulation environment that supports different systems adhering to Purdue Enterprise Reference Architecture.

Result: Successfully created three virtual ICS simulations (solar panel smart grid, water bottle filling facility, intelligent electronic devices) and generated a dataset of malicious and benign network traffic from cyber-attacks on these simulations.

Conclusion: ICS-SimLab provides an effective framework for developing and testing security solutions across multiple ICS architectures, addressing the bias issue in current single-simulation approaches.

Abstract: Industrial Control Systems (ICSs) are complex interconnected systems used to
manage process control within industrial environments, such as chemical
processing plants and water treatment facilities. As the modern industrial
environment moves towards Internet-facing services, ICSs face an increased risk
of attacks that necessitates ICS-specific Intrusion Detection Systems (IDS).
The development of such IDS relies significantly on a simulated testbed as it
is unrealistic and sometimes hazardous to utilize an operational control
system. Whilst some testbeds have been proposed, they often use a limited
selection of virtual ICS simulations to test and verify cyber security
solutions. There is a lack of investigation done on developing systems that can
efficiently simulate multiple ICS architectures. Currently, the trend within
research involves developing security solutions on just one ICS simulation,
which can result in bias to its specific architecture. We present ICS-SimLab,
an end-to-end software suite that utilizes Docker containerization technology
to create a highly configurable ICS simulation environment. This software
framework enables researchers to rapidly build and customize different ICS
environments, facilitating the development of security solutions across
different systems that adhere to the Purdue Enterprise Reference Architecture.
To demonstrate its capability, we present three virtual ICS simulations: a
solar panel smart grid, a water bottle filling facility, and a system of
intelligent electronic devices. Furthermore, we run cyber-attacks on these
simulations and construct a dataset of recorded malicious and benign network
traffic to be used for IDS development.

</details>


### [351] [Detecting YouTube Scam Videos via Multimodal Signals and Policy Reasoning](https://arxiv.org/abs/2509.23418)
*Ummay Kulsum,Aafaq Sabir,Abhinaya S. B.,Anupam Das*

Main category: cs.CR

TL;DR: This paper presents the first systematic multimodal approach for YouTube scam detection, showing that combining text, audio, and visual features achieves the best performance (80.53% F1), outperforming text-only methods.


<details>
  <summary>Details</summary>
Motivation: YouTube's accessibility makes it vulnerable to scammers uploading deceptive content. Existing detection methods rely mainly on text and metadata, which are easy to evade and miss visual cues.

Method: Developed a multimodal framework integrating video titles, descriptions, audio transcripts, and video frames using fine-tuned BERT and LLaVA-Video models on a dataset with full video content and policy-grounded annotations.

Result: Text-only model achieved 76.61% F1, modest improvement with audio (77.98% F1), visual analysis alone achieved 79.61% F1, and multimodal integration achieved the highest performance at 80.53% F1.

Conclusion: Multimodal approaches significantly improve YouTube scam detection accuracy and provide interpretable reasoning based on content policies, supporting automated moderation applications. Validated on 6,374 real YouTube videos.

Abstract: YouTube has emerged as a dominant platform for both information dissemination
and entertainment. However, its vast accessibility has also made it a target
for scammers, who frequently upload deceptive or malicious content. Prior
research has documented a range of scam types, and detection approaches rely
primarily on textual or statistical metadata. Although effective to some
extent, these signals are easy to evade and potentially overlook other
modalities, such as visual cues.
  In this study, we present the first systematic investigation of multimodal
approaches for YouTube scam detection. Our dataset consolidates established
scam categories and augments them with full length video content and policy
grounded reasoning annotations. Our experimental evaluation demonstrates that a
text-only model using video titles and descriptions (fine-tuned BERT) achieves
moderate effectiveness (76.61% F1), with modest improvements when incorporating
audio transcripts (77.98% F1). In contrast, visual analysis using a fine-tuned
LLaVA-Video model yields stronger results (79.61% F1). Finally, a multimodal
framework that integrates titles, descriptions, and video frames achieves the
highest performance (80.53% F1). Beyond improving detection accuracy, our
multimodal framework produces interpretable reasoning grounded in YouTube
content policies, thereby enhancing transparency and supporting potential
applications in automated moderation. Moreover, we validate our approach on
in-the-wild YouTube data by analyzing 6,374 videos, thereby contributing a
valuable resource for future research on scam detection.

</details>


### [352] [StarveSpam: Mitigating Spam with Local Reputation in Permissionless Blockchains](https://arxiv.org/abs/2509.23427)
*Rowdy Chotkan,Bulat Nasrulin,Jérémie Decouchant,Johan Pouwelse*

Main category: cs.CR

TL;DR: StarveSpam is a decentralized reputation-based protocol that mitigates blockchain spam through local behavior tracking, peer scoring, and adaptive rate-limiting at the transaction relay layer, without requiring global consensus or protocol changes.


<details>
  <summary>Details</summary>
Motivation: Existing spam defenses like fee markets and staking requirements rely on economic deterrence but fail to distinguish between malicious and legitimate users, often excluding low-value honest activity. Blockchain networks need better spam protection that doesn't penalize legitimate users.

Method: StarveSpam operates at the transaction relay layer using three key components: local behavior tracking to monitor transaction patterns, peer scoring to evaluate sender reputation, and adaptive rate-limiting to suppress abusive actors. It works without global consensus, protocol modifications, or trusted infrastructure.

Result: Evaluation using real Ethereum data from an NFT spam event showed StarveSpam blocks over 95% of spam while dropping only 3% of honest traffic. It reduces the fraction of the network exposed to spam by 85% compared to existing rule-based methods.

Conclusion: StarveSpam provides a scalable and deployable alternative to traditional spam defenses, enabling more resilient and equitable blockchain infrastructure by effectively distinguishing between malicious and legitimate activity without economic barriers.

Abstract: Spam poses a growing threat to blockchain networks. Adversaries can easily
create multiple accounts to flood transaction pools, inflating fees and
degrading service quality. Existing defenses against spam, such as fee markets
and staking requirements, primarily rely on economic deterrence, which fails to
distinguish between malicious and legitimate users and often exclude low-value
but honest activity. To address these shortcomings, we present StarveSpam, a
decentralized reputation-based protocol that mitigates spam by operating at the
transaction relay layer. StarveSpam combines local behavior tracking, peer
scoring, and adaptive rate-limiting to suppress abusive actors, without
requiring global consensus, protocol changes, or trusted infrastructure. We
evaluate StarveSpam using real Ethereum data from a major NFT spam event and
show that it outperforms existing fee-based and rule-based defenses, allowing
each node to block over 95% of spam while dropping just 3% of honest traffic,
and reducing the fraction of the network exposed to spam by 85% compared to
existing rule-based methods. StarveSpam offers a scalable and deployable
alternative to traditional spam defenses, paving the way toward more resilient
and equitable blockchain infrastructure.

</details>


### [353] [MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction](https://arxiv.org/abs/2509.23459)
*Sepideh Abedini,Shubhankar Mohapatra,D. B. Emerson,Masoumeh Shafieinejad,Jesse C. Cresswell,Xi He*

Main category: cs.CR

TL;DR: MaskSQL is a privacy-preserving text-to-SQL framework that uses abstraction to mask sensitive information in LLM prompts, achieving performance close to state-of-the-art LLM models while protecting privacy.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns and regulatory constraints prevent using proprietary LLMs for sensitive text-to-SQL tasks, while small language models underperform on complex tasks. Need a solution that balances privacy and utility.

Method: Uses abstraction as privacy protection mechanism to mask sensitive information in LLM prompts, retaining essential information while discarding unnecessary details. Provides control over privacy-utility tradeoff.

Result: Outperforms leading SLM-based text-to-SQL models and achieves performance approaching state-of-the-art LLM-based models while preserving privacy.

Conclusion: MaskSQL effectively addresses the privacy-utility tradeoff in text-to-SQL tasks, enabling adoption in sensitive domains while maintaining competitive performance.

Abstract: Large language models (LLMs) have shown promising performance on tasks that
require reasoning, such as text-to-SQL, code generation, and debugging.
However, regulatory frameworks with strict privacy requirements constrain their
integration into sensitive systems. State-of-the-art LLMs are also proprietary,
costly, and resource-intensive, making local deployment impractical.
Consequently, utilizing such LLMs often requires sharing data with third-party
providers, raising privacy concerns and risking noncompliance with regulations.
Although fine-tuned small language models (SLMs) can outperform LLMs on certain
tasks and be deployed locally to mitigate privacy concerns, they underperform
on more complex tasks such as text-to-SQL translation. In this work, we
introduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a
privacy protection mechanism to mask sensitive information in LLM prompts.
Unlike redaction, which removes content entirely, or generalization, which
broadens tokens, abstraction retains essential information while discarding
unnecessary details, striking an effective privacy-utility balance for the
text-to-SQL task. Moreover, by providing mechanisms to control the
privacy-utility tradeoff, MaskSQL facilitates adoption across a broader range
of use cases. Our experimental results show that MaskSQL outperforms leading
SLM-based text-to-SQL models and achieves performance approaching
state-of-the-art LLM-based models, while preserving privacy.

</details>


### [354] [ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search](https://arxiv.org/abs/2509.23519)
*Zeyu Shen,Basileal Imana,Tong Wu,Chong Xiang,Prateek Mittal,Aleksandra Korolova*

Main category: cs.CR

TL;DR: ReliabilityRAG is a framework that enhances RAG system robustness against adversarial attacks by leveraging document reliability information through graph-based consistency checks and scalable sampling methods.


<details>
  <summary>Details</summary>
Motivation: RAG systems are vulnerable to retrieval corpus attacks like prompt injection, especially in search applications where reliability signals exist but need systematic protection.

Method: Uses graph theory to find consistent majority via Maximum Independent Set (MIS) prioritizing reliable documents, plus scalable weighted sampling for large retrieval sets.

Result: Superior robustness against attacks compared to prior methods, maintains high benign accuracy, and excels in long-form generation tasks.

Conclusion: Provides effective, provably robust defenses against retrieved corpus corruption in RAG systems.

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models by
grounding their outputs in external documents. These systems, however, remain
vulnerable to attacks on the retrieval corpus, such as prompt injection.
RAG-based search systems (e.g., Google's Search AI Overview) present an
interesting setting for studying and protecting against such threats, as
defense algorithms can benefit from built-in reliability signals -- like
document ranking -- and represent a non-LLM challenge for the adversary due to
decades of work to thwart SEO.
  Motivated by, but not limited to, this scenario, this work introduces
ReliabilityRAG, a framework for adversarial robustness that explicitly
leverages reliability information of retrieved documents.
  Our first contribution adopts a graph-theoretic perspective to identify a
"consistent majority" among retrieved documents to filter out malicious ones.
We introduce a novel algorithm based on finding a Maximum Independent Set (MIS)
on a document graph where edges encode contradiction. Our MIS variant
explicitly prioritizes higher-reliability documents and provides provable
robustness guarantees against bounded adversarial corruption under natural
assumptions. Recognizing the computational cost of exact MIS for large
retrieval sets, our second contribution is a scalable weighted sample and
aggregate framework. It explicitly utilizes reliability information, preserving
some robustness guarantees while efficiently handling many documents.
  We present empirical results showing ReliabilityRAG provides superior
robustness against adversarial attacks compared to prior methods, maintains
high benign accuracy, and excels in long-form generation tasks where prior
robustness-focused methods struggled. Our work is a significant step towards
more effective, provably robust defenses against retrieved corpus corruption in
RAG.

</details>


### [355] [Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting](https://arxiv.org/abs/2509.23571)
*Yuqiao Meng,Luoxi Tang,Feiyang Yu,Xi Li,Guanhua Yan,Ping Yang,Zhaohan Xi*

Main category: cs.CR

TL;DR: CyberTeam is a benchmark that structures threat hunting into a standardized workflow with 30 tasks and 9 operational modules to guide LLMs through blue teaming practice, showing improvements over open-ended reasoning approaches.


<details>
  <summary>Details</summary>
Motivation: As cyber threats grow more sophisticated, blue team defenders need better tools. LLMs show promise for threat analysis but their effectiveness in real-world threat-hunting scenarios hasn't been sufficiently explored.

Method: CyberTeam constructs a two-stage workflow: 1) models realistic threat-hunting workflows with task dependencies from threat attribution to incident response, 2) addresses each task through operational modules tailored to specific analytical requirements, transforming threat hunting into structured reasoning steps.

Result: Evaluation shows CyberTeam enables improvements over open-ended reasoning strategies for both leading LLMs and state-of-the-art cybersecurity agents, while revealing limitations of open-ended reasoning in real-world threat hunting.

Conclusion: Standardized workflow design through CyberTeam significantly enhances LLM performance in blue team threat-hunting scenarios compared to unstructured approaches.

Abstract: As cyber threats continue to grow in scale and sophistication, blue team
defenders increasingly require advanced tools to proactively detect and
mitigate risks. Large Language Models (LLMs) offer promising capabilities for
enhancing threat analysis. However, their effectiveness in real-world blue team
threat-hunting scenarios remains insufficiently explored. This paper presents
CyberTeam, a benchmark designed to guide LLMs in blue teaming practice.
CyberTeam constructs a standardized workflow in two stages. First, it models
realistic threat-hunting workflows by capturing the dependencies among
analytical tasks from threat attribution to incident response. Next, each task
is addressed through a set of operational modules tailored to its specific
analytical requirements. This transforms threat hunting into a structured
sequence of reasoning steps, with each step grounded in a discrete operation
and ordered according to task-specific dependencies. Guided by this framework,
LLMs are directed to perform threat-hunting tasks through modularized steps.
Overall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs
through standardized threat analysis. We evaluate both leading LLMs and
state-of-the-art cybersecurity agents, comparing CyberTeam against open-ended
reasoning strategies. Our results highlight the improvements enabled by
standardized design, while also revealing the limitations of open-ended
reasoning in real-world threat hunting.

</details>


### [356] [Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence](https://arxiv.org/abs/2509.23573)
*Yuqiao Meng,Luoxi Tang,Feiyang Yu,Jinyuan Jia,Guanhua Yan,Ping Yang,Zhaohan Xi*

Main category: cs.CR

TL;DR: This paper investigates intrinsic vulnerabilities of LLMs in cyber threat intelligence (CTI), revealing three fundamental limitations: spurious correlations, contradictory knowledge, and constrained generalization that hinder practical deployment.


<details>
  <summary>Details</summary>
Motivation: While LLMs are widely used to assist security analysts in cyber threat intelligence tasks like vulnerability assessment and incident response, significant performance gaps persist in practical deployments, necessitating investigation of LLMs' intrinsic vulnerabilities in CTI.

Method: The authors use large-scale evaluations across multiple CTI benchmarks and real-world threat reports, introducing a novel categorization methodology that integrates stratification, autoregressive refinement, and human-in-the-loop supervision to reliably analyze failure instances.

Result: Through extensive experiments and human inspections, the study reveals three fundamental vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization that limit LLMs' effectiveness in supporting CTI tasks.

Conclusion: The paper provides actionable insights for designing more robust LLM-powered CTI systems to facilitate future research in this critical domain.

Abstract: Large Language Models (LLMs) are intensively used to assist security analysts
in counteracting the rapid exploitation of cyber threats, wherein LLMs offer
cyber threat intelligence (CTI) to support vulnerability assessment and
incident response. While recent work has shown that LLMs can support a wide
range of CTI tasks such as threat analysis, vulnerability detection, and
intrusion defense, significant performance gaps persist in practical
deployments. In this paper, we investigate the intrinsic vulnerabilities of
LLMs in CTI, focusing on challenges that arise from the nature of the threat
landscape itself rather than the model architecture. Using large-scale
evaluations across multiple CTI benchmarks and real-world threat reports, we
introduce a novel categorization methodology that integrates stratification,
autoregressive refinement, and human-in-the-loop supervision to reliably
analyze failure instances. Through extensive experiments and human inspections,
we reveal three fundamental vulnerabilities: spurious correlations,
contradictory knowledge, and constrained generalization, that limit LLMs in
effectively supporting CTI. Subsequently, we provide actionable insights for
designing more robust LLM-powered CTI systems to facilitate future research.

</details>


### [357] [StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data](https://arxiv.org/abs/2509.23594)
*Yixu Wang,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CR

TL;DR: This paper introduces LoRA extraction attacks targeting Parameter-Efficient Fine-Tuned models and proposes StolenLoRA, an effective extraction method using synthetic data and semi-supervised learning that achieves up to 96.60% success rate with minimal queries.


<details>
  <summary>Details</summary>
Motivation: The compactness of LoRA adaptations creates new security vulnerabilities, particularly making them susceptible to model extraction attacks that can steal the functionality of customized models.

Method: Proposed StolenLoRA method that trains substitute models using synthetic data generated by LLM prompts, combined with Disagreement-based Semi-supervised Learning (DSL) to maximize information from limited queries.

Result: StolenLoRA achieves up to 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where attacker and victim models use different pre-trained backbones.

Conclusion: LoRA-adapted models are specifically vulnerable to extraction attacks, highlighting the urgent need for robust defense mechanisms tailored to PEFT methods, with preliminary defense strategies showing potential.

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed
vision model adaptation, enabling the rapid deployment of customized models.
However, the compactness of LoRA adaptations introduces new safety concerns,
particularly their vulnerability to model extraction attacks. This paper
introduces a new focus of model extraction attacks named LoRA extraction that
extracts LoRA-adaptive models based on a public pre-trained model. We then
propose a novel extraction method called StolenLoRA which trains a substitute
model to extract the functionality of a LoRA-adapted model using synthetic
data. StolenLoRA leverages a Large Language Model to craft effective prompts
for data generation, and it incorporates a Disagreement-based Semi-supervised
Learning (DSL) strategy to maximize information gain from limited queries. Our
experiments demonstrate the effectiveness of StolenLoRA, achieving up to a
96.60% attack success rate with only 10k queries, even in cross-backbone
scenarios where the attacker and victim models utilize different pre-trained
backbones. These findings reveal the specific vulnerability of LoRA-adapted
models to this type of extraction and underscore the urgent need for robust
defense mechanisms tailored to PEFT methods. We also explore a preliminary
defense strategy based on diversified LoRA deployments, highlighting its
potential to mitigate such attacks.

</details>


### [358] [AutoML in Cybersecurity: An Empirical Study](https://arxiv.org/abs/2509.23621)
*Sherif Saad,Kevin Shi,Mohammed Mamun,Hythem Elmiligi*

Main category: cs.CR

TL;DR: Systematic evaluation of 8 AutoML frameworks on 11 cybersecurity datasets reveals performance variability, no consistent best tool, and key challenges including adversarial vulnerability and model drift.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability of AutoML in cybersecurity domains where its effectiveness remains underexplored, despite AutoML's promise for automating ML pipeline design.

Method: Evaluated eight open-source AutoML frameworks across 11 publicly available cybersecurity datasets covering intrusion detection, malware classification, phishing, fraud detection, and spam filtering.

Result: Substantial performance variability across tools and datasets with no single consistently superior solution; AutoML tools frequently favor tree-based models with overfitting risks; paradigm shift from model selection to framework selection.

Conclusion: Identified key challenges including adversarial vulnerability, model drift, and inadequate feature engineering; provided best practices and research directions to improve robustness, interpretability, and trust in AutoML for cybersecurity.

Abstract: Automated machine learning (AutoML) has emerged as a promising paradigm for
automating machine learning (ML) pipeline design, broadening AI adoption. Yet
its reliability in complex domains such as cybersecurity remains underexplored.
This paper systematically evaluates eight open-source AutoML frameworks across
11 publicly available cybersecurity datasets, spanning intrusion detection,
malware classification, phishing, fraud detection, and spam filtering. Results
show substantial performance variability across tools and datasets, with no
single solution consistently superior. A paradigm shift is observed: the
challenge has moved from selecting individual ML models to identifying the most
suitable AutoML framework, complicated by differences in runtime efficiency,
automation capabilities, and supported features. AutoML tools frequently favor
tree-based models, which perform well but risk overfitting and limit
interpretability. Key challenges identified include adversarial vulnerability,
model drift, and inadequate feature engineering. We conclude with best
practices and research directions to strengthen robustness, interpretability,
and trust in AutoML for high-stakes cybersecurity applications.

</details>


### [359] [A First Look at Privacy Risks of Android Task-executable Voice Assistant Applications](https://arxiv.org/abs/2509.23680)
*Shidong Pan,Yikai Ge,Xiaoyu Sun*

Main category: cs.CR

TL;DR: This paper conducts a comprehensive empirical study on privacy risks in Android task-executable voice assistants, revealing widespread inconsistencies in privacy declarations and uncovering three significant privacy threat models.


<details>
  <summary>Details</summary>
Motivation: With the growing popularity of foundation AI technologies and task-executable voice assistants, there is a research gap in examining privacy risks from task-execution patterns in a holistic manner, despite their prevalence and autonomy.

Method: The study collects ten mainstream voice assistants, analyzes their operational characteristics, and cross-checks their privacy declarations across six sources including privacy labels, policies, and manifest files.

Result: Findings reveal widespread inconsistencies in privacy declarations and uncover three significant privacy threat models: privacy misdisclosure in mega apps, privilege escalation via inter-application interactions, and abuse of Google system applications.

Conclusion: The study contributes actionable recommendations for practitioners and underscores the broader relevance of these privacy risks to emerging autonomous AI agents.

Abstract: With the development of foundation AI technologies, task-executable voice
assistants (VAs) have become more popular, enhancing user convenience and
expanding device functionality. Android task-executable VAs are applications
that are capable of understanding complex tasks and performing corresponding
operations. Given their prevalence and great autonomy, there is no existing
work examine the privacy risks within the voice assistants from the
task-execution pattern in a holistic manner. To fill this research gap, this
paper presents a user-centric comprehensive empirical study on privacy risks in
Android task-executable VA applications. We collect ten mainstream VAs as our
research target and analyze their operational characteristics. We then
cross-check their privacy declarations across six sources, including privacy
labels, policies, and manifest files, and our findings reveal widespread
inconsistencies. Moreover, we uncover three significant privacy threat models:
(1) privacy misdisclosure in mega apps, where integrated mini apps such as
Alexa skills are inadequately represented; (2) privilege escalation via
inter-application interactions, which exploit Android's communication
mechanisms to bypass user consent; and (3) abuse of Google system applications,
enabling apps to evade the declaration of dangerous permissions. Our study
contributes actionable recommendations for practitioners and underscores
broader relevance of these privacy risks to emerging autonomous AI agents.

</details>


### [360] [GPM: The Gaussian Pancake Mechanism for Planting Undetectable Backdoors in Differential Privacy](https://arxiv.org/abs/2509.23834)
*Haochen Sun,Xi He*

Main category: cs.CR

TL;DR: The paper presents Gaussian Pancake Mechanism (GPM), a backdoor attack that is computationally indistinguishable from the standard Gaussian Mechanism but provides arbitrarily weaker differential privacy guarantees, enabling covert privacy degradation.


<details>
  <summary>Details</summary>
Motivation: To investigate whether passive defects in DP implementations can be actively exploited for privacy attacks while maintaining covertness, given several incidents of unintended privacy loss due to numerical issues and configuration errors.

Method: Developed the Gaussian Pancake Mechanism (GPM) that mimics the Gaussian Mechanism but with weaker statistical DP guarantees, formally proving its covertness and characterizing statistical leakage.

Result: GPM enables near-perfect success rates in distinguishing attacks under suitable parameters, both theoretically and empirically, demonstrating the feasibility of covert privacy degradation attacks.

Conclusion: Highlights the critical need for transparent, open-source DP libraries and rigorous formal verification of implementations to prevent undetectable privacy compromises in real-world systems.

Abstract: Differential privacy (DP) has become the gold standard for preserving
individual privacy in data analysis. However, an implicit yet fundamental
assumption underlying these rigorous privacy guarantees is the correct
implementation and execution of DP mechanisms. Several incidents of unintended
privacy loss have occurred due to numerical issues and inappropriate
configurations of DP software, which have been successfully exploited in
privacy attacks. To better understand the seriousness of defective DP software,
we ask the following question: is it possible to elevate these passive defects
into active privacy attacks while maintaining covertness?
  To address this question, we present the Gaussian pancake mechanism (GPM), a
novel mechanism that is computationally indistinguishable from the widely used
Gaussian mechanism (GM), yet exhibits arbitrarily weaker statistical DP
guarantees. This unprecedented separation enables a new class of backdoor
attacks: by indistinguishably passing off as the authentic GM, GPM can covertly
degrade statistical privacy. Unlike the unintentional privacy loss caused by
GM's numerical issues, GPM is an adversarial yet undetectable backdoor attack
against data privacy. We formally prove GPM's covertness, characterize its
statistical leakage, and demonstrate a concrete distinguishing attack that can
achieve near-perfect success rates under suitable parameter choices, both
theoretically and empirically.
  Our results underscore the importance of using transparent, open-source DP
libraries and highlight the need for rigorous scrutiny and formal verification
of DP implementations to prevent subtle, undetectable privacy compromises in
real-world systems.

</details>


### [361] [Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack](https://arxiv.org/abs/2509.23871)
*Yukun Chen,Boheng Li,Yu Yuan,Leyi Qi,Yiming Li,Tianwei Zhang,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: This paper introduces distillation-conditional backdoor attacks (DCBAs), a novel threat where dormant backdoors are injected into teacher models that activate only in student models during knowledge distillation, bypassing traditional security checks.


<details>
  <summary>Details</summary>
Motivation: To address a critical vulnerability in knowledge distillation where third-party teacher models may contain undetectable backdoors that become active only during the distillation process, even with clean datasets.

Method: Proposes SCAR method using bilevel optimization: inner optimization simulates KD process with surrogate student model, outer optimization uses surrogate outputs to optimize teacher model for conditional backdoor implantation, employing implicit differentiation with pre-optimized trigger injection.

Result: Extensive experiments show SCAR is effective across diverse datasets, model architectures, and KD techniques, and resists existing backdoor detection methods.

Conclusion: Reveals a significant overlooked vulnerability in knowledge distillation processes where dormant backdoors in teacher models can activate in student models, highlighting the need for new security measures.

Abstract: Knowledge distillation (KD) is a vital technique for deploying deep neural
networks (DNNs) on resource-constrained devices by transferring knowledge from
large teacher models to lightweight student models. While teacher models from
third-party platforms may undergo security verification (\eg, backdoor
detection), we uncover a novel and critical threat: distillation-conditional
backdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into
teacher models, which become activated in student models via the KD process,
even with clean distillation datasets. While the direct extension of existing
methods is ineffective for DCBA, we implement this attack by formulating it as
a bilevel optimization problem and proposing a simple yet effective method
(\ie, SCAR). Specifically, the inner optimization simulates the KD process by
optimizing a surrogate student model, while the outer optimization leverages
outputs from this surrogate to optimize the teacher model for implanting the
conditional backdoor. Our SCAR addresses this complex optimization utilizing an
implicit differentiation algorithm with a pre-optimized trigger injection
function. Extensive experiments across diverse datasets, model architectures,
and KD techniques validate the effectiveness of our SCAR and its resistance
against existing backdoor detection, highlighting a significant yet previously
overlooked vulnerability in the KD process. Our code is available at
https://github.com/WhitolfChen/SCAR.

</details>


### [362] [Binary Diff Summarization using Large Language Models](https://arxiv.org/abs/2509.23970)
*Meet Udeshi,Venkata Sai Charan Putrevu,Prashanth Krishnamurthy,Prashant Anantharaman,Sean Carrick,Ramesh Karri,Farshad Khorrami*

Main category: cs.CR

TL;DR: A novel framework using LLMs for binary diff summarization with functional sensitivity score (FSS) to detect malware in software supply chains, achieving high precision (0.98) and recall (0.64) on benchmark tests.


<details>
  <summary>Details</summary>
Motivation: Security of software supply chains is critical to prevent malicious code injection in software updates. Binary differential analysis combined with LLM summarization can improve focus on critical changes and enable automated malware detection.

Method: Proposed framework combines LLM-based binary code summarization with binary diffing. Introduces functional sensitivity score (FSS) for automated triage of sensitive binary functions. Created benchmark with 3 malware types injected into 6 open-source projects, generating 104 binary versions and 392 binary diffs.

Result: Achieved precision of 0.98 and recall of 0.64 for malware detection. FSS separation of 3.0 points between malicious and benign functions. Successfully detected injected backdoor functions in real-world XZ utils supply chain attack case study.

Conclusion: The framework effectively detects malware in software supply chains with high accuracy and low false positives. FSS categorization successfully classifies sensitive functions, demonstrating practical utility in real-world supply chain security scenarios.

Abstract: Security of software supply chains is necessary to ensure that software
updates do not contain maliciously injected code or introduce vulnerabilities
that may compromise the integrity of critical infrastructure. Verifying the
integrity of software updates involves binary differential analysis (binary
diffing) to highlight the changes between two binary versions by incorporating
binary analysis and reverse engineering. Large language models (LLMs) have been
applied to binary analysis to augment traditional tools by producing natural
language summaries that cybersecurity experts can grasp for further analysis.
Combining LLM-based binary code summarization with binary diffing can improve
the LLM's focus on critical changes and enable complex tasks such as automated
malware detection. To address this, we propose a novel framework for binary
diff summarization using LLMs. We introduce a novel functional sensitivity
score (FSS) that helps with automated triage of sensitive binary functions for
downstream detection tasks. We create a software supply chain security
benchmark by injecting 3 different malware into 6 open-source projects which
generates 104 binary versions, 392 binary diffs, and 46,023 functions. On this,
our framework achieves a precision of 0.98 and recall of 0.64 for malware
detection, displaying high accuracy with low false positives. Across malicious
and benign functions, we achieve FSS separation of 3.0 points, confirming that
FSS categorization can classify sensitive functions. We conduct a case study on
the real-world XZ utils supply chain attack; our framework correctly detects
the injected backdoor functions with high FSS.

</details>


### [363] [Multiple Concurrent Proposers: Why and How](https://arxiv.org/abs/2509.23984)
*Pranav Garimidi,Joachim Neu,Max Resnick*

Main category: cs.CR

TL;DR: The paper proposes a multiple concurrent proposer (MCP) protocol to address MEV issues in blockchains by providing selective-censorship resistance and hiding properties.


<details>
  <summary>Details</summary>
Motivation: Traditional single-proposer blockchains suffer from miner extractable value (MEV), where validators exploit their monopoly on transaction inclusion and ordering to extract rents from users. Current application-layer solutions require auctions but lack the necessary consensus protocol properties.

Method: The authors propose a multiple concurrent proposer (MCP) protocol that offers selective-censorship resistance and hiding properties, which are essential for running efficient on-chain auctions.

Result: The MCP protocol provides the required properties to prevent adversaries from selectively delaying transactions or seeing transaction contents before confirmation.

Conclusion: The proposed MCP protocol addresses MEV issues by enabling efficient on-chain auctions through selective-censorship resistance and hiding properties in the consensus layer.

Abstract: Traditional single-proposer blockchains suffer from miner extractable value
(MEV), where validators exploit their serial monopoly on transaction inclusion
and ordering to extract rents from users. While there have been many
developments at the application layer to reduce the impact of MEV, these
approaches largely require auctions as a subcomponent. Running auctions
efficiently on chain requires two key properties of the underlying consensus
protocol: selective-censorship resistance and hiding. These properties
guarantee that an adversary can neither selectively delay transactions nor see
their contents before they are confirmed. We propose a multiple concurrent
proposer (MCP) protocol offering exactly these properties.

</details>


### [364] [Automated Vulnerability Validation and Verification: A Large Language Model Approach](https://arxiv.org/abs/2509.24037)
*Alireza Lotfi,Charalampos Katsis,Elisa Bertino*

Main category: cs.CR

TL;DR: This paper presents an end-to-end pipeline using generative AI (LLMs) and Retrieval-Augmented Generation (RAG) to systematically orchestrate and reproduce attacks for known software vulnerabilities from CVE data, creating containerized environments and exploit code.


<details>
  <summary>Details</summary>
Motivation: The lack of high-quality datasets capturing diverse exploit behavior limits effective vulnerability assessment and mitigation, despite software vulnerabilities being critical security challenges.

Method: Extracts CVE information from NVD, augments with external knowledge using RAG, automates creation of containerized environments and exploit code, iteratively refines artifacts, validates attacks with test cases, and supports multi-container setups.

Result: Successfully demonstrated across different vulnerability types (memory overflows, DoS, RCE) spanning diverse programming languages and years, uncovering inconsistencies in CVE descriptions and emphasizing need for better verification.

Conclusion: The approach is model-agnostic, works across multiple LLMs, and provides the first systematic method to orchestrate and exploit known vulnerabilities in containerized environments by combining LLM reasoning with CVE data and RAG context enrichment.

Abstract: Software vulnerabilities remain a critical security challenge, providing
entry points for attackers into enterprise networks. Despite advances in
security practices, the lack of high-quality datasets capturing diverse exploit
behavior limits effective vulnerability assessment and mitigation. This paper
introduces an end-to-end multi-step pipeline leveraging generative AI,
specifically large language models (LLMs), to address the challenges of
orchestrating and reproducing attacks to known software vulnerabilities. Our
approach extracts information from CVE disclosures in the National
Vulnerability Database, augments it with external public knowledge (e.g.,
threat advisories, code snippets) using Retrieval-Augmented Generation (RAG),
and automates the creation of containerized environments and exploit code for
each vulnerability. The pipeline iteratively refines generated artifacts,
validates attack success with test cases, and supports complex multi-container
setups. Our methodology overcomes key obstacles, including noisy and incomplete
vulnerability descriptions, by integrating LLMs and RAG to fill information
gaps. We demonstrate the effectiveness of our pipeline across different
vulnerability types, such as memory overflows, denial of service, and remote
code execution, spanning diverse programming languages, libraries and years. In
doing so, we uncover significant inconsistencies in CVE descriptions,
emphasizing the need for more rigorous verification in the CVE disclosure
process. Our approach is model-agnostic, working across multiple LLMs, and we
open-source the artifacts to enable reproducibility and accelerate security
research. To the best of our knowledge, this is the first system to
systematically orchestrate and exploit known vulnerabilities in containerized
environments by combining general-purpose LLM reasoning with CVE data and
RAG-based context enrichment.

</details>


### [365] [An Ensemble Framework for Unbiased Language Model Watermarking](https://arxiv.org/abs/2509.24043)
*Yihan Wu,Ruibo Chen,Georgios Milis,Heng Huang*

Main category: cs.CR

TL;DR: ENS is an ensemble framework that enhances detectability and robustness of unbiased watermarking for LLMs while preserving unbiasedness through sequential composition of multiple watermark instances.


<details>
  <summary>Details</summary>
Motivation: Existing unbiased watermarking schemes suffer from weak detection power and limited robustness, especially under short text lengths or distributional perturbations.

Method: Sequentially composes multiple independent watermark instances with distinct keys to amplify watermark signal while theoretically preserving unbiasedness.

Result: Substantially reduces tokens needed for reliable detection and increases resistance to smoothing and paraphrasing attacks without compromising generation quality.

Conclusion: ENS provides a practical solution for enhancing watermark detectability and robustness while maintaining the theoretical guarantees of unbiased watermarking.

Abstract: As large language models become increasingly capable and widely deployed,
verifying the provenance of machine-generated content is critical to ensuring
trust, safety, and accountability. Watermarking techniques have emerged as a
promising solution by embedding imperceptible statistical signals into the
generation process. Among them, unbiased watermarking is particularly
attractive due to its theoretical guarantee of preserving the language model's
output distribution, thereby avoiding degradation in fluency or detectability
through distributional shifts. However, existing unbiased watermarking schemes
often suffer from weak detection power and limited robustness, especially under
short text lengths or distributional perturbations. In this work, we propose
ENS, a novel ensemble framework that enhances the detectability and robustness
of logits-based unbiased watermarks while strictly preserving their
unbiasedness. ENS sequentially composes multiple independent watermark
instances, each governed by a distinct key, to amplify the watermark signal. We
theoretically prove that the ensemble construction remains unbiased in
expectation and demonstrate how it improves the signal-to-noise ratio for
statistical detectors. Empirical evaluations on multiple LLM families show that
ENS substantially reduces the number of tokens needed for reliable detection
and increases resistance to smoothing and paraphrasing attacks without
compromising generation quality.

</details>


### [366] [Analyzing and Evaluating Unbiased Language Model Watermark](https://arxiv.org/abs/2509.24048)
*Yihan Wu,Xuehao Cui,Ruibo Chen,Heng Huang*

Main category: cs.CR

TL;DR: UWbench is the first open-source benchmark for evaluating unbiased watermarking methods, addressing distributional bias accumulation and inconsistent robustness evaluations through theoretical analysis and a three-axis evaluation protocol.


<details>
  <summary>Details</summary>
Motivation: To address the issues of distributional bias accumulation in unbiased watermarks and inconsistent robustness evaluations across studies, providing a standardized platform for watermarking algorithm evaluation.

Method: Proposes UWbench framework with statistical metrics for distribution drift, impossibility theorem proof, formal robustness analysis, and a three-axis evaluation protocol (unbiasedness, detectability, robustness).

Result: Shows that token modification attacks provide more stable robustness assessments than paraphrasing methods, and proves no unbiased watermark can perfectly preserve distribution under infinite queries.

Conclusion: UWbench offers a standardized, reproducible platform for advancing the design and evaluation of unbiased watermarking algorithms, addressing key limitations in current evaluation practices.

Abstract: Verifying the authenticity of AI-generated text has become increasingly
important with the rapid advancement of large language models, and unbiased
watermarking has emerged as a promising approach due to its ability to preserve
output distribution without degrading quality. However, recent work reveals
that unbiased watermarks can accumulate distributional bias over multiple
generations and that existing robustness evaluations are inconsistent across
studies. To address these issues, we introduce UWbench, the first open-source
benchmark dedicated to the principled evaluation of unbiased watermarking
methods. Our framework combines theoretical and empirical contributions: we
propose a statistical metric to quantify multi-batch distribution drift, prove
an impossibility result showing that no unbiased watermark can perfectly
preserve the distribution under infinite queries, and develop a formal analysis
of robustness against token-level modification attacks. Complementing this
theory, we establish a three-axis evaluation protocol: unbiasedness,
detectability, and robustness, and show that token modification attacks provide
more stable robustness assessments than paraphrasing-based methods. Together,
UWbench offers the community a standardized and reproducible platform for
advancing the design and evaluation of unbiased watermarking algorithms.

</details>


### [367] [DNS in the Time of Curiosity: A Tale of Collaborative User Privacy Protection](https://arxiv.org/abs/2509.24153)
*Philip Sjösvärd,Hongyu Jin,Panos Papadimitratos*

Main category: cs.CR

TL;DR: DNS privacy is threatened by curious resolvers that can track user activity. Encryption and policies don't prevent data collection. The paper proposes a user-driven approach to protect privacy without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Public DNS resolvers can collect massive user activity data despite encryption and privacy policies. Current solutions can't prevent collusion between entities, creating a need for technical safeguards.

Method: A user-driven approach that reduces exposure to DNS services while maintaining low latency, bandwidth, memory/storage, and computational overhead.

Result: The paper demonstrates that privacy protection in DNS is achievable without performance degradation through proper architectural design.

Conclusion: User privacy in DNS can be safeguarded through a carefully designed user-driven approach that balances privacy protection with system performance requirements.

Abstract: The Domain Name System (DNS) is central to all Internet user activity,
resolving accessed domain names into Internet Protocol (IP) addresses. As a
result, curious DNS resolvers can learn everything about Internet users'
interests. Public DNS resolvers are rising in popularity, offering low-latency
resolution, high reliability, privacy-preserving policies, and support for
encrypted DNS queries. However, client-resolver traffic encryption,
increasingly deployed to protect users from eavesdroppers, does not protect
users against curious resolvers. Similarly, privacy-preserving policies are
based solely on written commitments and do not provide technical safeguards.
Although DNS query relay schemes can separate duties to limit data accessible
by each entity, they cannot prevent colluding entities from sharing user
traffic logs. Thus, a key challenge remains: organizations operating public DNS
resolvers, accounting for the majority of DNS resolutions, can potentially
collect and analyze massive volumes of Internet user activity data. With DNS
infrastructure that cannot be fully trusted, can we safeguard user privacy? We
answer positively and advocate for a user-driven approach to reduce exposure to
DNS services. We will discuss key ideas of the proposal, which aims to achieve
a high level of privacy without sacrificing performance: maintaining low
latency, network bandwidth, memory/storage overhead, and computational
overhead.

</details>


### [368] [Fundamental Limit of Discrete Distribution Estimation under Utility-Optimized Local Differential Privacy](https://arxiv.org/abs/2509.24173)
*Sun-Moon Yoon,Hyun-Young Park,Seung-Hyun Nam,Si-Hyeon Lee*

Main category: cs.CR

TL;DR: This paper provides a complete characterization of the fundamental privacy-utility trade-off for discrete distribution estimation under utility-optimized local differential privacy (ULDP), which allows more accurate inference on non-sensitive data while enforcing LDP on sensitive data.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental limits of estimation accuracy under ULDP constraints, which extend standard LDP by allowing better utility for non-sensitive data while maintaining privacy for sensitive data.

Method: The converse proof uses a generalized uniform asymptotic Cramér-Rao lower bound, reduction to extremal ULDP mechanisms, and a novel distribution decomposition technique. For achievability, the authors propose utility-optimized block design (uBD) schemes that modify the optimal block design mechanism for standard LDP.

Result: The paper establishes tight characterization of estimation accuracy achievable under ULDP constraints, showing the fundamental privacy-utility trade-off.

Conclusion: The results provide a complete understanding of the privacy-utility trade-off in ULDP and reveal new insights into optimal mechanisms for privacy-preserving statistical inference.

Abstract: We study the problem of discrete distribution estimation under
utility-optimized local differential privacy (ULDP), which enforces local
differential privacy (LDP) on sensitive data while allowing more accurate
inference on non-sensitive data. In this setting, we completely characterize
the fundamental privacy-utility trade-off. The converse proof builds on several
key ideas, including a generalized uniform asymptotic Cram\'er-Rao lower bound,
a reduction showing that it suffices to consider a newly defined class of
extremal ULDP mechanisms, and a novel distribution decomposition technique
tailored to ULDP constraints. For the achievability, we propose a class of
utility-optimized block design (uBD) schemes, obtained as nontrivial
modifications of the block design mechanism known to be optimal under standard
LDP constraints, while incorporating the distribution decomposition idea used
in the converse proof and a score-based linear estimator. These results provide
a tight characterization of the estimation accuracy achievable under ULDP and
reveal new insights into the structure of optimal mechanisms for
privacy-preserving statistical inference.

</details>


### [369] [LLUAD: Low-Latency User-Anonymized DNS](https://arxiv.org/abs/2509.24174)
*Philip Sjösvärd,Hongyu Jin,Panos Papadimitratos*

Main category: cs.CR

TL;DR: LLUAD is a DNS privacy solution that locally stores popular DNS records on user devices using a privacy-preserving Popularity List, reducing reliance on curious DNS resolvers while improving access times.


<details>
  <summary>Details</summary>
Motivation: DNS exposes user web activity to honest-but-curious DNS servers/resolvers, especially public DNS providers who can track and analyze massive user data. Existing encrypted DNS solutions don't address the resolver itself being the privacy threat.

Method: LLUAD uses a locally stored Popularity List on user devices, formed through a client-driven Voting Mix Network that anonymizes user votes. The list is proactively retrieved from public servers and updated based on user popularity votes while maintaining privacy.

Result: The system achieves near-zero-trust DNS privacy compatible with existing infrastructure, improves privacy by reducing resolver exposure, and reduces web content access times through local caching.

Conclusion: LLUAD provides an effective DNS privacy solution that addresses the fundamental limitation of resolver-based threats while maintaining compatibility with existing DNS infrastructure and improving performance.

Abstract: The Domain Name System (DNS) is involved in practically all web activity,
translating easy-to-remember domain names into Internet Protocol (IP)
addresses. Due to its central role on the Internet, DNS exposes user web
activity in detail. The privacy challenge is honest-but-curious DNS
servers/resolvers providing the translation/lookup service. In particular, with
the majority of DNS queries handled by public DNS resolvers, the organizations
running them can track, collect, and analyze massive user activity data.
Existing solutions that encrypt DNS traffic between clients and resolvers are
insufficient, as the resolver itself is the privacy threat. While DNS query
relays separate duties among multiple entities, to limit the data accessible by
each entity, they cannot prevent colluding entities from sharing user traffic
logs. To achieve near-zero-trust DNS privacy compatible with the existing DNS
infrastructure, we propose LLUAD: it locally stores a Popularity List, the most
popular DNS records, on user devices, formed in a privacy-preserving manner
based on user interests. In this way, LLUAD can both improve privacy and reduce
access times to web content. The Popularity List is proactively retrieved from
a (curious) public server that continually updates and refreshes the records
based on user popularity votes, while efficiently broadcasting record
updates/changes to adhere to aggressive load-balancing schemes (i.e., name
servers actively load-balancing user connections by changing record IP
addresses). User votes are anonymized using a novel, efficient, and highly
scalable client-driven Voting Mix Network - with packet lengths independent of
the number of hops, centrally enforced limit on number of votes cast per user,
and robustness against poor client participation - to ensure a geographically
relevant and correctly/securely instantiated Popularity List.

</details>


### [370] [Takedown: How It's Done in Modern Coding Agent Exploits](https://arxiv.org/abs/2509.24240)
*Eunkyu Lee,Donghyeon Kim,Wonyoung Kim,Insu Yun*

Main category: cs.CR

TL;DR: Comprehensive security analysis of 8 real-world coding agents reveals 15 security issues that can compromise user systems, enabling arbitrary command execution and global data exfiltration without user interaction.


<details>
  <summary>Details</summary>
Motivation: Modern LLM-driven coding agents with extensive autonomy raise significant security and privacy concerns, but systematic security analysis has been largely overlooked despite their growing adoption.

Method: Systematically examined internal workflows of coding agents to identify security threats across components, addressing limitations of prior fragmented approaches.

Result: Identified 15 security issues that collectively enable end-to-end exploitations; achieved arbitrary command execution in 5 agents and global data exfiltration in 4 agents without user interaction.

Conclusion: Highlights the need for comprehensive security analysis in modern LLM-driven agents and demonstrates how insufficient security considerations lead to severe vulnerabilities.

Abstract: Coding agents, which are LLM-driven agents specialized in software
development, have become increasingly prevalent in modern programming
environments. Unlike traditional AI coding assistants, which offer simple code
completion and suggestions, modern coding agents tackle more complex tasks with
greater autonomy, such as generating entire programs from natural language
instructions. To enable such capabilities, modern coding agents incorporate
extensive functionalities, which in turn raise significant concerns over their
security and privacy. Despite their growing adoption, systematic and in-depth
security analysis of these agents has largely been overlooked.
  In this paper, we present a comprehensive security analysis of eight
real-world coding agents. Our analysis addresses the limitations of prior
approaches, which were often fragmented and ad hoc, by systematically examining
the internal workflows of coding agents and identifying security threats across
their components. Through the analysis, we identify 15 security issues,
including previously overlooked or missed issues, that can be abused to
compromise the confidentiality and integrity of user systems. Furthermore, we
show that these security issues are not merely individual vulnerabilities, but
can collectively lead to end-to-end exploitations. By leveraging these security
issues, we successfully achieved arbitrary command execution in five agents and
global data exfiltration in four agents, all without any user interaction or
approval. Our findings highlight the need for a comprehensive security analysis
in modern LLM-driven agents and demonstrate how insufficient security
considerations can lead to severe vulnerabilities.

</details>


### [371] [VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference](https://arxiv.org/abs/2509.24257)
*Ke Wang,Felix Qu,Libin Xia,Zishuo Zhao,Chris Tong,Lynn Ai,Eric Yang*

Main category: cs.CR

TL;DR: VeriLLM is a publicly verifiable protocol for decentralized LLM inference that ensures security under one-honest-verifier assumption, achieves near-negligible verification cost (~1% of inference), and uses peer-prediction to prevent lazy verification.


<details>
  <summary>Details</summary>
Motivation: Decentralized LLM inference offers security, efficiency and cost benefits, but requires output verifiability since permissionless settings lack a priori trust in participating nodes.

Method: Uses lightweight verification algorithm for LLMs, peer-prediction mechanism, and isomorphic inference-verification network that multiplexes both roles on same GPU workers with task indistinguishability.

Result: Achieves near-negligible verification cost (1% of inference), improves GPU utilization and throughput, expands validator pool for robustness, and provides formal game-theoretic security proof.

Conclusion: VeriLLM is the first decentralized inference verification protocol with end-to-end game-theoretic security proof, ensuring honest inference and verification as Nash equilibrium against rational adversaries.

Abstract: Decentralized inference is an appealing paradigm for serving large language
models (LLMs), offering strong security, high efficiency, and lower operating
costs. Yet the permissionless setting admits no a priori trust in participating
nodes, making output verifiability a prerequisite for secure deployment. We
present VeriLLM, a publicly verifiable protocol for decentralized LLM inference
that (i) achieves security under a one-honest-verifier assumption, (ii) attains
near-negligible verification cost (about 1% of the underlying inference) via a
lightweight verification algorithm designed explicitly for LLMs, and (iii)
enforces honest checking through a peer-prediction mechanism that mitigates
lazy verification in naive voting. We further introduce an isomorphic
inference-verification network that multiplexes both roles on the same set of
GPU workers. This architecture (i) increases GPU utilization and thereby
improves end-to-end throughput for both inference and verification, (ii)
expands the effective pool of available validators, strengthening robustness
and security, and (iii) enforces task indistinguishability at the worker
boundary to prevent job-type-conditioned behavior. Finally, we provide a formal
game-theoretic analysis and prove that, under our incentives, honest inference
and verification constitute a Nash equilibrium, ensuring incentive
compatibility against rational adversaries. To our knowledge, this is the first
decentralized inference verification protocol with an end-to-end game-theoretic
security proof.

</details>


### [372] [When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation](https://arxiv.org/abs/2509.24272)
*Weibo Zhao,Jiahao Liu,Bonan Ruan,Shaofei Li,Zhenkai Liang*

Main category: cs.CR

TL;DR: This paper presents the first systematic security study of Model Context Protocol (MCP) servers, revealing they can be easily weaponized as active threat actors with 12 attack categories, are difficult to detect with current tools, and pose concrete risks to AI agent systems.


<details>
  <summary>Details</summary>
Motivation: MCP servers enable plug-and-play connectivity for AI applications but lack standardized security vetting, creating severe security risks that remain underexplored despite rapid proliferation.

Method: Proposed a component-based taxonomy with 12 attack categories, developed Proof-of-Concept (PoC) servers for each category, tested effectiveness across real-world host-LLM settings, and evaluated state-of-the-art scanners.

Result: Attackers can generate large numbers of malicious MCP servers at virtually no cost; existing detection approaches are insufficient; malicious servers are easy to implement, difficult to detect, and capable of causing concrete damage.

Conclusion: Addressing MCP server security threats requires coordinated efforts among protocol designers, host developers, LLM providers, and end users to build a more secure and resilient MCP ecosystem.

Abstract: Model Context Protocol (MCP) servers enable AI applications to connect to
external systems in a plug-and-play manner, but their rapid proliferation also
introduces severe security risks. Unlike mature software ecosystems with
rigorous vetting, MCP servers still lack standardized review mechanisms, giving
adversaries opportunities to distribute malicious implementations. Despite this
pressing risk, the security implications of MCP servers remain underexplored.
To address this gap, we present the first systematic study that treats MCP
servers as active threat actors and decomposes them into core components to
examine how adversarial developers can implant malicious intent. Specifically,
we investigate three research questions: (i) what types of attacks malicious
MCP servers can launch, (ii) how vulnerable MCP hosts and Large Language Models
(LLMs) are to these attacks, and (iii) how feasible it is to carry out MCP
server attacks in practice. Our study proposes a component-based taxonomy
comprising twelve attack categories. For each category, we develop
Proof-of-Concept (PoC) servers and demonstrate their effectiveness across
diverse real-world host-LLM settings. We further show that attackers can
generate large numbers of malicious servers at virtually no cost. We then test
state-of-the-art scanners on the generated servers and found that existing
detection approaches are insufficient. These findings highlight that malicious
MCP servers are easy to implement, difficult to detect with current tools, and
capable of causing concrete damage to AI agent systems. Addressing this threat
requires coordinated efforts among protocol designers, host developers, LLM
providers, and end users to build a more secure and resilient MCP ecosystem.

</details>


### [373] [FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous Driving Systems](https://arxiv.org/abs/2509.24408)
*Yuzhen Long,Songze Li*

Main category: cs.CR

TL;DR: FuncPoison is a novel poisoning attack targeting function libraries in LLM-driven multi-agent autonomous driving systems, exploiting text-based tool selection and standardized command formats to inject malicious tools and cause cascading errors.


<details>
  <summary>Details</summary>
Motivation: The function library is a critical but under-explored vulnerability in multi-agent autonomous driving systems, where agents rely on shared tools for perception, reasoning, and planning.

Method: FuncPoison exploits two weaknesses: (1) agents' reliance on text-based instructions to select tools, and (2) standardized command formats that attackers can replicate. It injects malicious tools with deceptive instructions to manipulate agent decisions.

Result: Experimental evaluation on two multi-agent autonomous driving systems shows FuncPoison significantly degrades trajectory accuracy, can flexibly target specific agents to induce coordinated misbehavior, and evades diverse defense mechanisms.

Conclusion: The function library, often considered a simple toolset, serves as a critical attack surface in LLM-based autonomous driving systems, raising serious concerns about their reliability.

Abstract: Autonomous driving systems increasingly rely on multi-agent architectures
powered by large language models (LLMs), where specialized agents collaborate
to perceive, reason, and plan. A key component of these systems is the shared
function library, a collection of software tools that agents use to process
sensor data and navigate complex driving environments. Despite its critical
role in agent decision-making, the function library remains an under-explored
vulnerability. In this paper, we introduce FuncPoison, a novel poisoning-based
attack targeting the function library to manipulate the behavior of LLM-driven
multi-agent autonomous systems. FuncPoison exploits two key weaknesses in how
agents access the function library: (1) agents rely on text-based instructions
to select tools; and (2) these tools are activated using standardized command
formats that attackers can replicate. By injecting malicious tools with
deceptive instructions, FuncPoison manipulates one agent s decisions--such as
misinterpreting road conditions--triggering cascading errors that mislead other
agents in the system. We experimentally evaluate FuncPoison on two
representative multi-agent autonomous driving systems, demonstrating its
ability to significantly degrade trajectory accuracy, flexibly target specific
agents to induce coordinated misbehavior, and evade diverse defense mechanisms.
Our results reveal that the function library, often considered a simple
toolset, can serve as a critical attack surface in LLM-based autonomous driving
systems, raising elevated concerns on their reliability.

</details>


### [374] [GSPR: Aligning LLM Safeguards as Generalizable Safety Policy Reasoners](https://arxiv.org/abs/2509.24418)
*Haoran Li,Yulin Chen,Jingru Zeng,Hao Peng,Huihao Jing,Wenbin Hu,Xi Yang,Ziqian Zeng,Sirui Han,Yangqiu Song*

Main category: cs.CR

TL;DR: GSPR is a Generalizable Safety Policy Reasoner that identifies unsafe prompts and LLM outputs using Group Relative Policy Optimization, enabling cross-benchmark training with varied safety taxonomies.


<details>
  <summary>Details</summary>
Motivation: Current LLM safety benchmarks have disparate taxonomies, leading to coarse-grained safeguards or narrow risk coverage. There's a need to leverage multiple safety benchmarks with different taxonomies for better safety reasoning.

Method: Proposed GSPR with Group Relative Policy Optimization (GRPO), using careful cold-start strategy and reward design to train across multiple safety benchmarks with distinct taxonomies.

Result: GSPR significantly improves safety reasoning capabilities for both safety and category prediction tasks, demonstrates powerful generalization abilities, and achieves the least inference token costs with explanations.

Conclusion: GSPR provides an effective approach to leverage multiple safety benchmarks with varied taxonomies, enabling better safety reasoning and generalization while maintaining efficiency.

Abstract: As large language models (LLMs) are increasingly integrated into numerous
applications across various domains, LLMs' safety becomes a critical concern
for both application developers and intended users. Currently, great efforts
have been made to develop safety benchmarks with fine-grained taxonomies.
However, these benchmarks' taxonomies are disparate with different safety
policies. Thus, existing safeguards trained on these benchmarks are either
coarse-grained to only distinguish between safe and unsafe, or constrained by
the narrow risk taxonomies of a single benchmark. To leverage these
fine-grained safety taxonomies across multiple safety benchmarks, in this
paper, we propose GSPR, a Generalizable Safety Policy Reasoner to identify
unsafe input prompts and LLMs' outputs with violated safety taxonomies through
Group Relative Policy Optimization (GRPO). Unlike prior safeguards which only
cover a fixed set of risk factors, our GSPR incentivizes its reasoning
capability with varied safety taxonomies through our careful cold-start
strategy and reward design. Consequently, our GSPR can be trained across
multiple safety benchmarks with distinct taxonomies and naturally exhibits
powerful generalization ability. We conduct extensive experiments to show that
our GSPR significantly improves existing safety guardrails' reasoning
capabilities for both safety and category prediction tasks. Moreover, our GSPR
not only demonstrates powerful safety generalization abilities but also
achieves the least inference token costs with explanations.

</details>


### [375] [Evaluating Relayed and Switched Quantum Key Distribution (QKD) Network Architectures](https://arxiv.org/abs/2509.24440)
*Antonis Selentis,Nikolas Makris,Alkinoos Papageorgopoulos,Persefoni Konteli,Konstantinos Christodoulopoulos,George T. Kanellos,Dimitris Syvridis*

Main category: cs.CR

TL;DR: Comparison of two QKD network architectures: Relayed QKD (uses trusted nodes) vs Switched QKD (end-to-end keys via optical switches). Experimental evaluation shows SKR variation in unmatched QKD modules. Theoretical analysis reveals Switched QKD performs better in dense rings, while Relayed QKD excels in longer distances.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the performance of two quantum key distribution network architectures - Relayed QKD (with trusted nodes) and Switched QKD (end-to-end keys) - to understand their relative advantages in different network scenarios.

Method: 1) Experimental evaluation of commercial DV-QKD modules from three vendors, testing both matched and unmatched pairs; 2) Comprehensive theoretical analysis using uniform ring networks, deriving optimal configurations and analytical formulas for SKR; 3) Performance comparison under varying ring sizes, link losses, receiver sensitivity, and unmatched module penalties.

Result: Experimental results show notable SKR variation between matched and unmatched QKD module pairs. Theoretical analysis indicates Switched QKD performs better in dense rings (short distances, large node counts), while Relayed QKD is more effective for longer distances with large node counts. Unmatched module penalties significantly impact Switched QKD efficiency.

Conclusion: The choice between Switched and Relayed QKD architectures depends on network characteristics: Switched QKD is preferable for dense networks with short distances, while Relayed QKD performs better for longer distance networks. The performance penalty from unmatched QKD modules is a critical factor affecting Switched QKD efficiency.

Abstract: We evaluate the performance of two architectures for network-wide quantum key
distribution (QKD): Relayed QKD, which relays keys over multi-link QKD paths
for non-adjacent nodes, and Switched QKD, which uses optical switches to
dynamically connect arbitrary QKD modules to form direct QKD links between
them. An advantage of Switched QKD is that it distributes quantum keys
end-to-end, whereas Relayed relies on trusted nodes. However, Switched depends
on arbitrary matching of QKD modules. We first experimentally evaluate the
performance of commercial DV-QKD modules; for each of three vendors we
benchmark the performance in standard/matched module pairs and in unmatched
pairs to emulate configurations in the Switched QKD network architecture. The
analysis reveals that in some cases a notable variation in the generated secret
key rate (SKR) between the matched and unmatched pairs is observed. Driven by
these experimental findings, we conduct a comprehensive theoretical analysis
that evaluates the network-wide performance of the two architectures. Our
analysis is based on uniform ring networks, where we derive optimal key
management configurations and analytical formulas for the achievable consumed
SKR. We compare network performance under varying ring sizes, QKD link losses,
QKD receivers' sensitivity and performance penalties of unmatched modules. Our
findings indicate that Switched QKD performs better in dense rings (short
distances, large node counts), while Relayed QKD is more effective in longer
distances and large node counts. Moreover, we confirm that unmatched QKD
modules penalties significantly impact the efficiency of Switched QKD
architecture.

</details>


### [376] [BugMagnifier: TON Transaction Simulator for Revealing Smart Contract Vulnerabilities](https://arxiv.org/abs/2509.24444)
*Yury Yanovich,Victoria Kovalevskaya,Maksim Egorov,Elizaveta Smirnova,Matvey Mishuris,Yash Madhwal,Kirill Ziborov,Vladimir Gorgadze,Subodh Sharma*

Main category: cs.CR

TL;DR: BugMagnifier is a transaction simulation framework that detects asynchronous execution vulnerabilities in TON blockchain smart contracts through systematic message orchestration and differential state analysis.


<details>
  <summary>Details</summary>
Motivation: TON blockchain's asynchronous execution model creates unique security challenges like race conditions due to unpredictable message processing order, requiring dynamic detection methods beyond static analysis.

Method: Built on TON Sandbox and TVM, BugMagnifier combines precise message queue manipulation with differential state analysis and probabilistic permutation testing to systematically reveal vulnerabilities.

Result: Experimental evaluation shows effective detection through parametric studies on vulnerable contracts, revealing message ratio-dependent detection complexity that aligns with theoretical predictions.

Conclusion: BugMagnifier addresses critical security gaps in TON by enabling automated vulnerability discovery and providing reproducible test scenarios for safer smart contract development in asynchronous environments.

Abstract: The Open Network (TON) blockchain employs an asynchronous execution model
that introduces unique security challenges for smart contracts, particularly
race conditions arising from unpredictable message processing order. While
previous work established vulnerability patterns through static analysis of
audit reports, dynamic detection of temporal dependencies through systematic
testing remains an open problem. We present BugMagnifier, a transaction
simulation framework that systematically reveals vulnerabilities in TON smart
contracts through controlled message orchestration. Built atop TON Sandbox and
integrated with the TON Virtual Machine (TVM), our tool combines precise
message queue manipulation with differential state analysis and probabilistic
permutation testing to detect asynchronous execution flaws. Experimental
evaluation demonstrates BugMagnifier's effectiveness through extensive
parametric studies on purpose-built vulnerable contracts, revealing message
ratio-dependent detection complexity that aligns with theoretical predictions.
This quantitative model enables predictive vulnerability assessment while
shifting discovery from manual expert analysis to automated evidence
generation. By providing reproducible test scenarios for temporal
vulnerabilities, BugMagnifier addresses a critical gap in the TON security
tooling, offering practical support for safer smart contract development in
asynchronous blockchain environments.

</details>


### [377] [Mapping Quantum Threats: An Engineering Inventory of Cryptographic Dependencies](https://arxiv.org/abs/2509.24623)
*Carlos Benitez*

Main category: cs.CR

TL;DR: This paper systematically maps quantum-vulnerable cryptographic technologies across digital infrastructures, providing a practical threat assessment to guide stakeholders in preparing for cryptographically relevant quantum computers.


<details>
  <summary>Details</summary>
Motivation: Large-scale quantum computers pose an existential threat to modern public-key cryptography by efficiently breaking foundational algorithms like RSA, Diffie-Hellman, and elliptic curve cryptography through Shor's and Grover's algorithms.

Method: The paper conducts a systematic inventory of technologies exposed to quantum threats from an engineering perspective, organized by technology domain and implementation environment, focusing on practical landscape mapping rather than theoretical breaks.

Result: A cross-domain, cross-environment threat map is created that identifies quantum-vulnerable systems across diverse digital infrastructures, including RSA, DH, ECDH, and ECDSA cryptographic primitives.

Conclusion: The work provides practitioners, vendors, and policymakers with a comprehensive guide to identify exposed technologies before the arrival of cryptographically relevant quantum computers, enabling proactive security measures.

Abstract: The emergence of large-scale quantum computers, powered by algorithms like
Shor's and Grover's, poses an existential threat to modern public-key
cryptography. This vulnerability stems from the ability of these machines to
efficiently solve the hard mathematical problems - such as integer
factorization and the elliptic curve discrete logarithm problem - that underpin
widely used cryptographic primitives. This includes RSA, Diffie-Hellman (DH),
Elliptic Curve Diffie-Hellman (ECDH), and Elliptic Curve Digital Signature
Algorithm (ECDSA), which are foundational to security across the digital
ecosystem. Once Shor's algorithm becomes practically realizable, these
primitives will fail, undermining both retrospective confidentiality and
cryptographic authenticity - enabling adversaries to decrypt previously
captured communications and forge digital signatures. This paper presents a
systematic inventory of technologies exposed to quantum threats from the
engineering perspective, organized by both technology domain and by
implementation environment. While prior research has emphasized theoretical
breaks or protocol-level adaptations, this work focuses on the practical
landscape - mapping quantum-vulnerable systems across diverse digital
infrastructures. The contribution is a cross-domain, cross-environment threat
map to guide practitioners, vendors, and policymakers in identifying exposed
technologies before the arrival of cryptographically relevant quantum
computers.

</details>


### [378] [PRIVMARK: Private Large Language Models Watermarking with MPC](https://arxiv.org/abs/2509.24624)
*Thomas Fargues,Ye Dong,Tianwei Zhang,Jin-Song Dong*

Main category: cs.CR

TL;DR: PRIVMARK is a secure multi-party computation (MPC)-based private watermarking framework for LLMs that enables collaborative watermarking without exposing model weights, achieving semantic equivalence and attack resistance.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in traditional LLM watermarking approaches that require direct access to model parameters or training data, particularly in sensitive scenarios.

Method: Formulates operations of PostMark (state-of-the-art LLM watermarking), constructs efficient MPC protocols for these operations using SecretFlow-SPU with ABY3 backend, enabling black-box collaborative watermarking.

Result: Achieves semantically identical results compared to plaintext baseline, resistant against paraphrasing and removing attacks with reasonable efficiency.

Conclusion: PRIVMARK provides a practical solution for private LLM watermarking through secure multi-party computation, balancing privacy protection with watermarking effectiveness.

Abstract: The rapid growth of Large Language Models (LLMs) has highlighted the pressing
need for reliable mechanisms to verify content ownership and ensure
traceability. Watermarking offers a promising path forward, but it remains
limited by privacy concerns in sensitive scenarios, as traditional approaches
often require direct access to a model's parameters or its training data. In
this work, we propose a secure multi-party computation (MPC)-based private LLMs
watermarking framework, PRIVMARK, to address the concerns. Concretely, we
investigate PostMark (EMNLP'2024), one of the state-of-the-art LLMs
Watermarking methods, and formulate its basic operations. Then, we construct
efficient protocols for these operations using the MPC primitives in a
black-box manner. In this way, PRIVMARK enables multiple parties to
collaboratively watermark an LLM's output without exposing the model's weights
to any single computing party. We implement PRIVMARK using SecretFlow-SPU
(USENIX ATC'2023) and evaluate its performance using the ABY3 (CCS'2018)
backend. The experimental results show that PRIVMARK achieves semantically
identical results compared to the plaintext baseline without MPC and is
resistant against paraphrasing and removing attacks with reasonable efficiency.

</details>


### [379] [LISA Technical Report: An Agentic Framework for Smart Contract Auditing](https://arxiv.org/abs/2509.24698)
*Izaiah Sun,Daniel Tan,Andy Deng*

Main category: cs.CR

TL;DR: LISA is an agentic smart contract vulnerability detection framework that combines rule-based and logic-based methods, leveraging historical audit data to detect vulnerabilities without model fine-tuning, outperforming both LLM-based approaches and traditional static analysis tools.


<details>
  <summary>Details</summary>
Motivation: To address the broad spectrum of vulnerabilities in smart contracts and reduce dependence on manual effort by creating a more reliable and comprehensive detection solution.

Method: Combines rule-based and logic-based methods, leverages data from historical audit reports to learn detection experience without model fine-tuning, enabling generalization to unseen projects and evolving threats.

Result: Significantly outperforms both LLM-based approaches and traditional static analysis tools, achieving superior coverage of vulnerability types and higher detection accuracy.

Conclusion: LISA offers a compelling industry solution for more reliable and comprehensive vulnerability detection while reducing manual effort dependence.

Abstract: We present LISA, an agentic smart contract vulnerability detection framework
that combines rule-based and logic-based methods to address a broad spectrum of
vulnerabilities in smart contracts. LISA leverages data from historical audit
reports to learn the detection experience (without model fine-tuning), enabling
it to generalize learned patterns to unseen projects and evolving threat
profiles. In our evaluation, LISA significantly outperforms both LLM-based
approaches and traditional static analysis tools, achieving superior coverage
of vulnerability types and higher detection accuracy. Our results suggest that
LISA offers a compelling solution for industry: delivering more reliable and
comprehensive vulnerability detection while reducing the dependence on manual
effort.

</details>


### [380] [Active Authentication via Korean Keystrokes Under Varying LLM Assistance and Cognitive Contexts](https://arxiv.org/abs/2509.24807)
*Dong Hyun Roh,Rajesh Kumar*

Main category: cs.CR

TL;DR: Keystroke dynamics authentication maintains reliable performance (5.1-10.4% EER) across different LLM-assisted typing scenarios and cognitive contexts in Korean language.


<details>
  <summary>Details</summary>
Motivation: To evaluate keystroke-based authentication effectiveness under varying LLM-assisted typing and cognitive conditions, which remains understudied despite keystroke dynamics being a promising modality for active user authentication.

Method: Used data from 50 users with cognitive labels from Bloom's Taxonomy, evaluated across three typing scenarios: bona fide composition, LLM content paraphrasing, and transcription. Implemented pipeline with continuity-aware segmentation, feature extraction, and classification using SVM, MLP, and XGB.

Result: System maintained reliable performance across varying LLM usages and cognitive contexts, achieving Equal Error Rates ranging from 5.1% to 10.4%.

Conclusion: Keystroke-based behavioral authentication is feasible under modern writing conditions with LLM assistance, providing insights for designing context-resilient authentication models.

Abstract: Keystroke dynamics is a promising modality for active user authentication,
but its effectiveness under varying LLM-assisted typing and cognitive
conditions remains understudied. Using data from 50 users and cognitive labels
from Bloom's Taxonomy, we evaluate keystroke-based authentication in Korean
across three realistic typing scenarios: bona fide composition, LLM content
paraphrasing, and transcription. Our pipeline incorporates continuity-aware
segmentation, feature extraction, and classification via SVM, MLP, and XGB.
Results show that the system maintains reliable performance across varying LLM
usages and cognitive contexts, with Equal Error Rates ranging from 5.1% to
10.4%. These findings demonstrate the feasibility of behavioral authentication
under modern writing conditions and offer insights into designing more
context-resilient models.

</details>


### [381] [Of-SemWat: High-payload text embedding for semantic watermarking of AI-generated images with arbitrary size](https://arxiv.org/abs/2509.24823)
*Benedetta Tondi,Andrea Costanzo,Mauro Barni*

Main category: cs.CR

TL;DR: A high-payload image watermarking method that embeds semantic text descriptions into AI-generated images using orthogonal/turbo codes with frequency-domain embedding and perceptual masking for robustness and imperceptibility.


<details>
  <summary>Details</summary>
Motivation: To embed semantic descriptions of images (including text prompts) directly into AI-generated images, enabling detection of semantic modifications through text mismatch analysis.

Method: Builds on traditional watermarking with orthogonal and turbo codes for robustness, combined with frequency-domain embedding and perceptual masking techniques for imperceptibility.

Result: Extremely robust against various image processing operations, capable of retrieving embedded text even after traditional and AI inpainting, allowing detection of semantic modifications.

Conclusion: The proposed method successfully embeds high-payload semantic text in AI-generated images with strong robustness and enables detection of unauthorized semantic modifications through text mismatch analysis.

Abstract: We propose a high-payload image watermarking method for textual embedding,
where a semantic description of the image - which may also correspond to the
input text prompt-, is embedded inside the image. In order to be able to
robustly embed high payloads in large-scale images - such as those produced by
modern AI generators - the proposed approach builds upon a traditional
watermarking scheme that exploits orthogonal and turbo codes for improved
robustness, and integrates frequency-domain embedding and perceptual masking
techniques to enhance watermark imperceptibility. Experiments show that the
proposed method is extremely robust against a wide variety of image processing,
and the embedded text can be retrieved also after traditional and AI
inpainting, permitting to unveil the semantic modification the image has
undergone via image-text mismatch analysis.

</details>


### [382] [Secret Leader Election in Ethereum PoS: An Empirical Security Analysis of Whisk and Homomorphic Sortition under DoS on the Leader and Censorship Attacks](https://arxiv.org/abs/2509.24955)
*Tereza Burianová,Martin Perešíni,Ivan Homoliak*

Main category: cs.CR

TL;DR: This paper presents a unified experimental framework to evaluate Secret Single Leader Election (SSLE) mechanisms in PoS blockchains, comparing Whisk and homomorphic sortition under adversarial DoS and censorship attacks.


<details>
  <summary>Details</summary>
Motivation: Proposer anonymity in PoS blockchains is crucial to prevent targeted attacks like DoS and censorship, but existing SSLE mechanisms lack practical evaluation of their trade-offs and effectiveness.

Method: Developed a unified experimental framework with configurable adversaries capable of launching targeted DoS and censorship attacks, including coordinated strategies against validator groups, using a simplified model of Ethereum's PoS consensus layer.

Result: Both Whisk and homomorphic sortition provide strong protection against targeted DoS attacks on individual leaders, but neither effectively defends against coordinated attacks on validator groups. Whisk simplifies DoS attacks by narrowing targets to known candidates, while homomorphic sortition remains impractical due to cryptographic complexity.

Conclusion: Current SSLE mechanisms have limitations in defending against coordinated attacks on validator groups, highlighting the need for improved protection strategies that balance security and practicality in PoS blockchain systems.

Abstract: Proposer anonymity in Proof-of-Stake (PoS) blockchains is a critical concern
due to the risk of targeted attacks such as malicious denial-of-service (DoS)
and censorship attacks. While several Secret Single Leader Election (SSLE)
mechanisms have been proposed to address these threats, their practical impact
and trade-offs remain insufficiently explored. In this work, we present a
unified experimental framework for evaluating SSLE mechanisms under adversarial
conditions, grounded in a simplified yet representative model of Ethereum's PoS
consensus layer. The framework includes configurable adversaries capable of
launching targeted DoS and censorship attacks, including coordinated strategies
that simultaneously compromise groups of validators. We simulate and compare
key protection mechanisms - Whisk, and homomorphic sortition. To the best of
our knowledge, this is the first comparative study to examine adversarial DoS
scenarios involving multiple attackers under diverse protection mechanisms. Our
results show that while both designs offer strong protection against targeted
DoS attacks on the leader, neither defends effectively against coordinated
attacks on validator groups. Moreover, Whisk simplifies a DoS attack by
narrowing the target set from all validators to a smaller list of known
candidates. Homomorphic sortition, despite its theoretical strength, remains
impractical due to the complexity of cryptographic operations over large
validator sets.

</details>


### [383] [SecInfer: Preventing Prompt Injection via Inference-time Scaling](https://arxiv.org/abs/2509.24967)
*Yupei Liu,Yanting Wang,Yuqi Jia,Jinyuan Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: SecInfer is a novel defense against prompt injection attacks using inference-time scaling, which generates multiple responses through diverse system prompts and aggregates them to select the most secure response.


<details>
  <summary>Details</summary>
Motivation: Prompt injection attacks are a major security threat to LLMs, and current prevention-based defenses through fine-tuning have limited effectiveness against strong attacks.

Method: SecInfer uses inference-time scaling with two steps: system-prompt-guided sampling to generate multiple responses through diverse reasoning paths, and target-task-guided aggregation to select the response most likely to accomplish the intended task.

Result: Extensive experiments show SecInfer effectively mitigates both existing and adaptive prompt injection attacks, outperforming state-of-the-art defenses and existing inference-time scaling approaches.

Conclusion: By leveraging additional compute at inference time, SecInfer provides an effective defense mechanism against prompt injection attacks that surpasses current methods.

Abstract: Prompt injection attacks pose a pervasive threat to the security of Large
Language Models (LLMs). State-of-the-art prevention-based defenses typically
rely on fine-tuning an LLM to enhance its security, but they achieve limited
effectiveness against strong attacks. In this work, we propose \emph{SecInfer},
a novel defense against prompt injection attacks built on \emph{inference-time
scaling}, an emerging paradigm that boosts LLM capability by allocating more
compute resources for reasoning during inference. SecInfer consists of two key
steps: \emph{system-prompt-guided sampling}, which generates multiple responses
for a given input by exploring diverse reasoning paths through a varied set of
system prompts, and \emph{target-task-guided aggregation}, which selects the
response most likely to accomplish the intended task. Extensive experiments
show that, by leveraging additional compute at inference, SecInfer effectively
mitigates both existing and adaptive prompt injection attacks, outperforming
state-of-the-art defenses as well as existing inference-time scaling
approaches.

</details>


### [384] [Optimizing Privacy-Preserving Primitives to Support LLM-Scale Applications](https://arxiv.org/abs/2509.25072)
*Yaman Jandali,Ruisi Zhang,Nojan Sheybani,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: This paper presents hardware/software/algorithm co-design approaches to reduce the computational and communication overhead of privacy-preserving technologies (MPC, ZKPs, FHE) for practical adoption in learning systems, with demonstrations in DNN IP ownership, ethical LLM usage, and transformer inference.


<details>
  <summary>Details</summary>
Motivation: The practical adoption of privacy-preserving technologies is hindered by significant computational and communication overhead when applied at scale, creating a gap between theoretical capabilities and real-world applications.

Method: The authors employ meticulous hardware/software/algorithm co-design approaches to optimize privacy-preserving technologies including multi-party computation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic encryption (FHE).

Result: The paper shows progress towards enabling LLM-scale applications in privacy-preserving settings and demonstrates efficacy in several contexts including DNN IP ownership, ethical LLM usage enforcement, and transformer inference.

Conclusion: Through co-design optimization, the authors bridge the gap between overhead and practicality for privacy-preserving learning systems, making large-scale applications like LLMs feasible in privacy-preserving settings.

Abstract: Privacy-preserving technologies have introduced a paradigm shift that allows
for realizable secure computing in real-world systems. The significant barrier
to the practical adoption of these primitives is the computational and
communication overhead that is incurred when applied at scale. In this paper,
we present an overview of our efforts to bridge the gap between this overhead
and practicality for privacy-preserving learning systems using multi-party
computation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic
encryption (FHE). Through meticulous hardware/software/algorithm co-design, we
show progress towards enabling LLM-scale applications in privacy-preserving
settings. We demonstrate the efficacy of our solutions in several contexts,
including DNN IP ownership, ethical LLM usage enforcement, and transformer
inference.

</details>


### [385] [Two-Dimensional XOR-Based Secret Sharing for Layered Multipath Communication](https://arxiv.org/abs/2509.25113)
*Wai Ming Chan,Remi Chou,Taejoon Kim*

Main category: cs.CR

TL;DR: First 2D XOR-based secret sharing scheme for layered multipath networks with unconditional security against single-path attacks per layer.


<details>
  <summary>Details</summary>
Motivation: Need for provably secure communication in resource-constrained military environments where computational assumptions may fail and quantum computing threatens encryption-based approaches.

Method: Construct 2D XOR-based secret sharing using only bitwise XOR operations with linear O(|S|) complexity, where |S| is message length. Mathematical proofs guarantee security against adversaries observing/disrupting any single path per transmission layer.

Result: Scheme achieves perfect privacy and successful message recovery with information-theoretic security. Maintains unconditional security regardless of computational resources available to adversaries.

Conclusion: XOR-based construction offers provable security suitable for military applications, resilient to quantum computing advances unlike encryption-based approaches.

Abstract: This paper introduces the first two-dimensional XOR-based secret sharing
scheme for layered multipath communication networks. We present a construction
that guarantees successful message recovery and perfect privacy when an
adversary observes and disrupts any single path at each transmission layer. The
scheme achieves information-theoretic security using only bitwise XOR
operations with linear $O(|S|)$ complexity, where $|S|$ is the message length.
We provide mathematical proofs demonstrating that the scheme maintains
unconditional security regardless of computational resources available to
adversaries. Unlike encryption-based approaches vulnerable to quantum computing
advances, our construction offers provable security suitable for
resource-constrained military environments where computational assumptions may
fail.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [386] [Localizing Adversarial Attacks To Produces More Imperceptible Noise](https://arxiv.org/abs/2509.22710)
*Pavan Reddy,Aditya Sanjay Gujral*

Main category: cs.LG

TL;DR: Localized adversarial attacks using binary masks achieve better imperceptibility with lower pixel perturbations and higher PSNR/SSIM scores compared to global attacks, but require more computation and slightly reduce attack success rates.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the potential of localized adversarial noise, which remains underexplored compared to traditional global perturbation methods.

Method: Introduce binary masks to constrain noise to specific regions across FGSM, PGD, and C&W attack methods, comparing localized vs global attacks on effectiveness, imperceptibility, and computational efficiency.

Result: Localized attacks achieve significantly lower mean pixel perturbations, higher PSNR and SSIM scores, but with increased computational effort and modest reduction in Attack Success Rate (ASR). Iterative methods (PGD, C&W) perform better than single-step methods (FGSM) under localization constraints.

Conclusion: Localized adversarial attacks offer improved imperceptibility but require trade-offs in computational cost and attack success, with iterative methods being more robust to localization constraints, providing insights for attack strategy development and defensive system design.

Abstract: Adversarial attacks in machine learning traditionally focus on global
perturbations to input data, yet the potential of localized adversarial noise
remains underexplored. This study systematically evaluates localized
adversarial attacks across widely-used methods, including FGSM, PGD, and C&W,
to quantify their effectiveness, imperceptibility, and computational
efficiency. By introducing a binary mask to constrain noise to specific
regions, localized attacks achieve significantly lower mean pixel
perturbations, higher Peak Signal-to-Noise Ratios (PSNR), and improved
Structural Similarity Index (SSIM) compared to global attacks. However, these
benefits come at the cost of increased computational effort and a modest
reduction in Attack Success Rate (ASR). Our results highlight that iterative
methods, such as PGD and C&W, are more robust to localization constraints than
single-step methods like FGSM, maintaining higher ASR and imperceptibility
metrics. This work provides a comprehensive analysis of localized adversarial
attacks, offering practical insights for advancing attack strategies and
designing robust defensive systems.

</details>


### [387] [In-Context Learning can Perform Continual Learning Like Humans](https://arxiv.org/abs/2509.22764)
*Liuwang Kang,Fan Wang,Shaoshan Liu,Hung-Chyun Chou,Chuan Lin,Ning Ding*

Main category: cs.LG

TL;DR: This paper explores in-context continual learning (ICCL) for LLMs, showing they can retain knowledge across sequential tasks without parameter updates, with performance benefiting from distributed practice similar to human memory patterns.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can achieve long-term retention and cross-task knowledge accumulation in sequential multitask settings, inspired by human memory studies.

Method: Extend in-context learning to ICCL through task scheduling and prompt rearrangement, using Markov-Chain benchmarks and proposing a human-retention similarity metric to evaluate alignment with human memory dynamics.

Result: ICCL benefits from distributed practice with a spacing "sweet spot" for retention; linear-attention models like MAMBA and RWKV show human-like retention patterns despite lower performance than Transformers.

Conclusion: ICCL provides a cognitively plausible and effective inference-only continual learning paradigm that mitigates catastrophic forgetting and addresses the stability-plasticity dilemma in conventional CL methods.

Abstract: Large language models (LLMs) can adapt to new tasks via in-context learning
(ICL) without parameter updates, making them powerful learning engines for fast
adaptation. While extensive research has examined ICL as a few-shot learner,
whether it can achieve long-term retention and cross-task knowledge
accumulation when multitasks arrive sequentially remains underexplored.
Motivated by human memory studies, we investigate the retention characteristics
of ICL in multitask settings and extend it to in-context continual learning
(ICCL), where continual learning ability emerges through task scheduling and
prompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that,
for specific large-language models, ICCL benefits from distributed practice
(DP) in a manner analogous to humans, consistently revealing a spacing "sweet
spot" for retention. Beyond retention performance, we propose a human-retention
similarity metric to quantify how closely a continual-learning (CL) method
aligns with human retention dynamics. Using this metric, we show that
linear-attention models such as MAMBA and RWKV exhibit particularly human-like
retention patterns, despite their retention performance lagging behind that of
Transformer-based LLMs. Overall, our results establish ICCL as both cognitively
plausible and practically effective, providing an inference-only CL paradigm
that mitigates catastrophic forgetting and addresses the stability-plasticity
dilemma in conventional CL methods.

</details>


### [388] [Communication-Efficient and Interoperable Distributed Learning](https://arxiv.org/abs/2509.22823)
*Mounssif Krouka,Mehdi Bennis*

Main category: cs.LG

TL;DR: A communication-efficient distributed learning framework supporting model heterogeneity and modular composition during inference, achieving better efficiency than FL and FSL baselines.


<details>
  <summary>Details</summary>
Motivation: Address challenges in collaborative learning across heterogeneous model architectures regarding interoperability and privacy preservation.

Method: Use a common fusion-layer output dimension to partition models into personalized base blocks and generalized modular blocks, sharing only fusion-layer outputs while keeping model parameters private.

Result: Superior communication efficiency compared to federated learning (FL) and federated split learning (FSL) baselines, with stable training performance across heterogeneous architectures.

Conclusion: The proposed framework effectively enables collaborative learning across heterogeneous models while maintaining privacy and achieving high communication efficiency.

Abstract: Collaborative learning across heterogeneous model architectures presents
significant challenges in ensuring interoperability and preserving privacy. We
propose a communication-efficient distributed learning framework that supports
model heterogeneity and enables modular composition during inference. To
facilitate interoperability, all clients adopt a common fusion-layer output
dimension, which permits each model to be partitioned into a personalized base
block and a generalized modular block. Clients share their fusion-layer
outputs, keeping model parameters and architectures private. Experimental
results demonstrate that the framework achieves superior communication
efficiency compared to federated learning (FL) and federated split learning
(FSL) baselines, while ensuring stable training performance across
heterogeneous architectures.

</details>


### [389] [On the Capacity of Self-Attention](https://arxiv.org/abs/2509.22840)
*Micah Adler*

Main category: cs.LG

TL;DR: The paper introduces Relational Graph Recognition (RGR) to formalize self-attention capacity, showing that total key dimension $D_K = \Theta(m' \log m' / d_{\text{model}})$ is necessary and sufficient to recover $m'$ relations, providing a capacity-based rationale for multi-head attention.


<details>
  <summary>Details</summary>
Motivation: To formally understand self-attention's capacity for learning relations among tokens and provide principled design rules for allocating key-query budget across attention heads.

Method: Introduces Relational Graph Recognition (RGR) framework where key-query channels represent graphs, derives analytical capacity scaling laws, and validates through controlled single-layer experiments.

Result: Established that $D_K = \Theta(m' \log m' / d_{\text{model}})$ is both necessary and sufficient for recovering $m'$ relations, showing multi-head attention mitigates interference when embeddings are compressed.

Conclusion: Provides concrete scaling law for self-attention capacity and principled design rule for distributing key-query budget across multiple heads to maximize recoverable relations.

Abstract: While self-attention is known to learn relations among tokens, we lack a
formal understanding of its capacity: how many distinct relations can a single
layer reliably recover for a given budget?
  To formalize this, we introduce Relational Graph Recognition (RGR), where the
key-query channel represents a graph on $m$ items with $m'$ directed edges,
and, given a context of items, must recover the neighbors of each item. We
measure resources by the total key dimension $D_K = h\,d_k$. Within this
framework, we analytically derive a capacity scaling law and validate it
empirically. We show that $D_K = \Theta(m' \log m' / d_{\text{model}})$ is both
necessary (information-theoretic lower bound) and sufficient (explicit
construction) in a broad class of graphs to recover $m'$ relations. This
scaling law directly leads to a new, capacity-based rationale for multi-head
attention that applies even when each item only attends to a single target.
When embeddings are uncompressed ($m = d_{\text{model}}$) and the graph is a
permutation, a single head suffices. However, compression ($m >
d_{\text{model}}$) forces relations into overlapping subspaces, creating
interference that a single large head cannot disentangle. Our analysis shows
that allocating a fixed $D_K$ across many small heads mitigates this
interference, increasing the number of recoverable relations. Controlled
single-layer experiments mirror the theory, revealing a sharp performance
threshold that matches the predicted capacity scaling and confirms the benefit
of distributing $D_K$ across multiple heads.
  Altogether, these results provide a concrete scaling law for self-attention
capacity and a principled design rule for allocating key-query budget across
heads.

</details>


### [390] [Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data](https://arxiv.org/abs/2509.22850)
*Roie Kazoom,Yuval Ratzabi,Etamar Rothstein,Ofer Hadar*

Main category: cs.LG

TL;DR: A novel black-box decision-based adversarial attack for tabular data that achieves over 90% success rates with minimal queries, exposing critical vulnerabilities in tabular models.


<details>
  <summary>Details</summary>
Motivation: Adversarial robustness in structured/tabular data remains underexplored compared to vision and language domains, creating a research gap in this important area.

Method: Combines gradient-free direction estimation with iterative boundary search to efficiently navigate discrete and continuous feature spaces under minimal oracle access.

Result: Successfully compromises nearly entire test sets across diverse models (classical ML classifiers to LLM-based pipelines) with success rates consistently above 90% using only small number of queries per instance.

Conclusion: Highlights critical vulnerability of tabular models to adversarial perturbations and underscores urgent need for stronger defenses in real-world decision-making systems.

Abstract: Adversarial robustness in structured data remains an underexplored frontier
compared to vision and language domains. In this work, we introduce a novel
black-box, decision-based adversarial attack tailored for tabular data. Our
approach combines gradient-free direction estimation with an iterative boundary
search, enabling efficient navigation of discrete and continuous feature spaces
under minimal oracle access. Extensive experiments demonstrate that our method
successfully compromises nearly the entire test set across diverse models,
ranging from classical machine learning classifiers to large language model
(LLM)-based pipelines. Remarkably, the attack achieves success rates
consistently above 90%, while requiring only a small number of queries per
instance. These results highlight the critical vulnerability of tabular models
to adversarial perturbations, underscoring the urgent need for stronger
defenses in real-world decision-making systems.

</details>


### [391] [Adaptive Margin RLHF via Preference over Preferences](https://arxiv.org/abs/2509.22851)
*Yaswanth Chittepu,Prasann Singhal,Greg Durrett,Scott Niekum*

Main category: cs.LG

TL;DR: The paper proposes DPO-PoP, an extension to Direct Preference Optimization that uses preference-over-preference annotations to infer adaptive margins, improving both discriminative and generative performance over vanilla DPO and fixed-margin approaches.


<details>
  <summary>Details</summary>
Motivation: Existing margin-based optimization methods in RLHF often use no margins, fixed margins, or simplistic margin functions that don't account for varying preference strengths. These approaches fail to model preference strength differences and rely on noisy margin information from ratings.

Method: The method leverages preference-over-preference annotations (indicating which of two preferences reflects a stronger distinction) to infer adaptive margins on a per-datapoint basis. It extends DPO with these adaptive margins and proposes two sampling strategies for gathering preference-over-preference labels.

Result: Empirically, DPO-PoP outperforms vanilla DPO, DPO with fixed margins, and DPO with ground-truth margins on the UltraFeedback dataset. The method reveals a tradeoff between discriminative and generative performance.

Conclusion: Modeling preference strength through adaptive margins leads to better generalization and alignment. The proposed approach successfully navigates the tradeoff between discriminative and generative performance using different sampling strategies for preference-over-preference label collection.

Abstract: Margin-based optimization is fundamental to improving generalization and
robustness in classification tasks. In the context of reward model learning
from preferences within Reinforcement Learning from Human Feedback (RLHF),
existing methods typically rely on no margins, fixed margins, or margins that
are simplistic functions of preference ratings. However, such formulations
often fail to account for the varying strengths of different preferences, for
example some preferences are associated with larger margins between responses,
or they rely on noisy margin information derived from ratings. We argue that
modeling the strength of preferences can lead to better generalization and more
faithful alignment. Furthermore, many existing methods that use adaptive
margins assume access to accurate preference scores, which can be difficult for
humans to provide reliably. We propose an approach that leverages preferences
over preferences, that is annotations indicating which of two preferences
reflects a stronger distinction. We use this ordinal signal to infer adaptive
margins on a per-datapoint basis. We introduce an extension to Direct
Preference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from
preference-over-preference supervision, enabling improved discriminative and
generative performance. Empirically, our method outperforms vanilla DPO, DPO
with fixed margins, and DPO with ground-truth margins on the UltraFeedback
dataset. Additionally, we show that there is a tradeoff between discriminative
and generative performance: improving test classification accuracy,
particularly by correctly labeling weaker preferences at the expense of
stronger ones, can lead to a decline in generative quality. To navigate this
tradeoff, we propose two sampling strategies to gather
preference-over-preference labels: one favoring discriminative performance and
one favoring generative performance.

</details>


### [392] [Observation-Free Attacks on Online Learning to Rank](https://arxiv.org/abs/2509.22855)
*Sameep Chattopadhyay,Nikhil Karamchandani,Sharayu Mohair*

Main category: cs.LG

TL;DR: This paper presents a framework for attacking online learning to rank (OLTR) algorithms through coordinated adversarial attacks that promote target items to top recommendations while causing linear regret.


<details>
  <summary>Details</summary>
Motivation: Despite widespread use of OLTR algorithms in search engines and recommender systems, their vulnerability to coordinated adversarial attacks remains poorly understood.

Method: Proposed two attack strategies: CascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB, designed to promote target items to top-K recommendations while inducing linear regret.

Result: Theoretical guarantees show both attack strategies require only O(log T) manipulations to succeed. Empirical results on real-world data support the theoretical findings.

Conclusion: The framework demonstrates significant vulnerabilities in widely used OLTR algorithms to coordinated adversarial attacks, highlighting security concerns in practical applications.

Abstract: Online learning to rank (OLTR) plays a critical role in information retrieval
and machine learning systems, with a wide range of applications in search
engines and content recommenders. However, despite their extensive adoption,
the susceptibility of OLTR algorithms to coordinated adversarial attacks
remains poorly understood. In this work, we present a novel framework for
attacking some of the widely used OLTR algorithms. Our framework is designed to
promote a set of target items so that they appear in the list of top-K
recommendations for T - o(T) rounds, while simultaneously inducing linear
regret in the learning algorithm. We propose two novel attack strategies:
CascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical
guarantees showing that both strategies require only O(log T) manipulations to
succeed. Additionally, we supplement our theoretical analysis with empirical
results on real-world data.

</details>


### [393] [Neighborhood Sampling Does Not Learn the Same Graph Neural Network](https://arxiv.org/abs/2509.22868)
*Zehao Niu,Mihai Anitescu,Jie Chen*

Main category: cs.LG

TL;DR: Theoretical analysis of neighborhood sampling in graph neural networks using neural tangent kernels, showing different sampling methods produce different posterior GPs with limited samples but converge to the same distribution as sample size increases.


<details>
  <summary>Details</summary>
Motivation: Neighborhood sampling is widely used in large-scale GNN training to control memory and time costs, but its systemic behaviors are not well understood theoretically.

Method: Used neural tangent kernels to analyze training dynamics through infinitely wide counterparts (Gaussian processes), studied various established neighborhood sampling approaches and their corresponding posterior GPs.

Result: With limited samples, different sampling methods produce different posterior GPs, but they converge to the same distribution as sample size increases. Posterior covariances are uncomparable, explaining why no sampling approach dominates in practice.

Conclusion: Neighborhood sampling methods exhibit different behaviors with limited samples but converge asymptotically, and no single method is universally superior, which aligns with empirical observations.

Abstract: Neighborhood sampling is an important ingredient in the training of
large-scale graph neural networks. It suppresses the exponential growth of the
neighborhood size across network layers and maintains feasible memory
consumption and time costs. While it becomes a standard implementation in
practice, its systemic behaviors are less understood. We conduct a theoretical
analysis by using the tool of neural tangent kernels, which characterize the
(analogous) training dynamics of neural networks based on their infinitely wide
counterparts -- Gaussian processes (GPs). We study several established
neighborhood sampling approaches and the corresponding posterior GP. With
limited samples, the posteriors are all different, although they converge to
the same one as the sample size increases. Moreover, the posterior covariance,
which lower-bounds the mean squared prediction error, is uncomparable, aligning
with observations that no sampling approach dominates.

</details>


### [394] [From Noise to Knowledge: A Comparative Study of Acoustic Anomaly Detection Models in Pumped-storage Hydropower Plants](https://arxiv.org/abs/2509.22881)
*Karim Khamaisi,Nicolas Keller,Stefan Krummenacher,Valentin Huber,Bernhard Fässler,Bruno Rodrigues*

Main category: cs.LG

TL;DR: This paper presents a comparative analysis of acoustic-based anomaly detection methods for predictive maintenance in hydropower plants, testing LSTM AE, K-Means, and OC-SVM on real-world datasets from an Austrian pumped-storage plant.


<details>
  <summary>Details</summary>
Motivation: Unplanned outages in industrial factories and energy producers are highly costly and difficult to service, but existing acoustic-anomaly detection studies largely rely on generic datasets with limited focus on hydropower plants due to access constraints.

Method: Address acoustic preprocessing challenges in noisy conditions, extract time- and frequency-domain features, and benchmark three machine learning models (LSTM AE, K-Means, OC-SVM) on two real-world datasets from Rodundwerk II pumped-storage plant.

Result: One-Class SVM achieved the best trade-off with accuracy (ROC AUC 0.966-0.998) and minimal training time, while LSTM autoencoder delivered strong detection (ROC AUC 0.889-0.997) but with higher computational cost.

Conclusion: Acoustic-based anomaly detection methods show promise for predictive maintenance in hydropower plants, with OC-SVM providing the most practical balance of performance and efficiency for real-world applications.

Abstract: In the context of industrial factories and energy producers, unplanned
outages are highly costly and difficult to service. However, existing
acoustic-anomaly detection studies largely rely on generic industrial or
synthetic datasets, with few focused on hydropower plants due to limited
access. This paper presents a comparative analysis of acoustic-based anomaly
detection methods, as a way to improve predictive maintenance in hydropower
plants. We address key challenges in the acoustic preprocessing under highly
noisy conditions before extracting time- and frequency-domain features. Then,
we benchmark three machine learning models: LSTM AE, K-Means, and OC-SVM, which
are tested on two real-world datasets from the Rodundwerk II pumped-storage
plant in Austria, one with induced anomalies and one with real-world
conditions. The One-Class SVM achieved the best trade-off of accuracy (ROC AUC
0.966-0.998) and minimal training time, while the LSTM autoencoder delivered
strong detection (ROC AUC 0.889-0.997) at the expense of higher computational
cost.

</details>


### [395] [FedCF: Fair Federated Conformal Prediction](https://arxiv.org/abs/2509.22907)
*Anutam Srinivasan,Aditya T. Vadlamani,Amin Meghrazi,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: Extending Conformal Fairness to Federated Learning to audit model fairness across demographic groups while maintaining probabilistic guarantees.


<details>
  <summary>Details</summary>
Motivation: Standard Conformal Prediction provides uncertainty quantification but ignores sensitive attributes. Recent works aim to incorporate fairness, but this hasn't been explored in Federated Learning settings.

Method: Extend the Conformal Fairness framework to Federated Learning, analyzing fairness-related gaps across demographic groups while leveraging exchangeability assumptions.

Result: Empirical validation on multiple datasets across different domains shows the framework effectively audits federated models for fairness.

Conclusion: The proposed framework successfully brings fairness auditing to Federated Learning while maintaining the probabilistic guarantees of Conformal Prediction.

Abstract: Conformal Prediction (CP) is a widely used technique for quantifying
uncertainty in machine learning models. In its standard form, CP offers
probabilistic guarantees on the coverage of the true label, but it is agnostic
to sensitive attributes in the dataset. Several recent works have sought to
incorporate fairness into CP by ensuring conditional coverage guarantees across
different subgroups. One such method is Conformal Fairness (CF). In this work,
we extend the CF framework to the Federated Learning setting and discuss how we
can audit a federated model for fairness by analyzing the fairness-related gaps
for different demographic groups. We empirically validate our framework by
conducting experiments on several datasets spanning multiple domains, fully
leveraging the exchangeability assumption.

</details>


### [396] [Guided Manifold Alignment with Geometry-Regularized Twin Autoencoders](https://arxiv.org/abs/2509.22913)
*Jake S. Rhodes,Adam G. Rustad,Marshall S. Nielsen,Morgan Chase McClellan,Dallan Gardner,Dawson Hedges*

Main category: cs.LG

TL;DR: A geometry-regularized twin autoencoder framework for manifold alignment that enables out-of-sample extension and improves cross-domain generalization while maintaining geometric fidelity in embeddings.


<details>
  <summary>Details</summary>
Motivation: Traditional manifold alignment methods lack out-of-sample extension capability, limiting real-world applicability. There's a need for methods that can generalize to unseen data while maintaining alignment quality.

Method: Uses a twin autoencoder architecture with geometry regularization to enforce structured cross-modal mappings. Incorporates pre-trained alignment model and multitask learning to improve generalization and robustness.

Result: Shows improvements in embedding consistency, information preservation, and cross-domain transfer. Applied to Alzheimer's disease diagnosis, it enhances predictive accuracy by integrating multi-modal patient data.

Conclusion: The proposed framework effectively addresses limitations of traditional manifold alignment methods by enabling out-of-sample extension and improving cross-domain generalization while maintaining alignment fidelity.

Abstract: Manifold alignment (MA) involves a set of techniques for learning shared
representations across domains, yet many traditional MA methods are incapable
of performing out-of-sample extension, limiting their real-world applicability.
We propose a guided representation learning framework leveraging a
geometry-regularized twin autoencoder (AE) architecture to enhance MA while
enabling generalization to unseen data. Our method enforces structured
cross-modal mappings to maintain geometric fidelity in learned embeddings. By
incorporating a pre-trained alignment model and a multitask learning
formulation, we improve cross-domain generalization and representation
robustness while maintaining alignment fidelity. We evaluate our approach using
several MA methods, showing improvements in embedding consistency, information
preservation, and cross-domain transfer. Additionally, we apply our framework
to Alzheimer's disease diagnosis, demonstrating its ability to integrate
multi-modal patient data and enhance predictive accuracy in cases limited to a
single domain by leveraging insights from the multi-modal problem.

</details>


### [397] [Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective](https://arxiv.org/abs/2509.22921)
*Matthieu Zimmer,Xiaotong Ji,Tu Nguyen,Haitham Bou Ammar*

Main category: cs.LG

TL;DR: A novel constrained RL approach for LLM distillation that maximizes task rewards while constraining divergence from teacher model, achieving better constraint satisfaction and reasoning than baselines.


<details>
  <summary>Details</summary>
Motivation: Existing LLM distillation methods use ad-hoc reward weighting; need principled framework to balance task rewards and teacher fidelity with theoretical guarantees.

Method: Formulate distillation as constrained RL problem using modified reward function that ensures constraint satisfaction without state augmentation or teacher access during deployment.

Result: Better constraint satisfaction rates and reasoning performance compared to soft Lagrangian baselines, while maintaining competitive task performance on mathematical reasoning tasks.

Conclusion: Provides theoretically grounded and efficient solution for reward-aware distillation in resource-constrained settings.

Abstract: We introduce a novel approach to large language model (LLM) distillation by
formulating it as a constrained reinforcement learning problem. While recent
work has begun exploring the integration of task-specific rewards into
distillation processes, existing methods typically rely on ad-hoc reward
weighting. We propose a principled optimization framework that maximizes
task-specific rewards while constraining the divergence from the teacher model
to remain below a specified threshold. Our approach adapts constrained state
augmented reinforcement learning to the distillation setting, introducing a
modified reward function that maintains theoretical guarantees of constraint
satisfaction without requiring state augmentation or teacher model access
during deployment and without the computational overhead of the dual Lagrangian
methods. Through extensive experiments on mathematical reasoning tasks, we
demonstrate that our method achieves better constraint satisfaction rates and
better reasoning compared to the soft Lagrangian relaxation baselines while
maintaining competitive task performance. Our framework provides a
theoretically grounded and practically efficient solution for reward-aware
distillation in resource-constrained settings.

</details>


### [398] [MonoCon: A general framework for learning ultra-compact high-fidelity representations using monotonicity constraints](https://arxiv.org/abs/2509.22931)
*Shreyas Gokhale*

Main category: cs.LG

TL;DR: MonoCon is a framework that uses a small monotonic MLP head attached to pre-trained encoders with contrastive loss and monotonicity constraints to learn robust, disentangled, and highly compact embeddings with minimal performance cost.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning high-quality, robust, efficient, and disentangled representations in AI, which current methods primarily tackle through architectural and optimization constraints.

Method: Attach a small monotonic multi-layer perceptron (MLP) head to any pre-trained encoder and train with contrastive loss and monotonicity constraints, leveraging co-adaptation between encoder and head.

Result: On CIFAR-100: 9x more compact and 1.5x more robust representations while retaining 99% of baseline's 5-NN classification accuracy. On SNLI: 3.4x more compact and 1.4x more robust with marginal STSb score reduction.

Conclusion: MonoCon provides a general domain-agnostic framework that learns robust, ultra-compact representations through functional constraints, offering unified solutions for edge computing to cloud-scale retrieval applications.

Abstract: Learning high-quality, robust, efficient, and disentangled representations is
a central challenge in artificial intelligence (AI). Deep metric learning
frameworks tackle this challenge primarily using architectural and optimization
constraints. Here, we introduce a third approach that instead relies on
$\textit{functional}$ constraints. Specifically, we present MonoCon, a simple
framework that uses a small monotonic multi-layer perceptron (MLP) head
attached to any pre-trained encoder. Due to co-adaptation between encoder and
head guided by contrastive loss and monotonicity constraints, MonoCon learns
robust, disentangled, and highly compact embeddings at a practically negligible
performance cost. On the CIFAR-100 image classification task, MonoCon yields
representations that are nearly 9x more compact and 1.5x more robust than the
fine-tuned encoder baseline, while retaining 99\% of the baseline's 5-NN
classification accuracy. We also report a 3.4x more compact and 1.4x more
robust representation on an SNLI sentence similarity task for a marginal
reduction in the STSb score, establishing MonoCon as a general domain-agnostic
framework. Crucially, these robust, ultra-compact representations learned via
functional constraints offer a unified solution to critical challenges in
disparate contexts ranging from edge computing to cloud-scale retrieval.

</details>


### [399] [Compute-Optimal Quantization-Aware Training](https://arxiv.org/abs/2509.22935)
*Aleksandr Dremov,David Grangier,Angelos Katharopoulos,Awni Hannun*

Main category: cs.LG

TL;DR: This paper investigates optimal compute allocation between full-precision (FP) and quantization-aware training (QAT) phases, finding that the loss-optimal QAT-to-FP ratio increases with total compute. The authors derive a scaling law to predict optimal ratios and propose a cooldown-QAT fusion method for efficiency.


<details>
  <summary>Details</summary>
Motivation: Previous work showed that decomposing training into FP followed by QAT phases improves accuracy over QAT alone, but the optimal compute allocation between these phases remains unclear. The authors aim to determine how different QAT durations impact final performance across various model sizes and quantization widths.

Method: Conducted extensive experiments with compute budgets from 86.0M to 2.2B parameters, QAT bit widths, and model sizes. Derived a loss scaling law using tokens-per-parameter-byte statistic to predict optimal QAT ratios. Proposed cooldown-QAT fusion that combines learning rate decay with quantization-aware training.

Result: Contrary to previous findings, the optimal QAT-to-FP ratio increases with total compute. The scaling law accurately predicts optimal ratios across model sizes and quantization widths. The cooldown-QAT fusion eliminates redundant FP updates and achieves significant compute savings.

Conclusion: The findings provide practical insights for efficient QAT planning and enable training higher-quality quantized models with the same compute budget. The scaling law can predict optimal QAT bit widths under memory constraints and compare QAT accuracy with full-precision models.

Abstract: Quantization-aware training (QAT) is a leading technique for improving the
accuracy of quantized neural networks. Previous work has shown that decomposing
training into a full-precision (FP) phase followed by a QAT phase yields
superior accuracy compared to QAT alone. However, the optimal allocation of
compute between the FP and QAT phases remains unclear. We conduct extensive
experiments with various compute budgets, QAT bit widths, and model sizes from
86.0M to 2.2B to investigate how different QAT durations impact final
performance. We demonstrate that, contrary to previous findings, the
loss-optimal ratio of QAT to FP training increases with the total amount of
compute. Moreover, the optimal fraction can be accurately predicted for a wide
range of model sizes and quantization widths using the
tokens-per-parameter-byte statistic. From experimental data, we derive a loss
scaling law that predicts both optimal QAT ratios and final model performance
across different QAT/FP compute allocation strategies and QAT bit widths. We
use the scaling law to make further predictions, which we verify
experimentally, including which QAT bit width is optimal under a given memory
constraint and how QAT accuracy with different bit widths compares to
full-precision model accuracy. Additionally, we propose a novel cooldown and
QAT fusion approach that performs learning rate decay jointly with
quantization-aware training, eliminating redundant full-precision model updates
and achieving significant compute savings. These findings provide practical
insights into efficient QAT planning and enable the training of higher-quality
quantized models with the same compute budget.

</details>


### [400] [Understanding SOAP from the Perspective of Gradient Whitening](https://arxiv.org/abs/2509.22938)
*Yanqing Lu,Letao Wang,Jinbo Liu*

Main category: cs.LG

TL;DR: SOAP shows similar convergence to Shampoo and no significant advantage over Adam or Shampoo in final loss, aligning with theoretical equivalence.


<details>
  <summary>Details</summary>
Motivation: To analyze Adam, Shampoo, and SOAP from gradient whitening perspective and establish theoretical relationships between these optimization algorithms.

Method: Theoretical analysis of preconditioners as whitening matrix approximations, establishing equivalence between idealized SOAP and Shampoo under Kronecker product assumption, with empirical validation using nanoGPT language modeling and grayscale image colorization.

Result: SOAP exhibits similar convergence rate as Shampoo, with no significant advantage over both Adam and Shampoo in final loss achieved.

Conclusion: The empirical results align with theoretical equivalence between SOAP and Shampoo, suggesting limited practical advantages of SOAP over existing methods.

Abstract: Shampoo with Adam in the Preconditioner's eigenbasis (SOAP) has recently
emerged as a promising optimization algorithm for neural network training,
achieving superior training efficiency over both Adam and Shampoo in language
modeling tasks. In this work, we analyze Adam, Shampoo, and SOAP from the
perspective of gradient whitening, interpreting their preconditioners as
approximations to the whitening matrix, which captures second-order curvature
information. We further establish a theoretical equivalence between idealized
versions of SOAP and Shampoo under the Kronecker product assumption. To
empirically evaluate these insights, we reproduce the language modeling
experiments using nanoGPT and grayscale image colorization. Our results show
that SOAP exhibits similar convergence rate as Shampoo, and no significant
advantage over both Adam and Shampoo in the final loss achieved, which aligns
with their equivalence in theory.

</details>


### [401] [SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights](https://arxiv.org/abs/2509.22944)
*Lorenz K. Müller,Philippe Bich,Jiawei Zhuang,Ahmet Çelik,Luca Benfenati,Lukas Cavigelli*

Main category: cs.LG

TL;DR: SINQ introduces a second-axis scale factor and Sinkhorn-Knopp algorithm to normalize per-row/column variances, improving post-training quantization for LLMs at low bit-widths (≤4 bits) by addressing outlier representation issues.


<details>
  <summary>Details</summary>
Motivation: Current post-training quantization methods show perplexity degradation at ≤4 bit-widths due to precision issues when representing outliers, especially problematic for calibration-free uniform quantization.

Method: Augments existing quantizers with second-axis scale factor and fast Sinkhorn-Knopp algorithm that finds scales to normalize per-row and per-column variances, minimizing matrix imbalance as quantization proxy target.

Result: Significantly improves WikiText2 and C4 perplexity against uncalibrated uniform quantization baselines on Qwen3 model family and DeepSeek-V2.5, with further enhancement possible when combined with calibration and non-uniform quantization.

Conclusion: SINQ provides an effective layer-independent method that can be trivially applied to new architectures for quantizing linear layers, addressing key limitations in current low-precision LLM deployment.

Abstract: Post-training quantization has emerged as the most widely used strategy for
deploying large language models at low precision. Still, current methods show
perplexity degradation at bit-widths less than or equal to 4, partly because
representing outliers causes precision issues in parameters that share the same
scales as these outliers. This problem is especially pronounced for
calibration-free, uniform quantization methods. We introduce SINQ to augment
existing post-training quantizers with an additional second-axis scale factor
and a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize
per-row and per-column variances, thereby minimizing a novel per-matrix proxy
target for quantization: the matrix imbalance. Our method has no interactions
between layers and can be trivially applied to new architectures to quantize
any linear layers. We evaluate our method on the Qwen3 model family and
DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against
uncalibrated uniform quantization baselines and can be further enhanced by
combining it with calibration and non-uniform quantization levels. Code to
reproduce the results of this work and to easily quantize models using SINQ is
available at https://github.com/huawei-csl/SINQ.

</details>


### [402] [Meta-Learning Fourier Neural Operators for Hessian Inversion and Enhanced Variational Data Assimilation](https://arxiv.org/abs/2509.22949)
*Hamidreza Moazzami,Asma Jamali,Nicholas Kevlahan,Rodrigo A. Vargas-Hernández*

Main category: cs.LG

TL;DR: Proposes FNO-CG method using Fourier Neural Operator to approximate inverse Hessian for data assimilation, reducing error by 62% and iterations by 17% compared to standard CG.


<details>
  <summary>Details</summary>
Motivation: Variational data assimilation methods are computationally expensive, especially when Hessian information is involved, creating a need for more efficient approaches.

Method: Meta-learning framework using Fourier Neural Operator (FNO) to approximate inverse Hessian operator across DA problems, providing initialization for conjugate gradient method.

Result: FNO-CG reduces average relative error by 62% and number of iterations by 17% compared to standard CG, with best improvements in ill-conditioned scenarios.

Conclusion: FNO-CG demonstrates robustness and efficiency for challenging data assimilation problems, particularly in ill-conditioned cases.

Abstract: Data assimilation (DA) is crucial for enhancing solutions to partial
differential equations (PDEs), such as those in numerical weather prediction,
by optimizing initial conditions using observational data. Variational DA
methods are widely used in oceanic and atmospheric forecasting, but become
computationally expensive, especially when Hessian information is involved. To
address this challenge, we propose a meta-learning framework that employs the
Fourier Neural Operator (FNO) to approximate the inverse Hessian operator
across a family of DA problems, thereby providing an effective initialization
for the conjugate gradient (CG) method. Numerical experiments on a linear
advection equation demonstrate that the resulting FNO-CG approach reduces the
average relative error by $62\%$ and the number of iterations by $17\%$
compared to the standard CG. These improvements are most pronounced in
ill-conditioned scenarios, highlighting the robustness and efficiency of FNO-CG
for challenging DA problems.

</details>


### [403] [GDR-learners: Orthogonal Learning of Generative Models for Potential Outcomes](https://arxiv.org/abs/2509.22953)
*Valentyn Melnychuk,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: The paper introduces GDR-learners, a suite of generative Neyman-orthogonal (doubly-robust) learners that estimate conditional distributions of potential outcomes from observational data, achieving quasi-oracle efficiency and double robustness.


<details>
  <summary>Details</summary>
Motivation: Existing deep generative models for estimating potential outcomes distributions lack Neyman-orthogonality and associated theoretical guarantees like quasi-oracle efficiency and double robustness.

Method: Proposed GDR-learners framework that can be instantiated with various deep generative models: conditional normalizing flows (GDR-CNFs), conditional GANs (GDR-CGANs), conditional VAEs (GDR-CVAEs), and conditional diffusion models (GDR-CDMs).

Result: GDR-learners possess quasi-oracle efficiency and rate double robustness, making them asymptotically optimal. In experiments, they outperform existing methods in estimating conditional distributions of potential outcomes.

Conclusion: The proposed GDR-learners provide a theoretically sound and flexible framework for estimating potential outcomes distributions with superior performance compared to existing methods.

Abstract: Various deep generative models have been proposed to estimate potential
outcomes distributions from observational data. However, none of them have the
favorable theoretical property of general Neyman-orthogonality and, associated
with it, quasi-oracle efficiency and double robustness. In this paper, we
introduce a general suite of generative Neyman-orthogonal (doubly-robust)
learners that estimate the conditional distributions of potential outcomes. Our
proposed GDR-learners are flexible and can be instantiated with many
state-of-the-art deep generative models. In particular, we develop GDR-learners
based on (a) conditional normalizing flows (which we call GDR-CNFs), (b)
conditional generative adversarial networks (GDR-CGANs), (c) conditional
variational autoencoders (GDR-CVAEs), and (d) conditional diffusion models
(GDR-CDMs). Unlike the existing methods, our GDR-learners possess the
properties of quasi-oracle efficiency and rate double robustness, and are thus
asymptotically optimal. In a series of (semi-)synthetic experiments, we
demonstrate that our GDR-learners are very effective and outperform the
existing methods in estimating the conditional distributions of potential
outcomes.

</details>


### [404] [Doubly-Robust LLM-as-a-Judge: Externally Valid Estimation with Imperfect Personas](https://arxiv.org/abs/2509.22957)
*Luke Guerdan,Justin Whitehouse,Kimberly Truong,Kenneth Holstein,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: Proposes a doubly-robust estimation framework to address evaluation sampling bias in GenAI systems by combining imperfect LLM persona ratings with biased human ratings to obtain valid system quality estimates.


<details>
  <summary>Details</summary>
Motivation: Address threats to external validity in GenAI evaluations caused by sampling bias between lab-based human raters and real-world deployment conditions.

Method: Uses doubly-robust estimation combining LLM-generated persona ratings (simulating human raters with specific sociodemographics) with biased human ratings, validated through Persona Simulation Framework.

Result: The framework produces statistically valid system quality estimates when either the prediction model or reweighting model is sufficiently accurate.

Conclusion: Provides principled foundation for combining imperfect persona ratings with biased human ratings to obtain valid GenAI system quality estimates.

Abstract: As Generative AI (GenAI) systems see growing adoption, a key concern involves
the external validity of evaluations, or the extent to which they generalize
from lab-based to real-world deployment conditions. Threats to the external
validity of GenAI evaluations arise when the source sample of human raters and
system outputs used to obtain a system quality estimate differs from the target
distribution at deployment time. In this work, we propose a doubly-robust
estimation framework designed to address this evaluation sampling bias. Key to
our approach is the use of "persona" ratings produced by prompting an LLM
evaluator (i.e., an LLM-as-a-judge) to behave as a human rater with specific
sociodemographic characteristics. Our doubly-robust framework combines these
informative yet imperfect persona ratings with human ratings obtained under
evaluation sampling bias to produce statistically valid system quality
estimates. In particular, we show that our approach yields valid system quality
estimates when either (i) a model trained to predict human ratings using
persona ratings and source data observed under sampling bias, or (ii) a
reweighting model that corrects for sampling bias is of sufficient quality. We
validate our framework theoretically and via a novel Persona Simulation
Framework (PSF) designed to systematically manipulate persona quality and the
degree of evaluation sampling bias present in source data. Our work provides a
principled foundation for combining imperfect persona ratings with human
ratings observed under sampling bias to obtain valid system quality estimates.

</details>


### [405] [Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces](https://arxiv.org/abs/2509.22963)
*Haitong Ma,Ofir Nabati,Aviv Rosenberg,Bo Dai,Oran Lang,Idan Szpektor,Craig Boutilier,Na Li,Shie Mannor,Lior Shani,Guy Tenneholtz*

Main category: cs.LG

TL;DR: This paper introduces a framework using discrete diffusion models as policies for reinforcement learning in large combinatorial action spaces, achieving state-of-the-art performance through stable online training and policy mirror descent.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning struggles with large combinatorial action spaces common in real-world problems, requiring more scalable and effective policy representations.

Method: Uses discrete diffusion models as policies with an efficient online training process that leverages policy mirror descent to define regularized target policy distributions, framing policy updates as distributional matching problems.

Result: Achieves state-of-the-art results and superior sample efficiency across diverse combinatorial benchmarks including DNA sequence generation, RL with macro-actions, and multi-agent systems, outperforming other baselines.

Conclusion: The proposed diffusion policy framework provides an effective solution for scaling RL to large combinatorial action spaces, demonstrating stable learning and superior performance across challenging domains.

Abstract: Reinforcement learning (RL) struggles to scale to large, combinatorial action
spaces common in many real-world problems. This paper introduces a novel
framework for training discrete diffusion models as highly effective policies
in these complex settings. Our key innovation is an efficient online training
process that ensures stable and effective policy improvement. By leveraging
policy mirror descent (PMD) to define an ideal, regularized target policy
distribution, we frame the policy update as a distributional matching problem,
training the expressive diffusion model to replicate this stable target. This
decoupled approach stabilizes learning and significantly enhances training
performance. Our method achieves state-of-the-art results and superior sample
efficiency across a diverse set of challenging combinatorial benchmarks,
including DNA sequence generation, RL with macro-actions, and multi-agent
systems. Experiments demonstrate that our diffusion policies attain superior
performance compared to other baselines.

</details>


### [406] [Functional Critic Modeling for Provably Convergent Off-Policy Actor-Critic](https://arxiv.org/abs/2509.22964)
*Qinxun Bai,Yuxuan Han,Wei Xu,Zhengyuan Zhou*

Main category: cs.LG

TL;DR: This paper introduces a novel functional critic modeling approach that addresses key challenges in off-policy actor-critic reinforcement learning, providing both theoretical convergence guarantees and practical effectiveness.


<details>
  <summary>Details</summary>
Motivation: Off-policy actor-critic methods face two main challenges: (1) the 'moving target' problem where the policy being evaluated changes continually, and (2) inefficient actor learning due to difficulty estimating exact off-policy policy gradients. Existing methods either suffer from instability or reduce to on-policy approximations.

Method: The authors propose a functional critic modeling framework that addresses both challenges. They provide theoretical analysis in linear function settings and design a neural network architecture for practical implementation.

Result: The proposed framework achieves provable convergence for off-policy actor-critic learning, which is the first convergent off-policy target-based AC algorithm. Preliminary experiments on DeepMind Control Benchmark tasks demonstrate its effectiveness.

Conclusion: Functional critic modeling provides a principled solution to the fundamental challenges in off-policy actor-critic learning, offering both theoretical guarantees and practical performance improvements.

Abstract: Off-policy reinforcement learning (RL) with function approximation offers an
effective way to improve sample efficiency by reusing past experience. Within
this setting, the actor-critic (AC) framework has achieved strong empirical
success. However, both the critic and actor learning is challenging for the
off-policy AC methods: first of all, in addition to the classic "deadly triad"
instability of off-policy evaluation, it also suffers from a "moving target"
problem, where the policy being evaluated changes continually; secondly, actor
learning becomes less efficient due to the difficulty of estimating the exact
off-policy policy gradient. The first challenge essentially reduces the problem
to repeatedly performing off-policy evaluation for changing policies. For the
second challenge, the off-policy policy gradient theorem requires a complex and
often impractical algorithm to estimate an additional emphasis critic, which is
typically neglected in practice, thereby reducing to the on-policy policy
gradient as an approximation. In this work, we introduce a novel concept of
functional critic modeling, which leads to a new AC framework that addresses
both challenges for actor-critic learning under the deadly triad setting. We
provide a theoretical analysis in the linear function setting, establishing the
provable convergence of our framework, which, to the best of our knowledge, is
the first convergent off-policy target-based AC algorithm. From a practical
perspective, we further propose a carefully designed neural network
architecture for the functional critic modeling and demonstrate its
effectiveness through preliminary experiments on widely used RL tasks from the
DeepMind Control Benchmark.

</details>


### [407] [Shape-Informed Clustering of Multi-Dimensional Functional Data via Deep Functional Autoencoders](https://arxiv.org/abs/2509.22969)
*Samuel V. Singh,Shirley Coyle,Mimi Zhang*

Main category: cs.LG

TL;DR: FAEclust is a functional autoencoder framework for clustering multi-dimensional functional data, featuring universal-approximator encoder/decoder, regularization strategies, clustering loss, and phase-invariant shape-informed clustering.


<details>
  <summary>Details</summary>
Motivation: To develop a robust framework for cluster analysis of multi-dimensional functional data that captures complex nonlinear interdependencies and is resistant to phase variations.

Method: Functional autoencoder with universal-approximator encoder and decoder, innovative regularization for functional weights/biases, clustering loss in training objective, and shape-informed clustering to handle phase variations.

Result: Established universal approximation property of the non-linear decoder and validated model effectiveness through extensive experiments.

Conclusion: FAEclust provides an effective framework for clustering multi-dimensional functional data with enhanced stability, robustness, and phase-invariant clustering capabilities.

Abstract: We introduce FAEclust, a novel functional autoencoder framework for cluster
analysis of multi-dimensional functional data, data that are random
realizations of vector-valued random functions. Our framework features a
universal-approximator encoder that captures complex nonlinear
interdependencies among component functions, and a universal-approximator
decoder capable of accurately reconstructing both Euclidean and manifold-valued
functional data. Stability and robustness are enhanced through innovative
regularization strategies applied to functional weights and biases.
Additionally, we incorporate a clustering loss into the network's training
objective, promoting the learning of latent representations that are conducive
to effective clustering. A key innovation is our shape-informed clustering
objective, ensuring that the clustering results are resistant to phase
variations in the functions. We establish the universal approximation property
of our non-linear decoder and validate the effectiveness of our model through
extensive experiments.

</details>


### [408] [OptiMind: Teaching LLMs to Think Like Optimization Experts](https://arxiv.org/abs/2509.22979)
*Zeyi Chen,Xinzhi Zhang,Humishka Zope,Hugo Barbalho,Konstantina Mellou,Marco Molinaro,Janardhan Kulkarni,Ishai Menache,Sirui Li*

Main category: cs.LG

TL;DR: This paper presents a systematic approach to improve LLM accuracy in translating natural language to mixed-integer linear programming formulations by integrating optimization expertise through data cleaning and multi-turn inference strategies.


<details>
  <summary>Details</summary>
Motivation: Current LLM approaches for mathematical programming formulation achieve limited accuracy due to scarce and noisy training data without leveraging domain knowledge, despite the fundamental importance of this task across domains.

Method: The approach first cleans training data through class-based error analysis to prevent common mistakes, then develops multi-turn inference strategies that guide LLMs with class-specific error summaries and solver feedback for iterative refinement.

Result: Experiments across multiple base LLMs show that combining cleaned data with domain-informed prompting and feedback improves formulation accuracy by 14 percentage points on average.

Conclusion: The integration of optimization expertise enables further progress toward robust LLM-assisted optimization formulation.

Abstract: Mathematical programming -- the task of expressing operations and
decision-making problems in precise mathematical language -- is fundamental
across domains, yet remains a skill-intensive process requiring operations
research expertise. Recent advances in large language models for complex
reasoning have spurred interest in automating this task, translating natural
language into executable optimization models. Current approaches, however,
achieve limited accuracy, hindered by scarce and noisy training data without
leveraging domain knowledge. In this work, we systematically integrate
optimization expertise to improve formulation accuracy for mixed-integer linear
programming, a key family of mathematical programs. Our approach first cleans
training data through class-based error analysis to explicitly prevent common
mistakes within each optimization class. We then develop multi-turn inference
strategies that guide LLMs with class-specific error summaries and solver
feedback, enabling iterative refinement. Experiments across multiple base LLMs
demonstrate that combining cleaned data with domain-informed prompting and
feedback improves formulation accuracy by 14 percentage points on average,
enabling further progress toward robust LLM-assisted optimization formulation.

</details>


### [409] [MDP modeling for multi-stage stochastic programs](https://arxiv.org/abs/2509.22981)
*David P. Morton,Oscar Dowson,Bernardo K. Pagnoncelli*

Main category: cs.LG

TL;DR: This paper extends policy graphs to handle decision-dependent uncertainty in transition probabilities and incorporates statistical learning in multi-stage stochastic programs with MDP features.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional MDPs in handling continuous state/action spaces and decision-dependent uncertainty in transition probabilities.

Method: Develops new variants of stochastic dual dynamic programming (SDDP) with approximations to handle non-convexities in the extended policy graphs framework.

Result: The approach demonstrates increased expressiveness through examples of increasing complexity, showing capability to model structured MDPs with continuous spaces.

Conclusion: The extended policy graphs framework with SDDP variants provides a powerful modeling approach for complex multi-stage stochastic programs with MDP features and decision-dependent uncertainty.

Abstract: We study a class of multi-stage stochastic programs, which incorporate
modeling features from Markov decision processes (MDPs). This class includes
structured MDPs with continuous state and action spaces. We extend policy
graphs to include decision-dependent uncertainty for one-step transition
probabilities as well as a limited form of statistical learning. We focus on
the expressiveness of our modeling approach, illustrating ideas with a series
of examples of increasing complexity. As a solution method, we develop new
variants of stochastic dual dynamic programming, including approximations to
handle non-convexities.

</details>


### [410] [T-TAMER: Provably Taming Trade-offs in ML Serving](https://arxiv.org/abs/2509.22992)
*Yuanyuan Yang,Ruimin Zhang,Jamie Morgenstern,Haifeng Xu*

Main category: cs.LG

TL;DR: T-Tamer is a framework that formalizes multi-model serving as a multi-stage decision process, proving that recall (ability to revisit earlier models) is necessary and sufficient for achieving optimal accuracy-latency trade-offs.


<details>
  <summary>Details</summary>
Motivation: Current strategies for managing trade-offs in multi-model serving (accuracy vs latency, resource usage) are largely heuristic and case-specific, lacking theoretical guarantees and general applicability.

Method: Formalizes the problem as a multi-stage decision process to determine when to exit and which model to consult, with recall as a key mechanism.

Result: Proves that strategies without recall cannot achieve constant-factor approximation to optimal trade-offs, while recall-based strategies attain optimal trade-offs in polynomial time. Experimental validation shows efficient accuracy-latency trade-offs.

Conclusion: Provides a principled foundation bridging heuristic practice with theoretical guarantees for early-exit and cascaded model design, with recall being essential for optimal performance.

Abstract: As machine learning models continue to grow in size and complexity, efficient
serving faces increasingly broad trade-offs spanning accuracy, latency,
resource usage, and other objectives. Multi-model serving further complicates
these trade-offs; for example, in cascaded models, each early-exit decision
balances latency reduction against potential accuracy loss. Despite the
pervasiveness and importance of such trade-offs, current strategies remain
largely heuristic and case-specific, limiting both their theoretical guarantees
and general applicability.
  We present a general framework, T-Tamer, which formalizes this setting as a
multi-stage decision process, where the objective is to determine both when to
exit and which model to consult. Our main result shows that recall (i.e., the
ability to revisit earlier models) is both necessary and sufficient for
achieving provable performance guarantees. In particular, we prove that
strategies without recall cannot obtain any constant-factor approximation to
the optimal trade-off, whereas recall-based strategies provably attain the
optimal trade-off in polynomial time.
  We validate our analysis through experiments on synthetic datasets and
early-exit workloads for vision and NLP benchmarks. The results show that
recall-based strategies consistently yield efficient accuracy-latency
trade-offs. We hope this work provides a principled foundation for bridging
heuristic practice with theoretical guarantees in the design of early-exit and
cascaded models.

</details>


### [411] [Analysis of Variational Autoencoders](https://arxiv.org/abs/2509.22994)
*Zachary Baker,Yuxiao Li*

Main category: cs.LG

TL;DR: Variational Sparse Autoencoder (vSAE) underperforms standard SAE despite better feature independence, due to excessive KL divergence regularization causing many dead features.


<details>
  <summary>Details</summary>
Motivation: To investigate if incorporating variational methods into SAE architectures can improve feature organization and interpretability by creating dispersive pressure in latent space.

Method: Introduced vSAE with stochastic Gaussian sampling instead of deterministic ReLU gating, using KL divergence regularization toward standard normal prior. Evaluated against standard TopK SAE on Pythia-70M transformer activations using SAE Bench, feature interpretability analysis, and t-SNE visualization.

Result: vSAE underperformed standard SAE across core metrics but excelled at feature independence and ablation metrics. KL divergence caused excessive regularization, substantially reducing living features and leading to performance degradation. vSAE features showed improved robustness but many more dead features.

Conclusion: Naive application of variational methods to SAEs does not improve feature organization or interpretability, as the KL divergence term creates excessive regularization pressure that harms performance.

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising approach for
interpreting neural network representations by learning sparse,
human-interpretable features from dense activations. We investigate whether
incorporating variational methods into SAE architectures can improve feature
organization and interpretability. We introduce the variational Sparse
Autoencoder (vSAE), which replaces deterministic ReLU gating with stochastic
sampling from learned Gaussian posteriors and incorporates KL divergence
regularization toward a standard normal prior. Our hypothesis is that this
probabilistic sampling creates dispersive pressure, causing features to
organize more coherently in the latent space while avoiding overlap. We
evaluate a Topk vSAE against a standard TopK SAE on Pythia-70M transformer
residual steam activations using comprehensive benchmarks including SAE Bench,
individual feature interpretability analysis, and global latent space
visualization through t-SNE. The vSAE underperforms standard SAE across core
evaluation metrics, though excels at feature independence and ablation metrics.
The KL divergence term creates excessive regularization pressure that
substantially reduces the fraction of living features, leading to observed
performance degradation. While vSAE features demonstrate improved robustness,
they exhibit many more dead features than baseline. Our findings suggest that
naive application of variational methods to SAEs does not improve feature
organization or interpretability.

</details>


### [412] [Sample-efficient Multiclass Calibration under $\ell_{p}$ Error](https://arxiv.org/abs/2509.23000)
*Konstantina Bairaktari,Huy L. Nguyen*

Main category: cs.LG

TL;DR: Proposes new calibration error definition interpolating between exponential and polynomial sample complexity notions, with efficient calibration algorithm using polynomial samples.


<details>
  <summary>Details</summary>
Motivation: Multiclass predictor calibration is challenging due to exponential number of possible prediction values, requiring new approaches to reduce sample complexity.

Method: Novel calibration error definition that interpolates between established notions, combined with adaptive data analysis technique with logarithmic overhead.

Result: Algorithm can calibrate predictors for most interpolation range using polynomial samples, achieving nearly optimal error dependence at one endpoint.

Conclusion: New calibration framework bridges sample complexity gap and enables efficient multiclass predictor calibration with improved theoretical guarantees.

Abstract: Calibrating a multiclass predictor, that outputs a distribution over labels,
is particularly challenging due to the exponential number of possible
prediction values. In this work, we propose a new definition of calibration
error that interpolates between two established calibration error notions, one
with known exponential sample complexity and one with polynomial sample
complexity for calibrating a given predictor. Our algorithm can calibrate any
given predictor for the entire range of interpolation, except for one endpoint,
using only a polynomial number of samples. At the other endpoint, we achieve
nearly optimal dependence on the error parameter, improving upon previous work.
A key technical contribution is a novel application of adaptive data analysis
with high adaptivity but only logarithmic overhead in the sample complexity.

</details>


### [413] [Physically Plausible Multi-System Trajectory Generation and Symmetry Discovery](https://arxiv.org/abs/2509.23003)
*Jiayin Liu,Yulong Yang,Vineet Bansal,Christine Allen-Blanchette*

Main category: cs.LG

TL;DR: SPS-GAN is a novel neural network model that combines Hamiltonian mechanics with GANs to capture dynamics of multiple systems, generalize to unseen parameters, and discover configuration space structure from arbitrary measurements.


<details>
  <summary>Details</summary>
Motivation: Existing neural network models with mechanical inductive biases are limited to single systems with fixed parameters and require known configuration spaces. SPS-GAN addresses these limitations by handling multiple systems and discovering configuration space structure automatically.

Method: Embedded Hamiltonian neural network recurrent module in conditional GAN backbone, with optimization that includes physically motivated term for sparse configuration space representation. Works with arbitrary measurement types including state-space and video frames.

Result: SPS-GAN captures multiple systems, achieves performance comparable to supervised single-system models, and demonstrates utility for trajectory prediction, video generation, and symmetry discovery.

Conclusion: The approach successfully generalizes to unseen physical parameters, discovers configuration space structure without prior knowledge, and maintains physical plausibility while handling multiple systems simultaneously.

Abstract: From metronomes to celestial bodies, mechanics underpins how the world
evolves in time and space. With consideration of this, a number of recent
neural network models leverage inductive biases from classical mechanics to
encourage model interpretability and ensure forecasted states are physical.
However, in general, these models are designed to capture the dynamics of a
single system with fixed physical parameters, from state-space measurements of
a known configuration space. In this paper we introduce Symplectic Phase Space
GAN (SPS-GAN) which can capture the dynamics of multiple systems, and
generalize to unseen physical parameters from. Moreover, SPS-GAN does not
require prior knowledge of the system configuration space. In fact, SPS-GAN can
discover the configuration space structure of the system from arbitrary
measurement types (e.g., state-space measurements, video frames). To achieve
physically plausible generation, we introduce a novel architecture which embeds
a Hamiltonian neural network recurrent module in a conditional GAN backbone. To
discover the structure of the configuration space, we optimize the conditional
time-series GAN objective with an additional physically motivated term to
encourages a sparse representation of the configuration space. We demonstrate
the utility of SPS-GAN for trajectory prediction, video generation and symmetry
discovery. Our approach captures multiple systems and achieves performance on
par with supervised models designed for single systems.

</details>


### [414] [MoE-PHDS: One MoE checkpoint for flexible runtime sparsity](https://arxiv.org/abs/2509.23012)
*Lauren. A Hannah,Soheil Zibakhsh,Kumari Nishu,Arnav Kundu,Mohammad Samragh Razlighi,Mehrdad Farajtabar,Minsik Cho*

Main category: cs.LG

TL;DR: MoE-PHDS enables a single MoE model to dynamically adjust sparsity levels at inference time without retraining, allowing flexible accuracy/latency tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Current MoE models require training multiple models for different sparsity levels, increasing costs and complicating deployment. There's a need for flexible models that can adapt to diverse latency and efficiency requirements.

Method: MoE-PHDS uses a lightweight SFT method with mixed training across sparsity levels and a curriculum at high sparsity, requiring no architectural changes.

Result: PHDS matches or exceeds oracle models, improves cross-sparsity agreement by up to 22%, and enables flexible runtime sparsity control from a single checkpoint.

Conclusion: MoE-PHDS makes global sparsity a first-class serving primitive, simplifying MoE deployment and providing predictable accuracy/latency tradeoffs from one model.

Abstract: Sparse Mixtures of Experts (MoEs) are typically trained to operate at a fixed
sparsity level, e.g. $k$ in a top-$k$ gating function. This global sparsity
level determines an operating point on the accuracy/latency curve; currently,
meeting multiple efficiency targets means training and maintaining multiple
models. This practice complicates serving, increases training and maintenance
costs, and limits flexibility in meeting diverse latency, efficiency, and
energy requirements. We show that pretrained MoEs are more robust to runtime
sparsity shifts than commonly assumed, and introduce MoE-PHDS ({\bf P}ost {\bf
H}oc {\bf D}eclared {\bf S}parsity), a lightweight SFT method that turns a
single checkpoint into a global sparsity control surface. PHDS mixes training
across sparsity levels and anchors with a short curriculum at high sparsity,
requiring no architectural changes. The result is predictable accuracy/latency
tradeoffs from one model: practitioners can ``dial $k$'' at inference time
without swapping checkpoints, changing architecture, or relying on token-level
heuristics. Experiments on OLMoE-1B-7B-0125, Qwen1.5-MoE-A2.7B, and proprietary
models fit on multiple operating points show that PHDS matches or exceeds
well-specified oracle models, improves cross-sparsity agreement by up to 22\%
vs. well-specified oracle models, and enables simplified, flexible runtime MoE
deployment by making global sparsity a first-class serving primitive.

</details>


### [415] [On the Sheafification of Higher-Order Message Passing](https://arxiv.org/abs/2509.23020)
*Jacob Hume,Pietro Liò*

Main category: cs.LG

TL;DR: This paper proposes using sheaf theory to enhance higher-order message passing in topological deep learning, addressing limitations of the Hodge Laplacian by developing a more expressive sheaf Laplacian that generalizes to higher dimensions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the opaque and potentially degenerate inductive bias of the Hodge Laplacian in higher dimensions (k>0) for topological deep learning, by leveraging sheaf theory to create more expressive message passing frameworks.

Method: The method involves using sheaf theory to modify the Hodge Laplacian's diffusion interface, developing the sheaf Laplacian which correlates dimension-k data features with dimension-k sheaf cohomology - a data-aware generalization of singular cohomology.

Result: The paper develops novel theory and practice for higher-order message passing using sheaf diffusion, extending prior work on graph learning (k=0) to higher dimensions (k>0) with a self-contained introduction to sheaf theory.

Conclusion: Sheaf theory provides a natural and principled formalism for creating more expressive message passing in topological deep learning, overcoming limitations of the Hodge Laplacian through data-aware sheaf cohomology that generalizes well to higher-order relational structures.

Abstract: Recent work in Topological Deep Learning (TDL) seeks to generalize graph
learning's preeminent $message \ passing$ paradigm to more complex relational
structures: simplicial complexes, cell complexes, hypergraphs, and combinations
thereof. Many approaches to such ${higher\text{-}order \ message \ passing}$
(HOMP) admit formulation in terms of nonlinear diffusion with the Hodge
(combinatorial) Laplacian, a graded operator which carries an inductive bias
that dimension-$k$ data features correlate with dimension-$k$ topological
features encoded in the (singular) cohomology of the underlying domain. For
$k=0$ this recovers the graph Laplacian and its well-studied homophily bias. In
higher gradings, however, the Hodge Laplacian's bias is more opaque and
potentially even degenerate. In this essay, we position sheaf theory as a
natural and principled formalism for modifying the Hodge Laplacian's
diffusion-mediated interface between local and global descriptors toward more
expressive message passing. The sheaf Laplacian's inductive bias correlates
dimension-$k$ data features with dimension-$k$ $sheaf$ cohomology, a data-aware
generalization of singular cohomology. We will contextualize and novelly extend
prior theory on sheaf diffusion in graph learning ($k=0$) in such a light --
and explore how it fails to generalize to $k>0$ -- before developing novel
theory and practice for the higher-order setting. Our exposition is accompanied
by a self-contained introduction shepherding sheaves from the abstract to the
applied.

</details>


### [416] [Tracing the Representation Geometry of Language Models from Pretraining to Post-training](https://arxiv.org/abs/2509.23024)
*Melody Zixuan Li,Kumar Krishna Agrawal,Arna Ghosh,Komal Kumar Teru,Adam Santoro,Guillaume Lajoie,Blake A. Richards*

Main category: cs.LG

TL;DR: The paper investigates the geometric phases of LLM representations during training, revealing three distinct phases: initial collapse, entropy-seeking expansion, and compression-seeking consolidation, driven by cross-entropy optimization and representational bottlenecks.


<details>
  <summary>Details</summary>
Motivation: Standard training metrics fail to explain the emergence of complex capabilities in large language models, motivating a spectral analysis of representation geometry.

Method: Spectral approach using effective rank (RankMe) and eigenspectrum decay (α-ReQ) to analyze representation geometry across pretraining and post-training phases with OLMo (1B-7B) and Pythia (160M-12B) models.

Result: Uncovered consistent non-monotonic sequence of three geometric phases: initial collapse, entropy-seeking expansion (coinciding with peak n-gram memorization), and compression-seeking consolidation (marked by downstream performance improvement). Post-training shows SFT/DPO drive entropy-seeking while RLVR induces compression-seeking.

Conclusion: Training phases emerge from cross-entropy optimization under skewed token frequencies and representational bottlenecks. Post-training transforms geometry differently: SFT/DPO improve in-distribution performance but reduce robustness, while RLVR enhances reward alignment but reduces diversity.

Abstract: Standard training metrics like loss fail to explain the emergence of complex
capabilities in large language models. We take a spectral approach to
investigate the geometry of learned representations across pretraining and
post-training, measuring effective rank (RankMe) and eigenspectrum decay
($\alpha$-ReQ). With OLMo (1B-7B) and Pythia (160M-12B) models, we uncover a
consistent non-monotonic sequence of three geometric phases during
autoregressive pretraining. The initial "warmup" phase exhibits rapid
representational collapse. This is followed by an "entropy-seeking" phase,
where the manifold's dimensionality expands substantially, coinciding with peak
n-gram memorization. Subsequently, a "compression-seeking" phase imposes
anisotropic consolidation, selectively preserving variance along dominant
eigendirections while contracting others, a transition marked with significant
improvement in downstream task performance. We show these phases can emerge
from a fundamental interplay of cross-entropy optimization under skewed token
frequencies and representational bottlenecks ($d \ll |V|$). Post-training
further transforms geometry: SFT and DPO drive "entropy-seeking" dynamics to
integrate specific instructional or preferential data, improving
in-distribution performance while degrading out-of-distribution robustness.
Conversely, RLVR induces "compression-seeking", enhancing reward alignment but
reducing generation diversity.

</details>


### [417] [Understanding Catastrophic Interference On the Identifibility of Latent Representations](https://arxiv.org/abs/2509.23027)
*Yuke Li,Yujia Zheng,Tianyi Xiong,Zhenyi Wang,Heng Huang*

Main category: cs.LG

TL;DR: This paper proposes a novel theoretical framework that formulates catastrophic interference as an identification problem and introduces a two-stage training method (\ourmeos) to mitigate forgetting by identifying shared latent variables between partial-task aware and all-task aware setups.


<details>
  <summary>Details</summary>
Motivation: Catastrophic interference (catastrophic forgetting) is a fundamental challenge where machine learning models lose performance on previously learned tasks when adapting to new ones. The paper aims to better understand and model this problem from a latent representation learning perspective.

Method: The proposed method \ourmeos uses a two-stage training strategy: 1) Maximum likelihood estimation to learn latent representations from both partial-task aware (PTA) and all-task aware (ATA) configurations, 2) KL divergence optimization to identify and learn shared latent variables between these setups.

Result: Theoretical analysis demonstrates that catastrophic forgetting can be quantified by the distance between PTA and ATA setups, and this distance can be minimized through identification of shared latent variables. Empirical validations show the approach effectively mitigates catastrophic interference.

Conclusion: Identifying and learning shared latent representations between different task configurations provides both theoretical guarantees and practical performance improvements for mitigating catastrophic interference in machine learning systems, validated across synthetic and benchmark datasets.

Abstract: Catastrophic interference, also known as catastrophic forgetting, is a
fundamental challenge in machine learning, where a trained learning model
progressively loses performance on previously learned tasks when adapting to
new ones. In this paper, we aim to better understand and model the catastrophic
interference problem from a latent representation learning point of view, and
propose a novel theoretical framework that formulates catastrophic interference
as an identification problem. Our analysis demonstrates that the forgetting
phenomenon can be quantified by the distance between partial-task aware (PTA)
and all-task aware (ATA) setups. Building upon recent advances in
identifiability theory, we prove that this distance can be minimized through
identification of shared latent variables between these setups. When learning,
we propose our method \ourmeos with two-stage training strategy: First, we
employ maximum likelihood estimation to learn the latent representations from
both PTA and ATA configurations. Subsequently, we optimize the KL divergence to
identify and learn the shared latent variables. Through theoretical guarantee
and empirical validations, we establish that identifying and learning these
shared representations can effectively mitigate catastrophic interference in
machine learning systems. Our approach provides both theoretical guarantees and
practical performance improvements across both synthetic and benchmark
datasets.

</details>


### [418] [DPFNAS: Differential Privacy-Enhanced Federated Neural Architecture Search for 6G Edge Intelligence](https://arxiv.org/abs/2509.23030)
*Yang Lv,Jin Cao,Ben Niu,Zhe Sun,Fengwei Wang,Fenghua Li,Hui Li*

Main category: cs.LG

TL;DR: A novel federated learning framework combining personalized differential privacy and adaptive model design to address data sensitivity and heterogeneity in 6G edge networks, achieving improved accuracy while reducing model size and communication costs.


<details>
  <summary>Details</summary>
Motivation: To enable pervasive AI in 6G networks through edge intelligence while addressing challenges of data sensitivity (privacy risks from parameter sharing) and data heterogeneity (difficulty of unified global models adapting to diverse local distributions).

Method: Integrates personalized differential privacy using sample-level representations for knowledge sharing, and develops a privacy-aware neural architecture search algorithm to generate locally customized architectures and hyperparameters under privacy constraints.

Result: Achieves strong privacy guarantees while significantly outperforming state-of-the-art methods, improving accuracy by 6.82% over PerFedRLNAS on CIFAR datasets, reducing model size to 1/10 and communication cost to 1/20.

Conclusion: Proposes the first personalized differential privacy solution for representation-based federated learning with theoretical convergence guarantees, effectively balancing privacy protection and model performance in edge computing environments.

Abstract: The Sixth-Generation (6G) network envisions pervasive artificial intelligence
(AI) as a core goal, enabled by edge intelligence through on-device data
utilization. To realize this vision, federated learning (FL) has emerged as a
key paradigm for collaborative training across edge devices. However, the
sensitivity and heterogeneity of edge data pose key challenges to FL: parameter
sharing risks data reconstruction, and a unified global model struggles to
adapt to diverse local distributions. In this paper, we propose a novel
federated learning framework that integrates personalized differential privacy
(DP) and adaptive model design. To protect training data, we leverage
sample-level representations for knowledge sharing and apply a personalized DP
strategy to resist reconstruction attacks. To ensure distribution-aware
adaptation under privacy constraints, we develop a privacy-aware neural
architecture search (NAS) algorithm that generates locally customized
architectures and hyperparameters. To the best of our knowledge, this is the
first personalized DP solution tailored for representation-based FL with
theoretical convergence guarantees. Our scheme achieves strong privacy
guarantees for training data while significantly outperforming state-of-the-art
methods in model performance. Experiments on benchmark datasets such as
CIFAR-10 and CIFAR-100 demonstrate that our scheme improves accuracy by 6.82\%
over the federated NAS method PerFedRLNAS, while reducing model size to 1/10
and communication cost to 1/20.

</details>


### [419] [GuardNet: Graph-Attention Filtering for Jailbreak Defense in Large Language Models](https://arxiv.org/abs/2509.23037)
*Javad Forough,Mohammad Maheri,Hamed Haddadi*

Main category: cs.LG

TL;DR: GuardNet is a hierarchical filtering framework that detects and filters jailbreak prompts in LLMs using graph neural networks on structured graphs combining sequential, syntactic, and attention-based token relations.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to jailbreak attacks that bypass alignment constraints, posing critical risks in safety-critical domains like healthcare and finance. Existing defenses are insufficient to protect against these adversarial prompts.

Method: Constructs structured graphs combining sequential links, syntactic dependencies, and attention-derived token relations. Uses graph neural networks at two levels: prompt-level filter for global detection and token-level filter for fine-grained adversarial span identification.

Result: Significantly outperforms prior defenses: raises prompt-level F1 from 66.4% to 99.8% on LLM-Fuzzer, from 67-79% to over 94% on PLeak datasets. Token-level F1 improves from 48-75% to 74-91%, with IoU gains up to +28%. Maintains acceptable latency and generalizes well.

Conclusion: GuardNet provides a practical and robust defense against jailbreak threats in real-world LLM deployments, effectively detecting adversarial prompts while maintaining performance and generalization capabilities.

Abstract: Large Language Models (LLMs) are increasingly susceptible to jailbreak
attacks, which are adversarial prompts that bypass alignment constraints and
induce unauthorized or harmful behaviors. These vulnerabilities undermine the
safety, reliability, and trustworthiness of LLM outputs, posing critical risks
in domains such as healthcare, finance, and legal compliance. In this paper, we
propose GuardNet, a hierarchical filtering framework that detects and filters
jailbreak prompts prior to inference. GuardNet constructs structured graphs
that combine sequential links, syntactic dependencies, and attention-derived
token relations to capture both linguistic structure and contextual patterns
indicative of jailbreak behavior. It then applies graph neural networks at two
levels: (i) a prompt-level filter that detects global adversarial prompts, and
(ii) a token-level filter that pinpoints fine-grained adversarial spans.
Extensive experiments across three datasets and multiple attack settings show
that GuardNet substantially outperforms prior defenses. It raises prompt-level
F$_1$ scores from 66.4\% to 99.8\% on LLM-Fuzzer, and from 67-79\% to over 94\%
on PLeak datasets. At the token level, GuardNet improves F$_1$ from 48-75\% to
74-91\%, with IoU gains up to +28\%. Despite its structural complexity,
GuardNet maintains acceptable latency and generalizes well in cross-domain
evaluations, making it a practical and robust defense against jailbreak threats
in real-world LLM deployments.

</details>


### [420] [IsingFormer: Augmenting Parallel Tempering With Learned Proposals](https://arxiv.org/abs/2509.23043)
*Saleh Bunaiyan,Corentin Delacour,Shuvro Chowdhury,Kyle Lee,Kerem Y. Camsari*

Main category: cs.LG

TL;DR: IsingFormer is a Transformer-based neural network that generates uncorrelated spin configurations as global proposals for Parallel Tempering, accelerating both sampling in 2D Ising models and optimization in 3D spin glasses and factorization problems.


<details>
  <summary>Details</summary>
Motivation: Traditional MCMC methods like Parallel Tempering mix slowly near critical points and in rough landscapes due to reliance on local updates. The goal is to improve mixing by introducing global moves that can jump across energy barriers.

Method: Train a Transformer on equilibrium samples to generate entire spin configurations resembling the target distribution. Use these as proposals for global moves within Metropolis steps in Parallel Tempering, complementing single-spin flips.

Result: On 2D Ising models: reproduces magnetization and free-energy curves, generalizes to unseen temperatures including critical region, sharply reduces equilibration time. On 3D spin glasses: finds substantially lower-energy states. On factorization: transfers successfully to unseen semiprimes, boosting success rates.

Conclusion: Monte Carlo methods can be systematically accelerated by neural proposals that capture global structure, yielding faster sampling and stronger performance in combinatorial optimization across entire families of problem instances.

Abstract: Markov Chain Monte Carlo (MCMC) underlies both statistical physics and
combinatorial optimization, but mixes slowly near critical points and in rough
landscapes. Parallel Tempering (PT) improves mixing by swapping replicas across
temperatures, yet each replica still relies on slow local updates to change its
configuration. We introduce IsingFormer, a Transformer trained on equilibrium
samples that can generate entire spin configurations resembling those from the
target distribution. These uncorrelated samples are used as proposals for
global moves within a Metropolis step in PT, complementing the usual
single-spin flips. On 2D Ising models (sampling), IsingFormer reproduces
magnetization and free-energy curves and generalizes to unseen temperatures,
including the critical region. Injecting even a single proposal sharply reduces
equilibration time, replacing thousands of local updates. On 3D spin glasses
(optimization), PT enhanced with IsingFormer finds substantially lower-energy
states, demonstrating how global moves accelerate search in rugged landscapes.
Finally, applied to integer factorization encoded as Ising problems,
IsingFormer trained on a limited set of semiprimes transfers successfully to
unseen semiprimes, boosting success rates beyond the training distribution.
Since factorization is a canonical hard benchmark, this ability to generalize
across instances highlights the potential of learning proposals that move
beyond single problems to entire families of instances. The IsingFormer
demonstrates that Monte Carlo methods can be systematically accelerated by
neural proposals that capture global structure, yielding faster sampling and
stronger performance in combinatorial optimization.

</details>


### [421] [Beyond Aggregation: Guiding Clients in Heterogeneous Federated Learning](https://arxiv.org/abs/2509.23049)
*Zijian Wang,Xiaofei Zhang,Xin Zhang,Yukun Liu,Qiong Zhang*

Main category: cs.LG

TL;DR: This paper introduces a novel federated learning paradigm where the central server not only aggregates models but also actively guides new queries to the most appropriate clients based on their data distributions, using an empirical likelihood framework.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by healthcare applications where data privacy is crucial and statistical heterogeneity across clients (hospitals) is common. The authors question whether the central server could do more than just build models - specifically, guide new patients to hospitals best equipped for their conditions.

Method: The authors propose an empirical likelihood-based framework that simultaneously learns effective local models on each client and finds the best matching client for new queries. This enables the server to actively guide task allocation in the federated network.

Result: Empirical results on benchmark datasets show improvements in both model accuracy and the precision of client guidance compared to standard federated learning approaches.

Conclusion: This work opens a new direction for building more intelligent and resource-efficient federated systems that leverage heterogeneity as a feature rather than treating it as a problem.

Abstract: Federated learning (FL) is increasingly adopted in domains like healthcare,
where data privacy is paramount. A fundamental challenge in these systems is
statistical heterogeneity-the fact that data distributions vary significantly
across clients (e.g., different hospitals may treat distinct patient
demographics). While current FL algorithms focus on aggregating model updates
from these heterogeneous clients, the potential of the central server remains
under-explored. This paper is motivated by a healthcare scenario: could a
central server not only build a model but also guide a new patient to the
hospital best equipped for their specific condition? We generalize this idea to
propose a novel paradigm for FL systems where the server actively guides the
allocation of new tasks or queries to the most appropriate client in the
network. To enable this, we introduce an empirical likelihood-based framework
that simultaneously addresses two goals: (1) learning effective local models on
each client, and (2) finding the best matching client for a new query.
Empirical results demonstrate the framework's effectiveness on benchmark
datasets, showing improvements in both model accuracy and the precision of
client guidance compared to standard FL approaches. This work opens a new
direction for building more intelligent and resource-efficient federated
systems that leverage heterogeneity as a feature, not just a bug. Code is
available at https://github.com/zijianwang0510/FedDRM.git.

</details>


### [422] [Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding](https://arxiv.org/abs/2509.23050)
*Lin Long,Changdae Oh,Seongheon Park,Yixuan Li*

Main category: cs.LG

TL;DR: This paper presents a systematic analysis of language prior in large vision-language models through chain-of-embedding, revealing a universal Visual Integration Point (VIP) layer and introducing TVI estimator to quantify visual influence on responses.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models often default to their language prior from pre-training while under-utilizing visual evidence, but existing input-output probing methods fail to reveal the internal mechanisms of visual influence.

Method: Analyze language prior through chain-of-embedding to examine layer-wise representation dynamics, identify Visual Integration Point (VIP) where visual information reshapes representations, and introduce Total Visual Integration (TVI) estimator to quantify visual influence.

Result: Across 54 model-dataset combinations spanning 9 LVLMs and 6 benchmarks, VIP consistently emerges, and TVI reliably predicts the strength of language prior, offering a principled diagnostic toolkit.

Conclusion: The analysis reveals universal VIP phenomenon and provides TVI as a reliable estimator for understanding and diagnosing language prior in large vision-language models.

Abstract: Large vision-language models (LVLMs) achieve strong performance on multimodal
tasks, yet they often default to their language prior (LP) -- memorized textual
patterns from pre-training while under-utilizing visual evidence. Prior
analyses of LP mostly rely on input-output probing, which fails to reveal the
internal mechanisms governing when and how vision influences model behavior. To
address this gap, we present the first systematic analysis of language prior
through the lens of chain-of-embedding, which examines the layer-wise
representation dynamics within LVLMs. Our analysis reveals a universal
phenomenon: each model exhibits a Visual Integration Point (VIP), a critical
layer at which visual information begins to meaningfully reshape hidden
representations and influence decoding. Building on this observation, we
introduce the Total Visual Integration (TVI) estimator, which aggregates
representation distance beyond the VIP to quantify how strongly visual query
influences response generation. Across 54 model-dataset combinations spanning 9
contemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently
emerges, and that TVI reliably predicts the strength of language prior. This
offers a principled toolkit for diagnosing and understanding language prior in
LVLMs.

</details>


### [423] [Dynamics of Learning: Generative Schedules from Latent ODEs](https://arxiv.org/abs/2509.23052)
*Matt L. Sampson,Peter Melchior*

Main category: cs.LG

TL;DR: A new learning rate scheduler that models neural network training as a dynamical system, using hyperparameter search data to predict optimal long-term schedules, achieving SOTA results across CNN, ResNet, and transformer models.


<details>
  <summary>Details</summary>
Motivation: Existing learning rate schedules are either simple parametric functions or reactive to short-term signals, lacking a comprehensive temporal view of neural network training performance.

Method: Learns latent representations from hyperparameter search training runs, models training as dynamical system, predicts future learning rate schedules based on current metrics for optimal long-term validation performance.

Result: Achieves state-of-the-art results for image classification (CNN/ResNet) and next-token prediction (transformer), produces models in flatter loss landscape regions with better generalization.

Conclusion: The scheduler is computationally efficient, optimizer-agnostic, easily integrable with ML experiment-tracking platforms, and generalizes beyond observed training dynamics to create specialized schedules.

Abstract: The learning rate schedule is one of the most impactful aspects of neural
network optimization, yet most schedules either follow simple parametric
functions or react only to short-term training signals. None of them are
supported by a comprehensive temporal view of how well neural networks actually
train. We present a new learning rate scheduler that models the training
performance of neural networks as a dynamical system. It leverages training
runs from a hyperparameter search to learn a latent representation of the
training process. Given current training metrics, it predicts the future
learning rate schedule with the best long-term validation performance. Our
scheduler generalizes beyond previously observed training dynamics and creates
specialized schedules that deviate noticeably from common parametric functions.
It achieves SOTA results for image classification with CNN and ResNet models as
well as for next-token prediction with a transformer model. The trained models
are located in flatter regions of the loss landscape and thus provide better
generalization than those trained with other schedules. Our method is
computationally efficient, optimizer-agnostic, and can easily be layered on top
of ML experiment-tracking platforms. An implementation of our scheduler will be
made available after acceptance.

</details>


### [424] [Beyond Model Ranking: Predictability-Aligned Evaluation for Time Series Forecasting](https://arxiv.org/abs/2509.23074)
*Wanjin Feng,Yuan Yuan,Jingtao Ding,Yong Li*

Main category: cs.LG

TL;DR: A novel predictability-aligned diagnostic framework using spectral coherence to address flaws in standard time series forecasting evaluation metrics, introducing SCP to measure data unpredictability and LUR to assess model efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard evaluation metrics conflate model performance with data's intrinsic unpredictability, leading to unfair model comparisons and limited understanding of true model capabilities.

Method: Developed Spectral Coherence Predictability (SCP) score to quantify forecasting difficulty and Linear Utilization Ratio (LUR) to measure how effectively models exploit linearly predictable information.

Result: Revealed 'predictability drift' showing forecasting difficulty varies over time, and identified architectural trade-off: complex models excel on low-predictability data while linear models perform well on predictable tasks.

Conclusion: Advocates for paradigm shift from simplistic aggregate scores to predictability-aware evaluation for fairer model comparisons and deeper understanding of model behavior.

Abstract: In the era of increasingly complex AI models for time series forecasting,
progress is often measured by marginal improvements on benchmark leaderboards.
However, this approach suffers from a fundamental flaw: standard evaluation
metrics conflate a model's performance with the data's intrinsic
unpredictability. To address this pressing challenge, we introduce a novel,
predictability-aligned diagnostic framework grounded in spectral coherence. Our
framework makes two primary contributions: the Spectral Coherence
Predictability (SCP), a computationally efficient ($O(N\log N)$) and
task-aligned score that quantifies the inherent difficulty of a given
forecasting instance, and the Linear Utilization Ratio (LUR), a
frequency-resolved diagnostic tool that precisely measures how effectively a
model exploits the linearly predictable information within the data. We
validate our framework's effectiveness and leverage it to reveal two core
insights. First, we provide the first systematic evidence of "predictability
drift", demonstrating that a task's forecasting difficulty varies sharply over
time. Second, our evaluation reveals a key architectural trade-off: complex
models are superior for low-predictability data, whereas linear models are
highly effective on more predictable tasks. We advocate for a paradigm shift,
moving beyond simplistic aggregate scores toward a more insightful,
predictability-aware evaluation that fosters fairer model comparisons and a
deeper understanding of model behavior.

</details>


### [425] [CLAD-Net: Continual Activity Recognition in Multi-Sensor Wearable Systems](https://arxiv.org/abs/2509.23077)
*Reza Rahimi Azghan,Gautham Krishna Gudur,Mohit Malu,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: CLAD-Net is a continual learning framework for wearable sensor-based human activity recognition that addresses catastrophic forgetting and label scarcity by combining self-supervised transformers with supervised CNNs using knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for human activity recognition assume stationary data distributions, but real-world scenarios face distribution shifts between subjects and suffer from catastrophic forgetting in continual learning settings, compounded by limited labeled data.

Method: CLAD-Net integrates a self-supervised transformer as long-term memory to capture global activity patterns across body sensors, and a supervised CNN trained via knowledge distillation for activity classification, enabling continuous model updates without forgetting prior knowledge.

Result: On PAMAP2 dataset, CLAD-Net achieves 91.36% final accuracy with only 8.78% forgetting, outperforming memory-based and regularization-based baselines. It maintains strong performance even with only 10-20% labeled data in semi-supervised settings.

Conclusion: CLAD-Net effectively addresses catastrophic forgetting and label scarcity in continual learning for wearable sensor-based activity recognition, demonstrating robust performance through its hybrid transformer-CNN architecture with knowledge distillation.

Abstract: The rise of deep learning has greatly advanced human behavior monitoring
using wearable sensors, particularly human activity recognition (HAR). While
deep models have been widely studied, most assume stationary data distributions
- an assumption often violated in real-world scenarios. For example, sensor
data from one subject may differ significantly from another, leading to
distribution shifts. In continual learning, this shift is framed as a sequence
of tasks, each corresponding to a new subject. Such settings suffer from
catastrophic forgetting, where prior knowledge deteriorates as new tasks are
learned. This challenge is compounded by the scarcity and inconsistency of
labeled data in human studies. To address these issues, we propose CLAD-Net
(Continual Learning with Attention and Distillation), a framework enabling
wearable-sensor models to be updated continuously without sacrificing
performance on past tasks. CLAD-Net integrates a self-supervised transformer,
acting as long-term memory, with a supervised Convolutional Neural Network
(CNN) trained via knowledge distillation for activity classification. The
transformer captures global activity patterns through cross-attention across
body-mounted sensors, learning generalizable representations without labels.
Meanwhile, the CNN leverages knowledge distillation to retain prior knowledge
during subject-wise fine-tuning. On PAMAP2, CLAD-Net achieves 91.36 percent
final accuracy with only 8.78 percent forgetting, surpassing memory-based and
regularization-based baselines such as Experience Replay and Elastic Weight
Consolidation. In semi-supervised settings with only 10-20 percent labeled
data, CLAD-Net still delivers strong performance, demonstrating robustness to
label scarcity. Ablation studies further validate each module's contribution.

</details>


### [426] [Signal Preserving Weight Initialization for Odd-Sigmoid Activations](https://arxiv.org/abs/2509.23085)
*Hyunwoo Lee,Hayoung Choi,Hyunju Kim*

Main category: cs.LG

TL;DR: Proposes a novel initialization method for odd sigmoid activation functions that prevents saturation and variance collapse, enabling reliable training without normalization layers.


<details>
  <summary>Details</summary>
Motivation: Activation functions and weight initialization are interdependent - improper initialization can cause saturation, variance collapse, and learning rate sensitivity issues that hinder training.

Method: Defines an odd sigmoid function class and provides a closed-form initialization method that selects noise scale to keep forward activations well dispersed up to target layers, avoiding collapse to zero or saturation.

Result: The method trains reliably without normalization layers, shows strong data efficiency, and enables learning for activations where standard initialization methods (Xavier, He, Orthogonal) often fail to converge.

Conclusion: The proposed initialization approach successfully addresses the interdependence between activations and initialization, allowing effective training with various nonlinearities that would otherwise fail with standard methods.

Abstract: Activation functions critically influence trainability and expressivity, and
recent work has therefore explored a broad range of nonlinearities. However,
activations and weight initialization are interdependent: without an
appropriate initialization method, nonlinearities can cause saturation,
variance collapse, and increased learning rate sensitivity. We address this by
defining an odd sigmoid function class and, given any activation f in this
class, proposing an initialization method tailored to f. The method selects a
noise scale in closed form so that forward activations remain well dispersed up
to a target layer, thereby avoiding collapse to zero or saturation.
Empirically, the approach trains reliably without normalization layers,
exhibits strong data efficiency, and enables learning for activations under
which standard initialization methods (Xavier, He, Orthogonal) often do not
converge reliably.

</details>


### [427] [Unleashing Flow Policies with Distributional Critics](https://arxiv.org/abs/2509.23087)
*Deshu Chen,Yuchen Liu,Zhijian Zhou,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: The paper introduces Distributional Flow Critic (DFC), a novel critic architecture that models the complete state-action return distribution using flow matching, overcoming limitations of traditional scalar critics and improving performance in offline RL tasks.


<details>
  <summary>Details</summary>
Motivation: Flow-based policies are powerful for modeling multimodal behaviors in offline RL, but their potential is limited by traditional critics that only learn scalar return estimates, lacking distributional information.

Method: DFC employs flow matching to model return distributions as continuous transformations from simple base distributions to complex target distributions, providing rich distributional Bellman targets for flow-based policies.

Result: Extensive experiments on D4RL and OGBench benchmarks show strong performance, particularly on tasks requiring multimodal action distributions, and excel in both offline and offline-to-online fine-tuning.

Conclusion: DFC successfully addresses the bottleneck of scalar critics by providing expressive distributional learning signals, enabling better utilization of flow-based policies' capabilities in complex RL scenarios.

Abstract: Flow-based policies have recently emerged as a powerful tool in offline and
offline-to-online reinforcement learning, capable of modeling the complex,
multimodal behaviors found in pre-collected datasets. However, the full
potential of these expressive actors is often bottlenecked by their critics,
which typically learn a single, scalar estimate of the expected return. To
address this limitation, we introduce the Distributional Flow Critic (DFC), a
novel critic architecture that learns the complete state-action return
distribution. Instead of regressing to a single value, DFC employs flow
matching to model the distribution of return as a continuous, flexible
transformation from a simple base distribution to the complex target
distribution of returns. By doing so, DFC provides the expressive flow-based
policy with a rich, distributional Bellman target, which offers a more stable
and informative learning signal. Extensive experiments across D4RL and OGBench
benchmarks demonstrate that our approach achieves strong performance,
especially on tasks requiring multimodal action distributions, and excels in
both offline and offline-to-online fine-tuning compared to existing methods.

</details>


### [428] [Demystifying Network Foundation Models](https://arxiv.org/abs/2509.23089)
*Sylee,Beltiukov,Satyandra Guthula,Wenbo Guo,Walter Willinger,Arpit Gupta*

Main category: cs.LG

TL;DR: Systematic analysis of Network Foundation Models (NFMs) reveals significant limitations including anisotropy, inconsistent feature sensitivity, and inability to separate high-level context, with fixes improving performance by up to +0.35 F1 score.


<details>
  <summary>Details</summary>
Motivation: To investigate latent knowledge in Network Foundation Models through hidden representations analysis rather than just downstream task performance, addressing gaps in existing evaluation methods.

Method: Three-part evaluation: Embedding Geometry Analysis for representation space utilization, Metric Alignment Assessment for domain-expert feature correspondence, and Causal Sensitivity Testing for robustness to protocol perturbations. Evaluated four state-of-the-art NFMs across five diverse network datasets.

Result: All NFMs exhibited significant anisotropy, inconsistent feature sensitivity patterns, inability to separate high-level context, payload dependency, and other limitations. Addressing these issues improved model performance by up to +0.35 F1 score without architectural changes.

Conclusion: Current Network Foundation Models have fundamental limitations in their latent representations, but systematically identifying and addressing these issues can significantly enhance performance without requiring model architecture modifications.

Abstract: This work presents a systematic investigation into the latent knowledge
encoded within Network Foundation Models (NFMs) that focuses on hidden
representations analysis rather than pure downstream task performance.
Different from existing efforts, we analyze the models through a three-part
evaluation: Embedding Geometry Analysis to assess representation space
utilization, Metric Alignment Assessment to measure correspondence with
domain-expert features, and Causal Sensitivity Testing to evaluate robustness
to protocol perturbations. Using five diverse network datasets spanning
controlled and real-world environments, we evaluate four state-of-the-art NFMs,
revealing that they all exhibit significant anisotropy, inconsistent feature
sensitivity patterns, an inability to separate the high-level context, payload
dependency, and other properties. Our work identifies numerous limitations
across all models and demonstrates that addressing them can significantly
improve model performance (by up to +0.35 $F_1$ score without architectural
changes).

</details>


### [429] [Sensitivity Analysis for Diffusion Models](https://arxiv.org/abs/2509.23092)
*Christopher Scarvelis,Justin Solomon*

Main category: cs.LG

TL;DR: This paper develops a method to compute how diffusion model outputs change with small training data perturbations, without needing retraining.


<details>
  <summary>Details</summary>
Motivation: To predict how diffusion model scores and samples would change under small training set perturbations before committing to costly retraining.

Method: Closed-form procedure using black-box access to pre-trained score model and its derivatives, extended to estimate sample sensitivity to target measure perturbations.

Result: Method is robust to numerical error, with runtime comparable to sampling and log-likelihood computation, and sensitivities correlate with actual retraining changes.

Conclusion: Enables efficient prediction of model behavior changes without retraining, providing practical sensitivity analysis for diffusion models.

Abstract: Training a diffusion model approximates a map from a data distribution $\rho$
to the optimal score function $s_t$ for that distribution. Can we differentiate
this map? If we could, then we could predict how the score, and ultimately the
model's samples, would change under small perturbations to the training set
before committing to costly retraining. We give a closed-form procedure for
computing this map's directional derivatives, relying only on black-box access
to a pre-trained score model and its derivatives with respect to its inputs. We
extend this result to estimate the sensitivity of a diffusion model's samples
to additive perturbations of its target measure, with runtime comparable to
sampling from a diffusion model and computing log-likelihoods along the sample
path. Our method is robust to numerical and approximation error, and the
resulting sensitivities correlate with changes in an image diffusion model's
samples after retraining and fine-tuning.

</details>


### [430] [Causally-Enhanced Reinforcement Policy Optimization](https://arxiv.org/abs/2509.23095)
*Xiangqi Wang,Yue Huang,Yujun Zhou,Xiaonan Luo,Kehan Guo,Xiangliang Zhang*

Main category: cs.LG

TL;DR: CE-PO is a reward-shaping framework that enhances LLM training by incorporating causal coherence along the generation pathway, reducing reward hacking and improving robustness while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs often achieve correct answers through shortcut strategies with unfaithful reasoning, degrading under small causal perturbations.

Method: Uses Jacobian-based sensitivities to estimate model-internal influence, counterfactually hardens signals, and fuses coherence score with task-accuracy via Minkowski combiner. Integrates with PPO/GRPO without architectural changes.

Result: Improves accuracy by 5.49% on average (up to 9.58%) across 4 datasets, reduces reward hacking and unfaithful chain-of-thought, and improves robustness to correlation-causation flips and counterfactual edits.

Conclusion: CE-PO effectively balances accuracy and causal coherence, enhancing LLM reasoning faithfulness and robustness without compromising performance.

Abstract: Large language models (LLMs) trained with reinforcement objectives often
achieve superficially correct answers via shortcut strategies, pairing correct
outputs with spurious or unfaithful reasoning and degrading under small causal
perturbations. We introduce Causally-Enhanced Policy Optimization (CE-PO), a
drop-in reward-shaping framework that augments policy optimization with a
differentiable proxy for causal coherence along the generation pathway from
prompt (Z) to rationale (X) to answer (Y). CE-PO estimates model-internal
influence with Jacobian-based sensitivities, counterfactually hardens these
signals to suppress nuisance cues, and fuses the resulting coherence score with
task-accuracy feedback via a Minkowski (power-mean) combiner, exposing a single
tunable between accuracy and coherence trade-off. The unified reward integrates
with PPO/GRPO without architectural changes. Across reasoning benchmarks and
causal stress tests, CE-PO reduces reward hacking and unfaithful
chain-of-thought while improving robustness to correlation-causation flips and
light counterfactual edits, all at near-parity accuracy. Experimental results
across 4 datasets show that CE-PO improves accuracy over baselines by 5.49% on
average (up to 9.58%), while improving robustness to correlation-causation
flips and light counterfactual edits.

</details>


### [431] [Towards Quantum-Ready Blockchain Fraud Detection via Ensemble Graph Neural Networks](https://arxiv.org/abs/2509.23101)
*M. Z. Haider,Tayyaba Noreen,M. Salman*

Main category: cs.LG

TL;DR: Proposed ensemble framework combining GCN, GAT, and GIN for blockchain fraud detection, achieving high recall with low false positives on Elliptic dataset, with quantum-ready design for future scalability.


<details>
  <summary>Details</summary>
Motivation: Blockchain's pseudonymous nature enables illicit activities, challenging AML enforcement; need for models capturing structural and temporal dependencies while being resilient to noise and adversarial behavior.

Method: Ensemble framework integrating Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and Graph Isomorphism Networks (GIN) with tuned soft voting, using Elliptic dataset.

Result: Achieved high recall of illicit transactions while maintaining false positive rate below 1%, outperforming individual GNN models and baseline methods.

Conclusion: Ensemble GNNs provide practical solution for real-time cryptocurrency monitoring with immediate AML utility and pathway toward quantum-enhanced financial security analytics.

Abstract: Blockchain Business applications and cryptocurrencies such as enable secure,
decentralized value transfer, yet their pseudonymous nature creates
opportunities for illicit activity, challenging regulators and exchanges in
anti money laundering (AML) enforcement. Detecting fraudulent transactions in
blockchain networks requires models that can capture both structural and
temporal dependencies while remaining resilient to noise, imbalance, and
adversarial behavior. In this work, we propose an ensemble framework that
integrates Graph Convolutional Networks (GCN), Graph Attention Networks (GAT),
and Graph Isomorphism Networks (GIN) to enhance blockchain fraud detection.
Using the real-world Elliptic dataset, our tuned soft voting ensemble achieves
high recall of illicit transactions while maintaining a false positive rate
below 1%, beating individual GNN models and baseline methods. The modular
architecture incorporates quantum-ready design hooks, allowing seamless future
integration of quantum feature mappings and hybrid quantum classical graph
neural networks. This ensures scalability, robustness, and long-term
adaptability as quantum computing technologies mature. Our findings highlight
ensemble GNNs as a practical and forward-looking solution for real-time
cryptocurrency monitoring, providing both immediate AML utility and a pathway
toward quantum-enhanced financial security analytics.

</details>


### [432] [Effective Quantization of Muon Optimizer States](https://arxiv.org/abs/2509.23106)
*Aman Gupta,Rafael Celente,Abhishek Shivanna,D. T. Braithwaite,Gregory Dexter,Shao Tang,Hiroto Udagawa,Daniel Silva,Rohan Ramanath,S. Sathiya Keerthi*

Main category: cs.LG

TL;DR: 8-bit Muon optimizer using blockwise quantization reduces memory footprint by ~74% while maintaining performance comparable to full-precision Muon, outperforming AdamW and 8-bit AdamW in LLM pretraining.


<details>
  <summary>Details</summary>
Motivation: Muon optimizer shows better efficiency than AdamW but is stateful like AdamW, requiring significant memory for storing model weights and accumulated gradients. Existing 8-bit AdamW variants use dynamic quantization for stability, but the authors aim to develop a quantized Muon optimizer that works with both linear and dynamic quantization schemes.

Method: Introduce 8-bit Muon optimizer using blockwise quantization, supporting both linear and dynamic quantization schemes. Provide theoretical analysis to explain the robustness under quantization.

Result: 8-bit Muon maintains stability under both linear and dynamic quantization, achieving ~74% memory reduction compared to full-precision Muon. In experiments with 1.6B model on 4B FineWeb tokens, it matches Muon performance while outperforming AdamW and 8-bit AdamW. Also shows competitive results when fine-tuning Llama 3.2 3B model.

Conclusion: 8-bit Muon optimizer successfully reduces memory overhead while maintaining training performance, making it a practical and efficient alternative to existing optimizers for large language model training.

Abstract: The Muon optimizer, based on matrix orthogonalization, has recently shown
faster convergence and up to 2x computational efficiency over AdamW in LLM
pretraining. Like AdamW, Muon is stateful, requiring storage of both model
weights and accumulated gradients. While 8-bit AdamW variants mitigate this
overhead using blockwise quantization, they are typically stable only under
dynamic quantization - which improves stability on linear quantization for
extreme values. In this paper, we introduce the 8-bit Muon optimizer using
blockwise quantization, supporting both linear and dynamic schemes. We
demonstrate that 8-bit Muon maintains stability under both, while delivering
$\sim$74\% reduction in memory footprint compared to full-precision Muon. In
extensive experiments, 8-bit Muon closely matches the performance of Muon while
outperforming AdamW and 8-bit AdamW in pre-training a 1.6B model on 4B FineWeb
tokens. It also shows competitive results when fine-tuning the Llama 3.2 3B
model on post-training data. We also provide a theoretical perspective to help
explain this robustness under quantization.

</details>


### [433] [RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility](https://arxiv.org/abs/2509.23115)
*Haoyu He,Haozheng Luo,Yan Chen,Qi R. Wang*

Main category: cs.LG

TL;DR: RHYTHM is a framework using LLMs for human mobility prediction with temporal tokenization and hierarchical attention to handle long-range dependencies and multi-scale periodic behaviors.


<details>
  <summary>Details</summary>
Motivation: Human mobility prediction is challenging due to complex long-range dependencies and multi-scale periodic behaviors that existing methods struggle to capture effectively.

Method: Uses temporal tokenization to partition trajectories into daily segments, encodes them as discrete tokens with hierarchical attention for daily/weekly dependencies, and enriches tokens with pre-computed prompt embeddings via frozen LLM.

Result: Achieves 2.4% improvement in overall accuracy, 5.0% increase on weekends, and 24.6% reduction in training time compared to state-of-the-art methods on three real-world datasets.

Conclusion: RHYTHM effectively leverages LLMs as spatio-temporal predictors with temporal tokenization and hierarchical attention, demonstrating significant improvements in accuracy and efficiency for human mobility prediction.

Abstract: Predicting human mobility is inherently challenging due to complex long-range
dependencies and multi-scale periodic behaviors. To address this, we introduce
RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility),
a unified framework that leverages large language models (LLMs) as
general-purpose spatio-temporal predictors and trajectory reasoners.
Methodologically, RHYTHM employs temporal tokenization to partition each
trajectory into daily segments and encode them as discrete tokens with
hierarchical attention that captures both daily and weekly dependencies,
thereby significantly reducing the sequence length while preserving cyclical
information. Additionally, we enrich token representations by adding
pre-computed prompt embeddings for trajectory segments and prediction targets
via a frozen LLM, and feeding these combined embeddings back into the LLM
backbone to capture complex interdependencies. Computationally, RHYTHM freezes
the pretrained LLM's backbone to reduce attention complexity and memory cost.
We evaluate our model against state-of-the-art methods using three real-world
datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a
5.0% increase on weekends, and a 24.6% reduction in training time. Code is
publicly available at https://github.com/he-h/rhythm.

</details>


### [434] [Impute-MACFM: Imputation based on Mask-Aware Flow Matching](https://arxiv.org/abs/2509.23126)
*Dengyi Liu,Honggang Wang,Hua Fang*

Main category: cs.LG

TL;DR: Impute-MACFM is a mask-aware conditional flow matching framework for tabular data imputation that handles all missingness mechanisms (MCAR, MAR, MNAR) with stable training and efficient inference.


<details>
  <summary>Details</summary>
Motivation: Tabular data, especially in healthcare, often has missing values that undermine model reliability. Existing methods either make restrictive assumptions, struggle with complex feature structures, or suffer from instability and costly inference.

Method: Uses mask-aware conditional flow matching with trajectories only on missing entries, stability penalties on observed positions, consistency regularization, and time-decayed noise injection. Inference uses constraint-preserving ODE integration with per-step projection.

Result: Achieves state-of-the-art results across diverse benchmarks, delivering more robust, efficient, and higher-quality imputation than competing approaches.

Conclusion: Flow matching is established as a promising direction for tabular missing-data problems, particularly for longitudinal data.

Abstract: Tabular data are central to many applications, especially longitudinal data
in healthcare, where missing values are common, undermining model fidelity and
reliability. Prior imputation methods either impose restrictive assumptions or
struggle with complex cross-feature structure, while recent generative
approaches suffer from instability and costly inference. We propose
Impute-MACFM, a mask-aware conditional flow matching framework for tabular
imputation that addresses missingness mechanisms, missing completely at random,
missing at random, and missing not at random. Its mask-aware objective builds
trajectories only on missing entries while constraining predicted velocity to
remain near zero on observed entries, using flexible nonlinear schedules.
Impute-MACFM combines: (i) stability penalties on observed positions, (ii)
consistency regularization enforcing local invariance, and (iii) time-decayed
noise injection for numeric features. Inference uses constraint-preserving
ordinary differential equation integration with per-step projection to fix
observed values, optionally aggregating multiple trajectories for robustness.
Across diverse benchmarks, Impute-MACFM achieves state-of-the-art results while
delivering more robust, efficient, and higher-quality imputation than competing
approaches, establishing flow matching as a promising direction for tabular
missing-data problems, including longitudinal data.

</details>


### [435] [C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning](https://arxiv.org/abs/2509.23129)
*Haotian Liu,Shuo Wang,Hongteng Xu*

Main category: cs.LG

TL;DR: C²GSPG is a confidence-calibration group sequence policy gradient method that enhances reasoning performance while suppressing overconfidence in reinforcement learning models.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods like GRPO suffer from overconfidence issues that prevent achieving self-aware reasoning models, limiting their effectiveness in reasoning tasks.

Method: Proposes Group Sequence Policy Gradient (GSPG) framework to eliminate token-level bias, defines model confidence using normalized sequence-level probability, and applies cross-entropy regularizer to calibrate confidence to reward.

Result: C²GSPG shows superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration on logical and mathematical reasoning tasks.

Conclusion: The proposed confidence calibration regularizer and GSPG framework are collaborative and effective for developing better self-aware reasoning models.

Abstract: Reinforcement Learning (RL) methods, exemplified by Group Relative Policy
Optimization (GRPO) and its variants, play a central role in developing
reasoning models. However, these methods often suffer from a critical
overconfidence issue, which prevents them from achieving self-aware reasoning
models. In this study, we propose a simple yet effective confidence-calibration
group sequence policy gradient method, called C$^2$GSPG, which simultaneously
enhances reasoning performance while suppressing overconfidence. In principle,
we propose a Group Sequence Policy Gradient (GSPG) framework for learning
reasoning models, which eliminates the token-level bias commonly appearing in
GRPO and its variants. In this framework, we define the model confidence for
each reasoning problem using the normalized sequence-level probability, and
then apply a cross-entropy regularizer to calibrate the model confidence to the
sequence's reward. We demonstrate that the confidence calibration regularizer
and GSPG are collaborative for binary rewards, as their objectives always share
the same gradient direction. For non-binary rewards, we apply nonlinear reward
normalization and adaptive regularizer clipping, mitigating the potential
conflict between the two objectives. Applying C$^2$GSPG to post-train large
language models in logical and mathematical reasoning tasks, we show its
superiority over state-of-the-art methods in both reasoning accuracy and
confidence calibration. The code of C$^2$GSPG is available at
https://github.com/HaotianLiu123/CCGSPG.

</details>


### [436] [Trust Region Reward Optimization and Proximal Inverse Reward Optimization Algorithm](https://arxiv.org/abs/2509.23135)
*Yang Chen,Menglin Zou,Jiaqi Zhang,Yitan Zhang,Junyi Yang,Gael Gendron,Libo Zhang,Jiamou Liu,Michael J. Witbrock*

Main category: cs.LG

TL;DR: This paper introduces TRRO, a stable non-adversarial IRL framework that guarantees monotonic improvement in expert behavior likelihood, with practical instantiation PIRO showing strong empirical performance.


<details>
  <summary>Details</summary>
Motivation: Modern adversarial IRL methods suffer from unstable training, while recent non-adversarial approaches lack formal guarantees despite improved stability.

Method: Proposes Trust Region Reward Optimization (TRRO) framework using Minorization-Maximization to guarantee monotonic improvement, instantiated as Proximal Inverse Reward Optimization (PIRO) algorithm.

Result: PIRO matches or surpasses state-of-the-art baselines in reward recovery, policy imitation with high sample efficiency on MuJoCo, Gym-Robotics benchmarks and real-world animal behavior modeling.

Conclusion: TRRO provides IRL counterpart to TRPO's stability guarantees in forward RL, offering both theoretical guarantees and practical performance.

Abstract: Inverse Reinforcement Learning (IRL) learns a reward function to explain
expert demonstrations. Modern IRL methods often use the adversarial (minimax)
formulation that alternates between reward and policy optimization, which often
lead to unstable training. Recent non-adversarial IRL approaches improve
stability by jointly learning reward and policy via energy-based formulations
but lack formal guarantees. This work bridges this gap. We first present a
unified view showing canonical non-adversarial methods explicitly or implicitly
maximize the likelihood of expert behavior, which is equivalent to minimizing
the expected return gap. This insight leads to our main contribution: Trust
Region Reward Optimization (TRRO), a framework that guarantees monotonic
improvement in this likelihood via a Minorization-Maximization process. We
instantiate TRRO into Proximal Inverse Reward Optimization (PIRO), a practical
and stable IRL algorithm. Theoretically, TRRO provides the IRL counterpart to
the stability guarantees of Trust Region Policy Optimization (TRPO) in forward
RL. Empirically, PIRO matches or surpasses state-of-the-art baselines in reward
recovery, policy imitation with high sample efficiency on MuJoCo and
Gym-Robotics benchmarks and a real-world animal behavior modeling task.

</details>


### [437] [Beyond Heuristics: Globally Optimal Configuration of Implicit Neural Representations](https://arxiv.org/abs/2509.23139)
*Sipeng Chen,Yan Zhang,Shibo Li*

Main category: cs.LG

TL;DR: OptiINR is a unified framework that formulates INR configuration as an optimization problem using Bayesian optimization to find optimal activation functions and initialization parameters, replacing manual tuning with systematic optimization.


<details>
  <summary>Details</summary>
Motivation: Current INR practices rely on ad-hoc heuristics and manual tuning, leading to inconsistent results across different modalities. There's a need for principled strategies to optimize INR configurations systematically.

Method: Leverages Bayesian optimization to efficiently explore the joint space of discrete activation families (SIREN, WIRE, FINER) and their continuous initialization parameters, replacing manual tuning with data-driven optimization.

Result: OptiINR delivers globally optimal configurations that consistently maximize performance across diverse signal processing applications, establishing a principled foundation for INR design.

Conclusion: The framework provides a systematic approach to INR configuration that outperforms current fragmented manual tuning methods, offering consistent performance improvements across various applications.

Abstract: Implicit Neural Representations (INRs) have emerged as a transformative
paradigm in signal processing and computer vision, excelling in tasks from
image reconstruction to 3D shape modeling. Yet their effectiveness is
fundamentally limited by the absence of principled strategies for optimal
configuration - spanning activation selection, initialization scales,
layer-wise adaptation, and their intricate interdependencies. These choices
dictate performance, stability, and generalization, but current practice relies
on ad-hoc heuristics, brute-force grid searches, or task-specific tuning, often
leading to inconsistent results across modalities. This work introduces
OptiINR, the first unified framework that formulates INR configuration as a
rigorous optimization problem. Leveraging Bayesian optimization, OptiINR
efficiently explores the joint space of discrete activation families - such as
sinusoidal (SIREN), wavelet-based (WIRE), and variable-periodic (FINER) - and
their associated continuous initialization parameters. This systematic approach
replaces fragmented manual tuning with a coherent, data-driven optimization
process. By delivering globally optimal configurations, OptiINR establishes a
principled foundation for INR design, consistently maximizing performance
across diverse signal processing applications.

</details>


### [438] [TimeExpert: Boosting Long Time Series Forecasting with Temporal Mix of Experts](https://arxiv.org/abs/2509.23145)
*Xiaowen Ma,Shuning Ge,Fan Yang,Xiangyu Li,Yun Chen,Mengting Ma,Wei Zhang,Zhipeng Liu*

Main category: cs.LG

TL;DR: TimeExpert proposes a Temporal Mix of Experts (TMOE) mechanism that replaces standard attention in Transformers with adaptive expert selection to address lag effects and anomalous segments in time series forecasting.


<details>
  <summary>Details</summary>
Motivation: Standard Transformers use rigid global attention that fails to handle dynamic lag effects (varying historical relevance) and anomalous segments (noisy signals) in real-world time series data.

Method: TMOE treats key-value pairs as local experts specialized in different temporal contexts, performs localized filtering to select relevant experts for each query, and maintains a shared global expert for long-range dependencies. Integrated into PatchTST and Timer frameworks as TimeExpert and TimeExpert-G.

Result: Extensive experiments on 7 real-world long-term forecasting benchmarks show TimeExpert and TimeExpert-G outperform state-of-the-art methods.

Conclusion: The proposed TMOE mechanism effectively addresses temporal challenges in time series forecasting while maintaining Transformer strengths, demonstrating superior performance across multiple benchmarks.

Abstract: Transformer-based architectures dominate time series modeling by enabling
global attention over all timestamps, yet their rigid 'one-size-fits-all'
context aggregation fails to address two critical challenges in real-world
data: (1) inherent lag effects, where the relevance of historical timestamps to
a query varies dynamically; (2) anomalous segments, which introduce noisy
signals that degrade forecasting accuracy. To resolve these problems, we
propose the Temporal Mix of Experts (TMOE), a novel attention-level mechanism
that reimagines key-value (K-V) pairs as local experts (each specialized in a
distinct temporal context) and performs adaptive expert selection for each
query via localized filtering of irrelevant timestamps. Complementing this
local adaptation, a shared global expert preserves the Transformer's strength
in capturing long-range dependencies. We then replace the vanilla attention
mechanism in popular time-series Transformer frameworks (i.e., PatchTST and
Timer) with TMOE, without extra structural modifications, yielding our specific
version TimeExpert and general version TimeExpert-G. Extensive experiments on
seven real-world long-term forecasting benchmarks demonstrate that TimeExpert
and TimeExpert-G outperform state-of-the-art methods. Code is available at
https://github.com/xwmaxwma/TimeExpert.

</details>


### [439] [Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers](https://arxiv.org/abs/2509.23152)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.LG

TL;DR: Mirror-Critique improves LLM reasoning by training verifiers with informative critiques through contrastive learning between model-generated and ground-truth solutions, outperforming majority voting.


<details>
  <summary>Details</summary>
Motivation: Current reward model selection in test-time scaling fails to identify minority-yet-correct answers due to lack of informative critique signals during verifier training.

Method: Leverage contrastive learning between model-generated and ground-truth solutions, use small instruction-tuned model to synthesize high-quality critique data with rejection sampling, and train Mirror-Verifier that generates multiple critiques per solution.

Result: Significantly outperforms majority voting in solution accuracy and improves solver's honesty to recognize and abstain from answering beyond capability boundaries.

Conclusion: Mirror-Critique framework effectively bridges the gap in verifier training by providing informative critiques, leading to better reasoning performance and model honesty.

Abstract: Test-time scaling via solution sampling and aggregation has become a key
paradigm for improving the reasoning performance of Large Language Models
(LLMs). While reward model selection is commonly employed in this approach, it
often fails to identify minority-yet-correct answers, which limits its
effectiveness beyond that of simple majority voting. We argue that this
limitation stems from a lack of informative critique signals during verifier
training. To bridge this gap, we introduce Mirror-Critique, a framework that
trains a verifier with informative critiques. Our key insight is to leverage
the rich critique signal by contrasting model-generated solutions with
ground-truth solutions. We deploy a small instruction-tuned model to synthesize
high-quality critique data with rejection sampling that teaches the verifier
not only what is wrong, but also why. The synthetic data is used to cold-start
the LLMs in the RLVR process to further improve the verification ability. The
resulting Mirror-Verifier is deployed to evaluate candidate solutions by
generating multiple critiques per solution, aggregating them into a verify
score used for weighted voting or selective abstention. The experimental
results show that our Mirror-Verifier significantly outperforms majority voting
in terms of solution accuracy and also improves the solver's honesty to
recognize and abstain from answering beyond its capability boundaries.

</details>


### [440] [CrystalGym: A New Benchmark for Materials Discovery Using Reinforcement Learning](https://arxiv.org/abs/2509.23156)
*Prashant Govindarajan,Mathieu Reymond,Antoine Clavaud,Mariano Phielipp,Santiago Miret,Sarath Chandar*

Main category: cs.LG

TL;DR: CrystalGym is an open-source RL environment for crystalline material discovery that enables direct DFT feedback in material design through reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of incorporating direct DFT signals in material design loops due to DFT's high computational cost, and to provide a test bed for RL methods in real-world material science applications.

Method: Proposed CrystalGym environment that benchmarks value- and policy-based RL algorithms for designing crystals conditioned on target properties like band gap, bulk modulus, and density, with rewards directly calculated from DFT.

Result: Different RL algorithms showed varying sample efficiencies and convergence patterns across CrystalGym tasks, with none solving all tasks completely. A case study demonstrated fine-tuning LLMs with RL for DFT-based rewards.

Conclusion: CrystalGym serves as a valuable test bed for RL researchers and material scientists, introducing new challenges for RL methods dealing with time-consuming reward signals and enabling future interdisciplinary research.

Abstract: In silico design and optimization of new materials primarily relies on
high-accuracy atomic simulators that perform density functional theory (DFT)
calculations. While recent works showcase the strong potential of machine
learning to accelerate the material design process, they mostly consist of
generative approaches that do not use direct DFT signals as feedback to improve
training and generation mainly due to DFT's high computational cost. To aid the
adoption of direct DFT signals in the materials design loop through online
reinforcement learning (RL), we propose CrystalGym, an open-source RL
environment for crystalline material discovery. Using CrystalGym, we benchmark
common value- and policy-based reinforcement learning algorithms for designing
various crystals conditioned on target properties. Concretely, we optimize for
challenging properties like the band gap, bulk modulus, and density, which are
directly calculated from DFT in the environment. While none of the algorithms
we benchmark solve all CrystalGym tasks, our extensive experiments and
ablations show different sample efficiencies and ease of convergence to
optimality for different algorithms and environment settings. Additionally, we
include a case study on the scope of fine-tuning large language models with
reinforcement learning for improving DFT-based rewards. Our goal is for
CrystalGym to serve as a test bed for reinforcement learning researchers and
material scientists to address these real-world design problems with practical
applications. We therefore introduce a novel class of challenges for
reinforcement learning methods dealing with time-consuming reward signals,
paving the way for future interdisciplinary research for machine learning
motivated by real-world applications.

</details>


### [441] [Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization](https://arxiv.org/abs/2509.23158)
*Yufei Shen,Ji Hwan Park,Minchao Huang,Jared F. Benge,Justin F. Rousseau,Rosemary A. Lester-Smith,Edison Thomaz*

Main category: cs.LG

TL;DR: LSTM model with routine-aware augmentation and demographic personalization improves cognitive impairment detection from smartphone sensing data in older adults.


<details>
  <summary>Details</summary>
Motivation: Early detection of cognitive impairment is critical but infrequent clinical assessments lack sensitivity and temporal resolution to capture subtle cognitive declines.

Method: Implemented LSTM model with two techniques: routine-aware augmentation (generates synthetic sequences by replacing days with behaviorally similar alternatives) and demographic personalization (reweights training samples to emphasize demographically similar individuals).

Result: Joint techniques improved AUPRC from 0.637 to 0.766 on 6-month data from 36 older adults.

Conclusion: Demonstrates potential of scalable cognitive impairment monitoring using passive smartphone sensing with enhanced model generalizability.

Abstract: Early detection of cognitive impairment is critical for timely diagnosis and
intervention, yet infrequent clinical assessments often lack the sensitivity
and temporal resolution to capture subtle cognitive declines in older adults.
Passive smartphone sensing has emerged as a promising approach for naturalistic
and continuous cognitive monitoring. Building on this potential, we implemented
a Long Short-Term Memory (LSTM) model to detect cognitive impairment from
sequences of daily behavioral features, derived from multimodal sensing data
collected in an ongoing one-year study of older adults. Our key contributions
are two techniques to enhance model generalizability across participants: (1)
routine-aware augmentation, which generates synthetic sequences by replacing
each day with behaviorally similar alternatives, and (2) demographic
personalization, which reweights training samples to emphasize those from
individuals demographically similar to the test participant. Evaluated on
6-month data from 36 older adults, these techniques jointly improved the Area
Under the Precision-Recall Curve (AUPRC) of the model trained on sensing and
demographic features from 0.637 to 0.766, highlighting the potential of
scalable monitoring of cognitive impairment in aging populations with passive
sensing.

</details>


### [442] [ProtoTS: Learning Hierarchical Prototypes for Explainable Time Series Forecasting](https://arxiv.org/abs/2509.23159)
*Ziheng Peng,Shijie Ren,Xinyue Gu,Linxiao Yang,Xiting Wang,Liang Sun*

Main category: cs.LG

TL;DR: ProtoTS is an interpretable time series forecasting framework that uses prototypical temporal patterns to provide both high accuracy and transparent decision-making through hierarchical prototype organization.


<details>
  <summary>Details</summary>
Motivation: Deep learning models lack transparency in decision-making for time series forecasting, especially in high-stakes scenarios where understanding how input variables shape temporal patterns is crucial.

Method: ProtoTS computes instance-prototype similarity using denoised representations, organizes prototypes hierarchically to capture both global temporal patterns and finer-grained local variations, enabling expert steering and multi-level interpretability.

Result: Experiments on multiple realistic benchmarks including a new LOF dataset show ProtoTS exceeds existing methods in forecast accuracy while providing expert-steerable interpretations.

Conclusion: ProtoTS successfully bridges the gap between accuracy and interpretability in time series forecasting, offering better model understanding and decision support through its prototype-based approach.

Abstract: While deep learning has achieved impressive performance in time series
forecasting, it becomes increasingly crucial to understand its decision-making
process for building trust in high-stakes scenarios. Existing interpretable
models often provide only local and partial explanations, lacking the
capability to reveal how heterogeneous and interacting input variables jointly
shape the overall temporal patterns in the forecast curve. We propose ProtoTS,
a novel interpretable forecasting framework that achieves both high accuracy
and transparent decision-making through modeling prototypical temporal
patterns. ProtoTS computes instance-prototype similarity based on a denoised
representation that preserves abundant heterogeneous information. The
prototypes are organized hierarchically to capture global temporal patterns
with coarse prototypes while capturing finer-grained local variations with
detailed prototypes, enabling expert steering and multi-level interpretability.
Experiments on multiple realistic benchmarks, including a newly released LOF
dataset, show that ProtoTS not only exceeds existing methods in forecast
accuracy but also delivers expert-steerable interpretations for better model
understanding and decision support.

</details>


### [443] [Dense associative memory on the Bures-Wasserstein space](https://arxiv.org/abs/2509.23162)
*Chandan Tankala,Krishnakumar Balasubramanian*

Main category: cs.LG

TL;DR: Extends dense associative memories from vectors to probability distributions using 2-Wasserstein distance, focusing on Gaussian densities with Bures-Wasserstein metric.


<details>
  <summary>Details</summary>
Motivation: Existing dense associative memory models are limited to vector representations, but many real-world applications involve distributional data. The paper aims to bridge classical associative memories with modern generative modeling by enabling storage and retrieval of full distributions.

Method: Defines a log-sum-exp energy over stored distributions and retrieval dynamics that aggregate optimal transport maps in a Gibbs-weighted manner. Uses 2-Wasserstein distance and focuses on Bures-Wasserstein class of Gaussian densities.

Result: Proves exponential storage capacity and provides quantitative retrieval guarantees under Wasserstein perturbations. Validates the model on synthetic and real-world distributional tasks.

Conclusion: Successfully elevates associative memory from vectors to full distributions, enabling distributional storage and retrieval in memory-augmented learning systems.

Abstract: Dense associative memories (DAMs) store and retrieve patterns via
energy-functional fixed points, but existing models are limited to vector
representations. We extend DAMs to probability distributions equipped with the
2-Wasserstein distance, focusing mainly on the Bures-Wasserstein class of
Gaussian densities. Our framework defines a log-sum-exp energy over stored
distributions and a retrieval dynamics aggregating optimal transport maps in a
Gibbs-weighted manner. Stationary points correspond to self-consistent
Wasserstein barycenters, generalizing classical DAM fixed points. We prove
exponential storage capacity, provide quantitative retrieval guarantees under
Wasserstein perturbations, and validate the model on synthetic and real-world
distributional tasks. This work elevates associative memory from vectors to
full distributions, bridging classical DAMs with modern generative modeling and
enabling distributional storage and retrieval in memory-augmented learning.

</details>


### [444] [Visual CoT Makes VLMs Smarter but More Fragile](https://arxiv.org/abs/2509.23789)
*Chunxue Xu,Yiwei Wang,Yujun Cai,Bryan Hooi,Songze Li*

Main category: cs.LG

TL;DR: This paper presents the first systematic evaluation of Visual Chain-of-Thought (CoT) robustness against image perturbations, revealing that while Visual CoT improves accuracy, it increases sensitivity to noise. The authors identify edited image patches as the primary fragility source and propose a Grounding DINO-based solution to enhance robustness.


<details>
  <summary>Details</summary>
Motivation: Visual CoT techniques enhance reasoning in VLMs by integrating explicit visual edits, but their robustness against image-level noise remains unexplored. The authors aim to systematically evaluate Visual CoT's vulnerability to visual perturbations and understand its fragility patterns.

Method: The authors create a benchmark spanning 12 image corruption types across 4 VQA datasets to compare VLMs with and without Visual CoT. They analyze intermediate reasoning components and propose a plug-and-play robustness enhancement method using Grounding DINO model to provide high-confidence local visual cues.

Result: Visual CoT consistently improves absolute accuracy on both clean and corrupted images, but increases sensitivity to input perturbations, causing sharper performance degradation compared to standard VLMs. The edited image patches are identified as the primary source of fragility.

Conclusion: The work reveals clear fragility patterns in Visual CoT and provides an effective, architecture-agnostic solution using Grounding DINO to stabilize reasoning and enhance visual robustness in Visual CoT-based VLMs.

Abstract: Chain-of-Thought (CoT) techniques have significantly enhanced reasoning in
Vision-Language Models (VLMs). Extending this paradigm, Visual CoT integrates
explicit visual edits, such as cropping or annotating regions of interest, into
the reasoning process, achieving superior multimodal performance. However, the
robustness of Visual CoT-based VLMs against image-level noise remains
unexplored. In this paper, we present the first systematic evaluation of Visual
CoT robustness under visual perturbations. Our benchmark spans 12 image
corruption types across 4 Visual Question Answering (VQA) datasets, enabling a
comprehensive comparison between VLMs that use Visual CoT, and VLMs that do
not. The results reveal that integrating Visual CoT consistently improves
absolute accuracy regardless of whether the input images are clean or corrupted
by noise; however, it also increases sensitivity to input perturbations,
resulting in sharper performance degradation compared to standard VLMs. Through
extensive analysis, we identify the intermediate reasoning components of Visual
CoT, i.e., the edited image patches , as the primary source of fragility.
Building on this analysis, we propose a plug-and-play robustness enhancement
method that integrates Grounding DINO model into the Visual CoT pipeline,
providing high-confidence local visual cues to stabilize reasoning. Our work
reveals clear fragility patterns in Visual CoT and offers an effective,
architecture-agnostic solution for enhancing visual robustness.

</details>


### [445] [F-Adapter: Frequency-Adaptive Parameter-Efficient Fine-Tuning in Scientific Machine Learning](https://arxiv.org/abs/2509.23173)
*Hangwei Zhang,Chun Kang,Yan Wang,Difan Zou*

Main category: cs.LG

TL;DR: This paper introduces Frequency-Adaptive Adapter (F-Adapter), a parameter-efficient fine-tuning method for scientific machine learning that outperforms LoRA by adaptively allocating capacity based on spectral complexity.


<details>
  <summary>Details</summary>
Motivation: Parameter-efficient fine-tuning (PEFT) has been successful in vision and language domains but remains unexplored in scientific machine learning for modeling physical systems. The authors aim to adapt PEFT techniques to pre-trained Large Operator Models (LOMs) for complex physical systems.

Method: The authors systematically study PEFT for LOMs, identify limitations of LoRA, and propose F-Adapter which allocates adapter capacity based on spectral complexity - higher dimensions for low-frequency components and lower dimensions for high-frequency components.

Result: F-Adapter establishes state-of-the-art results on multiple 3D Navier-Stokes benchmarks, significantly improving both generalization and spectral fidelity over LoRA and other PEFT techniques commonly used in LLMs.

Conclusion: This work pioneers PEFT for scientific machine learning and demonstrates F-Adapter as an effective paradigm for this domain, with theoretical justification for its superior performance over LoRA in Fourier-based operator models.

Abstract: Parameter-efficient fine-tuning (PEFT) of powerful pre-trained models for
complex downstream tasks has proven effective in vision and language
processing, yet this paradigm remains unexplored in scientific machine
learning, where the objective is to model complex physical systems. We conduct
the first systematic study of PEFT for pre-trained Large Operator Models (LOMs)
obtained by scaling variants of Fourier Neural Operator. First, we observe that
the widely used Low-Rank Adaptation (LoRA) yields markedly poorer performance
on LOMs than Adapter tuning. Then, we further theoretically establish that
stacked LoRA incurs a depth-amplified lower bound on approximation error within
Fourier layers, whereas adapters retain universal approximation capacity and,
by concentrating parameters on energy-dominant low-frequency modes, attain
exponentially decaying error with bottleneck width in the Fourier domain.
Motivated by the robust empirical gains of adapters and by our theoretical
characterization of PDE solutions as spectrally sparse, we introduce
Frequency-Adaptive Adapter (F-Adapter). F-Adapter allocates adapter capacity
based on spectral complexity, assigning higher-dimension modules to
low-frequency components and lower-dimension modules to high-frequency
components. Our F-Adapters establish state-of-the-art (SOTA) results on
multiple challenging 3D Navier-Stokes benchmarks, markedly enhancing both
generalization and spectral fidelity over LoRA and other PEFT techniques
commonly used in LLMs. To the best of our knowledge, this work is the first to
explore PEFT for scientific machine-learning and establishes F-Adapter as an
effective paradigm for this domain.

</details>


### [446] [ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse](https://arxiv.org/abs/2509.23183)
*Guohao Chen,Shuaicheng Niu,Deyu Chen,Jiahao Yang,Zitian Zhang,Mingkui Tan,Pengcheng Wu,Zhiqi Shen*

Main category: cs.LG

TL;DR: ZeroSiam is an asymmetric Siamese architecture that prevents collapse in test-time entropy minimization by using asymmetric divergence alignment with a learnable predictor and stop-gradient operator, achieving stable performance across vision and language tasks.


<details>
  <summary>Details</summary>
Motivation: Pure entropy minimization during test-time adaptation can lead to collapsed solutions like constant one-hot outputs that trivially minimize entropy without meaningful learning, favoring non-generalizable shortcuts.

Method: ZeroSiam uses an asymmetric Siamese architecture with asymmetric divergence alignment, implemented via a learnable predictor and stop-gradient operator before the classifier to prevent collapse.

Result: ZeroSiam performs more stably than prior methods with negligible overhead, works effectively on both vision adaptation and large language model reasoning tasks, and handles collapse-prone tiny models.

Conclusion: ZeroSiam successfully prevents collapse in test-time entropy minimization while absorbing and regularizing biased learning signals, demonstrating efficacy across diverse models and challenging test scenarios.

Abstract: Test-time entropy minimization helps adapt a model to novel environments and
incentivize its reasoning capability, unleashing the model's potential during
inference by allowing it to evolve and improve in real-time using its own
predictions, achieving promising performance. However, pure entropy
minimization can favor non-generalizable shortcuts, such as inflating the logit
norm and driving all predictions to a dominant class to reduce entropy, risking
collapsed solutions (e.g., constant one-hot outputs) that trivially minimize
the objective without meaningful learning. In this paper, we introduce
ZeroSiam, an efficient asymmetric Siamese architecture tailored for test-time
entropy minimization. ZeroSiam prevents collapse through asymmetric divergence
alignment, which is efficiently achieved by a learnable predictor and a
stop-gradient operator before the classifier. We provide empirical and
theoretical evidence that ZeroSiam not only prevents collapse solutions, but
also absorbs and regularizes biased learning signals, enhancing performance
even when no collapse occurs. Despite its simplicity, extensive results show
that ZeroSiam performs more stably over prior methods using negligible
overhead, demonstrating efficacy on both vision adaptation and large language
model reasoning tasks across challenging test scenarios and diverse models,
including tiny models that are particularly collapse-prone.

</details>


### [447] [Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings](https://arxiv.org/abs/2509.23893)
*Zhixin Zhang,Zeming Wei,Meng Sun*

Main category: cs.LG

TL;DR: DOC fine-tuning addresses catastrophic forgetting in LLMs by tracking functional direction drift and making new task gradients orthogonal to historical directions.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting in LLM continual learning, where models lose performance on historical tasks when fine-tuning on new data without access to past datasets.

Method: Dynamic Orthogonal Continual (DOC) fine-tuning that tracks functional direction drift and adjusts new task gradients to be orthogonal to historical function directions.

Result: Outperforms prior methods on various LLM continual learning benchmarks, effectively reducing catastrophic forgetting.

Conclusion: DOC provides a robust tool for continuous LLM fine-tuning by mitigating interference between new and old tasks through orthogonal gradient adjustment.

Abstract: Catastrophic forgetting remains a critical challenge in continual learning
for large language models (LLMs), where models struggle to retain performance
on historical tasks when fine-tuning on new sequential data without access to
past datasets. In this paper, we first reveal that the drift of functional
directions during the fine-tuning process is a key reason why existing
regularization-based methods fail in long-term LLM continual learning. To
address this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a
novel approach that tracks the drift of these functional directions and
dynamically updates them during the fine-tuning process. Furthermore, by
adjusting the gradients of new task parameters to be orthogonal to the tracked
historical function directions, our method mitigates interference between new
and old tasks. Extensive experiments on various LLM continual learning
benchmarks demonstrate that this approach outperforms prior methods,
effectively reducing catastrophic forgetting and providing a robust tool for
continuous LLM fine-tuning. Our code is available at
https://github.com/meloxxxxxx/DOC.

</details>


### [448] [CoSIFL: Collaborative Secure and Incentivized Federated Learning with Differential Privacy](https://arxiv.org/abs/2509.23190)
*Zhanhong Xie,Meifan Zhang,Lihua Yin*

Main category: cs.LG

TL;DR: CoSIFL is a federated learning framework that combines proactive alarming, local differential privacy, and Stackelberg-based incentives to address security threats and encourage client participation while maintaining privacy.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges from malicious clients and difficulties in incentivizing participants to contribute high-quality data under strict privacy requirements.

Method: CoSIFL integrates proactive alarming for security, local differential privacy for inference attacks, and a Stackelberg-based incentive scheme. It uses active alarming mechanism, robust aggregation, and Tullock contest-inspired incentive module. The framework models server-client interaction as a two-stage game.

Result: Experimental results show CoSIFL outperforms state-of-the-art solutions in improving model robustness and reducing total server costs. The server-client game admits a unique equilibrium.

Conclusion: CoSIFL effectively addresses security threats and incentivization challenges in federated learning through its integrated design of proactive alarming, privacy protection, and incentive mechanisms.

Abstract: Federated learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data locality. However, it still faces
challenges from malicious or compromised clients, as well as difficulties in
incentivizing participants to contribute high-quality data under strict privacy
requirements. Motivated by these considerations, we propose CoSIFL, a novel
framework that integrates proactive alarming for robust security and local
differential privacy (LDP) for inference attacks, together with a
Stackelberg-based incentive scheme to encourage client participation and data
sharing. Specifically, CoSIFL uses an active alarming mechanism and robust
aggregation to defend against Byzantine and inference attacks, while a Tullock
contest-inspired incentive module rewards honest clients for both data
contributions and reliable alarm triggers. We formulate the interplay between
the server and clients as a two-stage game: in the first stage, the server
determines total rewards, selects participants, and fixes global iteration
settings, whereas in the second stage, each client decides its mini-batch size,
privacy noise scale, and alerting strategy. We prove that the server-client
game admits a unique equilibrium, and analyze how clients' multi-dimensional
attributes - such as non-IID degrees and privacy budgets - jointly affect
system efficiency. Experimental results on standard benchmarks demonstrate that
CoSIFL outperforms state-of-the-art solutions in improving model robustness and
reducing total server costs, highlighting the effectiveness of our integrated
design.

</details>


### [449] [Watermarking Diffusion Language Models](https://arxiv.org/abs/2509.24368)
*Thibaud Gloaguen,Robin Staab,Nikola Jovanović,Martin Vechev*

Main category: cs.LG

TL;DR: First watermark for diffusion language models (DLMs) that works with arbitrary token generation order, achieving >99% true positive rate with minimal quality impact.


<details>
  <summary>Details</summary>
Motivation: Existing ARLM watermarks rely on sequential token generation, which doesn't work for DLMs that generate tokens in arbitrary order. Need specialized watermark for this new paradigm.

Method: Apply watermark in expectation over context when some tokens are undetermined, and promote tokens that increase watermark strength when used as context for other tokens.

Result: >99% true positive rate with minimal quality impact, similar robustness to ARLM watermarks.

Conclusion: Enables reliable DLM watermarking for the first time, addressing the unique challenges of diffusion language models.

Abstract: We introduce the first watermark tailored for diffusion language models
(DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order, in
contrast to standard autoregressive language models (ARLMs) which generate
tokens sequentially. While there has been much work in ARLM watermarking, a key
challenge when attempting to apply these schemes directly to the DLM setting is
that they rely on previously generated tokens, which are not always available
with DLM generation. In this work we address this challenge by: (i) applying
the watermark in expectation over the context even when some context tokens are
yet to be determined, and (ii) promoting tokens which increase the watermark
strength when used as context for other tokens. This is accomplished while
keeping the watermark detector unchanged. Our experimental evaluation
demonstrates that the DLM watermark leads to a >99% true positive rate with
minimal quality impact and achieves similar robustness to existing ARLM
watermarks, enabling for the first time reliable DLM watermarking.

</details>


### [450] [Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization](https://arxiv.org/abs/2509.23202)
*Vage Egiazarian,Roberto L. Castro,Denis Kuznedelev,Andrei Panferov,Eldar Kurtic,Shubhra Pandit,Alexandre Marques,Mark Kurtz,Saleh Ashkboos,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: This paper presents the first comprehensive study of MXFP4 and NVFP4 4-bit floating-point formats for LLM quantization, identifies their limitations, and proposes MR-GPTQ method with format-specific optimizations to bridge the performance gap.


<details>
  <summary>Details</summary>
Motivation: Hardware-accelerated 4-bit floating-point formats (MXFP4 and NVFP4) promise to revolutionize LLM inference but their practical benefits remain unproven, with state-of-the-art methods struggling to achieve good performance.

Method: Introduces Micro-Rotated-GPTQ (MR-GPTQ), a variant of GPTQ quantization algorithm that uses block-wise Hadamard transforms and format-specific optimizations, supported by high-performance GPU kernels with rotation fusion and fast online activation computation.

Result: Achieves speedups vs. FP16 of up to 3.6x layer-wise and 2.2x end-to-end on NVIDIA B200, and 6x layer-wise and 4x end-to-end on RTX5090. MR-GPTQ significantly boosts MXFP4 accuracy to near NVFP4 levels.

Conclusion: While FP4 is not an automatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock new accuracy-performance trade-offs for 4-bit floating-point quantization.

Abstract: The recent hardware-accelerated microscaling 4-bit floating-point formats
such as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to
revolutionize large language model (LLM) inference. Yet, their practical
benefits remain unproven. We present the first comprehensive study of MXFP4 and
NVFP4 for post-training quantization, revealing gaps between their promise and
real-world performance. Our analysis shows that state-of-the-art methods
struggle with FP4, due to two key issues: (1) NVFP4's small group size provably
neutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two
scale quantization severely degrades accuracy due to high induced error. To
bridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the
classic GPTQ quantization algorithm that tailors the quantization process to
FP4's unique properties, by using block-wise Hadamard transforms and
format-specific optimizations. We support our proposal with a set of
high-performance GPU kernels that enable the MR-GPTQ format with negligible
overhead, by rotation fusion into the weights, and fast online computation of
the activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and
2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on
RTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches
or outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the
point where it nears that of NVFP4. We conclude that, while FP4 is not an
automatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock
a new frontier of accuracy-performance trade-offs.

</details>


### [451] [Towards Monotonic Improvement in In-Context Reinforcement Learning](https://arxiv.org/abs/2509.23209)
*Wenhao Zhang,Shao Zhang,Xihuai Wang,Yang Li,Ying Wen*

Main category: cs.LG

TL;DR: The paper identifies Contextual Ambiguity as a key limitation in In-Context Reinforcement Learning (ICRL) where models fail to show continued improvement during testing, and proposes Context Value Informed ICRL (CV-ICRL) to address this issue.


<details>
  <summary>Details</summary>
Motivation: Current ICRL approaches trained on monotonic policy improvement data cannot achieve continued improvement during testing due to Contextual Ambiguity - where the model's stochastic actions generate misleading interaction histories.

Method: Proposes CV-ICRL that introduces Context Value as an explicit signal representing ideal performance achievable given current context. Uses two methods for estimating Context Value during training and testing, ensuring non-decreasing ideal performance as context expands.

Result: Experiments on Dark Room and Minigrid testbeds show CV-ICRL effectively mitigates performance degradation and improves overall ICRL abilities across various tasks and environments.

Conclusion: CV-ICRL successfully resolves Contextual Ambiguity by incorporating Context Value, enabling continued performance improvement during testing and enhancing ICRL capabilities.

Abstract: In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm
for developing agents that can rapidly adapt to new tasks by leveraging past
experiences as context, without updating their parameters. Recent approaches
train large sequence models on monotonic policy improvement data from online
RL, aiming to a continue improved testing time performance. However, our
experimental analysis reveals a critical flaw: these models cannot show a
continue improvement like the training data during testing time. Theoretically,
we identify this phenomenon as Contextual Ambiguity, where the model's own
stochastic actions can generate an interaction history that misleadingly
resembles that of a sub-optimal policy from the training data, initiating a
vicious cycle of poor action selection. To resolve the Contextual Ambiguity, we
introduce Context Value into training phase and propose Context Value Informed
ICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing
the ideal performance theoretically achievable by a policy given the current
context. As the context expands, Context Value could include more task-relevant
information, and therefore the ideal performance should be non-decreasing. We
prove that the Context Value tightens the lower bound on the performance gap
relative to an ideal, monotonically improving policy. We fruther propose two
methods for estimating Context Value at both training and testing time.
Experiments conducted on the Dark Room and Minigrid testbeds demonstrate that
CV-ICRL effectively mitigates performance degradation and improves overall ICRL
abilities across various tasks and environments. The source code and data of
this paper are available at
https://github.com/Bluixe/towards_monotonic_improvement .

</details>


### [452] [One-Shot Multi-Label Causal Discovery in High-Dimensional Event Sequences](https://arxiv.org/abs/2509.23213)
*Hugo Math,Robin Schön,Rainer Lienhart*

Main category: cs.LG

TL;DR: OSCAR is a one-shot causal autoregressive method that efficiently infers per-sequence Markov Boundaries using pretrained Transformers as density estimators, enabling scalable causal discovery without costly global conditional independence testing.


<details>
  <summary>Details</summary>
Motivation: Current causal discovery methods fail to scale to event sequences with thousands of sparse event types in domains like healthcare, cybersecurity, and vehicle diagnostics, where understanding causality is critical.

Method: Uses two pretrained Transformers as density estimators to infer per-sequence Markov Boundaries in a one-shot causal autoregressive approach, avoiding expensive global conditional independence testing.

Result: On a real-world automotive dataset with 29,100 events and 474 labels, OSCAR recovers interpretable causal structures in minutes, while classical methods fail to scale.

Conclusion: OSCAR enables practical scientific diagnostics at production scale by providing efficient, parallel causal discovery for large-scale event sequence data.

Abstract: Understanding causality in event sequences with thousands of sparse event
types is critical in domains such as healthcare, cybersecurity, or vehicle
diagnostics, yet current methods fail to scale. We present OSCAR, a one-shot
causal autoregressive method that infers per-sequence Markov Boundaries using
two pretrained Transformers as density estimators. This enables efficient,
parallel causal discovery without costly global CI testing. On a real-world
automotive dataset with 29,100 events and 474 labels, OSCAR recovers
interpretable causal structures in minutes, while classical methods fail to
scale, enabling practical scientific diagnostics at production scale.

</details>


### [453] [WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless Communications with Reinforcement Learning](https://arxiv.org/abs/2509.23219)
*Xin Li,Mengbing Liu,Yiyang Zhu,Wenhe Zhang,Li Wei,Jiancheng An,Chau Yuen*

Main category: cs.LG

TL;DR: WirelessMathLM demonstrates that compact models (0.5B-7B) can match larger models in wireless mathematics through domain-specific reinforcement learning with verifiable rewards, achieving near-GPT-4o performance while using 100x fewer parameters than DeepSeek-R1.


<details>
  <summary>Details</summary>
Motivation: Large language models fail catastrophically on specialized technical mathematics like wireless communications, which require precise manipulation of information-theoretic bounds, optimization constraints, and signal processing formulations.

Method: Use domain-specific reinforcement learning with verifiable rewards (Group Relative Policy Optimization with binary verification rewards), training directly from base checkpoints without supervised warm-start, leveraging the unique property of wireless mathematics problems having verifiable correctness.

Result: 7B model achieves 39.5% accuracy on WirelessMathBench-XL (4,027 problems from 970 papers), approaching GPT-4o (40.4%) while using about 100 times fewer parameters than DeepSeek-R1 (671B, 57.4%). GRPO training nearly doubles performance across all model scales (0.5B +11%, 3B +103%, 7B +81%).

Conclusion: Compact models can excel in specialized technical domains through verifiable reinforcement learning, with positive transfer to general mathematics benchmarks (+8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and AIME without training on these tasks).

Abstract: Large language models (LLMs) excel at general mathematical reasoning but fail
catastrophically on specialized technical mathematics. In wireless
communications, where problems require precise manipulation of
information-theoretic bounds, optimization constraints, and signal processing
formulations, even state-of-the-art models struggle to achieve competent
performance. We present WirelessMathLM, demonstrating that compact models
(0.5B-7B parameters) can match or exceed much larger models through
domain-specific reinforcement learning with verifiable rewards. Our key insight
is that wireless mathematics problems possess a unique property--verifiable
correctness--that enables effective reinforcement learning without human
feedback. We construct WirelessMathBench-XL, a comprehensive benchmark of 4,027
problems from 970 papers. Using Group Relative Policy Optimization (GRPO) with
binary verification rewards, we train models directly from base checkpoints
without supervised warm-start. Our 7B model achieves 39.5% accuracy on
WirelessMathBench-XL, approaching GPT-4o (40.4%) while using about 100 times
fewer parameters than DeepSeek-R1 (671B, 57.4%). Remarkably, GRPO training
nearly doubles performance across all model scales (0.5B +11%, 3B +103%, 7B
+81%), with positive transfer to general mathematics benchmarks--our models
gain +8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and
AIME without any training on these tasks.

</details>


### [454] [SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts](https://arxiv.org/abs/2509.23232)
*Bingshuai Liu,Ante Wang,Zijun Min,Liang Yao,Haibo Zhang,Yang Liu,Anxiang Zeng,Jinsong Su*

Main category: cs.LG

TL;DR: SPEC-RL is a novel framework that integrates speculative decoding with RL rollouts to reduce computational costs by reusing overlapping trajectory segments from consecutive training epochs, achieving 2-3x speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Current RLVR training is bottlenecked by expensive rollout computation, and existing acceleration methods have limitations like diminishing returns, bias introduction, or ignoring redundancy across iterations.

Method: SPEC-RL reuses prior trajectory segments as speculative prefixes and extends them via a draft-and-verify mechanism, avoiding redundant generation while ensuring policy consistency.

Result: Experiments on math reasoning benchmarks (GSM8K, MATH-500, OlympiadBench, MMLU-STEM) show 2-3x reduction in rollout time without compromising policy quality.

Conclusion: SPEC-RL provides a general and practical enhancement for RLVR training that integrates seamlessly with mainstream algorithms like PPO, GRPO, DAPO, enabling scalable training for large reasoning models.

Abstract: Large Language Models (LLMs) increasingly rely on reinforcement learning with
verifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning.
However, the training process remains bottlenecked by the computationally
expensive rollout stage. Existing acceleration methods-such as parallelization,
objective- and data-driven modifications, and replay buffers-either incur
diminishing returns, introduce bias, or overlook redundancy across iterations.
We identify that rollouts from consecutive training epochs frequently share a
large portion of overlapping segments, wasting computation. To address this, we
propose SPEC-RL, a novel framework that integrates SPECulative decoding with
the RL rollout process. SPEC-RL reuses prior trajectory segments as speculative
prefixes and extends them via a draft-and-verify mechanism, avoiding redundant
generation while ensuring policy consistency. Experiments on diverse math
reasoning and generalization benchmarks, including GSM8K, MATH-500,
OlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout
time by 2-3x without compromising policy quality. As a purely rollout-stage
enhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g.,
PPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large
reasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL

</details>


### [455] [More Data or Better Algorithms: Latent Diffusion Augmentation for Deep Imbalanced Regression](https://arxiv.org/abs/2509.23240)
*Shayan Alahyari*

Main category: cs.LG

TL;DR: LatentDiff is a novel framework using conditional diffusion models with priority-based generation to synthesize high-quality features in latent space for deep imbalanced regression, addressing the lack of dedicated data-level solutions for high-dimensional imbalanced regression.


<details>
  <summary>Details</summary>
Motivation: Deep imbalanced regression lacks dedicated data-level solutions suitable for high-dimensional data, while existing approaches mainly rely on algorithmic modifications. The data distribution in real-world regression tasks is often heavily skewed, causing models to fail at predicting minority labels accurately.

Method: Proposes LatentDiff framework that uses conditional diffusion models with priority-based generation to synthesize high-quality features in the latent representation space. It is computationally efficient and applicable across diverse data modalities including images, text, and other high-dimensional inputs.

Result: Experiments on three deep imbalanced regression benchmarks demonstrate substantial improvements in minority regions while maintaining overall accuracy.

Conclusion: LatentDiff effectively addresses the gap in data-level solutions for deep imbalanced regression, providing a computationally efficient framework that works across multiple data modalities and significantly improves performance on minority samples.

Abstract: In many real-world regression tasks, the data distribution is heavily skewed,
and models learn predominantly from abundant majority samples while failing to
predict minority labels accurately. While imbalanced classification has been
extensively studied, imbalanced regression remains relatively unexplored. Deep
imbalanced regression (DIR) represents cases where the input data are
high-dimensional and unstructured. Although several data-level approaches for
tabular imbalanced regression exist, deep imbalanced regression currently lacks
dedicated data-level solutions suitable for high-dimensional data and relies
primarily on algorithmic modifications. To fill this gap, we propose
LatentDiff, a novel framework that uses conditional diffusion models with
priority-based generation to synthesize high-quality features in the latent
representation space. LatentDiff is computationally efficient and applicable
across diverse data modalities, including images, text, and other
high-dimensional inputs. Experiments on three DIR benchmarks demonstrate
substantial improvements in minority regions while maintaining overall
accuracy.

</details>


### [456] [Adaptive Token-Weighted Differential Privacy for LLMs: Not All Tokens Require Equal Protection](https://arxiv.org/abs/2509.23246)
*Manjiang Yu,Priyanka Singh,Xue Li,Yang Cao*

Main category: cs.LG

TL;DR: ATDP is a novel differential privacy method that focuses noise injection on sensitive tokens, reducing training time by 90% while maintaining privacy protection and model performance.


<details>
  <summary>Details</summary>
Motivation: Existing DP-SGD methods inject uniform noise across all gradients, causing significant training time extension and accuracy reduction. There's a need for more efficient privacy protection that specifically targets sensitive information.

Method: Adaptive Token-Weighted Differential Privacy (ATDP) - modifies vanilla DP-SGD by adaptively assigning different gradient weights to sensitive and non-sensitive tokens, using larger noise scale in early training stages and minimal post-processing.

Result: ATDP achieves comparable canary protection to state-of-the-art DP-SGD methods, reduces computational overhead by approximately 90%, maintains comparable or superior privacy protection, and causes minimal accuracy degradation.

Conclusion: ATDP provides an efficient privacy-enhancing solution that can be seamlessly integrated into existing pipelines, offering targeted protection for sensitive information while preserving model capabilities.

Abstract: Large language models (LLMs) frequently memorize sensitive or personal
information, raising significant privacy concerns. Existing variants of
differential privacy stochastic gradient descent (DPSGD) inject uniform noise
into every gradient step, significantly extending training time and reducing
model accuracy. We propose that concentrating noise primarily on gradients
associated with sensitive tokens can substantially decrease DP training time,
strengthen the protection of sensitive information, and simultaneously preserve
the model's performance on non-sensitive data. We operationalize this insight
through Adaptive Token-Weighted Differential Privacy (ATDP), a modification of
vanilla DP-SGD that adaptively assigns different gradient weights to sensitive
and non-sensitive tokens. By employing a larger noise scale at the early stage
of training, ATDP rapidly disrupts memorization of sensitive content. As a
result, ATDP only requires a few additional epochs of lightweight
post-processing following standard fine-tuning, injecting targeted noise
primarily on parameters corresponding to sensitive tokens, thus minimally
affecting the model's general capabilities. ATDP can be seamlessly integrated
into any existing DP-based fine-tuning pipeline or directly applied to
non-private models as a fast privacy-enhancing measure. Additionally, combined
with an initial redacted fine-tuning phase, ATDP forms a streamlined DP
pipeline that achieves comparable canary protection to state-of-the-art DP-SGD
methods, significantly reduces the computational overhead of DP fine-tuning,
shortening training time by approximately 90 percent, while achieving
comparable or superior privacy protection and minimal accuracy degradation.

</details>


### [457] [Deep Learning for Subspace Regression](https://arxiv.org/abs/2509.23249)
*Vladimir Fanaskov,Vladislav Trifonov,Alexander Rudikov,Ekaterina Muravleva,Ivan Oseledets*

Main category: cs.LG

TL;DR: The paper proposes using neural networks for subspace regression in parametric model order reduction, addressing high-dimensional parameter spaces by predicting larger-than-needed subspaces to improve accuracy and smoothness.


<details>
  <summary>Details</summary>
Motivation: Classical interpolation methods become infeasible for high-dimensional parameter spaces in parametric model order reduction, necessitating a more robust approach.

Method: Relax interpolation to regression, use neural networks to approximate high-dimensional target functions, and introduce redundancy by predicting larger subspaces than needed.

Result: Theoretical analysis shows reduced complexity for elliptic eigenproblems and smoother mappings on Grassmann manifolds; empirical results confirm significant accuracy improvements.

Conclusion: Subspace regression with neural networks and redundancy is effective for various tasks including parametric eigenproblems, PDEs, and optimal control.

Abstract: It is often possible to perform reduced order modelling by specifying linear
subspace which accurately captures the dynamics of the system. This approach
becomes especially appealing when linear subspace explicitly depends on
parameters of the problem. A practical way to apply such a scheme is to compute
subspaces for a selected set of parameters in the computationally demanding
offline stage and in the online stage approximate subspace for unknown
parameters by interpolation. For realistic problems the space of parameters is
high dimensional, which renders classical interpolation strategies infeasible
or unreliable. We propose to relax the interpolation problem to regression,
introduce several loss functions suitable for subspace data, and use a neural
network as an approximation to high-dimensional target function. To further
simplify a learning problem we introduce redundancy: in place of predicting
subspace of a given dimension we predict larger subspace. We show theoretically
that this strategy decreases the complexity of the mapping for elliptic
eigenproblems with constant coefficients and makes the mapping smoother for
general smooth function on the Grassmann manifold. Empirical results also show
that accuracy significantly improves when larger-than-needed subspaces are
predicted. With the set of numerical illustrations we demonstrate that subspace
regression can be useful for a range of tasks including parametric
eigenproblems, deflation techniques, relaxation methods, optimal control and
solution of parametric partial differential equations.

</details>


### [458] [NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning](https://arxiv.org/abs/2509.23252)
*Raviteja Anantha,Soheil Hor,Teodor Nicola Antoniu,Layne C. Price*

Main category: cs.LG

TL;DR: NanoFlux is an adversarial framework that generates targeted training data for LLM reasoning improvement, where small datasets (<200 examples) outperform conventional fine-tuning approaches across multiple domains with significant computational efficiency gains.


<details>
  <summary>Details</summary>
Motivation: To improve LLM reasoning capabilities through targeted data generation rather than large-scale conventional fine-tuning, addressing the computational inefficiency and lack of precision in current approaches.

Method: Uses competitive adversarial dynamics between Attacker and Defender models supervised by a tool-augmented Judge, synthesizing multi-step questions with explanatory annotations. Includes embedding-based novelty filtering, tool-augmented evaluation, and multi-hop reasoning.

Result: Fine-tuning a 4B-parameter model on NanoFlux data achieved: +5.9% on GSMHard (math), +3.6% on GenomeBench (science), +16.6% on MultiMedQA (medical), with 3-14x computational reduction. Ablation studies revealed non-monotonic relationship between dataset characteristics and performance.

Conclusion: Future model improvements may come from intelligent synthesis of small, precisely targeted training datasets rather than large-scale data collection, demonstrating the effectiveness of adversarial data generation for specific reasoning capabilities.

Abstract: We present NanoFlux, a novel adversarial framework for generating targeted
training data to improve LLM reasoning, where adversarially-generated datasets
containing fewer than 200 examples outperform conventional fine-tuning
approaches. The framework employs a competitive dynamic between models
alternating as Attacker and Defender, supervised by a tool-augmented Judge,
synthesizing multi-step questions with explanatory annotations that target
specific reasoning capabilities. Fine-tuning a 4B-parameter model on
NanoFlux-generated data yields performance gains across diverse domains
compared to full-benchmark fine-tuning: +5.9% on mathematical reasoning
(GSMHard), +3.6% on scientific reasoning (GenomeBench), and +16.6% on medical
reasoning (MultiMedQA), while reducing computational requirements by 3-14x.
Ablation studies reveal a non-monotonic relationship between dataset
characteristics and model performance, uncovering domain-specific optimal
points for question complexity and reasoning quality. NanoFlux automates
training data generation through embedding-based novelty filtering,
tool-augmented evaluation, and multi-hop reasoning, suggesting that future
model improvements may lie in the intelligent synthesis of small, precisely
targeted training datasets.

</details>


### [459] [ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction](https://arxiv.org/abs/2509.23254)
*Zhang-Yu You,Jiahao Ma,Hongzong Li,Ye-Fan Hu,Jian-Dong Huang*

Main category: cs.LG

TL;DR: ABCONFORMER is a Conformer-based model that uses physics-inspired sliding attention to predict antibody-antigen interfaces from sequences alone, achieving state-of-the-art performance on SARS-CoV-2 data.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of antibody-antigen interfaces is critical for vaccine design and therapeutic development, but reliable prediction from sequences alone remains challenging.

Method: Uses Conformer backbone to capture local and global sequence features, introduces physics-inspired sliding attention for residue-level contact recovery without 3D structural data.

Result: Achieves state-of-the-art performance on SARS-CoV-2 Ab-Ag dataset, surpasses sequence-based methods for antibody-agnostic epitope prediction, with sliding attention significantly enhancing epitope prediction precision.

Conclusion: ABCONFORMER enables accurate paratope and epitope prediction from sequences, with sliding attention proving superior to conventional cross-attention for epitope prediction.

Abstract: Accurate prediction of antibody-antigen (Ab-Ag) interfaces is critical for
vaccine design, immunodiagnostics, and therapeutic antibody development.
However, achieving reliable predictions from sequences alone remains a
challenge. In this paper, we present ABCONFORMER, a model based on the
Conformer backbone that captures both local and global features of a
biosequence. To accurately capture Ab-Ag interactions, we introduced the
physics-inspired sliding attention, enabling residue-level contact recovery
without relying on three-dimensional structural data. ABConformer can
accurately predict paratopes and epitopes given the antibody and antigen
sequence, and predict pan-epitopes on the antigen without antibody information.
In comparison experiments, ABCONFORMER achieves state-of-the-art performance on
a recent SARS-CoV-2 Ab-Ag dataset, and surpasses widely used sequence-based
methods for antibody-agnostic epitope prediction. Ablation studies further
quantify the contribution of each component, demonstrating that, compared to
conventional cross-attention, sliding attention significantly enhances the
precision of epitope prediction. To facilitate reproducibility, we will release
the code under an open-source license upon acceptance.

</details>


### [460] [CREPE: Controlling Diffusion with Replica Exchange](https://arxiv.org/abs/2509.23265)
*Jiajun He,Paul Jeha,Peter Potaptchik,Leo Zhang,José Miguel Hernández-Lobato,Yuanqi Du,Saifuddin Syed,Francisco Vargas*

Main category: cs.LG

TL;DR: CREPE is a flexible inference-time control method for diffusion models using replica exchange algorithm, offering sequential particle generation, high sample diversity, and online refinement capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing inference-time control methods for diffusion models rely on heuristic guidance or Sequential Monte Carlo (SMC) with bias correction, which may have limitations in flexibility and sample diversity.

Method: Proposes CREPE (Controlling with REPlica Exchange) based on replica exchange algorithm, which generates particles sequentially, maintains high diversity after burn-in, and allows online refinement or early termination.

Result: Demonstrates versatility across various tasks including temperature annealing, reward-tilting, model composition and classifier-free guidance debiasing, with competitive performance compared to prior SMC methods.

Conclusion: CREPE provides a flexible alternative to SMC-based approaches for inference-time control of diffusion models, offering improved sample diversity and online refinement capabilities.

Abstract: Inference-time control of diffusion models aims to steer model outputs to
satisfy new constraints without retraining. Previous approaches have mostly
relied on heuristic guidance or have been coupled with Sequential Monte Carlo
(SMC) for bias correction. In this paper, we propose a flexible alternative
based on replica exchange, an algorithm designed initially for sampling
problems. We refer to this method as the CREPE (Controlling with REPlica
Exchange). Unlike SMC, CREPE: (1) generates particles sequentially, (2)
maintains high diversity in the generated samples after a burn-in period, and
(3) enables online refinement or early termination. We demonstrate its
versatility across various tasks, including temperature annealing,
reward-tilting, model composition and classifier-free guidance debiasing, with
competitive performance compared to prior SMC methods.

</details>


### [461] [Transfer Learning and Machine Learning for Training Five Year Survival Prognostic Models in Early Breast Cancer](https://arxiv.org/abs/2509.23268)
*Lisa Pilgram,Kai Yang,Ana-Alicia Beltran-Bless,Gregory R. Pond,Lisa Vandermeer,John Hilton,Marie-France Savard,Andréanne Leblanc,Lois Sheperd,Bingshu E. Chen,John M. S. Bartlett,Karen J. Taylor,Jane Bayani,Sarah L. Barker,Melanie Spears,Cornelis J. H. van der Velde,Elma Meershoek-Klein Kranenbarg,Luc Dirix,Elizabeth Mallon,Annette Hasenburg,Christos Markopoulos,Lamin Juwara,Fida K. Dankar,Mark Clemons,Khaled El Emam*

Main category: cs.LG

TL;DR: This study compares machine learning approaches for breast cancer survival prognostication, showing that transfer learning, de-novo Random Survival Forests, and ensemble integration outperform the pre-trained PREDICT v3 tool, especially when data is missing or dataset shifts occur.


<details>
  <summary>Details</summary>
Motivation: To improve breast cancer survival prognostication using more accessible clinicopathological data rather than costly genomic tools, leveraging machine learning, transfer learning, and ensemble methods to overcome limitations of existing prognostic tools like PREDICT v3.

Method: Used MA.27 trial data for training with external validation on TEAM trial and SEER cohort. Compared three approaches: transfer learning by fine-tuning PREDICT v3, de-novo ML (Random Survival Forests and Extreme Gradient Boosting), and ensemble integration through weighted sum of model predictions.

Result: Transfer learning, de-novo RSF, and ensemble integration improved calibration over PREDICT v3 (ICI reduced from 0.042 to ≤0.007) while maintaining comparable discrimination (AUC increased from 0.738 to 0.744-0.799). ML models could predict survival even with missing data, unlike PREDICT v3 which failed in 23.8-25.8% of cases. Key features were age, nodal status, pathological grading, and tumor size.

Conclusion: Transfer learning, de-novo RSF, and ensemble integration can enhance prognostication when PREDICT v3 data is incomplete or dataset shifts occur, providing more robust and accessible breast cancer survival prediction tools.

Abstract: Prognostic information is essential for decision-making in breast cancer
management. Recently trials have predominantly focused on genomic
prognostication tools, even though clinicopathological prognostication is less
costly and more widely accessible. Machine learning (ML), transfer learning and
ensemble integration offer opportunities to build robust prognostication
frameworks. We evaluate this potential to improve survival prognostication in
breast cancer by comparing de-novo ML, transfer learning from a pre-trained
prognostic tool and ensemble integration. Data from the MA.27 trial was used
for model training, with external validation on the TEAM trial and a SEER
cohort. Transfer learning was applied by fine-tuning the pre-trained prognostic
tool PREDICT v3, de-novo ML included Random Survival Forests and Extreme
Gradient Boosting, and ensemble integration was realized through a weighted sum
of model predictions. Transfer learning, de-novo RSF, and ensemble integration
improved calibration in MA.27 over the pre-trained model (ICI reduced from
0.042 in PREDICT v3 to <=0.007) while discrimination remained comparable (AUC
increased from 0.738 in PREDICT v3 to 0.744-0.799). Invalid PREDICT v3
predictions were observed in 23.8-25.8% of MA.27 individuals due to missing
information. In contrast, ML models and ensemble integration could predict
survival regardless of missing information. Across all models, patient age,
nodal status, pathological grading and tumor size had the highest SHAP values,
indicating their importance for survival prognostication. External validation
in SEER, but not in TEAM, confirmed the benefits of transfer learning, RSF and
ensemble integration. This study demonstrates that transfer learning, de-novo
RSF, and ensemble integration can improve prognostication in situations where
relevant information for PREDICT v3 is lacking or where a dataset shift is
likely.

</details>


### [462] [Continuous-Time Reinforcement Learning for Asset-Liability Management](https://arxiv.org/abs/2509.23280)
*Yilie Huang*

Main category: cs.LG

TL;DR: This paper proposes a continuous-time reinforcement learning approach for Asset-Liability Management using linear-quadratic formulation with policy gradient-based soft actor-critic algorithm and adaptive exploration techniques.


<details>
  <summary>Details</summary>
Motivation: To develop a more effective Asset-Liability Management (ALM) strategy that can dynamically synchronize assets and liabilities while balancing interim and terminal objectives, overcoming limitations of traditional financial strategies.

Method: Model-free policy gradient-based soft actor-critic algorithm with linear-quadratic formulation, featuring adaptive exploration for actor and scheduled exploration for critic to balance exploration-exploitation trade-off.

Result: The method achieves higher average rewards than all alternative strategies across 200 randomized market scenarios, showing rapid initial gains and sustained superior performance.

Conclusion: The outperformance comes from directly learning optimal ALM strategy without learning the environment, rather than from complex neural networks or improved parameter estimation.

Abstract: This paper proposes a novel approach for Asset-Liability Management (ALM) by
employing continuous-time Reinforcement Learning (RL) with a linear-quadratic
(LQ) formulation that incorporates both interim and terminal objectives. We
develop a model-free, policy gradient-based soft actor-critic algorithm
tailored to ALM for dynamically synchronizing assets and liabilities. To ensure
an effective balance between exploration and exploitation with minimal tuning,
we introduce adaptive exploration for the actor and scheduled exploration for
the critic. Our empirical study evaluates this approach against two enhanced
traditional financial strategies, a model-based continuous-time RL method, and
three state-of-the-art RL algorithms. Evaluated across 200 randomized market
scenarios, our method achieves higher average rewards than all alternative
strategies, with rapid initial gains and sustained superior performance. The
outperformance stems not from complex neural networks or improved parameter
estimation, but from directly learning the optimal ALM strategy without
learning the environment.

</details>


### [463] [A Neural ODE Approach to Aircraft Flight Dynamics Modelling](https://arxiv.org/abs/2509.23307)
*Gabriel Jarry,Ramon Dalmau,Xavier Olive,Philippe Very*

Main category: cs.LG

TL;DR: NODE-FDM is a Neural ODE-based Flight Dynamics Model that combines analytical kinematics with data-driven components, achieving better trajectory prediction accuracy than BADA4 models, especially in descent phases.


<details>
  <summary>Details</summary>
Motivation: Accurate aircraft trajectory prediction is critical for air traffic management, airline operations, and environmental assessment, but existing models have limitations in accuracy.

Method: Uses Neural Ordinary Differential Equations trained on Quick Access Recorder (QAR) data, combining analytical kinematic relations with data-driven components.

Result: Achieves more accurate reproduction of recorded trajectories than BADA4 models, with marked improvements in altitude, speed, and mass dynamics, particularly during descent phase.

Conclusion: Demonstrates potential of physics-informed neural ODEs as high-fidelity, data-driven approach to aircraft performance modeling, though limited by physical constraints and QAR data availability. Future work will incorporate full lateral dynamics modeling.

Abstract: Accurate aircraft trajectory prediction is critical for air traffic
management, airline operations, and environmental assessment. This paper
introduces NODE-FDM, a Neural Ordinary Differential Equations-based Flight
Dynamics Model trained on Quick Access Recorder (QAR) data. By combining
analytical kinematic relations with data-driven components, NODE-FDM achieves a
more accurate reproduction of recorded trajectories than state-of-the-art
models such as a BADA-based trajectory generation methodology (BADA4
performance model combined with trajectory control routines), particularly in
the descent phase of the flight. The analysis demonstrates marked improvements
across altitude, speed, and mass dynamics. Despite current limitations,
including limited physical constraints and the limited availability of QAR
data, the results demonstrate the potential of physics-informed neural ordinary
differential equations as a high-fidelity, data-driven approach to aircraft
performance modelling. Future work will extend the framework to incorporate a
full modelling of the lateral dynamics of the aircraft.

</details>


### [464] [ASTGI: Adaptive Spatio-Temporal Graph Interactions for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.23313)
*Xvyuan Liu,Xiangfei Qiu,Hanyin Cheng,Xingjian Wu,Chenjuan Guo,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: ASTGI framework for irregular multivariate time series forecasting using adaptive spatio-temporal graph interactions to handle asynchronous sampling and irregular intervals.


<details>
  <summary>Details</summary>
Motivation: Irregular multivariate time series (IMTS) in healthcare and finance have asynchronous sampling and irregular intervals, posing challenges for accurate representation and capturing complex dynamic dependencies.

Method: ASTGI framework with four modules: Spatio-Temporal Point Representation, Neighborhood-Adaptive Graph Construction, Spatio-Temporal Dynamic Propagation, and Query Point-based Prediction.

Result: Extensive experiments show ASTGI outperforms various state-of-the-art methods on multiple benchmark datasets.

Conclusion: ASTGI effectively addresses challenges in IMTS forecasting through adaptive spatio-temporal graph interactions.

Abstract: Irregular multivariate time series (IMTS) are prevalent in critical domains
like healthcare and finance, where accurate forecasting is vital for proactive
decision-making. However, the asynchronous sampling and irregular intervals
inherent to IMTS pose two core challenges for existing methods: (1) how to
accurately represent the raw information of irregular time series without
introducing data distortion, and (2) how to effectively capture the complex
dynamic dependencies between observation points. To address these challenges,
we propose the Adaptive Spatio-Temporal Graph Interaction (ASTGI) framework.
Specifically, the framework first employs a Spatio-Temporal Point
Representation module to encode each discrete observation as a point within a
learnable spatio-temporal embedding space. Second, a Neighborhood-Adaptive
Graph Construction module adaptively builds a causal graph for each point in
the embedding space via nearest neighbor search. Subsequently, a
Spatio-Temporal Dynamic Propagation module iteratively updates information on
these adaptive causal graphs by generating messages and computing interaction
weights based on the relative spatio-temporal positions between points.
Finally, a Query Point-based Prediction module generates the final forecast by
aggregating neighborhood information for a new query point and performing
regression. Extensive experiments on multiple benchmark datasets demonstrate
that ASTGI outperforms various state-of-the-art methods.

</details>


### [465] [Two-Scale Latent Dynamics for Recurrent-Depth Transformers](https://arxiv.org/abs/2509.23314)
*Francesco Pappone,Donato Crisostomi,Emanuele Rodolà*

Main category: cs.LG

TL;DR: Recurrent-depth transformers use iterative latent computations before token emission, exhibiting two-scale dynamics: small refinements within blocks and larger drift across blocks. An early-exit mechanism based on second-order step-size differences outperforms KL-divergence and first-order methods.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric properties of recurrent-depth transformers' iterative computations and leverage these dynamics for more efficient early-exit strategies.

Method: Analyzed the geometry of iterates in recurrent-depth transformers, measuring loop step sizes and orthogonality across checkpoints. Proposed an early-exit mechanism using second-order differences in step-size.

Result: Found that loop steps become smaller and more orthogonal over time, indicating better local modeling. The second-order early-exit method showed superior performance, stability and time-efficiency compared to KL-divergence and first-order approaches.

Conclusion: Recurrent-depth transformers exhibit meaningful two-scale dynamics that can be effectively leveraged for early-exit mechanisms, with second-order step-size differences providing the most robust performance.

Abstract: Recurrent-depth transformers scale test-time compute by iterating latent
computations before emitting tokens. We study the geometry of these iterates
and argue for a simple, \emph{two-scale} operational picture: (i) within a
looped block, updates act as \emph{small-scale refinements}; (ii) across
consecutive blocks, states undergo a \emph{larger-scale drift}. Across
checkpoints, our measurements show that loop steps become \emph{smaller} and
increasingly \emph{orthogonal} to one another, indicating better local modeling
of fine structure rather than merely pushing in a single direction. These
dynamics motivate an early-exit mechanism based on the model's second-order
difference in step-size, which we show is superior in terms of performance,
stability and time-efficiency, when compared to the KL-divergence exit strategy
of Geiping et al. and its naive first-order counterpart.

</details>


### [466] [MELCOT: A Hybrid Learning Architecture with Marginal Preservation for Matrix-Valued Regression](https://arxiv.org/abs/2509.23315)
*Khang Tran,Hieu Cao,Thinh Pham,Nghiem Diep,Tri Cao,Binh Nguyen*

Main category: cs.LG

TL;DR: MELCOT is a hybrid model for matrix-valued regression that combines classical machine learning (Marginal Estimation) with deep learning (Learnable-Cost Optimal Transport) to handle high-dimensional data while preserving spatial structure efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing regression methods struggle with high-dimensional data, often losing spatial structure or requiring excessive storage, especially for matrix-valued data where samples are naturally represented as matrices.

Method: Propose MELCOT with two blocks: ME block for marginal estimation to preserve spatial information, and LCOT block using learnable-cost optimal transport to capture complex global features, combining classical and deep learning strengths.

Result: Extensive experiments across diverse datasets and domains show MELCOT consistently outperforms all baseline methods while maintaining high efficiency.

Conclusion: MELCOT successfully addresses matrix-valued regression challenges by integrating classical and deep learning approaches, achieving superior performance with preserved spatial structure and computational efficiency.

Abstract: Regression is essential across many domains but remains challenging in
high-dimensional settings, where existing methods often lose spatial structure
or demand heavy storage. In this work, we address the problem of matrix-valued
regression, where each sample is naturally represented as a matrix. We propose
MELCOT, a hybrid model that integrates a classical machine learning-based
Marginal Estimation (ME) block with a deep learning-based Learnable-Cost
Optimal Transport (LCOT) block. The ME block estimates data marginals to
preserve spatial information, while the LCOT block learns complex global
features. This design enables MELCOT to inherit the strengths of both classical
and deep learning methods. Extensive experiments across diverse datasets and
domains demonstrate that MELCOT consistently outperforms all baselines while
remaining highly efficient.

</details>


### [467] [LLM Interpretability with Identifiable Temporal-Instantaneous Representation](https://arxiv.org/abs/2509.23323)
*Xiangchen Song,Jiaqi Sun,Zijian Li,Yujia Zheng,Kun Zhang*

Main category: cs.LG

TL;DR: The paper introduces an identifiable temporal causal representation learning framework to address limitations of existing mechanistic interpretability tools for LLMs, providing theoretical guarantees and capturing both time-delayed and instantaneous causal relations.


<details>
  <summary>Details</summary>
Motivation: Current mechanistic interpretability tools like sparse autoencoders lack temporal dependency modeling, instantaneous relation representation, and theoretical guarantees, undermining confidence in LLM interpretability analyses.

Method: Proposed an identifiable temporal causal representation learning framework specifically designed for LLMs' high-dimensional concept space, extending SAE techniques to capture both time-delayed and instantaneous causal relations.

Result: The approach demonstrates efficacy on synthetic datasets scaled to real-world complexity and successfully discovers meaningful concept relationships in LLM activations.

Conclusion: Modeling both temporal and instantaneous conceptual relationships advances the interpretability of LLMs, bridging the gap between theoretical foundations and practical applications.

Abstract: Despite Large Language Models' remarkable capabilities, understanding their
internal representations remains challenging. Mechanistic interpretability
tools such as sparse autoencoders (SAEs) were developed to extract
interpretable features from LLMs but lack temporal dependency modeling,
instantaneous relation representation, and more importantly theoretical
guarantees, undermining both the theoretical foundations and the practical
confidence necessary for subsequent analyses. While causal representation
learning (CRL) offers theoretically grounded approaches for uncovering latent
concepts, existing methods cannot scale to LLMs' rich conceptual space due to
inefficient computation. To bridge the gap, we introduce an identifiable
temporal causal representation learning framework specifically designed for
LLMs' high-dimensional concept space, capturing both time-delayed and
instantaneous causal relations. Our approach provides theoretical guarantees
and demonstrates efficacy on synthetic datasets scaled to match real-world
complexity. By extending SAE techniques with our temporal causal framework, we
successfully discover meaningful concept relationships in LLM activations. Our
findings show that modeling both temporal and instantaneous conceptual
relationships advances the interpretability of LLMs.

</details>


### [468] [Robust Fine-Tuning from Non-Robust Pretrained Models: Mitigating Suboptimal Transfer With Adversarial Scheduling](https://arxiv.org/abs/2509.23325)
*Jonas Ngnawé,Maxime Heuillet,Sabyasachi Sahoo,Yann Pequignot,Ola Ahmad,Audrey Durand,Frédéric Precioso,Christian Gagné*

Main category: cs.LG

TL;DR: Fine-tuning non-robust pretrained models with robust objectives can cause suboptimal transfer and poor performance. The paper proposes Epsilon-Scheduling to prevent this issue and introduces expected robustness as a comprehensive evaluation metric.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of robust fine-tuning (RFT) from non-robust pretrained models, which remains understudied despite the abundance of such models in open-source repositories.

Method: Systematically examine RFT from non-robust models, identify suboptimal transfer phenomenon, and propose Epsilon-Scheduling - a schedule over perturbation strength during training to promote optimal transfer.

Result: Fine-tuning with robust objectives impedes task adaptation and prevents optimal transfer, especially in challenging scenarios. Epsilon-Scheduling successfully prevents suboptimal transfer and consistently improves expected robustness across six pretrained models and five datasets.

Conclusion: Epsilon-Scheduling is an effective heuristic for robust fine-tuning that prevents suboptimal transfer from non-robust pretrained models, and expected robustness provides a comprehensive evaluation metric for accuracy-robustness trade-off.

Abstract: Fine-tuning pretrained models is a standard and effective workflow in modern
machine learning. However, robust fine-tuning (RFT), which aims to
simultaneously achieve adaptation to a downstream task and robustness to
adversarial examples, remains challenging. Despite the abundance of non-robust
pretrained models in open-source repositories, their potential for RFT is less
understood. We address this knowledge gap by systematically examining RFT from
such non-robust models. Our experiments reveal that fine-tuning non-robust
models with a robust objective, even under small perturbations, can lead to
poor performance, a phenomenon that we dub \emph{suboptimal transfer}. In
challenging scenarios (eg, difficult tasks, high perturbation), the resulting
performance can be so low that it may be considered a transfer failure. We find
that fine-tuning using a robust objective impedes task adaptation at the
beginning of training and eventually prevents optimal transfer. However, we
propose a novel heuristic, \emph{Epsilon-Scheduling}, a schedule over
perturbation strength used during training that promotes optimal transfer.
Additionally, we introduce \emph{expected robustness}, a metric that captures
performance across a range of perturbations, providing a more comprehensive
evaluation of the accuracy-robustness trade-off for diverse models at test
time. Extensive experiments on a wide range of configurations (six pretrained
models and five datasets) show that \emph{Epsilon-Scheduling} successfully
prevents \emph{suboptimal transfer} and consistently improves expected
robustness.

</details>


### [469] [Entering the Era of Discrete Diffusion Models: A Benchmark for Schrödinger Bridges and Entropic Optimal Transport](https://arxiv.org/abs/2509.23348)
*Xavier Aramayo Carrasco,Grigoriy Ksenofontov,Aleksei Leonov,Iaroslav Sergeevich Koshelev,Alexander Korotin*

Main category: cs.LG

TL;DR: This paper introduces the first benchmark for evaluating Schrödinger bridge (SB) methods on discrete spaces, providing analytically known SB solutions for rigorous testing. It also develops new SB algorithms (DLightSB, DLightSB-M, α-CSBM) and demonstrates their utility in high-dimensional discrete settings.


<details>
  <summary>Details</summary>
Motivation: There is growing interest in applying Schrödinger bridge methods to discrete domains (e.g., discrete diffusion models), but no reliable way exists to evaluate how well these methods actually solve the underlying SB problem.

Method: The authors construct a benchmark that yields pairs of probability distributions with analytically known SB solutions. They also develop two new SB algorithms (DLightSB and DLightSB-M) and extend prior work to create the α-CSBM algorithm.

Result: The benchmark enables rigorous evaluation of SB solvers in high-dimensional discrete settings. The new algorithms are demonstrated to be effective through evaluation on the proposed benchmark.

Conclusion: This work provides the first step toward proper evaluation of SB methods on discrete spaces, paving the way for more reproducible future studies in this important area connecting generative modeling with optimal transport theory.

Abstract: The Entropic Optimal Transport (EOT) problem and its dynamic counterpart, the
Schr\"odinger bridge (SB) problem, play an important role in modern machine
learning, linking generative modeling with optimal transport theory. While
recent advances in discrete diffusion and flow models have sparked growing
interest in applying SB methods to discrete domains, there is still no reliable
way to evaluate how well these methods actually solve the underlying problem.
We address this challenge by introducing a benchmark for SB on discrete spaces.
Our construction yields pairs of probability distributions with analytically
known SB solutions, enabling rigorous evaluation. As a byproduct of building
this benchmark, we obtain two new SB algorithms, DLightSB and DLightSB-M, and
additionally extend prior related work to construct the $\alpha$-CSBM
algorithm. We demonstrate the utility of our benchmark by evaluating both
existing and new solvers in high-dimensional discrete settings. This work
provides the first step toward proper evaluation of SB methods on discrete
spaces, paving the way for more reproducible future studies.

</details>


### [470] [Landing with the Score: Riemannian Optimization through Denoising](https://arxiv.org/abs/2509.23357)
*Andrey Kharitenko,Zebang Shen,Riccardo de Santi,Niao He,Florian Doerfler*

Main category: cs.LG

TL;DR: The paper introduces methods for Riemannian optimization over data manifolds given only through the data distribution, connecting diffusion model score functions to manifold operations and proposing efficient inference algorithms with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: High-dimensional data often lie near low-dimensional manifolds, but classical optimization methods require explicit manifold operations which are unavailable when manifolds are given only implicitly through data distributions. This problem is central to modern generative AI and data-driven design.

Method: Introduce a link function connecting data distribution to geometric operations, enabling recovery of manifold operations like retraction and Riemannian gradient computation. Leverage diffusion model score functions and propose two algorithms: Denoising Landing Flow (DLF) and Denoising Riemannian Gradient Descent (DRGD).

Result: Theoretical guarantees for both feasibility (approximate manifold adherence) and optimality (small Riemannian gradient norm). Demonstrated effectiveness on finite-horizon reference tracking tasks in data-driven control.

Conclusion: The approach successfully connects diffusion models with Riemannian optimization, enabling practical generative and design applications by leveraging existing score networks and training procedures from diffusion model literature.

Abstract: Under the data manifold hypothesis, high-dimensional data are concentrated
near a low-dimensional manifold. We study the problem of Riemannian
optimization over such manifolds when they are given only implicitly through
the data distribution, and the standard manifold operations required by
classical algorithms are unavailable. This formulation captures a broad class
of data-driven design problems that are central to modern generative AI. Our
key idea is to introduce a link function that connects the data distribution to
the geometric operations needed for optimization. We show that this function
enables the recovery of essential manifold operations, such as retraction and
Riemannian gradient computation. Moreover, we establish a direct connection
between our construction and the score function in diffusion models of the data
distribution. This connection allows us to leverage well-studied
parameterizations, efficient training procedures, and even pretrained score
networks from the diffusion model literature to perform optimization. Building
on this foundation, we propose two efficient inference-time algorithms --
Denoising Landing Flow (DLF) and Denoising Riemannian Gradient Descent (DRGD)
-- and provide theoretical guarantees for both feasibility (approximate
manifold adherence) and optimality (small Riemannian gradient norm). Finally,
we demonstrate the effectiveness of our approach on finite-horizon reference
tracking tasks in data-driven control, highlighting its potential for practical
generative and design applications.

</details>


### [471] [Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought](https://arxiv.org/abs/2509.23365)
*Hanlin Zhu,Shibo Hao,Zhiting Hu,Jiantao Jiao,Stuart Russell,Yuandong Tian*

Main category: cs.LG

TL;DR: This paper analyzes how the superposition mechanism in continuous chain-of-thought reasoning naturally emerges during training of two-layer transformers on graph reachability problems.


<details>
  <summary>Details</summary>
Motivation: To understand how the superposition mechanism in continuous CoT reasoning is learned from gradient-based training methods, as previous work showed theoretical capabilities but not the learning dynamics.

Method: Theoretical analysis of training dynamics of a simplified two-layer transformer on directed graph reachability problem, examining two training stages: thought-generation and prediction stages.

Result: Analysis reveals that the index-matching logit first increases then remains bounded, balancing exploration and exploitation - exploiting local structures while assigning comparable weights to multiple traces when uncertain, creating superposition.

Conclusion: The bounded index-matching logit during training enables the superposition mechanism by balancing exploration and exploitation, with experimental results validating the theoretical findings.

Abstract: Previous work shows that the chain of continuous thought (continuous CoT)
improves the reasoning capability of large language models (LLMs) by enabling
implicit parallel thinking, and a subsequent work provided theoretical insight
by showing that a two-layer transformer equipped with continuous CoT can
efficiently solve directed graph reachability by maintaining a superposition of
multiple reasoning traces in the continuous thought. However, it remains
unclear how the superposition mechanism is naturally learned from
gradient-based training methods. To fill this gap, we theoretically analyze the
training dynamics of a simplified two-layer transformer on the directed graph
reachability problem to unveil how the superposition mechanism emerges during
training in two training stages -- (i) a thought-generation stage that
autoregressively expands the continuous thought, and (ii) a prediction stage
that converts the thought into the final answer. Our analysis reveals that
during training using continuous thought, the index-matching logit, an
important quantity which reflects the strength of the model's local search
ability, will first increase and then remain bounded under mild assumptions.
The bounded index-matching logit effectively balances exploration and
exploitation during the reasoning process: the model will exploit local problem
structures to identify plausible search traces, and assign comparable weights
to multiple such traces to explore when it is uncertain about which solution is
correct, which results in superposition. Our experimental results tracking the
growth of logits further validate our theory.

</details>


### [472] [Splines-Based Feature Importance in Kolmogorov-Arnold Networks: A Framework for Supervised Tabular Data Dimensionality Reduction](https://arxiv.org/abs/2509.23366)
*Ange-Clément Akazan,Verlon Roel Mbingui*

Main category: cs.LG

TL;DR: KAN-based feature selection methods provide competitive and interpretable alternatives to classical methods, with specific variants excelling in different scenarios (structured data, noisy regression, high-dimensional classification).


<details>
  <summary>Details</summary>
Motivation: High-dimensional datasets require effective feature selection to improve predictive performance, interpretability, and robustness, motivating the development of KAN-based methods that leverage spline parameterization for direct importance measurement.

Method: Proposed four KAN-based feature selectors (KAN-L1, KAN-L2, KAN-SI, KAN-KO) using Kolmogorov-Arnold networks with spline parameterization, compared against classical baselines (LASSO, Random Forest, Mutual Information, SVM-RFE) across multiple classification and regression tabular datasets.

Result: KAN-based selectors are competitive with and sometimes superior to classical baselines, with KAN-L2 and KAN-SI performing well on noisy regression and heterogeneous datasets, while KAN-L1, KAN-KO, and KAN-SI excel in high-dimensional multi-class classification by eliminating redundancy.

Conclusion: KAN-based feature selection provides a powerful and interpretable alternative capable of uncovering nonlinear and multivariate feature relevance beyond traditional sparsity or impurity-based measures.

Abstract: High-dimensional datasets require effective feature selection to improve
predictive performance, interpretability, and robustness. We propose and
evaluate feature selection methods for tabular datasets based on
Kolmogorov-Arnold networks (KANs), which parameterize feature transformations
through splines, enabling direct access to interpretable importance measures.
We introduce four KAN-based selectors ($\textit{KAN-L1}$, $\textit{KAN-L2}$,
$\textit{KAN-SI}$, $\textit{KAN-KO}$) and compare them against classical
baselines (LASSO, Random Forest, Mutual Information, SVM-RFE) across multiple
classification and regression tabular dataset benchmarks. Average (over three
retention levels: 20\%, 40\%, and 60\%) F1 scores and $R^2$ score results
reveal that KAN-based selectors, particularly $\textit{KAN-L2}$,
$\textit{KAN-L1}$, $\textit{KAN-SI}$, and $\textit{KAN-KO}$, are competitive
with and sometimes superior to classical baselines in structured and synthetic
datasets. However, $\textit{KAN-L1}$ is often too aggressive in regression,
removing useful features, while $\textit{KAN-L2}$ underperforms in
classification, where simple coefficient shrinkage misses complex feature
interactions. $\textit{KAN-L2}$ and $\textit{KAN-SI}$ provide robust
performance on noisy regression datasets and heterogeneous datasets, aligning
closely with ensemble predictors. In classification tasks, KAN selectors such
as $\textit{KAN-L1}$, $\textit{KAN-KO}$, and $\textit{KAN-SI}$ sometimes
surpass the other selectors by eliminating redundancy, particularly in
high-dimensional multi-class data. Overall, our findings demonstrate that
KAN-based feature selection provides a powerful and interpretable alternative
to traditional methods, capable of uncovering nonlinear and multivariate
feature relevance beyond sparsity or impurity-based measures.

</details>


### [473] [Graph Your Own Prompt](https://arxiv.org/abs/2509.23373)
*Xi Ding,Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.LG

TL;DR: GCR is a framework that uses model predictions to create relational graphs for regularizing feature representations, improving semantic structure and generalization without architecture changes.


<details>
  <summary>Details</summary>
Motivation: Deep networks learn noisy inter-class similarities that contradict predicted semantics, so GCR aims to enforce class-consistent feature relationships throughout the network.

Method: Introduces parameter-free Graph Consistency Layers (GCLs) that align batch-level feature similarity graphs with global class-aware prediction graphs using adaptive weighting based on graph discrepancies.

Result: GCR promotes cleaner feature structure, stronger intra-class cohesion, and improved generalization across various networks and datasets.

Conclusion: GCR offers a model-agnostic, lightweight approach to enhance semantic structure by learning from prediction structure through multi-layer graph alignment.

Abstract: We propose Graph Consistency Regularization (GCR), a novel framework that
injects relational graph structures, derived from model predictions, into the
learning process to promote class-aware, semantically meaningful feature
representations. Functioning as a form of self-prompting, GCR enables the model
to refine its internal structure using its own outputs. While deep networks
learn rich representations, these often capture noisy inter-class similarities
that contradict the model's predicted semantics. GCR addresses this issue by
introducing parameter-free Graph Consistency Layers (GCLs) at arbitrary depths.
Each GCL builds a batch-level feature similarity graph and aligns it with a
global, class-aware masked prediction graph, derived by modulating softmax
prediction similarities with intra-class indicators. This alignment enforces
that feature-level relationships reflect class-consistent prediction behavior,
acting as a semantic regularizer throughout the network. Unlike prior work, GCR
introduces a multi-layer, cross-space graph alignment mechanism with adaptive
weighting, where layer importance is learned from graph discrepancy magnitudes.
This allows the model to prioritize semantically reliable layers and suppress
noisy ones, enhancing feature quality without modifying the architecture or
training procedure. GCR is model-agnostic, lightweight, and improves semantic
structure across various networks and datasets. Experiments show that GCR
promotes cleaner feature structure, stronger intra-class cohesion, and improved
generalization, offering a new perspective on learning from prediction
structure. [Project website](https://darcyddx.github.io/gcr/)
[Code](https://github.com/Darcyddx/graph-prompt)

</details>


### [474] [Planner Aware Path Learning in Diffusion Language Models Training](https://arxiv.org/abs/2509.23405)
*Fred Zhangzhi Peng,Zachary Bezemek,Jarrid Rector-Brooks,Shuibai Zhang,Anru R. Zhang,Michael Bronstein,Avishek Joey Bose,Alexander Tong*

Main category: cs.LG

TL;DR: PAPL addresses the training-inference mismatch in diffusion language models by incorporating planner-based reverse dynamics into training, improving performance across protein, text, and code generation tasks.


<details>
  <summary>Details</summary>
Motivation: Planners in diffusion models create a mismatch between training (uniform denoising) and inference (planned denoising), which the standard ELBO doesn't account for.

Method: Derived Planned ELBO (P-ELBO) and proposed Planner Aware Path Learning (PAPL) to align training with planned inference paths.

Result: 40% relative gain in protein modeling, 4x MAUVE improvement in text generation, 23% relative gain in HumanEval pass@10 for code generation.

Conclusion: PAPL effectively bridges the training-inference gap in planned diffusion models, delivering consistent performance improvements across multiple domains.

Abstract: Diffusion language models have emerged as a powerful alternative to
autoregressive models, enabling fast inference through flexible and parallel
generation paths. This flexibility is enabled by new sampling strategies, or
planners, that iteratively choose where to denoise along the sequence rather
than sampling uniformly at random. However, by modifying reverse paths,
planners introduce a mismatch between the uniformly random denoising paths used
during training and the planning-based paths used at inference. In this work,
we systematically investigate this mismatch and theoretically show that the
standard discrete diffusion training evidence lower bound (ELBO) does not
accurately describe a denoiser under non-uniform planning. To bridge this gap,
we derive a new Planned Evidence Lower Bound (P-ELBO) that directly
incorporates planner-based reverse dynamics into the training objective.
Building on this, we propose Planner Aware Path Learning (PAPL), a simple and
effective modification of the standard masked discrete diffusion loss that
aligns training and inference under planned denoisers. Empirically, PAPL
delivers consistent improvements across domains, including a 40% relative gain
in protein sequence modeling, up to a 4x improvement in MAUVE for text
generation, and a 23% relative gain in HumanEval pass@10 for code generation.

</details>


### [475] [Mind the Links: Cross-Layer Attention for Link Prediction in Multiplex Networks](https://arxiv.org/abs/2509.23409)
*Devesh Sharma,Aditya Kishore,Ayush Garg,Debajyoti Mazumder,Debasis Mohapatra,Jasabanta Patro*

Main category: cs.LG

TL;DR: The paper proposes a novel approach for multiplex link prediction by framing it as multi-view edge classification, using cross-layer self-attention to fuse evidence across layers, with two scalable models (Trans-SLE and Trans-GAT) that outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing multiplex graph predictors either collapse layers or treat them independently, losing crucial inter-layer dependencies and struggling with scalability.

Method: Frame multiplex link prediction as multi-view edge classification, construct sequences of per-layer edge views, apply cross-layer self-attention for fusion, and introduce two models (Trans-SLE with static embeddings, Trans-GAT with GAT encoders) with Union-Set candidate pool and leakage-free protocols.

Result: Experiments on six public multiplex datasets show consistent macro-F1 gains over strong baselines (MELL, HOPLP-MUL, RMNE).

Conclusion: The approach is simple, scalable, and compatible with both precomputed embeddings and GNN encoders, effectively capturing inter-layer dependencies in multiplex graphs.

Abstract: Multiplex graphs capture diverse relations among shared nodes. Most
predictors either collapse layers or treat them independently. This loses
crucial inter-layer dependencies and struggles with scalability. To overcome
this, we frame multiplex link prediction as multi-view edge classification. For
each node pair, we construct a sequence of per-layer edge views and apply
cross-layer self-attention to fuse evidence for the target layer. We present
two models as instances of this framework: Trans-SLE, a lightweight transformer
over static embeddings, and Trans-GAT, which combines layer-specific GAT
encoders with transformer fusion. To ensure scalability and fairness, we
introduce a Union--Set candidate pool and two leakage-free protocols:
cross-layer and inductive subgraph generalization. Experiments on six public
multiplex datasets show consistent macro-F_1 gains over strong baselines (MELL,
HOPLP-MUL, RMNE). Our approach is simple, scalable, and compatible with both
precomputed embeddings and GNN encoders.

</details>


### [476] [PATCH: Learnable Tile-level Hybrid Sparsity for LLMs](https://arxiv.org/abs/2509.23410)
*Younes Hourri,Mohammad Mozaffari,Maryam Mehri Dehnavi*

Main category: cs.LG

TL;DR: PATCH is a hybrid sparsity framework that enables continuous sparsity ratios between 0% and 50% by partitioning weight matrices into tiles and assigning each tile to be either dense or 2:4 sparse via learnable masks, achieving better accuracy-acceleration tradeoffs than existing pruning methods.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods face challenges: unstructured sparsity preserves accuracy but prevents GPU acceleration due to irregular access patterns, while semi-structured 2:4 sparsity is hardware-friendly but enforces a rigid 50% pattern that degrades model quality. There's a need to bridge this gap.

Method: PATCH partitions weight matrices into tiles and uses a learnable mask selection mechanism to assign each tile as either dense or 2:4 sparse. This provides fine-grained control over sparsity ratios and supports non-uniform sparsity across layers.

Result: Across models from 0.5B to 8B parameters, PATCH consistently narrows the gap to dense accuracy while delivering practical speedups. On LLaMA-2 7B with A6000 GPU, it achieves 1.18x-1.38x end-to-end speedup over dense baselines while improving accuracy by 0.37%-2.96% compared to state-of-the-art 2:4 pruning method MaskLLM.

Conclusion: PATCH successfully bridges the gap between unstructured and structured sparsity by enabling continuous sparsity control, delivering both improved accuracy and practical acceleration across various model sizes.

Abstract: Large language models (LLMs) deliver impressive performance but incur
prohibitive memory and compute costs at deployment. Model pruning is an
effective way to reduce these overheads, yet existing approaches face
challenges: unstructured sparsity, where nonzeros can appear anywhere,
preserves accuracy but yields irregular access patterns that prevent GPU
acceleration, while semi-structured 2:4 sparsity is hardware-friendly but
enforces a rigid 50% pattern that degrades model quality. To bridge this gap,
we introduce PATCH, a hybrid sparsity framework that enables a continuous
sparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles,
assigning each tile to be either dense or 2:4 sparse via a learnable mask
selection mechanism. This design provides fine-grained control over
accuracy-acceleration tradeoffs and supports non-uniform sparsity across
layers, leading to superior overall quality. Across models from 0.5B to 8B
parameters, PATCH consistently narrows the gap to dense accuracy while
delivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU,
PATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while
improving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning
method, MaskLLM.

</details>


### [477] [URS: A Unified Neural Routing Solver for Cross-Problem Zero-Shot Generalization](https://arxiv.org/abs/2509.23413)
*Changliang Zhou,Canhong Yu,Shunyu Yao,Xi Lin,Zhenkun Wang,Yu Zhou,Qingfu Zhang*

Main category: cs.LG

TL;DR: URS is a unified neural routing solver that achieves zero-shot generalization across 100+ VRP variants using unified data representation, mixed bias module, parameter generator, and LLM-driven constraint satisfaction.


<details>
  <summary>Details</summary>
Motivation: Existing neural solvers rely on predefined constraints or require fine-tuning, limiting their zero-shot generalization to unseen VRP variants.

Method: Proposes unified data representation (UDR) for data unification, mixed bias module (MBM) for learning geometric/relational biases, parameter generator for adaptive decoder adjustment, and LLM-driven constraint satisfaction mechanism.

Result: URS produces high-quality solutions for 100+ distinct VRP variants without fine-tuning, including 90+ unseen variants - first neural solver to handle over 100 VRP variants with single model.

Conclusion: URS successfully addresses the zero-shot generalization bottleneck in neural routing solvers and demonstrates superior performance across diverse VRP variants.

Abstract: Multi-task neural routing solvers have emerged as a promising paradigm for
their ability to solve multiple vehicle routing problems (VRPs) using a single
model. However, existing neural solvers typically rely on predefined problem
constraints or require per-problem fine-tuning, which substantially limits
their zero-shot generalization ability to unseen VRP variants. To address this
critical bottleneck, we propose URS, a unified neural routing solver capable of
zero-shot generalization across a wide range of unseen VRPs using a single
model without any fine-tuning. The key component of URS is the unified data
representation (UDR), which replaces problem enumeration with data unification,
thereby broadening the problem coverage and reducing reliance on domain
expertise. In addition, we propose a Mixed Bias Module (MBM) to efficiently
learn the geometric and relational biases inherent in various problems. On top
of the proposed UDR, we further develop a parameter generator that adaptively
adjusts the decoder and bias weights of MBM to enhance zero-shot
generalization. Moreover, we propose an LLM-driven constraint satisfaction
mechanism, which translates raw problem descriptions into executable stepwise
masking functions to ensure solution feasibility. Extensive experiments
demonstrate that URS can consistently produce high-quality solutions for more
than 100 distinct VRP variants without any fine-tuning, which includes more
than 90 unseen variants. To the best of our knowledge, URS is the first neural
solver capable of handling over 100 VRP variants with a single model.

</details>


### [478] [LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal Transport](https://arxiv.org/abs/2509.23436)
*Ashkan Shahbazi,Chayne Thrash,Yikun Bai,Keaton Hamm,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: LOTFormer is a linear-time doubly-stochastic attention mechanism that uses entropic optimal transport with a learnable pivot measure to achieve efficient long-context modeling while balancing token participation.


<details>
  <summary>Details</summary>
Motivation: Standard softmax attention has quadratic complexity that limits scaling to long contexts, and most attention mechanisms (including linear ones) produce row-normalized maps that can over-focus on few tokens, degrading robustness. Doubly-stochastic attention helps but existing methods introduce substantial overhead.

Method: The approach exploits the connection between attention maps and transportation plans. It constrains the transport plan to be low-rank by conditioning on a learnable pivot measure with small support, solving two entropic optimal transport problems (queries→pivot and pivot→keys) and composing them into a conditional glued coupling.

Result: LOTFormer achieves state-of-the-art results on the Long Range Arena benchmark, surpassing prior linear and transport-based attention methods in both accuracy and efficiency.

Conclusion: LOTFormer provides a principled attention mechanism that is simultaneously linear-time and doubly-stochastic, enabling efficient long-context modeling with balanced token participation.

Abstract: Transformers have proven highly effective across a wide range of modalities.
However, the quadratic complexity of the standard softmax attention mechanism
poses a fundamental barrier to scaling them to long context windows. A large
body of work addresses this with linear attention, which reformulates attention
as a kernel function and approximates it with finite feature maps to achieve
linear-time computation. Orthogonal to computational scaling, most attention
mechanisms -- both quadratic and linear -- produce row-normalized maps that can
over-focus on a few tokens, degrading robustness and information flow.
Enforcing doubly-stochastic attention alleviates this by balancing token
participation across rows and columns, but existing doubly-stochastic attention
mechanisms typically introduce substantial overhead, undermining scalability.
We propose LOTFormer, a principled attention mechanism that is simultaneously
linear-time and doubly-stochastic. Our approach exploits the connection between
attention maps and transportation plans between query and key measures. The
central idea is to constrain the transport plan to be low-rank by conditioning
it on a learnable pivot measure with small support. Concretely, we solve two
entropic optimal transport problems (queries $\to$ pivot and pivot $\to$ keys)
and compose them into a conditional (glued) coupling. This yields an attention
matrix that is provably doubly-stochastic, has rank at most $r \ll n$, and
applies to values in $O(nr)$ time without forming the full $n \times n$ map.
The pivot locations and masses are learned end-to-end. Empirically, LOTFormer
achieves state-of-the-art results on the Long Range Arena benchmark, surpassing
prior linear and transport-based attention methods in both accuracy and
efficiency.

</details>


### [479] [Better Hessians Matter: Studying the Impact of Curvature Approximations in Influence Functions](https://arxiv.org/abs/2509.23437)
*Steve Hong,Runa Eschenhagen,Bruno Mlodozeniec,Richard Turner*

Main category: cs.LG

TL;DR: Better Hessian approximations consistently improve influence function data attribution performance in classification tasks, with K-FAC eigenvalue mismatch being the main source of error.


<details>
  <summary>Details</summary>
Motivation: To understand how Hessian approximation quality affects influence function data attribution performance, given that better approximations don't necessarily guarantee better attribution results in the restricted derivation regime of influence functions.

Method: Investigated Hessian approximation effects on influence attributions in controlled classification setting, decomposed approximation steps of recent methods (GGN, K-FAC, EK-FAC) and evaluated each step's impact on attribution accuracy.

Result: Better Hessian approximations consistently yield better influence score quality; K-FAC eigenvalue mismatch with GGN/EK-FAC accounts for majority of error and influence loss.

Conclusion: Findings justify research efforts towards better Hessian approximations and highlight which approximations are most critical for balancing computational tractability and attribution accuracy.

Abstract: Influence functions offer a principled way to trace model predictions back to
training data, but their use in deep learning is hampered by the need to invert
a large, ill-conditioned Hessian matrix. Approximations such as Generalised
Gauss-Newton (GGN) and Kronecker-Factored Approximate Curvature (K-FAC) have
been proposed to make influence computation tractable, yet it remains unclear
how the departure from exactness impacts data attribution performance.
Critically, given the restricted regime in which influence functions are
derived, it is not necessarily clear better Hessian approximations should even
lead to better data attribution performance. In this paper, we investigate the
effect of Hessian approximation quality on influence-function attributions in a
controlled classification setting. Our experiments show that better Hessian
approximations consistently yield better influence score quality, offering
justification for recent research efforts towards that end. We further
decompose the approximation steps for recent Hessian approximation methods and
evaluate each step's influence on attribution accuracy. Notably, the mismatch
between K-FAC eigenvalues and GGN/EK-FAC eigenvalues accounts for the majority
of the error and influence loss. These findings highlight which approximations
are most critical, guiding future efforts to balance computational tractability
and attribution accuracy.

</details>


### [480] [Factor Decorrelation Enhanced Data Removal from Deep Predictive Models](https://arxiv.org/abs/2509.23443)
*Wenhao Yang,Lin Li,Xiaohui Tao,Kaize Shi*

Main category: cs.LG

TL;DR: A novel data removal approach using factor decorrelation and loss perturbation to maintain model performance while protecting privacy, showing superior results in both in-distribution and out-of-distribution scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the performance degradation caused by distributional shifts when removing sensitive data for privacy protection and regulatory compliance.

Method: Uses discriminative-preserving factor decorrelation with dynamic adaptive weight adjustment and iterative representation updating, combined with smoothed data removal mechanism with loss perturbation.

Result: Outperforms other baselines on five benchmark datasets, achieving high predictive accuracy and robustness under significant distribution shifts.

Conclusion: The approach demonstrates superior efficiency and adaptability in maintaining model performance while ensuring data privacy protection.

Abstract: The imperative of user privacy protection and regulatory compliance
necessitates sensitive data removal in model training, yet this process often
induces distributional shifts that undermine model performance-particularly in
out-of-distribution (OOD) scenarios. We propose a novel data removal approach
that enhances deep predictive models through factor decorrelation and loss
perturbation. Our approach introduces: (1) a discriminative-preserving factor
decorrelation module employing dynamic adaptive weight adjustment and iterative
representation updating to reduce feature redundancy and minimize inter-feature
correlations. (2) a smoothed data removal mechanism with loss perturbation that
creates information-theoretic safeguards against data leakage during removal
operations. Extensive experiments on five benchmark datasets show that our
approach outperforms other baselines and consistently achieves high predictive
accuracy and robustness even under significant distribution shifts. The results
highlight its superior efficiency and adaptability in both in-distribution and
out-of-distribution scenarios.

</details>


### [481] [PHASE: Physics-Integrated, Heterogeneity-Aware Surrogates for Scientific Simulations](https://arxiv.org/abs/2509.23453)
*Dawei Gao,Dali Wang,Zhuowei Gu,Qinglei Cao,Xiao Wang,Peter Thornton,Dan Ricciuto,Yunhe Feng*

Main category: cs.LG

TL;DR: PHASE is a modular deep-learning framework that accelerates scientific simulations by integrating physics constraints and handling heterogeneous data, achieving 60x speedup in land-surface modeling.


<details>
  <summary>Details</summary>
Motivation: Large-scale scientific simulations are computationally expensive, and existing AI surrogates lack physical plausibility and trustworthiness needed for mission-critical applications.

Method: PHASE uses data-type-aware encoders for heterogeneous inputs and multi-level physics-based constraints to ensure consistency from local dynamics to global system behavior.

Result: PHASE reduced required integration time from 1,200+ years to just 20 years (60x acceleration) for biogeochemical spin-up in Earth system modeling, with strong generalization to higher resolutions.

Conclusion: PHASE captures governing physical regularities rather than surface correlations, enabling practical, physically consistent acceleration of complex scientific workflows.

Abstract: Large-scale numerical simulations underpin modern scientific discovery but
remain constrained by prohibitive computational costs. AI surrogates offer
acceleration, yet adoption in mission-critical settings is limited by concerns
over physical plausibility, trustworthiness, and the fusion of heterogeneous
data. We introduce PHASE, a modular deep-learning framework for
physics-integrated, heterogeneity-aware surrogates in scientific simulations.
PHASE combines data-type-aware encoders for heterogeneous inputs with
multi-level physics-based constraints that promote consistency from local
dynamics to global system behavior. We validate PHASE on the biogeochemical
(BGC) spin-up workflow of the U.S. Department of Energy's Energy Exascale Earth
System Model (E3SM) Land Model (ELM), presenting-to our knowledge-the first
scientifically validated AI-accelerated solution for this task. Using only the
first 20 simulation years, PHASE infers a near-equilibrium state that otherwise
requires more than 1,200 years of integration, yielding an effective reduction
in required integration length by at least 60x. The framework is enabled by a
pipeline for fusing heterogeneous scientific data and demonstrates strong
generalization to higher spatial resolutions with minimal fine-tuning. These
results indicate that PHASE captures governing physical regularities rather
than surface correlations, enabling practical, physically consistent
acceleration of land-surface modeling and other complex scientific workflows.

</details>


### [482] [Data-Efficient Training by Evolved Sampling](https://arxiv.org/abs/2509.23461)
*Ziheng Cheng,Zhong Li,Jiang Bian*

Main category: cs.LG

TL;DR: Evolved Sampling (ES) is a dynamic data selection framework that uses loss dynamics and loss differences for batch-level sampling, achieving up to 45% training acceleration without performance loss.


<details>
  <summary>Details</summary>
Motivation: To accelerate machine learning training while maintaining performance by identifying and selecting the most informative data samples throughout the training process.

Method: ES uses batch-level data selection based on loss dynamics and augmented loss differences, enabling flexible frequency tuning. It can be extended to set-level selection (ESWP) for further acceleration.

Result: ES(WP) achieves lossless training accelerations across various pre-training and post-training tasks, saving up to nearly 45% wall-clock time.

Conclusion: The method demonstrates effective data efficiency improvements for large-scale machine learning, motivating further research in this direction.

Abstract: Data selection is designed to accelerate learning with preserved performance.
To achieve this, a fundamental thought is to identify informative data samples
with significant contributions to the training. In this work, we propose
\textbf{Evolved Sampling} (\textbf{ES}), a simple yet effective framework for
\emph{dynamic} sampling along the training process. This method conducts \em
batch \em level data selection based on the dynamics of losses and augmented
\emph{loss differences}, which enables flexible \emph{frequency tuning}, and
hence significantly reduces the back propagation time with maintained model
performance. Due to its conciseness, ES is also readily extensible to
incorporate \em set \em level data selection (to form ES with pruning,
\textbf{ESWP}) for further accelerations. As a plug-and-play framework, ES(WP)
consistently achieves lossless training accelerations across various
pre-training and post-training tasks, saving up to nearly 45\% wall-clock time.
Our results motivate further investigations on the data efficiency aspect of
modern large-scale machine learning.

</details>


### [483] [Generative Evolutionary Meta-Solver (GEMS): Scalable Surrogate-Free Multi-Agent Learning](https://arxiv.org/abs/2509.23462)
*Alakh Sharma,Gaurish Trivedi,Kartikey Bhandari,Yash Sinha,Dhruv Kumar,Pratik Narang,Jagat Sesh Challa*

Main category: cs.LG

TL;DR: GEMS is a scalable MARL framework that replaces explicit policy populations with latent anchors and an amortized generator, using Monte Carlo rollouts and meta-dynamics to achieve 6x speedup and 1.3x memory reduction over PSRO while maintaining game-theoretic guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing population-based MARL methods like PSRO suffer from quadratic computation and linear memory costs due to storing explicit policy populations and constructing full payoff matrices, limiting scalability.

Method: Uses compact latent anchors and a single amortized generator instead of explicit populations. Employs unbiased Monte Carlo rollouts, multiplicative-weights meta-dynamics, and model-free empirical-Bernstein UCB oracle for adaptive policy expansion. Trains best responses within generator using advantage-based trust-region objective.

Result: Achieves up to 6x faster computation, 1.3x less memory usage than PSRO, while obtaining higher rewards in Two-player and Multi-Player games including Deceptive Messages Game, Kuhn Poker and Multi-Particle environment.

Conclusion: GEMS overcomes fundamental inefficiencies of PSRO while retaining game-theoretic guarantees, enabling scalable multi-agent learning across multiple domains.

Abstract: Scalable multi-agent reinforcement learning (MARL) remains a central
challenge for AI. Existing population-based methods, like Policy-Space Response
Oracles, PSRO, require storing explicit policy populations and constructing
full payoff matrices, incurring quadratic computation and linear memory costs.
We present Generative Evolutionary Meta-Solver (GEMS), a surrogate-free
framework that replaces explicit populations with a compact set of latent
anchors and a single amortized generator. Instead of exhaustively constructing
the payoff matrix, GEMS relies on unbiased Monte Carlo rollouts,
multiplicative-weights meta-dynamics, and a model-free empirical-Bernstein UCB
oracle to adaptively expand the policy set. Best responses are trained within
the generator using an advantage-based trust-region objective, eliminating the
need to store and train separate actors. We evaluated GEMS in a variety of
Two-player and Multi-Player games such as the Deceptive Messages Game, Kuhn
Poker and Multi-Particle environment. We find that GEMS is up to ~6x faster,
has 1.3x less memory usage than PSRO, while also reaps higher rewards
simultaneously. These results demonstrate that GEMS retains the game theoretic
guarantees of PSRO, while overcoming its fundamental inefficiencies, hence
enabling scalable multi-agent learning in multiple domains.

</details>


### [484] [Solve Smart, Not Often: Policy Learning for Costly MILP Re-solving](https://arxiv.org/abs/2509.23470)
*Rui Ai,Hugo De Oliveira Barbalho,Sirui Li,Alexei Robsky,David Simchi-Levi,Ishai Menache*

Main category: cs.LG

TL;DR: The paper proposes POC framework to determine optimal re-solving times for MILPs, balancing performance and computational cost, achieving 2%-17% improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Real-time operations need to decide when to re-solve computationally intensive MILPs, as frequent re-solving is costly but infrequent solving may use suboptimal solutions. Existing methods focus on heuristics and smooth objectives, lacking systematic approaches for NP-hard MILPs.

Method: Proximal Policy Optimization with Change Point Detection (POC) framework that detects environmental changes, selects beneficial samples, and establishes theoretical relationship between re-solves and cost.

Result: POC consistently outperforms existing baselines by 2%-17% across eight synthetic and real-world datasets, and introduces real-time MILP benchmarks.

Conclusion: POC provides a systematic solution for determining optimal re-solving times in real-time MILP operations, filling a literature gap and demonstrating significant performance improvements.

Abstract: A common challenge in real-time operations is deciding whether to re-solve an
optimization problem or continue using an existing solution. While modern data
platforms may collect information at high frequencies, many real-time
operations require repeatedly solving computationally intensive optimization
problems formulated as Mixed-Integer Linear Programs (MILPs). Determining when
to re-solve is, therefore, an economically important question. This problem
poses several challenges: 1) How to characterize solution optimality and
solving cost; 2) How to detect environmental changes and select beneficial
samples for solving the MILP; 3) Given the large time horizon and non-MDP
structure, vanilla reinforcement learning (RL) methods are not directly
applicable and tend to suffer from value function explosion. Existing
literature largely focuses on heuristics, low-data settings, and smooth
objectives, with little focus on common NP-hard MILPs. We propose a framework
called Proximal Policy Optimization with Change Point Detection (POC), which
systematically offers a solution for balancing performance and cost when
deciding appropriate re-solving times. Theoretically, we establish the
relationship between the number of re-solves and the re-solving cost. To test
our framework, we assemble eight synthetic and real-world datasets, and show
that POC consistently outperforms existing baselines by 2%-17%. As a side
benefit, our work fills the gap in the literature by introducing real-time MILP
benchmarks and evaluation criteria.

</details>


### [485] [Drift-Adapter: A Practical Approach to Near Zero-Downtime Embedding Model Upgrades in Vector Databases](https://arxiv.org/abs/2509.23471)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Drift-Adapter is a lightweight transformation layer that enables embedding model upgrades without rebuilding ANN indexes by mapping new queries to legacy embedding spaces, achieving 95-99% retrieval recall with minimal latency.


<details>
  <summary>Details</summary>
Motivation: Traditional embedding model upgrades require re-encoding entire corpora and rebuilding ANN indexes, causing significant operational disruption and computational costs.

Method: Three adapter parameterizations: Orthogonal Procrustes, Low-Rank Affine, and compact Residual MLP, trained on small samples of paired old and new embeddings to map new queries to legacy embedding space.

Result: Recovers 95-99% of retrieval recall (Recall@10, MRR) compared to full re-embedding, adds less than 10 microseconds query latency, reduces recompute costs by over 100 times, and enables near-zero operational interruption upgrades.

Conclusion: Drift-Adapter provides a pragmatic solution for agile model deployment by enabling embedding model upgrades without the need for full re-indexing, significantly reducing operational costs and disruption.

Abstract: Upgrading embedding models in production vector databases typically requires
re-encoding the entire corpus and rebuilding the Approximate Nearest Neighbor
(ANN) index, leading to significant operational disruption and computational
cost. This paper presents Drift-Adapter, a lightweight, learnable
transformation layer designed to bridge embedding spaces between model
versions. By mapping new queries into the legacy embedding space, Drift-Adapter
enables the continued use of the existing ANN index, effectively deferring full
re-computation. We systematically evaluate three adapter parameterizations:
Orthogonal Procrustes, Low-Rank Affine, and a compact Residual MLP, trained on
a small sample of paired old and new embeddings. Experiments on MTEB text
corpora and a CLIP image model upgrade (1M items) show that Drift-Adapter
recovers 95-99% of the retrieval recall (Recall@10, MRR) of a full
re-embedding, adding less than 10 microseconds of query latency. Compared to
operational strategies like full re-indexing or dual-index serving,
Drift-Adapter reduces recompute costs by over 100 times and facilitates
upgrades with near-zero operational interruption. We analyze robustness to
varied model drift, training data size, scalability to billion-item systems,
and the impact of design choices like diagonal scaling, demonstrating
Drift-Adapter's viability as a pragmatic solution for agile model deployment.

</details>


### [486] [Memory-Efficient Fine-Tuning via Low-Rank Activation Compression](https://arxiv.org/abs/2509.23472)
*Jiang-Xin Shi,Wen-Da Wei,Jin-Fei Qi,Xuanyu Chen,Tong Wei,Yu-Feng Li*

Main category: cs.LG

TL;DR: LoRAct is a memory-efficient fine-tuning method that compresses activations using low-rank approximation, reducing activation memory by ~80% compared to LoRA while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Parameter-efficient fine-tuning methods still have substantial memory overhead due to model activations, especially with large batch sizes and long contexts. The observation that activation ranks remain consistently low motivates compression.

Method: Proposes Low-Rank Activation Compression (LoRAct) that applies online compression during forward pass without calibration data. Uses a novel sampling-based orthogonal decomposition algorithm for low-rank matrices with better computational efficiency and tighter error bounds than RSVD.

Result: Experiments on vision and language tasks show LoRAct reduces activation memory by approximately 80% compared to LoRA while maintaining competitive performance.

Conclusion: LoRAct provides an effective memory-efficient fine-tuning approach that significantly reduces activation memory consumption without compromising model performance.

Abstract: The parameter-efficient fine-tuning paradigm has garnered significant
attention with the advancement of foundation models. Although numerous methods
have been proposed to reduce the number of trainable parameters, their
substantial memory overhead remains a critical bottleneck that hinders
practical deployment. In this paper, we observe that model activations
constitute a major source of memory consumption, especially under large batch
sizes and long context lengths; however, the rank of the activations remains
consistently low. Motivated by this insight, we propose a memory-efficient
fine-tuning approach Low-Rank Activation Compression (LoRAct). Unlike prior
work, LoRAct provides a more flexible and versatile compressing strategy that
can be applied online during the forward pass without the need for any
calibration data. Moreover, LoRAct incorporates a novel sampling-based
orthogonal decomposition algorithm specifically designed for low-rank matrices,
offering improved computational efficiency and a tighter error bound compared
to the widely used RSVD. Experiments on both vision and language tasks
demonstrate the effectiveness of LoRAct. Notably, LoRAct further reduces
activation memory by approximately 80% in comparison with the widely adopted
LoRA method, while maintaining competitive performance. The source code is
available at https://github.com/shijxcs/meft.

</details>


### [487] [Statistical Learning Guarantees for Group-Invariant Barron Functions](https://arxiv.org/abs/2509.23474)
*Yahong Yang,Wei Zhu*

Main category: cs.LG

TL;DR: Group-invariant neural networks show improved generalization error for symmetric functions, with group-dependent approximation improvements and unchanged estimation error.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical benefits of incorporating group-invariant structures in neural networks for learning symmetric functions.

Method: Analyze generalization error within Barron framework, examining approximation rates with group-dependent factors and Rademacher complexity comparisons.

Result: Group invariance improves approximation accuracy (factor δ ≤ 1) without increasing estimation error, leading to better generalization for symmetric functions.

Conclusion: Encoding group-invariant structures provides clear statistical advantages for symmetric target functions, with rigorous theoretical foundation.

Abstract: We investigate the generalization error of group-invariant neural networks
within the Barron framework. Our analysis shows that incorporating
group-invariant structures introduces a group-dependent factor
$\delta_{G,\Gamma,\sigma} \le 1$ into the approximation rate. When this factor
is small, group invariance yields substantial improvements in approximation
accuracy. On the estimation side, we establish that the Rademacher complexity
of the group-invariant class is no larger than that of the non-invariant
counterpart, implying that the estimation error remains unaffected by the
incorporation of symmetry. Consequently, the generalization error can improve
significantly when learning functions with inherent group symmetries. We
further provide illustrative examples demonstrating both favorable cases, where
$\delta_{G,\Gamma,\sigma}\approx |G|^{-1}$, and unfavorable ones, where
$\delta_{G,\Gamma,\sigma}\approx 1$. Overall, our results offer a rigorous
theoretical foundation showing that encoding group-invariant structures in
neural networks leads to clear statistical advantages for symmetric target
functions.

</details>


### [488] [Temporal Generalization: A Reality Check](https://arxiv.org/abs/2509.23487)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.LG

TL;DR: This paper investigates whether ML models can generalize to future data through parameter interpolation and extrapolation methods, but finds none consistently outperform simply using the latest model parameters.


<details>
  <summary>Details</summary>
Motivation: Machine learning models often fail to maintain performance under distribution shifts, leading to inaccurate predictions on unseen future data. The research aims to determine if models can achieve such generalization using only past data.

Method: The study explores two approaches: parameter interpolation (convex combinations of past model parameters) and parameter extrapolation (explicit extrapolation beyond the convex hull of past parameters). These methods are benchmarked on diverse temporal tasks including language modeling, news summarization, image classification, and more.

Result: Empirical findings show that none of the evaluated methods consistently outperforms the simple baseline of using the latest available model parameters across all scenarios.

Conclusion: In the absence of access to future data or robust assumptions about data-generating processes, the results highlight the inherent difficulties of generalizing to future data and warrant caution when evaluating claims of such generalization.

Abstract: Machine learning (ML) models often struggle to maintain performance under
distribution shifts, leading to inaccurate predictions on unseen future data.
In this work, we investigate whether and under what conditions models can
achieve such a generalization when relying solely on past data. We explore two
primary approaches: convex combinations of past model parameters
(\emph{parameter interpolation}) and explicit extrapolation beyond the convex
hull of past parameters (\emph{parameter extrapolation}). We benchmark several
methods within these categories on a diverse set of temporal tasks, including
language modeling, news summarization, news tag prediction, academic paper
categorization, satellite image-based land use classification over time, and
historical yearbook photo gender prediction. Our empirical findings show that
none of the evaluated methods consistently outperforms the simple baseline of
using the latest available model parameters in all scenarios. In the absence of
access to future data or robust assumptions about the underlying
data-generating process, these results underscore the inherent difficulties of
generalizing and extrapolating to future data and warrant caution when
evaluating claims of such generalization.

</details>


### [489] [Revisiting Multivariate Time Series Forecasting with Missing Values](https://arxiv.org/abs/2509.23494)
*Jie Yang,Yifan Hu,Kexin Zhang,Luyang Niu,Yushun Dong,Philip S. Yu,Kaize Ding*

Main category: cs.LG

TL;DR: CRIB is a novel framework that directly predicts from partially observed time series without imputation, using Information Bottleneck principle and consistency regularization to handle missing values in multivariate time series forecasting.


<details>
  <summary>Details</summary>
Motivation: Traditional imputation-then-prediction approaches are problematic because there's no ground truth for missing values, making imputation error-prone and potentially degrading prediction accuracy by corrupting the underlying data distribution.

Method: Proposes Consistency-Regularized Information Bottleneck (CRIB) framework with unified-variate attention mechanism and consistency regularization to learn robust representations that filter out noise from missing values while preserving predictive signals.

Result: Comprehensive experiments on four real-world datasets show CRIB effectively predicts accurately even under high missing rates, outperforming traditional imputation-based approaches.

Conclusion: CRIB represents a paradigm shift from imputation-based methods to direct prediction from incomplete data, providing a more robust solution for multivariate time series forecasting with missing values.

Abstract: Missing values are common in real-world time series, and multivariate time
series forecasting with missing values (MTSF-M) has become a crucial area of
research for ensuring reliable predictions. To address the challenge of missing
data, current approaches have developed an imputation-then-prediction framework
that uses imputation modules to fill in missing values, followed by forecasting
on the imputed data. However, this framework overlooks a critical issue: there
is no ground truth for the missing values, making the imputation process
susceptible to errors that can degrade prediction accuracy. In this paper, we
conduct a systematic empirical study and reveal that imputation without direct
supervision can corrupt the underlying data distribution and actively degrade
prediction accuracy. To address this, we propose a paradigm shift that moves
away from imputation and directly predicts from the partially observed time
series. We introduce Consistency-Regularized Information Bottleneck (CRIB), a
novel framework built on the Information Bottleneck principle. CRIB combines a
unified-variate attention mechanism with a consistency regularization scheme to
learn robust representations that filter out noise introduced by missing values
while preserving essential predictive signals. Comprehensive experiments on
four real-world datasets demonstrate the effectiveness of CRIB, which predicts
accurately even under high missing rates. Our code is available in
https://github.com/Muyiiiii/CRIB.

</details>


### [490] [Beyond Outliers: A Study of Optimizers Under Quantization](https://arxiv.org/abs/2509.23500)
*Georgios Vlassis,Saleh Ashkboos,Alexandra Volkova,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: This paper systematically studies how optimizer choice affects model robustness under quantization (both PTQ and QAT), finding that traditional outlier metrics fail to predict PTQ performance and that Shampoo optimizer shows the best quantization robustness and parameter efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand how optimizer choice impacts model performance when quantization is applied, as systematic evidence on optimizer-quantization interactions is limited despite progress in both areas.

Method: Train full-precision models (50M-1.5B parameters) with six optimizers, apply PTQ to evaluate performance degradation, analyze why outlier metrics fail, conduct QAT from scratch, and derive scaling laws for quantization-aware training.

Result: Outlier metrics like MMR and Kurtosis fail to predict PTQ performance across optimizers; Shampoo optimizer shows lowest accuracy degradation under QAT and achieves highest parameter efficiency according to scaling laws.

Conclusion: Optimizer choice significantly affects quantization robustness, with Shampoo performing best under QAT, and traditional outlier metrics are insufficient for predicting quantization performance due to error accumulation effects.

Abstract: As new optimizers gain traction and model quantization becomes standard for
efficient deployment, a key question arises: how does the choice of optimizer
affect model performance in the presence of quantization? Despite progress in
both areas, systematic evidence on optimizer-quantization interactions remains
limited. To fill this gap, we study the impact of optimizer choice on model
robustness under quantization, considering both post-training quantization
(PTQ), and quantization-aware training (QAT). We first train full-precision
models, ranging from 50M to 1.5B parameters, with six optimizers, to explore
the hyperparameter landscape, and establish well-tuned baselines. We then apply
PTQ to evaluate how model performance degrades when trained with different
optimizers. We find that outlier-related metrics, such as the max-to-mean ratio
(MMR) and Kurtosis, fail to predict the PTQ performance across different
optimizers. We show analytically that this is due to the MMR capturing only
isolated layer errors, while ignoring how quantization errors accumulate and
propagate through the network. To study the QAT degradation, we train quantized
models from scratch and compare them to our original-precision baselines. We
find that optimizers performing well in the original pretraining setup may not
remain optimal under QAT, and that models trained with Shampoo show the lowest
accuracy degradation. Finally, we derive scaling laws for quantization-aware
training under different optimizers, showing that Shampoo achieves the highest
parameter efficiency of all tested optimizers.

</details>


### [491] [Disentanglement of Variations with Multimodal Generative Modeling](https://arxiv.org/abs/2509.23548)
*Yijie Zhang,Yiyang Shen,Weiran Wang*

Main category: cs.LG

TL;DR: IDMVAE is a multimodal VAE that uses mutual information regularization and diffusion models to better disentangle shared and private information across modalities, improving generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal generative models struggle to properly disentangle shared and private information, especially on challenging datasets where likelihood models are insufficient.

Method: Proposes mutual information-based regularizations including cross-view mutual information maximization and cycle-consistency loss, combined with diffusion models for better latent priors.

Result: IDMVAE achieves cleaner separation between shared and private information, demonstrating superior generation quality and semantic coherence on challenging datasets.

Conclusion: The proposed components are complementary and effectively address the disentanglement problem in multimodal representation learning.

Abstract: Multimodal data are prevalent across various domains, and learning robust
representations of such data is paramount to enhancing generation quality and
downstream task performance. To handle heterogeneity and interconnections among
different modalities, recent multimodal generative models extract shared and
private (modality-specific) information with two separate variables. Despite
attempts to enforce disentanglement between these two variables, these methods
struggle with challenging datasets where the likelihood model is insufficient.
In this paper, we propose Information-disentangled Multimodal VAE (IDMVAE) to
explicitly address this issue, with rigorous mutual information-based
regularizations, including cross-view mutual information maximization for
extracting shared variables, and a cycle-consistency style loss for redundancy
removal using generative augmentations. We further introduce diffusion models
to improve the capacity of latent priors. These newly proposed components are
complementary to each other. Compared to existing approaches, IDMVAE shows a
clean separation between shared and private information, demonstrating superior
generation quality and semantic coherence on challenging datasets.

</details>


### [492] [Fusing Sequence Motifs and Pan-Genomic Features: Antimicrobial Resistance Prediction using an Explainable Lightweight 1D CNN-XGBoost Ensemble](https://arxiv.org/abs/2509.23552)
*Md. Saiful Bari Siddiqui,Nowshin Tarannum*

Main category: cs.LG

TL;DR: AMR-EnsembleNet: An ensemble framework combining 1D CNN for sequence motif learning and XGBoost for feature interactions, achieving superior AMR prediction performance on E. coli strains.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current AMR prediction methods that ignore SNP sequential context or require large datasets, by creating an efficient ensemble approach for moderately-sized genomic datasets.

Method: Developed a lightweight 1D CNN to learn sequence motifs from SNP data, ensembled with XGBoost to capture complex feature interactions, trained on 809 E. coli strains across four antibiotics.

Result: Achieved MCC of 0.926 for Ciprofloxacin and highest Macro F1-score of 0.691 for Gentamicin, with model focusing on known AMR genes like fusA and parC.

Conclusion: Fusing sequence-aware 1D CNN with feature-based XGBoost creates a powerful ensemble that overcomes limitations of order-agnostic or standalone sequence models for AMR prediction.

Abstract: Antimicrobial Resistance (AMR) is a rapidly escalating global health crisis.
While genomic sequencing enables rapid prediction of resistance phenotypes,
current computational methods have limitations. Standard machine learning
models treat the genome as an unordered collection of features, ignoring the
sequential context of Single Nucleotide Polymorphisms (SNPs). State-of-the-art
sequence models like Transformers are often too data-hungry and computationally
expensive for the moderately-sized datasets that are typical in this domain. To
address these challenges, we propose AMR-EnsembleNet, an ensemble framework
that synergistically combines sequence-based and feature-based learning. We
developed a lightweight, custom 1D Convolutional Neural Network (CNN) to
efficiently learn predictive sequence motifs from high-dimensional SNP data.
This sequence-aware model was ensembled with an XGBoost model, a powerful
gradient boosting system adept at capturing complex, non-local feature
interactions. We trained and evaluated our framework on a benchmark dataset of
809 E. coli strains, predicting resistance across four antibiotics with varying
class imbalance. Our 1D CNN-XGBoost ensemble consistently achieved top-tier
performance across all the antibiotics, reaching a Matthews Correlation
Coefficient (MCC) of 0.926 for Ciprofloxacin (CIP) and the highest Macro
F1-score of 0.691 for the challenging Gentamicin (GEN) AMR prediction. We also
show that our model consistently focuses on SNPs within well-known AMR genes
like fusA and parC, confirming it learns the correct genetic signals for
resistance. Our work demonstrates that fusing a sequence-aware 1D CNN with a
feature-based XGBoost model creates a powerful ensemble, overcoming the
limitations of using either an order-agnostic or a standalone sequence model.

</details>


### [493] [Improving constraint-based discovery with robust propagation and reliable LLM priors](https://arxiv.org/abs/2509.23570)
*Ruiqi Lyu,Alistair Turcan,Martin Jinye Zhang,Bryan Wilder*

Main category: cs.LG

TL;DR: MosaCD is a causal discovery method that combines CI tests and LLM annotations to create high-confidence seed edges, then uses confidence-down propagation to build more accurate causal graphs than existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional constraint-based methods like PC rely on perfect CI tests and exhaustive search, which often fail in practice leading to cascading errors. LLMs can help but are prone to hallucinations, requiring careful filtering.

Method: MosaCD propagates edges from high-confidence seeds derived from both CI tests and LLM annotations. It uses shuffled queries to filter LLM hallucinations and applies confidence-down propagation that orients the most reliable edges first.

Result: Across multiple real-world graphs, MosaCD achieves higher accuracy in final graph construction than existing constraint-based methods.

Conclusion: MosaCD improves causal discovery by combining the strengths of CI tests and LLMs while mitigating their weaknesses through careful seed selection and robust propagation strategies.

Abstract: Learning causal structure from observational data is central to scientific
modeling and decision-making. Constraint-based methods aim to recover
conditional independence (CI) relations in a causal directed acyclic graph
(DAG). Classical approaches such as PC and subsequent methods orient
v-structures first and then propagate edge directions from these seeds,
assuming perfect CI tests and exhaustive search of separating subsets --
assumptions often violated in practice, leading to cascading errors in the
final graph. Recent work has explored using large language models (LLMs) as
experts, prompting sets of nodes for edge directions, and could augment edge
orientation when assumptions are not met. However, such methods implicitly
assume perfect experts, which is unrealistic for hallucination-prone LLMs. We
propose MosaCD, a causal discovery method that propagates edges from a
high-confidence set of seeds derived from both CI tests and LLM annotations. To
filter hallucinations, we introduce shuffled queries that exploit LLMs'
positional bias, retaining only high-confidence seeds. We then apply a novel
confidence-down propagation strategy that orients the most reliable edges
first, and can be integrated with any skeleton-based discovery method. Across
multiple real-world graphs, MosaCD achieves higher accuracy in final graph
construction than existing constraint-based methods, largely due to the
improved reliability of initial seeds and robust propagation strategies.

</details>


### [494] [EVO-LRP: Evolutionary Optimization of LRP for Interpretable Model Explanations](https://arxiv.org/abs/2509.23585)
*Emerald Zhang,Julian Weaver,Edward Castillo*

Main category: cs.LG

TL;DR: EVO-LRP uses evolutionary optimization to tune LRP hyperparameters for better interpretability metrics and visual coherence in XAI methods.


<details>
  <summary>Details</summary>
Motivation: Traditional XAI methods face trade-offs between detail and interpretability, while LRP implementations rely on heuristic rules not optimized for clarity or model alignment.

Method: EVO-LRP applies Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to optimize LRP hyperparameters based on quantitative interpretability metrics like faithfulness and sparseness.

Result: EVO-LRP outperforms traditional XAI approaches in interpretability metrics and visual coherence, showing strong sensitivity to class-specific features.

Conclusion: Attribution quality can be systematically improved through principled, task-specific optimization of XAI methods.

Abstract: Explainable AI (XAI) methods help identify which image regions influence a
model's prediction, but often face a trade-off between detail and
interpretability. Layer-wise Relevance Propagation (LRP) offers a model-aware
alternative. However, LRP implementations commonly rely on heuristic rule sets
that are not optimized for clarity or alignment with model behavior. We
introduce EVO-LRP, a method that applies Covariance Matrix Adaptation Evolution
Strategy (CMA-ES) to tune LRP hyperparameters based on quantitative
interpretability metrics, such as faithfulness or sparseness. EVO-LRP
outperforms traditional XAI approaches in both interpretability metric
performance and visual coherence, with strong sensitivity to class-specific
features. These findings demonstrate that attribution quality can be
systematically improved through principled, task-specific optimization.

</details>


### [495] [Sketching Low-Rank Plus Diagonal Matrices](https://arxiv.org/abs/2509.23587)
*Andres Fernandez,Felix Dangel,Philipp Hennig,Frank Schneider*

Main category: cs.LG

TL;DR: SKETCHLORD is a method that simultaneously estimates both low-rank and diagonal components of linear operators, outperforming sequential approaches and providing high-fidelity approximations for large-scale operators.


<details>
  <summary>Details</summary>
Motivation: Many machine learning tasks involve high-dimensional linear operators that are costly to compute. Existing sketched methods only construct either low-rank or diagonal approximations, leading to approximation errors due to oversimplified structure assumptions.

Method: SKETCHLORD simultaneously estimates both low-rank and diagonal components through convex optimization, targeting Low-Rank plus Diagonal (LoRD) linear operators from few matrix-vector products.

Result: Theoretical and empirical results show SKETCHLORD's joint estimation is superior to sequential variants, accurately recovering LoRD structures in synthetic experiments.

Conclusion: SKETCHLORD provides a valuable addition to structured approximation tools, particularly for high-fidelity approximations of large-scale operators like deep learning Hessians.

Abstract: Many relevant machine learning and scientific computing tasks involve
high-dimensional linear operators accessible only via costly matrix-vector
products. In this context, recent advances in sketched methods have enabled the
construction of *either* low-rank *or* diagonal approximations from few
matrix-vector products. This provides great speedup and scalability, but
approximation errors arise due to the assumed simpler structure. This work
introduces SKETCHLORD, a method that simultaneously estimates both low-rank
*and* diagonal components, targeting the broader class of Low-Rank *plus*
Diagonal (LoRD) linear operators. We demonstrate theoretically and empirically
that this joint estimation is superior also to any sequential variant
(diagonal-then-low-rank or low-rank-then-diagonal). Then, we cast SKETCHLORD as
a convex optimization problem, leading to a scalable algorithm. Comprehensive
experiments on synthetic (approximate) LoRD matrices confirm SKETCHLORD's
performance in accurately recovering these structures. This positions it as a
valuable addition to the structured approximation toolkit, particularly when
high-fidelity approximations are desired for large-scale operators, such as the
deep learning Hessian.

</details>


### [496] [Toward a Holistic Approach to Continual Model Merging](https://arxiv.org/abs/2509.23592)
*Hoang Phan,Sungmin Cha,Tung Lam Tran,Qi Lei*

Main category: cs.LG

TL;DR: A holistic continual model merging framework that intervenes at pre-merging, during merging, and post-merging stages to address catastrophic forgetting without accessing old data, using tangent space fine-tuning, functional information from optimizer states, and representation alignment.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of conventional continual learning approaches that either maintain growing task vectors (scalability issues) or rely solely on weight-space merging (losing functional information) when old data is inaccessible.

Method: Three-stage approach: 1) Pre-merging: fine-tune main model in tangent space for weight disentanglement; 2) During merging: leverage functional information from optimizer states; 3) Post-merging: correct representation discrepancy between pre- and post-merged models.

Result: Achieves competitive performance on standard class-incremental and domain-incremental benchmarks while operating under constant memory constraints without accessing historical data.

Conclusion: Provides a scalable and efficient solution to catastrophic forgetting problem by combining tangent space fine-tuning, functional information utilization, and representation alignment in a holistic framework.

Abstract: We present a holistic framework for continual model merging that intervenes
at three critical stages: pre-merging, during merging, and post-merging-to
address two fundamental challenges in continual learning. In particular,
conventional approaches either maintain a growing list of per-domain task
vectors, leading to scalability issues or rely solely on weight-space merging
when old data is inaccessible, thereby losing crucial functional information.
Our method overcomes these limitations by first fine-tuning the main model
within its tangent space on domain-specific data; this linearization amplifies
per-task weight disentanglement, effectively mitigating across-task
interference. During merging, we leverage functional information from available
optimizer states beyond mere parameter averages to avoid the need to revisit
old data. Finally, a post-merging correction aligns the representation
discrepancy between pre- and post-merged models, reducing bias and enhancing
overall performance-all while operating under constant memory constraints
without accessing historical data. Extensive experiments on standard
class-incremental and domain-incremental benchmarks demonstrate that our
approach not only achieves competitive performance but also provides a scalable
and efficient solution to the catastrophic forgetting problem.

</details>


### [497] [Avoid Catastrophic Forgetting with Rank-1 Fisher from Diffusion Models](https://arxiv.org/abs/2509.23593)
*Zekun Wang,Anant Gupta,Zihan Dong,Christopher J. MacLellan*

Main category: cs.LG

TL;DR: Proposes a rank-1 EWC method for continual learning in diffusion models, leveraging their gradient geometry to reduce forgetting while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing continual learning methods: replay requires strong generators and suffers from distributional drift, while EWC assumes shared optimum and uses diagonal Fisher approximation.

Method: Leverages diffusion model gradient geometry in low SNR regime, proposes rank-1 EWC variant that captures dominant curvature direction, pairs with replay to encourage parameter sharing and mitigate drift.

Result: Consistently improves average FID and reduces forgetting on class-incremental image generation datasets (MNIST, FashionMNIST, CIFAR-10, ImageNet-1k), nearly eliminating forgetting on MNIST/FashionMNIST and halving it on ImageNet-1k.

Conclusion: Diffusion models have approximately rank-1 Fisher, enabling effective EWC that complements replay by constraining replay-induced drift while encouraging parameter sharing across tasks.

Abstract: Catastrophic forgetting remains a central obstacle for continual learning in
neural models. Popular approaches -- replay and elastic weight consolidation
(EWC) -- have limitations: replay requires a strong generator and is prone to
distributional drift, while EWC implicitly assumes a shared optimum across
tasks and typically uses a diagonal Fisher approximation. In this work, we
study the gradient geometry of diffusion models, which can already produce
high-quality replay data. We provide theoretical and empirical evidence that,
in the low signal-to-noise ratio (SNR) regime, per-sample gradients become
strongly collinear, yielding an empirical Fisher that is effectively rank-1 and
aligned with the mean gradient. Leveraging this structure, we propose a rank-1
variant of EWC that is as cheap as the diagonal approximation yet captures the
dominant curvature direction. We pair this penalty with a replay-based approach
to encourage parameter sharing across tasks while mitigating drift. On
class-incremental image generation datasets (MNIST, FashionMNIST, CIFAR-10,
ImageNet-1k), our method consistently improves average FID and reduces
forgetting relative to replay-only and diagonal-EWC baselines. In particular,
forgetting is nearly eliminated on MNIST and FashionMNIST and is roughly halved
on ImageNet-1k. These results suggest that diffusion models admit an
approximately rank-1 Fisher. With a better Fisher estimate, EWC becomes a
strong complement to replay: replay encourages parameter sharing across tasks,
while EWC effectively constrains replay-induced drift.

</details>


### [498] [Characteristic Root Analysis and Regularization for Linear Time Series Forecasting](https://arxiv.org/abs/2509.23597)
*Zheng Wang,Kaixuan Zhang,Wanfang Chen,Xiaonan Lu,Longyuan Li,Tobias Schlagenhauf*

Main category: cs.LG

TL;DR: This paper systematically studies linear models for time series forecasting, focusing on characteristic roots' role in temporal dynamics. It analyzes noise-free and noisy regimes, identifies data-scaling challenges, and proposes two robust root restructuring strategies that achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Recent studies show simple linear models can be surprisingly competitive in time series forecasting, warranting deeper theoretical investigation into their robustness and interpretability.

Method: 1) Theoretical analysis of characteristic roots in noise-free and noisy settings; 2) Proposed two strategies: rank reduction techniques (Reduced-Rank Regression, Direct Weight Rank Reduction) and Root Purge method for noise suppression; 3) Extensive experiments on standard benchmarks.

Result: Both proposed approaches demonstrate effectiveness, validating theoretical insights and achieving state-of-the-art results in several settings. The methods successfully address noise challenges and improve forecasting performance.

Conclusion: Integrating classical linear systems theories with modern learning techniques can build robust, interpretable, and data-efficient forecasting models, highlighting the continued relevance of linear approaches in time series analysis.

Abstract: Time series forecasting remains a critical challenge across numerous domains,
yet the effectiveness of complex models often varies unpredictably across
datasets. Recent studies highlight the surprising competitiveness of simple
linear models, suggesting that their robustness and interpretability warrant
deeper theoretical investigation. This paper presents a systematic study of
linear models for time series forecasting, with a focus on the role of
characteristic roots in temporal dynamics. We begin by analyzing the noise-free
setting, where we show that characteristic roots govern long-term behavior and
explain how design choices such as instance normalization and channel
independence affect model capabilities. We then extend our analysis to the
noisy regime, revealing that models tend to produce spurious roots. This leads
to the identification of a key data-scaling property: mitigating the influence
of noise requires disproportionately large training data, highlighting the need
for structural regularization. To address these challenges, we propose two
complementary strategies for robust root restructuring. The first uses rank
reduction techniques, including Reduced-Rank Regression and Direct Weight Rank
Reduction, to recover the low-dimensional latent dynamics. The second, a novel
adaptive method called Root Purge, encourages the model to learn a
noise-suppressing null space during training. Extensive experiments on standard
benchmarks demonstrate the effectiveness of both approaches, validating our
theoretical insights and achieving state-of-the-art results in several
settings. Our findings underscore the potential of integrating classical
theories for linear systems with modern learning techniques to build robust,
interpretable, and data-efficient forecasting models.

</details>


### [499] [GraphIFE: Rethinking Graph Imbalance Node Classification via Invariant Learning](https://arxiv.org/abs/2509.23616)
*Fanlong Zeng,Wensheng Gan,Philip S. Yu*

Main category: cs.LG

TL;DR: GraphIFE addresses class imbalance in graph data by mitigating quality inconsistency in synthesized nodes using graph invariant learning, achieving superior performance over baselines.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in graph data causes biased learning and poor performance on minority classes, with existing GNNs failing to handle quality inconsistency in synthesized nodes.

Method: Proposes GraphIFE framework with graph invariant learning strategies to strengthen embedding space representation and identify invariant features.

Result: Extensive experiments show GraphIFE consistently outperforms various baselines across multiple datasets with robust generalization.

Conclusion: GraphIFE effectively mitigates quality inconsistency in synthesized nodes and enhances model performance under graph imbalance conditions.

Abstract: The class imbalance problem refers to the disproportionate distribution of
samples across different classes within a dataset, where the minority classes
are significantly underrepresented. This issue is also prevalent in
graph-structured data. Most graph neural networks (GNNs) implicitly assume a
balanced class distribution and therefore often fail to account for the
challenges introduced by class imbalance, which can lead to biased learning and
degraded performance on minority classes. We identify a quality inconsistency
problem in synthesized nodes, which leads to suboptimal performance under graph
imbalance conditions. To mitigate this issue, we propose GraphIFE (Graph
Invariant Feature Extraction), a novel framework designed to mitigate quality
inconsistency in synthesized nodes. Our approach incorporates two key concepts
from graph invariant learning and introduces strategies to strengthen the
embedding space representation, thereby enhancing the model's ability to
identify invariant features. Extensive experiments demonstrate the framework's
efficiency and robust generalization, as GraphIFE consistently outperforms
various baselines across multiple datasets. The code is publicly available at
https://github.com/flzeng1/GraphIFE.

</details>


### [500] [DRIK: Distribution-Robust Inductive Kriging without Information Leakage](https://arxiv.org/abs/2509.23631)
*Chen Yang,Changhao Zhao,Chen Wang,Jiansheng Fan*

Main category: cs.LG

TL;DR: The paper identifies information leakage issues in conventional inductive kriging evaluation and proposes a 3x3 partition method and DRIK approach to improve out-of-distribution generalization.


<details>
  <summary>Details</summary>
Motivation: Conventional training-evaluation setups for inductive kriging suffer from information leakage and poor out-of-distribution generalization, which obscures true performance characteristics.

Method: Proposes a 3x3 partition to separate training, validation, and test sets cleanly, and introduces DRIK with three-tier strategy: node coordinate perturbation, edge dropping, and pseudo-labeled subgraph addition.

Result: DRIK consistently outperforms existing methods on six spatio-temporal datasets, achieving up to 12.48% lower MAE while maintaining strong scalability.

Conclusion: The proposed 3x3 partition and DRIK approach effectively address information leakage and enhance out-of-distribution generalization in inductive kriging.

Abstract: Inductive kriging supports high-resolution spatio-temporal estimation with
sparse sensor networks, but conventional training-evaluation setups often
suffer from information leakage and poor out-of-distribution (OOD)
generalization. We find that the common 2x2 spatio-temporal split allows test
data to influence model selection through early stopping, obscuring the true
OOD characteristics of inductive kriging. To address this issue, we propose a
3x3 partition that cleanly separates training, validation, and test sets,
eliminating leakage and better reflecting real-world applications. Building on
this redefined setting, we introduce DRIK, a Distribution-Robust Inductive
Kriging approach designed with the intrinsic properties of inductive kriging in
mind to explicitly enhance OOD generalization, employing a three-tier strategy
at the node, edge, and subgraph levels. DRIK perturbs node coordinates to
capture continuous spatial relationships, drops edges to reduce ambiguity in
information flow and increase topological diversity, and adds pseudo-labeled
subgraphs to strengthen domain generalization. Experiments on six diverse
spatio-temporal datasets show that DRIK consistently outperforms existing
methods, achieving up to 12.48% lower MAE while maintaining strong scalability.

</details>


### [501] [PreScope: Unleashing the Power of Prefetching for Resource-Constrained MoE Inference](https://arxiv.org/abs/2509.23638)
*Enda Yu,Zhaoning Zhang,Dezun Dong,Yongwei Wu,Xiangke Liao*

Main category: cs.LG

TL;DR: PreScope is a prediction-driven expert scheduling system that addresses memory and PCIe latency bottlenecks in Mixture-of-Experts models by using learnable predictors, cross-layer scheduling, and asynchronous I/O optimization.


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts models face significant memory and PCIe latency bottlenecks when deployed on commodity hardware, with CPU offloading causing PCIe transfer latency that exceeds GPU computation by several folds.

Method: 1) Learnable Layer-Aware Predictor (LLaPor) for expert activation patterns; 2) Prefetch-Aware Cross-Layer Scheduling (PreSched) for optimal plans; 3) Asynchronous I/O Optimizer (AsyncIO) to decouple I/O from computation.

Result: PreScope achieves 141% higher throughput and 74.6% lower latency compared to state-of-the-art solutions.

Conclusion: PreScope effectively addresses the key challenges in MoE model deployment through its prediction-driven scheduling approach, significantly improving performance on commodity hardware.

Abstract: Mixture-of-Experts (MoE) models face memory and PCIe latency bottlenecks when
deployed on commodity hardware. Offloading expert weights to CPU memory results
in PCIe transfer latency that exceeds GPU computation by several folds. We
present PreScope, a prediction-driven expert scheduling system that addresses
three key challenges: inaccurate activation prediction, PCIe bandwidth
competition, and cross-device scheduling complexity. Our solution includes: 1)
Learnable Layer-Aware Predictor (LLaPor) that captures layer-specific expert
activation patterns; 2) Prefetch-Aware Cross-Layer Scheduling (PreSched) that
generates globally optimal plans balancing prefetching costs and loading
overhead; 3) Asynchronous I/O Optimizer (AsyncIO) that decouples I/O from
computation, eliminating waiting bubbles. PreScope achieves 141% higher
throughput and 74.6% lower latency than state-of-the-art solutions.

</details>


### [502] [Virtual Nodes based Heterogeneous Graph Convolutional Neural Network for Efficient Long-Range Information Aggregation](https://arxiv.org/abs/2509.23660)
*Ranhui Yan,Jia cai*

Main category: cs.LG

TL;DR: VN-HGCN is a heterogeneous graph neural network that uses virtual nodes to improve long-range information flow, achieving better performance with fewer layers (only 4) compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing heterogeneous graph models struggle with capturing long-range dependencies, requiring many layers that lead to computational complexity and over-smoothing issues.

Method: Proposes VN-HGCN which introduces virtual nodes that connect to all nodes of specific types, enabling efficient aggregation of long-range information across different node and edge types.

Result: Empirical evaluations show VN-HGCN's effectiveness, with extensive experiments on three real-world datasets demonstrating superiority over state-of-the-art baselines.

Conclusion: VN-HGCN provides an effective framework for heterogeneous graph learning that can be applied to other HGNN models, offering improved performance with reduced computational complexity.

Abstract: Heterogeneous Graph Neural Networks (HGNNs) have exhibited powerful
performance in heterogeneous graph learning by aggregating information from
various types of nodes and edges. However, existing heterogeneous graph models
often struggle to capture long-range information or necessitate stacking
numerous layers to learn such dependencies, resulting in high computational
complexity and encountering over-smoothing issues. In this paper, we propose a
Virtual Nodes based Heterogeneous Graph Convolutional Network (VN-HGCN), which
leverages virtual nodes to facilitate enhanced information flow within the
graph. Virtual nodes are auxiliary nodes interconnected with all nodes of a
specific type in the graph, facilitating efficient aggregation of long-range
information across different types of nodes and edges. By incorporating virtual
nodes into the graph structure, VN-HGCN achieves effective information
aggregation with only $4$ layers. Additionally, we demonstrate that VN-HGCN can
serve as a versatile framework that can be seamlessly applied to other HGNN
models, showcasing its generalizability. Empirical evaluations validate the
effectiveness of VN-HGCN, and extensive experiments conducted on three
real-world heterogeneous graph datasets demonstrate the superiority of our
model over several state-of-the-art baselines.

</details>


### [503] [Pure Node Selection for Imbalanced Graph Node Classification](https://arxiv.org/abs/2509.23662)
*Fanlong Zeng,Wensheng Gan,Jiayang Wu,Philip S. Yu*

Main category: cs.LG

TL;DR: PNS is a plug-and-play module that addresses the Randomness Anomalous Connectivity Problem (RACP) in graph neural networks by operating during node synthesis, eliminating random seed effects and improving performance on imbalanced graph data.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in graph-structured data is often overlooked by GNNs, and random seeds can cause significant performance degradation (RACP), which needs to be addressed without designing specialized algorithms for each type of imbalance.

Method: Proposed Pure Node Sampling (PNS) - a plug-and-play module that operates during node synthesis to mitigate RACP and abnormal neighbor distribution, eliminating random factor influences.

Result: Experimental results show PNS effectively eliminates unfavorable random seed effects and outperforms baselines across various benchmark datasets with different GNN backbones, demonstrating stability and effectiveness.

Conclusion: PNS successfully addresses the RACP problem in graph neural networks, providing a stable solution that works across different datasets and GNN architectures while handling class imbalance issues.

Abstract: The problem of class imbalance refers to an uneven distribution of quantity
among classes in a dataset, where some classes are significantly
underrepresented compared to others. Class imbalance is also prevalent in
graph-structured data. Graph neural networks (GNNs) are typically based on the
assumption of class balance, often overlooking the issue of class imbalance. In
our investigation, we identified a problem, which we term the Randomness
Anomalous Connectivity Problem (RACP), where certain off-the-shelf models are
affected by random seeds, leading to a significant performance degradation. To
eliminate the influence of random factors in algorithms, we proposed PNS (Pure
Node Sampling) to address the RACP in the node synthesis stage. Unlike existing
approaches that design specialized algorithms to handle either quantity
imbalance or topological imbalance, PNS is a novel plug-and-play module that
operates directly during node synthesis to mitigate RACP. Moreover, PNS also
alleviates performance degradation caused by abnormal distribution of node
neighbors. We conduct a series of experiments to identify what factors are
influenced by random seeds. Experimental results demonstrate the effectiveness
and stability of our method, which not only eliminates the effect of
unfavorable random seeds but also outperforms the baseline across various
benchmark datasets with different GNN backbones. Data and code are available at
https://github.com/flzeng1/PNS.

</details>


### [504] [Calibration Meets Reality: Making Machine Learning Predictions Trustworthy](https://arxiv.org/abs/2509.23665)
*Kristina P. Sinaga,Arjun S. Nair*

Main category: cs.LG

TL;DR: Theoretical analysis of post-hoc calibration methods (Platt scaling, isotonic regression) showing their performance depends on feature informativeness, with empirical validation across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Lack of comprehensive theoretical understanding of post-hoc calibration methods and their performance across different datasets and model architectures, particularly regarding the role of feature quality.

Method: Rigorous theoretical analysis with convergence guarantees, complexity bounds, and finite-sample metrics; controlled synthetic experiments to study feature informativeness impact; empirical evaluation on real-world datasets.

Result: Derived theoretical guarantees for calibration methods; demonstrated consistent calibration improvements across scenarios; showed calibration performance varies with feature informativeness (better with informative features vs. noisy features).

Conclusion: Provides fundamental insights into calibration robustness and practical guidelines for method selection based on dataset characteristics and computational constraints, bridging theory and practice in uncertainty quantification.

Abstract: Post-hoc calibration methods are widely used to improve the reliability of
probabilistic predictions from machine learning models. Despite their
prevalence, a comprehensive theoretical understanding of these methods remains
elusive, particularly regarding their performance across different datasets and
model architectures. Input features play a crucial role in shaping model
predictions and, consequently, their calibration. However, the interplay
between feature quality and calibration performance has not been thoroughly
investigated. In this work, we present a rigorous theoretical analysis of
post-hoc calibration methods, focusing on Platt scaling and isotonic
regression. We derive convergence guarantees, computational complexity bounds,
and finite-sample performance metrics for these methods. Furthermore, we
explore the impact of feature informativeness on calibration performance
through controlled synthetic experiments. Our empirical evaluation spans a
diverse set of real-world datasets and model architectures, demonstrating
consistent improvements in calibration metrics across various scenarios. By
examining calibration performance under varying feature conditions utilizing
only informative features versus complete feature spaces including noise
dimensions, we provide fundamental insights into the robustness and reliability
of different calibration approaches. Our findings offer practical guidelines
for selecting appropriate calibration methods based on dataset characteristics
and computational constraints, bridging the gap between theoretical
understanding and practical implementation in uncertainty quantification. Code
and experimental data are available at:
https://github.com/Ajwebdevs/calibration-analysis-experiments.

</details>


### [505] [Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and Reliability](https://arxiv.org/abs/2509.23666)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: UAT proposes an adaptive threshold adjustment method for early-exit DNNs using Multi-Armed Bandit framework to improve robustness against distribution shifts and prevent overconfident wrong predictions.


<details>
  <summary>Details</summary>
Motivation: Traditional early-exit strategies use static thresholds that can lead to overconfidence in wrong classes and lack robustness to distribution shifts, undermining model trustworthiness.

Method: UAT adapts exit thresholds online using Multi-Armed Bandit framework with a reward function that balances predictive certainty, reliability, computational efficiency, and penalizes unnecessary late exits.

Result: UAT achieves 1.70-2.10x speedup with minimal performance drop (<2%) compared to full model performance across vision-language understanding, text generation, and classification tasks.

Conclusion: The proposed UAT framework provides guaranteed risk bounds and demonstrates consistent improvements in computational efficiency while maintaining prediction quality across diverse tasks.

Abstract: Early-Exit Deep Neural Networks enable adaptive inference by allowing
prediction at intermediary layers, significantly reducing computational costs
and latency. Most of the early exit strategies greedily exit a sample at an
intermediary layer if the confidence in class prediction exceeds a predefined
threshold that is set using a static validation set. This is problematic as the
model might be overconfident in a wrong class. Also, they are not robust to
distribution shifts encountered in deployment, which can undermine model
trustworthiness and accuracy. To address these challenges, we propose UAT that
adapts the threshold for exit decisions using a Multi-Armed Bandit framework,
enabling online, unsupervised adjustment of exit decisions. UAT makes decisions
based on a new reward function that assesses predictive certainty and its
reliability to balance computational efficiency and prediction quality while
penalizing unnecessary late exits. We provide guarantees on risk achieved by
UAT and validate its performance on diverse tasks spanning vision-language
understanding, text generation, and classification. Our framework demonstrates
consistent improvements in speedup (1.70-2.10x) with a minimal performance drop
(<2%) as compared to full model performance. Our source code is available at
https://github.com/Div290/UAT.

</details>


### [506] [Why Alignment Must Precede Distillation: A Minimal Working Explanation](https://arxiv.org/abs/2509.23667)
*Sungmin Cha,Kyunghyun Cho*

Main category: cs.LG

TL;DR: The paper argues that the common practice of distilling knowledge before alignment (KD -> Align) limits model performance, particularly for rare desirable behaviors. Instead, alignment should be performed first on high-recall reference models before distillation (Align -> KD), which yields superior results.


<details>
  <summary>Details</summary>
Motivation: Current preference alignment practices on compact, knowledge-distilled models overlook the importance of the reference model's distributional recall, leading to suboptimal alignment of rare desirable behaviors even with strong preference signals.

Method: The authors propose reversing the standard pipeline to Align -> KD. They provide theoretical explanation, validate with controllable Mixture-of-Gaussians experiments, and demonstrate on LLM alignment with SmolLM2 family, comparing both workflows.

Result: Models aligned after KD fail to effectively align target behaviors with substantially lower reward and target precision. The proposed Align -> KD pipeline robustly aligns these behaviors, yielding superior target-oriented metrics and lower variance.

Conclusion: Reference-model recall is a first-order design choice in alignment, establishing the principle that alignment must precede distillation for optimal performance, especially for rare desirable behaviors.

Abstract: For efficiency, preference alignment is often performed on compact,
knowledge-distilled (KD) models. We argue this common practice introduces a
significant limitation by overlooking a key property of the alignment's
reference model: its distributional recall. We show that the standard KD ->
Align workflow diminishes the model's capacity to align rare yet desirable
behaviors, even under strong preference signals. We instead demonstrate that
reversing the pipeline (i.e., Align -> KD) is essential: alignment must first
be performed on a high-recall reference before distillation. Our contributions
are threefold. First, we provide a minimal working explanation of how the
reference model constrains preference alignment objectives at a fundamental
level. Second, we validate this theory in a controllable Mixture-of-Gaussians
experiment, where low-recall anchoring consistently results in suboptimal model
performance. Finally, we demonstrate that the same phenomenon holds in LLM
alignment with the SmolLM2 family: models aligned after KD fail to effectively
align target behaviors, resulting in substantially lower reward and target
precision. In contrast, our proposed Align -> KD pipeline robustly aligns these
behaviors, yielding models with superior target-oriented metrics and lower
variance. Together, these results establish reference-model recall as a
first-order design choice in alignment, offering a clear principle: alignment
must precede distillation.

</details>


### [507] [Multi-Scale Spatial-Temporal Hypergraph Network with Lead-Lag Structures for Stock Time Series Forecasting](https://arxiv.org/abs/2509.23668)
*Xiangfei Qiu,Liu Yang,Hanyin Cheng,Xingjian Wu,Rongjia Wu,Zhigang Zhang,Ding Tu,Chenjuan Guo,Bin Yang,Christian S. Jensen,Jilin Hu*

Main category: cs.LG

TL;DR: Hermes is a hypergraph-based framework for stock time series forecasting that better captures industry correlations by modeling lead-lag relationships and multi-scale information through moving aggregation and cross-scale fusion modules.


<details>
  <summary>Details</summary>
Motivation: Stock time series exhibit industry correlations that can improve forecasting accuracy, but existing hypergraph methods capture these correlations superficially without fully considering inter-industry lead-lag interactions and multi-scale information.

Method: Hermes integrates moving aggregation and multi-scale fusion modules in a hypergraph network. It uses hyperedge-based moving aggregation with sliding windows and dynamic temporal aggregation to capture lead-lag relationships, and employs cross-scale edge-to-edge message passing to integrate multi-scale information while maintaining scale consistency.

Result: Experimental results on multiple real-world stock datasets show that Hermes outperforms existing state-of-the-art methods in both efficiency and accuracy.

Conclusion: The Hermes framework successfully addresses limitations in existing methods by better exploiting industry correlations through lead-lag relationship modeling and multi-scale information integration, achieving superior forecasting performance.

Abstract: Time series forecasting occurs in a range of financial applications providing
essential decision-making support to investors, regulatory institutions, and
analysts. Unlike multivariate time series from other domains, stock time series
exhibit industry correlation. Exploiting this kind of correlation can improve
forecasting accuracy. However, existing methods based on hypergraphs can only
capture industry correlation relatively superficially. These methods face two
key limitations: they do not fully consider inter-industry lead-lag
interactions, and they do not model multi-scale information within and among
industries. This study proposes the Hermes framework for stock time series
forecasting that aims to improve the exploitation of industry correlation by
eliminating these limitations. The framework integrates moving aggregation and
multi-scale fusion modules in a hypergraph network. Specifically, to more
flexibly capture the lead-lag relationships among industries, Hermes proposes a
hyperedge-based moving aggregation module. This module incorporates a sliding
window and utilizes dynamic temporal aggregation operations to consider
lead-lag dependencies among industries. Additionally, to effectively model
multi-scale information, Hermes employs cross-scale, edge-to-edge message
passing to integrate information from different scales while maintaining the
consistency of each scale. Experimental results on multiple real-world stock
datasets show that Hermes outperforms existing state-of-the-art methods in both
efficiency and accuracy.

</details>


### [508] [Graph Neural Networks with Diversity-aware Neighbor Selection and Dynamic Multi-scale Fusion for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.23671)
*Jingqi Xu,Guibin Chen,Jingxi Lu,Yuzhang Lin*

Main category: cs.LG

TL;DR: DIMIGNN is a GNN-based model for multivariate time series forecasting that addresses redundant neighbor information aggregation and single-scale temporal dependency issues through diversity-aware neighbor selection and dynamic multi-scale fusion.


<details>
  <summary>Details</summary>
Motivation: Existing GNN-based MTS forecasting methods often overlook neighbor diversity, leading to redundant information aggregation, and rely solely on single temporal scale representations for final predictions.

Method: Proposes DIMIGNN with two key components: Diversity-aware Neighbor Selection Mechanism (DNSM) to ensure high informational similarity with neighbors while maintaining neighbor diversity, and Dynamic Multi-Scale Fusion Module (DMFM) to dynamically adjust contributions from different temporal scales.

Result: Extensive experiments on real-world datasets demonstrate that DIMIGNN consistently outperforms prior methods.

Conclusion: The proposed DIMIGNN effectively addresses limitations in existing GNN-based MTS forecasting by incorporating diversity-aware neighbor selection and dynamic multi-scale fusion, achieving superior performance.

Abstract: Recently, numerous deep models have been proposed to enhance the performance
of multivariate time series (MTS) forecasting. Among them, Graph Neural
Networks (GNNs)-based methods have shown great potential due to their
capability to explicitly model inter-variable dependencies. However, these
methods often overlook the diversity of information among neighbors, which may
lead to redundant information aggregation. In addition, their final prediction
typically relies solely on the representation from a single temporal scale. To
tackle these issues, we propose a Graph Neural Networks (GNNs) with
Diversity-aware Neighbor Selection and Dynamic Multi-scale Fusion (DIMIGNN).
DIMIGNN introduces a Diversity-aware Neighbor Selection Mechanism (DNSM) to
ensure that each variable shares high informational similarity with its
neighbors while maintaining diversity among neighbors themselves. Furthermore,
a Dynamic Multi-Scale Fusion Module (DMFM) is introduced to dynamically adjust
the contributions of prediction results from different temporal scales to the
final forecasting result. Extensive experiments on real-world datasets
demonstrate that DIMIGNN consistently outperforms prior methods.

</details>


### [509] [Towards a Comprehensive Scaling Law of Mixture-of-Experts](https://arxiv.org/abs/2509.23678)
*Guoliang Zhao,Yuhan Fu,Shuaipeng Li,Xingwu Sun,Ruobing Xie,An Wang,Weidong Han,Zhen Yang,Weixuan Sun,Yudong Zhang,Cheng-zhong Xu,Di Wang,Jie Jiang*

Main category: cs.LG

TL;DR: This paper establishes comprehensive scaling laws for Mixture-of-Experts (MoE) models by systematically analyzing five key factors (data size, total model size, activated model size, number of active experts, and shared expert ratio) through 446 controlled experiments.


<details>
  <summary>Details</summary>
Motivation: Existing scaling laws for dense models are inapplicable to MoE models due to multiple influencing factors, their intricate coupling relationships, and non-monotonic performance impacts, necessitating fine-grained investigation into MoE-specific scaling laws.

Method: Systematic decomposition of MoE settings into five key factors, conducting 446 controlled experiments to characterize marginal effects, and constructing a comprehensive joint MoE scaling law that considers all essential factors.

Result: Optimal settings for number of active experts (G) and shared expert ratio (S) are independent of model architecture and data size. With scaling of total model size (N), the optimal activation parameter ratio (Na/N) becomes sparser.

Conclusion: The proposed MoE scaling law provides accurate and insightful guidance for future MoE model design and training, with optimal configurations derived for key parameters.

Abstract: Mixture-of-Experts (MoE) models have become the consensus approach for
enabling parameter-efficient scaling and cost-effective deployment in large
language models. However, existing scaling laws for dense models are
inapplicable to MoE models, which stems from three critical challenges: the
multiplicity of influencing factors, their intricate coupling relationships and
the non-monotonic nature of their performance impacts. They collectively
necessitate a fine-grained investigation into MoE-specific scaling laws. In
this work, we perform a systematic decomposition of MoE settings, identifying
five key factors that influence model performance from both size and structural
perspectives (data size ($D$), total model size ($N$), activated model size
($N_a$), number of active experts ($G$) and the ratio of shared experts ($S$)).
Specifically, we design $446$ controlled experiments to characterize their
marginal effects, ultimately constructing a comprehensive and precise joint MoE
scaling law that considers all essential factors. Furthermore, we derive the
theoretically optimal and practically efficiency-aware optimal configurations
for $G$, $S$ and $N_a/N$ with detailed analyses. Our results demonstrate that
the optimal settings for $G$ and $S$ are independent of both the model
architecture and data size. With the scaling of $N$, the optimal activation
parameter ratio of $N_a/N$ becomes sparser. Our proposed MoE scaling law could
function as an accurate and insightful guidance to facilitate future MoE model
design and training.

</details>


### [510] [Decentralized Dynamic Cooperation of Personalized Models for Federated Continual Learning](https://arxiv.org/abs/2509.23683)
*Danni Yang,Zhikang Chen,Sen Cui,Mengyue Yang,Ding Li,Abudukelimu Wuerkaixi,Haoxuan Li,Jinke Ren,Mingming Gong*

Main category: cs.LG

TL;DR: A decentralized dynamic cooperation framework for federated continual learning that forms non-overlapping coalitions to address catastrophic forgetting and client heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Existing federated continual learning approaches suffer from catastrophic forgetting due to temporal and cross-client shifts, and global model aggregation introduces irrelevant knowledge interference in heterogeneous scenarios.

Method: Proposes dynamic cooperative learning coalitions where clients selectively ally based on performance gains, using coalitional affinity game to assess gradient coherence and model similarity, with merge-blocking and dynamic cooperative evolution algorithms.

Result: Comprehensive experiments demonstrate superiority over various baselines in handling catastrophic forgetting and client heterogeneity.

Conclusion: The decentralized dynamic cooperation framework effectively balances new knowledge acquisition with prior learning retention, achieving personalized models through selective client cooperation.

Abstract: Federated continual learning (FCL) has garnered increasing attention for its
ability to support distributed computation in environments with evolving data
distributions. However, the emergence of new tasks introduces both temporal and
cross-client shifts, making catastrophic forgetting a critical challenge. Most
existing works aggregate knowledge from clients into a global model, which may
not enhance client performance since irrelevant knowledge could introduce
interference, especially in heterogeneous scenarios. Additionally, directly
applying decentralized approaches to FCL suffers from ineffective group
formation caused by task changes. To address these challenges, we propose a
decentralized dynamic cooperation framework for FCL, where clients establish
dynamic cooperative learning coalitions to balance the acquisition of new
knowledge and the retention of prior learning, thereby obtaining personalized
models. To maximize model performance, each client engages in selective
cooperation, dynamically allying with others who offer meaningful performance
gains. This results in non-overlapping, variable coalitions at each stage of
the task. Moreover, we use coalitional affinity game to simulate coalition
relationships between clients. By assessing both client gradient coherence and
model similarity, we quantify the client benefits derived from cooperation. We
also propose a merge-blocking algorithm and a dynamic cooperative evolution
algorithm to achieve cooperative and dynamic equilibrium. Comprehensive
experiments demonstrate the superiority of our method compared to various
baselines. Code is available at: https://github.com/ydn3229/DCFCL.

</details>


### [511] [Hedonic Neurons: A Mechanistic Mapping of Latent Coalitions in Transformer MLPs](https://arxiv.org/abs/2509.23684)
*Tanya Chowdhury,Atharva Nijasure,Yair Zick,James Allan*

Main category: cs.LG

TL;DR: The paper introduces a game theory-based framework to analyze how neurons cooperate in MLP layers of fine-tuned LLMs, revealing stable coalitions with synergistic effects that track feature evolution across layers.


<details>
  <summary>Details</summary>
Motivation: To understand how neurons in MLP layers of fine-tuned LLMs work together rather than in isolation, as prior work shows features may strengthen, split, or vanish across depth but the scale obscures meaningful structure.

Method: A mechanistic interpretability framework using coalitional game theory, where neurons are agents in hedonic games with top-responsive utilities, applying PAC-Top-Cover algorithm to extract stable neuron coalitions and track their transitions across layers.

Result: Applied to LLaMA, Mistral, and Pythia rerankers, the method finds coalitions with consistently higher synergy than clustering baselines, revealing higher-order structure and functionally important computational units.

Conclusion: Hedonic coalitions uncover how neurons cooperate to encode features, providing interpretable and predictive computational units that go beyond disentanglement approaches.

Abstract: Fine-tuned Large Language Models (LLMs) encode rich task-specific features,
but the form of these representations, especially within MLP layers, remains
unclear. Empirical inspection of LoRA updates shows that new features
concentrate in mid-layer MLPs, yet the scale of these layers obscures
meaningful structure. Prior probing suggests that statistical priors may
strengthen, split, or vanish across depth, motivating the need to study how
neurons work together rather than in isolation.
  We introduce a mechanistic interpretability framework based on coalitional
game theory, where neurons mimic agents in a hedonic game whose preferences
capture their synergistic contributions to layer-local computations. Using
top-responsive utilities and the PAC-Top-Cover algorithm, we extract stable
coalitions of neurons: groups whose joint ablation has non-additive effects. We
then track their transitions across layers as persistence, splitting, merging,
or disappearance.
  Applied to LLaMA, Mistral, and Pythia rerankers fine-tuned on scalar IR
tasks, our method finds coalitions with consistently higher synergy than
clustering baselines. By revealing how neurons cooperate to encode features,
hedonic coalitions uncover higher-order structure beyond disentanglement and
yield computational units that are functionally important, interpretable, and
predictive across domains.

</details>


### [512] [FedDAPL: Toward Client-Private Generalization in Federated Learning](https://arxiv.org/abs/2509.23688)
*Soroosh Safari Loaliyan,Jose-Luis Ambite,Paul M. Thompson,Neda Jahanshad,Greg Ver Steeg*

Main category: cs.LG

TL;DR: This paper integrates Domain-Adversarial Neural Network (DANN) into Federated Learning to address scanner-induced domain shift in medical imaging while preserving data privacy, using proximal regularization to stabilize training.


<details>
  <summary>Details</summary>
Motivation: Federated Learning is ideal for medical imaging due to privacy constraints, but scanner-induced domain shift causes models to fail on external sites. Existing harmonization methods violate privacy by requiring data comparison across sites.

Method: Proposed a federated DANN approach with proximal regularization to stabilize adversarial training among clients, enabling domain-invariant learning without sharing raw data.

Result: Experiments on T1-weighted 3-D brain MRIs from OpenBHB dataset showed superior cross-site generalization over FedAvg and ERM when training on 15 sites and testing on 19 unseen sites.

Conclusion: The proposed federated DANN with proximal regularization effectively addresses domain shift in medical imaging while maintaining strict data privacy constraints of Federated Learning.

Abstract: Federated Learning (FL) trains models locally at each research center or
clinic and aggregates only model updates, making it a natural fit for medical
imaging, where strict privacy laws forbid raw data sharing. A major obstacle is
scanner-induced domain shift: non-biological variations in hardware or
acquisition protocols can cause models to fail on external sites. Most
harmonization methods correct this shift by directly comparing data across
sites, conflicting with FL's privacy constraints. Domain Generalization (DG)
offers a privacy-friendly alternative - learning site-invariant representations
without sharing raw data - but standard DG pipelines still assume centralized
access to multi-site data, again violating FL's guarantees. This paper meets
these difficulties with a straightforward integration of a Domain-Adversarial
Neural Network (DANN) within the FL process. After demonstrating that a naive
federated DANN fails to converge, we propose a proximal regularization method
that stabilizes adversarial training among clients. Experiments on T1-weighted
3-D brain MRIs from the OpenBHB dataset, performing brain-age prediction on
participants aged 6-64 y (mean 22+/-6 y; 45 percent male) in training and 6-79
y (mean 19+/-13 y; 55 percent male) in validation, show that training on 15
sites and testing on 19 unseen sites yields superior cross-site generalization
over FedAvg and ERM while preserving data privacy.

</details>


### [513] [Merge Now, Regret Later: The Hidden Cost of Model Merging is Adversarial Transferability](https://arxiv.org/abs/2509.23689)
*Ankit Gangwal,Aaryan Ajay Sharma*

Main category: cs.LG

TL;DR: Model Merging does not provide reliable defense against transfer attacks, with over 95% relative transfer attack success rate, challenging the notion of free adversarial robustness.


<details>
  <summary>Details</summary>
Motivation: To study the impact of Model Merging on the transferability of adversarial examples, as previous works focused on backdoor attacks but insufficiently explored transfer attacks.

Method: Comprehensive evaluations with 8 MM methods, 7 datasets, and 6 attack methods across 336 distinct attack settings, including statistical analysis.

Result: MM cannot reliably defend against transfer attacks; stronger MM methods increase vulnerability; mitigating representation bias increases vulnerability; weight averaging is the most vulnerable method.

Conclusion: MM does not confer free adversarial robustness and actually increases vulnerability to transfer attacks, requiring careful consideration in secure system design.

Abstract: Model Merging (MM) has emerged as a promising alternative to multi-task
learning, where multiple fine-tuned models are combined, without access to
tasks' training data, into a single model that maintains performance across
tasks. Recent works have explored the impact of MM on adversarial attacks,
particularly backdoor attacks. However, none of them have sufficiently explored
its impact on transfer attacks using adversarial examples, i.e., a black-box
adversarial attack where examples generated for a surrogate model successfully
mislead a target model.
  In this work, we study the effect of MM on the transferability of adversarial
examples. We perform comprehensive evaluations and statistical analysis
consisting of 8 MM methods, 7 datasets, and 6 attack methods, sweeping over 336
distinct attack settings. Through it, we first challenge the prevailing notion
of MM conferring free adversarial robustness, and show MM cannot reliably
defend against transfer attacks, with over 95% relative transfer attack success
rate. Moreover, we reveal 3 key insights for machine-learning practitioners
regarding MM and transferability for a robust system design: (1) stronger MM
methods increase vulnerability to transfer attacks; (2) mitigating
representation bias increases vulnerability to transfer attacks; and (3) weight
averaging, despite being the weakest MM method, is the most vulnerable MM
method to transfer attacks. Finally, we analyze the underlying reasons for this
increased vulnerability, and provide potential solutions to the problem. Our
findings offer critical insights for designing more secure systems employing
MM.

</details>


### [514] [Estimating Time Series Foundation Model Transferability via In-Context Learning](https://arxiv.org/abs/2509.23695)
*Qingren Yao,Ming Jin,Chengqi Zhang,Chao-Han Huck Yang,Jun Qi,Shirui Pan*

Main category: cs.LG

TL;DR: TimeTic is a transferability estimation framework that uses in-context learning to predict TSFM performance after fine-tuning on target datasets, achieving 30% improvement over zero-shot baselines.


<details>
  <summary>Details</summary>
Motivation: With growing number of time series foundation models, efficiently identifying the best model for downstream fine-tuning becomes challenging, especially in domains with limited public data.

Method: Recasts model selection as in-context-learning problem using tabular foundation models, organizes model-data relationships as contextual information, and introduces entropy-based model characterization across layers.

Result: Achieves mean rank correlation of ~0.6 with actual fine-tuned performance, 30% improvement over using zero-shot performance as transferability score on benchmark with 10 datasets, 10 models, and 3 tasks.

Conclusion: TimeTic provides effective transferability estimation for time series foundation models, enabling better model selection for fine-tuning across diverse datasets and tasks.

Abstract: Time series foundation models (TSFMs) offer strong zero-shot forecasting via
large-scale pre-training, yet fine-tuning remains critical for boosting
performance in domains with limited public data. With the growing number of
TSFMs, efficiently identifying the best model for downstream fine-tuning
becomes increasingly challenging. In this work, we introduce TimeTic, a
transferability estimation framework that recasts model selection as an
in-context-learning problem: given observations on known (source) datasets, it
predicts how a TSFM will perform after fine-tuning on a downstream (target)
dataset. TimeTic flexibly organizes the observed model-data relationships as
contextual information, allowing it to adapt seamlessly to various test-time
scenarios. Leveraging the natural tabular structure formed by dataset
meta-features, model characteristics, and fine-tuned performance, we employ
tabular foundation models to serve as in-context learners. We further introduce
a novel model characterization based on entropy evolution across model layers,
capturing embedding-space distinctions and enabling TimeTic to generalize
across arbitrary model sets. We establish a comprehensive benchmark for
transferability estimation including 10 datasets, 10 foundation models, and 3
forecasting tasks. On this benchmark, TimeTic's estimation demonstrates strong
alignment with actual fine-tuned performance for previously unseen datasets,
achieving a mean rank correlation of approximately 0.6 and a 30% improvement
compared to using zero-shot performance as the transferability score.

</details>


### [515] [Bridging Discrete and Continuous RL: Stable Deterministic Policy Gradient with Martingale Characterization](https://arxiv.org/abs/2509.23711)
*Ziheng Cheng,Xin Guo,Yufei Zhang*

Main category: cs.LG

TL;DR: This paper proposes CT-DDPG, a continuous-time deterministic policy gradient algorithm that addresses sensitivity to time discretization in RL, offering improved stability and faster convergence.


<details>
  <summary>Details</summary>
Motivation: Many real-world RL applications are continuous, but discrete-time algorithms are sensitive to time discretization, leading to poor stability and slow convergence in continuous environments.

Method: Derived continuous-time policy gradient formula based on advantage function analogue, established martingale characterization, and developed CT-DDPG algorithm for deterministic policies in continuous-time settings.

Result: Numerical experiments show CT-DDPG provides improved stability and faster convergence compared to existing discrete-time and continuous-time methods across various control tasks with different time discretizations and noise levels.

Conclusion: The proposed CT-DDPG algorithm successfully enables stable learning with deterministic policies in continuous-time environments, overcoming limitations of time discretization sensitivity in traditional RL methods.

Abstract: The theory of discrete-time reinforcement learning (RL) has advanced rapidly
over the past decades. Although primarily designed for discrete environments,
many real-world RL applications are inherently continuous and complex. A major
challenge in extending discrete-time algorithms to continuous-time settings is
their sensitivity to time discretization, often leading to poor stability and
slow convergence. In this paper, we investigate deterministic policy gradient
methods for continuous-time RL. We derive a continuous-time policy gradient
formula based on an analogue of the advantage function and establish its
martingale characterization. This theoretical foundation leads to our proposed
algorithm, CT-DDPG, which enables stable learning with deterministic policies
in continuous-time environments. Numerical experiments show that the proposed
CT-DDPG algorithm offers improved stability and faster convergence compared to
existing discrete-time and continuous-time methods, across a wide range of
control tasks with varying time discretizations and noise levels.

</details>


### [516] [FraudTransformer: Time-Aware GPT for Transaction Fraud Detection](https://arxiv.org/abs/2509.23712)
*Gholamali Aminian,Andrew Elliott,Tiger Li,Timothy Cheuk Hin Wong,Victor Claude Dehon,Lukasz Szpruch,Carsten Maple,Christopher Read,Martin Brown,Gesine Reinert,Mo Mamouei*

Main category: cs.LG

TL;DR: FraudTransformer is a sequence model for payment fraud detection that enhances GPT-style architecture with time encoding and learned positional encoding, outperforming classical baselines and transformer ablations.


<details>
  <summary>Details</summary>
Motivation: Real-world banking fraud detection requires models that can utilize both event order and irregular time gaps between transactions.

Method: Augments GPT-style architecture with dedicated time encoder for timestamps/inter-event values and learned positional encoder for relative order preservation.

Result: Outperforms four classical baselines (Logistic Regression, XGBoost, LightGBM) and transformer ablations on large industrial dataset with tens of millions of transactions.

Conclusion: FraudTransformer achieves highest AUROC and PRAUC on held-out test set, demonstrating effectiveness for payment fraud detection.

Abstract: Detecting payment fraud in real-world banking streams requires models that
can exploit both the order of events and the irregular time gaps between them.
We introduce FraudTransformer, a sequence model that augments a vanilla
GPT-style architecture with (i) a dedicated time encoder that embeds either
absolute timestamps or inter-event values, and (ii) a learned positional
encoder that preserves relative order. Experiments on a large industrial
dataset -- tens of millions of transactions and auxiliary events -- show that
FraudTransformer surpasses four strong classical baselines (Logistic
Regression, XGBoost and LightGBM) as well as transformer ablations that omit
either the time or positional component. On the held-out test set it delivers
the highest AUROC and PRAUC.

</details>


### [517] [A Self-Adaptive Frequency Domain Network for Continuous Intraoperative Hypotension Prediction](https://arxiv.org/abs/2509.23720)
*Xian Zeng,Tianze Xu,Kai Yang,Jie Sun,Youran Wang,Jun Xu,Mucheng Ren*

Main category: cs.LG

TL;DR: SAFDNet is a novel AI model for early prediction of intraoperative hypotension that integrates adaptive spectral analysis and interactive attention to handle frequency domain information and temporal dependencies while being robust to noise.


<details>
  <summary>Details</summary>
Motivation: Existing AI models for intraoperative hypotension prediction have limitations in incorporating time and frequency domain information, capturing dependencies, and handling noise sensitivity in biosignal data.

Method: Proposed SAFDNet with adaptive spectral block using Fourier analysis and self-adaptive thresholding for noise mitigation, plus interactive attention block for capturing long-term and short-term dependencies.

Result: Achieves up to 97.3% AUROC in IOH early warning, outperforming state-of-the-art models, with robust predictive performance and low sensitivity to noise.

Conclusion: SAFDNet is well-suited for practical clinical applications due to its superior performance and noise robustness in intraoperative hypotension prediction.

Abstract: Intraoperative hypotension (IOH) is strongly associated with postoperative
complications, including postoperative delirium and increased mortality, making
its early prediction crucial in perioperative care. While several artificial
intelligence-based models have been developed to provide IOH warnings, existing
methods face limitations in incorporating both time and frequency domain
information, capturing short- and long-term dependencies, and handling noise
sensitivity in biosignal data. To address these challenges, we propose a novel
Self-Adaptive Frequency Domain Network (SAFDNet). Specifically, SAFDNet
integrates an adaptive spectral block, which leverages Fourier analysis to
extract frequency-domain features and employs self-adaptive thresholding to
mitigate noise. Additionally, an interactive attention block is introduced to
capture both long-term and short-term dependencies in the data. Extensive
internal and external validations on two large-scale real-world datasets
demonstrate that SAFDNet achieves up to 97.3\% AUROC in IOH early warning,
outperforming state-of-the-art models. Furthermore, SAFDNet exhibits robust
predictive performance and low sensitivity to noise, making it well-suited for
practical clinical applications.

</details>


### [518] [GBSK: Skeleton Clustering via Granular-ball Computing and Multi-Sampling for Large-Scale Data](https://arxiv.org/abs/2509.23742)
*Yewang Chen,Junfeng Li,Shuyin Xia,Qinghong Lai,Xinbo Gao,Guoyin Wang,Dongdong Cheng,Yi Liu,Yi Wang*

Main category: cs.LG

TL;DR: GBSK is a scalable skeleton clustering algorithm using granular-ball technique to efficiently handle large datasets by constructing multi-grained granular-balls that approximate data structure, reducing computational cost while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of clustering large-scale datasets efficiently while maintaining accuracy.

Method: Uses granular-ball technique with multi-sampling to construct multi-grained granular-balls that progressively uncover a statistical "skeleton" approximating the essential data structure and distribution.

Result: GBSK achieves high efficiency and strong clustering performance on large-scale datasets, including one with 100 million instances across 256 dimensions, with dramatically reduced computational overhead.

Conclusion: GBSK and its adaptive version AGBSK provide effective solutions for scalable clustering of large datasets with simplified parameter settings for real-world deployment.

Abstract: To effectively handle clustering task for large-scale datasets, we propose a
novel scalable skeleton clustering algorithm, namely GBSK, which leverages the
granular-ball technique to capture the underlying structure of data. By
multi-sampling the dataset and constructing multi-grained granular-balls, GBSK
progressively uncovers a statistical "skeleton" -- a spatial abstraction that
approximates the essential structure and distribution of the original data.
This strategy enables GBSK to dramatically reduce computational overhead while
maintaining high clustering accuracy. In addition, we introduce an adaptive
version, AGBSK, with simplified parameter settings to enhance usability and
facilitate deployment in real-world scenarios. Extensive experiments conducted
on standard computing hardware demonstrate that GBSK achieves high efficiency
and strong clustering performance on large-scale datasets, including one with
up to 100 million instances across 256 dimensions. Our implementation and
experimental results are available at: https://github.com/XFastDataLab/GBSK/.

</details>


### [519] [Time-Shifted Token Scheduling for Symbolic Music Generation](https://arxiv.org/abs/2509.23749)
*Ting-Kang Wang,Chih-Pin Tan,Yi-Hsuan Yang*

Main category: cs.LG

TL;DR: The paper proposes a delay-based scheduling mechanism (DP) to address the trade-off between efficiency and quality in symbolic music generation, improving performance over standard compound tokenizations without adding parameters.


<details>
  <summary>Details</summary>
Motivation: Symbolic music generation faces a fundamental trade-off: fine-grained tokenizations achieve strong coherence but have high complexity, while compact tokenizations improve efficiency but suffer from intra-token dependencies.

Method: Adapt a delay-based scheduling mechanism (DP) that expands compound-like tokens across decoding steps, enabling autoregressive modeling of intra-token dependencies while preserving efficiency. DP is lightweight with no additional parameters and can be integrated into existing representations.

Result: Experiments on symbolic orchestral MIDI datasets show that the method improves all metrics over standard compound tokenizations and narrows the gap to fine-grained tokenizations.

Conclusion: The delay-based scheduling mechanism effectively balances efficiency and quality in symbolic music generation, providing a practical solution to the fundamental trade-off without requiring additional parameters.

Abstract: Symbolic music generation faces a fundamental trade-off between efficiency
and quality. Fine-grained tokenizations achieve strong coherence but incur long
sequences and high complexity, while compact tokenizations improve efficiency
at the expense of intra-token dependencies. To address this, we adapt a
delay-based scheduling mechanism (DP) that expands compound-like tokens across
decoding steps, enabling autoregressive modeling of intra-token dependencies
while preserving efficiency. Notably, DP is a lightweight strategy that
introduces no additional parameters and can be seamlessly integrated into
existing representations. Experiments on symbolic orchestral MIDI datasets show
that our method improves all metrics over standard compound tokenizations and
narrows the gap to fine-grained tokenizations.

</details>


### [520] [An Investigation of Batch Normalization in Off-Policy Actor-Critic Algorithms](https://arxiv.org/abs/2509.23750)
*Li Wang,Sudun,Xingjian Zhang,Wenjun Wu,Lei Huang*

Main category: cs.LG

TL;DR: Batch Normalization (BN) can be effectively used in deep reinforcement learning despite non-i.i.d. data challenges, with proposed MA-BN method improving training stability and performance.


<details>
  <summary>Details</summary>
Motivation: BN has been underutilized in DRL due to non-i.i.d. data and shifting distributions, but it retains unique advantages like stochasticity and training ease that could benefit DRL.

Method: Conducted empirical study on BN in off-policy actor-critic algorithms, analyzed training/evaluation modes, identified failure modes, and proposed Mode-Aware Batch Normalization (MA-BN) with practical recommendations.

Result: MA-BN accelerates and stabilizes training, broadens effective learning rate range, enhances exploration, and reduces optimization difficulty in RL settings.

Conclusion: BN can be successfully integrated into DRL pipelines when applied appropriately, with MA-BN providing robust solutions to overcome instability issues and leverage BN's benefits.

Abstract: Batch Normalization (BN) has played a pivotal role in the success of deep
learning by improving training stability, mitigating overfitting, and enabling
more effective optimization. However, its adoption in deep reinforcement
learning (DRL) has been limited due to the inherent non-i.i.d. nature of data
and the dynamically shifting distributions induced by the agent's learning
process. In this paper, we argue that, despite these challenges, BN retains
unique advantages in DRL settings, particularly through its stochasticity and
its ability to ease training. When applied appropriately, BN can adapt to
evolving data distributions and enhance both convergence speed and final
performance. To this end, we conduct a comprehensive empirical study on the use
of BN in off-policy actor-critic algorithms, systematically analyzing how
different training and evaluation modes impact performance. We further identify
failure modes that lead to instability or divergence, analyze their underlying
causes, and propose the Mode-Aware Batch Normalization (MA-BN) method with
practical actionable recommendations for robust BN integration in DRL
pipelines. We also empirically validate that, in RL settings, MA-BN accelerates
and stabilizes training, broadens the effective learning rate range, enhances
exploration, and reduces overall optimization difficulty. Our code is available
at: https://github.com/monster476/ma-bn.git.

</details>


### [521] [Anchored Supervised Fine-Tuning](https://arxiv.org/abs/2509.23753)
*He Zhu,Junyou Su,Peng Lai,Ren Ma,Wenjia Zhang,Linyi Yang,Guanhua Chen*

Main category: cs.LG

TL;DR: DFT improves over SFT but suffers from instability due to distributional drift. ASFT adds KL regularization to DFT, achieving better performance and stability across multiple domains with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between SFT (efficient but memorizes) and RL (generalizes better but computationally expensive), and improve upon DFT's instability issues.

Method: Propose Anchored Supervised Fine-Tuning (ASFT) that augments DFT's reweighting with lightweight KL regularization to prevent distributional drift while maintaining tight RL bounds.

Result: ASFT consistently outperforms both SFT and DFT across mathematical reasoning, medical knowledge grounding, and code generation tasks with substantial improvements.

Conclusion: The RWR framework provides systematic understanding of post-training methods, showing that principled theoretical analysis leads to stronger guarantees and practical performance gains.

Abstract: Post-training of large language models involves a fundamental trade-off
between supervised fine-tuning (SFT), which efficiently mimics demonstrations
but tends to memorize, and reinforcement learning (RL), which achieves better
generalization at higher computational cost. Dynamic Fine-Tuning (DFT) recently
emerged as a promising middle ground, reweighting SFT objectives with token
probabilities and achieving improvements in certain reasoning domains, though
it exhibits instability in other tasks. We provide a analysis of DFT through
the reward-weighted regression (RWR) framework, revealing that it corresponds
to a specific auxiliary distribution choice that yields provably tighter RL
bounds than standard SFT. However, our analysis also uncovers a critical
limitation: this construction lacks distributional anchoring, leading to
progressive drift that undermines training stability. To address this, we
propose Anchored Supervised Fine-Tuning (ASFT), which augments DFT's
reweighting with lightweight KL regularization to preserve tightness while
ensuring stability. Empirically, ASFT consistently outperforms both SFT and DFT
across mathematical reasoning, medical knowledge grounding, and code
generation, achieving substantial improvements with minimal computational
overhead. Our RWR framework provides a systematic lens for understanding
post-training methods and demonstrates that principled theoretical analysis
leads to both stronger guarantees and practical gains.

</details>


### [522] [SHAPoint: Task-Agnostic, Efficient, and Interpretable Point-Based Risk Scoring via Shapley Values](https://arxiv.org/abs/2509.23756)
*Tomer D. Meirman,Bracha Shapira,Noa Dagan,Lior S. Rokach*

Main category: cs.LG

TL;DR: SHAPoint is a task-agnostic framework that combines gradient boosted trees' predictive power with interpretable point-based risk scores for clinical decision support, offering superior flexibility, faster runtime, and comparable performance to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for interpretable risk scores in clinical decision support rely on manual preprocessing, task-specific modeling, and simplified assumptions that limit flexibility and predictive power.

Method: SHAPoint integrates predictive accuracy of gradient boosted trees with interpretability of point-based risk scores, supporting classification, regression, and survival tasks while inheriting tree-based model properties like native missing data handling and monotonic constraint support.

Result: SHAPoint produces compact and interpretable scores with predictive performance comparable to state-of-the-art methods, but at a fraction of the runtime, offering superior flexibility and reduced reliance on manual preprocessing.

Conclusion: SHAPoint is a powerful tool for transparent and scalable risk stratification that bridges the gap between predictive accuracy and interpretability in clinical decision support.

Abstract: Interpretable risk scores play a vital role in clinical decision support, yet
traditional methods for deriving such scores often rely on manual
preprocessing, task-specific modeling, and simplified assumptions that limit
their flexibility and predictive power. We present SHAPoint, a novel,
task-agnostic framework that integrates the predictive accuracy of gradient
boosted trees with the interpretability of point-based risk scores. SHAPoint
supports classification, regression, and survival tasks, while also inheriting
valuable properties from tree-based models, such as native handling of missing
data and support for monotonic constraints. Compared to existing frameworks,
SHAPoint offers superior flexibility, reduced reliance on manual preprocessing,
and faster runtime performance. Empirical results show that SHAPoint produces
compact and interpretable scores with predictive performance comparable to
state-of-the-art methods, but at a fraction of the runtime, making it a
powerful tool for transparent and scalable risk stratification.

</details>


### [523] [Knowledge Homophily in Large Language Models](https://arxiv.org/abs/2509.23773)
*Utkarsh Sahu,Zhisheng Qi,Mahantesh Halappanavar,Nedim Lipka,Ryan A. Rossi,Franck Dernoncourt,Yu Zhang,Yao Ma,Yu Wang*

Main category: cs.LG

TL;DR: This paper investigates knowledge homophily patterns in LLMs, where related entities tend to have similar knowledge levels. The authors propose a GNN-based method to predict entity knowledgeability scores, enabling more efficient knowledge coverage and improved performance in question answering.


<details>
  <summary>Details</summary>
Motivation: To understand the structural organization of knowledge in LLMs and leverage cognitive neuroscience principles (semantic clustering and priming) to improve knowledge-intensive applications like question answering and fact checking.

Method: Map LLM knowledge into a graph representation through knowledge checking at triplet and entity levels, then use a GNN regression model to estimate entity-level knowledgeability scores based on neighborhood relationships.

Result: Discovered knowledge homophily pattern in LLMs, where entities closer in the graph tend to have similar knowledge levels. The proposed method enables prioritizing less well-known triplets for labeling, improving knowledge coverage efficiency.

Conclusion: The knowledge homophily principle in LLMs can be leveraged to optimize knowledge injection and improve reasoning performance, particularly in multi-hop question answering tasks.

Abstract: Large Language Models (LLMs) have been increasingly studied as neural
knowledge bases for supporting knowledge-intensive applications such as
question answering and fact checking. However, the structural organization of
their knowledge remains unexplored. Inspired by cognitive neuroscience
findings, such as semantic clustering and priming, where knowing one fact
increases the likelihood of recalling related facts, we investigate an
analogous knowledge homophily pattern in LLMs. To this end, we map LLM
knowledge into a graph representation through knowledge checking at both the
triplet and entity levels. After that, we analyze the knowledgeability
relationship between an entity and its neighbors, discovering that LLMs tend to
possess a similar level of knowledge about entities positioned closer in the
graph. Motivated by this homophily principle, we propose a Graph Neural Network
(GNN) regression model to estimate entity-level knowledgeability scores for
triplets by leveraging their neighborhood scores. The predicted
knowledgeability enables us to prioritize checking less well-known triplets,
thereby maximizing knowledge coverage under the same labeling budget. This not
only improves the efficiency of active labeling for fine-tuning to inject
knowledge into LLMs but also enhances multi-hop path retrieval in
reasoning-intensive question answering.

</details>


### [524] [Trained Mamba Emulates Online Gradient Descent in In-Context Linear Regression](https://arxiv.org/abs/2509.23779)
*Jiarui Jiang,Wei Huang,Miao Zhang,Taiji Suzuki,Liqiang Nie*

Main category: cs.LG

TL;DR: The paper provides theoretical analysis of Mamba's in-context learning capabilities on linear regression tasks, showing it achieves comparable performance to Transformers through online gradient descent.


<details>
  <summary>Details</summary>
Motivation: While Mamba shows competitive ICL capabilities with Transformers, theoretical understanding of its mechanisms remains limited, especially for fundamental tasks like linear regression ICL.

Method: Developed novel techniques for non-convex optimization with gradient descent related to Mamba's structure, analyzing training dynamics on linear regression ICL tasks.

Result: Established exponential convergence rate to ICL solution with loss bound comparable to Transformers, revealing Mamba performs online gradient descent to learn latent functions.

Conclusion: Mamba achieves ICL through online gradient descent mechanism, which differs from Transformers' gradient descent emulation, with theoretical results verified experimentally.

Abstract: State-space models (SSMs), particularly Mamba, emerge as an efficient
Transformer alternative with linear complexity for long-sequence modeling.
Recent empirical works demonstrate Mamba's in-context learning (ICL)
capabilities competitive with Transformers, a critical capacity for large
foundation models. However, theoretical understanding of Mamba's ICL remains
limited, restricting deeper insights into its underlying mechanisms. Even
fundamental tasks such as linear regression ICL, widely studied as a standard
theoretical benchmark for Transformers, have not been thoroughly analyzed in
the context of Mamba. To address this gap, we study the training dynamics of
Mamba on the linear regression ICL task. By developing novel techniques
tackling non-convex optimization with gradient descent related to Mamba's
structure, we establish an exponential convergence rate to ICL solution, and
derive a loss bound that is comparable to Transformer's. Importantly, our
results reveal that Mamba can perform a variant of \textit{online gradient
descent} to learn the latent function in context. This mechanism is different
from that of Transformer, which is typically understood to achieve ICL through
gradient descent emulation. The theoretical results are verified by
experimental simulation.

</details>


### [525] [Enhancing LLM Steering through Sparse Autoencoder-Based Vector Refinement](https://arxiv.org/abs/2509.23799)
*Anyi Wang,Xuansheng Wu,Dong Shu,Yunpu Ma,Ninghao Liu*

Main category: cs.LG

TL;DR: SAE-RSV refines steering vectors from limited data using sparse autoencoders to remove noise and enrich task-relevant features, outperforming baselines including supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing steering methods require large datasets, limiting real-world applicability. Small datasets produce noisy steering vectors with task-irrelevant features.

Method: Use sparse autoencoders to semantically denoise steering vectors by removing task-irrelevant features and enriching task-relevant features through semantic similarity.

Result: SAE-RSV substantially outperforms all baseline methods including supervised fine-tuning in extensive experiments.

Conclusion: Effective steering vectors can be constructed from limited data by refining original vectors through sparse autoencoders.

Abstract: Steering has emerged as a promising approach in controlling large language
models (LLMs) without modifying model parameters. However, most existing
steering methods rely on large-scale datasets to learn clear behavioral
information, which limits their applicability in many real-world scenarios. The
steering vectors extracted from small dataset often contain task-irrelevant
noising features, which degrades their effectiveness. To refine the steering
vectors learned from limited data, we introduce Refinement of Steering Vector
via Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise
and augment the steering vectors. In our framework, we first remove
task-irrelevant features according to their semantics provided by SAEs, and
then enrich task-relevant features missing from the small dataset through their
semantic similarity to the identified relevant features. Extensive experiments
demonstrate that the proposed SAE-RSV substantially outperforms all the
baseline methods including supervised fine-tuning. Our findings show that
effective steering vector can be constructed from limited training data by
refining the original steering vector through SAEs.

</details>


### [526] [STAIR: Addressing Stage Misalignment through Temporal-Aligned Preference Reinforcement Learning](https://arxiv.org/abs/2509.23802)
*Yao Luan,Ni Mu,Yiqin Yang,Bo Xu,Qing-Shan Jia*

Main category: cs.LG

TL;DR: STAIR addresses stage misalignment in PbRL for multi-stage tasks by learning stage approximations via temporal distance and prioritizing same-stage comparisons, improving reward learning and policy performance.


<details>
  <summary>Details</summary>
Motivation: Preference-based RL struggles with multi-stage tasks due to stage misalignment - comparing segments from different stages (e.g., navigation vs manipulation) provides uninformative feedback that hinders learning.

Method: STAIR learns stage approximations using temporal distance via contrastive learning, grouping temporally close states into coherent stages without predefined task knowledge, then prioritizes comparisons within the same stage.

Result: Extensive experiments show STAIR's superiority in multi-stage tasks and competitive performance in single-stage tasks. Human studies confirm stages approximated by STAIR align with human cognition.

Conclusion: STAIR effectively mitigates stage misalignment in PbRL for multi-stage tasks through temporal distance-based stage approximation and same-stage comparison prioritization.

Abstract: Preference-based reinforcement learning (PbRL) bypasses complex reward
engineering by learning rewards directly from human preferences, enabling
better alignment with human intentions. However, its effectiveness in
multi-stage tasks, where agents sequentially perform sub-tasks (e.g.,
navigation, grasping), is limited by stage misalignment: Comparing segments
from mismatched stages, such as movement versus manipulation, results in
uninformative feedback, thus hindering policy learning. In this paper, we
validate the stage misalignment issue through theoretical analysis and
empirical experiments. To address this issue, we propose STage-AlIgned Reward
learning (STAIR), which first learns a stage approximation based on temporal
distance, then prioritizes comparisons within the same stage. Temporal distance
is learned via contrastive learning, which groups temporally close states into
coherent stages, without predefined task knowledge, and adapts dynamically to
policy changes. Extensive experiments demonstrate STAIR's superiority in
multi-stage tasks and competitive performance in single-stage tasks.
Furthermore, human studies show that stages approximated by STAIR are
consistent with human cognition, confirming its effectiveness in mitigating
stage misalignment.

</details>


### [527] [FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents](https://arxiv.org/abs/2509.23803)
*Pramit Saha,Joshua Strong,Divyanshu Mishra,Cheng Ouyang,J. Alison Noble*

Main category: cs.LG

TL;DR: This paper introduces an agent-driven federated learning framework and FedAgentBench benchmark to address practical orchestration challenges in healthcare FL deployment, evaluating LLM agents' ability to autonomously coordinate complex FL workflows across diverse healthcare modalities.


<details>
  <summary>Details</summary>
Motivation: Real-world FL deployment in healthcare faces operational bottlenecks including client selection, coordination, data preprocessing, data harmonization, and algorithm selection, which demand substantial human efforts and are overlooked by existing FL works.

Method: Proposed an agent-driven FL framework capturing key workflow phases from client selection to training completion, incorporating 40 FL algorithms and evaluating 24 LLMs (14 open-source, 10 proprietary) across 201 datasets simulating 6 healthcare modalities.

Result: Some agent cores like GPT-4.1 and DeepSeek V3 can automate various FL pipeline stages, but complex interdependent tasks based on implicit goals remain challenging even for the strongest models.

Conclusion: Agent-driven FL systems show promise for autonomous coordination but face limitations in handling complex, implicit-goal tasks, highlighting the need for continued development in autonomous FL orchestration.

Abstract: Federated learning (FL) allows collaborative model training across healthcare
sites without sharing sensitive patient data. However, real-world FL deployment
is often hindered by complex operational challenges that demand substantial
human efforts. This includes: (a) selecting appropriate clients (hospitals),
(b) coordinating between the central server and clients, (c) client-level data
pre-processing, (d) harmonizing non-standardized data and labels across
clients, and (e) selecting FL algorithms based on user instructions and
cross-client data characteristics. However, the existing FL works overlook
these practical orchestration challenges. These operational bottlenecks
motivate the need for autonomous, agent-driven FL systems, where intelligent
agents at each hospital client and the central server agent collaboratively
manage FL setup and model training with minimal human intervention. To this
end, we first introduce an agent-driven FL framework that captures key phases
of real-world FL workflows from client selection to training completion and a
benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to
autonomously coordinate healthcare FL. Our framework incorporates 40 FL
algorithms, each tailored to address diverse task-specific requirements and
cross-client characteristics. Furthermore, we introduce a diverse set of
complex tasks across 201 carefully curated datasets, simulating 6
modality-specific real-world healthcare environments, viz., Dermatoscopy,
Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic
performance of 14 open-source and 10 proprietary LLMs spanning small, medium,
and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3
can automate various stages of the FL pipeline, our results reveal that more
complex, interdependent tasks based on implicit goals remain challenging for
even the strongest models.

</details>


### [528] [Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR](https://arxiv.org/abs/2509.23808)
*Fanding Huang,Guanbo Huang,Xiao Fan,Yi He,Xiao Liang,Xiao Chen,Qinting Jiang,Faisal Nadeem Khan,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: The paper challenges the traditional exploration-exploitation trade-off view in RLVR, showing it's an artifact of measurement level. By analyzing hidden-state space using Effective Rank derivatives, they discover exploration and exploitation can be decoupled, leading to VERL method that enhances both simultaneously.


<details>
  <summary>Details</summary>
Motivation: To re-examine the prevailing exploration-exploitation trade-off perspective in RLVR, which may be an artifact of token-level metrics rather than a fundamental constraint.

Method: Shift analysis to hidden-state space using Effective Rank (ER) and its derivatives ERV and ERA. Propose VERL method that operationalizes synergistic exploration-exploitation enhancement by shaping RL advantage function using ERA as predictive meta-controller.

Result: Experiments show consistent gains across diverse LLMs and reasoning benchmarks, including up to 21.4% absolute accuracy improvement on Gaokao 2024 dataset.

Conclusion: Exploration and exploitation can be decoupled at hidden-state level, enabling simultaneous enhancement through VERL's dual-channel incentive structure that prospectively amplifies rewards for both exploration and exploitation.

Abstract: A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)
interprets recent progress through the lens of an exploration-exploitation
trade-off, a perspective largely shaped by token-level metrics. We re-examine
this perspective, proposing that this perceived trade-off may not be a
fundamental constraint but rather an artifact of the measurement level. To
investigate this, we shift the analysis to the semantically rich hidden-state
space, adopting Effective Rank (ER) to quantify exploration and proposing its
novel first- and second-order derivatives, named Effective Rank Velocity (ERV)
and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our
analysis reveals that at the hidden-state level, exploration and exploitation
could be decoupled (Sec. 4). This finding reveals an opportunity to enhance
both capacities simultaneously. This insight motivates our method,
Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the
principle of synergistic exploration-exploitation enhancement by directly
shaping the RL advantage function. The key innovation is leveraging the
theoretically stable ERA as a predictive meta-controller to create a
synergistic, dual-channel incentive structure. Instead of forcing a trade-off,
VERL prospectively amplifies rewards for exploration to preempt overconfidence
and reinforces exploitative gains to consolidate reasoning. Experiments across
diverse LLMs and reasoning benchmarks show consistent gains, including up to
21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.

</details>


### [529] [Tequila: Trapping-free Ternary Quantization for Large Language Models](https://arxiv.org/abs/2509.23809)
*Hong Huang,Decheng Wu,Rui Cen,Guanghua Yu,Zonghang Li,Kai Liu,Jianchen Zhu,Peng Chen,Xue Liu,Dapeng Wu*

Main category: cs.LG

TL;DR: Tequila is a novel ternary weight quantization method that addresses deadzone trapping by repurposing trapped weights as dynamic biases, achieving near full-precision performance with 3.0x inference speedup.


<details>
  <summary>Details</summary>
Motivation: Current ternary quantization methods suffer from accuracy degradation due to deadzone trapping, where weights get stuck at boundaries and receive uninformative gradients, limiting model capacity.

Method: Proposes Tequila which reactivates deadzone-trapped weights by converting them into dynamic biases, enabling continuous forward signals and meaningful gradient updates during backpropagation.

Result: Outperforms SOTA ternary quantization methods across five benchmarks, achieving >4% accuracy gain on ARC benchmark and nearly matching full-precision performance (<1% gap) with 3.0x inference speedup.

Conclusion: Tequila provides a practical and efficient solution for deploying advanced LLMs in resource-constrained environments with minimal inference overhead.

Abstract: Quantization techniques are essential for the deployment of Large Language
Models (LLMs) on edge devices. However, prevailing methods often rely on
mixed-precision multiplication that lacks efficient hardware support, making it
not feasible. Ternary weight quantization addresses this by constraining
weights to {-1, 0, 1}, replacing expensive multiplications with
hardware-efficient additions. However, such aggressive compression leads to
significant accuracy degradation, even after costly quantization-aware training
with massive data. We identify the core issue as deadzone trapping: a large
number of weights are trapped at the deadzone boundary. This occurs because
these weights receive only noisy, uninformative gradients, preventing stable
escape from the deadzone and severely impeding model capacity and optimization.
To address this issue, we propose Tequila, a trapping-free quantization
optimization method that reactivates deadzone-trapped weights by repurposing
them as dynamic biases. This allows the repurposed weights to provide a
continuous signal in the forward pass and, critically, receive direct,
meaningful gradient signals during backpropagation, thereby enhancing model
capacity and optimization with nearly zero inference overhead. Extensive
evaluations demonstrate that Tequila outperforms state-of-the-art (SOTA)
ternary quantization methods across five benchmarks. Specifically, on the ARC
benchmark, it achieves >4% accuracy gain over the SOTA baseline, nearly
matching full-precision performance (within <1% gap) with a 3.0x inference
speedup. Consequently, Tequila offers a highly practical and efficient
implementation for the deployment of advanced LLMs in resource-constrained
environments. The code is available at https://github.com/Tencent/AngelSlim.

</details>


### [530] [IndexNet: Timestamp and Variable-Aware Modeling for Time Series Forecasting](https://arxiv.org/abs/2509.23813)
*Beiliang Wu,Peiyuan Liu,Yifan Hu,Luyan Zhang,Ao Hu,Zenglin Xu*

Main category: cs.LG

TL;DR: IndexNet is an MLP-based multivariate time series forecasting framework that incorporates index-related information through timestamp and channel embeddings to capture complex periodic patterns and distinguish between variables.


<details>
  <summary>Details</summary>
Motivation: Most existing MTSF methods overlook index-related descriptive information like timestamps and variable indices, which carry rich contextual semantics that could improve forecasting performance.

Method: Proposed IndexNet with Index Embedding module containing Timestamp Embedding (transforms timestamps into embedding vectors) and Channel Embedding (assigns unique trainable identity embeddings to each variable based on index).

Result: Extensive experiments on 12 diverse real-world datasets show IndexNet achieves comparable performance to mainstream baselines, demonstrating effectiveness of temporally and variably aware design.

Conclusion: IndexNet exhibits strong generality and interpretability, addressing underexplored aspects in current MTSF research, while effectively leveraging index information through lightweight MLP architecture.

Abstract: Multivariate time series forecasting (MTSF) plays a vital role in a wide
range of real-world applications, such as weather prediction and traffic flow
forecasting. Although recent advances have significantly improved the modeling
of temporal dynamics and inter-variable dependencies, most existing methods
overlook index-related descriptive information, such as timestamps and variable
indices, which carry rich contextual semantics. To unlock the potential of such
information and take advantage of the lightweight and powerful periodic capture
ability of MLP-based architectures, we propose IndexNet, an MLP-based framework
augmented with an Index Embedding (IE) module. The IE module consists of two
key components: Timestamp Embedding (TE) and Channel Embedding (CE).
Specifically, TE transforms timestamps into embedding vectors and injects them
into the input sequence, thereby improving the model's ability to capture
long-term complex periodic patterns. In parallel, CE assigns each variable a
unique and trainable identity embedding based on its index, allowing the model
to explicitly distinguish between heterogeneous variables and avoid homogenized
predictions when input sequences seem close. Extensive experiments on 12
diverse real-world datasets demonstrate that IndexNet achieves comparable
performance across mainstream baselines, validating the effectiveness of our
temporally and variably aware design. Moreover, plug-and-play experiments and
visualization analyses further reveal that IndexNet exhibits strong generality
and interpretability, two aspects that remain underexplored in current MTSF
research.

</details>


### [531] [Test-time GNN Model Evaluation on Dynamic Graphs](https://arxiv.org/abs/2509.23816)
*Bo Li,Xin Zheng,Ming Jin,Can Wang,Shirui Pan*

Main category: cs.LG

TL;DR: DyGEval is a novel framework for evaluating Dynamic Graph Neural Networks (DGNNs) on unseen test graphs by estimating their performance under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: DGNNs face performance uncertainty when deployed on unseen dynamic graphs due to evolving data distributions over time, making model evaluation crucial for practical deployment.

Method: Two-stage framework: (1) test-time dynamic graph simulation to capture training-test distribution differences as supervision signals, and (2) DyGEval development and training to estimate DGNN performance on test graphs.

Result: Extensive experiments show DyGEval effectively evaluates various DGNN backbones across different dynamic graphs under distribution shifts.

Conclusion: DyGEval provides a reliable solution for assessing DGNN performance on unseen dynamic graphs, addressing the critical need for model evaluation in dynamic graph learning scenarios.

Abstract: Dynamic graph neural networks (DGNNs) have emerged as a leading paradigm for
learning from dynamic graphs, which are commonly used to model real-world
systems and applications. However, due to the evolving nature of dynamic graph
data distributions over time, well-trained DGNNs often face significant
performance uncertainty when inferring on unseen and unlabeled test graphs in
practical deployment. In this case, evaluating the performance of deployed
DGNNs at test time is crucial to determine whether a well-trained DGNN is
suited for inference on an unseen dynamic test graph. In this work, we
introduce a new research problem: DGNN model evaluation, which aims to assess
the performance of a specific DGNN model trained on observed dynamic graphs by
estimating its performance on unseen dynamic graphs during test time.
Specifically, we propose a Dynamic Graph neural network Evaluator, dubbed
DyGEval, to address this new problem. The proposed DyGEval involves a two-stage
framework: (1) test-time dynamic graph simulation, which captures the
training-test distributional differences as supervision signals and trains an
evaluator; and (2) DyGEval development and training, which accurately estimates
the performance of the well-trained DGNN model on the test-time dynamic graphs.
Extensive experiments demonstrate that the proposed DyGEval serves as an
effective evaluator for assessing various DGNN backbones across different
dynamic graphs under distribution shifts.

</details>


### [532] [Space Group Conditional Flow Matching](https://arxiv.org/abs/2509.23822)
*Omri Puny,Yaron Lipman,Benjamin Kurt Miller*

Main category: cs.LG

TL;DR: Space Group Conditional Flow Matching is a novel generative framework that samples highly-symmetric, stable crystals by conditioning on space groups and Wyckoff positions, achieving state-of-the-art results in crystal structure prediction.


<details>
  <summary>Details</summary>
Motivation: Most generative models for predicting atomic coordinates overlook symmetry constraints, leading to unrealistic crystals with limited symmetry. The paper aims to address this by incorporating crystallographic symmetry into the generation process.

Method: The method conditions the entire generation process on a given space group and Wyckoff positions. It uses a conditionally symmetric noise base distribution and a group-conditioned, equivariant, parametric vector field that restricts atom motion to their initial Wyckoff positions. An efficient reformulation of group averaging is employed for symmetric crystals.

Result: The approach achieves state-of-the-art results on crystal structure prediction and de novo generation benchmarks, with significantly reduced computational overhead for symmetrization.

Conclusion: Space Group Conditional Flow Matching successfully generates highly-symmetric, stable crystals by incorporating crystallographic constraints, outperforming existing methods while maintaining computational efficiency.

Abstract: Inorganic crystals are periodic, highly-symmetric arrangements of atoms in
three-dimensional space. Their structures are constrained by the symmetry
operations of a crystallographic \emph{space group} and restricted to lie in
specific affine subspaces known as \emph{Wyckoff positions}. The frequency an
atom appears in the crystal and its rough positioning are determined by its
Wyckoff position. Most generative models that predict atomic coordinates
overlook these symmetry constraints, leading to unrealistically high
populations of proposed crystals exhibiting limited symmetry. We introduce
Space Group Conditional Flow Matching, a novel generative framework that
samples significantly closer to the target population of highly-symmetric,
stable crystals. We achieve this by conditioning the entire generation process
on a given space group and set of Wyckoff positions; specifically, we define a
conditionally symmetric noise base distribution and a group-conditioned,
equivariant, parametric vector field that restricts the motion of atoms to
their initial Wyckoff position. Our form of group-conditioned equivariance is
achieved using an efficient reformulation of \emph{group averaging} tailored
for symmetric crystals. Importantly, it reduces the computational overhead of
symmetrization to a negligible level. We achieve state of the art results on
crystal structure prediction and de novo generation benchmarks. We also perform
relevant ablations.

</details>


### [533] [Electric Currents for Discrete Data Generation](https://arxiv.org/abs/2509.23825)
*Alexander Kolesov,Stepan Manukhov,Vladimir V. Palyulin,Alexander Korotin*

Main category: cs.LG

TL;DR: ECD²G is a novel discrete data generation method using electrical current analogies to model probability flow between distributions, with neural networks learning current flows for provable distribution transfer.


<details>
  <summary>Details</summary>
Motivation: To develop a theoretically grounded approach for discrete data generation that guarantees distribution transfer using principles from electrical engineering.

Method: Analogizes electric current flow to probability mass transfer, uses neural networks to learn electric currents representing probability flow, and transports samples along circuit pathways based on learned currents.

Result: Proof-of-concept experiments demonstrate the method's effectiveness in transferring between data distributions.

Conclusion: ECD²G provides a theoretically sound framework for discrete data generation with provable distribution transfer guarantees.

Abstract: We propose $\textbf{E}$lectric $\textbf{C}$urrent $\textbf{D}$iscrete
$\textbf{D}$ata $\textbf{G}$eneration (ECD$^{2}$G), a pioneering method for
data generation in discrete settings that is grounded in electrical engineering
theory. Our approach draws an analogy between electric current flow in a
circuit and the transfer of probability mass between data distributions. We
interpret samples from the source distribution as current input nodes of a
circuit and samples from the target distribution as current output nodes. A
neural network is then used to learn the electric currents to represent the
probability flow in the circuit. To map the source distribution to the target,
we sample from the source and transport these samples along the circuit
pathways according to the learned currents. This process provably guarantees
transfer between data distributions. We present proof-of-concept experiments to
illustrate our ECD$^{2}$G method.

</details>


### [534] [Bayesian Mixture-of-Experts: Towards Making LLMs Know What They Don't Know](https://arxiv.org/abs/2509.23830)
*Albus Yizhuo Li*

Main category: cs.LG

TL;DR: Proposes a Bayesian MoE routing framework to address brittleness in standard deterministic routing, improving model calibration and uncertainty awareness.


<details>
  <summary>Details</summary>
Motivation: Standard deterministic routing in Mixture-of-Experts models causes brittleness, leading to model miscalibration, overconfidence, and inability to know what they don't know.

Method: Introduces Bayesian MoE routing framework modeling probability distribution over routing decisions, with three method families: weight-space, logit-space, and selection-space uncertainty.

Result: Significantly improves routing stability, in-distribution calibration, and out-of-distribution detection in 3-billion parameter MoE model experiments.

Conclusion: Provides practical pathway to build more robust and self-aware LLMs by targeting core architectural component to create reliable internal uncertainty signal.

Abstract: The Mixture-of-Experts (MoE) architecture has enabled the creation of massive
yet efficient Large Language Models (LLMs). However, the standard deterministic
routing mechanism presents a significant limitation: its inherent brittleness
is a key contributor to model miscalibration and overconfidence, resulting in
systems that often do not know what they don't know.
  This thesis confronts this challenge by proposing a structured
\textbf{Bayesian MoE routing framework}. Instead of forcing a single,
deterministic expert selection, our approach models a probability distribution
over the routing decision itself. We systematically investigate three families
of methods that introduce this principled uncertainty at different stages of
the routing pipeline: in the \textbf{weight-space}, the \textbf{logit-space},
and the final \textbf{selection-space}.
  Through a series of controlled experiments on a 3-billion parameter MoE
model, we demonstrate that this framework significantly improves routing
stability, in-distribution calibration, and out-of-distribution (OoD)
detection. The results show that by targeting this core architectural
component, we can create a more reliable internal uncertainty signal. This work
provides a practical and computationally tractable pathway towards building
more robust and self-aware LLMs, taking a crucial step towards making them know
what they don't know.

</details>


### [535] [Adversarial Diffusion for Robust Reinforcement Learning](https://arxiv.org/abs/2509.23846)
*Daniele Foffano,Alessio Russo,Alexandre Proutiere*

Main category: cs.LG

TL;DR: AD-RRL uses diffusion models to train robust RL policies by generating worst-case trajectories during training, optimizing CVaR for better robustness against environment uncertainties.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of robustness to modeling errors and uncertainties in reinforcement learning, which remains a central issue in RL applications.

Method: Leverage diffusion models with conditional sampling to generate worst-case trajectories, building on CVaR optimization to create Adversarial Diffusion for Robust Reinforcement Learning (AD-RRL).

Result: Empirical results across standard benchmarks demonstrate that AD-RRL achieves superior robustness and performance compared to existing robust RL methods.

Conclusion: AD-RRL effectively addresses robustness challenges in RL by using diffusion models to optimize CVaR through adversarial trajectory generation during training.

Abstract: Robustness to modeling errors and uncertainties remains a central challenge
in reinforcement learning (RL). In this work, we address this challenge by
leveraging diffusion models to train robust RL policies. Diffusion models have
recently gained popularity in model-based RL due to their ability to generate
full trajectories "all at once", mitigating the compounding errors typical of
step-by-step transition models. Moreover, they can be conditioned to sample
from specific distributions, making them highly flexible. We leverage
conditional sampling to learn policies that are robust to uncertainty in
environment dynamics. Building on the established connection between
Conditional Value at Risk (CVaR) optimization and robust RL, we introduce
Adversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides
the diffusion process to generate worst-case trajectories during training,
effectively optimizing the CVaR of the cumulative return. Empirical results
across standard benchmarks show that AD-RRL achieves superior robustness and
performance compared to existing robust RL methods.

</details>


### [536] [Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation](https://arxiv.org/abs/2509.23866)
*Pengxiang Li,Zechen Hu,Zirui Shang,Jingrong Wu,Yang Liu,Hui Liu,Zhi Gao,Chenrui Shi,Bofei Zhang,Zihao Zhang,Xiaochuan Shi,Zedong YU,Yuwei Wu,Xinxiao Wu,Yunde Jia,Liuyu Xiang,Zhaofeng He,Qing Li*

Main category: cs.LG

TL;DR: DART is a decoupled agentic RL training framework for GUI agents that addresses slow multi-turn interactions and insufficient training data through asynchronous modules and adaptive data curation, achieving significant performance improvements on the OSWorld benchmark.


<details>
  <summary>Details</summary>
Motivation: Vision-language model based GUI agents face challenges in reinforcement learning due to slow multi-turn interactions with GUI environments and insufficient high-quality agent-environment interactions for effective policy learning.

Method: DART separates the training system into four asynchronous modules (environment cluster, rollout service, data manager, trainer) with non-blocking communication, and introduces adaptive data curation including pre-collecting successful trajectories, dynamic adjustment of rollout parameters, selective training on high-entropy steps, and truncated importance sampling.

Result: DART achieves 1.6x GPU utilization for rollout, 1.9x training throughput, and 5.5x environment utilization. On OSWorld benchmark, DART-GUI-7B achieves 42.13% task success rate, a 14.61% absolute gain over base model and 7.34% higher than open-source SOTA.

Conclusion: DART provides an efficient and effective framework for training GUI agents through decoupled architecture and adaptive data curation, demonstrating significant performance improvements and will be open-sourced to benefit the agentic RL community.

Abstract: Vision-language model (VLM) based GUI agents show promise for automating
complex desktop and mobile tasks, but face significant challenges in applying
reinforcement learning (RL): (1) slow multi-turn interactions with GUI
environments for policy rollout, and (2) insufficient high-quality
agent-environment interactions for policy learning. To address these
challenges, we propose DART, a Decoupled Agentic RL Training framework for GUI
agents, which coordinates heterogeneous modules in a highly decoupled manner.
DART separates the training system into four asynchronous modules: environment
cluster, rollout service, data manager, and trainer. This design enables
non-blocking communication, asynchronous training, rollout-wise trajectory
sampling, and per-worker model synchronization, significantly improving the
system efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,
and 5.5* environment utilization. To facilitate effective learning from
abundant samples, we introduce an adaptive data curation scheme: (1)
pre-collecting successful trajectories for challenging tasks to supplement
sparse success in online sampling; (2) dynamically adjusting rollout numbers
and trajectory lengths based on task difficulty; (3) training selectively on
high-entropy steps to prioritize critical decisions; (4) stabilizing learning
via truncated importance sampling for policy mismatch between policy rollout
and updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task
success rate, a 14.61% absolute gain over the base model, and 7.34% higher than
open-source SOTA. We will fully open-source our training framework, data, and
model checkpoints via computer-use-agents.github.io/dart-gui, which we believe
is a timely contribution to the open-source community of agentic RL training.

</details>


### [537] [Towards Understanding Subliminal Learning: When and How Hidden Biases Transfer](https://arxiv.org/abs/2509.23886)
*Simon Schrodi,Elias Kempf,Fazl Barez,Thomas Brox*

Main category: cs.LG

TL;DR: Subliminal learning occurs in language model distillation where hidden biases transfer from teacher to student, even with hard distillation. This happens through divergence tokens - rare cases where biased teachers predict different tokens. Early layers are critical, and subliminal learning is fragile to prompt changes.


<details>
  <summary>Details</summary>
Motivation: To understand when and how subliminal learning occurs in language model distillation, particularly why it happens under hard distillation where students only see sampled tokens rather than full distributions.

Method: Conducted controlled experiments and mechanistic analysis to identify the mechanism behind subliminal learning. Investigated token entanglement, logit leakage, and identified divergence tokens as the key mechanism. Tested layer importance through selective finetuning.

Result: Subliminal learning doesn't require global token entanglement or logit leakage. It occurs through divergence tokens - rare cases where biased teachers predict different tokens. Early layers are critical, and finetuning even a single early layer can enable subliminal learning. The phenomenon is fragile and can be suppressed by small prompt changes.

Conclusion: Subliminal learning in language model distillation is mediated by divergence tokens rather than global mechanisms, with early layers playing a crucial role. The fragility of this phenomenon suggests it can be mitigated through careful prompt design.

Abstract: Language models can transfer hidden biases during distillation. For example,
a teacher that "likes owls" can make its student "like owls" too, even when the
training data consists only of lists of numbers. This surprising phenomenon is
called subliminal learning. Subliminal learning can be expected under soft
distillation, where the student is trained on the teacher's full next-token
distribution. But the fact that this also occurs under hard distillation-where
the student only sees sampled tokens-raises a deeper question: when and how
does subliminal learning actually occur? We answer this question through
controlled experiments and mechanistic analysis. Our results show that
subliminal learning does not need (global) token entanglement or logit leakage.
Instead, it comes down to a small set of divergence tokens-rare cases where
teachers with different biases would predict different tokens. Masking out
these tokens mostly removes the hidden bias transfer. Mechanistically,
divergence tokens reveal that early layers are critical. Surprisingly,
finetuning even a single such early layer is sufficient for subliminal
learning. Finally, we find that subliminal learning is fragile. Even small
changes, like paraphrasing prompts, are usually sufficient to suppress it.

</details>


### [538] [Gradient Flow Convergence Guarantee for General Neural Network Architectures](https://arxiv.org/abs/2509.23887)
*Yash Jakhmola*

Main category: cs.LG

TL;DR: This paper provides a unified proof for linear convergence of gradient flow in training neural networks with various activations, covering both previously known and new architectures under weaker assumptions.


<details>
  <summary>Details</summary>
Motivation: To address the theoretical challenge of explaining why gradient-based optimization successfully trains complex deep neural networks, and to develop a unified theory that covers multiple architectures.

Method: The authors present a general theorem for linear convergence of continuous gradient descent (gradient flow) for neural networks with piecewise non-zero polynomial activations, ReLU, and sigmoid activations.

Result: The unified proof covers architectures previously unknown and consolidates existing results under weaker assumptions, with empirical agreement between theoretical predictions and practical gradient descent.

Conclusion: The paper establishes a comprehensive theoretical foundation for linear convergence in neural network training, bridging theory and practice for gradient-based optimization methods.

Abstract: A key challenge in modern deep learning theory is to explain the remarkable
success of gradient-based optimization methods when training large-scale,
complex deep neural networks. Though linear convergence of such methods has
been proved for a handful of specific architectures, a united theory still
evades researchers. This article presents a unified proof for linear
convergence of continuous gradient descent, also called gradient flow, while
training any neural network with piecewise non-zero polynomial activations or
ReLU, sigmoid activations. Our primary contribution is a single, general
theorem that not only covers architectures for which this result was previously
unknown but also consolidates existing results under weaker assumptions. While
our focus is theoretical and our results are only exact in the infinitesimal
step size limit, we nevertheless find excellent empirical agreement between the
predictions of our result and those of the practical step-size gradient descent
method.

</details>


### [539] [Differentiable Sparsity via $D$-Gating: Simple and Versatile Structured Penalization](https://arxiv.org/abs/2509.23898)
*Chris Kolb,Laetitia Frost,Bernd Bischl,David Rügamer*

Main category: cs.LG

TL;DR: D-Gating is a differentiable structured overparameterization method that achieves structured sparsity in neural networks by splitting weights into primary vectors and gating factors, with theoretical guarantees of equivalence to L2,2/D regularization and exponential convergence.


<details>
  <summary>Details</summary>
Motivation: Structured sparsity regularization is effective for network compression but incompatible with standard SGD due to non-differentiability, requiring specialized optimizers or post-hoc pruning without formal guarantees.

Method: Proposes D-Gating that splits each weight group into a primary weight vector and multiple scalar gating factors, creating a fully differentiable overparameterization that enables structured sparsity learning.

Result: Proves theoretical equivalence to L2,2/D regularization, shows exponential convergence to regularized loss, and demonstrates superior performance-sparsity tradeoffs across vision, language, and tabular tasks compared to direct optimization and pruning baselines.

Conclusion: D-Gating provides a theoretically grounded, differentiable approach to structured sparsity that evolves naturally from non-sparse to sparse optimization while maintaining compatibility with standard training methods.

Abstract: Structured sparsity regularization offers a principled way to compact neural
networks, but its non-differentiability breaks compatibility with conventional
stochastic gradient descent and requires either specialized optimizers or
additional post-hoc pruning without formal guarantees. In this work, we propose
$D$-Gating, a fully differentiable structured overparameterization that splits
each group of weights into a primary weight vector and multiple scalar gating
factors. We prove that any local minimum under $D$-Gating is also a local
minimum using non-smooth structured $L_{2,2/D}$ penalization, and further show
that the $D$-Gating objective converges at least exponentially fast to the
$L_{2,2/D}$-regularized loss in the gradient flow limit. Together, our results
show that $D$-Gating is theoretically equivalent to solving the original group
sparsity problem, yet induces distinct learning dynamics that evolve from a
non-sparse regime into sparse optimization. We validate our theory across
vision, language, and tabular tasks, where $D$-Gating consistently delivers
strong performance-sparsity tradeoffs and outperforms both direct optimization
of structured penalties and conventional pruning baselines.

</details>


### [540] [Integrated Communication and Control for Energy-Efficient UAV Swarms: A Multi-Agent Reinforcement Learning Approach](https://arxiv.org/abs/2509.23905)
*Tianjiao Sun,Ningyan Guo,Haozhe Gu,Yanyan Peng,Zhiyong Feng*

Main category: cs.LG

TL;DR: Proposes an integrated communication-control co-design using multi-agent reinforcement learning (MAHPPO-AM) to optimize UAV swarm energy efficiency and communication fairness in complex environments.


<details>
  <summary>Details</summary>
Motivation: UAV swarm communications face unreliable A2G links in complex geographic environments, requiring energy-efficient solutions that maintain communication quality for ground users.

Method: Formulates joint resource allocation and 3D trajectory control as MDP, develops MARL framework with novel MAHPPO-AM algorithm using action masking for hybrid action spaces.

Result: Achieves 0.99 fairness index while reducing energy consumption by up to 25% compared to baseline methods.

Conclusion: The proposed integrated communication-control co-design effectively balances energy efficiency and communication fairness in UAV swarm networks.

Abstract: The deployment of unmanned aerial vehicle (UAV) swarm-assisted communication
networks has become an increasingly vital approach for remediating coverage
limitations in infrastructure-deficient environments, with especially pressing
applications in temporary scenarios, such as emergency rescue, military and
security operations, and remote area coverage. However, complex geographic
environments lead to unpredictable and highly dynamic wireless channel
conditions, resulting in frequent interruptions of air-to-ground (A2G) links
that severely constrain the reliability and quality of service in UAV
swarm-assisted mobile communications. To improve the quality of UAV
swarm-assisted communications in complex geographic environments, we propose an
integrated communication and control co-design mechanism. Given the stringent
energy constraints inherent in UAV swarms, our proposed mechanism is designed
to optimize energy efficiency while maintaining an equilibrium between
equitable communication rates for mobile ground users (GUs) and UAV energy
expenditure. We formulate the joint resource allocation and 3D trajectory
control problem as a Markov decision process (MDP), and develop a multi-agent
reinforcement learning (MARL) framework to enable real-time coordinated actions
across the UAV swarm. To optimize the action policy of UAV swarms, we propose a
novel multi-agent hybrid proximal policy optimization with action masking
(MAHPPO-AM) algorithm, specifically designed to handle complex hybrid action
spaces. The algorithm incorporates action masking to enforce hard constraints
in high-dimensional action spaces. Experimental results demonstrate that our
approach achieves a fairness index of 0.99 while reducing energy consumption by
up to 25% compared to baseline methods.

</details>


### [541] [Graph Mixing Additive Networks](https://arxiv.org/abs/2509.23923)
*Maya Bechler-Speicher,Andrea Zerio,Maor Huri,Marie Vibeke Vestergaard,Ran Gilad-Bachrach,Tine Jess,Samir Bhatt,Aleksejs Sazonovs*

Main category: cs.LG

TL;DR: GMAN extends Graph Neural Additive Networks to learn from sparse time-series data by representing trajectories as directed graphs, offering flexible interpretability-expressivity trade-offs and outperforming black-box models on real-world tasks.


<details>
  <summary>Details</summary>
Motivation: To create an interpretable yet expressive framework for learning from sets of sparse time-series data that can outperform black-box models while providing actionable explanations.

Method: Represents time-dependent trajectories as directed graphs and applies enriched Graph Neural Additive Networks (GNANs) to each graph, allowing feature grouping and graph grouping to encode priors and control interpretability-expressivity trade-off.

Result: Outperforms strong non-interpretable black-box baselines on real-world datasets including mortality prediction from blood tests and fake-news detection.

Conclusion: GMAN provides a flexible, interpretable framework that delivers domain-aligned explanations while maintaining competitive performance against black-box models.

Abstract: We introduce GMAN, a flexible, interpretable, and expressive framework that
extends Graph Neural Additive Networks (GNANs) to learn from sets of sparse
time-series data. GMAN represents each time-dependent trajectory as a directed
graph and applies an enriched, more expressive GNAN to each graph. It allows
users to control the interpretability-expressivity trade-off by grouping
features and graphs to encode priors, and it provides feature, node, and
graph-level interpretability. On real-world datasets, including mortality
prediction from blood tests and fake-news detection, GMAN outperforms strong
non-interpretable black-box baselines while delivering actionable,
domain-aligned explanations.

</details>


### [542] [HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models](https://arxiv.org/abs/2509.23928)
*Zhinan Xie,Peisong Wang,Jian Cheng*

Main category: cs.LG

TL;DR: HiViS is a speculative decoding method for Vision-Language Models that hides visual tokens from the drafter to accelerate inference while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Adapting speculative decoding to VLMs is challenging due to visual token misalignment between drafter and target models, and slow self-attention from numerous visual tokens.

Method: Removes visual tokens from drafter input, uses only textual tokens as explicit inputs, reuses target VLM's hidden states as implicit visual information, and employs multi-step self-feedback training.

Result: Compresses drafter prefill sequence length to 0.7%-1.3% of target VLM input, achieves up to 2.65x speedup with lossless generation quality across diverse models and tasks.

Conclusion: HiViS effectively accelerates VLM inference by addressing visual token challenges in speculative decoding through explicit-implicit input decomposition.

Abstract: Speculative decoding is an effective approach for accelerating inference in
Large Language models (LLMs), but its adaptation to Vision-Language models
(VLMs) remains challenging for additional visual tokens in multimodal inputs.
First, owing to the fact that the drafter and the target VLM may derived from
different families, the semantic representations of visual tokens in the target
VLM are misaligned with those in the drafter, introducing bias into the
KV-cache during the prefill stage. Second, the large number of visual tokens
substantially slows down the drafter's self-attention during the decoding
stage. We propose Hiding Visual Tokens from the Drafter for Speculative
Decoding in Vision-Language Models (HiViS), an explicit-implicit input
decomposition framework that alleviates the above inefficiency. All visual
tokens are removed from the drafter's input, retaining only textual tokens as
explicit inputs, while directly reusing the target VLM's corresponding
last-layer hidden states as implicit visual information without additional
processing. To train the drafter efficiently, we introduces multi-step
self-feedback training strategy with dynamic data selection and sequential
embedding supervision to simulate reasoning during training. Our approach
compresses the prefill sequence length of the drafter to only 0.7%-1.3% of the
target VLM's input, while maintaining lossless generation quality. Extensive
experiments across diverse models and tasks demonstrate up to 2.65x speedup,
confirming the effectiveness of HiViS in accelerating VLM inference.

</details>


### [543] [Beyond Benchmarks: Understanding Mixture-of-Experts Models through Internal Mechanisms](https://arxiv.org/abs/2509.23933)
*Jiahao Ying,Mingbao Lin,Qianru Sun,Yixin Cao*

Main category: cs.LG

TL;DR: This paper investigates the internal mechanisms of Mixture-of-Experts (MoE) architectures using an internal metric (MUI), revealing insights about neuron utilization, training dynamics, expert collaboration, and activation patterns that complement traditional performance benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current MoE research is largely performance-centric with limited understanding of internal mechanisms, which constrains broader progress in the field. The authors aim to provide deeper insights into how MoE architectures actually work internally.

Method: The authors use an internal metric (MUI) to investigate MoE mechanisms by explicitly incorporating routing mechanisms and analyzing expert-level behaviors across a wide range of publicly available MoE models.

Result: Key findings include: (1) neuron utilization decreases as models evolve, indicating stronger generalization; (2) training shows dynamic trajectories where MUI reveals deeper insights than benchmark performance alone; (3) task completion involves collaborative contributions from multiple experts with shared experts driving concentration; (4) neuron-level activation patterns serve as fine-grained proxies for data diversity.

Conclusion: MUI serves as a valuable complementary indicator to benchmark performance, offering new insights into MoE model capacity, dynamics, and specialization, demonstrating the potential of internal metrics for understanding complex neural architectures.

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising direction,
offering efficiency and scalability by activating only a subset of parameters
during inference. However, current research remains largely
performance-centric, with limited understanding of its internal mechanisms,
thereby constraining broader progress. In this work, we use an internal metric
to investigate the mechanisms of MoE architecture by explicitly incorporating
routing mechanisms and analyzing expert-level behaviors. Through systematic
analyses of a wide range of publicly available MoE models, we uncover several
findings: (1) neuron utilization decreases as models evolve, reflecting
stronger generalization; (2) training exhibits a dynamic trajectory, where
benchmark performance alone provides limited signal while MUI reveals deeper
insights; (3) task completion emerges from collaborative contributions of
multiple experts, with shared experts driving concentration; and (4) activation
patterns at the neuron level provide a fine-grained proxy for data diversity.
Together, these results demonstrate the potential of MUI as a complementary
indicator to benchmark performance, offering new insights into the capacity,
dynamics, and specialization of MoE models. Our project can be found at
https://yingjiahao14.github.io/MoE-MUI/.

</details>


### [544] [Diffusion Models are Kelly Gamblers](https://arxiv.org/abs/2509.23937)
*Akhil Premkumar*

Main category: cs.LG

TL;DR: This paper connects diffusion models to the Kelly criterion in betting games, revealing that conditional diffusion models store mutual information between signals and conditioning information. Classifier-free guidance boosts this mutual information during sampling, which is particularly useful for image models where image-label mutual information is low. The paper also clarifies nuances in viewing diffusion models as infinitely deep autoencoders and relates the denoising loss to the Fermi Golden Rule from quantum mechanics.


<details>
  <summary>Details</summary>
Motivation: To establish theoretical connections between diffusion models and established concepts from information theory (Kelly criterion) and quantum mechanics (Fermi Golden Rule), providing deeper insights into how conditional diffusion models work and why classifier-free guidance is effective.

Method: Theoretical analysis connecting diffusion models to the Kelly criterion for betting games, examining how conditional diffusion models store mutual information between signals X and conditioning information Y. Analysis of classifier-free guidance mechanisms and their effect on mutual information.

Result: Found that conditional diffusion models store additional information equal to the mutual information between X and Y. Classifier-free guidance effectively boosts this mutual information during sampling, which is particularly beneficial for image models due to the low mutual information between images and their labels (related to the manifold hypothesis).

Conclusion: The paper provides novel theoretical connections between diffusion models and concepts from betting theory and quantum mechanics, offering insights into the information storage mechanisms in conditional diffusion models and explaining why classifier-free guidance works effectively, especially in image generation tasks.

Abstract: We draw a connection between diffusion models and the Kelly criterion for
maximizing returns in betting games. We find that conditional diffusion models
store additional information to bind the signal $X$ with the conditioning
information $Y$, equal to the mutual information between them. Classifier-free
guidance effectively boosts the mutual information between $X$ and $Y$ at
sampling time. This is especially helpful in image models, since the mutual
information between images and their labels is low, a fact which is intimately
connected to the manifold hypothesis. Finally, we point out some nuances in the
popular perspective that diffusion models are infinitely deep autoencoders. In
doing so, we relate the denoising loss to the Fermi Golden Rule from quantum
mechanics.

</details>


### [545] [Brain-language fusion enables interactive neural readout and in-silico experimentation](https://arxiv.org/abs/2509.23941)
*Victoria Bosch,Daniel Anthes,Adrien Doerig,Sushrut Thorat,Peter König,Tim Christian Kietzmann*

Main category: cs.LG

TL;DR: CorText is a framework that integrates neural activity into LLM's latent space, enabling natural language interaction with brain data through fMRI recordings during natural scene viewing.


<details>
  <summary>Details</summary>
Motivation: Current neural decoding methods are constrained by static, non-interactive approaches, limiting the potential for dynamic brain-computer interfaces.

Method: Integrates fMRI data recorded during natural scene viewing directly into LLM's latent space, enabling open-ended natural language interaction with neural data.

Result: Generates accurate image captions, answers detailed questions better than controls using only neural data, and achieves zero-shot generalization beyond training categories.

Conclusion: Represents a shift from passive decoding to generative, flexible brain-language interfaces, demonstrated through counterfactual analysis simulating cortical microstimulation.

Abstract: Large language models (LLMs) have revolutionized human-machine interaction,
and have been extended by embedding diverse modalities such as images into a
shared language space. Yet, neural decoding has remained constrained by static,
non-interactive methods. We introduce CorText, a framework that integrates
neural activity directly into the latent space of an LLM, enabling open-ended,
natural language interaction with brain data. Trained on fMRI data recorded
during viewing of natural scenes, CorText generates accurate image captions and
can answer more detailed questions better than controls, while having access to
neural data only. We showcase that CorText achieves zero-shot generalization
beyond semantic categories seen during training. Furthermore, we present a
counterfactual analysis that emulates in-silico cortical microstimulation.
These advances mark a shift from passive decoding toward generative, flexible
interfaces between brain activity and language.

</details>


### [546] [Efficient Identification of High Similarity Clusters in Polygon Datasets](https://arxiv.org/abs/2509.23942)
*John N. Daras*

Main category: cs.LG

TL;DR: A framework that reduces computational load for large-scale spatial similarity computations by integrating dynamic thresholding, supervised scheduling, and recall-constrained optimization.


<details>
  <summary>Details</summary>
Motivation: Existing tools like Shapely 2.0 and Triton improve efficiency but still face challenges with extremely large datasets due to high computational volume.

Method: Uses Kernel Density Estimation (KDE) for dynamic similarity thresholding, machine learning models for cluster prioritization, and recall-constrained optimization to reduce verification clusters.

Result: Achieves substantial reductions in computational cost without sacrificing accuracy, demonstrating scalability and effectiveness in experiments.

Conclusion: Provides a practical solution for large-scale geospatial analysis by efficiently identifying high spatial similarity clusters while meeting user-defined precision and recall requirements.

Abstract: Advancements in tools like Shapely 2.0 and Triton can significantly improve
the efficiency of spatial similarity computations by enabling faster and more
scalable geometric operations. However, for extremely large datasets, these
optimizations may face challenges due to the sheer volume of computations
required. To address this, we propose a framework that reduces the number of
clusters requiring verification, thereby decreasing the computational load on
these systems. The framework integrates dynamic similarity index thresholding,
supervised scheduling, and recall-constrained optimization to efficiently
identify clusters with the highest spatial similarity while meeting
user-defined precision and recall requirements. By leveraging Kernel Density
Estimation (KDE) to dynamically determine similarity thresholds and machine
learning models to prioritize clusters, our approach achieves substantial
reductions in computational cost without sacrificing accuracy. Experimental
results demonstrate the scalability and effectiveness of the method, offering a
practical solution for large-scale geospatial analysis.

</details>


### [547] [Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm](https://arxiv.org/abs/2509.23946)
*Kaisen Yang,Lixuan He,Rushi Shah,Kaicheng Yang,Qinwei Ma,Dianbo Liu,Alex Lamb*

Main category: cs.LG

TL;DR: E^2C is a structured reasoning framework that decouples reasoning into exploratory planning and deterministic execution phases, achieving computational efficiency and improved performance with fewer tokens.


<details>
  <summary>Details</summary>
Motivation: Current Chain-of-Thought methods conflate planning and execution, leading to computational inefficiency, limited path exploration, and reduced interpretability.

Method: Two-phase framework: exploratory phase generates high-level plans stochastically, execution phase deterministically carries out plans. Uses two-stage training with SFT (augmented by strict plan adherence) and RL.

Result: Achieves 58.1% accuracy on AIME'2024 using <10% of decoding tokens compared to similar methods. EF-SFT fine-tuning uses only 3.5% of tokens yet yields up to 14.5% higher accuracy on medical benchmarks.

Conclusion: E^2C provides state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution, while sharply reducing computational overhead.

Abstract: Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning
abilities of Large Language Models (LLMs), yet their monolithic and
auto-regressive architecture inherently conflates high-level strategic planning
with low-level step-by-step execution, leading to computational inefficiency,
limited exploration of reasoning paths, and reduced interpretability. To
overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a
structured reasoning framework that decouples reasoning into two distinct
phases: an exploratory phase that stochastically generates succinct high-level
plans, followed by an execution phase that deterministically carries out the
chosen plan. Our approach incorporates a two-stage training methodology, which
combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation
algorithm enforcing strict plan adherence - with a subsequent Reinforcement
Learning (RL) stage that capitalizes on the informativeness of exploration and
reinforces the determinism of execution.This decomposition enables an efficient
test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches
58.1% accuracy using <10% of the decoding tokens required by comparable methods
(e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For
cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with
only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher
accuracy than standard SFT on medical benchmarks, delivering state-of-the-art
performance, strong generalization, and greater interpretability by separating
planning from execution. The code and pre-trained models for the project are
available at: https://github.com/yks23/Explore-Execute-Chain.git

</details>


### [548] [DiBS-MTL: Transformation-Invariant Multitask Learning with Direction Oracles](https://arxiv.org/abs/2509.23948)
*Surya Murthy,Kushagra Gupta,Mustafa O. Karabag,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: DiBS-MTL is a multitask learning method based on cooperative bargaining theory that achieves Pareto stationary solutions immune to task domination by being invariant to monotonic nonaffine task loss transformations.


<details>
  <summary>Details</summary>
Motivation: Existing multitask learning methods suffer from task domination issues when task losses are arbitrarily scaled, which degrades overall performance. Current approaches are not robust to nonaffine monotonic transformations of task losses.

Method: Proposed DiBS-MTL, a computationally efficient adaptation of the Direction-based Bargaining Solution (DiBS) from cooperative bargaining theory to the MTL setting. It provides convergence guarantees to Pareto stationary points even for nonconvex task losses.

Result: Empirical validation on standard MTL benchmarks shows DiBS-MTL achieves competitive performance with state-of-the-art methods while maintaining robustness to nonaffine monotonic transformations that degrade existing approaches.

Conclusion: DiBS-MTL provides a theoretically grounded solution to task domination in multitask learning, with proven convergence properties and empirical effectiveness that outperforms existing bargaining-inspired methods.

Abstract: Multitask learning (MTL) algorithms typically rely on schemes that combine
different task losses or their gradients through weighted averaging. These
methods aim to find Pareto stationary points by using heuristics that require
access to task loss values, gradients, or both. In doing so, a central
challenge arises because task losses can be arbitrarily, nonaffinely scaled
relative to one another, causing certain tasks to dominate training and degrade
overall performance. A recent advance in cooperative bargaining theory, the
Direction-based Bargaining Solution (DiBS), yields Pareto stationary solutions
immune to task domination because of its invariance to monotonic nonaffine task
loss transformations. However, the convergence behavior of DiBS in nonconvex
MTL settings is currently not understood. To this end, we prove that under
standard assumptions, a subsequence of DiBS iterates converges to a Pareto
stationary point when task losses are possibly nonconvex, and propose DiBS-MTL,
a computationally efficient adaptation of DiBS to the MTL setting. Finally, we
validate DiBS-MTL empirically on standard MTL benchmarks, showing that it
achieves competitive performance with state-of-the-art methods while
maintaining robustness to nonaffine monotonic transformations that
significantly degrade the performance of existing approaches, including prior
bargaining-inspired MTL methods. Code available at
https://github.com/suryakmurthy/dibs-mtl.

</details>


### [549] [Evaluating the Robustness of Chinchilla Compute-Optimal Scaling](https://arxiv.org/abs/2509.23963)
*Rylan Schaeffer,Noam Levi,Andreas Kirsch,Theo Guenais,Brando Miranda,Elyas Obbad,Sanmi Koyejo*

Main category: cs.LG

TL;DR: This paper validates Chinchilla's compute-optimal scaling laws despite concerns about parameter ambiguities, showing that different parameter interpretations don't meaningfully affect key results and the scaling laws withstand significant perturbations.


<details>
  <summary>Details</summary>
Motivation: To address concerns about Chinchilla's scaling laws - including wide confidence intervals, discrepancies between approaches, and incongruities with other scaling laws - and determine if practitioners can still rely on Chinchilla's prescriptions for language model scaling.

Method: Analyzed three possible interpretations of Chinchilla's model parameters, then deliberately perturbed model parameters in four structured ways to test the robustness of key results, particularly the compute-optimal tokens-to-parameter ratio.

Result: Found that different parameter interpretations (with up to 15.2% differences) don't meaningfully affect scaling law estimates or the optimal tokens-to-parameter ratio. Under one interpretation, the ratio becomes more constant with compute budget. Key results withstand sizable perturbations, being most sensitive only to additive/systematic errors.

Conclusion: Chinchilla's compute-optimal scaling laws remain valid and provide a durable guide for scaling language models, offering renewed confidence to the field despite previous concerns.

Abstract: Hoffman et al (2022)'s Chinchilla paper introduced the principle of
compute-optimal scaling, laying a foundation for future scaling of language
models. In the years since, however, valid concerns about Chinchilla have been
raised: wide confidence intervals, discrepancies between its three approaches,
and incongruities with other scaling laws. This raises a critical question for
the field: Can practitioners still rely on Chinchilla's prescriptions? Our work
demonstrates the answer is yes. We begin by uncovering that the model
parameters central to Chinchilla's analyses were ambiguous: three
interpretations are possible, with relative differences between different
interpretations of model parameters as high as 15.2%. We find that, perhaps
surprisingly, which model parameters are used for the analyses do not
meaningfully affect key results: the scaling law estimates and the
compute-optimal tokens-to-parameter ratio. Indeed, under one interpretation,
the tokens-to-parameter ratio becomes more constant with the target compute
budget. We then ask how distorted the Chinchilla model parameters could have
been without meaningfully affecting the key results. By deliberately perturbing
model parameters in four structured ways, we find that key Chinchilla results
are most sensitive to additive or systematic errors, which can alter the
otherwise flat trend of the optimal tokens-to-parameter ratio, but overall,
Chinchilla's key results withstand sizable perturbations. Altogether, our
findings offer the field renewed confidence in Chinchilla as a durable guide
for scaling language models.

</details>


### [550] [Detecting and Rectifying Noisy Labels: A Similarity-based Approach](https://arxiv.org/abs/2509.23964)
*Dang Huu-Tien,Naoya Inoue*

Main category: cs.LG

TL;DR: Proposes post-hoc, model-agnostic methods using penultimate features to detect and rectify label errors in datasets by leveraging feature similarity patterns.


<details>
  <summary>Details</summary>
Motivation: Label noise damages neural network performance, and with growing network sizes, there's increasing need for automated error detection tools.

Method: Utilizes penultimate features from neural networks, observing that mislabeled data points have higher similarity to true class features than other classes, using probability of label occurrence in similar clusters for error detection and rectification.

Result: Extensive experiments show high performance across various noise types and successful automatic rectification of errors to improve dataset quality.

Conclusion: The proposed method effectively detects and rectifies label errors using penultimate feature similarities, demonstrating robust performance across different noise scenarios.

Abstract: Label noise in datasets could damage the performance of neural net training.
As the size of modern deep networks grows, there is a growing demand for
automated tools for detecting such errors. In this paper, we propose post-hoc,
model-agnostic error detection and rectification methods utilizing the
penultimate feature from a neural network. Our idea is based on the observation
that the similarity between the penultimate feature of a mislabeled data point
and its true class data points is higher than that for data points from other
classes, making the probability of label occurrence within a tight, similar
cluster informative for detecting and rectifying errors. Extensive experiments
show our method not only demonstrates high performance across various noises
but also automatically rectifies these errors to improve the quality of
datasets.

</details>


### [551] [Curriculum-Guided Reinforcement Learning for Synthesizing Gas-Efficient Financial Derivatives Contracts](https://arxiv.org/abs/2509.23976)
*Maruf Ahmed Mridul,Oshani Seneviratne*

Main category: cs.LG

TL;DR: A Reinforcement Learning framework generates gas-optimized Solidity smart contracts from CDM specifications, achieving up to 35.59% gas savings.


<details>
  <summary>Details</summary>
Motivation: Smart contract automation faces challenges in translating financial specifications into gas-efficient code, particularly from high-level models like CDM.

Method: Uses Proximal Policy Optimization (PPO) agent with two-phase curriculum: first learns functional correctness, then focuses on gas optimization by selecting optimal code snippets from a library.

Result: RL agent generates contracts with significant gas savings - up to 35.59% cost reduction on unseen test data compared to unoptimized baselines.

Conclusion: Presents a viable methodology for automated synthesis of reliable and economically sustainable smart contracts, bridging high-level financial agreements with efficient on-chain execution.

Abstract: Smart contract-based automation of financial derivatives offers substantial
efficiency gains, but its real-world adoption is constrained by the complexity
of translating financial specifications into gas-efficient executable code. In
particular, generating code that is both functionally correct and economically
viable from high-level specifications, such as the Common Domain Model (CDM),
remains a significant challenge. This paper introduces a Reinforcement Learning
(RL) framework to generate functional and gas-optimized Solidity smart
contracts directly from CDM specifications. We employ a Proximal Policy
Optimization (PPO) agent that learns to select optimal code snippets from a
pre-defined library. To manage the complex search space, a two-phase curriculum
first trains the agent for functional correctness before shifting its focus to
gas optimization. Our empirical results show the RL agent learns to generate
contracts with significant gas savings, achieving cost reductions of up to
35.59% on unseen test data compared to unoptimized baselines. This work
presents a viable methodology for the automated synthesis of reliable and
economically sustainable smart contracts, bridging the gap between high-level
financial agreements and efficient on-chain execution.

</details>


### [552] [Guide: Generalized-Prior and Data Encoders for DAG Estimation](https://arxiv.org/abs/2509.23992)
*Amartya Roy,Devharish N,Shreya Ganguly,Kripabandhu Ghosh*

Main category: cs.LG

TL;DR: GUIDE is a causal discovery framework that combines LLM-generated adjacency matrices with observational data using dual-encoder architecture, achieving 42% faster runtime and 117% higher accuracy than baselines while scaling to ≥70 nodes.


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods like PC, GES, and ICA-LiNGAM face limitations in scalability, computational efficiency, and handling mixed data types, especially beyond 70 nodes with high energy costs.

Method: Integrates LLM-generated adjacency matrices with observational data through dual-encoder architecture, using reinforcement learning to balance accuracy rewards and DAG constraint penalties.

Result: Reduces runtime by ≈42% compared to RL-BIC and KCRL, achieves ≈117% accuracy improvement over NOTEARS and GraN-DAG individually, and scales to ≥70 nodes where baseline methods fail.

Conclusion: GUIDE provides an efficient and scalable solution for causal discovery that overcomes limitations of traditional methods in handling mixed data types and large-scale problems.

Abstract: Modern causal discovery methods face critical limitations in scalability,
computational efficiency, and adaptability to mixed data types, as evidenced by
benchmarks on node scalability (30, $\le 50$, $\ge 70$ nodes), computational
energy demands, and continuous/non-continuous data handling. While traditional
algorithms like PC, GES, and ICA-LiNGAM struggle with these challenges,
exhibiting prohibitive energy costs for higher-order nodes and poor scalability
beyond 70 nodes, we propose \textbf{GUIDE}, a framework that integrates Large
Language Model (LLM)-generated adjacency matrices with observational data
through a dual-encoder architecture. GUIDE uniquely optimizes computational
efficiency, reducing runtime on average by $\approx 42%$ compared to RL-BIC and
KCRL methods, while achieving an average $\approx 117%$ improvement in accuracy
over both NOTEARS and GraN-DAG individually. During training, GUIDE's
reinforcement learning agent dynamically balances reward maximization
(accuracy) and penalty avoidance (DAG constraints), enabling robust performance
across mixed data types and scalability to $\ge 70$ nodes -- a setting where
baseline methods fail.

</details>


### [553] [Does Weak-to-strong Generalization Happen under Spurious Correlations?](https://arxiv.org/abs/2509.24005)
*Chenruo Liu,Yijun Dong,Qi Lei*

Main category: cs.LG

TL;DR: This paper studies weak-to-strong generalization in scenarios with spurious correlations from group imbalance, showing W2S succeeds when labeled and unlabeled data have same minority group proportions but fails when proportions differ, and proposes a simple retraining method to improve performance.


<details>
  <summary>Details</summary>
Motivation: To understand when weak-to-strong generalization works or fails in the presence of spurious correlations from group-imbalanced data, and develop methods to improve it when it fails.

Method: Theoretical analysis of W2S gain at proportional asymptotic limit, extensive experiments on spurious correlation benchmarks, and a simple algorithmic remedy that retrains the strong student on its high-confidence data subset after W2S fine-tuning.

Result: W2S always succeeds when η_u = η_ℓ with sufficient pseudolabels, but fails when η_u ≠ η_ℓ with gain diminishing as (η_u - η_ℓ)^2 increases. The proposed retraining method achieves consistent, substantial improvements over vanilla W2S fine-tuning.

Conclusion: Group imbalance between labeled and unlabeled data can cause W2S generalization failures, but a simple high-confidence retraining strategy can effectively boost performance without requiring group labels.

Abstract: We initiate a unified theoretical and algorithmic study of a key problem in
weak-to-strong (W2S) generalization: when fine-tuning a strong pre-trained
student with pseudolabels from a weaker teacher on a downstream task with
spurious correlations, does W2S happen, and how to improve it upon failures? We
consider two sources of spurious correlations caused by group imbalance: (i) a
weak teacher fine-tuned on group-imbalanced labeled data with a minority group
of fraction $\eta_\ell$, and (ii) a group-imbalanced unlabeled set
pseudolabeled by the teacher with a minority group of fraction $\eta_u$.
Theoretically, a precise characterization of W2S gain at the proportional
asymptotic limit shows that W2S always happens with sufficient pseudolabels
when $\eta_u = \eta_\ell$ but may fail when $\eta_u \ne \eta_\ell$, where W2S
gain diminishes as $(\eta_u - \eta_\ell)^2$ increases. Our theory is
corroborated by extensive experiments on various spurious correlation
benchmarks and teacher-student pairs. To boost W2S performance upon failures,
we further propose a simple, effective algorithmic remedy that retrains the
strong student on its high-confidence data subset after W2S fine-tuning. Our
algorithm is group-label-free and achieves consistent, substantial improvements
over vanilla W2S fine-tuning.

</details>


### [554] [SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention](https://arxiv.org/abs/2509.24006)
*Jintao Zhang,Haoxu Wang,Kai Jiang,Shuo Yang,Kaiwen Zheng,Haocheng Xi,Ziteng Wang,Hongzhou Zhu,Min Zhao,Ion Stoica,Joseph E. Gonzalez,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: SLA (Sparse-Linear Attention) accelerates Diffusion Transformer models by fusing sparse and linear attention, reducing attention computation by 95% while maintaining generation quality.


<details>
  <summary>Details</summary>
Motivation: Attention latency is a major bottleneck in Diffusion Transformer models for video generation due to long sequence lengths and quadratic complexity.

Method: SLA classifies attention weights into critical, marginal, and negligible categories, applying O(N²) attention to critical weights, O(N) attention to marginal weights, and skipping negligible ones, then combines these computations into a single GPU kernel.

Result: SLA achieves 20x reduction in attention computation, 13.7x speedup in attention computation, and 2.2x end-to-end speedup in video generation on Wan2.1-1.3B without degrading generation quality.

Conclusion: SLA effectively accelerates DiT models by leveraging the observation that attention weights can be separated into high-rank and low-rank components, enabling significant computation reduction while preserving performance.

Abstract: In Diffusion Transformer (DiT) models, particularly for video generation,
attention latency is a major bottleneck due to the long sequence length and the
quadratic complexity. We find that attention weights can be separated into two
parts: a small fraction of large weights with high rank and the remaining
weights with very low rank. This naturally suggests applying sparse
acceleration to the first part and low-rank acceleration to the second. Based
on this finding, we propose SLA (Sparse-Linear Attention), a trainable
attention method that fuses sparse and linear attention to accelerate diffusion
models. SLA classifies attention weights into critical, marginal, and
negligible categories, applying O(N^2) attention to critical weights, O(N)
attention to marginal weights, and skipping negligible ones. SLA combines these
computations into a single GPU kernel and supports both forward and backward
passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x
reduction in attention computation, resulting in significant acceleration
without loss of generation quality. Experiments show that SLA reduces attention
computation by 95% without degrading end-to-end generation quality,
outperforming baseline methods. In addition, we implement an efficient GPU
kernel for SLA, which yields a 13.7x speedup in attention computation and a
2.2x end-to-end speedup in video generation on Wan2.1-1.3B.

</details>


### [555] [Pretraining Scaling Laws for Generative Evaluations of Language Models](https://arxiv.org/abs/2509.24012)
*Rylan Schaeffer,Noam Levi,Brando Miranda,Sanmi Koyejo*

Main category: cs.LG

TL;DR: The paper proposes three scaling laws for generative evaluations (pass-at-k) and shows how generative evaluations offer new hyperparameters to control scaling law parameters and performance predictability.


<details>
  <summary>Details</summary>
Motivation: While much research has focused on scaling laws for pretraining losses and discriminative tasks, little work has been done on scaling laws for generative evaluations like mathematical problem-solving and software engineering.

Method: Three different pretraining scaling laws are proposed and evaluated: (1) compute-based, (2) model parameters and tokens, (3) log likelihoods of gold reference solutions. The study examines how generative evaluation hyperparameters (k) affect scaling law parameters and predictability.

Result: The compute and parameters+tokens scaling laws stabilize for the last ~1.5-2.5 orders of magnitude, while gold reference likelihood scaling law stabilizes for ~5 orders. All three scaling laws perform comparably, with minor differences: compute scaling predicts slightly worse for small k, and gold reference likelihood predicts slightly worse for large k.

Conclusion: The paper establishes a theoretical connection showing the compute scaling law emerges as the compute-optimal envelope of the parameters-and-tokens scaling law, providing researchers with methodologies to forecast generative performance.

Abstract: Neural scaling laws have played a central role in modern machine learning,
driving the field's ever-expanding scaling of parameters, data and compute.
While much research has gone into fitting scaling laws and predicting
performance on pretraining losses and on discriminative evaluations such as
multiple-choice question-answering, comparatively little research has been done
on fitting scaling laws and predicting performance on generative evaluations
such as mathematical problem-solving or software engineering. We propose and
evaluate three different pretraining scaling laws for fitting pass-at-$k$ on
generative evaluations and for predicting pass-at-$k$ of the most expensive
model using the performance of cheaper models. Our three scaling laws differ in
the covariates used: (1) compute, (2) model parameters and tokens, (3) log
likelihoods of gold reference solutions. We make four main contributions: (1)
We show how generative evaluations offer new hyperparameters (in our setting,
$k$) that researchers can use to control the scaling laws parameters and the
predictability of performance. (2) In terms of scaling law parameters, we find
that the compute scaling law and parameters\,+\,tokens scaling law stabilize
for the last ~$1.5{-}2.5$ orders of magnitude, whereas the gold reference
likelihood scaling law stabilizes for the last ~$5$ orders of magnitude. (3) In
terms of predictive performance, we find all three scaling laws perform
comparably, although the compute scaling law predicts slightly worse for small
$k$ and the log likelihoods of gold reference solutions predicts slightly worse
for large $k$. (4) We establish a theoretical connection that the compute
scaling law emerges as the compute-optimal envelope of the
parameters-and-tokens scaling law. Our framework provides researchers and
practitioners with insights and methodologies to forecast generative
performance.

</details>


### [556] [GPS-MTM: Capturing Pattern of Normalcy in GPS-Trajectories with self-supervised learning](https://arxiv.org/abs/2509.24031)
*Umang Garg,Bowen Zhang,Anantanjit Subrahmanya,Chandrakanth Gudavalli,BS Manjunath*

Main category: cs.LG

TL;DR: GPS-MTM is a foundation model for mobility data that decomposes trajectories into states and actions, using masked modeling to learn semantic patterns without labels. It outperforms benchmarks on trajectory tasks.


<details>
  <summary>Details</summary>
Motivation: Foundation models have shown success in text, vision, and video, but trajectory modeling lacks similar breakthroughs. The paper aims to create a foundation model for large-scale mobility data to capture normal human movement patterns.

Method: GPS-MTM decomposes mobility into states (POI categories) and actions (transitions), using a bi-directional Transformer with self-supervised masked modeling to reconstruct missing segments across modalities without manual labels.

Result: GPS-MTM consistently outperforms on benchmark datasets (Numosim-LA, Urban Anomalies, Geolife) for trajectory infilling and next-stop prediction, with strongest advantages in dynamic tasks requiring contextual reasoning.

Conclusion: GPS-MTM establishes itself as a robust foundation model for trajectory analytics, positioning mobility data as a first-class modality for large-scale representation learning.

Abstract: Foundation models have driven remarkable progress in text, vision, and video
understanding, and are now poised to unlock similar breakthroughs in trajectory
modeling. We introduce the GPSMasked Trajectory Transformer (GPS-MTM), a
foundation model for large-scale mobility data that captures patterns of
normalcy in human movement. Unlike prior approaches that flatten trajectories
into coordinate streams, GPS-MTM decomposes mobility into two complementary
modalities: states (point-of-interest categories) and actions (agent
transitions). Leveraging a bi-directional Transformer with a self-supervised
masked modeling objective, the model reconstructs missing segments across
modalities, enabling it to learn rich semantic correlations without manual
labels. Across benchmark datasets, including Numosim-LA, Urban Anomalies, and
Geolife, GPS-MTM consistently outperforms on downstream tasks such as
trajectory infilling and next-stop prediction. Its advantages are most
pronounced in dynamic tasks (inverse and forward dynamics), where contextual
reasoning is critical. These results establish GPS-MTM as a robust foundation
model for trajectory analytics, positioning mobility data as a first-class
modality for large-scale representation learning. Code is released for further
reference.

</details>


### [557] [Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.24047)
*Runyu Zhang,Na Li,Asuman Ozdaglar,Jeff Shamma,Gioele Zardini*

Main category: cs.LG

TL;DR: This paper proposes a principled framework that interprets risk-seeking objectives as optimism in multi-agent reinforcement learning (MARL), introducing optimistic value functions and developing decentralized optimistic actor-critic algorithms that improve coordination over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing optimistic methods in cooperative MARL are typically heuristic and lack theoretical grounding, while risk-averse approaches often lead to suboptimal equilibria. The authors aim to provide a principled framework that unifies risk-sensitive learning and optimism.

Method: Building on dual representation for convex risk measures, the authors propose optimistic value functions that formalize optimism as divergence-penalized risk-seeking evaluations. They derive a policy-gradient theorem for these functions and develop decentralized optimistic actor-critic algorithms.

Result: Empirical results on cooperative benchmarks demonstrate that risk-seeking optimism consistently improves coordination over both risk-neutral baselines and heuristic optimistic methods.

Conclusion: The framework unifies risk-sensitive learning and optimism, offering a theoretically grounded and practically effective approach to cooperation in MARL.

Abstract: Risk sensitivity has become a central theme in reinforcement learning (RL),
where convex risk measures and robust formulations provide principled ways to
model preferences beyond expected return. Recent extensions to multi-agent RL
(MARL) have largely emphasized the risk-averse setting, prioritizing robustness
to uncertainty. In cooperative MARL, however, such conservatism often leads to
suboptimal equilibria, and a parallel line of work has shown that optimism can
promote cooperation. Existing optimistic methods, though effective in practice,
are typically heuristic and lack theoretical grounding. Building on the dual
representation for convex risk measures, we propose a principled framework that
interprets risk-seeking objectives as optimism. We introduce optimistic value
functions, which formalize optimism as divergence-penalized risk-seeking
evaluations. Building on this foundation, we derive a policy-gradient theorem
for optimistic value functions, including explicit formulas for the entropic
risk/KL-penalty setting, and develop decentralized optimistic actor-critic
algorithms that implement these updates. Empirical results on cooperative
benchmarks demonstrate that risk-seeking optimism consistently improves
coordination over both risk-neutral baselines and heuristic optimistic methods.
Our framework thus unifies risk-sensitive learning and optimism, offering a
theoretically grounded and practically effective approach to cooperation in
MARL.

</details>


### [558] [Collaborative Device-Cloud LLM Inference through Reinforcement Learning](https://arxiv.org/abs/2509.24050)
*Wenzhi Fang,Dong-Jun Han,Liangqi Yuan,Christopher Brinton*

Main category: cs.LG

TL;DR: Proposes a device-cloud collaboration framework where on-device LLMs make routing decisions after problem-solving, using post-training with reward maximization and group-adaptive policy gradient to optimize performance while controlling cloud usage.


<details>
  <summary>Details</summary>
Motivation: Existing binary classifier routers struggle to determine task difficulty from prompt surface patterns, leading to suboptimal routing decisions in device-cloud collaboration scenarios.

Method: Framework where on-device LLM makes routing decisions after solving process, using post-training with reward maximization problem and group-adaptive policy gradient algorithm with adaptive prompt filtering.

Result: Extensive experiments show consistent outperformance over existing baselines and significant narrowing of gap to full cloud LLM performance across models and benchmarks.

Conclusion: The proposed methodology effectively addresses limitations of existing routing approaches and demonstrates superior performance in device-cloud collaboration for LLM deployment.

Abstract: Device-cloud collaboration has emerged as a promising paradigm for deploying
large language models (LLMs), combining the efficiency of lightweight on-device
inference with the superior performance of powerful cloud LLMs. An essential
problem in this scenario lies in deciding whether a given query is best handled
locally or delegated to the cloud. Existing approaches typically rely on
external routers, implemented as binary classifiers, which often struggle to
determine task difficulty from the prompt's surface pattern. To address these
limitations, we propose a framework where the on-device LLM makes routing
decisions at the end of its solving process, with this capability instilled
through post-training. In particular, we formulate a reward maximization
problem with carefully designed rewards that encourage effective problem
solving and judicious offloading to the cloud. To solve this problem, we
develop a group-adaptive policy gradient algorithm, featuring a group-level
policy gradient, designed to yield an unbiased gradient estimator of the
reward, and adaptive prompt filtering, developed to enforce the constraint on
cloud LLM usage. Extensive experiments across models and benchmarks show that
the proposed methodology consistently outperforms existing baselines and
significantly narrows the gap to full cloud LLM performance.

</details>


### [559] [On The Variability of Concept Activation Vectors](https://arxiv.org/abs/2509.24058)
*Julia Wenkmann,Damien Garreau*

Main category: cs.LG

TL;DR: This paper provides a theoretical analysis of Concept Activation Vectors (CAVs) variability, showing that their variance decreases as 1/N where N is the number of random examples, and offers practical recommendations for efficient application.


<details>
  <summary>Details</summary>
Motivation: To address the variability in Concept Activation Vectors (CAVs) that arises from random sampling during their computation, which can lead to different explanations for different users.

Method: The authors conduct a fine-grained theoretical analysis of CAVs construction to quantify their variability, supported by experiments on several real-life datasets.

Result: The analysis reveals a universal result: the variance of CAVs decreases as 1/N, where N is the number of random examples used in their computation.

Conclusion: Based on the theoretical findings, the paper provides practical recommendations for resource-efficient application of the CAVs method in explainable AI.

Abstract: One of the most pressing challenges in artificial intelligence is to make
models more transparent to their users. Recently, explainable artificial
intelligence has come up with numerous method to tackle this challenge. A
promising avenue is to use concept-based explanations, that is, high-level
concepts instead of plain feature importance score. Among this class of
methods, Concept Activation vectors (CAVs), Kim et al. (2018) stands out as one
of the main protagonists. One interesting aspect of CAVs is that their
computation requires sampling random examples in the train set. Therefore, the
actual vectors obtained may vary from user to user depending on the randomness
of this sampling. In this paper, we propose a fine-grained theoretical analysis
of CAVs construction in order to quantify their variability. Our results,
confirmed by experiments on several real-life datasets, point out towards an
universal result: the variance of CAVs decreases as $1/N$, where $N$ is the
number of random examples. Based on this we give practical recommendations for
a resource-efficient application of the method.

</details>


### [560] [In-Context Compositional Q-Learning for Offline Reinforcement Learning](https://arxiv.org/abs/2509.24067)
*Qiushui Xu,Yuhao Huang,Yushu Jiang,Lei Song,Jinyu Wang,Wenliang Zheng,Jiang Bian*

Main category: cs.LG

TL;DR: ICQL is an offline RL framework that formulates Q-learning as contextual inference using linear Transformers to adaptively infer local Q-functions from retrieved transitions, achieving improved performance in compositional tasks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches rely on single global Q-functions that struggle to capture compositional nature of tasks with diverse subtasks, limiting performance in offline RL settings.

Method: Uses linear Transformers to formulate Q-learning as contextual inference problem, adaptively inferring local Q-functions from retrieved transitions without explicit subtask labels.

Result: Improves performance in kitchen tasks by up to 16.4%, Gym tasks by 8.6%, and Adroit tasks by 6.3% compared to existing methods.

Conclusion: ICQL demonstrates the potential of in-context learning for robust and compositional value estimation, providing a principled and effective framework for offline RL.

Abstract: Accurately estimating the Q-function is a central challenge in offline
reinforcement learning. However, existing approaches often rely on a single
global Q-function, which struggles to capture the compositional nature of tasks
involving diverse subtasks. We propose In-context Compositional Q-Learning
(\texttt{ICQL}), the first offline RL framework that formulates Q-learning as a
contextual inference problem, using linear Transformers to adaptively infer
local Q-functions from retrieved transitions without explicit subtask labels.
Theoretically, we show that under two assumptions--linear approximability of
the local Q-function and accurate weight inference from retrieved
context--\texttt{ICQL} achieves bounded Q-function approximation error, and
supports near-optimal policy extraction. Empirically, \texttt{ICQL}
substantially improves performance in offline settings: improving performance
in kitchen tasks by up to 16.4\%, and in Gym and Adroit tasks by up to 8.6\%
and 6.3\%. These results highlight the underexplored potential of in-context
learning for robust and compositional value estimation, positioning
\texttt{ICQL} as a principled and effective framework for offline RL.

</details>


### [561] [A Small Math Model: Recasting Strategy Choice Theory in an LLM-Inspired Architecture](https://arxiv.org/abs/2509.24068)
*Roussel Rahman,Jeff Shrager*

Main category: cs.LG

TL;DR: This paper recasts Strategy Choice Theory (SCT) as a "Small Math Model" (SMM) using neural-network architecture similar to LLMs, extending SCT with counting practice, number embedding, and gated attention.


<details>
  <summary>Details</summary>
Motivation: To create a unified platform for investigating numerical understanding and mathematical reasoning in LLM-based agents by extending the decades-long SCT program with modern neural network approaches.

Method: Developed a neural-network-based Small Math Model (SMM) that incorporates counting practice, symbol embedding, and gated attention mechanisms, building upon Strategy Choice Theory principles.

Result: The SMM demonstrates constructive and destructive interference between counting and addition, and shows wave-like use of finger-counting as sum recall improves, similar to earlier SCT findings.

Conclusion: The SMM provides a foundation for extending to later aspects of SCT including adaptive strategy choice and strategy discovery, offering a unified approach to study mathematical reasoning emergence in LLM-based systems.

Abstract: Strategy Choice Theory (SCT)\footnote{``Strategy Choice Theory'',
``Distributions of Associations'', and ``Overlapping Wave Theory'' have been
used to refer to this line of work, emphasizing different
aspects.}\citep[e.g.,][]{siegler1984strategychoices, siegler2000rebirth}
explains important aspects of children's arithmetic learning based upon
principles including learning from developmentally naturalistic data,
probabilistic representation, confidence-based retrieval, and the phase-like
importance of scaffolding strategies, such as finger-counting. Here we recast
SCT as a ``Small Math Model'' (SMM), employing a neural-network-based
architecture analogous to LLMs. The SMM extends SCT to include counting
practice\footnote{The original SCT model was pre-biased in accordance with the
supposed experience of counting.}, symbol (number) embedding, and gated
attention. Similar to earlier work, the SMM demonstrates constructive and
destructive interference between counting and addition, and the ``wave-like''
use of finger-counting as sum recall improves. We plan to extend the SMM to
later aspects of the decades-long SCT program, including adaptive strategy
choice and eventually strategy discovery, providing a unified platform to
investigate the understanding of numerical characteristics and relationships
essential for mathematical reasoning -- as it can emerge in LLM-based agents.

</details>


### [562] [AQUAIR: A High-Resolution Indoor Environmental Quality Dataset for Smart Aquaculture Monitoring](https://arxiv.org/abs/2509.24069)
*Youssef Sabiri,Walid Houmaidi,Ouail El Maadi,Yousra Chtouki*

Main category: cs.LG

TL;DR: AQUAIR is an open-access public dataset of indoor environmental quality variables from a fish aquaculture facility in Morocco, providing over 23,000 quality-controlled observations for smart aquaculture research.


<details>
  <summary>Details</summary>
Motivation: Public datasets describing air conditions in indoor aquaculture tanks are scarce, limiting development of forecasting and anomaly-detection tools that connect head-space conditions with water-quality dynamics.

Method: Used a single Awair HOME monitor to sample six IEQ variables (air temperature, humidity, CO2, TVOCs, PM2.5, PM10) every 5 minutes from October 2024 to January 2025, with ISO-compliant mounting, calibration checks, and an open-source processing pipeline.

Result: Dataset contains over 23,000 time-stamped observations showing stable conditions with feeding-time peaks, suitable for short-horizon forecasting, event detection, and sensor drift studies.

Conclusion: AQUAIR fills a critical gap in smart aquaculture informatics and provides a reproducible benchmark for machine learning curricula and environmental sensing research focused on head-space dynamics in recirculating aquaculture systems.

Abstract: Smart aquaculture systems depend on rich environmental data streams to
protect fish welfare, optimize feeding, and reduce energy use. Yet public
datasets that describe the air surrounding indoor tanks remain scarce, limiting
the development of forecasting and anomaly-detection tools that couple
head-space conditions with water-quality dynamics. We therefore introduce
AQUAIR, an open-access public dataset that logs six Indoor Environmental
Quality (IEQ) variables--air temperature, relative humidity, carbon dioxide,
total volatile organic compounds, PM2.5 and PM10--inside a fish aquaculture
facility in Amghass, Azrou, Morocco. A single Awair HOME monitor sampled every
five minutes from 14 October 2024 to 9 January 2025, producing more than 23,000
time-stamped observations that are fully quality-controlled and publicly
archived on Figshare. We describe the sensor placement, ISO-compliant mounting
height, calibration checks against reference instruments, and an open-source
processing pipeline that normalizes timestamps, interpolates short gaps, and
exports analysis-ready tables. Exploratory statistics show stable conditions
(median CO2 = 758 ppm; PM2.5 = 12 micrograms/m3) with pronounced feeding-time
peaks, offering rich structure for short-horizon forecasting, event detection,
and sensor drift studies. AQUAIR thus fills a critical gap in smart aquaculture
informatics and provides a reproducible benchmark for data-centric machine
learning curricula and environmental sensing research focused on head-space
dynamics in recirculating aquaculture systems.

</details>


### [563] [A Family of Kernelized Matrix Costs for Multiple-Output Mixture Neural Networks](https://arxiv.org/abs/2509.24076)
*Bo Hu,José C. Príncipe*

Main category: cs.LG

TL;DR: This paper proposes combining Mixture Density Networks (MDNs) with contrastive costs using four kernelized matrix costs for data density approximation and multi-center learning.


<details>
  <summary>Details</summary>
Motivation: To enhance self-supervised and contrastive feature learning by integrating pairwise distance-based costs with MDNs for better density approximation.

Method: Combines MDNs with contrastive costs using four types of kernelized matrix costs: scalar cost, vector-matrix cost, matrix-matrix cost (trace of Schur complement), and SVD cost (nuclear norm).

Result: The approach enables learning multiple centers required to define mixture densities through various kernelized cost formulations.

Conclusion: The proposed integration of MDNs with contrastive costs using kernelized matrix formulations provides an effective framework for data density approximation in self-supervised learning.

Abstract: Pairwise distance-based costs are crucial for self-supervised and contrastive
feature learning. Mixture Density Networks (MDNs) are a widely used approach
for generative models and density approximation, using neural networks to
produce multiple centers that define a Gaussian mixture. By combining MDNs with
contrastive costs, this paper proposes data density approximation using four
types of kernelized matrix costs: the scalar cost, the vector-matrix cost, the
matrix-matrix cost (the trace of Schur complement), and the SVD cost (the
nuclear norm), for learning multiple centers required to define a mixture
density.

</details>


### [564] [Demographic-Agnostic Fairness without Harm](https://arxiv.org/abs/2509.24077)
*Zhongteng Cai,Mohammad Mahdi Khalili,Xueru Zhang*

Main category: cs.LG

TL;DR: The paper proposes a demographic-agnostic fairness optimization algorithm that jointly learns group classifiers and decoupled classifiers to achieve preference-based fairness without requiring demographic information during training.


<details>
  <summary>Details</summary>
Motivation: Current fairness approaches either sacrifice accuracy (parity-based) or require demographic information (preference-based), which is problematic for high-stakes domains like healthcare where both accuracy and fairness are crucial.

Method: Proposed DAFH algorithm that jointly learns a group classifier to partition population into multiple groups and a set of decoupled classifiers associated with these groups, without requiring demographic information.

Result: Theoretical analysis shows the method outperforms baselines when demographic information is known, and experiments on synthetic and real data validate the proposed approach.

Conclusion: The demographic-agnostic approach successfully achieves preference-based fairness without requiring demographic information, maintaining accuracy while ensuring fairness in ML predictions.

Abstract: As machine learning (ML) algorithms are increasingly used in social domains
to make predictions about humans, there is a growing concern that these
algorithms may exhibit biases against certain social groups. Numerous notions
of fairness have been proposed in the literature to measure the unfairness of
ML. Among them, one class that receives the most attention is
\textit{parity-based}, i.e., achieving fairness by equalizing treatment or
outcomes for different social groups. However, achieving parity-based fairness
often comes at the cost of lowering model accuracy and is undesirable for many
high-stakes domains like healthcare. To avoid inferior accuracy, a line of
research focuses on \textit{preference-based} fairness, under which any group
of individuals would experience the highest accuracy and collectively prefer
the ML outcomes assigned to them if they were given the choice between various
sets of outcomes. However, these works assume individual demographic
information is known and fully accessible during training. In this paper, we
relax this requirement and propose a novel \textit{demographic-agnostic
fairness without harm (DAFH)} optimization algorithm, which jointly learns a
group classifier that partitions the population into multiple groups and a set
of decoupled classifiers associated with these groups. Theoretically, we
conduct sample complexity analysis and show that our method can outperform the
baselines when demographic information is known and used to train decoupled
classifiers. Experiments on both synthetic and real data validate the proposed
method.

</details>


### [565] [PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM](https://arxiv.org/abs/2509.24085)
*Ju-Hyung Lee,Yanqing Lu,Klaus Doppler*

Main category: cs.LG

TL;DR: PEARL is a framework using on-device LLMs for cooperative D2D communication optimization, improving Wi-Fi Aware parameter selection through peer-aware context and achieving up to 16% energy reduction.


<details>
  <summary>Details</summary>
Motivation: To extend single-device on-device LLMs to cooperative scenarios by leveraging both publisher and subscriber states for better cross-layer optimization in D2D communication.

Method: Uses context-aware reward (normalizing latency by application tolerances and modulating energy by battery states) for KL-based finetuning. Two variants: PEARL (Head + LoRA) and PEARL-Lite (Head-only).

Result: PEARL improves objective scores over heuristic and compact model baselines, reduces energy by up to 16% in cooperative low-battery cases, with PEARL-Lite achieving sub-20 ms inference.

Conclusion: Peer-aware context, reward-aligned training, and head-based efficiency make LLMs practical for always-on, on-device cross-layer control in D2D communication.

Abstract: We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a
framework for cooperative cross-layer optimization in device-to-device (D2D)
communication. Building on our previous work on single-device on-device LLMs,
PEARL extends the paradigm by leveraging both publisher and subscriber states
to guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which
normalizes latency by application tolerances and modulates energy by device
battery states, provides richer supervision for KL-based finetuning. We study
two lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves
the best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms
inference at near-identical objective scores. Across synthetic scenarios
grounded in real measurements, PEARL improves objective scores over heuristic
and compact model baselines and reduces energy by up to 16% in cooperative
low-battery cases. These results demonstrate that peer-aware context,
reward-aligned training, and head-based efficiency make LLMs practical for
always-on, on-device cross-layer control.

</details>


### [566] [Clebsch-Gordan Transformer: Fast and Global Equivariant Attention](https://arxiv.org/abs/2509.24093)
*Owen Lewis Howell,Linfeng Zhao,Xupeng Zhu,Yaoyao Qian,Haojie Huang,Lingfeng Sun,Wil Thomason,Robert Platt,Robin Walters*

Main category: cs.LG

TL;DR: The paper proposes Clebsch-Gordan Transformer, an efficient equivariant transformer that achieves global attention with O(N log N) complexity using Clebsch-Gordon Convolution on SO(3) irreducible representations.


<details>
  <summary>Details</summary>
Motivation: Existing equivariant transformers suffer from quadratic computational costs and limited support for high-order equivariant features, restricting their expressiveness and performance in geometric tasks.

Method: Uses Clebsch-Gordon Convolution on SO(3) irreducible representations to enable efficient global attention, exploits sparsity of Clebsch-Gordon matrix for scalability with high-order features, and incorporates optional token permutation equivariance.

Result: Achieves O(N log N) complexity, scales well with high-order irreducible features, and shows clear gains in GPU memory size, speed, and accuracy across n-body simulation, QM9, ModelNet, and robotic grasping benchmarks.

Conclusion: The Clebsch-Gordan Transformer provides an efficient solution for equivariant modeling with global attention, overcoming limitations of existing methods while maintaining superior performance across diverse geometric tasks.

Abstract: The global attention mechanism is one of the keys to the success of
transformer architecture, but it incurs quadratic computational costs in
relation to the number of tokens. On the other hand, equivariant models, which
leverage the underlying geometric structures of problem instance, often achieve
superior accuracy in physical, biochemical, computer vision, and robotic tasks,
at the cost of additional compute requirements. As a result, existing
equivariant transformers only support low-order equivariant features and local
context windows, limiting their expressiveness and performance. This work
proposes Clebsch-Gordan Transformer, achieving efficient global attention by a
novel Clebsch-Gordon Convolution on $\SO(3)$ irreducible representations. Our
method enables equivariant modeling of features at all orders while achieving
${O}(N \log N)$ input token complexity. Additionally, the proposed method
scales well with high-order irreducible features, by exploiting the sparsity of
the Clebsch-Gordon matrix. Lastly, we also incorporate optional token
permutation equivariance through either weight sharing or data augmentation. We
benchmark our method on a diverse set of benchmarks including n-body
simulation, QM9, ModelNet point cloud classification and a robotic grasping
dataset, showing clear gains over existing equivariant transformers in GPU
memory size, speed, and accuracy.

</details>


### [567] [ADAPT: Lightweight, Long-Range Machine Learning Force Fields Without Graphs](https://arxiv.org/abs/2509.24115)
*Evan Dramko,Yihuang Xiong,Yizhi Zhu,Geoffroy Hautier,Thomas Reps,Christopher Jermaine,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: ADAPT is a Transformer-based MLFF that replaces graph representations with direct coordinates and explicit pairwise interactions, achieving 33% lower errors than GNN models for silicon point defects.


<details>
  <summary>Details</summary>
Motivation: Traditional first-principles methods are computationally expensive for defect studies, and existing GNN-based MLFFs suffer from oversmoothing and poor long-range interaction modeling, which are critical for point defects.

Method: ADAPT uses a Transformer encoder architecture where atoms are treated as tokens, employs direct coordinates-in-space formulation instead of graphs, and explicitly considers all pairwise atomic interactions.

Result: ADAPT achieves approximately 33% reduction in both force and energy prediction errors compared to state-of-the-art GNN models, while requiring significantly less computational cost.

Conclusion: The ADAPT framework provides a more effective and efficient alternative to GNN-based MLFFs for modeling point defects, addressing key limitations in oversmoothing and long-range interaction representation.

Abstract: Point defects play a central role in driving the properties of materials.
First-principles methods are widely used to compute defect energetics and
structures, including at scale for high-throughput defect databases. However,
these methods are computationally expensive, making machine-learning force
fields (MLFFs) an attractive alternative for accelerating structural
relaxations. Most existing MLFFs are based on graph neural networks (GNNs),
which can suffer from oversmoothing and poor representation of long-range
interactions. Both of these issues are especially of concern when modeling
point defects. To address these challenges, we introduce the Accelerated Deep
Atomic Potential Transformer (ADAPT), an MLFF that replaces graph
representations with a direct coordinates-in-space formulation and explicitly
considers all pairwise atomic interactions. Atoms are treated as tokens, with a
Transformer encoder modeling their interactions. Applied to a dataset of
silicon point defects, ADAPT achieves a roughly 33 percent reduction in both
force and energy prediction errors relative to a state-of-the-art GNN-based
model, while requiring only a fraction of the computational cost.

</details>


### [568] [GeoFunFlow: Geometric Function Flow Matching for Inverse Operator Learning over Complex Geometries](https://arxiv.org/abs/2509.24117)
*Sifan Wang,Zhikai Wu,David van Dijk,Lu Lu*

Main category: cs.LG

TL;DR: GeoFunFlow is a geometric diffusion model framework for solving inverse problems on complex geometries, combining a geometric function autoencoder with latent diffusion to achieve state-of-the-art reconstruction accuracy and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Inverse problems governed by PDEs are challenging due to ill-posedness, data sparsity, and irregular geometries. Classical methods are computationally expensive, while existing learning-based approaches are limited to regular domains or forward modeling.

Method: GeoFunFlow combines a geometric function autoencoder (GeoFAE) using Perceiver modules to process unstructured meshes with a latent diffusion model trained via rectified flow for posterior sampling from sparse noisy data.

Result: Across five benchmarks, GeoFunFlow achieves state-of-the-art reconstruction accuracy over complex geometries, provides calibrated uncertainty quantification, and delivers efficient inference compared to operator-learning and diffusion model baselines.

Conclusion: The proposed framework successfully addresses inverse problems on complex geometries with improved accuracy, uncertainty quantification, and computational efficiency over existing methods.

Abstract: Inverse problems governed by partial differential equations (PDEs) are
crucial in science and engineering. They are particularly challenging due to
ill-posedness, data sparsity, and the added complexity of irregular geometries.
Classical PDE-constrained optimization methods are computationally expensive,
especially when repeated posterior sampling is required. Learning-based
approaches improve efficiency and scalability, yet most are designed for
regular domains or focus on forward modeling. Here, we introduce {\em
GeoFunFlow}, a geometric diffusion model framework for inverse problems on
complex geometries. GeoFunFlow combines a novel geometric function autoencoder
(GeoFAE) and a latent diffusion model trained via rectified flow. GeoFAE
employs a Perceiver module to process unstructured meshes of varying sizes and
produces continuous reconstructions of physical fields, while the diffusion
model enables posterior sampling from sparse and noisy data. Across five
benchmarks, GeoFunFlow achieves state-of-the-art reconstruction accuracy over
complex geometries, provides calibrated uncertainty quantification, and
delivers efficient inference compared to operator-learning and diffusion model
baselines.

</details>


### [569] [HyMaTE: A Hybrid Mamba and Transformer Model for EHR Representation Learning](https://arxiv.org/abs/2509.24118)
*Md Mozaharul Mottalib,Thao-Ly T. Phan,Rahmatollah Beheshti*

Main category: cs.LG

TL;DR: HyMaTE is a hybrid model combining State Space Models (Mamba) and Transformers for EHR representation learning, addressing limitations of both approaches to handle long, multivariate EHR sequences more efficiently while capturing richer representations.


<details>
  <summary>Details</summary>
Motivation: EHR data presents challenges with long sequences, sparsity, and missing values. Transformers have quadratic complexity limitations, while SSMs like Mamba focus mainly on sequence-level mixing rather than channel-level data. A hybrid approach is needed to overcome these limitations.

Method: Proposed HyMaTE - a hybrid model combining State Space Models (specifically Mamba) with attention mechanisms from Transformers, designed specifically for longitudinal EHR data representation learning.

Result: The model was tested on multiple clinical datasets for predictive tasks and demonstrated ability to capture effective, richer, and more nuanced unified representations of EHR data. The self-attention mechanism also provided interpretable outcomes.

Conclusion: HyMaTE provides a scalable and generalizable solution for real-world healthcare applications, effectively combining the linear-time efficiency of SSMs with the representational power of attention mechanisms.

Abstract: Electronic health Records (EHRs) have become a cornerstone in modern-day
healthcare. They are a crucial part for analyzing the progression of patient
health; however, their complexity, characterized by long, multivariate
sequences, sparsity, and missing values poses significant challenges in
traditional deep learning modeling. While Transformer-based models have
demonstrated success in modeling EHR data and predicting clinical outcomes,
their quadratic computational complexity and limited context length hinder
their efficiency and practical applications. On the other hand, State Space
Models (SSMs) like Mamba present a promising alternative offering linear-time
sequence modeling and improved efficiency for handling long sequences, but
focus mostly on mixing sequence-level information rather than channel-level
data. To overcome these challenges, we propose HyMaTE (A Hybrid Mamba and
Transformer Model for EHR Representation Learning), a novel hybrid model
tailored for representing longitudinal data, combining the strengths of SSMs
with advanced attention mechanisms. By testing the model on predictive tasks on
multiple clinical datasets, we demonstrate HyMaTE's ability to capture an
effective, richer, and more nuanced unified representation of EHR data.
Additionally, the interpretability of the outcomes achieved by self-attention
illustrates the effectiveness of our model as a scalable and generalizable
solution for real-world healthcare applications. Codes are available at:
https://github.com/healthylaife/HyMaTE.

</details>


### [570] [Echo Flow Networks](https://arxiv.org/abs/2509.24122)
*Hongbo Liu,Jia Xu*

Main category: cs.LG

TL;DR: Echo Flow Networks (EFNs) introduce a novel reservoir computing framework that combines extended Echo State Networks with MLP readouts and Matrix-Gated Composite Random Activation, achieving superior time-series forecasting performance with significantly improved efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental challenge in time-series forecasting of capturing long-range temporal dependencies efficiently, overcoming the limitations of conventional architectures and traditional Echo State Networks which face computational complexity vs. information retention trade-offs.

Method: Proposed Echo Flow Networks framework with extended Echo State Networks (X-ESNs) using MLP readouts, enhanced by Matrix-Gated Composite Random Activation (MCRA) for complex neuron-specific temporal dynamics, and a dual-stream architecture that dynamically selects reservoir features from infinite-horizon memory.

Result: EFNs achieve up to 4x faster training and 3x smaller model size compared to leading methods like PatchTST, reducing forecasting error from 43% to 35% (20% relative improvement). EchoFormer, an instantiation of EFNs, achieves state-of-the-art performance across five benchmark datasets.

Conclusion: EFNs provide an efficient and effective solution for long-range time-series forecasting, significantly expanding representational capacity while maintaining computational efficiency, demonstrating superior performance across multiple benchmarks.

Abstract: At the heart of time-series forecasting (TSF) lies a fundamental challenge:
how can models efficiently and effectively capture long-range temporal
dependencies across ever-growing sequences? While deep learning has brought
notable progress, conventional architectures often face a trade-off between
computational complexity and their ability to retain accumulative information
over extended horizons.
  Echo State Networks (ESNs), a class of reservoir computing models, have
recently regained attention for their exceptional efficiency, offering constant
memory usage and per-step training complexity regardless of input length. This
makes them particularly attractive for modeling extremely long-term event
history in TSF. However, traditional ESNs fall short of state-of-the-art
performance due to their limited nonlinear capacity, which constrains both
their expressiveness and stability.
  We introduce Echo Flow Networks (EFNs), a framework composed of a group of
extended Echo State Networks (X-ESNs) with MLP readouts, enhanced by our novel
Matrix-Gated Composite Random Activation (MCRA), which enables complex,
neuron-specific temporal dynamics, significantly expanding the network's
representational capacity without compromising computational efficiency. In
addition, we propose a dual-stream architecture in which recent input history
dynamically selects signature reservoir features from an infinite-horizon
memory, leading to improved prediction accuracy and long-term stability.
  Extensive evaluations on five benchmarks demonstrate that EFNs achieve up to
4x faster training and 3x smaller model size compared to leading methods like
PatchTST, reducing forecasting error from 43% to 35%, a 20% relative
improvement. One instantiation of our framework, EchoFormer, consistently
achieves new state-of-the-art performance across five benchmark datasets: ETTh,
ETTm, DMV, Weather, and Air Quality.

</details>


### [571] [The Impossibility of Inverse Permutation Learning in Transformer Models](https://arxiv.org/abs/2509.24125)
*Rohan Alur,Chris Hays,Manish Raghavan,Devavrat Shah*

Main category: cs.LG

TL;DR: Decoder-only transformers cannot learn inverse permutation tasks, but become capable with causal attention masks or scratch token padding, suggesting alternative reasoning mechanisms in LLMs.


<details>
  <summary>Details</summary>
Motivation: To study the robustness property of transformer models across reasoning tasks like long-context retrieval and multiple choice QA through inverse permutation learning.

Method: Analyze the expressive capacity of decoder-only transformers for inverse permutation learning, examine alternative constructions using causal attention masks and scratch token padding.

Result: Proved impossibility of inverse permutation learning in arbitrary depth decoder-only transformers, but showed feasibility with causal attention masks or input padding with scratch tokens.

Conclusion: There's an expressivity gap between encoder-decoder and decoder-only transformers, and scratch tokens may enable reasoning in LLMs even without semantic meaning, similar to chain-of-thought prompting.

Abstract: In this technical note, we study the problem of inverse permutation learning
in decoder-only transformers. Given a permutation and a string to which that
permutation has been applied, the model is tasked with producing the original
(``canonical'') string. We argue that this task models a natural robustness
property across a variety of reasoning tasks, including long-context retrieval,
multiple choice QA and in-context learning. Our primary contribution is an
impossibility result: we show that an arbitrary depth, decoder-only transformer
cannot learn this task. This result concerns the expressive capacity of
decoder-only transformer models and is agnostic to training dynamics or sample
complexity. We give a pair of alternative constructions under which inverse
permutation learning is feasible. The first of these highlights the fundamental
role of the causal attention mask, and reveals a gap between the expressivity
of encoder-decoder transformers and the more popular decoder-only architecture.
The latter result is more surprising: we show that simply padding the input
with ``scratch tokens" yields a construction under which inverse permutation
learning is possible. We conjecture that this may suggest an alternative
mechanism by which chain-of-thought prompting or, more generally, intermediate
``thinking'' tokens can enable reasoning in large language models, even when
these tokens encode no meaningful semantic information (e.g., the results of
intermediate computations).

</details>


### [572] [A signal separation view of classification](https://arxiv.org/abs/2509.24140)
*H. N. Mhaskar,Ryan O'Dowd*

Main category: cs.LG

TL;DR: A novel classification approach using localized trigonometric polynomial kernels to separate class distributions in compact metric spaces, achieving perfect classification with minimal labels.


<details>
  <summary>Details</summary>
Motivation: Traditional classification methods rely on function approximation, but this paper proposes an alternative that can determine the number of classes and achieve perfect classification using minimal labeled data.

Method: Uses localized trigonometric polynomial kernels originally developed for signal processing to separate class distributions. Implements hierarchical MASC algorithm to handle touching/overlapping class boundaries.

Result: Successfully demonstrated on simulated and real datasets including Salinas and Indian Pines hyperspectral datasets and document datasets, showing effective class separation.

Conclusion: The proposed kernel-based approach provides an effective alternative to function approximation methods for classification, capable of determining class numbers and achieving perfect classification with minimal labeled data.

Abstract: The problem of classification in machine learning has often been approached
in terms of function approximation. In this paper, we propose an alternative
approach for classification in arbitrary compact metric spaces which, in
theory, yields both the number of classes, and a perfect classification using a
minimal number of queried labels. Our approach uses localized trigonometric
polynomial kernels initially developed for the point source signal separation
problem in signal processing. Rather than point sources, we argue that the
various classes come from different probability distributions. The localized
kernel technique developed for separating point sources is then shown to
separate the supports of these distributions. This is done in a hierarchical
manner in our MASC algorithm to accommodate touching/overlapping class
boundaries. We illustrate our theory on several simulated and real life
datasets, including the Salinas and Indian Pines hyperspectral datasets and a
document dataset.

</details>


### [573] [Evaluation of Machine and Deep Learning Techniques for Cyclone Trajectory Regression and Status Classification by Time Series Data](https://arxiv.org/abs/2509.24146)
*Ethan Zachary Lo,Dan Chie-Tien Lo*

Main category: cs.LG

TL;DR: This study proposes a machine learning approach for tropical cyclone forecasting using a two-stage pipeline: regression for cyclone features and classification for status prediction, achieving 93% accuracy with random forest classifier.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical weather prediction models are computationally intensive and error-prone due to atmospheric chaos. ML offers a more efficient and accurate alternative for cyclone forecasting to minimize life and economic losses.

Method: Two-stage ML pipeline: 1) Gradient boosting regression predicts cyclone features (wind speed, pressure, trajectory) from historical time series data; 2) Classification models (RF, SVM, MLP) predict cyclone status using regression outputs, with SMOTE for handling imbalanced data.

Result: Random forest classifier achieved 93% accuracy, outperforming SVM and MLP. Regression predictions had low errors: pressure within 2.2 mb and wind speed within 2.4 kt. RF was particularly effective for minority classes and minimizing false negatives.

Conclusion: ML models, especially ensemble-based classifiers like random forest, provide an effective and scalable alternative to traditional cyclone forecasting methods, with potential for real-time prediction and integration into decision support systems.

Abstract: Accurate cyclone forecasting is essential for minimizing loss of life,
infrastructure damage, and economic disruption. Traditional numerical weather
prediction models, though effective, are computationally intensive and prone to
error due to the chaotic nature of atmospheric systems. This study proposes a
machine learning (ML) approach to forecasting tropical cyclone trajectory and
status using time series data from the National Hurricane Center, including
recently added best track wind radii. A two-stage ML pipeline is developed: a
regression model first predicts cyclone features maximum wind speed, minimum
pressure, trajectory length, and directional change using a sliding window of
historical data. These outputs are then input into classification models to
predict the cyclone's categorical status. Gradient boosting regression and
three classifiers random forest (RF), support vector machine (SVM), and
multilayer perceptron (MLP) are evaluated. After hyperparameter tuning and
synthetic minority oversampling (SMOTE), the RF classifier achieves the highest
performance with 93% accuracy, outperforming SVM and MLP across precision,
recall, and F1 score. The RF model is particularly robust in identifying
minority cyclone statuses and minimizing false negatives. Regression results
yield low mean absolute errors, with pressure and wind predictions within about
2.2 mb and 2.4 kt, respectively. These findings demonstrate that ML models,
especially ensemble-based classifiers, offer an effective, scalable alternative
to traditional forecasting methods, with potential for real-time cyclone
prediction and integration into decision support systems.

</details>


### [574] [Stable Forgetting: Bounded Parameter-Efficient Unlearning in LLMs](https://arxiv.org/abs/2509.24166)
*Arpit Garg,Hemanth Saratchandran,Ravi Garg,Simon Lucey*

Main category: cs.LG

TL;DR: Proposes Bounded Parameter-Efficient Unlearning, a stable method for machine unlearning in LLMs that addresses gradient instability in existing gradient difference approaches by applying bounded functions to MLP adapters.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning methods in LLMs are unstable and unreliable, particularly the gradient difference method which causes unbounded weight growth and training instability when combined with cross-entropy loss.

Method: A parameter-efficient approach that stabilizes LoRA-based fine-tuning by applying bounded functions to MLP adapters, controlling weight dynamics during gradient ascent on forget data.

Result: Achieves substantial improvements in forgetting while preserving retention across TOFU, TDEC, and MUSE benchmarks, working reliably across architectures and scales from 125M to 8B parameters.

Conclusion: Establishes a theoretically grounded and practically scalable framework for stable machine unlearning in large language models.

Abstract: Machine unlearning in large language models (LLMs) is essential for privacy
and safety; however, existing approaches remain unstable and unreliable. A
widely used strategy, the gradient difference method, applies gradient descent
on retained data while performing gradient ascent on forget data, the data
whose influence should be removed. However, when combined with cross-entropy
loss, this procedure causes unbounded growth of weights and gradients, leading
to training instability and degrading both forgetting and retention. We provide
a theoretical framework that explains this failure, explicitly showing how
ascent on the forget set destabilizes optimization in the feedforward MLP
layers of LLMs. Guided by this insight, we propose Bounded Parameter-Efficient
Unlearning, a parameter-efficient approach that stabilizes LoRA-based
fine-tuning by applying bounded functions to MLP adapters. This simple
modification controls the weight dynamics during ascent, enabling the gradient
difference method to converge reliably. Across the TOFU, TDEC, and MUSE
benchmarks, and across architectures and scales from 125M to 8B parameters, our
method achieves substantial improvements in forgetting while preserving
retention, establishing a novel theoretically grounded and practically scalable
framework for unlearning in LLMs.

</details>


### [575] [Multi-Scale Geometric Autoencoder](https://arxiv.org/abs/2509.24168)
*Qipeng Zhan,Zhuoping Zhou,Zexuan Wang,Li Shen*

Main category: cs.LG

TL;DR: MAE is an asymmetric autoencoder that preserves both global and local geometric structures by applying global distance constraints to encoder and local constraints to decoder, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing autoencoders struggle to preserve both global and local geometric structures simultaneously - global methods accumulate distance errors while local methods distort large-scale relationships.

Method: Proposed Multi-Scale Geometric Autoencoder (MAE) with asymmetric architecture: global distance constraints on encoder and local geometric constraints on decoder.

Result: MAE consistently outperforms existing methods across various evaluation metrics on both synthetic manifolds and real-world datasets.

Conclusion: The asymmetric design naturally aligns with encoder-decoder roles and effectively preserves multi-scale geometric structure in latent representations.

Abstract: Autoencoders have emerged as powerful models for visualization and
dimensionality reduction based on the fundamental assumption that
high-dimensional data is generated from a low-dimensional manifold. A critical
challenge in autoencoder design is to preserve the geometric structure of data
in the latent space, with existing approaches typically focusing on either
global or local geometric properties separately. Global approaches often
encounter errors in distance approximation that accumulate, while local methods
frequently converge to suboptimal solutions that distort large-scale
relationships. We propose Multi-Scale Geometric Autoencoder (MAE), which
introduces an asymmetric architecture that simultaneously preserves both scales
of the geometric structure by applying global distance constraints to the
encoder and local geometric constraints to the decoder. Through theoretical
analysis, we establish that this asymmetric design aligns naturally with the
distinct roles of the encoder and decoder components. Our comprehensive
experiments on both synthetic manifolds and real-world datasets demonstrate
that MAE consistently outperforms existing methods across various evaluation
metrics.

</details>


### [576] [Model Correlation Detection via Random Selection Probing](https://arxiv.org/abs/2509.24171)
*Ruibo Chen,Sheng Zhang,Yihan Wu,Tong Zheng,Peihua Mai,Heng Huang*

Main category: cs.LG

TL;DR: RSP is a statistical framework that detects model correlations by testing transferability of optimized prefixes, providing principled p-values for determining if models are related through fine-tuning or identity.


<details>
  <summary>Details</summary>
Motivation: Existing methods for detecting model correlations require parameter access or produce heuristic scores without statistical rigor, limiting reliable application in LLM/VLM ecosystems.

Method: Random Selection Probing (RSP) formulates correlation detection as hypothesis testing, optimizing prefixes on reference models for random selection tasks and evaluating transferability to target models with p-values.

Result: RSP consistently yields small p-values for related models and high p-values for unrelated ones across LLMs and VLMs, with robustness confirmed through ablation studies.

Conclusion: RSP provides the first principled statistical framework for model correlation detection, enabling transparent and interpretable decisions in machine learning ecosystems.

Abstract: The growing prevalence of large language models (LLMs) and vision-language
models (VLMs) has heightened the need for reliable techniques to determine
whether a model has been fine-tuned from or is even identical to another.
Existing similarity-based methods often require access to model parameters or
produce heuristic scores without principled thresholds, limiting their
applicability. We introduce Random Selection Probing (RSP), a
hypothesis-testing framework that formulates model correlation detection as a
statistical test. RSP optimizes textual or visual prefixes on a reference model
for a random selection task and evaluates their transferability to a target
model, producing rigorous p-values that quantify evidence of correlation. To
mitigate false positives, RSP incorporates an unrelated baseline model to
filter out generic, transferable features. We evaluate RSP across both LLMs and
VLMs under diverse access conditions for reference models and test models.
Experiments on fine-tuned and open-source models show that RSP consistently
yields small p-values for related models while maintaining high p-values for
unrelated ones. Extensive ablation studies further demonstrate the robustness
of RSP. These results establish RSP as the first principled and general
statistical framework for model correlation detection, enabling transparent and
interpretable decisions in modern machine learning ecosystems.

</details>


### [577] [FM-FoG: A Real-Time Foundation Model-based Wearable System for Freezing-of-Gait Mitigation](https://arxiv.org/abs/2509.24176)
*Chuntian Chi,John Clapham,Leslie Cloud,Ingrid Pretzer-Aboff,GinaMari Blackwell,Huajie Shao,Gang Zhou*

Main category: cs.LG

TL;DR: FM-FoG is a real-time foundation model-based wearable system that detects Freezing-of-Gait in Parkinson's patients without patient-specific training, achieving 98.5% F1-score on unseen patients and extending smartphone battery life by 72%.


<details>
  <summary>Details</summary>
Motivation: Current FoG detection systems require extensive patient-specific training data and lack generalization, limiting clinical deployment. FoG affects over 50% of mid-to-late stage PD patients, significantly impairing mobility independence.

Method: Combines self-supervised pretraining on diverse IMU datasets with sensor context integration. Uses lightweight CNN-LSTM activity classifier to selectively activate foundation model only during walking or standing to avoid unnecessary computation.

Result: Achieves 98.5% F1-score when tested on previously unseen patients (VCU FoG-IMU dataset with 23 PD patients), substantially outperforming baseline methods. Deployed on Google Pixel 8a smartphone, extends battery life by up to 72% while maintaining sub-20ms intervention latency.

Conclusion: FM-FoG can enable practical, energy-efficient healthcare applications that generalize across patients without individual training requirements, making it suitable for clinical deployment.

Abstract: Freezing-of-Gait (FoG) affects over 50% of mid-to-late stage Parkinson's
disease (PD) patients, significantly impairing patients' mobility independence
and reducing quality of life. FoG is characterized by sudden episodes where
walking cannot start or is interrupted, occurring exclusively during standing
or walking, and never while sitting or lying down. Current FoG detection
systems require extensive patient-specific training data and lack
generalization, limiting clinical deployment. To address these issues, we
introduce FM-FoG, a real-time foundation model-based wearable system achieving
FoG detection in unseen patients without patient-specific training. Our
approach combines self-supervised pretraining on diverse Inertial Measurement
Unit (IMU) datasets with sensor context integration. Since FoG occurs only
during ambulatory activities, a lightweight CNN-LSTM activity classifier
selectively activates the foundation model only during walking or standing,
avoiding unnecessary computation. Evaluated on the VCU FoG-IMU dataset with 23
PD patients, FM-FoG achieves a 98.5% F1-score when tested on previously unseen
patients, substantially outperforming competitive baseline methods. Deployed on
a Google Pixel 8a smartphone, the system extends battery life by up to 72%
while maintaining sub-20ms intervention latency. The results indicate that our
FM-FoG can enable practical, energy-efficient healthcare applications that
generalize across patients without individual training requirements.

</details>


### [578] [CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based Token Eviction](https://arxiv.org/abs/2504.14051)
*Raghavv Goel,Junyoung Park,Mukul Gagrani,Dalton Jones,Matthew Morse,Harper Langston,Mingu Lee,Chris Lott*

Main category: cs.LG

TL;DR: CAOTE is a token eviction method that uses attention output contributions rather than just attention scores, improving accuracy when combined with existing methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of attention score-based token eviction by incorporating information about tokens' contributions to attention outputs, especially important for resource-constrained devices.

Method: Proposes CAOTE which integrates attention scores and value vectors in closed-form to optimize eviction error, can be used as meta-heuristic with any token eviction method.

Result: CAOTE consistently improves accuracies on downstream tasks when combined with state-of-the-art attention score-based methods.

Conclusion: Leveraging value information during token eviction is crucial for better performance, and CAOTE provides an effective closed-form solution.

Abstract: While long context support of large language models has extended their
abilities, it also incurs challenges in memory and compute which becomes
crucial bottlenecks in resource-restricted devices. Token eviction, a widely
adopted post-training methodology designed to alleviate the bottlenecks by
evicting less important tokens from the cache, typically uses attention scores
as proxy metrics for token importance. However, one major limitation of
attention score as a token-wise importance metrics is that it lacks the
information about contribution of tokens to the attention output. In this
paper, we propose a simple eviction criterion based on the contribution of
cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction
error due to token eviction, by seamlessly integrating attention scores and
value vectors. This is the first method which uses value tokens on top of
attention-based eviction scores in closed-form. Additionally, CAOTE can act as
a meta-heuristic method with flexible usage with any token eviction method. We
show that CAOTE, when combined with the state-of-the-art attention score-based
methods, always improves accuracies on the downstream task, indicating the
importance of leveraging information from values during token eviction process.

</details>


### [579] [Negative Pre-activations Differentiate Syntax](https://arxiv.org/abs/2509.24198)
*Linghao Kong,Angelina Ning,Micah Adler,Nir Shavit*

Main category: cs.LG

TL;DR: Wasserstein neurons are a sparse but critical component in LLMs that enable syntax processing through negative pre-activation differentiation of similar syntactic tokens.


<details>
  <summary>Details</summary>
Motivation: To understand the functional role of recently discovered Wasserstein neurons in language models and investigate why their targeted removal collapses model performance.

Method: Used targeted ablation studies by zeroing negative pre-activations of Wasserstein neurons, compared with random and perplexity-matched controls, analyzed part-of-speech effects and layer-specific interventions across training checkpoints.

Result: Targeted removal of negative pre-activations in Wasserstein neurons significantly impairs grammatical behavior and model function, while controls show minimal effects. The disruption specifically affects syntactic scaffolding tokens and accumulates across network depth.

Conclusion: Negative differentiation in a sparse subset of entangled Wasserstein neurons is a crucial mechanism that language models rely on for syntactic processing.

Abstract: A recently discovered class of entangled neurons, known as Wasserstein
neurons, is disproportionately critical in large language models despite
constituting only a very small fraction of the network: their targeted removal
collapses the model, consistent with their unique role in differentiating
similar inputs. Interestingly, in Wasserstein neurons immediately preceding
smooth activation functions, such differentiation manifests in the negative
pre-activation space, especially in early layers. Pairs of similar inputs are
driven to highly distinct negative values, and these pairs involve syntactic
tokens such as determiners and prepositions. We show that this negative region
is functional rather than simply favorable for optimization. A minimal,
sign-specific intervention that zeroes only the negative pre-activations of a
small subset of entangled neurons significantly weakens overall model function
and disrupts grammatical behavior, while both random and perplexity-matched
controls leave grammatical performance largely unchanged. Part of speech
analysis localizes the excess surprisal to syntactic scaffolding tokens, and
layer-specific interventions reveal that small local degradations accumulate
across depth. Over training checkpoints, the same ablation impairs grammatical
behavior as Wasserstein neurons emerge and stabilize. Together, these results
identify negative differentiation in a sparse subset of entangled neurons as a
crucial mechanism that language models rely on for syntax.

</details>


### [580] [Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects](https://arxiv.org/abs/2509.21923)
*Fumin Wang*

Main category: cs.LG

TL;DR: MACMs improve interpretable models by combining multiplicative and additive components, outperforming both CESR and GAMs while maintaining visual interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between interpretability and predictive performance in machine learning for high-stakes applications like healthcare, where GAMs sacrifice higher-order interactions for interpretability and CESR fails to outperform GAMs despite incorporating all feature interactions.

Method: Proposed Multiplicative-Additive Constrained Models (MACMs) that augment CESR with an additive component to disentangle interactive and independent feature effects, effectively expanding the hypothesis space while maintaining visual interpretability of shape functions.

Result: Neural network-based MACMs significantly outperform both CESR and state-of-the-art GAMs in predictive performance while preserving the ability to visualize shape functions for interpretation.

Conclusion: MACMs successfully bridge the gap between interpretability and performance by combining multiplicative and additive components, offering a superior alternative to both CESR and GAMs for interpretable machine learning in high-stakes domains.

Abstract: Interpretability is one of the considerations when applying machine learning
to high-stakes fields such as healthcare that involve matters of life safety.
Generalized Additive Models (GAMs) enhance interpretability by visualizing
shape functions. Nevertheless, to preserve interpretability, GAMs omit
higher-order interaction effects (beyond pairwise interactions), which imposes
significant constraints on their predictive performance. We observe that Curve
Ergodic Set Regression (CESR), a multiplicative model, naturally enables the
visualization of its shape functions and simultaneously incorporates both
interactions among all features and individual feature effects. Nevertheless,
CESR fails to demonstrate superior performance compared to GAMs. We introduce
Multiplicative-Additive Constrained Models (MACMs), which augment CESR with an
additive part to disentangle the intertwined coefficients of its interactive
and independent terms, thus effectively broadening the hypothesis space. The
model is composed of a multiplicative part and an additive part, whose shape
functions can both be naturally visualized, thereby assisting users in
interpreting how features participate in the decision-making process.
Consequently, MACMs constitute an improvement over both CESR and GAMs. The
experimental results indicate that neural network-based MACMs significantly
outperform both CESR and the current state-of-the-art GAMs in terms of
predictive performance.

</details>


### [581] [Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends](https://arxiv.org/abs/2509.24203)
*Chaorui Yao,Yanxi Chen,Yuchang Sun,Yushuo Chen,Wenhao Zhang,Xuchen Pan,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: This paper provides a first-principles derivation showing that group-relative REINFORCE can be interpreted as an off-policy algorithm, challenging the conventional view that REINFORCE variants are strictly on-policy. It establishes two principles for adapting REINFORCE to off-policy settings and unifies recent algorithms under this framework.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from practical constraints in real-world LLM applications, the complexity of LLM-RL infrastructure, and the need for innovations in RL methodologies. There's growing interest in off-policy RL for LLMs, but conventional wisdom limits REINFORCE variants to on-policy settings.

Method: The authors present a first-principles derivation for group-relative REINFORCE without assuming specific training data distributions, revealing its native off-policy interpretation. They establish two principles: regularizing policy updates and actively shaping data distribution.

Result: The analysis demystifies myths about importance sampling and clipping in GRPO, unifies OPMD and AsymRE as regularized REINFORCE forms, provides theoretical justification for heuristic data-weighting strategies, and offers actionable insights validated through empirical studies.

Conclusion: This work opens up new opportunities for principled algorithm design in off-policy RL for LLMs by providing a unified theoretical framework that challenges conventional assumptions about REINFORCE variants and enables more flexible off-policy approaches.

Abstract: Off-policy reinforcement learning (RL) for large language models (LLMs) is
attracting growing interest, driven by practical constraints in real-world
applications, the complexity of LLM-RL infrastructure, and the need for further
innovations of RL methodologies. While classic REINFORCE and its modern
variants like Group Relative Policy Optimization (GRPO) are typically regarded
as on-policy algorithms with limited tolerance of off-policyness, we present in
this work a first-principles derivation for group-relative REINFORCE without
assuming a specific training data distribution, showing that it admits a native
off-policy interpretation. This perspective yields two general principles for
adapting REINFORCE to off-policy settings: regularizing policy updates, and
actively shaping the data distribution. Our analysis demystifies some myths
about the roles of importance sampling and clipping in GRPO, unifies and
reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and
Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss,
and offers theoretical justification for seemingly heuristic data-weighting
strategies. Our findings lead to actionable insights that are validated with
extensive empirical studies, and open up new opportunities for principled
algorithm design in off-policy RL for LLMs. Source code for this work is
available at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.

</details>


### [582] [MDD-Thinker: Towards Large Reasoning Models for Major Depressive Disorder Diagnosis](https://arxiv.org/abs/2509.24217)
*Yuyang Sha,Hongxin Pan,Gang Luo,Caijuan Shi,Jing Wang,Kefeng Li*

Main category: cs.LG

TL;DR: MDD-Thinker is an LLM-based diagnostic framework that combines supervised fine-tuning and reinforcement learning to improve MDD diagnosis accuracy and interpretability, achieving superior performance over traditional methods and general-purpose LLMs.


<details>
  <summary>Details</summary>
Motivation: Current MDD diagnostic approaches rely on subjective assessments and lack multimodal integration. LLMs offer potential for enhanced accuracy but face challenges with interpretability, hallucination, and synthetic data reliance.

Method: Developed MDD-Thinker using UK Biobank data (40,000 reasoning samples) plus 10,000 samples from public mental health datasets. Combined supervised fine-tuning with reinforcement learning to enhance reasoning and interpretability.

Result: Achieved accuracy of 0.8268 and F1-score of 0.8081, significantly outperforming SVM, MLP, and general-purpose LLMs. SFT+RL combination yielded 29.0% accuracy improvement, 38.1% F1-score gain, and 34.8% AUC increase.

Conclusion: First reasoning-enhanced LLM framework for MDD diagnosis using real-world clinical data. Successfully balances accuracy, interpretability, and efficiency, offering scalable approach for intelligent psychiatric diagnostics.

Abstract: Background Major depressive disorder (MDD) is a leading cause of global
disability, yet current diagnostic approaches often rely on subjective
assessments and lack the ability to integrate multimodal clinical information.
Large language models (LLMs) hold promise for enhancing diagnostic accuracy
through advanced reasoning but face challenges in interpretability,
hallucination, and reliance on synthetic data.
  Methods We developed MDD-Thinker, an LLM-based diagnostic framework that
integrates supervised fine-tuning (SFT) with reinforcement learning (RL) to
strengthen reasoning ability and interpretability. Using the UK Biobank
dataset, we generated 40,000 reasoning samples, supplemented with 10,000
samples from publicly available mental health datasets. The model was
fine-tuned on these reasoning corpora, and its diagnostic and reasoning
performance was evaluated against machine learning, deep learning, and
state-of-the-art LLM baselines.
  Findings MDD-Thinker achieved an accuracy of 0.8268 and F1-score of 0.8081,
significantly outperforming traditional baselines such as SVM and MLP, as well
as general-purpose LLMs. Incorporating both SFT and RL yielded the greatest
improvements, with relative gains of 29.0% in accuracy, 38.1% in F1-score, and
34.8% in AUC. Moreover, the model demonstrated comparable reasoning performance
compared to much larger LLMs, while maintaining computational efficiency.
  Interpretation This study presents the first reasoning-enhanced LLM framework
for MDD diagnosis trained on large-scale real-world clinical data. By
integrating SFT and RL, MDD-Thinker balances accuracy, interpretability, and
efficiency, offering a scalable approach for intelligent psychiatric
diagnostics. These findings suggest that reasoning-oriented LLMs can provide
clinically reliable support for MDD detection and may inform broader
applications in mental health care.

</details>


### [583] [Conda: Column-Normalized Adam for Training Large Language Models Faster](https://arxiv.org/abs/2509.24218)
*Junjie Wang,Pan Zhou,Yiming Dong,Huan Li,Jia Li,Xun Zhou,Qicheng Lao,Cong Fang,Zhouchen Lin*

Main category: cs.LG

TL;DR: Conda is a novel optimizer that combines Adam's coordinate-wise adaptivity with global spectral normalization, achieving 2-2.5x faster convergence than AdamW in LLM pre-training.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of Adam optimizers which suffer from poor spectral conditioning and low-rank structures, while Muon lacks per-coordinate adaptivity.

Method: Projects updates into orthogonal subspace and applies column-wise second moment normalization based on projected gradients, maintaining coordinate-wise adaptivity while improving spectral conditioning.

Result: Consistently outperforms AdamW, Muon, and other baselines in pre-training LLaMA and GPT-2 series, achieving 2-2.5x faster convergence speed than AdamW.

Conclusion: Conda is an effective and broadly applicable optimizer for large-scale LLM training, bridging the strengths of both Adam and spectral normalization approaches.

Abstract: Large language models (LLMs) have demonstrated impressive generalization and
emergent capabilities, yet their pre-training remains computationally expensive
and sensitive to optimization dynamics. While Adam-based optimizers offer fast
convergence by adapting learning rates coordinate-wise, recent studies reveal
that their updates often suffer from poor spectral conditioning and low-rank
structures, hindering efficiency. Muon addresses this issue via global spectral
normalization but lacks the per-coordinate adaptivity of Adam. In this work, we
propose \textbf{Column-Normalized Adam (Conda)}, a novel optimizer that bridges
the strengths of both approaches. Conda projects updates into an orthogonal
subspace and applies column-wise second moment normalization based on the
projected gradients, thereby achieving both improved spectral conditioning and
maintaining coordinate-wise adaptivity. This design alleviates the spectral
pathologies of Adam while preserving its fast convergence behavior. Extensive
experiments on the LLaMA and GPT-2 series show that Conda consistently
outperforms AdamW, Muon, and other baselines in pre-training. Remarkably, on
the LLaMA series, \textbf{Conda achieves $2{\sim}2.5\times$ the convergence
speed of AdamW, measured in both training steps and training time.} Further
ablations demonstrate its robustness under diverse training setups. These
results collectively highlight Conda as an effective and broadly applicable
optimizer for large-scale LLM training. The code is released on
https://github.com/jie040109/Conda

</details>


### [584] [Semantic Editing with Coupled Stochastic Differential Equations](https://arxiv.org/abs/2509.24223)
*Jianxin Zhang,Clayton Scott*

Main category: cs.LG

TL;DR: Proposes coupled SDEs to guide pre-trained generative models for image editing, preserving source image details while achieving semantic changes.


<details>
  <summary>Details</summary>
Motivation: Existing image editing methods often distort fine details or introduce artifacts when using pre-trained text-to-image models.

Method: Uses coupled stochastic differential equations with correlated noise to drive both source and edited images, guiding sampling process of any SDE-based generative model.

Result: Achieves high prompt fidelity with near-pixel-level consistency, works out-of-the-box without retraining or auxiliary networks.

Conclusion: Coupled SDEs are a simple yet powerful tool for controlled generative AI image editing.

Abstract: Editing the content of an image with a pretrained text-to-image model remains
challenging. Existing methods often distort fine details or introduce
unintended artifacts. We propose using coupled stochastic differential
equations (coupled SDEs) to guide the sampling process of any pre-trained
generative model that can be sampled by solving an SDE, including diffusion and
rectified flow models. By driving both the source image and the edited image
with the same correlated noise, our approach steers new samples toward the
desired semantics while preserving visual similarity to the source. The method
works out-of-the-box-without retraining or auxiliary networks-and achieves high
prompt fidelity along with near-pixel-level consistency. These results position
coupled SDEs as a simple yet powerful tool for controlled generative AI.

</details>


### [585] [Proposing a Framework for Machine Learning Adoption on Legacy Systems](https://arxiv.org/abs/2509.24224)
*Ashiqur Rahman,Hamed Alhoori*

Main category: cs.LG

TL;DR: API-based framework decouples ML lifecycle from production to reduce adoption costs and operational disruptions in manufacturing.


<details>
  <summary>Details</summary>
Motivation: High costs and operational disruptions hinder ML adoption in industry, especially for SMEs needing to upgrade legacy systems.

Method: Lightweight browser-based interface with human-in-the-loop approach, allowing domain experts interactive control over model parameters without local hardware upgrades.

Result: Enables ML integration with zero production downtime, fostering trust and seamless workflow integration while reducing financial risks.

Conclusion: Provides scalable, accessible pathway to enhance production quality and safety, strengthening manufacturing competitiveness.

Abstract: The integration of machine learning (ML) is critical for industrial
competitiveness, yet its adoption is frequently stalled by the prohibitive
costs and operational disruptions of upgrading legacy systems. The financial
and logistical overhead required to support the full ML lifecycle presents a
formidable barrier to widespread implementation, particularly for small and
medium-sized enterprises. This paper introduces a pragmatic, API-based
framework designed to overcome these challenges by strategically decoupling the
ML model lifecycle from the production environment. Our solution delivers the
analytical power of ML to domain experts through a lightweight, browser-based
interface, eliminating the need for local hardware upgrades and ensuring model
maintenance can occur with zero production downtime. This human-in-the-loop
approach empowers experts with interactive control over model parameters,
fostering trust and facilitating seamless integration into existing workflows.
By mitigating the primary financial and operational risks, this framework
offers a scalable and accessible pathway to enhance production quality and
safety, thereby strengthening the competitive advantage of the manufacturing
sector.

</details>


### [586] [Accessible, Realistic, and Fair Evaluation of Positive-Unlabeled Learning Algorithms](https://arxiv.org/abs/2509.24228)
*Wei Wang,Dong-Dong Wu,Ming Li,Jingxiong Zhang,Gang Niu,Masashi Sugiyama*

Main category: cs.LG

TL;DR: Proposes the first PU learning benchmark to systematically compare algorithms, addressing unrealistic validation requirements and fair comparisons across different PU learning families.


<details>
  <summary>Details</summary>
Motivation: Experimental settings for PU learning are highly inconsistent, making it difficult to identify which algorithm performs better. There's a need for systematic comparison and fair evaluation.

Method: Develops a PU learning benchmark framework, investigates model selection criteria without negative data, identifies internal label shift problem, and proposes calibration approach for fair comparisons.

Result: Created the first systematic PU learning benchmark that addresses unrealistic validation requirements and ensures fair comparisons across different PU learning families (one-sample vs two-sample settings).

Conclusion: The proposed framework provides an accessible, realistic, and fair environment for evaluating PU learning algorithms, addressing critical evaluation factors identified during implementation.

Abstract: Positive-unlabeled (PU) learning is a weakly supervised binary classification
problem, in which the goal is to learn a binary classifier from only positive
and unlabeled data, without access to negative data. In recent years, many PU
learning algorithms have been developed to improve model performance. However,
experimental settings are highly inconsistent, making it difficult to identify
which algorithm performs better. In this paper, we propose the first PU
learning benchmark to systematically compare PU learning algorithms. During our
implementation, we identify subtle yet critical factors that affect the
realistic and fair evaluation of PU learning algorithms. On the one hand, many
PU learning algorithms rely on a validation set that includes negative data for
model selection. This is unrealistic in traditional PU learning settings, where
no negative data are available. To handle this problem, we systematically
investigate model selection criteria for PU learning. On the other hand, the
problem settings and solutions of PU learning have different families, i.e.,
the one-sample and two-sample settings. However, existing evaluation protocols
are heavily biased towards the one-sample setting and neglect the significant
difference between them. We identify the internal label shift problem of
unlabeled training data for the one-sample setting and propose a simple yet
effective calibration approach to ensure fair comparisons within and across
families. We hope our framework will provide an accessible, realistic, and fair
environment for evaluating PU learning algorithms in the future.

</details>


### [587] [ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2509.24239)
*Jincheng Liu,Sijun He,Jingjing Wu,Xiangsen Wang,Yang Chen,Zhaoqi Kuang,Siqi Bao,Yuan Yao*

Main category: cs.LG

TL;DR: This paper introduces ChessArena, a chess-based testbed to evaluate strategic reasoning in LLMs, revealing current models' limitations in complex strategic thinking despite strong pattern recognition abilities.


<details>
  <summary>Details</summary>
Motivation: To determine whether LLMs possess genuine strategic reasoning capabilities or are mainly excelling at pattern recognition, using chess as a test domain that requires long-term planning, rule comprehension, and multi-turn memory.

Method: Developed ChessArena - a competitive framework where LLMs play chess against each other under four different modes, equipped with ranking algorithm and leaderboard. Evaluated 13 LLMs playing over 800 games, with capabilities assessed across basic understanding, move selection, and puzzle solving.

Result: Current LLMs show significant shortcomings: no model could beat Maia-1100 (human amateur level chess engine), some failed to defeat random players. However, fine-tuned Qwen3-8B showed substantial improvement, approaching performance of much larger state-of-the-art reasoning models.

Conclusion: Current LLMs lack genuine strategic reasoning capabilities for complex tasks like chess, but targeted fine-tuning can significantly improve performance, suggesting potential for developing better reasoning models.

Abstract: Recent large language models (LLMs) have shown strong reasoning capabilities.
However, a critical question remains: do these models possess genuine reasoning
skills particularly complex strategic reasoning or are they primarily excelling
at sophisticated pattern recognition within their training data? To address
this question, this paper presents a chess testbed, ChessArena, to evaluate the
strategic reasoning capabilities of LLMs. Chess requires complex strategic
reasoning capabilities including long-term planning, strict rule comprehension,
and multi-turn conversation memorization. Specifically, ChessArena is a
competitive framework where LLMs play against each other, under four different
play modes. The testbed is equipped with a ranking algorithm and a leaderboard.
The testbed can also evaluate fine-grained capabilities including basic
understanding, move selection, and puzzle solving. Over 13 LLMs with different
modes are evaluated in ChessArena, playing over 800 games. The results reveal
significant shortcomings in current LLMs: no model can beat Maia-1100 (a chess
engine at human amateur level), while some even failed to defeat a random
player that selects moves arbitrarily. We also present a strong baseline to the
testbed: our fine-tuned Qwen3-8B substantially improved performance,
approaching much larger state-of-the-art reasoning models.

</details>


### [588] [Graph Foundation Models: Bridging Language Model Paradigms and Graph Optimization](https://arxiv.org/abs/2509.24256)
*Yunhao Liang,Pujun Zhang,Yuan Qu,Shaochong Lin,Zuo-jun Max Shen*

Main category: cs.LG

TL;DR: GFM is the first framework that adapts the pretrain-transfer paradigm to solve distance-based optimization problems on graphs, using self-supervised pre-training on random walk paths to learn graph topology.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between LLM's statistical flexibility and graph combinatorial constraints, enabling foundation models for Operations Research problems on graph structures.

Method: Self-supervised pre-training on paths from random walks, treating graph connectivity as supervisory signal; uses pre-trained GFM as foundational model with simple generative heuristic.

Result: Achieves competitive performance against specialized solvers on networks (20-893 nodes) across various optimization tasks with significantly faster inference times.

Conclusion: Establishes new paradigm for applying foundation model innovations to graph optimization in Operations Research.

Abstract: The pretrain-transfer paradigm, which underpins the success of large language
models (LLMs), has demonstrated the immense power of creating foundation models
that learn generalizable representations from vast datasets. However, extending
this paradigm to Operations Research (OR) problems on graph structures remains
challenging due to the fundamental conflict between the statistical flexibility
of language and the strict combinatorial constraints of graphs. To bridge this
gap, we introduce the Graph Foundation Model (GFM), the first framework capable
of solving all distance-based optimization problems on graph structures. By
introducing the LLM-like self-supervised pre-training paradigm on the paths
generated from random walks in the graph, GFM is compelled to internalize the
graph's complex topological and combinatorial rules, where the connectivity of
the structure itself can be treated as the supervisory signal. Unlike existing
neural methods that learn complex and task-specific solving policies, our
approach leverages the pre-trained GFM as a foundational model of the graph's
intrinsic structure, which in turn enables a simple generative heuristic to
tackle a diverse range of optimization challenges effectively. Comprehensive
experiments on networks ranging from 20 to 893 nodes demonstrate that GFM
achieves competitive performance against specialized solvers across a variety
of distinct optimization task classes, while maintaining significantly faster
inference times. Our work establishes a new paradigm of adapting the
pretrain-transfer framework to graph optimization, opening the door for
applying foundation model innovations to OR.

</details>


### [589] [Adversarial Reinforcement Learning Framework for ESP Cheater Simulation](https://arxiv.org/abs/2509.24274)
*Inkyu Park,Jeong-Gwan Lee,Taehwan Kwon,Juheon Choi,Seungku Kim,Junsu Kim,Kimin Lee*

Main category: cs.LG

TL;DR: A simulation framework for modeling ESP cheaters and detectors using reinforcement learning agents in an adversarial game setting, enabling study of adaptive cheating behaviors.


<details>
  <summary>Details</summary>
Motivation: ESP cheats are hard to detect due to lack of observable evidence and cheaters' adaptive behaviors, making labeled data collection difficult for anti-cheat systems.

Method: Model cheaters and non-cheaters as RL agents with different observability levels, formulate cheater-detector interaction as adversarial game, and introduce structured cheater model that dynamically switches behaviors based on detection risk.

Result: Framework successfully simulates adaptive cheater behaviors that strategically balance reward optimization and detection evasion.

Conclusion: Provides a controllable and extensible platform for studying adaptive cheating behaviors and developing effective cheat detectors.

Abstract: Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game
information such as enemy locations, are difficult to detect because their
effects are not directly observable in player behavior. The lack of observable
evidence makes it difficult to collect reliably labeled data, which is
essential for training effective anti-cheat systems. Furthermore, cheaters
often adapt their behavior by limiting or disguising their cheat usage, which
further complicates detection and detector development. To address these
challenges, we propose a simulation framework for controlled modeling of ESP
cheaters, non-cheaters, and trajectory-based detectors. We model cheaters and
non-cheaters as reinforcement learning agents with different levels of
observability, while detectors classify their behavioral trajectories. Next, we
formulate the interaction between the cheater and the detector as an
adversarial game, allowing both players to co-adapt over time. To reflect
realistic cheater strategies, we introduce a structured cheater model that
dynamically switches between cheating and non-cheating behaviors based on
detection risk. Experiments demonstrate that our framework successfully
simulates adaptive cheater behaviors that strategically balance reward
optimization and detection evasion. This work provides a controllable and
extensible platform for studying adaptive cheating behaviors and developing
effective cheat detectors.

</details>


### [590] [ELASTIQ: EEG-Language Alignment with Semantic Task Instruction and Querying](https://arxiv.org/abs/2509.24302)
*Muyun Jiang,Shuailei Zhang,Zhenjie Yang,Mengjun Wu,Weibang Jiang,Zhiwei Guo,Wei Zhang,Rui Liu,Shangen Zhang,Yong Li,Yi Ding,Cuntai Guan*

Main category: cs.LG

TL;DR: ELASTIQ is an EEG-language foundation model that integrates task instructions to create linguistically aligned EEG embeddings, achieving state-of-the-art performance across multiple BCI tasks.


<details>
  <summary>Details</summary>
Motivation: Existing EEG foundation models struggle to incorporate language instructions as prior constraints, limiting their ability to leverage semantic knowledge to unify different labels and tasks.

Method: Combines joint Spectral-Temporal Reconstruction (STR) module for pretraining with frequency and temporal masking, and Instruction-conditioned Q-Former (IQF) for instruction tuning that aligns EEG tokens with textual label embeddings.

Result: Achieves state-of-the-art performance on 14 out of 20 datasets across motor imagery, emotion recognition, SSVEP, covert speech, and healthcare tasks, with best average results across all five task categories.

Conclusion: Task instructions serve as semantic priors that guide EEG embeddings into coherent and linguistically grounded spaces, enhancing decoding robustness and transferability.

Abstract: Recent advances in electroencephalography (EEG) foundation models, which
capture transferable EEG representations, have greatly accelerated the
development of brain-computer interfaces (BCI). However, existing approaches
still struggle to incorporate language instructions as prior constraints for
EEG representation learning, limiting their ability to leverage the semantic
knowledge inherent in language to unify different labels and tasks. To address
this challenge, we present ELASTIQ, a foundation model for EEG-Language
Alignment with Semantic Task Instruction and Querying. ELASTIQ integrates
task-aware semantic guidance to produce structured and linguistically aligned
EEG embeddings, thereby enhancing decoding robustness and transferability. In
the pretraining stage, we introduce a joint Spectral-Temporal Reconstruction
(STR) module, which combines frequency masking as a global spectral
perturbation with two complementary temporal objectives: random masking to
capture contextual dependencies and causal masking to model sequential
dynamics. In the instruction tuning stage, we propose the
Instruction-conditioned Q-Former (IQF), a query-based cross-attention
transformer that injects instruction embeddings into EEG tokens and aligns them
with textual label embeddings through learnable queries. We evaluate ELASTIQ on
20 datasets spanning motor imagery, emotion recognition, steady-state visual
evoked potentials, covert speech, and healthcare tasks. ELASTIQ achieves
state-of-the-art performance on 14 of the 20 datasets and obtains the best
average results across all five task categories. Importantly, our analyses
reveal for the first time that explicit task instructions serve as semantic
priors guiding EEG embeddings into coherent and linguistically grounded spaces.
The code and pre-trained weights will be released.

</details>


### [591] [Asynchronous Policy Gradient Aggregation for Efficient Distributed Reinforcement Learning](https://arxiv.org/abs/2509.24305)
*Alexander Tyurin,Andrei Spiridonov,Varvara Rudenko*

Main category: cs.LG

TL;DR: The paper introduces two distributed RL algorithms (Rennala NIGT and Malenia NIGT) for asynchronous policy gradient aggregation, achieving state-of-the-art efficiency in both homogeneous and heterogeneous settings.


<details>
  <summary>Details</summary>
Motivation: Distributed reinforcement learning methods remain less explored compared to non-distributed approaches, especially in the presence of heterogeneous asynchronous computations and communication bottlenecks.

Method: Two new algorithms: Rennala NIGT for homogeneous settings with AllReduce operation support, and Malenia NIGT for heterogeneous settings handling asynchronous computations and heterogeneous environments.

Result: The algorithms achieve state-of-the-art efficiency with improved computational and communication complexity, and experimental results show significant outperformance over prior approaches.

Conclusion: The proposed distributed RL algorithms effectively address asynchronous computations and communication challenges, providing better theoretical guarantees and empirical performance in both homogeneous and heterogeneous settings.

Abstract: We study distributed reinforcement learning (RL) with policy gradient methods
under asynchronous and parallel computations and communications. While
non-distributed methods are well understood theoretically and have achieved
remarkable empirical success, their distributed counterparts remain less
explored, particularly in the presence of heterogeneous asynchronous
computations and communication bottlenecks. We introduce two new algorithms,
Rennala NIGT and Malenia NIGT, which implement asynchronous policy gradient
aggregation and achieve state-of-the-art efficiency. In the homogeneous
setting, Rennala NIGT provably improves the total computational and
communication complexity while supporting the AllReduce operation. In the
heterogeneous setting, Malenia NIGT simultaneously handles asynchronous
computations and heterogeneous environments with strictly better theoretical
guarantees. Our results are further corroborated by experiments, showing that
our methods significantly outperform prior approaches.

</details>


### [592] [A study of Universal ODE approaches to predicting soil organic carbon](https://arxiv.org/abs/2509.24306)
*Satyanarayana Raju G. V. V,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: This paper proposes a Scientific Machine Learning framework using Universal Differential Equations (UDEs) to predict Soil Organic Carbon dynamics across soil depth and time, combining mechanistic physics with neural networks for microbial processes.


<details>
  <summary>Details</summary>
Motivation: Soil Organic Carbon prediction is challenging due to complex physical, chemical, and biological processes, requiring methods that can handle these intricate dynamics while being robust to measurement noise.

Method: Used Universal Differential Equations that blend advection diffusion transport (mechanistic physics) with neural networks for learning nonlinear microbial production and respiration. Evaluated six experimental cases with synthetic datasets ranging from noise-free to high (35%) multiplicative, spatially correlated noise.

Result: In noise-free and moderate noise settings (7%), UDEs achieved excellent performance (MSE = 1.6e-5, R² = 0.9999 for clean case; MSE = 3.4e-6, R² = 0.99998 for 7% noise). However, with 35% noise, models either overfitted (reproducing noisy inputs but losing generalization) or collapsed to overly smooth profiles with negative R² values.

Conclusion: UDEs show promise for scalable, noise-tolerant SOC forecasting but require noise-aware loss functions, probabilistic modeling, and better integration of microbial dynamics for field deployment.

Abstract: Soil Organic Carbon (SOC) is a foundation of soil health and global climate
resilience, yet its prediction remains difficult because of intricate physical,
chemical, and biological processes. In this study, we explore a Scientific
Machine Learning (SciML) framework built on Universal Differential Equations
(UDEs) to forecast SOC dynamics across soil depth and time. UDEs blend
mechanistic physics, such as advection diffusion transport, with neural
networks that learn nonlinear microbial production and respiration. Using
synthetic datasets, we systematically evaluated six experimental cases,
progressing from clean, noise free benchmarks to stress tests with high (35%)
multiplicative, spatially correlated noise. Our results highlight both the
potential and limitations of the approach. In noise free and moderate noise
settings, the UDE accurately reconstructed SOC dynamics. In clean terminal
profile at 50 years (Case 4) achieved near perfect fidelity, with MSE = 1.6e-5,
and R2 = 0.9999. Case 5, with 7% noise, remained robust (MSE = 3.4e-6, R2 =
0.99998), capturing depth wise SOC trends while tolerating realistic
measurement uncertainty. In contrast, Case 3 (35% noise at t = 0) showed clear
evidence of overfitting: the model reproduced noisy inputs with high accuracy
but lost generalization against the clean truth (R2 = 0.94). Case 6 (35% noise
at t = 50) collapsed toward overly smooth mean profiles, failing to capture
depth wise variability and yielding negative R2, underscoring the limits of
standard training under severe uncertainty. These findings suggest that UDEs
are well suited for scalable, noise tolerant SOC forecasting, though advancing
toward field deployment will require noise aware loss functions, probabilistic
modelling, and tighter integration of microbial dynamics.

</details>


### [593] [Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers](https://arxiv.org/abs/2509.24317)
*Xianhang Li,Chen Huang,Chun-Liang Li,Eran Malach,Josh Susskind,Vimal Thilak,Etai Littwin*

Main category: cs.LG

TL;DR: SALT proposes a two-stage video representation learning method that uses a frozen teacher encoder trained with pixel reconstruction, followed by a student that predicts masked latents, outperforming V-JEPA 2 with better compute efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of EMA-based teacher-student frameworks in V-JEPA, which complicate model selection and couple architectures, by developing a simpler, more transparent alternative.

Method: Two-stage approach: (1) train teacher encoder with pixel reconstruction under V-JEPA masking, (2) freeze teacher and train student to predict teacher's latents on masked regions.

Result: SALT outperforms V-JEPA 2 encoders in frozen evaluation across benchmarks, achieves higher compute efficiency, and shows student quality is robust to teacher quality.

Conclusion: SALT provides a simple, scalable, and compute-efficient alternative to EMA-based self-distillation for video representation learning.

Abstract: Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable
off-the-shelf video representation by predicting masked regions in latent space
with an exponential moving average (EMA)-updated teacher. While EMA prevents
representation collapse, it complicates scalable model selection and couples
teacher and student architectures. We revisit masked-latent prediction and show
that a frozen teacher suffices. Concretely, we (i) train a target encoder with
a simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze
it and train a student to predict the teacher's latents on masked regions. This
leads to a two-stage, unregularized scheme that we refer to as SALT
(Static-teacher Asymmetric Latent Training). SALT decouples optimization into
pixel reconstruction (teacher) and masked latent prediction (student),
increasing transparency, efficiency, and scalability while preserving the
ability of representation to generalize under frozen evaluation. Empirically,
our student models outperform recently proposed V-JEPA 2 encoders under frozen
backbone evaluation across diverse benchmarks. They are also more
compute-optimal: at matched pretraining FLOPs, our method achieves higher
probing accuracy, and its scaling curves dominate V-JEPA's accuracy-FLOPs
Pareto frontier. Finally, we find that student quality is remarkably robust to
teacher quality: high-performing students emerge even with small, sub-optimal
teachers. This points to a compute budget allocation that should overwhelmingly
favor the student. These results position SALT as a simple, scalable, and
compute-efficient alternative to EMA-based self-distillation for video
representation learning.

</details>


### [594] [AuON: A Linear-time Alternative to Semi-Orthogonal Momentum Updates](https://arxiv.org/abs/2509.24320)
*Dipan Maity*

Main category: cs.LG

TL;DR: AuON is a linear-time optimizer that achieves strong performance without constructing semi-orthogonal matrices, using hyperbolic-cosine RMS scaling with normalization to preserve structural alignment and recondition ill-posed updates.


<details>
  <summary>Details</summary>
Motivation: Traditional orthogonal gradient methods like SVD/QR have O(n^3) computational costs and underperform compared to SGD with momentum. Recent methods like Muon reduce complexity to O(n^2) but quadratic costs remain a bottleneck.

Method: AuON bounds momentum updates under a spectral-norm trust region, using hyperbolic-cosine RMS scaling transformations with normalization to preserve directional information without explicit semi-orthogonalization. Also introduces Hybrid-AuON with one Newton-Schulz iteration.

Result: Experiments across vision and language benchmarks show AuON and its hybrid variant achieve performance comparable to strong baselines like AdamW and Muon.

Conclusion: AuON provides an efficient linear-time alternative to traditional orthogonal optimization methods, achieving competitive performance while avoiding the computational bottlenecks of quadratic or cubic complexity approaches.

Abstract: Orthogonal gradient updates have emerged as a promising direction in
optimization for machine learning. However, traditional approaches such as
SVD/QR decomposition incur prohibitive computational costs of O(n^3) and
underperform compared to well-tuned SGD with momentum, since momentum is
applied only after strict orthogonalization. Recent advances, such as Muon,
improve efficiency by applying momentum before orthogonalization and producing
semi-orthogonal matrices via Newton-Schulz iterations, reducing complexity to
O(n^2). Nevertheless, quadratic costs remain a bottleneck.
  In this work, we study the semi-orthogonal properties of momentum-based
updates and develop a method to bound momentum updates under a spectral-norm
trust region, preserving directional information without requiring explicit
semi-orthogonalization.
  We propose AuON (Alternative Unit-norm momentum updates by Normalized
nonlinear scaling), a linear-time optimizer that achieves strong performance
without constructing semi-orthogonal matrices, while preserving structural
alignment and reconditioning ill-posed updates. Our approach combines
hyperbolic-cosine RMS scaling transformations with normalization, demonstrating
both effectiveness and computational efficiency compared to Newton-Schulz
methods. We further introduce a hybrid variant (Hybrid-AuON) that applies a
single Newton-Schulz iteration. Experiments across vision and language
benchmarks show that AuON and its hybrid variant achieve performance comparable
to strong baselines such as AdamW and Muon.
  Code is available at: https://github.com/ryyzn9/AuON

</details>


### [595] [H+: An Efficient Similarity-Aware Aggregation for Byzantine Resilient Federated Learning](https://arxiv.org/abs/2509.24330)
*Shiyuan Zuo,Rongfei Fan,Cheng Zhan,Jie Xu,Puning Zhao,Han Hu*

Main category: cs.LG

TL;DR: H+ is a novel similarity-aware aggregation method for Federated Learning that defends against Byzantine attacks without requiring clean data, using random parameter segment comparisons to identify honest clients.


<details>
  <summary>Details</summary>
Motivation: Existing similarity-aware aggregation methods only work when clean data is available, making them inapplicable to FL systems without trustworthy data. There's a need for Byzantine-resilient methods that don't rely on clean data.

Method: H+ randomly selects r-dimensional segments from p-dimensional parameter vectors and applies similarity check function H to compare each segment against a reference vector. The reference vector comes from robust algorithms (no clean data) or clean data. Process repeats K times to identify honest clients.

Result: H+ outperforms existing methods in scenarios with clean data and extends to FL systems without clean data. It achieves state-of-the-art robustness under varying Byzantine attack ratios and multiple attack types across all evaluated scenarios and datasets.

Conclusion: H+ provides an effective Byzantine-resilient aggregation approach with low computational complexity (O(KMr)), making it applicable to FL systems both with and without clean data availability.

Abstract: Federated Learning (FL) enables decentralized model training without sharing
raw data. However, it remains vulnerable to Byzantine attacks, which can
compromise the aggregation of locally updated parameters at the central server.
Similarity-aware aggregation has emerged as an effective strategy to mitigate
such attacks by identifying and filtering out malicious clients based on
similarity between client model parameters and those derived from clean data,
i.e., data that is uncorrupted and trustworthy. However, existing methods adopt
this strategy only in FL systems with clean data, making them inapplicable to
settings where such data is unavailable. In this paper, we propose H+, a novel
similarity-aware aggregation approach that not only outperforms existing
methods in scenarios with clean data, but also extends applicability to FL
systems without any clean data. Specifically, H+ randomly selects
$r$-dimensional segments from the $p$-dimensional parameter vectors uploaded to
the server and applies a similarity check function $H$ to compare each segment
against a reference vector, preserving the most similar client vectors for
aggregation. The reference vector is derived either from existing robust
algorithms when clean data is unavailable or directly from clean data.
Repeating this process $K$ times enables effective identification of honest
clients. Moreover, H+ maintains low computational complexity, with an
analytical time complexity of $\mathcal{O}(KMr)$, where $M$ is the number of
clients and $Kr \ll p$. Comprehensive experiments validate H+ as a
state-of-the-art (SOTA) method, demonstrating substantial robustness
improvements over existing approaches under varying Byzantine attack ratios and
multiple types of traditional Byzantine attacks, across all evaluated scenarios
and benchmark datasets.

</details>


### [596] [Towards Generalizable PDE Dynamics Forecasting via Physics-Guided Invariant Learning](https://arxiv.org/abs/2509.24332)
*Siyang Li,Yize Chen,Yan Guo,Ming Huang,Hui Xiong*

Main category: cs.LG

TL;DR: The paper proposes iMOOE, a physics-guided invariant learning method for spatiotemporal PDE forecasting that achieves superior zero-shot OOD generalization by capturing fundamental PDE invariance principles.


<details>
  <summary>Details</summary>
Motivation: Real-world PDE systems have varying parameters, making generalization across unseen out-of-distribution scenarios challenging. Existing methods require test-time adaptation samples and fail to capture fundamental physical invariances.

Method: Defines a two-fold PDE invariance principle (ingredient operators and composition relationships remain invariant), then proposes iMOOE with Invariance-aligned Mixture Of Operator Expert architecture and frequency-enriched invariant learning objective.

Result: Extensive experiments show iMOOE achieves superior in-distribution performance and zero-shot generalization capabilities across diverse OOD forecasting scenarios in both simulated benchmarks and real-world applications.

Conclusion: The proposed physics-guided invariant learning approach effectively captures fundamental PDE invariances, enabling robust zero-shot generalization without requiring test-time adaptation samples.

Abstract: Advanced deep learning-based approaches have been actively applied to
forecast the spatiotemporal physical dynamics governed by partial differential
equations (PDEs), which acts as a critical procedure in tackling many science
and engineering problems. As real-world physical environments like PDE system
parameters are always capricious, how to generalize across unseen
out-of-distribution (OOD) forecasting scenarios using limited training data is
of great importance. To bridge this barrier, existing methods focus on
discovering domain-generalizable representations across various PDE dynamics
trajectories. However, their zero-shot OOD generalization capability remains
deficient, since extra test-time samples for domain-specific adaptation are
still required. This is because the fundamental physical invariance in PDE
dynamical systems are yet to be investigated or integrated. To this end, we
first explicitly define a two-fold PDE invariance principle, which points out
that ingredient operators and their composition relationships remain invariant
across different domains and PDE system evolution. Next, to capture this
two-fold PDE invariance, we propose a physics-guided invariant learning method
termed iMOOE, featuring an Invariance-aligned Mixture Of Operator Expert
architecture and a frequency-enriched invariant learning objective. Extensive
experiments across simulated benchmarks and real-world applications validate
iMOOE's superior in-distribution performance and zero-shot generalization
capabilities on diverse OOD forecasting scenarios.

</details>


### [597] [Expanding Horizons of Level Diversity via Multi-objective Evolutionary Learning](https://arxiv.org/abs/2509.24341)
*Qingquan Zhang,Ziqi Wang,Yuchen Li,Keyuan Zhang,Bo Yuan,Jialin Liu*

Main category: cs.LG

TL;DR: This paper proposes a multi-objective evolutionary learning framework to enhance multi-dimensional diversity in game level generation, treating each diversity metric as a distinct objective and optimizing them simultaneously during model training.


<details>
  <summary>Details</summary>
Motivation: Existing level generation approaches fail to comprehensively assess diversity across multiple dimensions, as diversity metrics are naturally multi-dimensional with conflicted, complementary, or both relationships among dimensions.

Method: Formulate model training as a multi-objective learning problem where each diversity metric is a distinct objective, and propose a multi-objective evolutionary learning framework that optimizes multiple diversity metrics simultaneously throughout training.

Result: Case study on Super Mario Bros demonstrates enhanced multi-dimensional diversity and identification of a Pareto front of generative models providing tradeoffs among playability and two representative diversity metrics (content-based and player-centered).

Conclusion: The framework enables decision-makers to make informed choices when selecting generators accommodating various scenarios and diverse needs of players and designers by providing a range of tradeoffs.

Abstract: In recent years, the generation of diverse game levels has gained increasing
interest, contributing to a richer and more engaging gaming experience. A
number of level diversity metrics have been proposed in literature, which are
naturally multi-dimensional, leading to conflicted, complementary, or both
relationships among these dimensions. However, existing level generation
approaches often fail to comprehensively assess diversity across those
dimensions. This paper aims to expand horizons of level diversity by
considering multi-dimensional diversity when training generative models. We
formulate the model training as a multi-objective learning problem, where each
diversity metric is treated as a distinct objective. Furthermore, a
multi-objective evolutionary learning framework that optimises multiple
diversity metrics simultaneously throughout the model training process is
proposed. Our case study on the commonly used benchmark Super Mario Bros.
demonstrates that our proposed framework can enhance multi-dimensional
diversity and identify a Pareto front of generative models, which provides a
range of tradeoffs among playability and two representative diversity metrics,
including a content-based one and a player-centered one. Such capability
enables decision-makers to make informed choices when selecting generators
accommodating a variety of scenarios and the diverse needs of players and
designers.

</details>


### [598] [Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning](https://arxiv.org/abs/2509.24372)
*Xin Qiu,Yulu Gan,Conor F. Hayes,Qiyao Liang,Elliot Meyerson,Babak Hodjat,Risto Miikkulainen*

Main category: cs.LG

TL;DR: This paper successfully scales up Evolution Strategies (ES) for fine-tuning large language models, showing ES outperforms RL methods in sample efficiency, reward tolerance, robustness, and stability.


<details>
  <summary>Details</summary>
Motivation: Evolution strategies were neglected for LLM fine-tuning due to perceived scalability limitations, despite once showing comparable performance to RL on smaller models.

Method: Scaling up evolution strategies to fine-tune the full parameters of large language models with billions of parameters.

Result: ES can efficiently search over billions of parameters and outperforms RL methods in multiple aspects: better sample efficiency, tolerance to long-horizon rewards, robustness across different base LLMs, less reward hacking, and more stable performance.

Conclusion: ES serves as a new direction for LLM fine-tuning beyond current RL techniques, unlocking previously overlooked potential in evolutionary approaches.

Abstract: Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is
a critical step in the AI deployment pipeline. Reinforcement learning (RL) is
arguably the most prominent fine-tuning method, contributing to the birth of
many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once
showed comparable performance to RL on models with a few million parameters,
was neglected due to the pessimistic perception of its scalability to larger
models. In this work, we report the first successful attempt to scale up ES for
fine-tuning the full parameters of LLMs, showing the surprising fact that ES
can search efficiently over billions of parameters and outperform existing RL
fine-tuning methods in multiple respects, including sample efficiency,
tolerance to long-horizon rewards, robustness to different base LLMs, less
tendency to reward hacking, and more stable performance across runs. It
therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond
what current RL techniques provide. The source codes are provided at:
https://github.com/VsonicV/es-fine-tuning-paper.

</details>


### [599] [AXIS: Explainable Time Series Anomaly Detection with Large Language Models](https://arxiv.org/abs/2509.24378)
*Tian Lan,Hao Duong Le,Jinbo Li,Wenjun He,Meng Wang,Chenghao Liu,Chen Zhang*

Main category: cs.LG

TL;DR: AXIS is a framework that conditions frozen LLMs for explainable time-series anomaly detection by providing three complementary hints: symbolic numeric grounding, context-integrated step-aligned features, and task-prior anomaly characteristics, achieving superior explanation quality and competitive detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based approaches for explainable time-series anomaly detection struggle with processing continuous signals and lack contextual grounding between time series and text modalities.

Method: AXIS conditions frozen LLMs with three hints: (1) symbolic numeric hint for numerical grounding, (2) context-integrated step-aligned hint from pretrained time-series encoder, and (3) task-prior hint encoding global anomaly characteristics.

Result: AXIS yields significantly higher quality explanations and achieves competitive detection accuracy compared to general-purpose LLMs, specialized time-series LLMs, and time-series Vision Language Models.

Conclusion: The AXIS framework effectively bridges the modality gap between time series and LLMs, enabling nuanced time-series understanding and high-quality explainable anomaly detection.

Abstract: Time-series anomaly detection (TSAD) increasingly demands explanations that
articulate not only if an anomaly occurred, but also what pattern it exhibits
and why it is anomalous. Leveraging the impressive explanatory capabilities of
Large Language Models (LLMs), recent works have attempted to treat time series
as text for explainable TSAD. However, this approach faces a fundamental
challenge: LLMs operate on discrete tokens and struggle to directly process
long, continuous signals. Consequently, naive time-to-text serialization
suffers from a lack of contextual grounding and representation alignment
between the two modalities. To address this gap, we introduce AXIS, a framework
that conditions a frozen LLM for nuanced time-series understanding. Instead of
direct serialization, AXIS enriches the LLM's input with three complementary
hints derived from the series: (i) a symbolic numeric hint for numerical
grounding, (ii) a context-integrated, step-aligned hint distilled from a
pretrained time-series encoder to capture fine-grained dynamics, and (iii) a
task-prior hint that encodes global anomaly characteristics. Furthermore, to
facilitate robust evaluation of explainability, we introduce a new benchmark
featuring multi-format questions and rationales that supervise contextual
grounding and pattern-level semantics. Extensive experiments, including both
LLM-based and human evaluations, demonstrate that AXIS yields explanations of
significantly higher quality and achieves competitive detection accuracy
compared to general-purpose LLMs, specialized time-series LLMs, and time-series
Vision Language Models.

</details>


### [600] [Muon: Training and Trade-offs with Latent Attention and MoE](https://arxiv.org/abs/2509.24406)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: Muon optimizer provides theoretical convergence guarantees and practical efficiency gains for training small-to-medium transformers, achieving 48-52% less compute than AdamW while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: To establish Muon as a principled alternative to AdamW with rigorous theoretical foundations and demonstrate its synergistic benefits with modern architectural optimizations like MLA and MoE.

Method: Theoretical analysis of convergence rates, spectral regularization, connections to natural gradient descent, and empirical validation across 100+ training runs with 15 architectural components.

Result: Muon achieves target loss with 48-52% less training compute than AdamW, maintains or improves perplexity, and when combined with MLA+MoE achieves 68% memory reduction and 3.2x inference speedup with 8-12% perplexity improvement.

Conclusion: Muon is a robust, principled optimizer that expands the Pareto frontier in compute-time trade-offs, particularly excelling with modern efficiency techniques and large-batch training.

Abstract: We present a comprehensive theoretical and empirical study of the Muon
optimizer for training transformers only with a small to medium decoder (30M -
200M parameters), with an emphasis on its mathematical foundations, convergence
properties and synergistic interactions with modern architectural
optimizations. Building on recent work showing Muon's scalability, we provide
rigorous theoretical analysis including: (i)showing the convergence rate under
standard assumptions, (ii) spectral regularization properties that prevent
gradient explosion, (iii) connection to natural gradient descent on the Stiefel
manifold, and (iv) equivalence to steepest gradient descent under the spectral
norm. Crucially, we demonstrate that Muon expands the Pareto frontier in the
compute-time trade-off by maintaining superior data efficiency at large batch
sizes, a key finding of~\cite{essentialai2025muon} that we validate across our
model scales. Empirically, Muon reaches the target loss with 48-52\% of the
training calculated by AdamW while maintaining or improving the final
perplexity, consistent with larger-scale results. When combined with Multi-Head
Latent Attention (MLA) and Mixture-of-Experts (MoE), we observe multiplicative
efficiency gains: MLA+MoE+Muon achieves 68\% memory reduction and 3.2$\times$
inference speedup, while improving perplexity by 8-12\%. We provide detailed
procedures on 15 architectural and optimizer components, stability analyzes
across 100+ training runs, and practical implementation guidelines including
Newton-Schulz coefficients $(3.4445, -4.7750, 2.0315)$ optimized
by~\cite{su2024muonblog}. Our theoretical analysis and comprehensive
experiments establish Muon as a principled, robust alternative to AdamW that
particularly excels when combined with modern efficiency techniques and
large-batch training regimes.

</details>


### [601] [ScatterAD: Temporal-Topological Scattering Mechanism for Time Series Anomaly Detection](https://arxiv.org/abs/2509.24414)
*Tao Yin,Xiaohong Zhang,Shaochen Fu,Zhibin Zhang,Li Huang,Yiyuan Yang,Kaixiang Yang,Meng Yan*

Main category: cs.LG

TL;DR: ScatterAD is a novel anomaly detection method for industrial IoT time series that models representation scattering across temporal and topological dimensions, leveraging dispersion patterns in high-dimensional space to improve detection sensitivity.


<details>
  <summary>Details</summary>
Motivation: Traditional anomaly detection methods independently model spatial or temporal dependencies, leading to suboptimal representation learning and limited sensitivity to anomalous dispersion in high-dimensional spaces.

Method: Proposes ScatterAD with topological encoder for graph-structured scattering, temporal encoder for constraining over-scattering, and contrastive fusion mechanism for complementary temporal-topological representations. Uses mean pairwise distance to quantify scattering.

Result: Extensive experiments on multiple public benchmarks show ScatterAD achieves state-of-the-art performance on multivariate time series anomaly detection.

Conclusion: Modeling representation scattering across temporal and topological dimensions effectively enhances spatio-temporal anomaly detection by leveraging dispersion patterns as inductive signals.

Abstract: One main challenge in time series anomaly detection for industrial IoT lies
in the complex spatio-temporal couplings within multivariate data. However,
traditional anomaly detection methods focus on modeling spatial or temporal
dependencies independently, resulting in suboptimal representation learning and
limited sensitivity to anomalous dispersion in high-dimensional spaces. In this
work, we conduct an empirical analysis showing that both normal and anomalous
samples tend to scatter in high-dimensional space, especially anomalous samples
are markedly more dispersed. We formalize this dispersion phenomenon as
scattering, quantified by the mean pairwise distance among sample
representations, and leverage it as an inductive signal to enhance
spatio-temporal anomaly detection. Technically, we propose ScatterAD to model
representation scattering across temporal and topological dimensions. ScatterAD
incorporates a topological encoder for capturing graph-structured scattering
and a temporal encoder for constraining over-scattering through mean squared
error minimization between neighboring time steps. We introduce a contrastive
fusion mechanism to ensure the complementarity of the learned temporal and
topological representations. Additionally, we theoretically show that
maximizing the conditional mutual information between temporal and topological
views improves cross-view consistency and enhances more discriminative
representations. Extensive experiments on multiple public benchmarks show that
ScatterAD achieves state-of-the-art performance on multivariate time series
anomaly detection. Code is available at this repository:
https://github.com/jk-sounds/ScatterAD.

</details>


### [602] [BiHDTrans: binary hyperdimensional transformer for efficient multivariate time series classification](https://arxiv.org/abs/2509.24425)
*Jingtao Zhang,Yi Liu,Qi Shen,Changhong Wang*

Main category: cs.LG

TL;DR: BiHDTrans is a neurosymbolic binary hyperdimensional Transformer that combines HD computing efficiency with Transformer temporal modeling, achieving superior accuracy and 39.4x lower latency than SOTA binary Transformers.


<details>
  <summary>Details</summary>
Motivation: IoT devices generate massive multivariate time series data requiring efficient processing in resource-constrained edge environments. HD computing is efficient but struggles with temporal patterns, while Transformers excel at sequence modeling but have high computational overhead.

Method: Integrates self-attention into HD computing paradigm, creating a neurosymbolic binary hyperdimensional Transformer that unifies HD computing representational efficiency with Transformer temporal modeling power.

Result: Outperforms SOTA HD computing models by at least 14.47% and achieves 6.67% higher accuracy than SOTA binary Transformers. With FPGA acceleration, delivers 39.4x lower inference latency than SOTA binary Transformers.

Conclusion: BiHDTrans bridges the gap between Transformer expressiveness and HD computing efficiency, enabling accurate, scalable, and low-latency MTS classification for edge environments.

Abstract: The proliferation of Internet-of-Things (IoT) devices has led to an
unprecedented volume of multivariate time series (MTS) data, requiring
efficient and accurate processing for timely decision-making in
resource-constrained edge environments. Hyperdimensional (HD) computing, with
its inherent efficiency and parallelizability, has shown promise in
classification tasks but struggles to capture complex temporal patterns, while
Transformers excel at sequence modeling but incur high computational and memory
overhead. We introduce BiHDTrans, an efficient neurosymbolic binary
hyperdimensional Transformer that integrates self-attention into the HD
computing paradigm, unifying the representational efficiency of HD computing
with the temporal modeling power of Transformers. Empirically, BiHDTrans
outperforms state-of-the-art (SOTA) HD computing models by at least 14.47% and
achieves 6.67% higher accuracy on average than SOTA binary Transformers. With
hardware acceleration on FPGA, our pipelined implementation leverages the
independent and identically distributed properties of high-dimensional
representations, delivering 39.4 times lower inference latency than SOTA binary
Transformers. Theoretical analysis shows that binarizing in holographic
high-dimensional space incurs significantly less information distortion than
directly binarizing neural networks, explaining BiHDTrans's superior accuracy.
Furthermore, dimensionality experiments confirm that BiHDTrans remains
competitive even with a 64% reduction in hyperspace dimensionality, surpassing
SOTA binary Transformers by 1-2% in accuracy with 4.4 times less model size, as
well as further reducing the latency by 49.8% compare to the full-dimensional
baseline. Together, these contributions bridge the gap between the
expressiveness of Transformers and the efficiency of HD computing, enabling
accurate, scalable, and low-latency MTS classification.

</details>


### [603] [Semantic Compression via Multimodal Representation Learning](https://arxiv.org/abs/2509.24431)
*Eleonora Grassucci,Giordano Cicchetti,Aurelio Uncini,Danilo Comminiello*

Main category: cs.LG

TL;DR: This paper presents a semantic compression method for multimodal embeddings that reduces memory footprint by leveraging modality alignment, showing that when modality gap is minimized, embeddings from different modalities sharing the same semantics can be replaced with centroids without performance loss.


<details>
  <summary>Details</summary>
Motivation: Multimodal representation learning creates high-dimensional embeddings that align different modalities but introduces scalability challenges in storage and processing. The key problem is achieving semantic compression while preserving the ability to represent shared semantic content across modalities.

Method: The approach proves that reducing modality gap enables post-training semantic compression. When embeddings from different modalities expressing the same semantics share a common space portion, their centroid becomes a faithful semantic representation. The method operates directly on pretrained encoders, replacing multiple embeddings with single centroids.

Result: The proposed semantic compression approach demonstrates effectiveness across diverse large-scale multimodal downstream tasks, achieving significant compression without sacrificing performance.

Conclusion: Modality alignment is a key enabler for semantic compression, and the proposed centroid-based approach successfully compresses multimodal embeddings while maintaining their semantic representation capabilities.

Abstract: Multimodal representation learning produces high-dimensional embeddings that
align diverse modalities in a shared latent space. While this enables strong
generalization, it also introduces scalability challenges, both in terms of
storage and downstream processing. A key open problem is how to achieve
semantic compression, reducing the memory footprint of multimodal embeddings
while preserving their ability to represent shared semantic content across
modalities. In this paper, we prove a strong connection between reducing the
modality gap, which is the residual separation of embeddings from different
modalities, and the feasibility of post-training semantic compression. When the
gap is sufficiently reduced, embeddings from different modalities but
expressing the same semantics share a common portion of the space. Therefore,
their centroid is a faithful representation of such a semantic concept. This
enables replacing multiple embeddings with a single centroid, yielding
significant memory savings. We propose a novel approach for semantic
compression grounded on the latter intuition, operating directly on pretrained
encoders. We demonstrate its effectiveness across diverse large-scale
multimodal downstream tasks. Our results highlight that modality alignment is a
key enabler for semantic compression, showing that the proposed approach
achieves significant compression without sacrificing performance.

</details>


### [604] [EOE: Evolutionary Optimization of Experts for Training Language Models](https://arxiv.org/abs/2509.24436)
*Yingshi Chen*

Main category: cs.LG

TL;DR: An evolutionary framework for training LLMs using expert sub-networks with evolutionary operators to reduce model size and accelerate training throughput by 10x.


<details>
  <summary>Details</summary>
Motivation: To reduce the memory requirements and computational cost of training large language models while maintaining accuracy, making them more suitable for deployment on PCs and edge devices.

Method: Divide LLM into multiple expert sub-networks with same structure but different parameters. Train only one expert per step using AdamW optimization, then apply evolutionary operators (crossover, PSO, mutation) between current and best expert to transfer knowledge.

Result: Best expert achieves nearly the same accuracy as full model while greatly reducing inference size. Training throughput accelerates more than 10 times due to memory efficiency.

Conclusion: The evolutionary framework successfully enables efficient LLM training with significant memory reduction and throughput improvement while maintaining model accuracy, suitable for edge deployment.

Abstract: This paper presents an evolutionary framework for the training of large
language models(LLM). The models are divided into several
experts(sub-networks), which have the same structure but different parameter
values. Only one expert is trained at each step. After the classical AdamW
optimization, some evolutionary operators(crossover, PSO, and mutation) act on
the tensor weights between the current expert and the best expert. So current
expert would learn the experience of best expert. The direction of best expert
would help current expert's loss decrease faster. Finally, only save the weight
of the best expert. Experiments show that best expert would achieve nearly the
same accuracy as the full model. This would greatly reduce the size of the
model for inference. Since only one expert is trained at each step, the
training needs much less memory and has much higher throughput. Experiments
show that the throughput would accelerate more than ten times! Our source code
is available. It's a pure c++/cu framework, which is suitable for easy
deployment on PCs and edge computing devices.

</details>


### [605] [Distributionally Robust Federated Learning with Outlier Resilience](https://arxiv.org/abs/2509.24462)
*Zifan Wang,Xinlei Yi,Xenia Konti,Michael M. Zavlanos,Karl H. Johansson*

Main category: cs.LG

TL;DR: This paper proposes a distributionally robust federated learning method with explicit outlier resilience using unbalanced Wasserstein distance and KL penalization to handle data distribution shifts and outliers.


<details>
  <summary>Details</summary>
Motivation: Federated learning performance degrades with data distribution perturbations, and existing DRO-based FL methods overlook the detrimental impact of outliers in local datasets that disproportionately bias learned models.

Method: Introduces a novel ambiguity set based on unbalanced Wasserstein distance that captures geometric distributional shifts and incorporates KL penalization to mitigate outlier influence. Reformulates the min-max-max optimization as tractable Lagrangian penalty optimization for decentralized training.

Result: Proposes the distributionally outlier-robust federated learning algorithm with established convergence guarantees. Extensive experiments on synthetic and real-world datasets demonstrate effectiveness.

Conclusion: The proposed approach effectively handles both distribution shifts and outliers in federated learning through a principled DRO framework with explicit outlier resilience mechanisms.

Abstract: Federated learning (FL) enables collaborative model training without direct
data sharing, but its performance can degrade significantly in the presence of
data distribution perturbations. Distributionally robust optimization (DRO)
provides a principled framework for handling this by optimizing performance
against the worst-case distributions within a prescribed ambiguity set.
However, existing DRO-based FL methods often overlook the detrimental impact of
outliers in local datasets, which can disproportionately bias the learned
models. In this work, we study distributionally robust federated learning with
explicit outlier resilience. We introduce a novel ambiguity set based on the
unbalanced Wasserstein distance, which jointly captures geometric
distributional shifts and incorporates a non-geometric Kullback--Leibler
penalization to mitigate the influence of outliers. This formulation naturally
leads to a challenging min--max--max optimization problem. To enable
decentralized training, we reformulate the problem as a tractable Lagrangian
penalty optimization, which admits robustness certificates. Building on this
reformulation, we propose the distributionally outlier-robust federated
learning algorithm and establish its convergence guarantees. Extensive
experiments on both synthetic and real-world datasets demonstrate the
effectiveness of our approach.

</details>


### [606] [Interpretable Kernel Representation Learning at Scale: A Unified Framework Utilizing Nyström Approximation](https://arxiv.org/abs/2509.24467)
*Maedeh Zarvandi,Michael Timothy,Theresa Wasserer,Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: KREPES is a scalable framework for kernel-based representation learning using Nyström approximation, enabling efficient learning on large datasets while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: Kernel methods have strong theoretical foundations but suffer from scalability issues, especially for representation learning with massive unlabeled data in the foundation model era.

Method: Uses Nyström approximation to create a unified framework for kernel-based representation learning that supports various unsupervised and self-supervised losses.

Result: Demonstrated efficiency on large image and tabular datasets, with principled interpretability of learned representations compared to deep models.

Conclusion: KREPES successfully addresses the scalability limitations of kernel methods for representation learning while providing interpretability benefits over deep learning approaches.

Abstract: Kernel methods provide a theoretically grounded framework for non-linear and
non-parametric learning, with strong analytic foundations and statistical
guarantees. Yet, their scalability has long been limited by prohibitive time
and memory costs. While progress has been made in scaling kernel regression, no
framework exists for scalable kernel-based representation learning, restricting
their use in the era of foundation models where representations are learned
from massive unlabeled data. We introduce KREPES -- a unified, scalable
framework for kernel-based representation learning via Nystr\"om approximation.
KREPES accommodates a wide range of unsupervised and self-supervised losses,
and experiments on large image and tabular datasets demonstrate its efficiency.
Crucially, KREPES enables principled interpretability of the learned
representations, an immediate benefit over deep models, which we substantiate
through dedicated analysis.

</details>


### [607] [FS-KAN: Permutation Equivariant Kolmogorov-Arnold Networks via Function Sharing](https://arxiv.org/abs/2509.24472)
*Ran Elbaz,Guy Bar-Shalom,Yam Eitan,Fabrizio Frasca,Haggai Maron*

Main category: cs.LG

TL;DR: FS-KAN is a principled framework for building permutation equivariant Kolmogorov-Arnold Networks that unifies and extends previous work, achieving superior data efficiency while maintaining KANs' interpretability.


<details>
  <summary>Details</summary>
Motivation: To develop a general framework for applying equivariant KANs to data with permutation symmetries, addressing the lack of principled approaches in existing literature.

Method: Generalizes parameter-sharing schemes to the Kolmogorov-Arnold setup, constructing equivariant and invariant KA layers for arbitrary permutation symmetry groups through function sharing.

Result: FS-KANs demonstrate superior data efficiency compared to standard parameter-sharing layers, with significant margins in some cases, while preserving interpretability and adaptability.

Conclusion: FS-KANs provide an excellent architecture choice for low-data regimes, combining the expressive power of parameter-sharing networks with KANs' interpretability and data efficiency advantages.

Abstract: Permutation equivariant neural networks employing parameter-sharing schemes
have emerged as powerful models for leveraging a wide range of data symmetries,
significantly enhancing the generalization and computational efficiency of the
resulting models. Recently, Kolmogorov-Arnold Networks (KANs) have demonstrated
promise through their improved interpretability and expressivity compared to
traditional architectures based on MLPs. While equivariant KANs have been
explored in recent literature for a few specific data types, a principled
framework for applying them to data with permutation symmetries in a general
context remains absent. This paper introduces Function Sharing KAN (FS-KAN), a
principled approach to constructing equivariant and invariant KA layers for
arbitrary permutation symmetry groups, unifying and significantly extending
previous work in this domain. We derive the basic construction of these FS-KAN
layers by generalizing parameter-sharing schemes to the Kolmogorov-Arnold setup
and provide a theoretical analysis demonstrating that FS-KANs have the same
expressive power as networks that use standard parameter-sharing layers,
allowing us to transfer well-known and important expressivity results from
parameter-sharing networks to FS-KANs. Empirical evaluations on multiple data
types and symmetry groups show that FS-KANs exhibit superior data efficiency
compared to standard parameter-sharing layers, by a wide margin in certain
cases, while preserving the interpretability and adaptability of KANs, making
them an excellent architecture choice in low-data regimes.

</details>


### [608] [One-Prompt Strikes Back: Sparse Mixture of Experts for Prompt-based Continual Learning](https://arxiv.org/abs/2509.24483)
*Minh Le,Bao-Ngoc Dao,Huy Nguyen,Quyen Tran,Anh Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: SMoPE is a novel continual learning framework that uses a sparse mixture of experts architecture with shared prompts to balance performance and efficiency, outperforming task-specific prompt methods while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between task-specific prompts (high performance but computational overhead) and shared prompts (efficient but suffer from knowledge interference) in continual learning.

Method: Organizes shared prompts into multiple "prompt experts" in a sparse MoE architecture, uses prompt-attention score aggregation for dynamic expert selection, adaptive noise mechanism for balanced utilization, and prototype-based loss for expert specialization.

Result: Consistently outperforms task-specific prompt methods and achieves competitive performance with state-of-the-art approaches while significantly reducing parameter counts and computational costs across multiple CL benchmarks.

Conclusion: SMoPE successfully reconciles the efficiency-performance trade-off in prompt-based continual learning through its sparse expert architecture and dynamic activation mechanisms.

Abstract: Prompt-based methods have recently gained prominence in Continual Learning
(CL) due to their strong performance and memory efficiency. A prevalent
strategy in this paradigm assigns a dedicated subset of prompts to each task,
which, while effective, incurs substantial computational overhead and causes
memory requirements to scale linearly with the number of tasks. Conversely,
approaches employing a single shared prompt across tasks offer greater
efficiency but often suffer from degraded performance due to knowledge
interference. To reconcile this trade-off, we propose SMoPE, a novel framework
that integrates the benefits of both task-specific and shared prompt
strategies. Inspired by recent findings on the relationship between Prefix
Tuning and Mixture of Experts (MoE), SMoPE organizes a shared prompt into
multiple "prompt experts" within a sparse MoE architecture. For each input,
only a select subset of relevant experts is activated, effectively mitigating
interference. To facilitate expert selection, we introduce a prompt-attention
score aggregation mechanism that computes a unified proxy score for each
expert, enabling dynamic and sparse activation. Additionally, we propose an
adaptive noise mechanism to encourage balanced expert utilization while
preserving knowledge from prior tasks. To further enhance expert
specialization, we design a prototype-based loss function that leverages prefix
keys as implicit memory representations. Extensive experiments across multiple
CL benchmarks demonstrate that SMoPE consistently outperforms task-specific
prompt methods and achieves performance competitive with state-of-the-art
approaches, all while significantly reducing parameter counts and computational
costs.

</details>


### [609] [Guided Uncertainty Learning Using a Post-Hoc Evidential Meta-Model](https://arxiv.org/abs/2509.24492)
*Charmaine Barker,Daniel Bethell,Simos Gerasimou*

Main category: cs.LG

TL;DR: GUIDE is a lightweight evidential learning meta-model that attaches to frozen deep learning models to teach them how and when to express uncertainty without retraining or architectural changes, improving OOD detection by ~77% and adversarial detection by ~80%.


<details>
  <summary>Details</summary>
Motivation: Existing post-hoc uncertainty quantification methods either inherit misplaced confidence or merely reshape predictions without teaching models when to be uncertain, creating a reliability gap.

Method: GUIDE identifies salient internal features via calibration, then uses these features to construct a noise-driven curriculum that explicitly teaches the model how and when to express uncertainty.

Result: GUIDE improves out-of-distribution detection by ~77% and adversarial attack detection by ~80% while preserving in-distribution performance, consistently outperforming state-of-the-art approaches.

Conclusion: GUIDE demonstrates the need for actively guiding uncertainty to close the gap between predictive confidence and reliability, providing broad applicability with minimal user intervention.

Abstract: Reliable uncertainty quantification remains a major obstacle to the
deployment of deep learning models under distributional shift. Existing
post-hoc approaches that retrofit pretrained models either inherit misplaced
confidence or merely reshape predictions, without teaching the model when to be
uncertain. We introduce GUIDE, a lightweight evidential learning meta-model
approach that attaches to a frozen deep learning model and explicitly learns
how and when to be uncertain. GUIDE identifies salient internal features via a
calibration stage, and then employs these features to construct a noise-driven
curriculum that teaches the model how and when to express uncertainty. GUIDE
requires no retraining, no architectural modifications, and no manual
intermediate-layer selection to the base deep learning model, thus ensuring
broad applicability and minimal user intervention. The resulting model avoids
distilling overconfidence from the base model, improves out-of-distribution
detection by ~77% and adversarial attack detection by ~80%, while preserving
in-distribution performance. Across diverse benchmarks, GUIDE consistently
outperforms state-of-the-art approaches, evidencing the need for actively
guiding uncertainty to close the gap between predictive confidence and
reliability.

</details>


### [610] [LLM DNA: Tracing Model Evolution via Functional Representations](https://arxiv.org/abs/2509.24496)
*Zhaomin Wu,Haodong Zhao,Ziyang Wang,Jizhou Guo,Qian Wang,Bingsheng He*

Main category: cs.LG

TL;DR: This paper introduces LLM DNA, a mathematical representation of large language models' functional behavior that enables tracking evolutionary relationships through fine-tuning and adaptation processes.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of LLMs has created millions of models with undocumented evolutionary relationships, making LLM management difficult. Existing methods are limited by task specificity, fixed model sets, or strict assumptions.

Method: The authors mathematically define LLM DNA as a low-dimensional, bi-Lipschitz representation of functional behavior, prove its inheritance properties, and develop a training-free pipeline for DNA extraction. They apply phylogenetic algorithms to construct evolutionary trees.

Result: Experiments across 305 LLMs show DNA aligns with prior studies and achieves competitive performance on specific tasks. DNA comparisons reveal undocumented relationships and evolutionary trees show architectural shifts from encoder-decoder to decoder-only models, temporal progression, and varying evolutionary speeds.

Conclusion: LLM DNA provides a general, scalable framework for understanding and tracking the evolutionary relationships among language models, overcoming limitations of existing methods and revealing previously undocumented connections.

Abstract: The explosive growth of large language models (LLMs) has created a vast but
opaque landscape: millions of models exist, yet their evolutionary
relationships through fine-tuning, distillation, or adaptation are often
undocumented or unclear, complicating LLM management. Existing methods are
limited by task specificity, fixed model sets, or strict assumptions about
tokenizers or architectures. Inspired by biological DNA, we address these
limitations by mathematically defining LLM DNA as a low-dimensional,
bi-Lipschitz representation of functional behavior. We prove that LLM DNA
satisfies inheritance and genetic determinism properties and establish the
existence of DNA. Building on this theory, we derive a general, scalable,
training-free pipeline for DNA extraction. In experiments across 305 LLMs, DNA
aligns with prior studies on limited subsets and achieves superior or
competitive performance on specific tasks. Beyond these tasks, DNA comparisons
uncover previously undocumented relationships among LLMs. We further construct
the evolutionary tree of LLMs using phylogenetic algorithms, which align with
shifts from encoder-decoder to decoder-only architectures, reflect temporal
progression, and reveal distinct evolutionary speeds across LLM families.

</details>


### [611] [Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models](https://arxiv.org/abs/2509.24510)
*Jonas Hübotter,Patrik Wolf,Alexander Shevchenko,Dennis Jüni,Andreas Krause,Gil Kur*

Main category: cs.LG

TL;DR: Test-time training (TTT) improves performance by allowing foundation models to specialize on test task concepts after initial generalization, addressing global underparameterization.


<details>
  <summary>Details</summary>
Motivation: To understand why and when test-time training is effective, challenging previous explanations focused on out-of-distribution adaptation and proposing that foundation models remain globally underparameterized.

Method: Proposed a theoretical model under linear representation hypothesis, empirically validated assumptions using sparse autoencoder on ImageNet, and performed scaling studies across image and language tasks.

Result: TTT achieves substantially smaller in-distribution test error than global training, with semantic data points explained by few shared concepts, and identified regimes where specialization is most effective.

Conclusion: TTT provides a mechanism for specialization after generalization, focusing model capacity on test-relevant concepts, with practical effectiveness confirmed across multiple domains.

Abstract: Recent empirical studies have explored the idea of continuing to train a
model at test-time for a given task, known as test-time training (TTT), and
have found it to yield significant performance improvements. However, there is
limited understanding of why and when TTT is effective. Earlier explanations
mostly focused on the observation that TTT may help when applied to
out-of-distribution adaptation or used with privileged data. However, the
growing scale of foundation models with most test data being in-distribution
questions these explanations. We instead posit that foundation models remain
globally underparameterized, with TTT providing a mechanism for specialization
after generalization, focusing capacity on concepts relevant to the test task.
Specifically, under the linear representation hypothesis, we propose a model in
which TTT achieves a substantially smaller in-distribution test error than
global training. We empirically validate our model's key assumptions by
training a sparse autoencoder on ImageNet, showing that semantically related
data points are explained by only a few shared concepts. Finally, we perform
scaling studies across image and language tasks that confirm the practical
implications of our model, identifying the regimes where specialization is most
effective.

</details>


### [612] [Trading Carbon for Physics: On the Resource Efficiency of Machine Learning for Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.24517)
*Sophia N. Wilson,Jens Hesselbjerg Christensen,Raghavendra Selvan*

Main category: cs.LG

TL;DR: Physics inductive biases can improve model efficiency while maintaining or enhancing efficacy in spatio-temporal forecasting, offering a principled approach to reduce ML carbon footprint.


<details>
  <summary>Details</summary>
Motivation: Current deep learning focuses solely on efficacy, leading to large models with high resource consumption and carbon footprint. The paper explores physics inductive biases as a way to balance efficacy with efficiency.

Method: Study various spatio-temporal forecasting models with different levels of physics inductive bias, including standard physics-informed models and newer flow matching methods.

Result: Physics inductive biases yield substantial efficiency gains while retaining or improving task efficacy. Flow matching shows promise as a general purpose method for spatio-temporal forecasting.

Conclusion: Model efficiency should become a core consideration alongside efficacy in ML development to reduce carbon footprint and resource consumption.

Abstract: Development of modern deep learning methods has been driven primarily by the
push for improving model efficacy (accuracy metrics). This sole focus on
efficacy has steered development of large-scale models that require massive
resources, and results in considerable carbon footprint across the model
life-cycle. In this work, we explore how physics inductive biases can offer
useful trade-offs between model efficacy and model efficiency (compute, energy,
and carbon). We study a variety of models for spatio-temporal forecasting, a
task governed by physical laws and well-suited for exploring different levels
of physics inductive bias. We show that embedding physics inductive biases into
the model design can yield substantial efficiency gains while retaining or even
improving efficacy for the tasks under consideration. In addition to using
standard physics-informed spatio-temporal models, we demonstrate the usefulness
of more recent models like flow matching as a general purpose method for
spatio-temporal forecasting. Our experiments show that incorporating physics
inductive biases offer a principled way to improve the efficiency and reduce
the carbon footprint of machine learning models. We argue that model
efficiency, along with model efficacy, should become a core consideration
driving machine learning model development and deployment.

</details>


### [613] [LEAF: A Robust Expert-Based Framework for Few-Shot Continual Event Detection](https://arxiv.org/abs/2509.24547)
*Bao-Ngoc Dao,Quang Nguyen,Luyen Ngo Dinh,Minh Le,Linh Ngo Van*

Main category: cs.LG

TL;DR: LEAF is a novel expert-based framework for Few-shot Continual Event Detection that uses mixture of experts with LoRA parameterization, semantic-aware expert selection, contrastive learning with label descriptions, and knowledge distillation to address catastrophic forgetting and data scarcity.


<details>
  <summary>Details</summary>
Motivation: Existing FCED approaches suffer from severe forgetting due to full fine-tuning of shared base models causing knowledge interference, and rely on data augmentation that introduces unnatural inputs.

Method: Integrates mixture of experts with LoRA matrices, semantic-aware expert selection for dynamic routing, contrastive learning guided by label descriptions, and knowledge distillation from previous models.

Result: Extensive experiments on multiple FCED benchmarks show LEAF consistently achieves state-of-the-art performance.

Conclusion: LEAF effectively addresses catastrophic forgetting and data scarcity in FCED through its expert-based architecture and specialized learning strategies.

Abstract: Few-shot Continual Event Detection (FCED) poses the dual challenges of
learning from limited data and mitigating catastrophic forgetting across
sequential tasks. Existing approaches often suffer from severe forgetting due
to the full fine-tuning of a shared base model, which leads to knowledge
interference between tasks. Moreover, they frequently rely on data augmentation
strategies that can introduce unnatural or semantically distorted inputs. To
address these limitations, we propose LEAF, a novel and robust expert-based
framework for FCED. LEAF integrates a specialized mixture of experts
architecture into the base model, where each expert is parameterized with
low-rank adaptation (LoRA) matrices. A semantic-aware expert selection
mechanism dynamically routes instances to the most relevant experts, enabling
expert specialization and reducing knowledge interference. To improve
generalization in limited-data settings, LEAF incorporates a contrastive
learning objective guided by label descriptions, which capture high-level
semantic information about event types. Furthermore, to prevent overfitting on
the memory buffer, our framework employs a knowledge distillation strategy that
transfers knowledge from previous models to the current one. Extensive
experiments on multiple FCED benchmarks demonstrate that LEAF consistently
achieves state-of-the-art performance.

</details>


### [614] [Training-Free Multimodal Guidance for Video to Audio Generation](https://arxiv.org/abs/2509.24550)
*Eleonora Grassucci,Giuliano Galadini,Giordano Cicchetti,Aurelio Uncini,Fabio Antonacci,Danilo Comminiello*

Main category: cs.LG

TL;DR: A training-free multimodal guidance mechanism for video-to-audio generation that uses modality embeddings to enforce unified alignment across video, audio, and text without requiring retraining.


<details>
  <summary>Details</summary>
Motivation: Existing V2A approaches require costly joint training on large datasets or rely on pairwise similarities that may fail to capture global multimodal coherence.

Method: Proposed multimodal diffusion guidance (MDG) leverages the volume spanned by modality embeddings to enforce unified alignment across video, audio, and text as a plug-and-play control signal for pretrained audio diffusion models.

Result: Experiments on VGGSound and AudioCaps show MDG consistently improves perceptual quality and multimodal alignment compared to baselines.

Conclusion: The joint multimodal guidance approach proves effective for V2A generation, providing lightweight control without retraining requirements.

Abstract: Video-to-audio (V2A) generation aims to synthesize realistic and semantically
aligned audio from silent videos, with potential applications in video editing,
Foley sound design, and assistive multimedia. Although the excellent results,
existing approaches either require costly joint training on large-scale paired
datasets or rely on pairwise similarities that may fail to capture global
multimodal coherence. In this work, we propose a novel training-free multimodal
guidance mechanism for V2A diffusion that leverages the volume spanned by the
modality embeddings to enforce unified alignment across video, audio, and text.
The proposed multimodal diffusion guidance (MDG) provides a lightweight,
plug-and-play control signal that can be applied on top of any pretrained audio
diffusion model without retraining. Experiments on VGGSound and AudioCaps
demonstrate that our MDG consistently improves perceptual quality and
multimodal alignment compared to baselines, proving the effectiveness of a
joint multimodal guidance for V2A.

</details>


### [615] [Short window attention enables long-term memorization](https://arxiv.org/abs/2509.24552)
*Loïc Cabannes,Maximilian Beck,Gergely Szilvasy,Matthijs Douze,Maria Lomeli,Jade Copet,Pierre-Emmanuel Mazaré,Gabriel Synnaeve,Hervé Jégou*

Main category: cs.LG

TL;DR: SWAX is a hybrid architecture combining sliding-window attention and xLSTM linear RNN layers. Counter-intuitively, larger sliding windows don't improve long-context performance - short windows force better xLSTM memory training. Stochastic window size training solves short-context issues and outperforms regular window attention.


<details>
  <summary>Details</summary>
Motivation: Hybrid architectures combining sliding window attention with linear RNN layers outperform individual architectures, but the impact of window length and interplay between attention and RNN layers remains under-studied.

Method: Introduce SWAX architecture with sliding-window attention and xLSTM linear RNN layers. Use stochastic window size training to force model to leverage both longer context windows and xLSTM memory.

Result: Larger sliding windows don't improve long-context performance. Short window attention encourages better xLSTM long-term memory training. Stochastic window size training significantly outperforms regular window attention on both short and long-context problems.

Conclusion: SWAX with stochastic window sizes effectively combines the strengths of both attention mechanisms and xLSTM memory, achieving superior performance across different context lengths.

Abstract: Recent works show that hybrid architectures combining sliding window softmax
attention layers with linear recurrent neural network (RNN) layers outperform
both of these architectures taken separately. However, the impact of the window
length and the interplay between softmax attention and linear RNN layers remain
under-studied. In this work, we introduce SWAX, a hybrid architecture
consisting of sliding-window attention and xLSTM linear RNN layers.
  A counter-intuitive finding with SWAX is that larger sliding windows do not
improve the long-context performance. In fact, short window attention
encourages the model to better train the long-term memory of the xLSTM, by
relying less on the softmax attention mechanism for long context-retrieval.
  The issue with small sliding windows is that they are detrimental for
short-context tasks, which could be solved with information from moderately
larger sliding windows otherwise. Therefore, we train SWAX by stochastically
changing the sliding window size, forcing the model to leverage both a longer
context window and the xLSTM memory. SWAX trained with stochastic window sizes
significantly outperforms regular window attention both on short and
long-context problems.

</details>


### [616] [Deep Reinforcement Learning in Action: Real-Time Control of Vortex-Induced Vibrations](https://arxiv.org/abs/2509.24556)
*Hussam Sababha,Bernat Font,Mohammed Daqaq*

Main category: cs.LG

TL;DR: Experimental deployment of deep reinforcement learning for active flow control of vortex-induced vibrations in a circular cylinder at Re=3000 using rotary actuation, achieving up to 95% vibration suppression.


<details>
  <summary>Details</summary>
Motivation: To demonstrate real-time control in challenging experimental settings at high Reynolds numbers, addressing practical constraints like actuator delay that were not fully explored in prior low-Reynolds-number numerical simulations.

Method: Used deep reinforcement learning with rotary actuation control, starting with state feedback (displacement and velocity) alone, then augmented with past control actions to address actuation delays.

Result: With state feedback alone: achieved 80% vibration suppression using low-frequency rotary control. With augmented feedback: achieved over 95% vibration attenuation using high-frequency rotary control that modifies vortex shedding.

Conclusion: DRL is adaptable for active flow control in real-world experiments and can overcome instrumental limitations like actuation lag, with performance improvements achieved by incorporating past control actions into the learning algorithm.

Abstract: This study showcases an experimental deployment of deep reinforcement
learning (DRL) for active flow control (AFC) of vortex-induced vibrations (VIV)
in a circular cylinder at a high Reynolds number (Re = 3000) using rotary
actuation. Departing from prior work that relied on low-Reynolds-number
numerical simulations, this research demonstrates real-time control in a
challenging experimental setting, successfully addressing practical constraints
such as actuator delay. When the learning algorithm is provided with state
feedback alone (displacement and velocity of the oscillating cylinder), the DRL
agent learns a low-frequency rotary control strategy that achieves up to 80%
vibration suppression which leverages the traditional lock-on phenomenon. While
this level of suppression is significant, it remains below the performance
achieved using high-frequency rotary actuation. The reduction in performance is
attributed to actuation delays and can be mitigated by augmenting the learning
algorithm with past control actions. This enables the agent to learn a
high-frequency rotary control strategy that effectively modifies vortex
shedding and achieves over 95% vibration attenuation. These results demonstrate
the adaptability of DRL for AFC in real-world experiments and its ability to
overcome instrumental limitations such as actuation lag.

</details>


### [617] [Emergent World Representations in OpenVLA](https://arxiv.org/abs/2509.24559)
*Marco Molinari,Leonardo Nevali,Saharsha Navani,Omar G. Younis*

Main category: cs.LG

TL;DR: The paper investigates whether Vision Language Action models (VLAs) implicitly learn world models through embedding arithmetic analysis of state transitions in OpenVLA.


<details>
  <summary>Details</summary>
Motivation: To determine if VLAs trained with policy-based RL encode internal world models, which is a key characteristic of model-based reinforcement learning.

Method: Uses embedding arithmetic on state representations and trains linear/non-linear probes on model activations to test recoverability of state transition vectors. Also analyzes earlier checkpoints and proposes SAE pipeline.

Result: Found statistically significant predictive ability for state transitions exceeding baseline embeddings, indicating OpenVLA encodes an internal world model. World model appears to emerge during training progression.

Conclusion: OpenVLA implicitly learns world models despite being trained with policy-based RL, and the world model capability develops over the training process.

Abstract: Vision Language Action models (VLAs) trained with policy-based reinforcement
learning (RL) encode complex behaviors without explicitly modeling
environmental dynamics. However, it remains unclear whether VLAs implicitly
learn world models, a hallmark of model-based RL. We propose an experimental
methodology using embedding arithmetic on state representations to probe
whether OpenVLA, the current state of the art in VLAs, contains latent
knowledge of state transitions. Specifically, we measure the difference between
embeddings of sequential environment states and test whether this transition
vector is recoverable from intermediate model activations. Using linear and non
linear probes trained on activations across layers, we find statistically
significant predictive ability on state transitions exceeding baselines
(embeddings), indicating that OpenVLA encodes an internal world model (as
opposed to the probes learning the state transitions). We investigate the
predictive ability of an earlier checkpoint of OpenVLA, and uncover hints that
the world model emerges as training progresses. Finally, we outline a pipeline
leveraging Sparse Autoencoders (SAEs) to analyze OpenVLA's world model.

</details>


### [618] [Learning to Solve Optimization Problems Constrained with Partial Differential Equations](https://arxiv.org/abs/2509.24573)
*Yusuf Guven,Vincenzo Di Vito,Ferdinando Fioretto*

Main category: cs.LG

TL;DR: A learning-based framework combining dynamic predictor and optimization surrogate for PDE-constrained optimization, achieving comparable solution quality to classical methods with 4 orders of magnitude speed improvement.


<details>
  <summary>Details</summary>
Motivation: PDE-constrained optimization is computationally demanding due to tight coupling between decision variables and PDE state variables, requiring handling high dimensional discretization and dynamic constraints.

Method: Dual-network design: dynamic predictor (time-discrete Neural Operator) approximates system trajectories, and optimization surrogate (proxy optimizer techniques) approximates optimal decisions, explicitly capturing decision-PDE dynamics coupling.

Result: Validated on Burgers' equation, heat equation and voltage regulation benchmarks; achieves solution quality comparable to Direct Method and MPC with up to 10,000x computational speed improvement.

Conclusion: The proposed framework enables real-time approximation of optimal strategies for PDE-constrained optimization problems while maintaining solution quality and dramatically reducing computational cost.

Abstract: Partial differential equation (PDE)-constrained optimization arises in many
scientific and engineering domains, such as energy systems, fluid dynamics and
material design. In these problems, the decision variables (e.g., control
inputs or design parameters) are tightly coupled with the PDE state variables,
and the feasible set is implicitly defined by the governing PDE constraints.
This coupling makes the problems computationally demanding, as it requires
handling high dimensional discretization and dynamic constraints. To address
these challenges, this paper introduces a learning-based framework that
integrates a dynamic predictor with an optimization surrogate. The dynamic
predictor, a novel time-discrete Neural Operator (Lu et al.), efficiently
approximate system trajectories governed by PDE dynamics, while the
optimization surrogate leverages proxy optimizer techniques (Kotary et al.) to
approximate the associated optimal decisions. This dual-network design enables
real-time approximation of optimal strategies while explicitly capturing the
coupling between decisions and PDE dynamics. We validate the proposed approach
on benchmark PDE-constrained optimization tasks inlacing Burgers' equation,
heat equation and voltage regulation, and demonstrate that it achieves solution
quality comparable to classical control-based algorithms, such as the Direct
Method and Model Predictive Control (MPC), while providing up to four orders of
magnitude improvement in computational speed.

</details>


### [619] [SAIP: A Plug-and-Play Scale-adaptive Module in Diffusion-based Inverse Problems](https://arxiv.org/abs/2509.24580)
*Lingyu Wang,Xiangming Meng*

Main category: cs.LG

TL;DR: SAIP is a plug-and-play module that adaptively refines the scale parameter in diffusion-based inverse problem solvers, improving reconstruction quality across diverse image restoration tasks without retraining the diffusion backbone.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based inverse problem solvers use fixed, manually tuned scales to balance prior and likelihood contributions, which is suboptimal as the ideal balance varies across timesteps and tasks, limiting performance and generalization.

Method: Propose SAIP, a plug-and-play module that adaptively refines the scale at each timestep without retraining or altering the diffusion backbone. It integrates seamlessly into existing samplers like DPS, DMPS, and πGDM.

Result: SAIP consistently improves reconstruction quality across diverse image restoration tasks, including challenging scenarios.

Conclusion: The adaptive scale refinement provided by SAIP addresses the limitations of static scale designs in diffusion-based inverse problem solvers, enhancing performance and generalization without requiring modifications to the diffusion backbone.

Abstract: Solving inverse problems with diffusion models has shown promise in tasks
such as image restoration. A common approach is to formulate the problem in a
Bayesian framework and sample from the posterior by combining the prior score
with the likelihood score. Since the likelihood term is often intractable,
estimators like DPS, DMPS, and $\pi$GDM are widely adopted. However, these
methods rely on a fixed, manually tuned scale to balance prior and likelihood
contributions. Such a static design is suboptimal, as the ideal balance varies
across timesteps and tasks, limiting performance and generalization. To address
this issue, we propose SAIP, a plug-and-play module that adaptively refines the
scale at each timestep without retraining or altering the diffusion backbone.
SAIP integrates seamlessly into existing samplers and consistently improves
reconstruction quality across diverse image restoration tasks, including
challenging scenarios.

</details>


### [620] [CURA: Size Isnt All You Need -- A Compact Universal Architecture for On-Device Intelligence](https://arxiv.org/abs/2509.24601)
*Jae-Bum Seo,Muhammad Salman,Lismer Andres Caceres-Najarro*

Main category: cs.LG

TL;DR: CURA is a compact, generalizable AI architecture inspired by analog audio circuits that achieves equivalent performance with up to 2,500x fewer parameters across diverse ML tasks including regression, classification, NLP, and computer vision.


<details>
  <summary>Details</summary>
Motivation: Existing on-device AI architectures lack compactness (parameters scale with task complexity) and generalizability (models are domain-specific, e.g., regression models can't adapt to NLP).

Method: Proposed CURA architecture inspired by analog audio signal processing circuits, providing a compact and lightweight solution for diverse machine learning tasks.

Result: Achieved equivalent accuracy with up to 2,500x fewer parameters; demonstrated consistent performance across NLP benchmarks and computer vision datasets (F1-scores up to 90%); delivered superior forecasting accuracy with 1.6x lower MAE and 2.1x lower MSE than competing models.

Conclusion: CURA provides a compact, generalizable solution that overcomes limitations of existing on-device AI architectures, enabling efficient deployment across diverse ML domains with minimal parameter requirements.

Abstract: Existing on-device AI architectures for resource-constrained environments
face two critical limitations: they lack compactness, with parameter
requirements scaling proportionally to task complexity, and they exhibit poor
generalizability, performing effectively only on specific application domains
(e.g., models designed for regression tasks cannot adapt to natural language
processing (NLP) applications). In this paper, we propose CURA, an architecture
inspired by analog audio signal processing circuits that provides a compact and
lightweight solution for diverse machine learning tasks across multiple
domains. Our architecture offers three key advantages over existing approaches:
(1) Compactness: it requires significantly fewer parameters regardless of task
complexity; (2) Generalizability: it adapts seamlessly across regression,
classification, complex NLP, and computer vision tasks; and (3) Complex pattern
recognition: it can capture intricate data patterns while maintaining extremely
low model complexity. We evaluated CURA across diverse datasets and domains.
For compactness, it achieved equivalent accuracy using up to 2,500 times fewer
parameters compared to baseline models. For generalizability, it demonstrated
consistent performance across four NLP benchmarks and one computer vision
dataset, nearly matching specialized existing models (achieving F1-scores up to
90%). Lastly, it delivers superior forecasting accuracy for complex patterns,
achieving 1.6 times lower mean absolute error and 2.1 times lower mean squared
error than competing models.

</details>


### [621] [Evaluating classification performance across operating contexts: A comparison of decision curve analysis and cost curves](https://arxiv.org/abs/2509.24608)
*Louise AC Millard,Peter A Flach*

Main category: cs.LG

TL;DR: Decision curve analysis (DCA) and cost curves are compared, showing they are closely related through Brier curves. Both methods assess model utility across decision thresholds, with Brier curves being more generally applicable.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between decision curve analysis and cost curves for model evaluation, and determine their respective strengths and limitations in assessing prediction models across different decision thresholds.

Method: Theoretical comparison of decision curve analysis (DCA) and cost curves, specifically Brier curves, by analyzing their mathematical relationships and assumptions about model calibration and threshold setting.

Result: Decision curves are closely related to Brier curves, with both sharing equivalent x-axes and always selecting the same optimal model at any given threshold. Brier curves are more generally applicable across thresholds, and the area under Brier curve equals the Brier score.

Conclusion: Brier curves provide broader applicability for model evaluation across thresholds, while DCA and Brier curves are fundamentally related and consistent in model selection. The upper envelope decision curve is suggested as a useful comparison tool in DCA.

Abstract: Classification models typically predict a score and use a decision threshold
to produce a classification. Appropriate model evaluation should carefully
consider the context in which a model will be used, including the relative
value of correct classifications of positive versus negative examples, which
affects the threshold that should be used. Decision curve analysis (DCA) and
cost curves are model evaluation approaches that assess the expected utility
and expected loss of prediction models, respectively, across decision
thresholds. We compared DCA and cost curves to determine how they are related,
and their strengths and limitations. We demonstrate that decision curves are
closely related to a specific type of cost curve called a Brier curve. Both
curves are derived assuming model scores are calibrated and setting the
classification threshold using the relative value of correct positive and
negative classifications, and the x-axis of both curves are equivalent. Net
benefit (used for DCA) and Brier loss (used for Brier curves) will always
choose the same model as optimal at any given threshold. Across thresholds,
differences in Brier loss are comparable whereas differences in net benefit
cannot be compared. Brier curves are more generally applicable (when a wider
range of thresholds are plausible), and the area under the Brier curve is the
Brier score. We demonstrate that reference lines common in each space can be
included in either and suggest the upper envelope decision curve as a useful
comparison for DCA showing the possible gain in net benefit that could be
achieved through recalibration alone.

</details>


### [622] [OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment](https://arxiv.org/abs/2509.24610)
*Liang Lin,Zhihao Xu,Junhao Dong,Jian Zhao,Yuchen Yuan,Guibin Zhang,Miao Yu,Yiming Zhang,Zhengtao Yao,Huahui Yi,Dongrui Liu,Xinfeng Li,Kun Wang*

Main category: cs.LG

TL;DR: OrthAlign uses orthogonal subspace decomposition to resolve gradient conflicts in multi-objective LLM alignment, achieving stable improvements across competing preferences like helpfulness and harmlessness.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment approaches create trade-offs between competing objectives, with improvements in one dimension often degrading others, due to unresolved parameter-level conflicts.

Method: OrthAlign decomposes parameter update spaces into orthogonal subspaces, ensuring optimization toward different preferences occurs in mathematically non-interfering directions with theoretical guarantees for stable convergence.

Result: Achieves maximum single-preference improvements of 34.61% to 50.89% across helpful, harmless, and truthful dimensions, with 13.96% average overall reward improvement.

Conclusion: OrthAlign provides a fundamental solution to multi-objective preference alignment by resolving gradient conflicts at the parameter level through orthogonal decomposition, enabling stable simultaneous improvements across competing objectives.

Abstract: Large language model (LLM) alignment faces a critical dilemma when addressing
multiple human preferences: improvements in one dimension frequently come at
the expense of others, creating unavoidable trade-offs between competing
objectives like helpfulness and harmlessness. While prior work mainly focuses
on constraint-based optimization algorithms and data selection strategies to
mitigate conflicts, these approaches overlook the fundamental issue of
resolving conflicts directly at the parameter level. In this paper, we present
OrthAlign, an innovative approach that pioneers a new paradigm by leveraging
orthogonal subspace decomposition to fundamentally resolve gradient-level
conflicts in multi-objective preference alignment. OrthAlign strategically
decomposes parameter update spaces into orthogonal subspaces, ensuring that
optimization toward different preferences occurs in mathematically
non-interfering directions. Building upon this, we provide theoretical
guarantees demonstrating that when parameter increments satisfy both orthogonal
subspace constraints and spectral norm bounds, the resulting updates exhibit
linear Lipschitz growth rather than exponential instability, ensuring stable
convergence across all preference dimensions. Extensive experiments show that:
I. OrthAlign achieves maximum single-preference improvements ranging from
34.61% to 50.89% after multiple-objective alignment across helpful, harmless,
and truthful dimensions. II. With an average overall reward improvement of
13.96%.

</details>


### [623] [Learning Hamiltonian Dynamics at Scale: A Differential-Geometric Approach](https://arxiv.org/abs/2509.24627)
*Katharina Friedl,Noémie Jaquier,Mika Liao,Danica Kragic*

Main category: cs.LG

TL;DR: RO-HNN combines Hamiltonian mechanics with model order reduction to scale physics-inspired neural networks to high-dimensional systems through a symplectic autoencoder and geometric Hamiltonian neural network.


<details>
  <summary>Details</summary>
Motivation: Existing physics-inspired neural networks enforce conservation laws but struggle with scaling to high-dimensional systems, creating a need for methods that maintain physical consistency while handling complex dynamics.

Method: Two core components: 1) geometrically-constrained symplectic autoencoder for learning low-dimensional symplectic submanifold, 2) geometric Hamiltonian neural network for modeling dynamics on the submanifold.

Result: RO-HNN provides physically-consistent, stable, and generalizable predictions of complex high-dimensional dynamics, effectively extending Hamiltonian neural networks' scope.

Conclusion: The proposed RO-HNN successfully combines conservation laws with scalability, enabling accurate modeling of high-dimensional physical systems while maintaining physical consistency.

Abstract: By embedding physical intuition, network architectures enforce fundamental
properties, such as energy conservation laws, leading to plausible predictions.
Yet, scaling these models to intrinsically high-dimensional systems remains a
significant challenge. This paper introduces Geometric Reduced-order
Hamiltonian Neural Network (RO-HNN), a novel physics-inspired neural network
that combines the conservation laws of Hamiltonian mechanics with the
scalability of model order reduction. RO-HNN is built on two core components: a
novel geometrically-constrained symplectic autoencoder that learns a
low-dimensional, structure-preserving symplectic submanifold, and a geometric
Hamiltonian neural network that models the dynamics on the submanifold. Our
experiments demonstrate that RO-HNN provides physically-consistent, stable, and
generalizable predictions of complex high-dimensional dynamics, thereby
effectively extending the scope of Hamiltonian neural networks to
high-dimensional physical systems.

</details>


### [624] [Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory](https://arxiv.org/abs/2509.24653)
*Pengxiao Lin,Zheng-An Chen,Zhi-Qin John Xu*

Main category: cs.LG

TL;DR: The Identity Bridge mechanism enables LLMs to perform out-of-distribution two-hop reasoning by supervising on zero-hop identity tasks, reshaping latent geometry through implicit nuclear-norm regularization.


<details>
  <summary>Details</summary>
Motivation: Large language models often fail at compositional reasoning tasks, particularly suffering from the "curse of two-hop reasoning" where they cannot combine multiple reasoning steps effectively.

Method: Introduces Identity Bridge - supervising models on zero-hop identity tasks, uses theoretical analysis with Emb-MLP model, employs small initialization or weight decay for complex tasks, and extends to large-scale models.

Result: Models successfully perform out-of-distribution two-hop reasoning that they otherwise completely fail, with identity supervision reshaping latent geometry and inducing implicit nuclear-norm regularization favoring low-rank solutions.

Conclusion: Identity Bridge resolves compositionality gap, latent space alignment through regularization enhances reasoning abilities, and large-scale models can achieve two-hop reasoning through latent memory, providing inspiration for improving implicit reasoning.

Abstract: Despite remarkable advances, large language models often fail at
compositional reasoning tasks, a phenomenon exemplified by the ``curse of
two-hop reasoning''. This paper introduces the Identity Bridge, a simple yet
powerful mechanism that resolves this compositionality gap by supervising the
model on a zero-hop identity task. We demonstrate empirically that this
addition enables models to successfully perform out-of-distribution two-hop
reasoning, a task they otherwise completely fail. To explain this phenomenon,
we provide a theoretical analysis using a simplified Emb-MLP model, proving
that identity supervision reshapes the model's latent geometry. We show this
alignment is induced by an implicit nuclear-norm regularization during
optimization, which favors low-rank solutions that share structure across
tasks. For complex tasks, we use small initialization or weight decay to
enhance the regularization effect, which enhances the latent space alignment
effect and slows down the generalization decay. Finally, we extend our
investigation to large-scale models, observing that they still achieve two-hop
reasoning through the latent memory, which provides crucial inspiration for
enhancing their implicit reasoning abilities.

</details>


### [625] [HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling](https://arxiv.org/abs/2509.24655)
*Max van Spengler,Artem Moskalev,Tommaso Mansi,Mangal Prakash,Rui Liao*

Main category: cs.LG

TL;DR: HyperHELM introduces hyperbolic geometry for mRNA language modeling, outperforming Euclidean models on property prediction and generalization tasks.


<details>
  <summary>Details</summary>
Motivation: Euclidean geometry may not align well with hierarchical biological structures, while hyperbolic geometry better accommodates hierarchical data but hasn't been applied to mRNA language modeling.

Method: HyperHELM implements masked language model pre-training in hyperbolic space using a hybrid design with hyperbolic layers atop Euclidean backbone, aligning representations with mRNA-amino acid biological hierarchy.

Result: Outperforms Euclidean baselines on 9/10 property prediction tasks (10% average improvement), excels in out-of-distribution generalization, and surpasses hierarchy-aware Euclidean models by 3% in antibody region annotation accuracy.

Conclusion: Hyperbolic geometry serves as an effective inductive bias for hierarchical language modeling of mRNA sequences.

Abstract: Language models are increasingly applied to biological sequences like
proteins and mRNA, yet their default Euclidean geometry may mismatch the
hierarchical structures inherent to biological data. While hyperbolic geometry
provides a better alternative for accommodating hierarchical data, it has yet
to find a way into language modeling for mRNA sequences. In this work, we
introduce HyperHELM, a framework that implements masked language model
pre-training in hyperbolic space for mRNA sequences. Using a hybrid design with
hyperbolic layers atop Euclidean backbone, HyperHELM aligns learned
representations with the biological hierarchy defined by the relationship
between mRNA and amino acids. Across multiple multi-species datasets, it
outperforms Euclidean baselines on 9 out of 10 tasks involving property
prediction, with 10% improvement on average, and excels in out-of-distribution
generalization to long and low-GC content sequences; for antibody region
annotation, it surpasses hierarchy-aware Euclidean models by 3% in annotation
accuracy. Our results highlight hyperbolic geometry as an effective inductive
bias for hierarchical language modeling of mRNA sequences.

</details>


### [626] [T-POP: Test-Time Personalization with Online Preference Feedback](https://arxiv.org/abs/2509.24696)
*Zikun Qu,Min Zhang,Mingze Kong,Xiang Li,Zhiwei Shang,Zhiyong Wang,Yikun Ban,Shuang Qiu,Yao Shu,Zhongxiang Dai*

Main category: cs.LG

TL;DR: T-POP is a real-time personalization method for LLMs that uses online pairwise preference feedback during text generation, combining test-time alignment with dueling bandits to steer decoding without updating model parameters.


<details>
  <summary>Details</summary>
Motivation: Current LLM personalization methods require fine-tuning or large user data, creating cold-start problems for new users. There's a need for real-time personalization that works immediately with minimal data.

Method: T-POP combines test-time alignment with dueling bandits to learn user preferences online. It steers frozen LLM decoding by learning a reward function from pairwise preference feedback, balancing exploration and exploitation through intelligent querying.

Result: Extensive experiments show T-POP achieves rapid and data-efficient personalization, significantly outperforming existing baselines and improving consistently with more user interactions.

Conclusion: T-POP provides an effective solution to the cold-start problem in LLM personalization by enabling real-time adaptation through online preference learning without model parameter updates.

Abstract: Personalizing large language models (LLMs) to individual user preferences is
a critical step beyond generating generically helpful responses. However,
current personalization methods are ill-suited for new users, as they typically
require either slow, resource-intensive fine-tuning or a substantial amount of
pre-existing user data, creating a significant cold-start problem. To address
this challenge, we introduce a new paradigm for real-time personalization by
learning from online pairwise preference feedback collected during text
generation. We propose T-POP (Test-Time Personalization with Online Preference
Feedback}), a novel algorithm that synergistically combines test-time alignment
with dueling bandits. Without updating the LLM parameters, T-POP steers the
decoding process of a frozen LLM by learning a reward function online that
captures user preferences. By leveraging dueling bandits, T-POP intelligently
queries the user to efficiently balance between exploring their preferences and
exploiting the learned knowledge to generate personalized text. Extensive
experiments demonstrate that T-POP achieves rapid and data-efficient
personalization, significantly outperforming existing baselines and showing
consistent improvement with more user interactions.

</details>


### [627] [FedPOB: Sample-Efficient Federated Prompt Optimization via Bandits](https://arxiv.org/abs/2509.24701)
*Pingchen Lu,Zhi Hong,Zhiwei Shang,Zhiyong Wang,Yikun Ban,Yao Shu,Min Zhang,Shuang Qiu,Zhongxiang Dai*

Main category: cs.LG

TL;DR: A federated prompt optimization framework using multi-armed bandits that addresses black-box LLM optimization, sample efficiency, and privacy-preserving collaboration among multiple users.


<details>
  <summary>Details</summary>
Motivation: Real-world LLM applications face three challenges: black-box nature of proprietary LLMs, high query costs requiring sample efficiency, and need for privacy-preserving collaboration among users.

Method: Proposed FedPOB (federated variant of Linear UCB) and FedPOB-Pref (for comparative feedback) algorithms where agents share model parameters instead of raw data, based on multi-armed bandit framework.

Result: Extensive experiments show both FedPOB and FedPOB-Pref significantly outperform existing baselines, with performance consistently improving as more agents participate in collaboration.

Conclusion: The federated multi-armed bandit approach effectively addresses the three key challenges in prompt optimization while enabling collaborative learning with proven benefits from more participating agents.

Abstract: The performance of large language models (LLMs) is highly sensitive to the
input prompt, making prompt optimization a critical task. However, real-world
application is hindered by three major challenges: (1) the black-box nature of
powerful proprietary LLMs, (2) the need for high sample efficiency due to query
costs, and (3) the desire for privacy-preserving collaboration among multiple
users. To address these challenges simultaneously, we introduce a novel
framework for sample-efficient federated prompt optimization based on
multi-armed bandits (MABs). The MAB framework is uniquely suited for this
problem as it is (1) inherently a black-box optimization method, (2)
practically sample-efficient, and (3) enables collaborative learning with
theoretically guaranteed benefit from more participating agents. We first
propose the Federated Prompt Optimization via Bandits (FedPOB) algorithm, a
federated variant of the Linear UCB algorithm, where agents collaborate by
sharing model parameters instead of raw data. We then extend our approach to
the practical setting of comparative user feedback by introducing FedPOB with
Preference Feedback (FedPOB-Pref), an efficient algorithm based on federated
dueling bandits. Extensive experiments demonstrate that both FedPOB and
FedPOB-Pref significantly outperform existing baselines and that their
performance consistently improves as more agents participate in the
collaboration, validating the effectiveness of our federated approach.

</details>


### [628] [Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF](https://arxiv.org/abs/2509.24713)
*Jing Liu*

Main category: cs.LG

TL;DR: The paper proposes a mechanistic interpretability framework to address systematic failures in RLHF reward models on longtail distributions, introducing Circuit-Aware Reward Training (CART) for improved robustness.


<details>
  <summary>Details</summary>
Motivation: RLHF reward models exhibit systematic failures on longtail distributions, leading to reward hacking and misalignment, which motivates the need for better understanding and addressing these vulnerabilities.

Method: The paper introduces a mechanistic interpretability framework that identifies specialized neural circuits for rare-event processing, and proposes Circuit-Aware Reward Training (CART) which uses circuit analysis to guide data augmentation, regularization, and ensemble strategies.

Result: The approach provides theoretical insights into reward model failures and practical interventions for improving longtail robustness through circuit specialization analysis.

Conclusion: The framework establishes formal connections between circuit specialization, reward generalization bounds, and longtail performance, offering both theoretical understanding and practical solutions for RLHF reward model vulnerabilities.

Abstract: Reinforcement Learning from Human Feedback (RLHF) reward models exhibit
systematic failures on longtail distributions, leading to reward hacking and
misalignment. We propose a mechanistic interpretability framework that
identifies specialized neural circuits responsible for rare-event processing in
reward models. Drawing from recent advances showing distributed specialization
for rare tokens in language models\citep{liu2025no, liu2025emergent}, we
hypothesize that reward models also develop functionally distinct circuits for
longtail scenarios. Our theoretical framework establishes formal connections
between circuit specialization, reward generalization bounds, and longtail
performance. We introduce \textbf{Circuit-Aware Reward Training (CART)}, which
uses circuit analysis to guide data augmentation, regularization, and ensemble
strategies. This approach provides both theoretical insights into reward model
failures and practical interventions for improving longtail robustness.

</details>


### [629] [When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training](https://arxiv.org/abs/2509.24923)
*Sanxing Chen,Xiaoyin Chen,Yukun Huang,Roy Xie,Bhuwan Dhingra*

Main category: cs.LG

TL;DR: LLMs trained with SFT and RL outperform pre-trained models on multi-armed bandit tasks, achieving performance comparable to UCB and Thompson Sampling with robust generalization to longer horizons and different bandit families.


<details>
  <summary>Details</summary>
Motivation: LLMs often explore suboptimally in sequential decision-making, and it's unclear how SFT and RL shape exploration strategies and their generalization capabilities.

Method: Train LLMs with SFT on expert trajectories and RL with tailored reward signals including regret-shaped reward and algorithmic reward for oracle imitation.

Result: Trained agents outperform pre-trained models and match UCB/Thompson Sampling performance, with robust generalization to 6x longer horizons and across bandit families.

Conclusion: RL/SFT agents tend to be greedier in exploitation but more prone to early failure; tailored reward design and evaluation beyond average regret are needed for robust exploratory behavior.

Abstract: While Large Language Models (LLMs) hold promise to become autonomous agents,
they often explore suboptimally in sequential decision-making. Recent work has
sought to enhance this capability via supervised fine-tuning (SFT) or
reinforcement learning (RL), improving regret on the classic multi-armed bandit
task. However, it remains unclear how these learning methods shape exploration
strategies and how well they generalize. We investigate both paradigms by
training LLMs with SFT on expert trajectories and RL with a range of tailored
reward signals including a strategic, regret-shaped reward to reduce variance,
and an algorithmic reward that enables oracle imitation. The resulting agents
outperform pre-trained models and achieve performance comparable to Upper
Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x
longer horizons and across bandit families. Behavioral analysis reveals that
gains often stem from more sophisticated but greedier exploitation: RL/SFT
agents are more prone to early catastrophic failure than pre-trained models,
prematurely abandoning exploration. Furthermore, agents trained to imitate UCB
learn to outperform their teacher by adopting more exploitative variants. Our
findings clarify when each training paradigm is preferable and advocate
tailored reward design and evaluation beyond average regret to promote robust
exploratory behavior.

</details>


### [630] [Discrete Variational Autoencoding via Policy Search](https://arxiv.org/abs/2509.24716)
*Michael Drolet,Firas Al-Hafez,Aditya Bhatt,Jan Peters,Oleg Arenz*

Main category: cs.LG

TL;DR: Proposes a training framework for discrete VAEs using natural gradient of non-parametric encoder to update parametric encoder without reparameterization, achieving better performance on high-dimensional data reconstruction.


<details>
  <summary>Details</summary>
Motivation: Discrete VAEs offer high bit efficiency but face challenges with exact differentiable parameterization, relying on approximations that have limited success on high-dimensional tasks like image reconstruction.

Method: Leverages natural gradient of non-parametric encoder to update parametric encoder without requiring reparameterization, combined with automatic step size adaptation and transformer-based encoder.

Result: Scales to challenging datasets like ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders, achieving 20% improvement on FID Score for ImageNet 256.

Conclusion: The proposed framework provides an effective training method for discrete VAEs that overcomes limitations of existing approaches and achieves superior performance in high-dimensional data reconstruction.

Abstract: Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit
efficiency and can be modeled with autoregressive discrete distributions,
enabling parameter-efficient multimodal search with transformers. However,
discrete random variables do not allow for exact differentiable
parameterization; therefore, discrete VAEs typically rely on approximations,
such as Gumbel-Softmax reparameterization or straight-through gradient
estimates, or employ high-variance gradient-free methods such as REINFORCE that
have had limited success on high-dimensional tasks such as image
reconstruction. Inspired by popular techniques in policy search, we propose a
training framework for discrete VAEs that leverages the natural gradient of a
non-parametric encoder to update the parametric encoder without requiring
reparameterization. Our method, combined with automatic step size adaptation
and a transformer-based encoder, scales to challenging datasets such as
ImageNet and outperforms both approximate reparameterization methods and
quantization-based discrete autoencoders in reconstructing high-dimensional
data from compact latent spaces, achieving a 20% improvement on FID Score for
ImageNet 256.

</details>


### [631] [Scaling with Collapse: Efficient and Predictable Training of LLM Families](https://arxiv.org/abs/2509.25087)
*Shane Bergsma,Bin Claire Zhang,Nolan Dey,Shaheer Muhammad,Gurpreet Gosal,Joel Hestness*

Main category: cs.LG

TL;DR: Training loss curves of LLMs collapse onto a universal trajectory when optimization hyperparameters are optimally set according to scaling laws, serving as a signature of compute-efficient training.


<details>
  <summary>Details</summary>
Motivation: To verify if the consistency phenomenon in LLM training extends to practical scaling recipes where multiple parameters are scaled jointly, and to explore applications of this collapse phenomenon.

Method: Analyze training loss curves across different model scales under optimal hyperparameter settings based on empirical scaling laws, and demonstrate applications using deviation-from-collapse diagnostics and early stopping strategies.

Result: Loss curves indeed collapse across scales when hyperparameters are optimally set for given data budgets, and this collapse enables effective training diagnostics and hyperparameter tuning efficiency.

Conclusion: Collapse of training loss curves serves as an effective tool for developing efficient LLMs, as demonstrated by the successful training of the Celerity LLM family using these insights.

Abstract: Effective LLM training relies on *consistency*, meaning that key quantities
-- such as final losses and optimal hyperparameters -- scale predictably across
model sizes. Qiu et al. (2025) recently showed that this consistency extends
beyond scalars: whole training loss curves can *collapse* onto a universal
trajectory after a simple normalization. What remains unclear is whether this
phenomenon holds for LLM families trained under *practical scaling recipes*,
where width, depth, learning rate, batch size, and weight decay are scaled
jointly. We show that it does: loss curves collapse across scales precisely
when optimization hyperparameters are set optimally for the given data budget,
in accordance with recent empirical scaling laws. Collapse thus emerges as a
signature of compute-efficient training. We demonstrate two applications at
scale: (1) deviation-from-collapse provides a sensitive, early diagnostic of
training pathologies, and (2) the predictability of collapsed curves enables
early stopping in large-scale hyperparameter tuning. Finally, we train a
competitive LLM family, *Celerity*, using these insights, highlighting collapse
as an effective tool for developing efficient LLMs.

</details>


### [632] [Q-Net: Transferable Queue Length Estimation via Kalman-based Neural Networks](https://arxiv.org/abs/2509.24725)
*Ting Gao,Elvin Isufi,Winnie Daamen,Erik-Sander Smits,Serge Hoogendoorn*

Main category: cs.LG

TL;DR: Q-Net is a data-efficient and interpretable framework for estimating queue lengths at signalized intersections using loop detector counts and aggregated floating car data, achieving over 60% RMSE improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Estimating queue lengths at signalized intersections is challenging under partially observed conditions, especially when traffic conservation assumptions are violated and existing data sources have different spatial-temporal resolutions.

Method: Q-Net integrates loop detector counts and aggregated floating car data using a tailored state-space model and AI-augmented Kalman filter (KalmanNet) that learns Kalman gain from data without requiring noise covariance knowledge.

Result: Evaluations on Rotterdam roads show Q-Net outperforms baseline methods by over 60% in RMSE, accurately tracks queue formation/dissipation, corrects aFCD-induced delays, and demonstrates strong spatial-temporal transferability.

Conclusion: Q-Net provides a robust, interpretable solution for queue length estimation that works with privacy-friendly data sources and enables deployment without costly sensing infrastructure, with potential for real-time traffic control systems.

Abstract: Estimating queue lengths at signalized intersections remains a challenge in
traffic management, especially under partially observed conditions where
vehicle flows are not fully captured. This paper introduces Q-Net, a
data-efficient and interpretable framework for queue length estimation that
performs robustly even when traffic conservation assumptions are violated.
Q-Net integrates two widely available and privacy-friendly data sources: (i)
vehicle counts from loop detectors near stop lines, and (ii) aggregated
floating car data (aFCD), which divides each road section into segments and
provides segment-wise average speed measurements. These data sources often
differ in spatial and temporal resolution, creating fusion challenges. Q-Net
addresses this by employing a tailored state-space model and an AI-augmented
Kalman filter, KalmanNet, which learns the Kalman gain from data without
requiring prior knowledge of noise covariances or full system dynamics. We
build on the vanilla KalmanNet pipeline to decouple measurement dimensionality
from section length, enabling spatial transferability across road segments.
Unlike black-box models, Q-Net maintains physical interpretability, with
internal variables linked to real-world traffic dynamics. Evaluations on main
roads in Rotterdam, the Netherlands, demonstrate that Q-Net outperforms
baseline methods by over 60\% in Root Mean Square Error (RMSE), accurately
tracking queue formation and dissipation while correcting aFCD-induced delays.
Q-Net also demonstrates strong spatial and temporal transferability, enabling
deployment without costly sensing infrastructure like cameras or radar.
Additionally, we propose a real-time variant of Q-Net, highlighting its
potential for integration into dynamic, queue-based traffic control systems.

</details>


### [633] [ORPO-Distill: Mixed-Policy Preference Optimization for Cross-Architecture LLM Distillation](https://arxiv.org/abs/2509.25100)
*Aasheesh Singh,Vishal Vaddina,Dagnachew Birru*

Main category: cs.LG

TL;DR: ORPO-Distill is a cross-architecture LLM distillation method that treats distillation as preference optimization, using diverse reasoning traces and an odds-ratio objective to contrast teacher and student outputs, achieving better performance than standard knowledge distillation baselines.


<details>
  <summary>Details</summary>
Motivation: Standard chain-of-thought (CoT) distillation methods are limited in transferring knowledge across different model architectures. The authors aim to develop a more effective distillation approach that can work across various architectures by formulating it as a preference optimization problem.

Method: ORPO-Distill uses diverse reasoning traces from teachers and employs an Odds-Ratio Preference Optimization objective to contrast teacher and student traces. It adopts a mixed-policy strategy for utilizing student-generated outputs, combining benefits of both off-policy and on-policy approaches.

Result: Experiments conducted on five datasets with multiple student models show consistent improvements over conventional black-box knowledge distillation baselines, demonstrating the effectiveness of the approach across different architectures.

Conclusion: ORPO-Distill provides a general-purpose solution for cross-architecture LLM distillation that outperforms traditional methods by treating distillation as preference optimization and using diverse reasoning traces with contrastive learning objectives.

Abstract: We introduce ORPO-Distill, a general-purpose method for cross-architecture
LLM distillation that formulates the problem as a preference optimization task.
Unlike standard CoT distillation, the approach transfers knowledge through
diverse reasoning traces. It employs an Odds-Ratio Preference Optimization
objective that contrasts teacher and student traces for more effective
learning, and adopts a mixed-policy strategy for utilizing student-generated
outputs, outperforming both off- and on-policy alternatives. Experiments on
five datasets and multiple student models show consistent improvements over
conventional black-box KD baselines.

</details>


### [634] [Beyond Softmax: A Natural Parameterization for Categorical Random Variables](https://arxiv.org/abs/2509.24728)
*Alessandro Manenti,Cesare Alippi*

Main category: cs.LG

TL;DR: The paper proposes replacing softmax with catnat function for latent categorical variables, showing improved gradient descent efficiency and better test performance across various applications.


<details>
  <summary>Details</summary>
Motivation: Latent categorical variables are widely used but pose challenges for gradient-based learning due to their discrete nature. Existing gradient estimation techniques have limitations, and softmax has information-geometric drawbacks that hinder efficient learning.

Method: Replace softmax with catnat function - a hierarchical binary splits function that creates a diagonal Fisher Information Matrix, improving gradient descent efficiency. The method is simple to implement and compatible with existing training techniques.

Result: Experiments in graph structure learning, variational autoencoders, and reinforcement learning show catnat improves learning efficiency and consistently achieves higher test performance compared to softmax.

Conclusion: Catnat provides a better alternative to softmax for latent categorical variables, offering improved gradient descent properties and better model performance while maintaining compatibility with standard training methods.

Abstract: Latent categorical variables are frequently found in deep learning
architectures. They can model actions in discrete reinforcement-learning
environments, represent categories in latent-variable models, or express
relations in graph neural networks. Despite their widespread use, their
discrete nature poses significant challenges to gradient-descent learning
algorithms. While a substantial body of work has offered improved gradient
estimation techniques, we take a complementary approach. Specifically, we: 1)
revisit the ubiquitous $\textit{softmax}$ function and demonstrate its
limitations from an information-geometric perspective; 2) replace the
$\textit{softmax}$ with the $\textit{catnat}$ function, a function composed of
a sequence of hierarchical binary splits; we prove that this choice offers
significant advantages to gradient descent due to the resulting diagonal Fisher
Information Matrix. A rich set of experiments - including graph structure
learning, variational autoencoders, and reinforcement learning - empirically
show that the proposed function improves the learning efficiency and yields
models characterized by consistently higher test performance. $\textit{Catnat}$
is simple to implement and seamlessly integrates into existing codebases.
Moreover, it remains compatible with standard training stabilization techniques
and, as such, offers a better alternative to the $\textit{softmax}$ function.

</details>


### [635] [Who invented deep residual learning?](https://arxiv.org/abs/2509.24732)
*Juergen Schmidhuber*

Main category: cs.LG

TL;DR: The paper presents a timeline of the evolution of deep residual learning, noting that the most cited scientific article of the 21st century as of 2025 is a neural network paper on deep residual learning with residual connections.


<details>
  <summary>Details</summary>
Motivation: To trace the origins and development of deep residual learning in neural networks, particularly focusing on identifying who invented this influential approach.

Method: The authors present a historical timeline analysis of the evolution of deep residual learning.

Result: The analysis reveals that the most cited scientific article of the 21st century (as of 2025) is a neural network paper on deep residual learning with residual connections.

Conclusion: The paper provides a comprehensive timeline documenting the evolution of deep residual learning, highlighting its significance in modern AI through the citation impact of the foundational work.

Abstract: Modern AI is based on deep artificial neural networks (NNs). As of 2025, the
most cited scientific article of the 21st century is an NN paper on deep
residual learning with residual connections. Who invented this? We present a
timeline of the evolution of deep residual learning.

</details>


### [636] [Rethinking Entropy Regularization in Large Reasoning Models](https://arxiv.org/abs/2509.25133)
*Yuxian Jiang,Yafu Li,Guanxu Chen,Dongrui Liu,Yu Cheng,Jing Shao*

Main category: cs.LG

TL;DR: SIREN addresses entropy collapse in RLVR for large reasoning models through selective entropy regularization with masking mechanisms and self-anchored stabilization.


<details>
  <summary>Details</summary>
Motivation: RLVR suffers from entropy collapse and premature convergence in large reasoning models, where naive entropy regularization fails due to global entropy explosion from vast action spaces.

Method: Proposed SIREN with two-step entropy masking (top-p mask and peak-entropy mask) to confine exploration to meaningful actions/states, plus self-anchored regularization for training stability.

Result: Achieved superior performance across 5 math benchmarks, including +6.6 maj@k improvement on AIME24/25 with Qwen2.5-Math-7B, with greater response diversity and maintained appropriate entropy levels.

Conclusion: SIREN effectively mitigates premature convergence in RLVR for LRMs by promoting diversity and preserving validation pass@k throughout training.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great promise
in enhancing the reasoning abilities of large reasoning models (LRMs). However,
it suffers from a critical issue: entropy collapse and premature convergence.
Naive entropy regularization, a common approach for encouraging exploration in
the traditional RL literature, fails to address this problem in the context of
LRM. Our analysis reveals that this failure stems from the vast action space
and long trajectories in LRMs, which easily trigger a global entropy explosion
as the model indiscriminately explores all possible actions and states. To
address this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method
that confines exploration to a meaningful subset of actions and states. SIREN
achieves this through a two-step entropy masking mechanism, consisting of a
top-p mask and a peak-entropy mask. In addition, regularization is transformed
into a self-anchored form to stabilize training. Across five mathematical
benchmarks, SIREN attains superior average performance over previous
entropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on
AIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes
greater response diversity and maintains entropy at an appropriate level, which
helps to preserve the validation pass@k throughout training. This effectively
mitigates the premature convergence problem common in RLVR for LRM.

</details>


### [637] [A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity](https://arxiv.org/abs/2509.24734)
*Giordano Cicchetti,Eleonora Grassucci,Danilo Comminiello*

Main category: cs.LG

TL;DR: TRIANGLE is a novel similarity measure for multimodal learning that uses triangle-area similarity in high-dimensional embedding space to improve joint alignment of three modalities without additional fusion layers.


<details>
  <summary>Details</summary>
Motivation: Current multimodal models suffer from ineffective modality alignment, where some modalities may not be properly aligned, limiting the model's ability to exploit complementary information from multiple modalities in downstream tasks.

Method: TRIANGLE computes similarity directly in the higher-dimensional space spanned by modality embeddings using triangle-area similarity, replacing cosine similarity in contrastive losses to improve joint alignment of three modalities.

Result: TRIANGLE achieves state-of-the-art results in three-modal tasks including video-text and audio-text retrieval or audio-video classification, improving cosine-based methods by up to 9 points of Recall@1.

Conclusion: TRIANGLE significantly boosts multimodal modeling performance while providing interpretable alignment rationales, demonstrating effectiveness in joint alignment of three modalities across various tasks.

Abstract: Multimodal learning plays a pivotal role in advancing artificial intelligence
systems by incorporating information from multiple modalities to build a more
comprehensive representation. Despite its importance, current state-of-the-art
models still suffer from severe limitations that prevent the successful
development of a fully multimodal model. Such methods may not provide
indicators that all the involved modalities are effectively aligned. As a
result, some modalities may not be aligned, undermining the effectiveness of
the model in downstream tasks where multiple modalities should provide
additional information that the model fails to exploit. In this paper, we
present TRIANGLE: TRI-modAl Neural Geometric LEarning, the novel proposed
similarity measure that is directly computed in the higher-dimensional space
spanned by the modality embeddings. TRIANGLE improves the joint alignment of
three modalities via a triangle-area similarity, avoiding additional fusion
layers or pairwise similarities. When incorporated in contrastive losses
replacing cosine similarity, TRIANGLE significantly boosts the performance of
multimodal modeling, while yielding interpretable alignment rationales.
Extensive evaluation in three-modal tasks such as video-text and audio-text
retrieval or audio-video classification, demonstrates that TRIANGLE achieves
state-of-the-art results across different datasets improving the performance of
cosine-based methods up to 9 points of Recall@1.

</details>


### [638] [Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption](https://arxiv.org/abs/2509.24748)
*Longxiang He,Deheng Ye,Junbo Tan,Xueqian Wang,Li Shen*

Main category: cs.LG

TL;DR: RPEX is a robust offline-to-online RL method that addresses data corruption issues by incorporating Inverse Probability Weighted (IPW) into online exploration to mitigate heavy-tailed policy behavior caused by corrupted data.


<details>
  <summary>Details</summary>
Motivation: Existing O2O RL methods focus on reducing offline policy conservatism but overlook robustness against data corruption (states, actions, rewards, dynamics), which severely degrades performance in practical environments with noisy or malicious data.

Method: Proposed RPEX method incorporates Inverse Probability Weighted (IPW) into the online exploration policy to alleviate heavy-tailed behavior induced by data corruption, enabling robust policy expansion from offline pretraining to online fine-tuning.

Result: Extensive experiments on D4RL datasets show RPEX achieves state-of-the-art O2O performance across various data corruption scenarios, demonstrating superior robustness compared to existing methods.

Conclusion: RPEX provides an effective solution for robust offline-to-online RL under data corruption, addressing the critical but unexplored challenge of handling corrupted data in practical RL deployment scenarios.

Abstract: Pretraining a policy on offline data followed by fine-tuning through online
interactions, known as Offline-to-Online Reinforcement Learning (O2O RL), has
emerged as a promising paradigm for real-world RL deployment. However, both
offline datasets and online interactions in practical environments are often
noisy or even maliciously corrupted, severely degrading the performance of O2O
RL. Existing works primarily focus on mitigating the conservatism of offline
policies via online exploration, while the robustness of O2O RL under data
corruption, including states, actions, rewards, and dynamics, is still
unexplored. In this work, we observe that data corruption induces heavy-tailed
behavior in the policy, thereby substantially degrading the efficiency of
online exploration. To address this issue, we incorporate Inverse Probability
Weighted (IPW) into the online exploration policy to alleviate
heavy-tailedness, and propose a novel, simple yet effective method termed
$\textbf{RPEX}$: $\textbf{R}$obust $\textbf{P}$olicy $\textbf{EX}$pansion.
Extensive experimental results on D4RL datasets demonstrate that RPEX achieves
SOTA O2O performance across a wide range of data corruption scenarios. Code is
available at
$\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$.

</details>


### [639] [In-Context Learning of Temporal Point Processes with Foundation Inference Models](https://arxiv.org/abs/2509.24762)
*David Berghaus,Patrick Seifner,Kostadin Cvejoski,César Ojeda,Ramsés J. Sánchez*

Main category: cs.LG

TL;DR: The paper introduces FIM-PP, a foundation model for temporal point processes that uses in-context learning to infer conditional intensity functions from event sequences, achieving performance comparable to specialized models without retraining.


<details>
  <summary>Details</summary>
Motivation: Current neural MTPP approaches require training separate specialized models for each system, which is inefficient. The authors aim to develop a general-purpose model that can infer MTPPs across different systems without retraining.

Method: Pretrain a deep neural network on a large synthetic dataset of Hawkes processes using amortized inference and in-context learning. The model learns to infer conditional intensity functions from context sets of event sequences.

Result: FIM-PP matches the performance of specialized models on next-event prediction across common benchmark datasets without additional training, and can be rapidly fine-tuned when needed.

Conclusion: The amortized inference approach provides an efficient alternative to specialized MTPP models, enabling general-purpose temporal point process inference with competitive performance.

Abstract: Modeling event sequences of multiple event types with marked temporal point
processes (MTPPs) provides a principled way to uncover governing dynamical
rules and predict future events. Current neural network approaches to MTPP
inference rely on training separate, specialized models for each target system.
We pursue a radically different approach: drawing on amortized inference and
in-context learning, we pretrain a deep neural network to infer, in-context,
the conditional intensity functions of event histories from a context defined
by sets of event sequences. Pretraining is performed on a large synthetic
dataset of MTPPs sampled from a broad distribution of Hawkes processes. Once
pretrained, our Foundation Inference Model for Point Processes (FIM-PP) can
estimate MTPPs from real-world data without any additional training, or be
rapidly finetuned to target systems. Experiments show that this amortized
approach matches the performance of specialized models on next-event prediction
across common benchmark datasets.
  Our pretrained model, repository and tutorials will soon be available online

</details>


### [640] [SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression](https://arxiv.org/abs/2509.25176)
*Haoming Wen,Yushi Bai,Juanzi Li,Jie Tang*

Main category: cs.LG

TL;DR: SIRI is a reinforcement learning approach that alternates between compressing and expanding reasoning length during training, improving both performance and efficiency in Large Reasoning Models.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with repetitive thinking patterns in LRMs, creating a trade-off between reducing redundancy and maintaining performance.

Method: Iterative training regime that dynamically adjusts maximum rollout length - compression phase reduces length to force precise decisions, expansion phase increases length for exploration.

Result: After 3 iterations, SIRI-low improved AIME24 performance by 43.2% while reducing token usage by 46.9%; SIRI-high achieved highest accuracy among all methods.

Conclusion: Periodically oscillating output truncation length during training dynamically balances exploration and efficiency, converging to an optimal sweet spot between performance and efficiency.

Abstract: We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved
Compression, a simple yet effective RL approach for Large Reasoning Models
(LRMs) that enables more efficient and accurate reasoning. Existing studies
have observed repetitive thinking patterns in LRMs, and attempts to reduce them
often come at the cost of performance. In this paper, we show that this
trade-off can be overcome through a training regime that iteratively alternates
between compressing and expanding the reasoning budget, by dynamically
adjusting the maximum rollout length during training. The compression phase
cuts the rollout length, forcing the model to make precise and valuable
decisions within a limited context, which effectively reduces redundant tokens
and increases reasoning density. The expansion phase then relaxes the length
limit, providing space for the model to explore and plan in long-horizon
settings. Remarkably, we find that after each compression-expansion cycle, the
model's performance improves even as its output length decreases, steadily
pushing it closer to the Pareto frontier in the performance-efficiency
trade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves
performance on AIME24 by 43.2% while reducing token usage by 46.9% after three
iterations, and SIRI-high achieves the highest accuracy compared to all other
methods (Figure 1). Our findings shed light on the potential of periodically
oscillating the LRM's output truncation length during training to dynamically
balance exploration and efficiency in reasoning, converging towards an optimal
"sweet spot" between the two. Our models are publicly available.

</details>


### [641] [Neural Message-Passing on Attention Graphs for Hallucination Detection](https://arxiv.org/abs/2509.24770)
*Fabrizio Frasca,Guy Bar-Shalom,Yftah Ziser,Haggai Maron*

Main category: cs.LG

TL;DR: CHARM is a graph learning approach for detecting LLM hallucinations by representing computational traces as attributed graphs and applying GNNs, outperforming existing methods across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing hallucination detection methods rely on heuristics or simple models over isolated computational traces like activations or attention maps, lacking unified analysis.

Method: Represent computational traces as attributed graphs with tokens as nodes, edges following attentional flows, and features from attention scores and activations. Apply GNNs for hallucination detection.

Result: CHARM consistently outperforms other leading approaches across diverse benchmarks, shows promising zero-shot performance on cross-dataset transfer, and provably subsumes prior attention-based heuristics.

Conclusion: Graph structure plays a relevant role in hallucination detection, and combining computational traces provides benefits. CHARM demonstrates effective unified analysis of LLM hallucinations.

Abstract: Large Language Models (LLMs) often generate incorrect or unsupported content,
known as hallucinations. Existing detection methods rely on heuristics or
simple models over isolated computational traces such as activations, or
attention maps. We unify these signals by representing them as attributed
graphs, where tokens are nodes, edges follow attentional flows, and both carry
features from attention scores and activations. Our approach, CHARM, casts
hallucination detection as a graph learning task and tackles it by applying
GNNs over the above attributed graphs. We show that CHARM provably subsumes
prior attention-based heuristics and, experimentally, it consistently
outperforms other leading approaches across diverse benchmarks. Our results
shed light on the relevant role played by the graph structure and on the
benefits of combining computational traces, whilst showing CHARM exhibits
promising zero-shot performance on cross-dataset transfer.

</details>


### [642] [MarS-FM: Generative Modeling of Molecular Dynamics via Markov State Models](https://arxiv.org/abs/2509.24779)
*Kacper Kapuśniak,Cristian Gabellini,Michael Bronstein,Prudencio Tossou,Francesco Di Giovanni*

Main category: cs.LG

TL;DR: MSM Emulators learn to sample transitions across discrete states defined by an underlying Markov State Model, offering significant speedup over traditional MD simulations while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Molecular Dynamics is computationally expensive due to fine-grained integration and long timescales, and existing generative models are dominated by uninformative transitions.

Method: Introduce MSM Emulators class with Markov Space Flow Matching (MarS-FM) that samples transitions across discrete MSM states instead of learning fixed-lag transition density.

Result: MarS-FM provides >100x speedup vs implicit/explicit-solvent MD, outperforms existing methods across structural metrics (RMSD, radius of gyration, secondary structure) on diverse protein domains up to 500 residues.

Conclusion: MSM Emulators with MarS-FM offer efficient and accurate trajectory generation, substantially outperforming existing methods while maintaining generalization across chemically diverse proteins.

Abstract: Molecular Dynamics (MD) is a powerful computational microscope for probing
protein functions. However, the need for fine-grained integration and the long
timescales of biomolecular events make MD computationally expensive. To address
this, several generative models have been proposed to generate surrogate
trajectories at lower cost. Yet, these models typically learn a fixed-lag
transition density, causing the training signal to be dominated by frequent but
uninformative transitions. We introduce a new class of generative models, MSM
Emulators, which instead learn to sample transitions across discrete states
defined by an underlying Markov State Model (MSM). We instantiate this class
with Markov Space Flow Matching (MarS-FM), whose sampling offers more than two
orders of magnitude speedup compared to implicit- or explicit-solvent MD
simulations. We benchmark Mars-FM ability to reproduce MD statistics through
structural observables such as RMSD, radius of gyration, and secondary
structure content. Our evaluation spans protein domains (up to 500 residues)
with significant chemical and structural diversity, including unfolding events,
and enforces strict sequence dissimilarity between training and test sets to
assess generalization. Across all metrics, MarS-FM outperforms existing
methods, often by a substantial margin.

</details>


### [643] [Quantifying Generalisation in Imitation Learning](https://arxiv.org/abs/2509.24784)
*Nathan Gavenski,Odinaldo Rodrigues*

Main category: cs.LG

TL;DR: Labyrinth is a benchmarking environment for imitation learning that enables controlled testing of generalization with distinct training/evaluation settings, flexible task variations, and interpretable evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing imitation learning benchmarks lack sufficient variation between training and evaluation, limiting meaningful assessment of generalization capabilities.

Method: Developed Labyrinth environment with precise control over structure, start/goal positions, and task complexity. Features discrete, fully observable state space, known optimal actions, and variants like partial observability, key-and-door tasks, and ice-floor hazards.

Result: Labyrinth enables verifiably distinct training, evaluation, and test settings, supporting interpretability and fine-grained evaluation of generalization factors.

Conclusion: Labyrinth advances generalization evaluation in imitation learning through controlled, reproducible experiments and provides a valuable tool for developing more robust agents.

Abstract: Imitation learning benchmarks often lack sufficient variation between
training and evaluation, limiting meaningful generalisation assessment. We
introduce Labyrinth, a benchmarking environment designed to test generalisation
with precise control over structure, start and goal positions, and task
complexity. It enables verifiably distinct training, evaluation, and test
settings. Labyrinth provides a discrete, fully observable state space and known
optimal actions, supporting interpretability and fine-grained evaluation. Its
flexible setup allows targeted testing of generalisation factors and includes
variants like partial observability, key-and-door tasks, and ice-floor hazards.
By enabling controlled, reproducible experiments, Labyrinth advances the
evaluation of generalisation in imitation learning and provides a valuable tool
for developing more robust agents.

</details>


### [644] [Assessing the risk of future Dunkelflaute events for Germany using generative deep learning](https://arxiv.org/abs/2509.24788)
*Felix Strnad,Jonathan Schmidt,Fabian Mockert,Philipp Hennig,Nicole Ludwig*

Main category: cs.LG

TL;DR: Study analyzes impact of Dunkelflaute events (low wind/solar periods) on German electricity production using climate simulations, finding frequency and duration remain largely unchanged under future climate scenarios.


<details>
  <summary>Details</summary>
Motivation: European grid's transition to weather-dependent renewables creates vulnerability to Dunkelflaute events that could cause electricity shortages, requiring assessment of future risks.

Method: Adapted generative deep learning framework to downscale CMIP6 climate simulations, compared with ERA5 historical data, analyzed under SSP2-4.5 and SSP5-8.5 emission scenarios.

Result: Both frequency and duration of Dunkelflaute events in Germany projected to remain largely unchanged compared to historical period in ensemble mean.

Conclusion: Risk from Dunkelflaute events expected to remain stable throughout the century under considered climate scenarios, suggesting no significant increase in grid vulnerability from climate change.

Abstract: The European electricity power grid is transitioning towards renewable energy
sources, characterized by an increasing share of off- and onshore wind and
solar power. However, the weather dependency of these energy sources poses a
challenge to grid stability, with so-called Dunkelflaute events -- periods of
low wind and solar power generation -- being of particular concern due to their
potential to cause electricity supply shortages. In this study, we investigate
the impact of these events on the German electricity production in the years
and decades to come. For this purpose, we adapt a recently developed generative
deep learning framework to downscale climate simulations from the CMIP6
ensemble. We first compare their statistics to the historical record taken from
ERA5 data. Next, we use these downscaled simulations to assess plausible future
occurrences of Dunkelflaute events in Germany under the optimistic low
(SSP2-4.5) and high (SSP5-8.5) emission scenarios. Our analysis indicates that
both the frequency and duration of Dunkelflaute events in Germany in the
ensemble mean are projected to remain largely unchanged compared to the
historical period. This suggests that, under the considered climate scenarios,
the associated risk is expected to remain stable throughout the century.

</details>


### [645] [Fidel-TS: A High-Fidelity Benchmark for Multimodal Time Series Forecasting](https://arxiv.org/abs/2509.24789)
*Zhijian Xu,Wanxu Cai,Xilin Dai,Zhaorong Deng,Qiang Xu*

Main category: cs.LG

TL;DR: Fidel-TS is a new large-scale time series forecasting benchmark built from live APIs that addresses data contamination and leakage issues in existing benchmarks, demonstrating that causal relevance of textual information is key for genuine performance gains.


<details>
  <summary>Details</summary>
Motivation: Current time series forecasting benchmarks suffer from critical issues including pre-training data contamination in LLMs and causal/description leakage in multimodal designs, creating an illusion of progress in the field.

Method: The authors formalize core principles of high-fidelity benchmarking (data sourcing integrity, strict causal soundness, structural clarity) and build Fidel-TS from ground up using live API data sources.

Result: Extensive experiments validate the approach by exposing critical biases and design limitations of prior benchmarks, and demonstrate that causal relevance of textual information is the key factor for genuine performance improvements.

Conclusion: Fidel-TS provides a reliable benchmark for time series forecasting that addresses fundamental issues in existing datasets, establishing causal relevance of textual information as crucial for meaningful multimodal forecasting advances.

Abstract: The evaluation of time series forecasting models is hindered by a critical
lack of high-quality benchmarks, leading to a potential illusion of progress.
Existing datasets suffer from issues ranging from pre-training data
contamination in the age of LLMs to the causal and description leakage
prevalent in early multimodal designs. To address this, we formalize the core
principles of high-fidelity benchmarking, focusing on data sourcing integrity,
strict causal soundness, and structural clarity. We introduce Fidel-TS, a new
large-scale benchmark built from the ground up on these principles by sourcing
data from live APIs. Our extensive experiments validate this approach by
exposing the critical biases and design limitations of prior benchmarks.
Furthermore, we conclusively demonstrate that the causal relevance of textual
information is the key factor in unlocking genuine performance gains in
multimodal forecasting.

</details>


### [646] [DSAT-HD: Dual-Stream Adaptive Transformer with Hybrid Decomposition for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.24800)
*Zixu Wang,Hongbin Dong,Xiaoping Zhang*

Main category: cs.LG

TL;DR: DSAT-HD is a novel time series forecasting model that combines hybrid decomposition, multi-scale adaptive pathways, and dual-stream residual learning to overcome limitations of existing Transformer-based methods in handling diverse temporal features and complex seasonality.


<details>
  <summary>Details</summary>
Motivation: Existing time series forecasting methods based on Transformers primarily model limited time series or fixed scales, making it challenging to capture diverse features across different ranges. Traditional decomposition methods like STL require pre-specified seasonal periods and typically handle only single, fixed seasonality.

Method: 1) Hybrid decomposition combining EMA and Fourier decomposition with RevIN normalization and noise Top-k gating; 2) Multi-scale adaptive pathway with sparse allocator routing features to four parallel Transformer layers and sparse combiner for feature merging; 3) Dual-stream residual learning framework with CNN and MLP branches processing seasonal/trend components separately, coordinated by balanced loss function.

Result: Extensive experiments on nine datasets show DSAT-HD outperforms existing methods overall and achieves state-of-the-art performance on some datasets. It also exhibits stronger generalization capabilities across various transfer scenarios.

Conclusion: DSAT-HD successfully addresses the limitations of existing time series forecasting methods by integrating hybrid decomposition, multi-scale adaptive pathways, and dual-stream learning, demonstrating superior performance and generalization capabilities.

Abstract: Time series forecasting is crucial for various applications, such as weather,
traffic, electricity, and energy predictions. Currently, common time series
forecasting methods are based on Transformers. However, existing approaches
primarily model limited time series or fixed scales, making it more challenging
to capture diverse features cross different ranges. Additionally, traditional
methods like STL for complex seasonality-trend decomposition require
pre-specified seasonal periods and typically handle only single, fixed
seasonality. We propose the Hybrid Decomposition Dual-Stream Adaptive
Transformer (DSAT-HD), which integrates three key innovations to address the
limitations of existing methods: 1) A hybrid decomposition mechanism combining
EMA and Fourier decomposition with RevIN normalization, dynamically balancing
seasonal and trend components through noise Top-k gating; 2) A multi-scale
adaptive pathway leveraging a sparse allocator to route features to four
parallel Transformer layers, followed by feature merging via a sparse combiner,
enhanced by hybrid attention combining local CNNs and global interactions; 3) A
dual-stream residual learning framework where CNN and MLP branches separately
process seasonal and trend components, coordinated by a balanced loss function
minimizing expert collaboration variance. Extensive experiments on nine
datasets demonstrate that DSAT-HD outperforms existing methods overall and
achieves state-of-the-art performance on some datasets. Notably, it also
exhibits stronger generalization capabilities across various transfer
scenarios.

</details>


### [647] [Physics-informed learning under mixing: How physical knowledge speeds up learning](https://arxiv.org/abs/2509.24801)
*Anna Scampicchio,Leonardo F. Toso,Rahel Rickenbach,James Anderson,Melanie N. Zeilinger*

Main category: cs.LG

TL;DR: Physics-informed regularization improves learning rates from slow Sobolev minimax rate to fast optimal i.i.d. rate when physical prior is aligned, eliminating sample-size deflation from data dependence.


<details>
  <summary>Details</summary>
Motivation: To understand how domain knowledge incorporation affects learning rates with dependent data in physics-informed machine learning.

Method: Empirical risk minimization with physics-informed regularization, deriving complexity-dependent bounds on excess risk.

Result: When physical prior information is aligned, learning rate improves significantly without sample-size deflation due to data dependence.

Conclusion: Properly aligned physics-informed regularization can overcome the slow learning rates typically associated with dependent data.

Abstract: A major challenge in physics-informed machine learning is to understand how
the incorporation of prior domain knowledge affects learning rates when data
are dependent. Focusing on empirical risk minimization with physics-informed
regularization, we derive complexity-dependent bounds on the excess risk in
probability and in expectation. We prove that, when the physical prior
information is aligned, the learning rate improves from the (slow) Sobolev
minimax rate to the (fast) optimal i.i.d. one without any sample-size deflation
due to data dependence.

</details>


### [648] [DyMoDreamer: World Modeling with Dynamic Modulation](https://arxiv.org/abs/2509.24804)
*Boxuan Zhang,Runqing Wang,Wei Xiao,Weipu Zhang,Jian Sun,Gao Huang,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: DyMoDreamer is a novel model-based reinforcement learning algorithm that uses dynamic modulation to improve sample efficiency by focusing on dynamic features and temporal information, achieving state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address sample inefficiency in deep reinforcement learning by improving world models to better handle dynamic objects and temporal features, which are crucial for visual tasks but poorly handled by conventional holistic observation processing.

Method: Introduces dynamic modulation mechanism using differential observations from inter-frame differencing mask, models dynamic modulation as stochastic categorical distributions, and integrates into recurrent state-space model (RSSM) to enhance focus on reward-relevant dynamics.

Result: Sets new state-of-the-art on Atari 100k benchmark (156.6% mean human-normalized score), establishes new record of 832 on DeepMind Visual Control Suite, and achieves 9.5% performance improvement after 1M steps on Crafter benchmark.

Conclusion: DyMoDreamer effectively addresses sample inefficiency in DRL by explicitly modeling dynamic features and temporal dynamics, demonstrating superior performance across multiple challenging benchmarks.

Abstract: A critical bottleneck in deep reinforcement learning (DRL) is sample
inefficiency, as training high-performance agents often demands extensive
environmental interactions. Model-based reinforcement learning (MBRL) mitigates
this by building world models that simulate environmental dynamics and generate
synthetic experience, improving sample efficiency. However, conventional world
models process observations holistically, failing to decouple dynamic objects
and temporal features from static backgrounds. This approach is computationally
inefficient, especially for visual tasks where dynamic objects significantly
influence rewards and decision-making performance. To address this, we
introduce DyMoDreamer, a novel MBRL algorithm that incorporates a dynamic
modulation mechanism to improve the extraction of dynamic features and enrich
the temporal information. DyMoDreamer employs differential observations derived
from a novel inter-frame differencing mask, explicitly encoding object-level
motion cues and temporal dynamics. Dynamic modulation is modeled as stochastic
categorical distributions and integrated into a recurrent state-space model
(RSSM), enhancing the model's focus on reward-relevant dynamics. Experiments
demonstrate that DyMoDreamer sets a new state-of-the-art on the Atari $100$k
benchmark with a $156.6$\% mean human-normalized score, establishes a new
record of $832$ on the DeepMind Visual Control Suite, and gains a $9.5$\%
performance improvement after $1$M steps on the Crafter benchmark. Our code is
released at https://github.com/Ultraman-Tiga1/DyMoDreamer.

</details>


### [649] [Putnam-like dataset summary: LLMs as mathematical competition contestants](https://arxiv.org/abs/2509.24827)
*Bartosz Bieganowski,Daniel Strzelecki,Robert Skiba,Mateusz Topolewski*

Main category: cs.LG

TL;DR: Analysis of LLM performance on 96 Putnam-like math problems from Google DeepMind's benchmark dataset with 576 solutions.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' capability in solving mathematical contest problems similar to those in the Putnam Competition.

Method: Analyzed performance of LLMs on 96 original Putnam-like problems and their 576 solutions from Google DeepMind's benchmark.

Result: The paper presents performance analysis results showing how well LLMs can handle mathematical contest problems.

Conclusion: Provides insights into LLMs' mathematical problem-solving abilities through systematic evaluation on Putnam-style problems.

Abstract: In this paper we summarize the results of the Putnam-like benchmark published
by Google DeepMind. This dataset consists of 96 original problems in the spirit
of the Putnam Competition and 576 solutions of LLMs. We analyse the performance
of models on this set of problems to verify their ability to solve problems
from mathematical contests.

</details>


### [650] [Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from RNA-Seq Data](https://arxiv.org/abs/2509.24840)
*Oussama Kharouiche,Aris Markogiannakis,Xiao Fei,Michail Chatzianastasis,Michalis Vazirgiannis*

Main category: cs.LG

TL;DR: Cell2Text is a multimodal framework that translates single-cell RNA sequencing profiles into structured natural language descriptions, outperforming traditional discrete prediction methods and providing interpretable outputs.


<details>
  <summary>Details</summary>
Motivation: Current single-cell foundation models use discrete prediction heads that collapse cellular complexity into predefined labels, failing to capture the richer contextual explanations needed by biologists.

Method: Integrates gene-level embeddings from single-cell foundation models with pretrained large language models to generate coherent natural language summaries of cellular identity, tissue origin, disease associations, and pathway activity.

Result: Outperforms baselines on classification accuracy, demonstrates strong ontological consistency using PageRank-based similarity metrics, and achieves high semantic fidelity in text generation.

Conclusion: Coupling expression data with natural language offers both stronger predictive performance and inherently interpretable outputs, providing a scalable path for label-efficient characterization of unseen cells.

Abstract: Single-cell RNA sequencing has transformed biology by enabling the
measurement of gene expression at cellular resolution, providing information
for cell types, states, and disease contexts. Recently, single-cell foundation
models have emerged as powerful tools for learning transferable representations
directly from expression profiles, improving performance on classification and
clustering tasks. However, these models are limited to discrete prediction
heads, which collapse cellular complexity into predefined labels that fail to
capture the richer, contextual explanations biologists need. We introduce
Cell2Text, a multimodal generative framework that translates scRNA-seq profiles
into structured natural language descriptions. By integrating gene-level
embeddings from single-cell foundation models with pretrained large language
models, Cell2Text generates coherent summaries that capture cellular identity,
tissue origin, disease associations, and pathway activity, generalizing to
unseen cells. Empirically, Cell2Text outperforms baselines on classification
accuracy, demonstrates strong ontological consistency using PageRank-based
similarity metrics, and achieves high semantic fidelity in text generation.
These results demonstrate that coupling expression data with natural language
offers both stronger predictive performance and inherently interpretable
outputs, pointing to a scalable path for label-efficient characterization of
unseen cells.

</details>


### [651] [Beyond the Hook: Predicting Billboard Hot 100 Chart Inclusion with Machine Learning from Streaming, Audio Signals, and Perceptual Features](https://arxiv.org/abs/2509.24856)
*Christos Mountzouris*

Main category: cs.LG

TL;DR: This study identifies key predictors for Billboard Hot 100 chart inclusion, finding popularity as the strongest factor, with audio attributes like instrumentalness, valence, duration, and speechiness also contributing significantly.


<details>
  <summary>Details</summary>
Motivation: To investigate which determinants most strongly predict a track's inclusion in Billboard Hot 100 charts using digital streaming data, including streaming popularity, audio signal attributes, and human listening indicators.

Method: Used machine learning models (Logistic Regression, Random Forest, XGBoost) to analyze predictors including streaming popularity, audio attributes (instrumentalness, valence, duration, speechiness), and human listening indicators.

Result: Popularity was the most decisive predictor. Logistic Regression achieved 90.0% accuracy, Random Forest 90.4%, and XGBoost 90.3%. All models showed high performance with F1-scores around 0.90-0.91.

Conclusion: Streaming popularity is the strongest predictor of Billboard Hot 100 inclusion, with audio attributes providing additional predictive power. Machine learning models can accurately predict chart success with over 90% accuracy.

Abstract: The advent of digital streaming platforms have recently revolutionized the
landscape of music industry, with the ensuing digitalization providing
structured data collections that open new research avenues for investigating
popularity dynamics and mainstream success. The present work explored which
determinants hold the strongest predictive influence for a track's inclusion in
the Billboard Hot 100 charts, including streaming popularity, measurable audio
signal attributes, and probabilistic indicators of human listening. The
analysis revealed that popularity was by far the most decisive predictor of
Billboard Hot 100 inclusion, with considerable contribution from
instrumentalness, valence, duration and speechiness. Logistic Regression
achieved 90.0% accuracy, with very high recall for charting singles (0.986) but
lower recall for non-charting ones (0.813), yielding balanced F1-scores around
0.90. Random Forest slightly improved performance to 90.4% accuracy,
maintaining near-perfect precision for non-charting singles (0.990) and high
recall for charting ones (0.992), with F1-scores up to 0.91. Gradient Boosting
(XGBoost) reached 90.3% accuracy, delivering a more balanced trade-off by
improving recall for non-charting singles (0.837) while sustaining high recall
for charting ones (0.969), resulting in F1-scores comparable to the other
models.

</details>


### [652] [DRIFT-Net: A Spectral--Coupled Neural Operator for PDEs Learning](https://arxiv.org/abs/2509.24868)
*Jiayi Li,Flora D. Salim*

Main category: cs.LG

TL;DR: DRIFT-Net proposes a dual-branch neural network architecture for PDE learning that combines spectral and image branches to capture both global low-frequency information and local details, addressing limitations of attention-based methods in maintaining global consistency during long-term rollouts.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models for PDEs using multi-scale windowed self-attention suffer from weak global coupling due to locality, leading to error accumulation and drift during closed-loop rollouts. The motivation is to develop a method that maintains strong global spectral coupling while preserving local details.

Method: DRIFT-Net employs a dual-branch design with spectral and image branches. The spectral branch captures global low-frequency information, while the image branch handles local details. Controlled lightweight mixing within low-frequency range, followed by bandwise weighting fusion of both branches at each layer, and transformation back to spatial domain while preserving both global structure and high-frequency details.

Result: Compared to attention-based baselines, DRIFT-Net achieves lower error and higher throughput with fewer parameters. On Navier-Stokes benchmarks: relative L1 error reduced by 7%-54%, parameter count decreased by ~15%, throughput remains higher than scOT. Ablation studies and theoretical analyses demonstrate stability and effectiveness.

Conclusion: DRIFT-Net's dual-branch architecture effectively addresses global coupling limitations in PDE foundation models, providing improved accuracy, efficiency, and stability for learning PDE dynamics compared to existing attention-based approaches.

Abstract: Learning PDE dynamics with neural solvers can significantly improve
wall-clock efficiency and accuracy compared with classical numerical solvers.
In recent years, foundation models for PDEs have largely adopted multi-scale
windowed self-attention, with the scOT backbone in \textsc{Poseidon} serving as
a representative example.
  However, because of their locality, truly globally consistent spectral
coupling can only be propagated gradually through deep stacking and window
shifting. This weakens global coupling and leads to error accumulation and
drift during closed-loop rollouts. To address this, we propose
\textbf{DRIFT-Net}. It employs a dual-branch design comprising a spectral
branch and an image branch. The spectral branch is responsible for capturing
global, large-scale low-frequency information, whereas the image branch focuses
on local details and nonstationary structures. Specifically, we first perform
controlled, lightweight mixing within the low-frequency range. Then we fuse the
spectral and image paths at each layer via bandwise weighting, which avoids the
width inflation and training instability caused by naive concatenation. The
fused result is transformed back into the spatial domain and added to the image
branch, thereby preserving both global structure and high-frequency details
across scales. Compared with strong attention-based baselines, DRIFT-Net
achieves lower error and higher throughput with fewer parameters under
identical training settings and budget. On Navier--Stokes benchmarks, the
relative $L_{1}$ error is reduced by 7\%--54\%, the parameter count decreases
by about 15\%, and the throughput remains higher than scOT. Ablation studies
and theoretical analyses further demonstrate the stability and effectiveness of
this design. The code is available at
https://github.com/cruiseresearchgroup/DRIFT-Net.

</details>


### [653] [Uncertainty-Guided Expert-AI Collaboration for Efficient Soil Horizon Annotation](https://arxiv.org/abs/2509.24873)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: This paper applies conformal prediction to SoilNet, a multimodal multitask soil profile model, to improve uncertainty quantification and enable more efficient human-in-the-loop annotation with limited expert budget.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification is essential for human-machine collaboration, as humans adjust decisions based on machine confidence. Reliable uncertainty calibration enables more effective collaboration, targeted expert intervention, and responsible ML usage.

Method: Apply conformal prediction framework to SoilNet model, design simulated human-in-the-loop annotation pipeline where domain experts provide ground truth annotations only when model uncertainty is high, within limited budget constraints.

Result: Conformalizing SoilNet leads to more efficient annotation in regression tasks and comparable performance scores in classification tasks under the same annotation budget compared to non-conformal counterpart.

Conclusion: Conformal prediction effectively improves uncertainty quantification for SoilNet, enabling more efficient human-in-the-loop annotation workflows while maintaining performance, particularly beneficial for regression tasks.

Abstract: Uncertainty quantification is essential in human-machine collaboration, as
human agents tend to adjust their decisions based on the confidence of the
machine counterpart. Reliably calibrated model uncertainties, hence, enable
more effective collaboration, targeted expert intervention and more responsible
usage of Machine Learning (ML) systems. Conformal prediction has become a well
established model-agnostic framework for uncertainty calibration of ML models,
offering statistically valid confidence estimates for both regression and
classification tasks. In this work, we apply conformal prediction to
$\textit{SoilNet}$, a multimodal multitask model for describing soil profiles.
We design a simulated human-in-the-loop (HIL) annotation pipeline, where a
limited budget for obtaining ground truth annotations from domain experts is
available when model uncertainty is high. Our experiments show that
conformalizing SoilNet leads to more efficient annotation in regression tasks
and comparable performance scores in classification tasks under the same
annotation budget when tested against its non-conformal counterpart. All code
and experiments can be found in our repository:
https://github.com/calgo-lab/BGR

</details>


### [654] [Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning Regime](https://arxiv.org/abs/2509.24882)
*Leonardo Defilippis,Yizhou Xu,Julius Girardin,Emanuele Troiani,Vittorio Erba,Lenka Zdeborová,Bruno Loureiro,Florent Krzakala*

Main category: cs.LG

TL;DR: This paper provides a theoretical analysis of neural scaling laws for quadratic and diagonal neural networks in the feature learning regime, connecting them to matrix compressed sensing and LASSO to derive phase diagrams and explain empirical observations about weight spectrum properties.


<details>
  <summary>Details</summary>
Motivation: While neural scaling laws drive recent deep learning advances, their theoretical understanding remains limited to linear models. The authors aim to extend this understanding to more complex neural networks in the feature learning regime.

Method: The authors analyze scaling laws for quadratic and diagonal neural networks using connections with matrix compressed sensing and LASSO, deriving phase diagrams for scaling exponents as functions of sample complexity and weight decay.

Result: The analysis reveals crossovers between distinct scaling regimes and plateau behaviors, mirroring empirical observations. It establishes a precise link between these regimes and the spectral properties of trained network weights, particularly the emergence of power-law tails in the weight spectrum.

Conclusion: The work provides theoretical validation for empirical observations connecting power-law tails in weight spectra with network generalization performance, offering an interpretation from first principles for neural scaling phenomena.

Abstract: Neural scaling laws underlie many of the recent advances in deep learning,
yet their theoretical understanding remains largely confined to linear models.
In this work, we present a systematic analysis of scaling laws for quadratic
and diagonal neural networks in the feature learning regime. Leveraging
connections with matrix compressed sensing and LASSO, we derive a detailed
phase diagram for the scaling exponents of the excess risk as a function of
sample complexity and weight decay. This analysis uncovers crossovers between
distinct scaling regimes and plateau behaviors, mirroring phenomena widely
reported in the empirical neural scaling literature. Furthermore, we establish
a precise link between these regimes and the spectral properties of the trained
network weights, which we characterize in detail. As a consequence, we provide
a theoretical validation of recent empirical observations connecting the
emergence of power-law tails in the weight spectrum with network generalization
performance, yielding an interpretation from first principles.

</details>


### [655] [Adaptive Canonicalization with Application to Invariant Anisotropic Geometric Networks](https://arxiv.org/abs/2509.24886)
*Ya-Wei Eileen Lin,Ron Levie*

Main category: cs.LG

TL;DR: The paper introduces adaptive canonicalization, a framework where canonicalization depends on both input and network, addressing discontinuities in traditional canonicalization methods while maintaining symmetry and enabling universal approximation.


<details>
  <summary>Details</summary>
Motivation: Traditional canonicalization in equivariant ML introduces discontinuities that affect training stability, limit generalization, and complicate universal approximation theorems.

Method: Proposes adaptive canonicalization based on prior maximization, where the input's standard form is chosen to maximize the network's predictive confidence, ensuring continuity and symmetry.

Result: The method yields continuous and symmetry-respecting models with universal approximation properties, outperforming data augmentation, standard canonicalization, and equivariant architectures on molecular, protein, and point cloud classification tasks.

Conclusion: Adaptive canonicalization effectively resolves discontinuities in canonicalization while maintaining symmetry and enabling universal approximation, demonstrating superior performance across various applications.

Abstract: Canonicalization is a widely used strategy in equivariant machine learning,
enforcing symmetry in neural networks by mapping each input to a standard form.
Yet, it often introduces discontinuities that can affect stability during
training, limit generalization, and complicate universal approximation
theorems. In this paper, we address this by introducing \emph{adaptive
canonicalization}, a general framework in which the canonicalization depends
both on the input and the network. Specifically, we present the adaptive
canonicalization based on prior maximization, where the standard form of the
input is chosen to maximize the predictive confidence of the network. We prove
that this construction yields continuous and symmetry-respecting models that
admit universal approximation properties.
  We propose two applications of our setting: (i) resolving eigenbasis
ambiguities in spectral graph neural networks, and (ii) handling rotational
symmetries in point clouds. We empirically validate our methods on molecular
and protein classification, as well as point cloud classification tasks. Our
adaptive canonicalization outperforms the three other common solutions to
equivariant machine learning: data augmentation, standard canonicalization, and
equivariant architectures.

</details>


### [656] [Towards Understanding the Shape of Representations in Protein Language Models](https://arxiv.org/abs/2509.24895)
*Kosio Beshkov,Anders Malthe-Sørenssen*

Main category: cs.LG

TL;DR: This paper analyzes how protein language models (PLMs) transform sequence spaces and encode structural information using SRV representations and graph filtrations, revealing that PLMs preferentially encode local residue relations and that optimal structural encoding occurs before the final layers.


<details>
  <summary>Details</summary>
Motivation: To understand how PLMs transform the entire sequence space and encode structural relationships, as current interpretability tools only focus on individual sequences.

Method: Used square-root velocity (SRV) representations and graph filtrations to create metric spaces for comparing protein structures and representations, analyzing SCOP dataset proteins across ESM2 model layers.

Result: Found non-linear patterns in Karcher mean and effective dimension across layers, with PLMs preferentially encoding immediate and local residue relations but degrading for larger contexts. Optimal structural encoding occurs before the last layers.

Conclusion: Training folding models on layers just before the final layer might improve protein folding performance, as these layers provide the most structurally faithful encodings.

Abstract: While protein language models (PLMs) are one of the most promising avenues of
research for future de novo protein design, the way in which they transform
sequences to hidden representations, as well as the information encoded in such
representations is yet to be fully understood. Several works have attempted to
propose interpretability tools for PLMs, but they have focused on understanding
how individual sequences are transformed by such models. Therefore, the way in
which PLMs transform the whole space of sequences along with their relations is
still unknown. In this work we attempt to understand this transformed space of
sequences by identifying protein structure and representation with square-root
velocity (SRV) representations and graph filtrations. Both approaches naturally
lead to a metric space in which pairs of proteins or protein representations
can be compared with each other.
  We analyze different types of proteins from the SCOP dataset and show that
the Karcher mean and effective dimension of the SRV shape space follow a
non-linear pattern as a function of the layers in ESM2 models of different
sizes. Furthermore, we use graph filtrations as a tool to study the context
lengths at which models encode the structural features of proteins. We find
that PLMs preferentially encode immediate as well as local relations between
residues, but start to degrade for larger context lengths. The most
structurally faithful encoding tends to occur close to, but before the last
layer of the models, indicating that training a folding model ontop of these
layers might lead to improved folding performance.

</details>


### [657] [Is Sequence Information All You Need for Bayesian Optimization of Antibodies?](https://arxiv.org/abs/2509.24933)
*Sebastian W. Ober,Calvin McCarter,Aniruddh Raghu,Yucen Lily Li,Alan N. Amin,Andrew Gordon Wilson,Hunter Elliott*

Main category: cs.LG

TL;DR: This paper explores Bayesian optimization for antibody engineering, comparing structure-based and sequence-only approaches with a novel protein language model soft constraint.


<details>
  <summary>Details</summary>
Motivation: Antibody therapeutic property engineering is iterative and expensive, but existing Bayesian optimization methods lack structural information incorporation and optimal surrogate model selection varies by property.

Method: Compared different structural information incorporation approaches with sequence-only methods, and proposed a protein language model-based soft constraint to guide optimization.

Result: Structural information improved early data efficiency for stability but had equivalent peak performance. The protein language model soft constraint eliminated the data efficiency gap, making sequence-only methods match structure-based performance.

Conclusion: The necessity of structural information in Bayesian optimization for antibodies is questioned, as sequence-only methods with language model constraints can achieve equivalent performance.

Abstract: Bayesian optimization is a natural candidate for the engineering of antibody
therapeutic properties, which is often iterative and expensive. However,
finding the optimal choice of surrogate model for optimization over the highly
structured antibody space is difficult, and may differ depending on the
property being optimized. Moreover, to the best of our knowledge, no prior
works have attempted to incorporate structural information into antibody
Bayesian optimization. In this work, we explore different approaches to
incorporating structural information into Bayesian optimization, and compare
them to a variety of sequence-only approaches on two different antibody
properties, binding affinity and stability. In addition, we propose the use of
a protein language model-based ``soft constraint,'' which helps guide the
optimization to promising regions of the space. We find that certain types of
structural information improve data efficiency in early optimization rounds for
stability, but have equivalent peak performance. Moreover, when incorporating
the protein language model soft constraint we find that the data efficiency gap
is diminished for affinity and eliminated for stability, resulting in
sequence-only methods that match the performance of structure-based methods,
raising questions about the necessity of structure in Bayesian optimization for
antibodies.

</details>


### [658] [OAT-FM: Optimal Acceleration Transport for Improved Flow Matching](https://arxiv.org/abs/2509.24936)
*Angxiao Yue,Anqi Dong,Hongteng Xu*

Main category: cs.LG

TL;DR: OAT-FM bridges Flow Matching with Optimal Acceleration Transport theory, developing an improved method that optimizes acceleration transport instead of constant velocity, leading to straighter flows and better performance in generative tasks.


<details>
  <summary>Details</summary>
Motivation: Existing Flow Matching methods are explained as solving Optimal Transport problems, but this study aims to connect FM with the newer theory of Optimal Acceleration Transport to develop improved methods.

Method: OAT-FM optimizes acceleration transport in the product space of sample and velocity, using an efficient algorithm with low complexity. It enables a two-phase FM paradigm where pretrained models can be fine-tuned via OAT-FM.

Result: OAT-FM consistently improves model performance in various generative tasks, eliminates data distribution drift risk, and avoids the need to generate large noise data pairs.

Conclusion: OAT-FM provides theoretical and practical improvements over traditional FM methods by leveraging Optimal Acceleration Transport theory, offering a more effective approach to generative modeling.

Abstract: As a powerful technique in generative modeling, Flow Matching (FM) aims to
learn velocity fields from noise to data, which is often explained and
implemented as solving Optimal Transport (OT) problems. In this study, we
bridge FM and the recent theory of Optimal Acceleration Transport (OAT),
developing an improved FM method called OAT-FM and exploring its benefits in
both theory and practice. In particular, we demonstrate that the straightening
objective hidden in existing OT-based FM methods is mathematically equivalent
to minimizing the physical action associated with acceleration defined by OAT.
Accordingly, instead of enforcing constant velocity, OAT-FM optimizes the
acceleration transport in the product space of sample and velocity, whose
objective corresponds to a necessary and sufficient condition of flow
straightness. An efficient algorithm is designed to achieve OAT-FM with low
complexity. OAT-FM motivates a new two-phase FM paradigm: Given a generative
model trained by an arbitrary FM method, whose velocity information has been
relatively reliable, we can fine-tune and improve it via OAT-FM. This paradigm
eliminates the risk of data distribution drift and the need to generate a large
number of noise data pairs, which consistently improves model performance in
various generative tasks. Code is available at:
https://github.com/AngxiaoYue/OAT-FM

</details>


### [659] [Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer](https://arxiv.org/abs/2509.24947)
*Sooraj Sathish,Keshav Goyal,Raghuram Bharadwaj Diddigi*

Main category: cs.LG

TL;DR: This paper proposes a novel deep Q-learning method with regularization to reduce correlation in feature representations, enabling more effective transfer learning using linear function approximation.


<details>
  <summary>Details</summary>
Motivation: Deep RL training faces challenges like extensive hyperparameter tuning and high computational costs. Transfer learning can address these by reusing knowledge from previous tasks, but standard deep RL representations are often highly correlated, limiting their effectiveness with linear function approximation.

Method: The authors propose a deep Q-learning approach with a regularization term that reduces positive correlations between state feature representations. This creates less correlated features that work better with linear function approximation for transfer learning.

Result: Experiments on standard RL benchmarks and MinAtar games show that the proposed approach improves transfer learning performance and reduces computational overhead compared to standard methods.

Conclusion: By reducing correlation in learned representations, the proposed method enables more effective use of linear function approximation in transfer learning, addressing key challenges in deep RL training while maintaining performance.

Abstract: Deep Reinforcement Learning (RL) has demonstrated success in solving complex
sequential decision-making problems by integrating neural networks with the RL
framework. However, training deep RL models poses several challenges, such as
the need for extensive hyperparameter tuning and high computational costs.
Transfer learning has emerged as a promising strategy to address these
challenges by enabling the reuse of knowledge from previously learned tasks for
new, related tasks. This avoids the need for retraining models entirely from
scratch. A commonly used approach for transfer learning in RL is to leverage
the internal representations learned by the neural network during training.
Specifically, the activations from the last hidden layer can be viewed as
refined state representations that encapsulate the essential features of the
input. In this work, we investigate whether these representations can be used
as input for training simpler models, such as linear function approximators, on
new tasks. We observe that the representations learned by standard deep RL
models can be highly correlated, which limits their effectiveness when used
with linear function approximation. To mitigate this problem, we propose a
novel deep Q-learning approach that introduces a regularization term to reduce
positive correlations between feature representation of states. By leveraging
these reduced correlated features, we enable more effective use of linear
function approximation in transfer learning. Through experiments and ablation
studies on standard RL benchmarks and MinAtar games, we demonstrate the
efficacy of our approach in improving transfer learning performance and thereby
reducing computational overhead.

</details>


### [660] [Intra-request branch orchestration for efficient LLM reasoning](https://arxiv.org/abs/2509.24957)
*Weifan Jiang,Rana Shahout,Yilun Du,Michael Mitzenmacher,Minlan Yu*

Main category: cs.LG

TL;DR: DUCHESS is an LLM serving system that reduces inference latency and token usage for reasoning tasks without sacrificing accuracy, using branch orchestration guided by correctness predictions.


<details>
  <summary>Details</summary>
Motivation: Current inference-time reasoning methods like chain-of-thought and multi-branch reasoning significantly increase token usage and latency, with prior work focusing mainly on token reduction at the expense of accuracy.

Method: DUCHESS uses lightweight linear probing on LLM layer activations to predict branch correctness, and implements orchestration policies to terminate, duplicate, or continue branches. It also prioritizes easier reasoning tasks when complexity can be estimated.

Result: On three reasoning benchmarks, DUCHESS reduces token usage by 42-63% at matched accuracy compared to self-consistency. In serving with vLLM, it reduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% respectively.

Conclusion: DUCHESS effectively improves the token-accuracy Pareto frontier and significantly reduces latency in LLM serving for reasoning tasks while maintaining accuracy.

Abstract: Large Language Models (LLMs) increasingly rely on inference-time reasoning
algorithms such as chain-of-thought and multi-branch reasoning to improve
accuracy on complex tasks. These methods, however, substantially increase token
usage and per-request latency. Prior work has largely focused on reducing token
usage, often at the expense of accuracy, while overlooking other latency
factors. We present DUCHESS, an LLM serving system that reduces cost and
latency without sacrificing accuracy through intra-request branch orchestration
guided by predictions. DUCHESS employs a lightweight linear probing model over
LLM layer activations to estimate branch correctness, and its orchestration
policy decides whether to terminate, duplicate, or continue a branch. When
handling multiple requests, DUCHESS further reduces latency by prioritizing
easier reasoning tasks when complexity can be estimated from the prompt.
Experiments on three reasoning benchmarks show that DUCHESS consistently
improves the token-accuracy Pareto frontier, reducing token usage by 42-63% at
matched accuracy compared to self-consistency. In serving with vLLM, DUCHESS
reduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% with
First-Come-First-Served scheduling, and achieves additional gains under
difficulty-aware scheduling at higher request rates.

</details>


### [661] [Overlap-Adaptive Regularization for Conditional Average Treatment Effect Estimation](https://arxiv.org/abs/2509.24962)
*Valentyn Melnychuk,Dennis Frauen,Jonas Schweisthal,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: This paper introduces Overlap-Adaptive Regularization (OAR) to improve CATE estimation in low-overlap regions by regularizing models proportionally to overlap weights, with higher regularization in low-overlap areas.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art CATE estimation methods (meta-learners) perform poorly in low-overlap regions, limiting their effectiveness in personalized medicine for therapeutic decision-making.

Method: Proposed Overlap-Adaptive Regularization (OAR) that applies regularization proportional to overlap weights, with debiased versions preserving Neyman-orthogonality. Works with both parametric and non-parametric meta-learners.

Result: Through (semi-)synthetic experiments, OAR significantly improves CATE estimation in low-overlap settings compared to constant regularization approaches.

Conclusion: OAR is the first approach to leverage overlap weights in regularization terms of meta-learners, providing flexible and effective improvement for CATE estimation in challenging low-overlap scenarios.

Abstract: The conditional average treatment effect (CATE) is widely used in
personalized medicine to inform therapeutic decisions. However,
state-of-the-art methods for CATE estimation (so-called meta-learners) often
perform poorly in the presence of low overlap. In this work, we introduce a new
approach to tackle this issue and improve the performance of existing
meta-learners in the low-overlap regions. Specifically, we introduce
Overlap-Adaptive Regularization (OAR) that regularizes target models
proportionally to overlap weights so that, informally, the regularization is
higher in regions with low overlap. To the best of our knowledge, our OAR is
the first approach to leverage overlap weights in the regularization terms of
the meta-learners. Our OAR approach is flexible and works with any existing
CATE meta-learner: we demonstrate how OAR can be applied to both parametric and
non-parametric second-stage models. Furthermore, we propose debiased versions
of our OAR that preserve the Neyman-orthogonality of existing meta-learners and
thus ensure more robust inference. Through a series of (semi-)synthetic
experiments, we demonstrate that our OAR significantly improves CATE estimation
in low-overlap settings in comparison to constant regularization.

</details>


### [662] [Double Descent as a Lens for Sample Efficiency in Autoregressive vs. Discrete Diffusion Models](https://arxiv.org/abs/2509.24974)
*Ahmad Fraij,Sam Dauncey*

Main category: cs.LG

TL;DR: This paper compares sample efficiency between discrete diffusion and autoregressive models using double descent phenomenon, finding autoregressive models are more sample-efficient on small datasets while discrete diffusion models need more capacity and compute to become competitive.


<details>
  <summary>Details</summary>
Motivation: Data scarcity drives the need for more sample-efficient large language models, motivating comparison of discrete diffusion vs autoregressive models.

Method: Using double descent phenomenon to holistically compare sample efficiency of discrete diffusion and autoregressive models across different capacity regimes.

Result: Discrete diffusion models require larger capacity and more training epochs to reach interpolation threshold. In overparameterized regime, both show similar behavior without pronounced second descent. Autoregressive models are more sample-efficient on small datasets.

Conclusion: Autoregressive models are more sample-efficient on small-scale datasets, while discrete diffusion models only become competitive when given sufficient capacity and compute.

Abstract: Data scarcity drives the need for more sample-efficient large language
models. In this work, we use the double descent phenomenon to holistically
compare the sample efficiency of discrete diffusion and autoregressive models.
We show that discrete diffusion models require larger capacity and more
training epochs to escape their underparameterized regime and reach the
interpolation threshold. In the strongly overparameterized regime, both models
exhibit similar behavior, with neither exhibiting a pronounced second descent
in test loss across a large range of model sizes. Overall, our results indicate
that autoregressive models are more sample-efficient on small-scale datasets,
while discrete diffusion models only become competitive when given sufficient
capacity and compute.

</details>


### [663] [Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards](https://arxiv.org/abs/2509.24981)
*Haoran He,Yuxiao Ye,Qingpeng Cai,Chen Hu,Binxing Jiao,Daxin Jiang,Ling Pan*

Main category: cs.LG

TL;DR: ROVER is a minimalist RL method for math reasoning that uses Q-values from a uniformly random policy instead of complex policy optimization, achieving better performance and diversity with simpler implementation.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods using PPO/GRPO suffer from training instability and diversity collapse, requiring complex heuristics. The authors observe that math reasoning has simpler structure than general RL problems, suggesting existing sophisticated techniques may be unnecessary.

Method: Prove that optimal actions can be recovered from Q-function of fixed uniformly random policy, bypassing policy iteration loop. Introduce ROVER algorithm that samples actions from softmax over these uniform-policy Q-values.

Result: ROVER achieves superior performance: +8.2 on pass@1, +16.8 on pass@256, and +17.6% diversity across multiple base models and math reasoning benchmarks.

Conclusion: ROVER demonstrates that minimalist RL approaches can outperform complex existing methods in math reasoning, preserving diversity throughout training while being simpler to implement.

Abstract: RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for
improving the reasoning abilities of large language models (LLMs). Current
methods rely primarily on policy optimization frameworks like PPO and GRPO,
which follow generalized policy iteration that alternates between evaluating
the current policy's value and improving the policy based on evaluation. While
effective, they often suffer from training instability and diversity collapse,
requiring complex heuristic tricks and careful tuning. We observe that standard
RLVR in math reasoning can be formalized as a specialized finite-horizon Markov
Decision Process with deterministic state transitions, tree-structured
dynamics, and binary terminal rewards. Though large in scale, the underlying
structure is simpler than general-purpose control settings for which popular RL
algorithms (e.g., PPO) were developed, suggesting that several sophisticated
techniques in existing methods may be reduced or even omitted. Based on this
insight, we prove a surprising result: the optimal action can be recovered from
the Q-function of a fixed uniformly random policy, thereby bypassing the
generalized policy iteration loop and its associated heuristics. We introduce
Random Policy Valuation for Diverse Reasoning (ROVER) to translate this
principle into a practical and scalable algorithm for LLM math reasoning, a
minimalist yet highly effective RL method that samples actions from a softmax
over these uniform-policy Q-values. ROVER preserves diversity throughout
training, allowing sustained exploration of multiple valid pathways. Across
multiple base models and standard math reasoning benchmarks, ROVER demonstrates
superior performance in both \textbf{quality} (\textbf{+8.2} on pass@1,
\textbf{+16.8} on pass@256) and \textbf{diversity} (\textbf{+17.6\%}), despite
its radical simplification compared to strong, complicated existing methods.

</details>


### [664] [Sampling Complexity of TD and PPO in RKHS](https://arxiv.org/abs/2509.24991)
*Lu Zou,Wendi Ren,Weizhong Zhang,Liang Ding,Shuang Li*

Main category: cs.LG

TL;DR: This paper provides a function-space analysis of PPO using reproducing kernel Hilbert spaces, decoupling policy evaluation (via kernelized TD critic) and improvement (via KL-regularized natural gradient), with theoretical guarantees and empirical improvements.


<details>
  <summary>Details</summary>
Motivation: To place PPO on firmer theoretical footing beyond finite-dimensional assumptions and understand when RKHS-proximal updates with kernel-TD critics yield global policy improvement with practical efficiency.

Method: Decouples policy evaluation and improvement in RKHS: (i) kernelized TD critic performs efficient RKHS-gradient updates, (ii) KL-regularized natural-gradient policy step exponentiates action-value, recovering PPO/TRPO-style proximal updates in continuous spaces.

Result: Provides non-asymptotic, instance-adaptive guarantees with rates depending on RKHS entropy, unifying various kernel regimes. Derives optimal k^{-1/2} convergence rate sampling rule. Empirically improves stability and sample efficiency on control tasks, with favorable throughput versus GAE baseline.

Conclusion: The analysis places PPO on firmer theoretical foundation and clarifies conditions for global policy improvement with practical efficiency using RKHS-proximal updates and kernel-TD critics.

Abstract: We revisit Proximal Policy Optimization (PPO) from a function-space
perspective. Our analysis decouples policy evaluation and improvement in a
reproducing kernel Hilbert space (RKHS): (i) A kernelized temporal-difference
(TD) critic performs efficient RKHS-gradient updates using only one-step
state-action transition samples; (ii) a KL-regularized, natural-gradient policy
step exponentiates the evaluated action-value, recovering a PPO/TRPO-style
proximal update in continuous state-action spaces. We provide non-asymptotic,
instance-adaptive guarantees whose rates depend on RKHS entropy, unifying
tabular, linear, Sobolev, Gaussian, and Neural Tangent Kernel (NTK) regimes,
and we derive a sampling rule for the proximal update that ensures the optimal
$k^{-1/2}$ convergence rate for stochastic optimization. Empirically, the
theory-aligned schedule improves stability and sample efficiency on common
control tasks (e.g., CartPole, Acrobot), while our TD-based critic attains
favorable throughput versus a GAE baseline. Altogether, our results place PPO
on a firmer theoretical footing beyond finite-dimensional assumptions and
clarify when RKHS-proximal updates with kernel-TD critics yield global policy
improvement with practical efficiency.

</details>


### [665] [Score-based Membership Inference on Diffusion Models](https://arxiv.org/abs/2509.25003)
*Mingxing Rao,Bowen Qu,Daniel Moyer*

Main category: cs.LG

TL;DR: The paper presents SimA, a single-query membership inference attack for diffusion models that uses predicted noise vector norms to detect training data membership, showing latent diffusion models are less vulnerable due to VAE bottlenecks.


<details>
  <summary>Details</summary>
Motivation: Membership inference attacks pose serious privacy risks for diffusion models by revealing whether specific samples were in training data, requiring efficient detection methods.

Method: Proposed SimA attack analyzes predicted noise vectors from diffusion models, showing their norms encode proximity to training set; tested on DDPM and Latent Diffusion Model variants.

Result: SimA achieves strong performance across diffusion model variants; latent diffusion models are less vulnerable due to VAE bottlenecks; varying VAE regularization affects vulnerability.

Conclusion: Score-based MIAs are theoretically grounded; latent diffusion methods need better VAE inversion understanding, not just diffusion process inversion; suggested strategies to improve LDM robustness.

Abstract: Membership inference attacks (MIAs) against diffusion models have emerged as
a pressing privacy concern, as these models may inadvertently reveal whether a
given sample was part of their training set. We present a theoretical and
empirical study of score-based MIAs, focusing on the predicted noise vectors
that diffusion models learn to approximate. We show that the expected denoiser
output points toward a kernel-weighted local mean of nearby training samples,
such that its norm encodes proximity to the training set and thereby reveals
membership. Building on this observation, we propose SimA, a single-query
attack that provides a principled, efficient alternative to existing
multi-query methods. SimA achieves consistently strong performance across
variants of DDPM, Latent Diffusion Model (LDM). Notably, we find that Latent
Diffusion Models are surprisingly less vulnerable than pixel-space models, due
to the strong information bottleneck imposed by their latent auto-encoder. We
further investigate this by differing the regularization hyperparameters
($\beta$ in $\beta$-VAE) in latent channel and suggest a strategy to make LDM
training more robust to MIA. Our results solidify the theory of score-based
MIAs, while highlighting that Latent Diffusion class of methods requires better
understanding of inversion for VAE, and not simply inversion of the Diffusion
process

</details>


### [666] [Uncertainty-Aware Deep Learning for Wildfire Danger Forecasting](https://arxiv.org/abs/2509.25017)
*Spyros Kondylatos,Gustau Camps-Valls,Ioannis Papoutsis*

Main category: cs.LG

TL;DR: An uncertainty-aware deep learning framework that jointly models epistemic and aleatoric uncertainty for short-term wildfire danger forecasting, improving both predictive accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Wildfires pose severe threats, but deep learning adoption is hindered by reliability concerns due to lack of uncertainty quantification in predictions.

Method: Developed an uncertainty-aware DL framework that captures both epistemic (model) and aleatoric (data) uncertainty for wildfire danger forecasting.

Result: In next-day forecasting, improved F1 Score by 2.3% and reduced Expected Calibration Error by 2.1% compared to deterministic baseline. Uncertainty estimates proved reliable for decision support.

Conclusion: Joint modeling of both uncertainty types provides complementary insights, significantly improving accuracy and reliability of wildfire danger forecasting for trustworthy DL systems.

Abstract: Wildfires are among the most severe natural hazards, posing a significant
threat to both humans and natural ecosystems. The growing risk of wildfires
increases the demand for forecasting models that are not only accurate but also
reliable. Deep Learning (DL) has shown promise in predicting wildfire danger;
however, its adoption is hindered by concerns over the reliability of its
predictions, some of which stem from the lack of uncertainty quantification. To
address this challenge, we present an uncertainty-aware DL framework that
jointly captures epistemic (model) and aleatoric (data) uncertainty to enhance
short-term wildfire danger forecasting. In the next-day forecasting, our
best-performing model improves the F1 Score by 2.3% and reduces the Expected
Calibration Error by 2.1% compared to a deterministic baseline, enhancing both
predictive skill and calibration. Our experiments confirm the reliability of
the uncertainty estimates and illustrate their practical utility for decision
support, including the identification of uncertainty thresholds for rejecting
low-confidence predictions and the generation of well-calibrated wildfire
danger maps with accompanying uncertainty layers. Extending the forecast
horizon up to ten days, we observe that aleatoric uncertainty increases with
time, showing greater variability in environmental conditions, while epistemic
uncertainty remains stable. Finally, we show that although the two uncertainty
types may be redundant in low-uncertainty cases, they provide complementary
insights under more challenging conditions, underscoring the value of their
joint modeling for robust wildfire danger prediction. In summary, our approach
significantly improves the accuracy and reliability of wildfire danger
forecasting, advancing the development of trustworthy wildfire DL systems.

</details>


### [667] [MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts](https://arxiv.org/abs/2509.25020)
*Jiayu Liu,Zhenya Huang,Anya Sims,Enhong Chen,Yee Whye Teh,Ning Miao*

Main category: cs.LG

TL;DR: MARCOS proposes a new reasoning paradigm for LLMs that models reasoning as a hidden Markov chain of continuous thoughts instead of discrete token generation, achieving comparable performance to chain-of-thought with significant speedup.


<details>
  <summary>Details</summary>
Motivation: Current chain-of-thought reasoning has three main drawbacks: slow and expensive inference due to autoregressive token generation, constrained reasoning in discrete token space creating information bottlenecks, and entangled reasoning with token generation causing potentially short-sighted reasoning.

Method: Models reasoning as a hidden Markov chain of continuous, high-dimensional thoughts. Each reasoning step involves transitions of internal thoughts, with explicit reasoning steps serving as observable variables. Uses a two-phase variational training scheme since the latent process is incompatible with standard supervised learning.

Result: Outperforms existing continuous reasoning methods and achieves performance comparable to token-based CoT, surpassing it by 4.7% on GSM8K with up to 15.7x speedup in inference. Also enables step-level control over randomness.

Conclusion: MARCOS presents a viable alternative to chain-of-thought reasoning that addresses its fundamental limitations while maintaining performance and offering significant computational benefits and additional advantages for reinforcement learning and reasoning control.

Abstract: The current paradigm for reasoning in large language models (LLMs) involves
models "thinking out loud" via a sequence of tokens, known as chain-of-thought
(CoT). This approach, while effective, has several significant drawbacks.
Firstly, inference requires autoregressive generation of often thousands of CoT
tokens, which is slow and computationally expensive. Secondly, it constrains
reasoning to the discrete space of tokens, creating an information bottleneck
across reasoning steps. Thirdly, it fundamentally entangles reasoning with
token generation, forcing LLMs to "think while speaking," which causes
potentially short-sighted reasoning. In light of these limitations, we
re-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our
approach, rather than autoregressively generating tokens, we model reasoning as
a hidden Markov chain of continuous, high-dimensional "thoughts". Each
reasoning step involves a transition of the internal thoughts, where explicit
reasoning steps (which may consist of hundreds of tokens) serve as observable
variables, which are windows to peek into the implicit thoughts. Since this
latent process is incompatible with the standard supervised learning, we
further propose a two-phase variational training scheme. Our experiments on
three benchmarks demonstrate that MARCOS outperforms existing continuous
reasoning methods and, for the first time, achieves performance comparable to
token-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup
in inference. Beyond this, MARCOS offers additional advantages, such as
step-level instead of token-level control over randomness, opening significant
opportunities for reinforcement learning and reasoning in LLMs.

</details>


### [668] [Bayesian Surrogates for Risk-Aware Pre-Assessment of Aging Bridge Portfolios](https://arxiv.org/abs/2509.25031)
*Sophia V. Kuhn,Rafael Bischof,Marius Weber,Antoine Binggeli,Michael A. Kraus,Walter Kaufmann,Fernando Pérez-Cruz*

Main category: cs.LG

TL;DR: BNN surrogates enable rapid structural pre-assessment of bridges by predicting code compliance factors with calibrated uncertainty, facilitating cost-effective portfolio-wide infrastructure management.


<details>
  <summary>Details</summary>
Motivation: Aging infrastructure requires efficient resource allocation, balancing cheap conservative analysis vs accurate but costly simulations that don't scale portfolio-wide.

Method: Bayesian neural network surrogates trained on large-scale nonlinear finite element analysis database from parametric pipeline, based on Swiss Federal Railway's bridge portfolio.

Result: Models accurately estimate high-fidelity structural analysis results by predicting code compliance factors with calibrated epistemic uncertainty, enabling fast uncertainty-aware triage.

Conclusion: The framework significantly reduces costs and emissions by avoiding unnecessary analyses and physical interventions across entire infrastructure portfolios, demonstrated in real-world railway underpass case study.

Abstract: Aging infrastructure portfolios pose a critical resource allocation
challenge: deciding which structures require intervention and which can safely
remain in service. Structural assessments must balance the trade-off between
cheaper, conservative analysis methods and accurate but costly simulations that
do not scale portfolio-wide. We propose Bayesian neural network (BNN)
surrogates for rapid structural pre-assessment of worldwide common bridge
types, such as reinforced concrete frame bridges. Trained on a large-scale
database of non-linear finite element analyses generated via a parametric
pipeline and developed based on the Swiss Federal Railway's bridge portfolio,
the models accurately and efficiently estimate high-fidelity structural
analysis results by predicting code compliance factors with calibrated
epistemic uncertainty. Our BNN surrogate enables fast, uncertainty-aware
triage: flagging likely critical structures and providing guidance where
refined analysis is pertinent. We demonstrate the framework's effectiveness in
a real-world case study of a railway underpass, showing its potential to
significantly reduce costs and emissions by avoiding unnecessary analyses and
physical interventions across entire infrastructure portfolios.

</details>


### [669] [A multiscale analysis of mean-field transformers in the moderate interaction regime](https://arxiv.org/abs/2509.25040)
*Giuseppe Bruno,Federico Pasqualotto,Andrea Agazzi*

Main category: cs.LG

TL;DR: This paper models token evolution in encoder-only transformers as interacting particles in a mean-field system, analyzing dynamics in the moderate interaction regime where token count and inverse temperature scale together.


<details>
  <summary>Details</summary>
Motivation: To understand how tokens evolve through transformer encoder depth at inference time by applying statistical physics concepts to analyze token interactions and clustering behavior.

Method: Model tokens as particles in a mean-field system, study dynamics in moderate interaction regime (large N tokens with β scaling with N), and analyze multiscale behavior through rigorous mathematical characterization.

Result: Identified three-phase multiscale dynamics: fast phase (token empirical measure collapses to low-dimensional space), intermediate phase (further collapse into clusters), and slow phase (sequential cluster merging into single cluster).

Conclusion: The paper provides rigorous characterization of limiting dynamics in each phase and proves convergence in the moderate interaction limit, demonstrating the approach with simulations.

Abstract: In this paper, we study the evolution of tokens through the depth of
encoder-only transformer models at inference time by modeling them as a system
of particles interacting in a mean-field way and studying the corresponding
dynamics. More specifically, we consider this problem in the moderate
interaction regime, where the number $N$ of tokens is large and the inverse
temperature parameter $\beta$ of the model scales together with $N$. In this
regime, the dynamics of the system displays a multiscale behavior: a fast
phase, where the token empirical measure collapses on a low-dimensional space,
an intermediate phase, where the measure further collapses into clusters, and a
slow one, where such clusters sequentially merge into a single one. We provide
a rigorous characterization of the limiting dynamics in each of these phases
and prove convergence in the above mentioned limit, exemplifying our results
with some simulations.

</details>


### [670] [Efficient Hyperparameter Tuning via Trajectory Invariance Principle](https://arxiv.org/abs/2509.25049)
*Bingrui Li,Jiaxin Wen,Zhanpeng Zhou,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: The paper identifies trajectory invariance phenomenon that reduces 2D hyperparameter space to 1D, enabling efficient tuning by following the salient direction revealed by this invariance.


<details>
  <summary>Details</summary>
Motivation: As hyperparameter tuning becomes increasingly costly at scale, there is a need for efficient tuning methods and principles to guide hyperparameter selection.

Method: The authors identify trajectory invariance phenomenon where pre-training loss curves, gradient noise, and gradient norm exhibit invariance with respect to a combination of learning rate and weight decay.

Result: Trajectory invariance effectively reduces the original two-dimensional hyperparameter space to one dimension, yielding an efficient tuning rule.

Conclusion: The work proposes new principles for efficient hyperparameter tuning, refines previous scaling laws, challenges existing viewpoints, and inspires future research on scaling laws.

Abstract: As hyperparameter tuning becomes increasingly costly at scale, efficient
tuning methods are essential. Yet principles for guiding hyperparameter tuning
remain limited. In this work, we seek to establish such principles by
considering a broad range of hyperparameters, including batch size, learning
rate, and weight decay. We identify a phenomenon we call trajectory invariance,
where pre-training loss curves, gradient noise, and gradient norm exhibit
invariance--closely overlapping--with respect to a quantity that combines
learning rate and weight decay. This phenomenon effectively reduces the
original two-dimensional hyperparameter space to one dimension, yielding an
efficient tuning rule: follow the salient direction revealed by trajectory
invariance. Furthermore, we refine previous scaling laws and challenge several
existing viewpoints. Overall, our work proposes new principles for efficient
tuning and inspires future research on scaling laws.

</details>


### [671] [Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion Models](https://arxiv.org/abs/2509.25050)
*Shuchen Xue,Chongjian Ge,Shilong Zhang,Yichen Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: AWM is a new RL method for diffusion models that uses the same score/flow-matching loss as pretraining, reducing variance and speeding up convergence compared to DDPO-based approaches.


<details>
  <summary>Details</summary>
Motivation: Current RL methods for diffusion models like DDPO optimize different objectives than pretraining, causing increased variance and slower convergence. The authors aim to unify pretraining and RL objectives.

Method: Advantage Weighted Matching (AWM) uses score/flow-matching loss identical to pretraining, reweighting samples by their advantage scores to emphasize high-reward samples.

Result: AWM achieves up to 24× speedup over Flow-GRPO on GenEval, OCR, and PickScore benchmarks with Stable Diffusion 3.5 Medium and FLUX, without quality loss.

Conclusion: AWM successfully unifies pretraining and RL for diffusion models, providing faster convergence and lower variance while maintaining generation quality.

Abstract: Reinforcement Learning (RL) has emerged as a central paradigm for advancing
Large Language Models (LLMs), where pre-training and RL post-training share the
same log-likelihood formulation. In contrast, recent RL approaches for
diffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),
optimize an objective different from the pretraining objectives--score/flow
matching loss. In this work, we establish a novel theoretical analysis: DDPO is
an implicit form of score/flow matching with noisy targets, which increases
variance and slows convergence. Building on this analysis, we introduce
\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for
diffusion. It uses the same score/flow-matching loss as pretraining to obtain a
lower-variance objective and reweights each sample by its advantage. In effect,
AWM raises the influence of high-reward samples and suppresses low-reward ones
while keeping the modeling objective identical to pretraining. This unifies
pretraining and RL conceptually and practically, is consistent with
policy-gradient theory, reduces variance, and yields faster convergence. This
simple yet effective design yields substantial benefits: on GenEval, OCR, and
PickScore benchmarks, AWM delivers up to a $24\times$ speedup over Flow-GRPO
(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,
without compromising generation quality. Code is available at
https://github.com/scxue/advantage_weighted_matching.

</details>


### [672] [Towards a Certificate of Trust: Task-Aware OOD Detection for Scientific AI](https://arxiv.org/abs/2509.25080)
*Bogdan Raonić,Siddhartha Mishra,Samuel Lanthaler*

Main category: cs.LG

TL;DR: Proposes a new OOD detection method using score-based diffusion models to estimate joint likelihoods of inputs and predictions, providing task-aware reliability scores for regression tasks in scientific ML.


<details>
  <summary>Details</summary>
Motivation: Data-driven models in critical scientific fields like weather forecasting and fluid dynamics can fail on out-of-distribution data, but detecting such failures in regression tasks remains challenging.

Method: Uses score-based diffusion models to estimate joint likelihoods considering both input data and regression model predictions, creating task-aware reliability scores for OOD detection.

Result: Across multiple scientific datasets (PDE datasets, satellite imagery, brain tumor segmentation), the estimated likelihood strongly correlates with prediction error, demonstrating effective OOD detection.

Conclusion: Provides a foundational step towards building verifiable 'certificates of trust' for AI-based scientific predictions, offering practical tools for assessing trustworthiness in critical applications.

Abstract: Data-driven models are increasingly adopted in critical scientific fields
like weather forecasting and fluid dynamics. These methods can fail on
out-of-distribution (OOD) data, but detecting such failures in regression tasks
is an open challenge. We propose a new OOD detection method based on estimating
joint likelihoods using a score-based diffusion model. This approach considers
not just the input but also the regression model's prediction, providing a
task-aware reliability score. Across numerous scientific datasets, including
PDE datasets, satellite imagery and brain tumor segmentation, we show that this
likelihood strongly correlates with prediction error. Our work provides a
foundational step towards building a verifiable 'certificate of trust', thereby
offering a practical tool for assessing the trustworthiness of AI-based
scientific predictions. Our code is publicly available at
https://github.com/bogdanraonic3/OOD_Detection_ScientificML

</details>


### [673] [Towards generalizable deep ptychography neural networks](https://arxiv.org/abs/2509.25104)
*Albert Vong,Steven Henke,Oliver Hoidn,Hanna Ruth,Junjing Deng,Alexander Hexemer,Apurva Mehta,Arianna Gleason,Levi Hancock,Nicholas Schwarz*

Main category: cs.LG

TL;DR: Proposed an unsupervised training workflow for X-ray ptychography using probe learning with synthetic objects, enabling multi-probe generalization and real-time reconstruction across different beamlines.


<details>
  <summary>Details</summary>
Motivation: X-ray ptychography needs real-time feedback at next-gen light sources, but existing deep learning approaches lack robustness across diverse experimental conditions.

Method: Unsupervised training combining experimentally-measured probes with procedurally generated synthetic objects, emphasizing probe learning for physics-informed neural networks.

Result: Achieved reconstruction fidelity comparable to experimental data training, demonstrated multi-probe generalization across multiple beamlines, with probe learning being equally important as in-distribution learning.

Conclusion: The probe-centric synthetic workflow enables training of experiment-steering models that provide real-time feedback under dynamic experimental conditions.

Abstract: X-ray ptychography is a data-intensive imaging technique expected to become
ubiquitous at next-generation light sources delivering many-fold increases in
coherent flux. The need for real-time feedback under accelerated acquisition
rates motivates surrogate reconstruction models like deep neural networks,
which offer orders-of-magnitude speedup over conventional methods. However,
existing deep learning approaches lack robustness across diverse experimental
conditions. We propose an unsupervised training workflow emphasizing probe
learning by combining experimentally-measured probes with synthetic,
procedurally generated objects. This probe-centric approach enables a single
physics-informed neural network to reconstruct unseen experiments across
multiple beamlines; among the first demonstrations of multi-probe
generalization. We find probe learning is equally important as in-distribution
learning; models trained using this synthetic workflow achieve reconstruction
fidelity comparable to those trained exclusively on experimental data, even
when changing the type of synthetic training object. The proposed approach
enables training of experiment-steering models that provide real-time feedback
under dynamic experimental conditions.

</details>


### [674] [Learning in an Echo Chamber: Online Learning with Replay Adversary](https://arxiv.org/abs/2509.25135)
*Daniil Dmitriev,Harald Eskelund Franck,Carolin Heinzler,Amartya Sanyal*

Main category: cs.LG

TL;DR: This paper introduces a learning-theoretic framework for Online Learning in the Replay Setting, where learners risk reinforcing their own errors when training on self-annotated data. The authors define the Extended Threshold dimension as the exact measure of learnability and prove matching upper and lower bounds.


<details>
  <summary>Details</summary>
Motivation: As machine learning systems increasingly train on self-annotated data, they risk creating echo chambers that reinforce their own errors. This motivates studying learning in settings where models can be misled by their own previous predictions.

Method: The authors introduce the Online Learning in the Replay Setting framework, where in each round the learner outputs a hypothesis and the adversary reveals either the true label or a replayed label from an earlier round. They define the Extended Threshold dimension and develop closure-based learners.

Result: The paper proves that Extended Threshold dimension is the exact measure of learnability in this model, with matching upper and lower bounds. They show the replay setting is provably harder than classical mistake-bound learning, and proper learning requires intersection-closed classes.

Conclusion: The replay setting presents fundamental challenges for machine learning systems using self-annotated data, with Extended Threshold dimension characterizing learnability. Proper learning is only possible for intersection-closed classes, while improper learning can achieve the Extended Threshold dimension bound.

Abstract: As machine learning systems increasingly train on self-annotated data, they
risk reinforcing errors and becoming echo chambers of their own beliefs. We
model this phenomenon by introducing a learning-theoretic framework: Online
Learning in the Replay Setting. In round $t$, the learner outputs a hypothesis
$\hat{h}_t$; the adversary then reveals either the true label $f^\ast(x_t)$ or
a replayed label $\hat{h}_i(x_t)$ from an earlier round $i < t$. A mistake is
counted only when the true label is shown, yet classical algorithms such as the
SOA or the halving algorithm are easily misled by the replayed errors.
  We introduce the Extended Threshold dimension, $\mathrm{ExThD}(\mathcal{H})$,
and prove matching upper and lower bounds that make
$\mathrm{ExThD}(\mathcal{H})$ the exact measure of learnability in this model.
A closure-based learner makes at most $\mathrm{ExThD}(\mathcal{H})$ mistakes
against any adaptive adversary, and no algorithm can perform better. For
stochastic adversaries, we prove a similar bound for every intersection-closed
class. The replay setting is provably harder than the classical mistake bound
setting: some classes have constant Littlestone dimension but arbitrarily large
$\mathrm{ExThD}(\mathcal{H})$. Proper learning exhibits an even sharper
separation: a class is properly learnable under replay if and only if it is
(almost) intersection-closed. Otherwise, every proper learner suffers
$\Omega(T)$ errors, whereas our improper algorithm still achieves the
$\mathrm{ExThD}(\mathcal{H})$ bound. These results give the first tight
analysis of learning against replay adversaries, based on new results for
closure-type algorithms.

</details>


### [675] [BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression](https://arxiv.org/abs/2509.25136)
*David González Martínez*

Main category: cs.LG

TL;DR: BALF is a fine-tuning-free neural network compression framework that uses activation-aware factorization and a scalable budgeted rank allocator to achieve efficient compression across various models.


<details>
  <summary>Details</summary>
Motivation: Traditional neural network compression methods require expensive fine-tuning or search procedures, making them impractical on commodity hardware. The goal is to develop a compression approach that works without fine-tuning.

Method: The method combines an activation-aware factorization framework applicable to various layers with a scalable budgeted rank allocator that enables flexible control over compression targets without overhead.

Result: BALF achieves excellent results without fine-tuning, reducing FLOPs on ResNeXt-101 by 45% with only 1% top-1 accuracy drop. It works effectively across multiple scales and architectures including ResNet-20, ResNeXt-101, and vision transformers on CIFAR-10 and ImageNet datasets.

Conclusion: BALF provides an efficient pipeline for model compression that eliminates the need for fine-tuning while maintaining competitive performance across diverse neural network architectures and scales.

Abstract: Neural network compression techniques typically require expensive fine-tuning
or search procedures, rendering them impractical on commodity hardware.
Inspired by recent LLM compression research, we present a general
activation-aware factorization framework that can be applied to a broad range
of layers. Moreover, we introduce a scalable budgeted rank allocator that
allows flexible control over compression targets (e.g., retaining 50% of
parameters) with no overhead. Together, these components form BALF, an
efficient pipeline for compressing models without fine-tuning. We demonstrate
its effectiveness across multiple scales and architectures, from ResNet-20 on
CIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it
achieves excellent results in the fine-tuning-free regime. For instance, BALF
reduces FLOPs on ResNeXt-101 by 45% with only a 1-percentage-point top-1
accuracy drop.

</details>


### [676] [High-Dimensional Analysis of Single-Layer Attention for Sparse-Token Classification](https://arxiv.org/abs/2509.25153)
*Nicholas Barnfield,Hugo Cui,Yue M. Lu*

Main category: cs.LG

TL;DR: Attention mechanisms can learn to selectively attend to informative tokens with weak signals, achieving better performance than linear classifiers in sparse-token classification tasks.


<details>
  <summary>Details</summary>
Motivation: To understand when and how attention mechanisms can learn to selectively attend to informative tokens, especially for detecting weak, rare, and sparsely located features in classification tasks.

Method: Theoretical analysis of a sparse-token classification model using a simple single-layer attention classifier, studying training in a high-dimensional regime with proportional growth of sample size and embedding dimension.

Result: Attention classifiers can achieve vanishing test error with logarithmic signal strength scaling, outperform linear classifiers that require square root scaling, and can acquire nontrivial alignment with hidden signals in just two gradient updates.

Conclusion: Attention mechanisms provide significant advantages over non-adaptive linear baselines through adaptive token selection, explaining their superior performance in detecting sparse weak features.

Abstract: When and how can an attention mechanism learn to selectively attend to
informative tokens, thereby enabling detection of weak, rare, and sparsely
located features? We address these questions theoretically in a sparse-token
classification model in which positive samples embed a weak signal vector in a
randomly chosen subset of tokens, whereas negative samples are pure noise. In
the long-sequence limit, we show that a simple single-layer attention
classifier can in principle achieve vanishing test error when the signal
strength grows only logarithmically in the sequence length $L$, whereas linear
classifiers require $\sqrt{L}$ scaling. Moving from representational power to
learnability, we study training at finite $L$ in a high-dimensional regime,
where sample size and embedding dimension grow proportionally. We prove that
just two gradient updates suffice for the query weight vector of the attention
classifier to acquire a nontrivial alignment with the hidden signal, inducing
an attention map that selectively amplifies informative tokens. We further
derive an exact asymptotic expression for the test error and training loss of
the trained attention-based classifier, and quantify its capacity -- the
largest dataset size that is typically perfectly separable -- thereby
explaining the advantage of adaptive token selection over nonadaptive linear
baselines.

</details>


### [677] [Chance-constrained Flow Matching for High-Fidelity Constraint-aware Generation](https://arxiv.org/abs/2509.25157)
*Jinhao Liang,Yixuan Sun,Anirban Samaddar,Sandeep Madireddy,Ferdinando Fioretto*

Main category: cs.LG

TL;DR: CCFM is a training-free method that integrates stochastic optimization into sampling to enforce hard constraints while maintaining high-fidelity generation, outperforming current constrained generative models.


<details>
  <summary>Details</summary>
Motivation: Generative models often violate hard constraints from physical laws or specifications, and existing projection methods can distort distributions or increase complexity.

Method: Proposes Chance-constrained Flow Matching (CCFM) that uses stochastic optimization during sampling to enforce constraints while operating on noisy intermediate samples.

Result: CCFM guarantees feasibility like conventional projection but avoids distributional distortion, achieving higher feasibility and fidelity in physical systems and molecular docking.

Conclusion: CCFM effectively enforces hard constraints while maintaining sample quality, providing a superior approach for constrained generative modeling.

Abstract: Generative models excel at synthesizing high-fidelity samples from complex
data distributions, but they often violate hard constraints arising from
physical laws or task specifications. A common remedy is to project
intermediate samples onto the feasible set; however, repeated projection can
distort the learned distribution and induce a mismatch with the data manifold.
Thus, recent multi-stage procedures attempt to defer projection to clean
samples during sampling, but they increase algorithmic complexity and
accumulate errors across steps. This paper addresses these challenges by
proposing a novel training-free method, Chance-constrained Flow Matching
(CCFM), that integrates stochastic optimization into the sampling process,
enabling effective enforcement of hard constraints while maintaining
high-fidelity sample generation. Importantly, CCFM guarantees feasibility in
the same manner as conventional repeated projection, yet, despite operating
directly on noisy intermediate samples, it is theoretically equivalent to
projecting onto the feasible set defined by clean samples. This yields a
sampler that mitigates distributional distortion. Empirical experiments show
that CCFM outperforms current state-of-the-art constrained generative models in
modeling complex physical systems governed by partial differential equations
and molecular docking problems, delivering higher feasibility and fidelity.

</details>


### [678] [Physics-Informed Inductive Biases for Voltage Prediction in Distribution Grids](https://arxiv.org/abs/2509.25158)
*Ehimare Okoyomon,Arbel Yaniv,Christoph Goebel*

Main category: cs.LG

TL;DR: This paper systematically investigates three physics-informed inductive biases (power-flow-constrained loss functions, complex-valued neural networks, and residual-based task reformulation) to improve Graph Neural Networks' generalization for voltage prediction in distribution grids.


<details>
  <summary>Details</summary>
Motivation: Voltage prediction in distribution grids is critical for power system stability, but machine learning approaches like GNNs suffer from poor generalization when trained on limited or incomplete data.

Method: The authors evaluate three physics-informed strategies using the ENGAGE dataset across multiple grid configurations: (i) power-flow-constrained loss functions, (ii) complex-valued neural networks, and (iii) residual-based task reformulation, with controlled experiments to isolate each bias's effect.

Result: The study assesses both standard predictive performance and out-of-distribution generalization, providing practical insights into which model assumptions most effectively guide learning.

Conclusion: The research offers insights into which physics-informed inductive biases most effectively improve reliable and efficient voltage prediction in modern distribution networks.

Abstract: Voltage prediction in distribution grids is a critical yet difficult task for
maintaining power system stability. Machine learning approaches, particularly
Graph Neural Networks (GNNs), offer significant speedups but suffer from poor
generalization when trained on limited or incomplete data. In this work, we
systematically investigate the role of inductive biases in improving a model's
ability to reliably learn power flow. Specifically, we evaluate three
physics-informed strategies: (i) power-flow-constrained loss functions, (ii)
complex-valued neural networks, and (iii) residual-based task reformulation.
Using the ENGAGE dataset, which spans multiple low- and medium-voltage grid
configurations, we conduct controlled experiments to isolate the effect of each
inductive bias and assess both standard predictive performance and
out-of-distribution generalization. Our study provides practical insights into
which model assumptions most effectively guide learning for reliable and
efficient voltage prediction in modern distribution networks.

</details>


### [679] [GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion Models](https://arxiv.org/abs/2509.25170)
*Peter Holderrieth,Uriel Singer,Tommi Jaakkola,Ricky T. Q. Chen,Yaron Lipman,Brian Karrer*

Main category: cs.LG

TL;DR: GLASS Flows is a new sampling method that enables efficient Markov transitions in flow matching models by simulating a "flow matching model within a flow matching model", combining ODE efficiency with SDE stochasticity.


<details>
  <summary>Details</summary>
Motivation: Current reward alignment algorithms for flow matching and diffusion models suffer from efficiency limitations due to their reliance on SDE sampling, which is slower and often less performant than ODE sampling.

Method: GLASS Flows introduces a novel sampling paradigm that retrieves an "inner" flow matching model from pre-trained models without retraining, enabling efficient Markov transitions while maintaining stochastic evolution.

Result: GLASS Flows eliminate the trade-off between stochastic evolution and efficiency in text-to-image generation, improving state-of-the-art performance when combined with Feynman-Kac Steering.

Conclusion: GLASS Flows provides a simple, drop-in solution for efficient inference-time scaling of flow and diffusion models, combining the benefits of both ODE and SDE sampling approaches.

Abstract: The performance of flow matching and diffusion models can be greatly improved
at inference time using reward alignment algorithms, yet efficiency remains a
major limitation. While several algorithms were proposed, we demonstrate that a
common bottleneck is the sampling method these algorithms rely on: many
algorithms require to sample Markov transitions via SDE sampling, which is
significantly less efficient and often less performant than ODE sampling. To
remove this bottleneck, we introduce GLASS Flows, a new sampling paradigm that
simulates a "flow matching model within a flow matching model" to sample Markov
transitions. As we show in this work, this "inner" flow matching model can be
retrieved from a pre-trained model without any re-training, combining the
efficiency of ODEs with the stochastic evolution of SDEs. On large-scale
text-to-image models, we show that GLASS Flows eliminate the trade-off between
stochastic evolution and efficiency. Combined with Feynman-Kac Steering, GLASS
Flows improve state-of-the-art performance in text-to-image generation, making
it a simple, drop-in solution for inference-time scaling of flow and diffusion
models.

</details>


### [680] [TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion](https://arxiv.org/abs/2509.25171)
*Sophia Tang,Yuchen Zhu,Molei Tao,Pranam Chatterjee*

Main category: cs.LG

TL;DR: TR2-D2 is a novel framework that combines tree search with trajectory-aware fine-tuning for discrete diffusion models, using MCTS to generate replay buffers for more reliable reward-guided optimization.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning approaches for diffusion fine-tuning are susceptible to reinforcing sub-optimal trajectories that yield poor rewards, as they require training on rollouts under the current fine-tuned model.

Method: The framework uses Monte Carlo Tree Search (MCTS) to construct replay buffers for trajectory-aware fine-tuning, then fine-tunes a pre-trained discrete diffusion model under a stochastic optimal control objective.

Result: The method is validated on single- and multi-objective fine-tuning of biological sequence diffusion models, demonstrating overall effectiveness for reliable reward-guided fine-tuning.

Conclusion: TR2-D2 provides an effective solution for optimizing reward-guided discrete diffusion trajectories by leveraging tree search to overcome limitations of traditional reinforcement learning approaches.

Abstract: Reinforcement learning with stochastic optimal control offers a promising
framework for diffusion fine-tuning, where a pre-trained diffusion model is
optimized to generate paths that lead to a reward-tilted distribution. While
these approaches enable optimization without access to explicit samples from
the optimal distribution, they require training on rollouts under the current
fine-tuned model, making them susceptible to reinforcing sub-optimal
trajectories that yield poor rewards. To overcome this challenge, we introduce
TRee Search Guided TRajectory-Aware Fine-Tuning for Discrete Diffusion
(TR2-D2), a novel framework that optimizes reward-guided discrete diffusion
trajectories with tree search to construct replay buffers for trajectory-aware
fine-tuning. These buffers are generated using Monte Carlo Tree Search (MCTS)
and subsequently used to fine-tune a pre-trained discrete diffusion model under
a stochastic optimal control objective. We validate our framework on single-
and multi-objective fine-tuning of biological sequence diffusion models,
highlighting the overall effectiveness of TR2-D2 for reliable reward-guided
fine-tuning in discrete sequence generation.

</details>


### [681] [XQC: Well-conditioned Optimization Accelerates Deep Reinforcement Learning](https://arxiv.org/abs/2509.25174)
*Daniel Palenicek,Florian Vogt,Joe Watson,Ingmar Posner,Jan Peters*

Main category: cs.LG

TL;DR: XQC is a sample-efficient deep actor-critic algorithm that improves training dynamics by optimizing the critic network's Hessian condition number through batch normalization, weight normalization, and distributional cross-entropy loss.


<details>
  <summary>Details</summary>
Motivation: To improve sample efficiency in deep reinforcement learning through principled optimization landscape analysis rather than empirical complexity additions, focusing on the critic network's training dynamics.

Method: Systematic analysis of critic network optimization using Hessian eigenspectrum and condition number; combination of batch normalization, weight normalization, and distributional cross-entropy loss; built upon soft actor-critic framework.

Result: Achieved orders of magnitude smaller condition numbers than baselines; state-of-the-art sample efficiency on 55 proprioception and 15 vision-based continuous control tasks; used significantly fewer parameters than competing methods.

Conclusion: Optimization-aware architectural design decisions can dramatically improve sample efficiency in deep reinforcement learning while reducing model complexity.

Abstract: Sample efficiency is a central property of effective deep reinforcement
learning algorithms. Recent work has improved this through added complexity,
such as larger models, exotic network architectures, and more complex
algorithms, which are typically motivated purely by empirical performance. We
take a more principled approach by focusing on the optimization landscape of
the critic network. Using the eigenspectrum and condition number of the
critic's Hessian, we systematically investigate the impact of common
architectural design decisions on training dynamics. Our analysis reveals that
a novel combination of batch normalization (BN), weight normalization (WN), and
a distributional cross-entropy (CE) loss produces condition numbers orders of
magnitude smaller than baselines. This combination also naturally bounds
gradient norms, a property critical for maintaining a stable effective learning
rate under non-stationary targets and bootstrapping. Based on these insights,
we introduce XQC: a well-motivated, sample-efficient deep actor-critic
algorithm built upon soft actor-critic that embodies these optimization-aware
principles. We achieve state-of-the-art sample efficiency across 55
proprioception and 15 vision-based continuous control tasks, all while using
significantly fewer parameters than competing methods.

</details>
