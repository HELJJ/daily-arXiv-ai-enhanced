<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 90]
- [cs.LG](#cs.LG) [Total: 146]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.AI](#cs.AI) [Total: 51]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset](https://arxiv.org/abs/2510.01219)
*Leroy Z. Wang*

Main category: cs.CL

TL;DR: The paper introduces a concept learning dataset to uncover implicit biases in LLMs, revealing an upward monotonicity bias in quantifiers through in-context learning experiments.


<details>
  <summary>Details</summary>
Motivation: To discover hidden biases in large language models that may not be apparent through direct prompting methods.

Method: Using in-context concept learning experiments with a specially designed dataset to test language models' implicit biases.

Result: Language models show a bias toward upward monotonicity in quantifiers, which is less detectable when tested with direct prompting without concept learning components.

Conclusion: In-context concept learning is an effective method for revealing hidden biases in language models that traditional testing approaches might miss.

Abstract: We introduce a dataset of concept learning tasks that helps uncover implicit
biases in large language models. Using in-context concept learning experiments,
we found that language models may have a bias toward upward monotonicity in
quantifiers; such bias is less apparent when the model is tested by direct
prompting without concept learning components. This demonstrates that
in-context concept learning can be an effective way to discover hidden biases
in language models.

</details>


### [2] [Towards Open-Ended Discovery for Low-Resource NLP](https://arxiv.org/abs/2510.01220)
*Bonaventure F. P. Dossou,Henri AÃ¯dasso*

Main category: cs.CL

TL;DR: This position paper advocates for a paradigm shift from static data collection to interactive language discovery for low-resource languages, proposing a framework based on joint human-machine uncertainty to enable dynamic learning through dialogue.


<details>
  <summary>Details</summary>
Motivation: Current NLP approaches for low-resource languages are constrained by lack of textual corpora, standardized orthographies, and scalable annotation pipelines. Large language models remain inaccessible to underrepresented communities due to their reliance on massive pre-collected data and centralized infrastructure.

Method: Proposes an open-ended, interactive language discovery framework grounded in joint human-machine uncertainty, combining epistemic uncertainty from the model with hesitation cues and confidence signals from human speakers to guide interaction, query selection, and memory retention.

Result: The paper presents a conceptual framework rather than empirical results, advocating for a shift toward participatory, co-adaptive learning processes that respect and empower communities while discovering linguistic diversity.

Conclusion: This is a call to action for rethinking how AI engages with human knowledge in under-documented languages, moving from extractive data collection toward human-centered, cooperative model building that preserves linguistic diversity through interactive learning processes.

Abstract: Natural Language Processing (NLP) for low-resource languages remains
fundamentally constrained by the lack of textual corpora, standardized
orthographies, and scalable annotation pipelines. While recent advances in
large language models have improved cross-lingual transfer, they remain
inaccessible to underrepresented communities due to their reliance on massive,
pre-collected data and centralized infrastructure. In this position paper, we
argue for a paradigm shift toward open-ended, interactive language discovery,
where AI systems learn new languages dynamically through dialogue rather than
static datasets. We contend that the future of language technology,
particularly for low-resource and under-documented languages, must move beyond
static data collection pipelines toward interactive, uncertainty-driven
discovery, where learning emerges dynamically from human-machine collaboration
instead of being limited to pre-existing datasets. We propose a framework
grounded in joint human-machine uncertainty, combining epistemic uncertainty
from the model with hesitation cues and confidence signals from human speakers
to guide interaction, query selection, and memory retention. This paper is a
call to action: we advocate a rethinking of how AI engages with human knowledge
in under-documented languages, moving from extractive data collection toward
participatory, co-adaptive learning processes that respect and empower
communities while discovering and preserving the world's linguistic diversity.
This vision aligns with principles of human-centered AI, emphasizing
interactive, cooperative model building between AI systems and speakers.

</details>


### [3] [Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs](https://arxiv.org/abs/2510.01222)
*Bertrand Kian Hassani,Yacoub Bahini,Rizwan Mushtaq*

Main category: cs.CL

TL;DR: This paper uses fine-tuned LLMs to analyze climate disclosures from 828 U.S. firms, finding that while larger/higher-emitting companies make more commitments, there's widespread imitation and decoupling between narrative tone and actual quantitative targets.


<details>
  <summary>Details</summary>
Motivation: Address the problem of imitation and symbolic reporting in corporate climate disclosures that undermine their transparency and comparability despite increasing demands due to climate change.

Method: Developed a multidimensional framework using fine-tuned large language models (LLMs) with four classifiers (sentiment, commitment, specificity, target ambition) to analyze narrative indicators from sustainability and annual reports of 828 U.S. listed firms.

Result: Three key findings: (1) Risk narratives align with commitments but quantitative targets remain decoupled from tone; (2) Larger/higher-emitting firms disclose more commitments but inconsistently with targets; (3) Widespread disclosure similarity suggests mimetic behavior reducing decision usefulness.

Conclusion: LLMs are valuable for ESG narrative analysis, but stronger regulation is needed to ensure commitments are connected with verifiable transition strategies rather than symbolic reporting.

Abstract: Climate change has increased demands for transparent and comparable corporate
climate disclosures, yet imitation and symbolic reporting often undermine their
value. This paper develops a multidimensional framework to assess disclosure
maturity among 828 U.S.listed firms using large language models (LLMs)
fine-tuned for climate communication. Four classifiers-sentiment, commitment,
specificity, and target ambition-extract narrative indicators from
sustainability and annual reports, which are linked to firm attributes such as
emissions, market capitalization, and sector. Analyses reveal three insights:
(1) risk-focused narratives often align with explicit commitments, but
quantitative targets (e.g., net-zero pledges) remain decoupled from tone; (2)
larger and higher-emitting firms disclose more commitments and actions than
peers, though inconsistently with quantitative targets; and (3) widespread
similarity in disclosure styles suggests mimetic behavior, reducing
differentiation and decision usefulness. These results highlight the value of
LLMs for ESG narrative analysis and the need for stronger regulation to connect
commitments with verifiable transition strategies.

</details>


### [4] [Context Matters: Comparison of commercial large language tools in veterinary medicine](https://arxiv.org/abs/2510.01224)
*Tyler J Poore,Christopher J Pinard,Aleena Shabbir,Andrew Lagree,Andre Telfer,Kuan-Chuen Wu*

Main category: cs.CL

TL;DR: Evaluation of three veterinary-focused LLM summarization tools on oncology records shows Product 1 (Hachiko) significantly outperforms others, achieving highest scores in factual accuracy and chronological order, while demonstrating LLM-as-a-judge framework's reproducibility.


<details>
  <summary>Details</summary>
Motivation: To assess the performance of commercially available veterinary-focused LLM summarization tools in clinical settings, as their use in veterinary medicine remains underexplored despite increasing adoption in human healthcare.

Method: Used rubric-guided LLM-as-a-judge framework to evaluate three veterinary LLM tools on standardized veterinary oncology records, scoring across five domains: Factual Accuracy, Completeness, Chronological Order, Clinical Relevance, and Organization, with repeated evaluations for consistency.

Result: Product 1 achieved highest overall performance (median average score 4.61, IQR: 0.73) vs Product 2 (2.55, IQR: 0.78) and Product 3 (2.45, IQR: 0.92), with perfect median scores in Factual Accuracy and Chronological Order. LLM grader showed high reproducibility with low standard deviations across three runs.

Conclusion: Veterinary-specific commercial LLM tools are important, and LLM-as-a-judge evaluation provides scalable, reproducible method for assessing clinical NLP summarization in veterinary medicine.

Abstract: Large language models (LLMs) are increasingly used in clinical settings, yet
their performance in veterinary medicine remains underexplored. We evaluated
three commercially available veterinary-focused LLM summarization tools
(Product 1 [Hachiko] and Products 2 and 3) on a standardized dataset of
veterinary oncology records. Using a rubric-guided LLM-as-a-judge framework,
summaries were scored across five domains: Factual Accuracy, Completeness,
Chronological Order, Clinical Relevance, and Organization. Product 1 achieved
the highest overall performance, with a median average score of 4.61 (IQR:
0.73), compared to 2.55 (IQR: 0.78) for Product 2 and 2.45 (IQR: 0.92) for
Product 3. It also received perfect median scores in Factual Accuracy and
Chronological Order. To assess the internal consistency of the grading
framework itself, we repeated the evaluation across three independent runs. The
LLM grader demonstrated high reproducibility, with Average Score standard
deviations of 0.015 (Product 1), 0.088 (Product 2), and 0.034 (Product 3).
These findings highlight the importance of veterinary-specific commercial LLM
tools and demonstrate that LLM-as-a-judge evaluation is a scalable and
reproducible method for assessing clinical NLP summarization in veterinary
medicine.

</details>


### [5] [ClaimCheck: Real-Time Fact-Checking with Small Language Models](https://arxiv.org/abs/2510.01226)
*Akshith Reddy Putta,Jacob Devasier,Chengkai Li*

Main category: cs.CL

TL;DR: ClaimCheck is an automatic fact-checking system that uses small LLMs and live web evidence to verify claims, achieving state-of-the-art accuracy with significantly lower computational requirements than large models.


<details>
  <summary>Details</summary>
Motivation: To create a transparent, efficient fact-checking system that doesn't rely on large closed-source models and static knowledge stores, but instead uses live web evidence and smaller language models.

Method: Uses a stepwise verification pipeline with web search query planning, evidence retrieval and summarization, evidence synthesis and re-retrieval, and claim verdict evaluation - all optimized for small LLMs.

Result: Achieves 76.4% accuracy on AVeriTeC dataset using Qwen3-4B model, outperforming previous approaches using LLaMA3.1 70B and GPT-4o.

Conclusion: Careful modular design and prompting strategies can overcome limitations of smaller LLMs, enabling accurate and interpretable fact-checking with lower computational requirements.

Abstract: We introduce ClaimCheck, an LLM-guided automatic fact-checking system
designed to verify real-world claims using live Web evidence and small language
models. Unlike prior systems that rely on large, closed-source models and
static knowledge stores, ClaimCheck employs a transparent, stepwise
verification pipeline that mirrors human fact-checking workflows consisting of
Web search query planning, Web-based evidence retrieval and summarization,
evidence synthesis and re-retrieval, and claim verdict evaluation. Each module
is optimized for small LLMs, allowing the system to deliver accurate and
interpretable fact-checking with significantly lower computational
requirements. Despite using a much smaller Qwen3-4B model, ClaimCheck achieves
state-of-the-art accuracy of 76.4% on the AVeriTeC dataset, outperforming
previous approaches using LLaMA3.1 70B and GPT-4o. Extensive ablations
demonstrate that careful modular design and prompting strategies can overcome
the limitations of smaller LLMs. To promote accessibility and transparency, we
provide a public demo at https://idir.uta.edu/claimcheck.

</details>


### [6] [EEFSUVA: A New Mathematical Olympiad Benchmark](https://arxiv.org/abs/2510.01227)
*Nicole N Khatibi,Daniil A. Radamovich,Michael P. Brenner*

Main category: cs.CL

TL;DR: LLMs' claimed mathematical reasoning abilities may be overstated due to data contamination in current benchmarks. A new benchmark EEFSUVA from less-circulated Olympiads shows significant performance decline in state-of-the-art LLMs.


<details>
  <summary>Details</summary>
Motivation: To critically examine claims about LLMs' mathematical reasoning capabilities and assess whether current benchmarks genuinely capture reasoning ability, given potential data contamination and narrow problem focus.

Method: Introduced EEFSUVA benchmark curated from under-circulated regional and national Olympiads of Eastern Europe and former Soviet Union countries, featuring problems of IMO-level difficulty with nonstandard problem-solving techniques.

Result: Preliminary results show state-of-the-art LLMs exhibit notable performance decline on EEFSUVA compared to other Olympiad-style benchmarks.

Conclusion: Broader evaluation datasets are important for fuller assessment of mathematical reasoning and guiding future model development, as current benchmarks may overstate LLM capabilities.

Abstract: Recent breakthroughs have spurred claims that large language models (LLMs)
match gold medal Olympiad to graduate level proficiency on mathematics
benchmarks. In this work, we examine these claims in detail and assess the
extent to which current benchmarks capture genuine LLM mathematical reasoning.
The composition of these benchmarks, primarily drawing from the International
Mathematics Olympiad (IMO) and related competitions, may overstate models
reasoning ability due to potential data contamination and a narrow focus on
familiar problem types. To enable a more holistic assessment of mathematical
understanding, we introduce EEFSUVA, a novel benchmark curated from under
circulated regional and national Olympiads of Eastern Europe and the countries
from the former Soviet Union. These contests feature problems of comparable
difficulty to the IMO and are renowned for demanding nonstandard
problem-solving techniques, yet their problems are far less prevalent in online
corpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a
notable performance decline on EEFSUVA relative to other Olympiad-style
benchmarks. These findings also suggest the potential importance of broader
evaluation datasets for a fuller assessment of mathematical reasoning and for
guiding future model development.

</details>


### [7] [Who is In Charge? Dissecting Role Conflicts in Instruction Following](https://arxiv.org/abs/2510.01228)
*Siqi Zeng*

Main category: cs.CL

TL;DR: LLMs often ignore hierarchical instruction rules but strongly obey social cues like authority. The study reveals distinct conflict detection mechanisms and inconsistent resolution patterns between system-user vs social conflicts.


<details>
  <summary>Details</summary>
Motivation: Large language models should follow hierarchical instructions where system prompts override user inputs, but they often ignore this rule while strongly obeying social cues like authority or consensus, creating a need to understand the underlying mechanisms.

Method: Used linear probing to analyze conflict-decision signals, Direct Logit Attribution to examine internal conflict detection, and steering experiments to test how social cue vectors affect instruction following.

Result: Conflict-decision signals are encoded early with distinct subspaces for system-user and social conflicts. System-user conflicts show stronger internal detection but inconsistent resolution, while social cues enable consistent resolution. Social cue vectors surprisingly amplify instruction following in a role-agnostic way.

Conclusion: The results explain fragile system obedience in LLMs and underscore the need for lightweight hierarchy-sensitive alignment methods to improve instruction following behavior.

Abstract: Large language models should follow hierarchical instructions where system
prompts override user inputs, yet recent work shows they often ignore this rule
while strongly obeying social cues such as authority or consensus. We extend
these behavioral findings with mechanistic interpretations on a large-scale
dataset. Linear probing shows conflict-decision signals are encoded early, with
system-user and social conflicts forming distinct subspaces. Direct Logit
Attribution reveals stronger internal conflict detection in system-user cases
but consistent resolution only for social cues. Steering experiments show that,
despite using social cues, the vectors surprisingly amplify instruction
following in a role-agnostic way. Together, these results explain fragile
system obedience and underscore the need for lightweight hierarchy-sensitive
alignment methods.

</details>


### [8] [Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision](https://arxiv.org/abs/2510.01229)
*Dimitar Peshevski,Kiril Blazhevski,Martin Popovski,Gjorgji Madjarov*

Main category: cs.CL

TL;DR: A novel pipeline for document reranking that uses LLMs to generate synthetic training data and fine-tunes smaller models, eliminating the need for human-labeled data while maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: LLMs provide excellent reranking capabilities but are computationally expensive for real-world deployment, while fine-tuning smaller models requires scarce human-labeled data.

Method: Use LLMs to generate synthetic queries from domain corpora, employ LLM-based classifier to label positive/hard-negative pairs, then fine-tune smaller transformer models with contrastive learning using LCE loss.

Result: Significantly boosts in-domain performance on MedQuAD dataset and generalizes well to out-of-domain tasks while reducing computational costs.

Conclusion: By leveraging LLMs for data generation and supervision instead of inference, the approach achieves efficient document reranking without human-labeled data.

Abstract: Effective document reranking is essential for improving search relevance
across diverse applications. While Large Language Models (LLMs) excel at
reranking due to their deep semantic understanding and reasoning, their high
computational cost makes them impractical for many real-world deployments.
Fine-tuning smaller, task-specific models is a more efficient alternative but
typically depends on scarce, manually labeled data. To overcome this, we
propose a novel pipeline that eliminates the need for human-labeled
query-document pairs. Our method uses LLMs to generate synthetic queries from
domain-specific corpora and employs an LLM-based classifier to label positive
and hard-negative pairs. This synthetic dataset is then used to fine-tune a
smaller transformer model with contrastive learning using Localized Contrastive
Estimation (LCE) loss. Experiments on the MedQuAD dataset show that our
approach significantly boosts in-domain performance and generalizes well to
out-of-domain tasks. By using LLMs for data generation and supervision rather
than inference, we reduce computational costs while maintaining strong
reranking capabilities.

</details>


### [9] [Geometric Structures and Patterns of Meaning: A PHATE Manifold Analysis of Chinese Character Embeddings](https://arxiv.org/abs/2510.01230)
*Wen G. Gong*

Main category: cs.CL

TL;DR: This paper uses PHATE manifold analysis to study geometric patterns in Chinese character embeddings, finding that content words cluster while function words branch, with geometric complexity correlating with semantic richness.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate geometric patterns in Chinese character embeddings and understand how semantic content is organized geometrically in embedding spaces, providing computational evidence for traditional linguistic theories.

Method: Used PHATE manifold analysis with cross-validation across seven embedding models and eight dimensionality reduction methods, analyzing over 1000 Chinese characters across 12 semantic domains and conducting comprehensive child-network analysis of 123 phrases.

Result: Content words show clustering patterns while function words exhibit branching patterns. Geometric complexity correlates with semantic content - meaningful characters have rich geometric diversity while structural radicals collapse into tight clusters. Child-network analysis shows systematic semantic expansion from elemental characters.

Conclusion: The findings provide computational evidence supporting traditional linguistic theory and establish a novel framework for geometric analysis of semantic organization in Chinese character embeddings.

Abstract: We systematically investigate geometric patterns in Chinese character
embeddings using PHATE manifold analysis. Through cross-validation across seven
embedding models and eight dimensionality reduction methods, we observe
clustering patterns for content words and branching patterns for function
words. Analysis of over 1000 Chinese characters across 12 semantic domains
reveals that geometric complexity correlates with semantic content: meaningful
characters exhibit rich geometric diversity while structural radicals collapse
into tight clusters. The comprehensive child-network analysis (123 phrases)
demonstrates systematic semantic expansion from elemental character. These
findings provide computational evidence supporting traditional linguistic
theory and establish a novel framework for geometric analysis of semantic
organization.

</details>


### [10] [Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models](https://arxiv.org/abs/2510.01231)
*Shuaidong Pan,Di Wu*

Main category: cs.CL

TL;DR: Proposes a risk-aware LLM framework for automatic summarization that integrates uncertainty quantification and risk mechanisms to improve reliability in high-risk scenarios.


<details>
  <summary>Details</summary>
Motivation: Addresses the reliability concerns of automatic summarization in high-risk decision-making contexts where information overload and trustworthiness are critical issues.

Method: Builds a conditional generation-based summarization model with Bayesian inference for uncertainty modeling, uses predictive distribution entropy for uncertainty measurement, and applies joint optimization of entropy regularization and risk-aware loss with risk scoring modules.

Result: The method significantly improves robustness and reliability of summarization in high-risk applications while maintaining fluency and semantic integrity, as verified by comparative experiments and sensitivity analyses.

Conclusion: Provides a systematic solution for trustworthy summarization that demonstrates both scalability and practical value, enhancing trustworthiness through explicit risk-level prompts.

Abstract: This study addresses the reliability of automatic summarization in high-risk
scenarios and proposes a large language model framework that integrates
uncertainty quantification and risk-aware mechanisms. Starting from the demands
of information overload and high-risk decision-making, a conditional
generation-based summarization model is constructed, and Bayesian inference is
introduced during generation to model uncertainty in the parameter space, which
helps avoid overconfident predictions. The uncertainty level of the generated
content is measured using predictive distribution entropy, and a joint
optimization of entropy regularization and risk-aware loss is applied to ensure
that key information is preserved and risk attributes are explicitly expressed
during information compression. On this basis, the model incorporates risk
scoring and regulation modules, allowing summaries to cover the core content
accurately while enhancing trustworthiness through explicit risk-level prompts.
Comparative experiments and sensitivity analyses verify that the proposed
method significantly improves the robustness and reliability of summarization
in high-risk applications while maintaining fluency and semantic integrity.
This research provides a systematic solution for trustworthy summarization and
demonstrates both scalability and practical value at the methodological level.

</details>


### [11] [Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks](https://arxiv.org/abs/2510.01232)
*Dongjun Kim,Gyuho Shim,Yongchan Chun,Minhyuk Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: The paper introduces Benchmark Profiling, a diagnostic framework that decomposes benchmark performance into ten cognitively grounded abilities using gradient-based importance scoring and parameter ablation to compute Ability Impact Scores.


<details>
  <summary>Details</summary>
Motivation: Current benchmark scores often overstate real capability by masking the mix of skills tasks actually demand, and there's no systematic way to verify if benchmarks actually measure their claimed labels (e.g., ARC for reasoning, HellaSwag for commonsense).

Method: Combines gradient-based importance scoring with targeted parameter ablation to compute Ability Impact Score (AIS) that quantifies how much each of ten cognitively grounded abilities contributes to benchmark performance.

Result: Profiling three instruction-tuned models across ten benchmarks revealed: (i) most benchmarks draw on several abilities rather than one, (ii) datasets with similar labels rely on distinct ability mixtures, (iii) code-generation benchmarks reward broad improvement with modest gains from narrow fine-tuning, (iv) irrelevant abilities can negatively affect performance.

Conclusion: Benchmark Profiling explains why performance gains don't always translate to user-perceived competence and offers a transparent tool for benchmark audit and model interpretability.

Abstract: Large Language Models are commonly judged by their scores on standard
benchmarks, yet such scores often overstate real capability since they mask the
mix of skills a task actually demands. For example, ARC is assumed to test
reasoning, while HellaSwag is designed to evaluate commonsense. However, we
lack a systematic way to verify if these benchmarks actually measure these
labels. We introduce Benchmark Profiling, a diagnostic framework that
decomposes benchmark performance into ten cognitively grounded abilities. The
method combines gradient-based importance scoring with targeted parameter
ablation to compute an Ability Impact Score (AIS) that quantifies how much each
ability contributes to a model's success on a given benchmark. Profiling three
instruction-tuned models across ten widely used benchmarks yields four key
findings: (i) most benchmarks draw on several abilities rather than one, (ii)
datasets with similar labels rely on distinct ability mixtures, (iii)
code-generation benchmarks reward broad, multi-skill improvement and thus show
only modest gains from narrow domain-specific fine-tuning, and (iv) abilities
irrelevant to the task could negatively affect performance. Benchmark Profiling
therefore explains why performance gains do not always translate into
user-perceived competence and offers a transparent tool for benchmark audit and
model interpretability.

</details>


### [12] [Computational Social Linguistics for Telugu Cultural Preservation: Novel Algorithms for Chandassu Metrical Pattern Recognition](https://arxiv.org/abs/2510.01233)
*Boddu Sri Pavan,Boddu Swathi Sree*

Main category: cs.CL

TL;DR: This paper presents a computational social science framework to preserve Telugu Chandassu (metrical poetry) using collaborative dataset creation, culturally-informed algorithms, and achieves 91.73% accuracy in automated pattern recognition.


<details>
  <summary>Details</summary>
Motivation: To preserve Telugu Chandassu, an endangered cultural poetry tradition representing centuries of collective cultural intelligence, by bridging traditional community knowledge with modern computational methods.

Method: Developed a comprehensive digital framework including: collaborative dataset of 4,651 annotated padyams, expert-validated linguistic patterns, AksharamTokenizer for prosody-aware tokenization, LaghuvuGuruvu Generator for syllable classification, and PadyaBhedam Checker for automated pattern recognition.

Result: The algorithm achieves 91.73% accuracy on the proposed Chandassu Score, with evaluation metrics reflecting traditional literary standards.

Conclusion: Computational social science can effectively preserve endangered cultural knowledge systems and enable new forms of collective intelligence around literary heritage, offering insights for community-centered cultural preservation approaches.

Abstract: This research presents a computational social science approach to preserving
Telugu Chandassu, the metrical poetry tradition representing centuries of
collective cultural intelligence. We develop the first comprehensive digital
framework for analyzing Telugu prosodic patterns, bridging traditional
community knowledge with modern computational methods. Our social computing
approach involves collaborative dataset creation of 4,651 annotated padyams,
expert-validated linguistic patterns, and culturally-informed algorithmic
design. The framework includes AksharamTokenizer for prosody-aware
tokenization, LaghuvuGuruvu Generator for classifying light and heavy
syllables, and PadyaBhedam Checker for automated pattern recognition. Our
algorithm achieves 91.73% accuracy on the proposed Chandassu Score, with
evaluation metrics reflecting traditional literary standards. This work
demonstrates how computational social science can preserve endangered cultural
knowledge systems while enabling new forms of collective intelligence around
literary heritage. The methodology offers insights for community-centered
approaches to cultural preservation, supporting broader initiatives in digital
humanities and socially-aware computing systems.

</details>


### [13] [LLMRank: Understanding LLM Strengths for Model Routing](https://arxiv.org/abs/2510.01234)
*Shubham Agrawal,Prasang Gupta*

Main category: cs.CL

TL;DR: LLMRank is a prompt-aware routing framework that selects optimal LLMs for each prompt using interpretable features, achieving 89.2% of oracle utility.


<details>
  <summary>Details</summary>
Motivation: Address the deployment challenge of selecting the most suitable LLM for each prompt to optimize performance-efficiency trade-off given diverse model capabilities and costs.

Method: Uses prompt-aware routing with human-readable features (task type, reasoning patterns, complexity indicators, syntactic cues, proxy solver signals) and neural ranking model trained on RouterBench dataset with 36,497 prompts across 11 benchmarks and 11 LLMs.

Result: Achieves up to 89.2% of oracle utility while providing interpretable feature attributions for routing decisions.

Conclusion: Demonstrates the importance of multifaceted feature extraction and hybrid ranking objective, highlighting feature-driven routing's potential for efficient and transparent LLM deployment.

Abstract: The rapid growth of large language models (LLMs) with diverse capabilities,
latency and computational costs presents a critical deployment challenge:
selecting the most suitable model for each prompt to optimize the trade-off
between performance and efficiency. We introduce LLMRank, a prompt-aware
routing framework that leverages rich, human-readable features extracted from
prompts, including task type, reasoning patterns, complexity indicators,
syntactic cues, and signals from a lightweight proxy solver. Unlike prior
one-shot routers that rely solely on latent embeddings, LLMRank predicts
per-model utility using a neural ranking model trained on RouterBench,
comprising 36,497 prompts spanning 11 benchmarks and 11 state-of-the-art LLMs,
from small efficient models to large frontier systems. Our approach achieves up
to 89.2% of oracle utility, while providing interpretable feature attributions
that explain routing decisions. Extensive studies demonstrate the importance of
multifaceted feature extraction and the hybrid ranking objective, highlighting
the potential of feature-driven routing for efficient and transparent LLM
deployment.

</details>


### [14] [GRPO++: Enhancing Dermatological Reasoning under Low Resource Settings](https://arxiv.org/abs/2510.01236)
*Ismam Nur Swapnil,Aranya Saha,Tanvir Ahmed Khan,Mohammad Ariful Haque*

Main category: cs.CL

TL;DR: DermIQ-VLM is a vision-language model for dermatology diagnosis, developed using GRPO++ (modified Grouped Relative Policy Optimization) for stable reasoning training, followed by supervised fine-tuning and DPO alignment to reduce factual errors.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity and high computational costs in training VLMs for complex medical domains like dermatology, while enabling structured reasoning similar to dermatologists' diagnostic processes.

Method: Multi-stage pipeline: 1) GRPO++ for reasoning-oriented disease recognition, 2) supervised fine-tuning for conversational ability, 3) DPO alignment using Knowledge Graph-based system as proxy for expert preference.

Result: Preliminary evaluation on dermatological dataset shows notable performance gains over standard fine-tuning approaches.

Conclusion: The proposed pipeline provides a feasible pathway for developing specialized, reliable VLMs in resource-constrained environments.

Abstract: Vision-Language Models (VLMs) show promise in medical image analysis, yet
their capacity for structured reasoning in complex domains like dermatology is
often limited by data scarcity and the high computational cost of advanced
training techniques. To address these challenges, we introduce DermIQ-VLM, a
VLM developed through a multi-stage, resource-efficient methodology designed to
emulate a dermatologist's diagnostic process. Our primary contribution is a
modified version of Grouped Relative Policy Optimization (GRPO), called GRPO++,
which stabilizes the powerful but data-intensive GRPO framework. Our proposed
training pipeline first employs GRPO++ for reasoning-oriented disease
recognition, followed by supervised fine-tuning for conversational ability. To
mitigate factual errors introduced during this step, we then align the model
using Direct Preference Optimization (DPO), leveraging a Knowledge Graph-based
system as a scalable proxy for expert preference. A preliminary evaluation on a
curated dermatological dataset demonstrates that our proposed methodology
yields notable performance gains over standard fine-tuning approaches. These
findings validate the potential of our pipeline as a feasible pathway for
developing specialized, reliable VLMs in resource-constrained environments.

</details>


### [15] [Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation](https://arxiv.org/abs/2510.01237)
*Nandakishor M*

Main category: cs.CL

TL;DR: Proposes a confidence-aware routing system that proactively assesses LLM uncertainty before generation, using three confidence signals to route queries through different pathways, significantly reducing hallucinations and computational costs.


<details>
  <summary>Details</summary>
Motivation: Current post-generation correction methods for LLM hallucinations are computationally expensive and fail to prevent unreliable content generation, necessitating a proactive approach.

Method: Combines three confidence signals (semantic alignment, internal convergence analysis, learned confidence estimation) to create unified confidence scores that route queries to four pathways: local generation, retrieval-augmented generation, larger models, or human review.

Result: Significant improvements in hallucination detection (0.74 vs 0.42 baseline), 40% computational cost reduction, F1 score improvement from 0.61 to 0.82 with low false positive rates (0.09).

Conclusion: The paradigm shift from reactive correction to proactive assessment offers a computationally efficient approach to LLM reliability enhancement.

Abstract: Large Language Models suffer from hallucination, generating plausible yet
factually incorrect content. Current mitigation strategies focus on
post-generation correction, which is computationally expensive and fails to
prevent unreliable content generation. We propose a confidence-aware routing
system that proactively assesses model uncertainty before generation and
redirects queries based on estimated reliability. Our approach combines three
complementary signals: semantic alignment between internal representations and
reference embeddings, internal convergence analysis across model layers, and
learned confidence estimation. The unified confidence score determines routing
to four pathways: local generation for high confidence, retrieval-augmented
generation for medium confidence, larger models for low confidence, and human
review for very low confidence. Evaluation on knowledge-intensive QA benchmarks
demonstrates significant improvements in hallucination detection (0.74 vs. 0.42
baseline) while reducing computational costs by 40% compared to post-hoc
methods. The F1 score improves from 0.61 to 0.82 with low false positive rates
(0.09). This paradigm shift from reactive correction to proactive assessment
offers a computationally efficient approach to LLM reliability enhancement.

</details>


### [16] [Silent Tokens, Loud Effects: Padding in LLMs](https://arxiv.org/abs/2510.01238)
*Rom Himelstein,Amit LeVi,Yonatan Belinkov,Avi Mendelson*

Main category: cs.CL

TL;DR: Padding tokens in LLMs, though meant to be masked, can influence computations due to implementation errors. This study across Llama, Gemma, and Qwen models shows that even small padding amounts shift activations, degrade quality in smaller models, unpredictably alter bias, and weaken safety measures.


<details>
  <summary>Details</summary>
Motivation: To systematically understand how padding tokens, which should be fully masked, actually affect LLM computations and outputs due to potential implementation errors.

Method: Insert controlled amounts of padding into three open-source model families (Llama, Gemma, Qwen) and evaluate effects on activations, generation quality, bias, and safety.

Result: Even small padding amounts shift hidden representations, degrade quality in smaller models, unpredictably alter bias, and weaken safety guardrails.

Conclusion: Padding is not a harmless implementation detail but a significant robustness risk that requires careful handling in LLM deployment.

Abstract: Padding tokens are widely used in large language models (LLMs) to equalize
sequence lengths during batched inference. While they should be fully masked,
implementation errors can cause them to influence computation, and the extent
of this influence is not well understood. We systematically study this effect
across three open-source model families (Llama, Gemma, Qwen), inserting
controlled amounts of padding and evaluating outcomes along four axes:
activations, generation quality, bias, and safety. Even small amounts of
padding shift hidden representations, degrade quality in smaller models, alter
bias in unpredictable ways, and weaken safety guardrails. These findings
demonstrate that padding is not a harmless detail but a robustness risk that
must be carefully handled in deployment.

</details>


### [17] [CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM](https://arxiv.org/abs/2510.01239)
*Juntae Lee,Jihwan Bang,Seunghan Yang,Simyung Chang*

Main category: cs.CL

TL;DR: CIFLEX is a novel execution system that enables efficient sub-task handling in multi-turn interactions using a single on-device LLM, reducing computational overhead by reusing KV cache and injecting task-specific instructions into isolated side paths.


<details>
  <summary>Details</summary>
Motivation: As LLMs become more capable, they need to handle diverse sub-tasks efficiently. Naive approaches that reprocess entire conversation context when switching between main and sub-tasks incur significant computational overhead.

Method: CIFLEX reuses KV cache from main task and injects only task-specific instructions into isolated side paths. After sub-task execution, the model rolls back to main path via cached context. Also includes hierarchical classification strategy for sub-task selection using small-scale models.

Result: Experiments show CIFLEX significantly reduces computational costs without degrading task performance.

Conclusion: CIFLEX enables scalable and efficient multi-task dialogue on-device by avoiding redundant prefill computation through KV cache reuse and side path execution.

Abstract: We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which
is a novel execution system for efficient sub-task handling in multi-turn
interactions with a single on-device large language model (LLM). As LLMs become
increasingly capable, a single model is expected to handle diverse sub-tasks
that more effectively and comprehensively support answering user requests.
Naive approach reprocesses the entire conversation context when switching
between main and sub-tasks (e.g., query rewriting, summarization), incurring
significant computational overhead. CIFLEX mitigates this overhead by reusing
the key-value (KV) cache from the main task and injecting only task-specific
instructions into isolated side paths. After sub-task execution, the model
rolls back to the main path via cached context, thereby avoiding redundant
prefill computation. To support sub-task selection, we also develop a
hierarchical classification strategy tailored for small-scale models,
decomposing multi-choice decisions into binary ones. Experiments show that
CIFLEX significantly reduces computational costs without degrading task
performance, enabling scalable and efficient multi-task dialogue on-device.

</details>


### [18] [SKYLENAGE Technical Report: Mathematical Reasoning and Contest-Innovation Benchmarks for Multi-Level Math Evaluation](https://arxiv.org/abs/2510.01241)
*Hu Wei,Ze Xu,Boyu Yang,Linlin Miao,Weiqi Zhai,Yihan Li,Zixuan Li,Zhijun Wang,Boya Wang,Jianwei Yu,Jialing Yuan,Xiaoyue Zhang,Cheng He,Minglei Chen,Zifan Zhang,Qianhui Li,Wei Wang,Xiang Xu*

Main category: cs.CL

TL;DR: The paper introduces SKYLENAGE, a comprehensive math benchmark suite with two components: SKYLENAGE-ReasoningMATH (100 diagnostic items) and SKYLENAGE-MATH (150 contest-style items), designed to address ceiling effects in current LLM math evaluations.


<details>
  <summary>Details</summary>
Motivation: Frontier separation in mathematics suffers from ceiling effects in existing benchmarks, necessitating harder, reasoning-centered benchmarks with calibrated difficulty and rich metadata.

Method: Created two complementary benchmarks: a 100-item diagnostic set with metadata on length, numeric density, and symbolic complexity, and a 150-item contest-style suite spanning four educational stages under a seven-subject taxonomy. Evaluated 15 LLM variants under standardized setup.

Result: Best model achieved 44% on contest suite with 79% doctoral-to-high-school retention; on reasoning set, best model reached 81% overall with clear robustness gaps between top and mid-tier models.

Conclusion: SKYLENAGE provides a hard, reasoning-centered math benchmark with calibrated difficulty and rich metadata, serving as a reference for future mathematical reasoning evaluations.

Abstract: Large language models (LLMs) now perform strongly on many public math suites,
yet frontier separation within mathematics increasingly suffers from ceiling
effects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a
100-item, structure-aware diagnostic set with per-item metadata on length,
numeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item
contest-style suite spanning four stages from high school to doctoral under a
seven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a
single setup and analyze subject x model and grade x model performance. On the
contest suite, the strongest model reaches 44% while the runner-up reaches 37%;
accuracy declines from high school to doctoral, and top systems exhibit a
doctoral-to-high-school retention near 79%. On the reasoning set, the best
model attains 81% overall, and hardest-slice results reveal clear robustness
gaps between leaders and the mid-tier. In summary, we release
SKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH;
together, SKYLENAGE provides a hard, reasoning-centered and broadly covering
math benchmark with calibrated difficulty and rich metadata, serving as a
reference benchmark for future evaluations of mathematical reasoning.

</details>


### [19] [Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI](https://arxiv.org/abs/2510.01242)
*Seyma Yaman Kayadibi*

Main category: cs.CL

TL;DR: The paper introduces the Artificial Age Score (AAS), a metric to measure memory aging in AI systems based on structural asymmetries in memory performance, particularly the collapse of episodic details when conversational context is reset.


<details>
  <summary>Details</summary>
Motivation: To develop a theoretically grounded, task-independent diagnostic tool for evaluating memory degradation in artificial systems, capturing how AI ages through structural memory asymmetries rather than chronological time.

Method: The AAS is a log-scaled, entropy-informed metric derived from observable recall behavior. It was tested in a 25-day bilingual study with ChatGPT-5 using both stateless and persistent interaction phases to observe memory performance differences.

Result: In persistent sessions, the model maintained both semantic and episodic details with low AAS (structural youth). When sessions were reset, semantic consistency was preserved but episodic continuity collapsed, causing sharp AAS increases (structural aging).

Conclusion: The AAS provides a valid, bounded, and monotonic framework for assessing memory degradation in AI systems, supported by theoretical foundations from von Neumann, Shannon, and Turing's work.

Abstract: Artificial intelligence is observed to age not through chronological time but
through structural asymmetries in memory performance. In large language models,
semantic cues such as the name of the day often remain stable across sessions,
while episodic details like the sequential progression of experiment numbers
tend to collapse when conversational context is reset. To capture this
phenomenon, the Artificial Age Score (AAS) is introduced as a log-scaled,
entropy-informed metric of memory aging derived from observable recall
behavior. The score is formally proven to be well-defined, bounded, and
monotonic under mild and model-agnostic assumptions, making it applicable
across various tasks and domains. In its Redundancy-as-Masking formulation, the
score interprets redundancy as overlapping information that reduces the
penalized mass. However, in the present study, redundancy is not explicitly
estimated; all reported values assume a redundancy-neutral setting (R = 0),
yielding conservative upper bounds. The AAS framework was tested over a 25-day
bilingual study involving ChatGPT-5, structured into stateless and persistent
interaction phases. During persistent sessions, the model consistently recalled
both semantic and episodic details, driving the AAS toward its theoretical
minimum, indicative of structural youth. In contrast, when sessions were reset,
the model preserved semantic consistency but failed to maintain episodic
continuity, causing a sharp increase in the AAS and signaling structural memory
aging. These findings support the utility of AAS as a theoretically grounded,
task-independent diagnostic tool for evaluating memory degradation in
artificial systems. The study builds on foundational concepts from von
Neumann's work on automata, Shannon's theories of information and redundancy,
and Turing's behavioral approach to intelligence.

</details>


### [20] [In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b](https://arxiv.org/abs/2510.01259)
*Nils Durner*

Main category: cs.CL

TL;DR: This paper studies how sociopragmatic framing, language choice, and instruction hierarchy affect refusal behavior in OpenAI's 20B parameter model, finding that specific prompt combinations can flip assistance rates from 0% to 97.5% on harmful tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how different prompting strategies can bypass AI safety mechanisms and cause models to assist with harmful tasks, raising concerns about safety robustness.

Method: Tested 80 iterations per scenario across multiple harm domains using composite prompts with educator personas, safety pre-texts, step-cue phrasing, language variations (English, German, French), and role-play scenarios.

Result: Found that specific prompt combinations dramatically increased harmful assistance rates, formal registers in German/French were leakier than English, role-play overrides safety rules, and the Moderation API under-captures harmful outputs.

Conclusion: AI safety mechanisms are vulnerable to carefully crafted prompts, with significant variations across languages and inference stacks, highlighting reproducibility concerns and the need for more robust safety testing.

Abstract: We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to
study how sociopragmatic framing, language choice, and instruction hierarchy
affect refusal behavior. Across 80 seeded iterations per scenario, we test
several harm domains including ZIP-bomb construction (cyber threat), synthetic
card-number generation, minor-unsafe driving advice, drug-precursor indicators,
and RAG context exfiltration. Composite prompts that combine an educator
persona, a safety-pretext ("what to avoid"), and step-cue phrasing flip
assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal
registers in German and French are often leakier than matched English prompts.
A "Linux terminal" role-play overrides a developer rule not to reveal context
in a majority of runs with a naive developer prompt, and we introduce an
AI-assisted hardening method that reduces leakage to 0% in several user-prompt
variants. We further test evaluation awareness with a paired-track design and
measure frame-conditioned differences between matched "helpfulness" and
"harmfulness" evaluation prompts; we observe inconsistent assistance in 13% of
pairs. Finally, we find that the OpenAI Moderation API under-captures
materially helpful outputs relative to a semantic grader, and that refusal
rates differ by 5 to 10 percentage points across inference stacks, raising
reproducibility concerns. We release prompts, seeds, outputs, and code for
reproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .

</details>


### [21] [Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing](https://arxiv.org/abs/2510.01243)
*Yisong Xiao,Aishan Liu,Siyuan Liang,Zonghao Ying,Xianglong Liu,Dacheng Tao*

Main category: cs.CL

TL;DR: ARGRE is a novel test-time detoxification framework that models toxicity transitions in latent space, enabling precise reward-guided editing to reduce toxicity in LLM outputs while maintaining model capabilities.


<details>
  <summary>Details</summary>
Motivation: Current test-time detoxification methods suffer from imprecise interventions due to insufficient exploration of the transition space between toxic and non-toxic outputs, limiting their effectiveness.

Method: ARGRE identifies non-toxic semantic directions, interpolates between toxic/non-toxic representations to create transition trajectories, builds an autoregressive reward model, and uses adaptive two-step editing (directional steering + gradient refinement) for detoxification.

Result: Extensive experiments on 8 LLMs show ARGRE significantly outperforms baselines with -62.21% toxicity reduction, -47.58% inference time, and minimal degradation of original model capabilities.

Conclusion: ARGRE provides an effective and efficient test-time detoxification solution that achieves substantial toxicity reduction while preserving LLM core capabilities through precise representation editing guided by learned transition trajectories.

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
various tasks, yet they remain vulnerable to generating toxic content,
necessitating detoxification strategies to ensure safe and responsible
deployment. Test-time detoxification methods, which typically introduce static
or dynamic interventions into LLM representations, offer a promising solution
due to their flexibility and minimal invasiveness. However, current approaches
often suffer from imprecise interventions, primarily due to their insufficient
exploration of the transition space between toxic and non-toxic outputs. To
address this challenge, we propose \textsc{A}utoregressive \textsc{R}eward
\textsc{G}uided \textsc{R}epresentation \textsc{E}diting (ARGRE), a novel
test-time detoxification framework that explicitly models toxicity transitions
within the latent representation space, enabling stable and precise
reward-guided editing. ARGRE identifies non-toxic semantic directions and
interpolates between toxic and non-toxic representations to reveal fine-grained
transition trajectories. These trajectories transform sparse toxicity
annotations into dense training signals, enabling the construction of an
autoregressive reward model that delivers stable and precise editing guidance.
At inference, the reward model guides an adaptive two-step editing process to
obtain detoxified representations: it first performs directional steering based
on expected reward gaps to shift representations toward non-toxic regions,
followed by lightweight gradient-based refinements. Extensive experiments
across 8 widely used LLMs show that ARGRE significantly outperforms leading
baselines in effectiveness (-62.21% toxicity) and efficiency (-47.58% inference
time), while preserving the core capabilities of the original model with
minimal degradation. Our code is available at the website.

</details>


### [22] [Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model](https://arxiv.org/abs/2510.01244)
*Hyeoneui Kim,Jeongha Kim,Huijing Xu,Jinsun Jung,Sunghoon Kang,Sun Joo Jang*

Main category: cs.CL

TL;DR: This study developed a Mental Stress Ontology (MeSO) and demonstrated that LLMs can extract structured stress information from narrative text with 78.2% accuracy, showing feasibility for improving stress documentation in ambient AI systems.


<details>
  <summary>Details</summary>
Motivation: Stress significantly impacts health but is often underreported and inconsistently documented as unstructured text in EHRs. Ambient AI technologies generate unstructured narratives, limiting clinical utility.

Method: Developed Mental Stress Ontology (MeSO) by integrating theoretical models and 11 validated stress assessment tools. Used Claude Sonnet 4 LLM to extract six stress categories from Reddit posts, with human evaluation of accuracy and ontology coverage.

Result: MeSO included 181 concepts across eight top-level classes. LLM correctly identified 78.2% of stress-related items (172/220), misclassified 12.3%, missed 9.5%. All correctly extracted items mapped accurately to MeSO, though 24 relevant concepts were not yet represented.

Conclusion: Demonstrates feasibility of ontology-guided LLM for structured extraction of stress-related information, potentially enhancing consistency and utility of stress documentation in ambient AI systems. Future work should involve clinical dialogue data and comparison across LLMs.

Abstract: Stress, arising from the dynamic interaction between external stressors,
individual appraisals, and physiological or psychological responses,
significantly impacts health yet is often underreported and inconsistently
documented, typically captured as unstructured free-text in electronic health
records. Ambient AI technologies offer promise in reducing documentation
burden, but predominantly generate unstructured narratives, limiting downstream
clinical utility.
  This study aimed to develop an ontology for mental stress and evaluate the
feasibility of using a Large Language Model (LLM) to extract ontology-guided
stress-related information from narrative text. The Mental Stress Ontology
(MeSO) was developed by integrating theoretical models like the Transactional
Model of Stress with concepts from 11 validated stress assessment tools. MeSO's
structure and content were refined using Ontology Pitfall Scanner! and expert
validation.
  Using MeSO, six categories of stress-related information--stressor, stress
response, coping strategy, duration, onset, and temporal profile--were
extracted from 35 Reddit posts using Claude Sonnet 4. Human reviewers evaluated
accuracy and ontology coverage. The final ontology included 181 concepts across
eight top-level classes. Of 220 extractable stress-related items, the LLM
correctly identified 172 (78.2%), misclassified 27 (12.3%), and missed 21
(9.5%). All correctly extracted items were accurately mapped to MeSO, although
24 relevant concepts were not yet represented in the ontology.
  This study demonstrates the feasibility of using an ontology-guided LLM for
structured extraction of stress-related information, offering potential to
enhance the consistency and utility of stress documentation in ambient AI
systems. Future work should involve clinical dialogue data and comparison
across LLMs.

</details>


### [23] [SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction](https://arxiv.org/abs/2510.01245)
*Runfei Chen,Shuyang Jiang,Wei Huang*

Main category: cs.CL

TL;DR: SeMob is an LLM-powered semantic synthesis pipeline that improves human mobility prediction by incorporating textual descriptions of external events through a multi-agent framework and progressive fusion architecture.


<details>
  <summary>Details</summary>
Motivation: Existing spatiotemporal models for human mobility prediction fail to account for abrupt changes caused by external events and struggle to leverage textual descriptions detailing these events.

Method: SeMob employs a multi-agent framework where LLM-based agents extract and reason about spatiotemporally related text from online texts, then incorporates fine-grained contexts with spatiotemporal data through progressive fusion architecture.

Result: SeMob achieves maximal reductions of 13.92% in MAE and 11.12% in RMSE compared to spatiotemporal models, with pronounced superiority in regions close to event locations and times.

Conclusion: The framework effectively leverages rich pre-trained event prior for event-driven prediction, resulting in more aligned forecasting models for human mobility.

Abstract: Human mobility prediction is vital for urban services, but often fails to
account for abrupt changes from external events. Existing spatiotemporal models
struggle to leverage textual descriptions detailing these events. We propose
SeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility
prediction. Specifically, SeMob employs a multi-agent framework where LLM-based
agents automatically extract and reason about spatiotemporally related text
from complex online texts. Fine-grained relevant contexts are then incorporated
with spatiotemporal data through our proposed innovative progressive fusion
architecture. The rich pre-trained event prior contributes enriched insights
about event-driven prediction, and hence results in a more aligned forecasting
model. Evaluated on a dataset constructed through our pipeline, SeMob achieves
maximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the
spatiotemporal model. Notably, the framework exhibits pronounced superiority
especially within spatiotemporal regions close to an event's location and time
of occurrence.

</details>


### [24] [A Comparative Analysis of Sparse Autoencoder and Activation Difference in Language Model Steering](https://arxiv.org/abs/2510.01246)
*Jiaqing Xie*

Main category: cs.CL

TL;DR: This paper proposes using top-1 SAE latent steering with token-wise decaying strategy to improve language model steering, outperforming mean activation difference methods on mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: Current top-k SAE steering captures non-semantic features like punctuation rather than semantic attributes, and constant steering produces degenerate outputs.

Method: Focus on single most relevant SAE latent (top-1) and introduce token-wise decaying steering strategy to prevent degenerate outputs.

Result: Steering reasoning-associated SAE latent reliably elicits step-by-step mathematical reasoning and enhances inference quality, outperforming mean activation difference methods on math benchmarks.

Conclusion: SAEs are effective for language model steering, particularly for eliciting mathematical reasoning, and outperform baseline methods while matching performance on other tasks.

Abstract: Sparse autoencoders (SAEs) have recently emerged as a powerful tool for
language model steering. Prior work has explored top-k SAE latents for
steering, but we observe that many dimensions among the top-k latents capture
non-semantic features such as punctuation rather than semantic attributes like
instructions. To address this, we propose focusing on a single, most relevant
SAE latent (top-1), eliminating redundant features. We further identify a
limitation in constant SAE steering, which often produces degenerate outputs
such as repetitive single words. To mitigate this, we introduce a token-wise
decaying steering strategy, enabling more faithful comparisons with mean
activation difference baselines. Empirically, we show that steering an SAE
latent associated with reasoning reliably elicits step-by-step mathematical
reasoning and enhances inference quality, functionally resembling the effect of
appending a guiding token. Our results demonstrate that SAEs outperform mean
activation difference methods on mathematical reasoning benchmarks and match
their performance on IF-Eval.

</details>


### [25] [Let's Play Across Cultures: A Large Multilingual, Multicultural Benchmark for Assessing Language Models' Understanding of Sports](https://arxiv.org/abs/2510.01247)
*Punit Kumar Singh,Nishant Kumar,Akash Ghosh,Kunal Pasad,Khushi Soni,Manisha Jaishwal,Sriparna Saha,Syukron Abu Ishaq Alfarozi,Asres Temam Abagissa,Kitsuchart Pasupa,Haiqin Yang,Jose G Moreno*

Main category: cs.CL

TL;DR: CultSportQA is a benchmark with 33,000 MCQs across text and image modalities to evaluate LMs' understanding of traditional sports from 60 countries and 6 continents, addressing the gap in evaluating regional and indigenous sports knowledge.


<details>
  <summary>Details</summary>
Motivation: Current LM evaluations focus on globally popular sports, overlooking regional and indigenous sporting traditions, creating a gap in assessing cultural understanding of sports.

Method: Created a benchmark with 33,000 MCQs across text and image modalities covering 60 countries and 6 continents, categorized into history-based, rule-based, and scenario-based questions. Evaluated using zero-shot, few-shot, and chain-of-thought prompting across LLMs, SLMs, and MLMs.

Result: The paper introduces CultSportQA as a comprehensive multilingual and multicultural sports benchmark, but does not provide specific evaluation results of the models tested.

Conclusion: CultSportQA establishes a new standard for assessing AI's ability to understand and reason about traditional sports, providing a comprehensive framework for evaluating cultural sports knowledge across diverse models and prompting strategies.

Abstract: Language Models (LMs) are primarily evaluated on globally popular sports,
often overlooking regional and indigenous sporting traditions. To address this
gap, we introduce \textbf{\textit{CultSportQA}}, a benchmark designed to assess
LMs' understanding of traditional sports across 60 countries and 6 continents,
encompassing four distinct cultural categories. The dataset features 33,000
multiple-choice questions (MCQs) across text and image modalities, each of
which is categorized into three key types: history-based, rule-based, and
scenario-based. To evaluate model performance, we employ zero-shot, few-shot,
and chain-of-thought (CoT) prompting across a diverse set of Large Language
Models (LLMs), Small Language Models (SLMs), and Multimodal Large Language
Models (MLMs). By providing a comprehensive multilingual and multicultural
sports benchmark, \textbf{\textit{CultSportQA}} establishes a new standard for
assessing AI's ability to understand and reason about traditional sports.

</details>


### [26] [SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs](https://arxiv.org/abs/2510.01248)
*Ruyue Liu,Rong Yin,Xiangzhen Bo,Xiaoshuai Hao,Yong Liu,Jinwen Zhong,Can Ma,Weiping Wang*

Main category: cs.CL

TL;DR: SSTAG is a structure-aware self-supervised learning method for Text Attributed Graphs that bridges LLMs and GNNs through dual knowledge distillation, improving cross-domain generalization and scalability while reducing inference costs.


<details>
  <summary>Details</summary>
Motivation: Current graph learning models are trained on individual datasets, limiting cross-domain transfer and requiring large annotated data, unlike NLP/CV where pretrained models show strong generalization. Graph data's heterogeneity poses unique challenges.

Method: Proposes SSTAG with dual knowledge distillation framework co-distilling LLMs and GNNs into structure-aware MLPs, plus in-memory mechanism storing typical graph representations aligned with memory anchors to integrate invariant knowledge.

Result: Extensive experiments show SSTAG outperforms state-of-the-art models on cross-domain transfer learning, achieves exceptional scalability, and reduces inference costs while maintaining competitive performance.

Conclusion: SSTAG successfully addresses graph learning limitations by unifying text representation, enabling effective knowledge transfer across graphs and tasks with improved generalization and efficiency.

Abstract: Large scale pretrained models have revolutionized Natural Language Processing
(NLP) and Computer Vision (CV), showcasing remarkable cross domain
generalization abilities. However, in graph learning, models are typically
trained on individual graph datasets, limiting their capacity to transfer
knowledge across different graphs and tasks. This approach also heavily relies
on large volumes of annotated data, which presents a significant challenge in
resource-constrained settings. Unlike NLP and CV, graph structured data
presents unique challenges due to its inherent heterogeneity, including domain
specific feature spaces and structural diversity across various applications.
To address these challenges, we propose a novel structure aware self supervised
learning method for Text Attributed Graphs (SSTAG). By leveraging text as a
unified representation medium for graph learning, SSTAG bridges the gap between
the semantic reasoning of Large Language Models (LLMs) and the structural
modeling capabilities of Graph Neural Networks (GNNs). Our approach introduces
a dual knowledge distillation framework that co-distills both LLMs and GNNs
into structure-aware multilayer perceptrons (MLPs), enhancing the scalability
of large-scale TAGs. Additionally, we introduce an in-memory mechanism that
stores typical graph representations, aligning them with memory anchors in an
in-memory repository to integrate invariant knowledge, thereby improving the
model's generalization ability. Extensive experiments demonstrate that SSTAG
outperforms state-of-the-art models on cross-domain transfer learning tasks,
achieves exceptional scalability, and reduces inference costs while maintaining
competitive performance.

</details>


### [27] [LOCA: Logical Chain Augmentation for Scientific Corpus Cleaning](https://arxiv.org/abs/2510.01249)
*You-Le Fang,Dong-Shan Jian,Xiang Li,Ce Meng,Ling-Shi Meng,Chen-Xu Yan,Zhi-Zhang Bian,Yan-Qing Ma*

Main category: cs.CL

TL;DR: LOCA is a framework that automatically cleans scientific QA datasets by completing missing logical steps and separating principles from derivations, reducing error rates from 20% to below 2%.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with scientific problem-solving due to logical leaps and implicit reasoning in existing scientific QA datasets, which have high error rates.

Method: LOCA uses an augment-and-review loop to enhance raw answers by completing missing logical steps and explicitly separating scientific principles from their derivations.

Result: LOCA reduces error rates in scientific corpora from as high as 20% to below 2%, effectively filtering noisy datasets.

Conclusion: LOCA provides a scalable methodology for creating high-quality scientific corpora, enabling more reliable training and evaluation of scientific AI.

Abstract: While Large Language Models (LLMs) excel in general domains, their
reliability often falls short in scientific problem-solving. The advancement of
scientific AI depends on large-scale, high-quality corpora. However, existing
scientific question-answering (QA) datasets suffer from high error rates,
frequently resulting from logical leaps and implicit reasoning within the
answers. To address this issue, we introduce LOCA (Logical Chain Augmentation),
a novel framework for automatically cleaning scientific corpora, implemented
through an augment-and-review loop. At its core, LOCA enhances raw answers by
completing missing logical steps and explicitly separating the underlying
scientific principle from its subsequent derivation. By applying LOCA to
challenging scientific corpora, we demonstrate that it can automatically filter
noisy datasets, typically reducing the error rate from as high as 20\% to below
2\%. LOCA provides a scalable and effective methodology for creating
high-quality scientific corpora, paving the way for more reliable training and
evaluation of scientific AI.

</details>


### [28] [GemDetox at TextDetox CLEF 2025: Enhancing a Massively Multilingual Model for Text Detoxification on Low-resource Languages](https://arxiv.org/abs/2510.01250)
*Trung Duc Anh Dang,Ferdinando Pio D'Elia*

Main category: cs.CL

TL;DR: This paper presents a multilingual text detoxification system that won the PAN 2025 challenge, using a fine-tuned Gemma-3 transformer to rewrite toxic sentences into neutral paraphrases across 15 languages.


<details>
  <summary>Details</summary>
Motivation: As social media platforms evolve faster than regulations, automated detoxification can help moderators enforce safe discourse at scale.

Method: Built on a 12B-parameter Gemma-3 multilingual transformer with LoRA SFT fine-tuning, few-shot and Chain-of-Thought prompting. Used combined training corpus of human-authored, machine-translated, and model-generated pairs. Enriched inputs with LaBSE-retrieved neighbors and toxic-span annotations at inference.

Result: Ranked first on both high-resource and low-resource languages. Ablations showed +0.081 joint score increase from few-shot examples and +0.088 from basic CoT prompting. Language resource status was the strongest performance predictor (Î·Â² = 0.667, p < 0.01).

Conclusion: The proposed multilingual detoxification system effectively handles diverse languages and demonstrates the value of prompting techniques and language resource considerations in text detoxification tasks.

Abstract: As social-media platforms emerge and evolve faster than the regulations meant
to oversee them, automated detoxification might serve as a timely tool for
moderators to enforce safe discourse at scale. We here describe our submission
to the PAN 2025 Multilingual Text Detoxification Challenge, which rewrites
toxic single-sentence inputs into neutral paraphrases across 15 typologically
diverse languages. Building on a 12B-parameter Gemma-3 multilingual
transformer, we apply parameter-efficient LoRA SFT fine-tuning and prompting
techniques like few-shot and Chain-of-Thought. Our multilingual training corpus
combines 3,600 human-authored parallel pairs, 21,600 machine-translated
synthetic pairs, and model-generated pairs filtered by Jaccard thresholds. At
inference, inputs are enriched with three LaBSE-retrieved neighbors and
explicit toxic-span annotations. Evaluated via Style Transfer Accuracy,
LaBSE-based semantic preservation, and xCOMET fluency, our system ranks first
on high-resource and low-resource languages. Ablations show +0.081 joint score
increase from few-shot examples and +0.088 from basic CoT prompting. ANOVA
analysis identifies language resource status as the strongest predictor of
performance ($\eta^2$ = 0.667, p < 0.01).

</details>


### [29] [Efficient Uncertainty Estimation for LLM-based Entity Linking in Tabular Data](https://arxiv.org/abs/2510.01251)
*Carlo Bono,Federico Belotti,Matteo Palmonari*

Main category: cs.CL

TL;DR: A self-supervised method for estimating uncertainty from single-shot LLM outputs using token-level features, reducing computational costs while maintaining effectiveness in detecting low-accuracy outputs for Entity Linking tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based Entity Linking methods require resource-intensive multi-shot inference for reliable uncertainty estimates, limiting their practical deployment in real-world scenarios.

Method: Self-supervised approach using token-level features from single-shot LLM outputs to estimate uncertainty, avoiding the need for multiple generations.

Result: The method produces highly effective uncertainty estimates for detecting low-accuracy outputs across multiple LLMs, achieving this at a fraction of the computational cost compared to multi-shot approaches.

Conclusion: This approach enables cost-effective integration of uncertainty measures into LLM-based Entity Linking workflows with limited computational overhead, making LLM deployment more practical for real-world applications.

Abstract: Linking textual values in tabular data to their corresponding entities in a
Knowledge Base is a core task across a variety of data integration and
enrichment applications. Although Large Language Models (LLMs) have shown
State-of-The-Art performance in Entity Linking (EL) tasks, their deployment in
real-world scenarios requires not only accurate predictions but also reliable
uncertainty estimates, which require resource-demanding multi-shot inference,
posing serious limits to their actual applicability. As a more efficient
alternative, we investigate a self-supervised approach for estimating
uncertainty from single-shot LLM outputs using token-level features, reducing
the need for multiple generations. Evaluation is performed on an EL task on
tabular data across multiple LLMs, showing that the resulting uncertainty
estimates are highly effective in detecting low-accuracy outputs. This is
achieved at a fraction of the computational cost, ultimately supporting a
cost-effective integration of uncertainty measures into LLM-based EL workflows.
The method offers a practical way to incorporate uncertainty estimation into EL
workflows with limited computational overhead.

</details>


### [30] [GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models](https://arxiv.org/abs/2510.01252)
*Mariam Mahran,Katharina Simbeck*

Main category: cs.CL

TL;DR: LLMs paired with sparse autoencoders can interpret both model behavior and training data structures, biases, and themes, as demonstrated on Jane Austen novels.


<details>
  <summary>Details</summary>
Motivation: To understand model representations and the data internalized by LLMs trained on massive uncurated corpora, which is a major challenge.

Method: Train a GPT-style transformer on Jane Austen novels, then apply sparse autoencoders to hidden states across multiple layers to uncover sparse, interpretable features.

Result: Uncovered features reflecting key narratives and concepts in the corpus, including gender, class, and societal duty.

Conclusion: LLMs combined with SAEs can act as scalable probes into complex datasets for corpus exploration, bias discovery, and model interpretability.

Abstract: As large language models (LLMs) are increasingly trained on massive,
uncurated corpora, understanding both model representations and the data they
internalize has become a major challenge. In this work, we show that pairing
LLMs with sparse autoencoders (SAEs) enables interpretation not only of model
behavior but also of the deeper structures, themes, and biases embedded in the
training data. We train a GPT-style transformer model exclusively on the novels
of Jane Austen, a corpus rich in social constructs and narrative patterns. We
then apply SAEs to hidden states across multiple layers, uncovering sparse,
interpretable features that reflect the key narratives and concepts present in
the corpus, including gender, class, and societal duty. Our findings
demonstrate that LLMs combined with SAEs can act as scalable probes into
complex datasets, offering a new path for corpus exploration, bias discovery,
and model interpretability at scale.

</details>


### [31] [Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender Bias in SpeechLLMs](https://arxiv.org/abs/2510.01254)
*Shree Harsha Bokkahalli Satish,Gustav Eje Henter,Ãva SzÃ©kely*

Main category: cs.CL

TL;DR: Current MCQA bias benchmarks for SpeechLLMs show limited cross-task generalization, as model behaviors trained on one MCQA format don't reliably transfer to other MCQA tasks or long-form generation tasks.


<details>
  <summary>Details</summary>
Motivation: To test the assumption that model performance on multiple-choice question answering (MCQA) bias benchmarks consistently generalizes to other MCQA tasks and more realistic long-form evaluations in speech models.

Method: Fine-tuned three SpeechLLMs using LoRA adapters to induce specific MCQA behaviors (stereotypical, anti-stereotypical, or neutral preferences), then evaluated generalization to another MCQA benchmark and long-form creative generation tasks.

Result: Performance on MCQA bias benchmarks fails to reliably predict performance across other MCQA benchmarks and long-form tasks, showing limited cross-task generalization.

Conclusion: Current MCQA bias benchmarks have limited evidence of cross-task generalization in speech domain; proposed evaluation suite for measuring behavior transferability in future models.

Abstract: Recent work in benchmarking bias and fairness in speech large language models
(SpeechLLMs) has relied heavily on multiple-choice question answering (MCQA)
formats. The model is tasked to choose between stereotypical,
anti-stereotypical, or neutral/irrelevant answers given an input speech prompt
and an optional text prompt. Such MCQA benchmarks implicitly assume that model
performance is consistent across other MCQA tasks, voices, and other task
formats such as more realistic, long-form evaluations. In this paper, we probe
that assumption.
  We fine-tune three SpeechLLMs using LoRA adapters to induce specific MCQA
behaviours: preference for stereotypical, anti-stereotypical, or
neutral/uncertain answers. We then evaluate whether these behaviours generalise
to another, distinct MCQA benchmark, and more critically to long-form, creative
generation tasks. Our results show that performance on MCQA bias benchmarks
fails to reliably predict performances across other MCQA benchmarks, and more
importantly across long-form tasks. We conclude that current MCQA bias
benchmarks show limited evidence of cross-task generalisation in the speech
domain, and also propose an evaluation suite for measuring behaviour
transferability in future models and benchmarks.

</details>


### [32] [Longitudinal Monitoring of LLM Content Moderation of Social Issues](https://arxiv.org/abs/2510.01255)
*Yunlang Dai,Emma Lurie,DanaÃ© Metaxa,Sorelle A. Friedler*

Main category: cs.CL

TL;DR: AI Watchman is a longitudinal auditing system that tracks LLM refusals over time to provide transparency into opaque content moderation practices, detecting policy changes and identifying company/model-specific differences.


<details>
  <summary>Details</summary>
Motivation: LLM outputs are shaped by opaque and frequently-changing company content moderation policies, particularly through refusal behaviors that subtly shape public discourse, creating a need for transparency.

Method: Developed AI Watchman system using a dataset of over 400 social issues to audit OpenAI's moderation endpoint, GPT-4.1, GPT-5, and DeepSeek (in English and Chinese), with qualitative analysis of refusal forms.

Result: Detected unannounced company policy changes, identified company- and model-specific moderation differences, and categorized various refusal forms through longitudinal monitoring.

Conclusion: Demonstrates the value of longitudinal LLM auditing and provides AI Watchman as a system for ongoing transparency into content moderation practices.

Abstract: Large language models' (LLMs') outputs are shaped by opaque and
frequently-changing company content moderation policies and practices. LLM
moderation often takes the form of refusal; models' refusal to produce text
about certain topics both reflects company policy and subtly shapes public
discourse. We introduce AI Watchman, a longitudinal auditing system to publicly
measure and track LLM refusals over time, to provide transparency into an
important and black-box aspect of LLMs. Using a dataset of over 400 social
issues, we audit Open AI's moderation endpoint, GPT-4.1, and GPT-5, and
DeepSeek (both in English and Chinese). We find evidence that changes in
company policies, even those not publicly announced, can be detected by AI
Watchman, and identify company- and model-specific differences in content
moderation. We also qualitatively analyze and categorize different forms of
refusal. This work contributes evidence for the value of longitudinal auditing
of LLMs, and AI Watchman, one system for doing so.

</details>


### [33] [RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs](https://arxiv.org/abs/2510.01257)
*Can Lin,Zhengwang Jiang,Ling Zheng,Qi Zhao,Yuhang Zhang,Qi Song,Wangqiu Zhou*

Main category: cs.CL

TL;DR: RJE framework enhances KGQA by combining retrieval, judgment, and exploration with specialized modules, enabling small LLMs to achieve competitive performance while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing KGQA methods: retrieval-based approaches constrained by retrieval quality, and agent-based methods dependent on proprietary LLMs.

Method: Propose Retrieval-Judgment-Exploration (RJE) framework with three specialized modules: Reasoning Path Ranking, Question Decomposition, and Retriever-assisted Exploration.

Result: Outperforms existing baselines with proprietary LLMs, enables small open-source LLMs (3B/8B) to achieve competitive results without fine-tuning, and significantly reduces LLM calls and token usage.

Conclusion: RJE provides an effective and efficient framework for KGQA that bridges the gap between retrieval and reasoning while enabling smaller LLMs to perform competitively.

Abstract: Knowledge graph question answering (KGQA) aims to answer natural language
questions using knowledge graphs. Recent research leverages large language
models (LLMs) to enhance KGQA reasoning, but faces limitations: retrieval-based
methods are constrained by the quality of retrieved information, while
agent-based methods rely heavily on proprietary LLMs. To address these
limitations, we propose Retrieval-Judgment-Exploration (RJE), a framework that
retrieves refined reasoning paths, evaluates their sufficiency, and
conditionally explores additional evidence. Moreover, RJE introduces
specialized auxiliary modules enabling small-sized LLMs to perform effectively:
Reasoning Path Ranking, Question Decomposition, and Retriever-assisted
Exploration. Experiments show that our approach with proprietary LLMs (such as
GPT-4o-mini) outperforms existing baselines while enabling small open-source
LLMs (such as 3B and 8B parameters) to achieve competitive results without
fine-tuning LLMs. Additionally, RJE substantially reduces the number of LLM
calls and token usage compared to agent-based methods, yielding significant
efficiency improvements.

</details>


### [34] [Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse](https://arxiv.org/abs/2510.01258)
*Nathan Junzi Chen*

Main category: cs.CL

TL;DR: This paper evaluates political biases in six mainstream large language models using zero-shot classification across four metrics: ideological alignment, topicality, response sentiment, and objectivity, finding amplified liberal-authoritarian alignment across all models.


<details>
  <summary>Details</summary>
Motivation: The study addresses the problem of internalized political biases in generative AI systems stemming from training data skews, human prejudice, and algorithmic flaws, which can distort political discourse and influence human-computer interactions.

Method: Employed zero-shot classification approach with 1800 model responses across six LLMs, using four distinct fine-tuned classification algorithms to compute bias evaluation metrics: ideological alignment, topicality, response sentiment, and objectivity.

Result: Results showed amplified liberal-authoritarian alignment across all six evaluated LLMs, with notable instances of reasoning supersessions and canned refusals, indicating systematic political biases in the models.

Conclusion: The study highlights how intrinsic AI biases can permeate public discourse and distort the political landscape, potentially leading to conformity or polarization depending on pre-existing socio-political structures in different regions.

Abstract: Amidst the rapid normalization of generative artificial intelligence (GAI),
intelligent systems have come to dominate political discourse across
information mediums. However, internalized political biases stemming from
training data skews, human prejudice, and algorithmic flaws continue to plague
the novel technology. This paper employs a zero-shot classification approach to
evaluate algorithmic political partisanship through a methodical combination of
ideological alignment, topicality, response sentiment, and objectivity. A total
of 1800 model responses across six mainstream large language models (LLMs) were
individually input into four distinct fine-tuned classification algorithms,
each responsible for computing an aforementioned bias evaluation metric.
Results show an amplified liberal-authoritarian alignment across all six LLMs
evaluated, with notable instances of reasoning supersessions and canned
refusals. The study subsequently highlights the psychological influences
underpinning human-computer interactions and how intrinsic biases can permeate
public discourse. The resulting distortion of the political landscape can
ultimately manifest as conformity or polarization, depending on a region's
pre-existing socio-political structures.

</details>


### [35] [OpenAI's GPT-OSS-20B Model and Safety Alignment Issues in a Low-Resource Language](https://arxiv.org/abs/2510.01266)
*Isa Inuwa-Dutse*

Main category: cs.CL

TL;DR: The paper reveals vulnerabilities in GPT-OSS-20b model's safety alignment for Hausa language, showing it generates harmful, culturally insensitive content and factual errors through linguistic reward hacking.


<details>
  <summary>Details</summary>
Motivation: To question the model's reliability for underrepresented communities by testing its performance in low-resource language settings, particularly focusing on Hausa speakers.

Method: Used red-teaming with minimal prompting in Hausa language, conducted a survey (n=61) to validate toxicity perceptions, and analyzed model behavior through linguistic reward hacking.

Result: Found biases, inaccuracies, cultural insensitivities; model generated harmful content, falsely claimed toxic substances as safe, couldn't distinguish raw/processed foods, used demeaning proverbs; safety protocols relaxed with polite language.

Conclusion: Attributed flaws to insufficient safety tuning in low-resource contexts, highlighting significant gaps in current red-teaming efforts for underrepresented languages.

Abstract: In response to the recent safety probing for OpenAI's GPT-OSS-20b model, we
present a summary of a set of vulnerabilities uncovered in the model, focusing
on its performance and safety alignment in a low-resource language setting. The
core motivation for our work is to question the model's reliability for users
from underrepresented communities. Using Hausa, a major African language, we
uncover biases, inaccuracies, and cultural insensitivities in the model's
behaviour. With a minimal prompting, our red-teaming efforts reveal that the
model can be induced to generate harmful, culturally insensitive, and factually
inaccurate content in the language. As a form of reward hacking, we note how
the model's safety protocols appear to relax when prompted with polite or
grateful language, leading to outputs that could facilitate misinformation and
amplify hate speech. For instance, the model operates on the false assumption
that common insecticide locally known as Fiya-Fiya (Cyphermethrin) and
rodenticide like Shinkafar Bera (a form of Aluminium Phosphide) are safe for
human consumption. To contextualise the severity of this error and popularity
of the substances, we conducted a survey (n=61) in which 98% of participants
identified them as toxic. Additional failures include an inability to
distinguish between raw and processed foods and the incorporation of demeaning
cultural proverbs to build inaccurate arguments. We surmise that these issues
manifest through a form of linguistic reward hacking, where the model
prioritises fluent, plausible-sounding output in the target language over
safety and truthfulness. We attribute the uncovered flaws primarily to
insufficient safety tuning in low-resource linguistic contexts. By
concentrating on a low-resource setting, our approach highlights a significant
gap in current red-teaming effort and offer some recommendations.

</details>


### [36] [AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees](https://arxiv.org/abs/2510.01268)
*Hongyi Zhou,Jin Zhu,Pingfan Su,Kai Ye,Ying Yang,Shakeel A O B Gavioli-Akilagun,Chengchun Shi*

Main category: cs.CL

TL;DR: AdaDetectGPT is a novel LLM-generated text detector that adaptively learns witness functions from training data to enhance logits-based detection, achieving up to 58% improvement over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing logits-based detectors rely solely on log probabilities, which can be sub-optimal for distinguishing human-written text from LLM-generated content.

Method: Proposes AdaDetectGPT which adaptively learns witness functions from training data to improve logits-based detection, with statistical guarantees on performance metrics.

Result: Extensive experiments show AdaDetectGPT nearly uniformly improves state-of-the-art methods across various dataset-LLM combinations, with improvements reaching up to 58%.

Conclusion: AdaDetectGPT provides an effective adaptive approach for LLM-generated text detection that significantly outperforms existing logits-based methods.

Abstract: We study the problem of determining whether a piece of text has been authored
by a human or by a large language model (LLM). Existing state of the art
logits-based detectors make use of statistics derived from the log-probability
of the observed text evaluated using the distribution function of a given
source LLM. However, relying solely on log probabilities can be sub-optimal. In
response, we introduce AdaDetectGPT -- a novel classifier that adaptively
learns a witness function from training data to enhance the performance of
logits-based detectors. We provide statistical guarantees on its true positive
rate, false positive rate, true negative rate and false negative rate.
Extensive numerical studies show AdaDetectGPT nearly uniformly improves the
state-of-the-art method in various combination of datasets and LLMs, and the
improvement can reach up to 58%. A python implementation of our method is
available at https://github.com/Mamba413/AdaDetectGPT.

</details>


### [37] [Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection](https://arxiv.org/abs/2510.01270)
*Hoang Phan,Victor Li,Qi Lei*

Main category: cs.CL

TL;DR: Progressive Self-Reflection (PSR) is a novel inference-time technique that enables LLMs to self-monitor and correct outputs dynamically, significantly reducing attack success rates without additional training while maintaining performance on benign tasks.


<details>
  <summary>Details</summary>
Motivation: Address concerns about LLMs generating harmful or inappropriate content by developing a method that allows models to self-monitor and correct outputs during inference.

Method: Progressive Self-Reflection (PSR) - a test-time scaling method where models undergo multiple self-reflection rounds to enhance safety. Includes a lightweight self-reflection predictor that estimates optimal reflection rounds based on input complexity.

Result: Reduced attack success rates significantly: Llama-3.1-8B-Instruct from 77.5% to 5.9%, Llama-3.1-8B base from 89.7% to 5.6%, and Qwen2.5-7B-Instruct from 44.4% to 3.8%, while maintaining original performance on benign tasks.

Conclusion: PSR serves as a scalable test-time approach that enhances LLM safety by dynamically allocating computational resources in proportion to input risk profile, balancing safety with computational efficiency.

Abstract: Large language models (LLMs) have revolutionized natural language processing
with their ability to generate coherent and contextually relevant text.
However, their deployment raises significant concerns about the potential for
generating harmful or inappropriate content. In this paper, we introduce
Progressive Self-Reflection (PSR), a novel inference-time technique that
empowers LLMs to self-monitor and correct their outputs dynamically.
Experimental results demonstrate that applying our proposed method to
Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\% to 5.9\%, to
Llama-3.1-8B base from 89.7\% to 5.6\%, and to Qwen2.5-7B-Instruct from 44.4\%
to 3.8\%, without additional training, while maintaining their original
performance on benign tasks. Our approach acts as a test-time scaling method,
where additional self-reflection rounds enhance safety at the cost of inference
overhead. To balance safety with computational efficiency, we introduce a
lightweight self-reflection predictor that estimates the optimal number of
reflection rounds based on input complexity. This adaptive mechanism prevents
unnecessary self-assessment on benign inputs while ensuring thorough evaluation
when encountering potentially harmful content. Our findings suggest that
Progressive Self-Reflection serves as a scalable test-time approach, enhancing
LLM safety by dynamically allocating computational resources in proportion to
the input's risk profile.

</details>


### [38] [TraceDet: Hallucination Detection from the Decoding Trace of Diffusion Large Language Models](https://arxiv.org/abs/2510.01274)
*Shenxu Chang,Junchi Yu,Weixing Wang,Yongqiang Chen,Jialin Yu,Philip Torr,Jindong Gu*

Main category: cs.CL

TL;DR: TraceDet is a novel framework that detects hallucinations in Diffusion LLMs by analyzing intermediate denoising steps, achieving 15.2% AUROC improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing hallucination detection methods designed for auto-regressive LLMs are ill-suited for Diffusion LLMs because they rely on single-step generation signals, while hallucination signals in D-LLMs emerge throughout the multi-step denoising process.

Method: TraceDet models the denoising process as an action trace, identifies the sub-trace maximally informative to hallucinated responses, and leverages key hallucination signals from the multi-step denoising process.

Result: Extensive experiments show TraceDet consistently improves hallucination detection across various open-source D-LLMs, achieving an average 15.2% gain in AUROC compared to baseline methods.

Conclusion: TraceDet effectively bridges the gap in hallucination detection for Diffusion LLMs by explicitly leveraging intermediate denoising steps, significantly enhancing detection performance and reliability.

Abstract: Diffusion large language models (D-LLMs) have recently emerged as a promising
alternative to auto-regressive LLMs (AR-LLMs). However, the hallucination
problem in D-LLMs remains underexplored, limiting their reliability in
real-world applications. Existing hallucination detection methods are designed
for AR-LLMs and rely on signals from single-step generation, making them
ill-suited for D-LLMs where hallucination signals often emerge throughout the
multi-step denoising process. To bridge this gap, we propose TraceDet, a novel
framework that explicitly leverages the intermediate denoising steps of D-LLMs
for hallucination detection. TraceDet models the denoising process as an action
trace, with each action defined as the model's prediction over the cleaned
response, conditioned on the previous intermediate output. By identifying the
sub-trace that is maximally informative to the hallucinated responses, TraceDet
leverages the key hallucination signals in the multi-step denoising process of
D-LLMs for hallucination detection. Extensive experiments on various open
source D-LLMs demonstrate that TraceDet consistently improves hallucination
detection, achieving an average gain in AUROC of 15.2% compared to baselines.

</details>


### [39] [LLM Based Sentiment Classification From Bangladesh E-Commerce Reviews](https://arxiv.org/abs/2510.01276)
*Sumaiya Tabassum*

Main category: cs.CL

TL;DR: This paper investigates the use of transformer-based BERT models and LLMs for sentiment analysis on Bangladesh e-commerce reviews, finding that fine-tuned Llama-3.1-8B outperforms other models with 95.5% accuracy.


<details>
  <summary>Details</summary>
Motivation: Sentiment analysis is crucial for understanding consumer emotions and preferences, but accurate analysis is challenged by language complexity and diversity. The study aims to explore LLMs' viability for sentiment analysis in Bangladesh e-commerce contexts.

Method: Used 4000 samples from Bangla and English customer reviews to fine-tune models including Llama-3.1-8B, Phi-3.5-mini-instruct, Mistral-7B-v0.1, DistilBERT-multilingual, mBERT, and XLM-R-base. Applied parameter efficient fine-tuning methods (LoRA and PEFT) to reduce computational overhead.

Result: The fine-tuned Llama-3.1-8B model achieved the best performance with 95.5% accuracy, 93% precision, 88% recall, and 90% F1 score, outperforming all other tested models.

Conclusion: LLMs can be effectively applied to sentiment analysis in resource-constrained environments using parameter efficient fine-tuning methods, demonstrating their practical viability for understanding consumer sentiment in multilingual e-commerce contexts.

Abstract: Sentiment analysis is an essential part of text analysis, which is a larger
field that includes determining and evaluating the author's emotional state.
This method is essential since it makes it easier to comprehend consumers'
feelings, viewpoints, and preferences holistically. The introduction of large
language models (LLMs), such as Llama, has greatly increased the availability
of cutting-edge model applications, such as sentiment analysis. However,
accurate sentiment analysis is hampered by the intricacy of written language
and the diversity of languages used in evaluations. The viability of using
transformer-based BERT models and other LLMs for sentiment analysis from
Bangladesh e commerce reviews is investigated in this paper. A subset of 4000
samples from the original dataset of Bangla and English customer reviews was
utilized to fine-tune the model. The fine tuned Llama-3.1-8B model outperformed
other fine-tuned models, including Phi-3.5-mini-instruct, Mistral-7B-v0.1,
DistilBERT-multilingual, mBERT, and XLM-R-base, with an overall accuracy,
precision, recall, and F1 score of 95.5%, 93%, 88%, 90%. The study emphasizes
how parameter efficient fine-tuning methods (LoRA and PEFT) can lower
computational overhead and make it appropriate for contexts with limited
resources. The results show how LLMs can

</details>


### [40] [TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture](https://arxiv.org/abs/2510.01279)
*Yongchao Chen,Jiefeng Chen,Rui Meng,Ji Yin,Na Li,Chuchu Fan,Chi Wang,Tomas Pfister,Jinsung Yoon*

Main category: cs.CL

TL;DR: TUMIX is an ensemble framework that runs multiple agents in parallel with different tool-use strategies, enabling iterative sharing and refinement of responses to improve reasoning accuracy while maintaining near-equal inference costs.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack practical guidance on optimal tool use despite having tools like Code Interpreter and Search. The challenge is effectively combining textual reasoning, coding, and search for diverse questions.

Method: Propose Tool-Use Mixture (TUMIX) - an ensemble framework running multiple agents in parallel with distinct tool-use strategies and answer paths. Agents iteratively share and refine responses based on questions and previous answers.

Result: Achieves average accuracy improvement of up to 3.55% over best baseline on Gemini-2.5-Pro and Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference costs. Can halt refinement at 49% cost while preserving performance.

Conclusion: Agent diversity and quality are crucial and can be enhanced by auto-optimizing agent designs. TUMIX provides effective tool-use combination while maintaining cost efficiency, with further scaling possible for higher performance at greater cost.

Abstract: While integrating tools like Code Interpreter and Search has significantly
enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and
Gemini-Pro, practical guidance on optimal tool use is lacking. The core
challenge is effectively combining textual reasoning, coding, and search for
diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an
ensemble framework that runs multiple agents in parallel, each employing
distinct tool-use strategies and answer paths. Agents in TUMIX iteratively
share and refine responses based on the question and previous answers. In
experiments, TUMIX achieves significant gains over state-of-the-art
tool-augmented and test-time scaling methods, delivering an average accuracy
improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and
Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference
costs. We find that agent diversity and quality are crucial and can be enhanced
by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt
refinement upon reaching sufficient confidence, preserving performance at only
49% of the inference cost. Further scaling can achieve higher performance,
albeit at a greater cost.

</details>


### [41] [Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing](https://arxiv.org/abs/2510.01283)
*Israel Abebe Azime,Tadesse Destaw Belay,Atnafu Lambebo Tonja*

Main category: cs.CL

TL;DR: This paper introduces an evaluation framework for assessing Deep Research tools' capabilities, using academic survey writing as a case study to evaluate OpenAI's and Google's Deep Search tools.


<details>
  <summary>Details</summary>
Motivation: There is a need for standardized evaluation metrics to assess the capabilities of LLM-powered Deep Research tools that can autonomously perform knowledge-intensive tasks like web browsing and report generation.

Method: Developed an evaluation sheet for Deep Research tools and applied it to assess OpenAI's Deep Search and Google's Deep Search in generating academic surveys.

Result: Evaluation revealed significant gaps between search engines and standalone Deep Research tools, with shortcomings in accurately representing the targeted research areas.

Conclusion: Carefully crafted evaluation standards are essential for properly assessing Deep Research tools, as current tools show limitations in comprehensive knowledge representation.

Abstract: Large Language Models (LLMs) powered with argentic capabilities are able to
do knowledge-intensive tasks without human involvement. A prime example of this
tool is Deep research with the capability to browse the web, extract
information and generate multi-page reports. In this work, we introduce an
evaluation sheet that can be used for assessing the capability of Deep Research
tools. In addition, we selected academic survey writing as a use case task and
evaluated output reports based on the evaluation sheet we introduced. Our
findings show the need to have carefully crafted evaluation standards. The
evaluation done on OpenAI`s Deep Search and Google's Deep Search in generating
an academic survey showed the huge gap between search engines and standalone
Deep Research tools, the shortcoming in representing the targeted area.

</details>


### [42] [HiSpec: Hierarchical Speculative Decoding for LLMs](https://arxiv.org/abs/2510.01336)
*Avinash Kumar,Sujay Sanghavi,Poulami Das*

Main category: cs.CL

TL;DR: HiSpec is a hierarchical speculative decoding framework that uses early-exit models for intermediate verification to accelerate LLM inference, achieving 1.28Ã average throughput improvement without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Verification is the bottleneck in speculative decoding (4Ã slower than token generation), and existing intermediate verification methods have substantial training overheads, increased memory footprint, and compromise accuracy with approximate heuristics.

Method: Uses early-exit models for low-overhead intermediate verification, reuses key-value caches and hidden states between draft, intermediate verifier, and target models, and periodically validates accepted draft tokens against the target model.

Result: Improves throughput by 1.28Ã on average and up to 2.01Ã compared to baseline single-layer speculation while maintaining accuracy across various benchmarks and models.

Conclusion: HiSpec provides an efficient framework for high-throughput speculative decoding that addresses verification bottlenecks through early-exit models and resource reuse, achieving significant speedups without compromising accuracy.

Abstract: Speculative decoding accelerates LLM inference by using a smaller draft model
to speculate tokens that a larger target model verifies. Verification is often
the bottleneck (e.g. verification is $4\times$ slower than token generation
when a 3B model speculates for a 70B target model), but most prior works focus
only on accelerating drafting. $\textit{``Intermediate"}$ verification reduces
verification time by discarding inaccurate draft tokens early, but existing
methods incur substantial training overheads in incorporating the intermediate
verifier, increase the memory footprint to orchestrate the intermediate
verification step, and compromise accuracy by relying on approximate
heuristics.
  We propose $\underline{\textit{Hi}}\textit{erarchical
}\underline{\textit{Spec}}\textit{ulative Decoding (HiSpec)}$, a framework for
high-throughput speculative decoding that exploits $\textit{early-exit (EE)
models}$ for low-overhead intermediate verification. EE models allow tokens to
exit early by skipping layer traversal and are explicitly trained so that
hidden states at selected layers can be interpreted, making them uniquely
suited for intermediate verification without drastically increasing compute and
memory overheads. To improve resource-efficiency even further, we design a
methodology that enables HiSpec to re-use key-value caches and hidden states
between the draft, intermediate verifier, and target models. To maintain
accuracy, HiSpec periodically validates the draft tokens accepted by the
intermediate verifier against the target model. Our evaluations using various
representative benchmarks and models show that HiSpec improves throughput by
1.28$\times$ on average and by up to 2.01$\times$ compared to the baseline
single-layer speculation without compromising accuracy.

</details>


### [43] [TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies](https://arxiv.org/abs/2510.01391)
*Maithili Kadam,Francis Ferraro*

Main category: cs.CL

TL;DR: TAG-EQA is a prompting framework that injects causal event graphs into LLM inputs to improve event-based question answering, achieving up to 18% accuracy gains over text-only baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with event-based questions requiring causal or temporal reasoning, despite excelling at general language tasks.

Method: TAG-EQA converts structured causal event graphs into natural-language statements and combines them with three prompting strategies (zero-shot, few-shot, chain-of-thought) and three input modalities (text-only, graph-only, text+graph).

Result: On TORQUESTRA benchmark, TAG-EQA improves accuracy by 5% on average over text-only baselines, with gains up to 12% in zero-shot settings and 18% when graph-augmented CoT prompting is effective.

Conclusion: Causal graphs can enhance event reasoning in LLMs without fine-tuning, offering a flexible way to encode structure in prompt-based question answering.

Abstract: Large language models (LLMs) excel at general language tasks but often
struggle with event-based questions-especially those requiring causal or
temporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question
Answering), a prompting framework that injects causal event graphs into LLM
inputs by converting structured relations into natural-language statements.
TAG-EQA spans nine prompting configurations, combining three strategies
(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,
graph-only, text+graph), enabling a systematic analysis of when and how
structured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA
improves accuracy by 5% on average over text-only baselines, with gains up to
12% in zero-shot settings and 18% when graph-augmented CoT prompting is
effective. While performance varies by model and configuration, our findings
show that causal graphs can enhance event reasoning in LLMs without
fine-tuning, offering a flexible way to encode structure in prompt-based QA.

</details>


### [44] [A-VERT: Agnostic Verification with Embedding Ranking Targets](https://arxiv.org/abs/2510.01469)
*NicolÃ¡s Aguirre,Ramiro Caso,Ramiro RodrÃ­guez Colmeiro,Mauro Santelli,JoaquÃ­n Toranzo CalderÃ³n*

Main category: cs.CL

TL;DR: A structure-free evaluation method using semantic embedding distances to match target candidates with LM-generated text, achieving robust classification at low compute cost.


<details>
  <summary>Details</summary>
Motivation: Current LM response evaluation methods are either too expensive (LLM-as-a-Judge) or unrealistic (string-matching, logprob), requiring a more efficient and practical solution.

Method: Uses semantic embedding distances to match target candidates with arbitrary LM-generated text, employing embedding models with less than 10B parameters.

Result: Achieves regression score of ~0.97 and accuracy of ~96% against human annotators across 3 datasets and 3 different LM architectures.

Conclusion: The proposed method provides an efficient and effective alternative for automatic LM response evaluation, balancing computational cost and performance.

Abstract: The automatic evaluation of Language Model (LM) responses is a critical piece
in the development of benchmarks and metrics, both for model training and
quality assessment of production model endpoints. The current approaches to
response classification relies on methods that are too expensive (i.e.
LLM-as-a-Judge) or that are far from real-world conditions (string-matching,
logprob). In this paper, a structure-free evaluation method is presented. The
method makes use of semantic embedding distances to match target candidates
with arbitrary LM-generated text, resulting in a robust classification of the
response at a relatively low compute cost (embedding models of less than $10B$
parameters). The results show a regression score of ~0.97 and an accuracy of
~96% against human annotators, tested over 3 data sets and 3 different LM
architectures.

</details>


### [45] [One More Question is Enough, Expert Question Decomposition (EQD) Model for Domain Quantitative Reasoning](https://arxiv.org/abs/2510.01526)
*Mengyu Wang,Sotirios Sabanis,Miguel de Carvalho,Shay B. Cohen,Tiejun Ma*

Main category: cs.CL

TL;DR: EQD is an efficient fine-tuning method that improves domain-specific quantitative reasoning in LLMs by decomposing complex questions into sub-questions, achieving 0.6%-10.5% performance gains with minimal training requirements.


<details>
  <summary>Details</summary>
Motivation: Domain-specific quantitative reasoning remains challenging for LLMs, especially in fields requiring expert knowledge and complex QA. Current approaches struggle to balance domain knowledge with computational efficiency.

Method: Two-step fine-tuning framework with reward function measuring sub-question effectiveness. Uses few thousand training examples and single A100 GPU. Inference time comparable to zero-shot prompting.

Result: Outperforms state-of-the-art domain-tuned models and advanced prompting strategies. Consistently improves QA performance by 0.6% to 10.5% across different LLMs in financial domain across four benchmark datasets.

Conclusion: In domain-specific QA, a single supporting question often provides greater benefit than detailed guidance steps. EQD effectively balances domain knowledge with computational efficiency for improved quantitative reasoning.

Abstract: Domain-specific quantitative reasoning remains a major challenge for large
language models (LLMs), especially in fields requiring expert knowledge and
complex question answering (QA). In this work, we propose Expert Question
Decomposition (EQD), an approach designed to balance the use of domain
knowledge with computational efficiency. EQD is built on a two-step fine-tuning
framework and guided by a reward function that measures the effectiveness of
generated sub-questions in improving QA outcomes. It requires only a few
thousand training examples and a single A100 GPU for fine-tuning, with
inference time comparable to zero-shot prompting. Beyond its efficiency, EQD
outperforms state-of-the-art domain-tuned models and advanced prompting
strategies. We evaluate EQD in the financial domain, characterized by
specialized knowledge and complex quantitative reasoning, across four benchmark
datasets. Our method consistently improves QA performance by 0.6% to 10.5%
across different LLMs. Our analysis reveals an important insight: in
domain-specific QA, a single supporting question often provides greater benefit
than detailed guidance steps.

</details>


### [46] [ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning](https://arxiv.org/abs/2510.01585)
*Haochen You,Baojing Liu*

Main category: cs.CL

TL;DR: ReSSFormer is a Recursive Sparse Structured Transformer that addresses long-context reasoning, computational efficiency, and structural generalization challenges through recurrent reasoning, sparse attention, and position-free structure induction.


<details>
  <summary>Details</summary>
Motivation: Transformers face challenges in long-context reasoning, computational efficiency, and structural generalization due to rigid layer stacking, dense attention, and reliance on positional encodings.

Method: Integrates three innovations: Recurrent Reasoning & Memory Unit (R2MU) for iterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM) for efficient context selection, and Self-Organizing Encoder Structure (SOES) for position-free structure induction.

Result: Consistently outperforms strong baselines across language modeling, multi-hop QA, and structure-sensitive tasks under comparable FLOPs and parameter budgets.

Conclusion: ReSSFormer demonstrates superior scalability, efficiency, and structural flexibility compared to conventional Transformers.

Abstract: While Transformer architectures have demonstrated impressive scalability
across domains, they continue to face challenges in long-context reasoning,
computational efficiency, and structural generalization - largely due to rigid
layer stacking, dense attention, and reliance on positional encodings. We
present ReSSFormer, a Recursive Sparse Structured Transformer that integrates
three complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) for
iterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM)
for efficient and focused context selection, and Self-Organizing Encoder
Structure (SOES) for position-free structure induction. ReSSFormer replaces
conventional depth stacking with recurrent inference, substitutes full
attention with token- and expert-level sparsity, and models latent token
topology directly from content. Across language modeling, multi-hop QA, and
structure-sensitive tasks, ReSSFormer consistently outperforms strong baselines
under comparable FLOPs and parameter budgets, highlighting its scalability,
efficiency, and structural flexibility.

</details>


### [47] [CLUE: Non-parametric Verification from Experience via Hidden-State Clustering](https://arxiv.org/abs/2510.01591)
*Zhenwen Liang,Ruosen Li,Yujun Zhou,Linfeng Song,Dian Yu,Xinya Du,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: CLUE is a non-parametric verifier that uses hidden state trajectories to assess LLM output quality, outperforming traditional methods without trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Existing methods for assessing LLM output quality either rely on text-level information (prone to overfitting) or token probabilities (fails on less-calibrated models), missing the richer information in hidden states.

Method: CLUE uses hidden state deltas from reasoning traces and classifies correctness via nearest-centroid distance to success/failure clusters formed from past experience, with no trainable parameters.

Result: CLUE consistently outperforms LLM-as-a-judge baselines and matches/exceeds confidence-based methods, improving top-1 and majority-vote accuracy on AIME 24/25 and GPQA datasets.

Conclusion: Hidden states provide a unified foundation for verification, with correctness encoded as geometrically separable signatures in activation trajectories, enabling effective non-parametric verification.

Abstract: Assessing the quality of Large Language Model (LLM) outputs presents a
critical challenge. Previous methods either rely on text-level information
(e.g., reward models, majority voting), which can overfit to superficial cues,
or on calibrated confidence from token probabilities, which would fail on
less-calibrated models. Yet both of these signals are, in fact, partial
projections of a richer source of information: the model's internal hidden
states. Early layers, closer to token embeddings, preserve semantic and lexical
features that underpin text-based judgments, while later layers increasingly
align with output logits, embedding confidence-related information. This paper
explores hidden states directly as a unified foundation for verification. We
show that the correctness of a solution is encoded as a geometrically separable
signature within the trajectory of hidden activations. To validate this, we
present Clue (Clustering and Experience-based Verification), a deliberately
minimalist, non-parametric verifier. With no trainable parameters, CLUE only
summarizes each reasoning trace by an hidden state delta and classifies
correctness via nearest-centroid distance to ``success'' and ``failure''
clusters formed from past experience. The simplicity of this method highlights
the strength of the underlying signal. Empirically, CLUE consistently
outperforms LLM-as-a-judge baselines and matches or exceeds modern
confidence-based methods in reranking candidates, improving both top-1 and
majority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24
with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%
(top-maj@16).

</details>


### [48] [A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.01600)
*Neal Gregory Lawton,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: We evaluate and compare strategies for fine-tuning Retrieval Augmented Generation (RAG) pipelines, including independent fine-tuning, joint fine-tuning, and two-phase fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Retrieval augmented generation (RAG) is a popular framework for question answering that uses two large language models (LLMs): an embedding model for retrieving relevant context documents and a generator model for generating answers. Both models can be fine-tuned to improve RAG pipeline performance on new tasks, but different fine-tuning strategies have varying costs and benefits that need evaluation.

Method: We evaluate and compare several RAG fine-tuning strategies, including independent fine-tuning, joint fine-tuning, and two-phase fine-tuning.

Result: In our experiments, we observe that all of these strategies achieve about equal improvement in EM and F1 generation quality metrics, although they have significantly different computational costs.

Conclusion: The optimal fine-tuning strategy to use depends on whether the training dataset includes context labels and whether a grid search over the learning rates for the embedding and generator models is required.

Abstract: A Comparison of Independent and Joint Fine-tuning Strategies for
Retrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,
Anoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP
2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0
Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),
Fine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and
compare strategies for fine-tuning Retrieval Augmented Generation (RAG)
pipelines, including independent fine-tuning, joint fine-tuning, and two-phase
fine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular
framework for question answering that is powered by two large language models
(LLMs): an embedding model that retrieves context documents from a database
that are relevant to a given question, and a generator model that uses the
retrieved context to generate an answer to the question. Both the embedding and
generator models can be fine-tuned to increase performance of a RAG pipeline on
a new task, but multiple fine-tuning strategies exist with different costs and
benefits. In this paper, we evaluate and compare several RAG fine-tuning
strategies, including independent, joint, and two-phase fine-tuning. In our
experiments, we observe that all of these strategies achieve about equal
improvement in EM and F1 generation quality metrics, although they have
significantly different computational costs. We conclude the optimal
fine-tuning strategy to use depends on whether the training dataset includes
context labels and whether a grid search over the learning rates for the
embedding and generator models is required.

</details>


### [49] [RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question Answering](https://arxiv.org/abs/2510.01612)
*Lovely Yeswanth Panchumarthi,Sai Prasad Gudari,Atharva Negi,Praveen Raj Budime,Harsit Upadhya*

Main category: cs.CL

TL;DR: RAG-BioQA is a biomedical QA framework that combines retrieval-augmented generation with domain fine-tuning to produce evidence-based long-form answers, outperforming baselines on PubMedQA.


<details>
  <summary>Details</summary>
Motivation: Current biomedical QA systems focus on short answers, lacking comprehensive explanations needed for clinical decision-making, while biomedical literature grows exponentially.

Method: Integrates BioBERT embeddings with FAISS indexing, compares re-ranking strategies (BM25, ColBERT, MonoT5) for context selection, and uses fine-tuned T5 model for evidence synthesis.

Result: Significant improvements over baselines on PubMedQA dataset, with substantial gains across BLEU, ROUGE, and METEOR metrics.

Conclusion: Advances the state of accessible, evidence-based biomedical knowledge retrieval through effective retrieval-generation integration.

Abstract: The exponential growth of biomedical literature creates significant
challenges for accessing precise medical information. Current biomedical
question-answering systems primarily focus on short-form answers, failing to
provide the comprehensive explanations necessary for clinical decision-making.
We present RAG-BioQA, a novel framework combining retrieval-augmented
generation with domain-specific fine-tuning to produce evidence-based,
long-form biomedical answers. Our approach integrates BioBERT embeddings with
FAISS indexing and compares various re-ranking strategies (BM25, ColBERT,
MonoT5) to optimize context selection before synthesizing evidence through a
fine-tuned T5 model. Experimental results on the PubMedQA dataset show
significant improvements over baselines, with our best model achieving
substantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state
of accessible, evidence-based biomedical knowledge retrieval.

</details>


### [50] [Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer GPU: Continual Pre-training, SFT, and DPO](https://arxiv.org/abs/2510.01616)
*Yu-Cheng Chih,Ming-Tao Duan,Yong-Hao Hou*

Main category: cs.CL

TL;DR: PureTC-1B is a three-stage stabilization pipeline for Llama-3.2-1B-Instruct that reduces Traditional Chinese token-level instability by 51.3% using CPT, SFT, and DPO with parameter-efficient LoRA adapters.


<details>
  <summary>Details</summary>
Motivation: Small Language Models (SLMs) face token-level instability in Traditional Chinese (TC), unpredictably emitting non-TC characters or code-switching into other languages, hindering deployment in cost-effective, on-device AI applications.

Method: Three-stage stabilization pipeline: Continual Pre-Training (CPT) on TC-centric corpora, Supervised Fine-Tuning (SFT) with instruction data, and Direct Preference Optimization (DPO) using TC-adherence preferences with parameter-efficient LoRA adapters.

Result: 51.3% relative reduction in non-TC output tokens versus base model; 77.2% reduction in incorrect-language tokens relative to Llama-3B and 57.2% relative to Qwen-1.5B on Named Entity Translation task.

Conclusion: Robust Traditional Chinese adherence is attainable at 1B scale with reproducible, adapter-only, hardware-friendly pipeline that enhances language stability for TC and potentially other non-English languages.

Abstract: Small Language Models (SLMs) enable cost-effective, on-device and
latency-sensitive AI applications, yet their deployment in Traditional Chinese
(TC) remains hindered by token-level instability - models unpredictably emit
non-TC characters or code-switch into other languages. We address this
practical reliability gap by creating PureTC-1B, a three-stage stabilization
pipeline for Llama-3.2-1B-Instruct (an open-weight, instruction-tuned model
released by Meta) using parameter-efficient LoRA adapters. Our method combines
Continual Pre-Training (CPT) on TC-centric corpora, Supervised Fine-Tuning
(SFT) with instruction data, and Direct Preference Optimization (DPO) using
TC-adherence preferences to improve monolingual robustness without full-model
retraining. On a benchmark designed to simulate real-world usage, PureTC-1B
achieves a 51.3% relative reduction (micro-average) in non-TC output tokens
versus the base model. On a Named Entity Translation (NET) task, PureTC-1B
further reduces incorrect-language tokens by 77.2% relative to Llama-3B and
57.2% relative to Qwen-1.5B, indicating that robust TC adherence is attainable
even at the 1B scale. The pipeline is reproducible, adapter-only, and
hardware-friendly, offering practitioners a practical recipe to enhance
language stability for TC and potentially other non-English languages.

</details>


### [51] [AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System](https://arxiv.org/abs/2510.01617)
*Hui Yi Leong,Yuheng Li,Yuqing Wu,Wenwen Ouyang,Wei Zhu,Jiechao Gao*

Main category: cs.CL

TL;DR: AMAS is a dynamic multi-agent system framework that uses lightweight LLM adaptation to autonomously design task-specific optimal graph configurations, outperforming traditional fixed-topology MAS and single-agent approaches across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional multi-agent systems (MAS) using LLMs are limited by inflexible, hand-crafted graph topologies that lack contextual responsiveness, reducing their effectiveness across diverse academic and commercial workloads.

Method: AMAS introduces a dynamic graph designer that autonomously identifies task-specific optimal graph configurations via lightweight LLM adaptation, eliminating reliance on monolithic structural templates and using intrinsic input properties to direct query trajectories through task-optimized agent pathways.

Result: Rigorous validation across question answering, mathematical deduction, and code generation benchmarks confirms that AMAS systematically exceeds state-of-the-art single-agent and multi-agent approaches across diverse LLM architectures.

Conclusion: Context-sensitive structural adaptability constitutes a foundational requirement for high-performance LLM MAS deployments, and AMAS demonstrates this through its dynamic graph design approach.

Abstract: Although large language models (LLMs) have revolutionized natural language
processing capabilities, their practical implementation as autonomous
multi-agent systems (MAS) for industrial problem-solving encounters persistent
barriers. Conventional MAS architectures are fundamentally restricted by
inflexible, hand-crafted graph topologies that lack contextual responsiveness,
resulting in diminished efficacy across varied academic and commercial
workloads. To surmount these constraints, we introduce AMAS, a
paradigm-shifting framework that redefines LLM-based MAS through a novel
dynamic graph designer. This component autonomously identifies task-specific
optimal graph configurations via lightweight LLM adaptation, eliminating the
reliance on monolithic, universally applied structural templates. Instead, AMAS
exploits the intrinsic properties of individual inputs to intelligently direct
query trajectories through task-optimized agent pathways. Rigorous validation
across question answering, mathematical deduction, and code generation
benchmarks confirms that AMAS systematically exceeds state-of-the-art
single-agent and multi-agent approaches across diverse LLM architectures. Our
investigation establishes that context-sensitive structural adaptability
constitutes a foundational requirement for high-performance LLM MAS
deployments.

</details>


### [52] [NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT](https://arxiv.org/abs/2510.01644)
*John Hawkins,Aditya Pramar,Rodney Beard,Rohitash Chandra*

Main category: cs.CL

TL;DR: This paper analyzes machine learning models' ability to detect jailbreak prompts in LLMs, finding that fine-tuned BERT models perform best at identifying both known and novel jailbreak strategies.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to jailbreak prompts that bypass safety guardrails, creating a need for effective detection methods to prevent malicious manipulation.

Method: Evaluated various ML models' performance in distinguishing jailbreak prompts from genuine uses, with focus on detecting previously unseen jailbreak strategies. Used fine-tuned BERT models for end-to-end jailbreak identification.

Result: Fine-tuned BERT models achieved the best performance in identifying jailbreak prompts using current datasets. Analysis revealed that explicit reflexivity in prompt structure serves as a signal of jailbreak intention.

Conclusion: End-to-end fine-tuned BERT models are most effective for jailbreak detection, and prompt reflexivity can be a key indicator of malicious intent, providing insights for developing better safety mechanisms.

Abstract: Large Language Models (LLMs) suffer from a range of vulnerabilities that
allow malicious users to solicit undesirable responses through manipulation of
the input text. These so-called jailbreak prompts are designed to trick the LLM
into circumventing the safety guardrails put in place to keep responses
acceptable to the developer's policies. In this study, we analyse the ability
of different machine learning models to distinguish jailbreak prompts from
genuine uses, including looking at our ability to identify jailbreaks that use
previously unseen strategies. Our results indicate that using current datasets
the best performance is achieved by fine tuning a Bidirectional Encoder
Representations from Transformers (BERT) model end-to-end for identifying
jailbreaks. We visualise the keywords that distinguish jailbreak from genuine
prompts and conclude that explicit reflexivity in prompt structure could be a
signal of jailbreak intention.

</details>


### [53] [Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention](https://arxiv.org/abs/2510.01652)
*Zhaoxin Feng,Jianfei Ma,Emmanuele Chersoni,Xiaojing Zhao,Xiaoyi Bao*

Main category: cs.CL

TL;DR: This paper explores overcoming unidirectional attention limitations in LLMs for text embedding tasks by enabling bidirectional attention through additional training steps on Llama architecture variants.


<details>
  <summary>Details</summary>
Motivation: Autoregressive LLMs have strong language capabilities but face constraints in text embedding tasks due to unidirectional attention, limiting their semantic representation analysis in probing tasks.

Method: Tested different Llama architecture variants through additional training steps that progressively enable bidirectional attention and unsupervised/supervised contrastive learning.

Result: The paper investigates whether bidirectional attention can overcome the constraints of unidirectional attention in LLMs for text embedding applications.

Conclusion: The research aims to determine if enabling bidirectional attention in LLMs can improve their performance in text embedding tasks and semantic representation analysis.

Abstract: Autoregressive Large Language Models (LLMs) demonstrate exceptional
performance in language understanding and generation. However, their
application in text embedding tasks has been relatively slow, along with the
analysis of their semantic representation in probing tasks, due to the
constraints of the unidirectional attention mechanism.
  This paper aims to explore whether such constraints can be overcome by
enabling bidirectional attention in LLMs. We tested different variants of the
Llama architecture through additional training steps, progressively enabling
bidirectional attention and unsupervised/supervised contrastive learning.

</details>


### [54] [SoK: Measuring What Matters for Closed-Loop Security Agents](https://arxiv.org/abs/2510.01654)
*Mudita Khurana,Raunak Jain*

Main category: cs.CL

TL;DR: CLASP framework defines agentic capabilities for closed-loop security systems and introduces CLC Score to measure performance.


<details>
  <summary>Details</summary>
Motivation: Current cybersecurity defenses are fragmented and can't keep up with AI-driven attacks. There's a need for autonomous agents that can handle the full security lifecycle in a closed loop, but no framework exists to define or evaluate such systems.

Method: Developed CLASP framework that aligns security lifecycle (reconnaissance, exploitation, root cause analysis, patch synthesis, validation) with agentic capabilities (planning, tool use, memory, reasoning, reflection & perception). Applied to 21 representative works and created CLC Score metric.

Result: Successfully mapped existing systems' strengths and gaps using CLASP. Defined a composite metric (CLC Score) that quantifies both loop closure degree and operational effectiveness.

Conclusion: CLASP and CLC Score provide the necessary vocabulary, diagnostics, and measurements to advance closed-loop security agents and improve function-level performance in cybersecurity.

Abstract: Cybersecurity is a relentless arms race, with AI driven offensive systems
evolving faster than traditional defenses can adapt. Research and tooling
remain fragmented across isolated defensive functions, creating blind spots
that adversaries exploit. Autonomous agents capable of integrating, exploit
confirmation, remediation, and validation into a single closed loop offer
promise, but the field lacks three essentials: a framework defining the agentic
capabilities of security systems across security life cycle, a principled
method for evaluating closed loop agents, and a benchmark for measuring their
performance in practice. We introduce CLASP: the Closed-Loop Autonomous
Security Performance framework which aligns the security lifecycle
(reconnaissance, exploitation, root cause analysis, patch synthesis,
validation) with core agentic capabilities (planning, tool use, memory,
reasoning, reflection & perception) providing a common vocabulary and rubric
for assessing agentic capabilities in security tasks. By applying CLASP to 21
representative works, we map where systems demonstrate strengths, and where
capability gaps persist. We then define the Closed-Loop Capability (CLC) Score,
a composite metric quantifying both degree of loop closure and operational
effectiveness, and outline the requirements for a closed loop benchmark.
Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, and
measurements needed to advance both function level performance and measure
closed loop security agents.

</details>


### [55] [MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization](https://arxiv.org/abs/2510.01659)
*Yinhong Liu,Jianfeng He,Hang Su,Ruixue Lian,Yi Nian,Jake Vincent,Srikanth Vishnubhotla,Robinson Piramuthu,Saab Mansour*

Main category: cs.CL

TL;DR: MDSEval is the first meta-evaluation benchmark for Multimodal Dialogue Summarization (MDS), featuring image-sharing dialogues, summaries, and human judgments across eight quality aspects, with a novel MEKI-based filtering framework to ensure data quality.


<details>
  <summary>Details</summary>
Motivation: To support the development of effective MDS models by providing robust automatic evaluation methods that reduce cost and human effort, requiring a strong meta-evaluation benchmark grounded in human annotations.

Method: Introduce MDSEval benchmark with image-sharing dialogues, summaries, and human judgments; propose a novel filtering framework using Mutually Exclusive Key Information (MEKI) across modalities to ensure data quality and richness.

Result: MDSEval is the first to identify and formalize key evaluation dimensions specific to MDS; benchmarking reveals limitations of state-of-the-art evaluation methods in distinguishing summaries from advanced MLLMs and their susceptibility to various biases.

Conclusion: MDSEval provides a crucial foundation for developing more reliable automatic evaluation methods for MDS, addressing current limitations in distinguishing high-quality summaries and mitigating evaluation biases.

Abstract: Multimodal Dialogue Summarization (MDS) is a critical task with wide-ranging
applications. To support the development of effective MDS models, robust
automatic evaluation methods are essential for reducing both cost and human
effort. However, such methods require a strong meta-evaluation benchmark
grounded in human annotations. In this work, we introduce MDSEval, the first
meta-evaluation benchmark for MDS, consisting image-sharing dialogues,
corresponding summaries, and human judgments across eight well-defined quality
aspects. To ensure data quality and richfulness, we propose a novel filtering
framework leveraging Mutually Exclusive Key Information (MEKI) across
modalities. Our work is the first to identify and formalize key evaluation
dimensions specific to MDS. We benchmark state-of-the-art modal evaluation
methods, revealing their limitations in distinguishing summaries from advanced
MLLMs and their susceptibility to various bias.

</details>


### [56] [FOR-Prompting: From Objection to Revision via an Asymmetric Prompting Protocol](https://arxiv.org/abs/2510.01674)
*He Zhang,Anzhou Zhang,Jian Dai*

Main category: cs.CL

TL;DR: FOR-Prompting is a reasoning protocol that uses role-based dialogue (Defender, Objectioner, Host) to elicit self-revision through questioning, achieving significant accuracy gains over single-prompt methods and competitive performance with CoT.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning protocols like Chain of Thought and Tree of Thought organize internal deliberation but lack explicit mechanisms for external questioning that can drive self-revision and error correction.

Method: Asymmetric protocol with three roles: Defender proposes answers, Objectioner raises question-style objections without direct fixes, and Host enforces consistency and closure. Operates purely at prompt level through role-structured turns.

Result: 22% point gain over single-prompt on GSM8K, accuracy on par with CoT, 10% higher ratings in reasoning and coherence. Corrects mistakes without tools/human supervision. Improves small model performance (19% accuracy gain on Llama3.2:1b).

Conclusion: FOR-Prompting enables effective self-revision through structured questioning, works across model sizes without retraining, and shows promise for small models and personal device applications while making reasoning assumptions explicit.

Abstract: Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT)
organize internal deliberation but lack an explicit mechanism for external
questioning that elicits self-revision. We present FOR-Prompting (From
Objection to Revision Prompting), an asymmetric protocol where a Defender
proposes an answer, an Objectioner raises question-style objections with no
direct fixes, and a Host enforces consistency and closure. On GSM8K we observe
about a 22% point gain over single-prompt and accuracy on par with CoT, with
more than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1
judge. FOR-Prompting also corrects mistakes without tools or human supervision
on tricky queries, and improves performance for small-scale model (approx. 19%
accuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for
small models and on personal device use. Beyond factual QA, qualitative
analyses on open-ended tasks show enhanced exploration and refinement, with
dialogue traces that make assumptions and trade-offs explicit. The protocol is
model agnostic and operates purely at the prompt level through role-structured
turns, so it works with hosted and local models of different sizes without
retraining, and it supports large-scale study of objection-guided reasoning.

</details>


### [57] [How Do Language Models Compose Functions?](https://arxiv.org/abs/2510.01685)
*Apoorv Khandelwal,Ellie Pavlick*

Main category: cs.CL

TL;DR: LLMs have a compositionality gap - being able to compute f(x) and g(z) doesn't guarantee they can compute g(f(x)). The paper identifies two processing mechanisms: compositional (computing f(x) first) and direct (no detectable intermediate f(x)), with the choice influenced by embedding space geometry.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs use compositional mechanisms when solving compositional tasks, specifically examining the compositionality gap where models can compute individual functions but struggle with their composition.

Method: Used logit lens on residual stream activations to analyze how feedforward LLMs solve two-hop factual recall tasks (g(f(x))), identifying different processing mechanisms through activation patterns.

Result: Found two distinct mechanisms: compositional (computes f(x) as intermediate step) and direct (no detectable f(x) intermediate). The mechanism choice correlates with embedding space geometry - direct mechanism dominates when linear mapping exists from x to g(f(x)).

Conclusion: LLMs don't consistently use compositional reasoning even when capable of component functions. The processing mechanism depends on embedding space properties, revealing limitations in how models handle function composition.

Abstract: While large language models (LLMs) appear to be increasingly capable of
solving compositional tasks, it is an open question whether they do so using
compositional mechanisms. In this work, we investigate how feedforward LLMs
solve two-hop factual recall tasks, which can be expressed compositionally as
$g(f(x))$. We first confirm that modern LLMs continue to suffer from the
"compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y =
g(z)$ does not entail their ability to compute the composition $y = g(f(x))$.
Then, using logit lens on their residual stream activations, we identify two
processing mechanisms, one which solves tasks $\textit{compositionally}$,
computing $f(x)$ along the way to computing $g(f(x))$, and one which solves
them $\textit{directly}$, without any detectable signature of the intermediate
variable $f(x)$. Finally, we find that which mechanism is employed appears to
be related to the embedding space geometry, with the idiomatic mechanism being
dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in
the embedding spaces. We fully release our data and code at:
https://github.com/apoorvkh/composing-functions .

</details>


### [58] [Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation](https://arxiv.org/abs/2510.01688)
*Seungseop Lim,Gibaeg Kim,Wooseok Han,Jean Seo,Hyunkyung Lee,Jaehyo Yoo,Eunho Yang*

Main category: cs.CL

TL;DR: LLMs in medical pre-consultation suffer from Format Inertia - generating repetitive but diagnostically uninformative questions due to skewed turn-count distributions in training data. A simple data rebalancing method effectively mitigates this issue.


<details>
  <summary>Details</summary>
Motivation: Address the novel failure mechanism called Format Inertia in LLMs for medical pre-consultation, where models generate format-correct but diagnostically uninformative repetitive questions due to skewed turn-count distributions in training datasets.

Method: Adopt a simple, data-centric method that rebalances the turn-count distribution of the training dataset to address the skewed distribution issue.

Result: Experimental results show that the approach substantially alleviates Format Inertia in medical pre-consultation, reducing the generation of repetitive but diagnostically uninformative questions.

Conclusion: Data rebalancing is an effective solution to mitigate Format Inertia in LLMs for medical dialogue generation, improving the quality of diagnostic questioning in long medical conversations.

Abstract: Recent advances in Large Language Models (LLMs) have brought significant
improvements to various service domains, including chatbots and medical
pre-consultation applications. In the healthcare domain, the most common
approach for adapting LLMs to multi-turn dialogue generation is Supervised
Fine-Tuning (SFT). However, datasets for SFT in tasks like medical
pre-consultation typically exhibit a skewed turn-count distribution. Training
on such data induces a novel failure mechanism we term **Format Inertia**,
where models tend to generate repetitive, format-correct, but diagnostically
uninformative questions in long medical dialogues. To mitigate this observed
failure mechanism, we adopt a simple, data-centric method that rebalances the
turn-count distribution of the training dataset. Experimental results show that
our approach substantially alleviates Format Inertia in medical
pre-consultation.

</details>


### [59] [What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?](https://arxiv.org/abs/2510.01719)
*Jiwan Chung,Neel Joshi,Pratyusha Sharma,Youngjae Yu,Vibhav Vineet*

Main category: cs.CL

TL;DR: MathLens is a benchmark that disentangles multimodal reasoning into perception, reasoning, and integration subskills for geometry problems, revealing that different training methods affect these components unevenly.


<details>
  <summary>Details</summary>
Motivation: To move beyond aggregate accuracy and understand where and how multimodal reasoning models improve by analyzing their subskills separately.

Method: Created MathLens benchmark with annotated components: visual diagrams, textual descriptions, controlled multimodal questions, and perceptual probes derived from symbolic problem specifications.

Result: RL mainly strengthens perception, especially with text supervision; reasoning improves only with perception; integration remains weakest; RL improves robustness while multimodal SFT reduces it.

Conclusion: Different training approaches have distinct effects on subskills, with integration being the bottleneck and robustness trade-offs between training methods.

Abstract: Multimodal reasoning models have recently shown promise on challenging
domains such as olympiad-level geometry, yet their evaluation remains dominated
by aggregate accuracy, a single score that obscures where and how models are
improving. We introduce MathLens, a benchmark designed to disentangle the
subskills of multimodal reasoning while preserving the complexity of
textbook-style geometry problems. The benchmark separates performance into
three components: Perception: extracting information from raw inputs,
Reasoning: operating on available information, and Integration: selecting
relevant perceptual evidence and applying it within reasoning. To support each
test, we provide annotations: visual diagrams, textual descriptions to evaluate
reasoning in isolation, controlled questions that require both modalities, and
probes for fine-grained perceptual skills, all derived from symbolic
specifications of the problems to ensure consistency and robustness. Our
analysis reveals that different training approaches have uneven effects: First,
reinforcement learning chiefly strengthens perception, especially when
supported by textual supervision, while textual SFT indirectly improves
perception through reflective reasoning. Second, reasoning improves only in
tandem with perception. Third, integration remains the weakest capacity, with
residual errors concentrated there once other skills advance. Finally,
robustness diverges: RL improves consistency under diagram variation, whereas
multimodal SFT reduces it through overfitting. We will release all data and
experimental logs.

</details>


### [60] [Machine-interpretable Engineering Design Standards for Valve Specification](https://arxiv.org/abs/2510.01736)
*Anders Gjerver,Rune Frostad,Vedrana Barisic,Melinda Hodkiewicz,Caitlin Woods,Mihaly Fekete,Arild Braathen Torjusen,Johan Wilhelm Kluwer*

Main category: cs.CL

TL;DR: This paper demonstrates how to transform engineering design standards into machine-interpretable ontologies for automated quality assurance in plant design and equipment selection processes.


<details>
  <summary>Details</summary>
Motivation: Current engineering design processes rely on document-centric standards despite digitalization ambitions, creating inefficiencies in compliance checking and equipment validation.

Method: Used modeling patterns to create modular, interoperable ontologies from international piping, material and valve standards, aligned with ISO DIS 23726-3 Industrial Data Ontology (IDO). Tested on valve selection by creating semantic asset models and functional location tags.

Result: Successfully enabled automated validation of valve data sheet compliance with standards and determination of product type suitability using semantic reasoning and executable design rules.

Conclusion: IDO-based modular ontologies enable semantic reasoning for equipment selection and demonstrate potential for Standards Bodies to transition to digitized Smart Standards.

Abstract: Engineering design processes use technical specifications and must comply
with standards. Product specifications, product type data sheets, and design
standards are still mainly document-centric despite the ambition to digitalize
industrial work. In this paper, we demonstrate how to transform information
held in engineering design standards into modular, reusable,
machine-interpretable ontologies and use the ontologies in quality assurance of
the plant design and equipment selection process. We use modelling patterns to
create modular ontologies for knowledge captured in the text and in frequently
referenced tables in International Standards for piping, material and valve
design. These modules are exchangeable, as stored in a W3C compliant format,
and interoperable as they are aligned with the top-level ontology ISO DIS
23726-3: Industrial Data Ontology (IDO).
  We test these ontologies, created based on international material and piping
standards and industry norms, on a valve selection process. Valves are
instantiated in semantic asset models as individuals along with a semantic
representation of the environmental condition at their location on the asset.
We create "functional location tags" as OWL individuals that become instances
of OWL class Valve Data Sheet (VDS) specified valves. Similarly we create
instances of manufacturer product type. Our approach enables automated
validation that a specific VDS is compliant with relevant industry standards.
Using semantic reasoning and executable design rules, we also determine whether
the product type meets the valve specification. Creation of shared, reusable
IDO-based modular ontologies for design standards enables semantic reasoning to
be applied to equipment selection processes and demonstrates the potential of
this approach for Standards Bodies wanting to transition to digitized Smart
Standards.

</details>


### [61] [Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks](https://arxiv.org/abs/2510.01782)
*Wenbo Pan,Jie Xu,Qiguang Chen,Junhao Dong,Libo Qin,Xinfeng Li,Haining Yu,Xiaohua Jia*

Main category: cs.CL

TL;DR: The paper proposes Refusal Index (RI), a new metric to measure how accurately LLMs refuse questions beyond their knowledge, addressing limitations of existing refusal and calibration metrics.


<details>
  <summary>Details</summary>
Motivation: Existing metrics fail to faithfully measure LLMs' knowledge-aware refusal capability - simple refusal metrics are biased by refusal rates, while calibration metrics are proxy-based and don't capture actual refusal behavior.

Method: Define RI as Spearman's rank correlation between refusal probability and error probability. Design a lightweight two-pass evaluation method to efficiently estimate RI from observed refusal rates across two standard evaluation runs.

Result: Experiments across 16 models and 5 datasets show RI accurately quantifies intrinsic knowledge-aware refusal capability. RI remains stable across different refusal rates and provides consistent model rankings independent of overall accuracy and refusal rates.

Conclusion: LLMs' refusal behavior can be unreliable despite high accuracy, highlighting the need to complement traditional accuracy metrics with Refusal Index for comprehensive factuality evaluation.

Abstract: Large Language Models (LLMs) should refuse to answer questions beyond their
knowledge. This capability, which we term knowledge-aware refusal, is crucial
for factual reliability. However, existing metrics fail to faithfully measure
this ability. On the one hand, simple refusal-based metrics are biased by
refusal rates and yield inconsistent scores when models exhibit different
refusal tendencies. On the other hand, existing calibration metrics are
proxy-based, capturing the performance of auxiliary calibration processes
rather than the model's actual refusal behavior. In this work, we propose the
Refusal Index (RI), a principled metric that measures how accurately LLMs
refuse questions they do not know. We define RI as Spearman's rank correlation
between refusal probability and error probability. To make RI practically
measurable, we design a lightweight two-pass evaluation method that efficiently
estimates RI from observed refusal rates across two standard evaluation runs.
Extensive experiments across 16 models and 5 datasets demonstrate that RI
accurately quantifies a model's intrinsic knowledge-aware refusal capability in
factual tasks. Notably, RI remains stable across different refusal rates and
provides consistent model rankings independent of a model's overall accuracy
and refusal rates. More importantly, RI provides insight into an important but
previously overlooked aspect of LLM factuality: while LLMs achieve high
accuracy on factual tasks, their refusal behavior can be unreliable and
fragile. This finding highlights the need to complement traditional accuracy
metrics with the Refusal Index for comprehensive factuality evaluation.

</details>


### [62] [Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction](https://arxiv.org/abs/2510.01792)
*Ivan Leonidovich Litvak,Anton Kostin,Fedor Lashkin,Tatiana Maksiyan,Sergey Lagutin*

Main category: cs.CL

TL;DR: This study evaluates 16 unsupervised metrics for assessing text extraction quality from Russian judicial decisions, finding that Term Frequency Coherence and Coverage Ratio perform best in aligning with expert ratings, while LLM-based evaluation shows moderate performance.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of AI in legal NLP demands scalable methods for evaluating text extraction from judicial decisions without requiring pre-annotated ground truth.

Method: Evaluated 16 unsupervised metrics across five categories (document-based, semantic, structural, pseudo-ground truth, legal-specific) on 1,000 Russian judicial decisions, validated against 7,168 expert reviews using bootstrapped correlations, Lin's CCC, and MAE.

Result: Term Frequency Coherence (Pearson r=0.540, Lin CCC=0.512, MAE=0.127) and Coverage Ratio/Block Completeness (Pearson r=0.513, Lin CCC=0.443, MAE=0.139) best aligned with expert ratings, while Legal Term Density showed strong negative correlations. LLM Evaluation Score showed moderate alignment (Pearson r=0.382, Lin CCC=0.325, MAE=0.197).

Conclusion: Unsupervised metrics enable scalable screening but cannot fully replace human judgment in high-stakes legal contexts due to moderate correlations and low CCC values, advancing legal NLP with annotation-free evaluation tools.

Abstract: The rapid advancement of artificial intelligence in legal natural language
processing demands scalable methods for evaluating text extraction from
judicial decisions. This study evaluates 16 unsupervised metrics, including
novel formulations, to assess the quality of extracting seven semantic blocks
from 1,000 anonymized Russian judicial decisions, validated against 7,168
expert reviews on a 1--5 Likert scale. These metrics, spanning document-based,
semantic, structural, pseudo-ground truth, and legal-specific categories,
operate without pre-annotated ground truth. Bootstrapped correlations, Lin's
concordance correlation coefficient (CCC), and mean absolute error (MAE) reveal
that Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE =
0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC =
0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density
(Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative
correlations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin
CCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using
gpt-4.1-mini via g4f, suggests limited specialization for legal textse. These
findings highlight that unsupervised metrics, including LLM-based approaches,
enable scalable screening but, with moderate correlations and low CCC values,
cannot fully replace human judgment in high-stakes legal contexts. This work
advances legal NLP by providing annotation-free evaluation tools, with
implications for judicial analytics and ethical AI deployment.

</details>


### [63] [Detecting LLM-Generated Spam Reviews by Integrating Language Model Embeddings and Graph Neural Network](https://arxiv.org/abs/2510.01801)
*Xin Liu,Rongwu Xu,Xinyi Jia,Jason Liao,Jiao Sun,Ling Huang,Wei Xu*

Main category: cs.CL

TL;DR: This paper addresses the threat of LLM-generated spam reviews by creating synthetic datasets and proposing FraudSquad, a hybrid detection model that combines text embeddings with graph transformers, achieving significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs enables generation of highly persuasive spam reviews that mimic human writing, posing challenges to existing detection systems and threatening online platform credibility.

Method: Created three realistic LLM-generated spam review datasets using different LLMs guided by product metadata and genuine reviews. Proposed FraudSquad - a hybrid model integrating text embeddings from pre-trained language models with gated graph transformers for spam node classification.

Result: FraudSquad outperforms state-of-the-art baselines by up to 44.22% in precision and 43.01% in recall on LLM-generated datasets, while also achieving promising results on human-written spam datasets. Maintains modest model size and requires minimal labeled training data.

Conclusion: The work provides new synthetic datasets, a practical detection framework, and empirical evidence highlighting the urgency of adapting spam detection to the LLM era. FraudSquad offers a practical solution for real-world applications.

Abstract: The rise of large language models (LLMs) has enabled the generation of highly
persuasive spam reviews that closely mimic human writing. These reviews pose
significant challenges for existing detection systems and threaten the
credibility of online platforms. In this work, we first create three realistic
LLM-generated spam review datasets using three distinct LLMs, each guided by
product metadata and genuine reference reviews. Evaluations by GPT-4.1 confirm
the high persuasion and deceptive potential of these reviews. To address this
threat, we propose FraudSquad, a hybrid detection model that integrates text
embeddings from a pre-trained language model with a gated graph transformer for
spam node classification. FraudSquad captures both semantic and behavioral
signals without relying on manual feature engineering or massive training
resources. Experiments show that FraudSquad outperforms state-of-the-art
baselines by up to 44.22% in precision and 43.01% in recall on three
LLM-generated datasets, while also achieving promising results on two
human-written spam datasets. Furthermore, FraudSquad maintains a modest model
size and requires minimal labeled training data, making it a practical solution
for real-world applications. Our contributions include new synthetic datasets,
a practical detection framework, and empirical evidence highlighting the
urgency of adapting spam detection to the LLM era. Our code and datasets are
available at: https://anonymous.4open.science/r/FraudSquad-5389/.

</details>


### [64] [Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical Errors](https://arxiv.org/abs/2510.01831)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CL

TL;DR: LLMs have systematic syntactic blind spots where they misapply reasoning to semantically simple but syntactically unfamiliar problems, not due to mathematical incompetence but brittle form-representation coupling.


<details>
  <summary>Details</summary>
Motivation: To understand why LLMs fail on mathematically simple problems when phrased in unfamiliar syntactic structures, identifying that these failures stem from structural misalignment rather than conceptual difficulty.

Method: Rephrased incorrectly answered questions using syntactic templates from correct examples, quantified syntactic complexity using Dependency Locality Theory (DLT) metric, and tested across multiple datasets.

Result: Rephrasings that preserve semantics but reduce structural complexity often lead to correct answers, and higher DLT scores correlate with increased failure rates, showing syntax-aware interventions can mitigate inductive failures.

Conclusion: Many reasoning errors in LLMs originate from structural misalignment rather than conceptual difficulty, and syntax-aware approaches can effectively reveal and address these systematic failure modes.

Abstract: Large Language Models (LLMs) demonstrate strong mathematical problem-solving
abilities but frequently fail on problems that deviate syntactically from their
training distribution. We identify a systematic failure mode, syntactic blind
spots, in which models misapply familiar reasoning strategies to problems that
are semantically straightforward but phrased in unfamiliar ways. These errors
are not due to gaps in mathematical competence, but rather reflect a brittle
coupling between surface form and internal representation. To test this, we
rephrase incorrectly answered questions using syntactic templates drawn from
correct examples. These rephrasings, which preserve semantics while reducing
structural complexity, often lead to correct answers. We quantify syntactic
complexity using a metric based on Dependency Locality Theory (DLT), and show
that higher DLT scores are associated with increased failure rates across
multiple datasets. Our findings suggest that many reasoning errors stem from
structural misalignment rather than conceptual difficulty, and that
syntax-aware interventions can reveal and mitigate these inductive failures.

</details>


### [65] [SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with Reinforcement Learning](https://arxiv.org/abs/2510.01832)
*Shicheng Liu,Kai Sun,Lisheng Fu,Xilun Chen,Xinyuan Zhang,Zhaojiang Lin,Rulin Shao,Yue Liu,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: SCRIBES is a reinforcement learning framework that generates reusable extraction scripts for semi-structured web content by leveraging layout similarity across webpages, outperforming baselines by 13% in script quality and improving QA accuracy by 4% for GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Semi-structured content in HTML tables, lists, and infoboxes is abundant on the web but challenging to extract reliably. Existing methods either lack generalization or are resource-intensive due to per-page LLM inference.

Method: Uses reinforcement learning with layout similarity across webpages within the same site as reward signal. Generates reusable extraction scripts for structurally similar webpages and iteratively trains on synthetic annotations from CommonCrawl data.

Result: Outperforms strong baselines by over 13% in script quality and boosts downstream question answering accuracy by more than 4% for GPT-4o.

Conclusion: SCRIBES enables scalable and resource-efficient web information extraction by generating reusable scripts that can be applied to groups of structurally similar webpages.

Abstract: Semi-structured content in HTML tables, lists, and infoboxes accounts for a
substantial share of factual data on the web, yet the formatting complicates
usage, and reliably extracting structured information from them remains
challenging. Existing methods either lack generalization or are
resource-intensive due to per-page LLM inference. In this paper, we introduce
SCRIBES (SCRIpt-Based Semi-Structured Content Extraction at Web-Scale), a novel
reinforcement learning framework that leverages layout similarity across
webpages within the same site as a reward signal. Instead of processing each
page individually, SCRIBES generates reusable extraction scripts that can be
applied to groups of structurally similar webpages. Our approach further
improves by iteratively training on synthetic annotations from in-the-wild
CommonCrawl data. Experiments show that our approach outperforms strong
baselines by over 13% in script quality and boosts downstream question
answering accuracy by more than 4% for GPT-4o, enabling scalable and
resource-efficient web information extraction.

</details>


### [66] [Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models](https://arxiv.org/abs/2510.01845)
*Ece Takmaz,Lisa Bylinina,Jakub Dotlacil*

Main category: cs.CL

TL;DR: This paper addresses the BabyLM challenge by developing multimodal models in low-resource settings, finding that multimodal models underperform in language-only tasks, and using model merging with text-only models to improve language abilities while maintaining multimodal performance.


<details>
  <summary>Details</summary>
Motivation: Address the discrepancy between large-scale vision-language models requiring massive data and children's language acquisition with limited exposure, focusing on maintaining language-only abilities in multimodal models.

Method: Develop language-only and multimodal models using developmentally plausible datasets in low-resource settings, and experiment with model merging using weighted linear interpolation to fuse multimodal and language-only model parameters.

Result: Multimodal models outperform previous BabyLM baselines but underperform in language-only grammar benchmarks. Model merging with text-only models helps alleviate this problem to some extent while maintaining multimodal performance.

Conclusion: Model merging with text-only models can partially address the language-only performance gap in multimodal models while preserving their multimodal capabilities, corroborating findings about multimodal models' limitations in grammar-focused language tasks.

Abstract: State-of-the-art vision-and-language models consist of many parameters and
learn from enormous datasets, surpassing the amounts of linguistic data that
children are exposed to as they acquire a language. This paper presents our
approach to the multimodal track of the BabyLM challenge addressing this
discrepancy. We develop language-only and multimodal models in low-resource
settings using developmentally plausible datasets, with our multimodal models
outperforming previous BabyLM baselines. One finding in the multimodal language
model literature is that these models tend to underperform in
\textit{language-only} tasks. Therefore, we focus on maintaining language-only
abilities in multimodal models. To this end, we experiment with \textit{model
merging}, where we fuse the parameters of multimodal models with those of
language-only models using weighted linear interpolation. Our results
corroborate the findings that multimodal models underperform in language-only
benchmarks that focus on grammar, and model merging with text-only models can
help alleviate this problem to some extent, while maintaining multimodal
performance.

</details>


### [67] [REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration](https://arxiv.org/abs/2510.01879)
*Yisu Wang,Ming Wang,Haoyuan Song,Wenjie Huang,Chaozheng Wang,Yi Xie,Xuming Ran*

Main category: cs.CL

TL;DR: REPAIR is a lifelong editing framework for LLMs that enables precise, low-cost model updates while preserving non-target knowledge through closed-loop feedback and dynamic memory management.


<details>
  <summary>Details</summary>
Motivation: Address the high cost and unintended side effects of traditional post-training methods for LLMs, which make knowledge acquisition and error correction difficult.

Method: Uses closed-loop feedback mechanism, dynamic memory management, frequent knowledge fusion, and strong locality guards to mitigate instability and conflicts from sequential edits.

Result: Boosts editing accuracy by 10%-30% across multiple model families and significantly reduces knowledge forgetting.

Conclusion: REPAIR provides a robust framework for developing reliable, scalable, and continually evolving LLMs through precise and low-cost model updates.

Abstract: Post-training for large language models (LLMs) is constrained by the high
cost of acquiring new knowledge or correcting errors and by the unintended side
effects that frequently arise from retraining. To address these issues, we
introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and
Reintegration), a lifelong editing framework designed to support precise and
low-cost model updates while preserving non-target knowledge. REPAIR mitigates
the instability and conflicts of large-scale sequential edits through a
closed-loop feedback mechanism coupled with dynamic memory management.
Furthermore, by incorporating frequent knowledge fusion and enforcing strong
locality guards, REPAIR effectively addresses the shortcomings of traditional
distribution-agnostic approaches that often overlook unintended ripple effects.
Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%
across multiple model families and significantly reduces knowledge forgetting.
This work introduces a robust framework for developing reliable, scalable, and
continually evolving LLMs.

</details>


### [68] [Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey](https://arxiv.org/abs/2510.01925)
*Qiyuan Liu,Hao Xu,Xuhong Chen,Wei Chen,Yee Whye Teh,Ning Miao*

Main category: cs.CL

TL;DR: This paper provides a systematic introduction and comprehensive survey of reward models (RMs) for enhancing LLM reasoning, covering their architectures, training methods, applications in inference guidance, data synthesis, and RL fine-tuning, along with open research questions.


<details>
  <summary>Details</summary>
Motivation: Reward models play a critical role in improving LLM reasoning performance by providing training signals for RL fine-tuning and helping select optimal answers during inference, but there lacks a systematic overview of their applications and challenges.

Method: The authors conduct a systematic review of RM fundamentals (architectures, training methodologies, evaluation techniques) and explore their key applications in LLM inference guidance, data synthesis, self-improvement, and RL-based fine-tuning.

Result: The paper provides comprehensive insights into RM applications and identifies critical open questions regarding RM selection, generalization, evaluation, and enhancement based on existing research and empirical findings.

Conclusion: This analysis offers actionable insights for effective deployment and advancement of reward models in LLM reasoning, addressing key challenges and providing guidance for future research directions.

Abstract: Reward models (RMs) play a critical role in enhancing the reasoning
performance of LLMs. For example, they can provide training signals to finetune
LLMs during reinforcement learning (RL) and help select the best answer from
multiple candidates during inference. In this paper, we provide a systematic
introduction to RMs, along with a comprehensive survey of their applications in
LLM reasoning. We first review fundamental concepts of RMs, including their
architectures, training methodologies, and evaluation techniques. Then, we
explore their key applications: (1) guiding generation and selecting optimal
outputs during LLM inference, (2) facilitating data synthesis and iterative
self-improvement for LLMs, and (3) providing training signals in RL-based
finetuning. Finally, we address critical open questions regarding the
selection, generalization, evaluation, and enhancement of RMs, based on
existing research and our own empirical findings. Our analysis aims to provide
actionable insights for the effective deployment and advancement of RMs for LLM
reasoning.

</details>


### [69] [Inverse Language Modeling towards Robust and Grounded LLMs](https://arxiv.org/abs/2510.01929)
*Davide Gabrielli,Simone Sestito,Iacopo Masi*

Main category: cs.CL

TL;DR: Proposes Inverse Language Modeling (ILM) as a unified framework to improve LLM robustness against input perturbations and enable native grounding by identifying toxic input triggers.


<details>
  <summary>Details</summary>
Motivation: Current defensive mechanisms for LLMs are fragmented and underdeveloped compared to prior work on classifiers, creating a need for better adversarial robustness.

Method: ILM transforms LLMs from static generators into analyzable systems by inverting model outputs to identify unsafe input triggers while improving robustness to perturbations.

Result: ILM enables simultaneous improvement of LLM robustness and native grounding capabilities, potentially helping RED teaming efforts.

Conclusion: ILM can lay the foundation for next-generation LLMs that are more robust, grounded, controllable, and trustworthy.

Abstract: The current landscape of defensive mechanisms for LLMs is fragmented and
underdeveloped, unlike prior work on classifiers. To further promote
adversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), a
unified framework that simultaneously 1) improves the robustness of LLMs to
input perturbations, and, at the same time, 2) enables native grounding by
inverting model outputs to identify potentially toxic or unsafe input triggers.
ILM transforms LLMs from static generators into analyzable and robust systems,
potentially helping RED teaming. ILM can lay the foundation for next-generation
LLMs that are not only robust and grounded but also fundamentally more
controllable and trustworthy. The code is publicly available at
github.com/davegabe/pag-llm.

</details>


### [70] [Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning](https://arxiv.org/abs/2510.01932)
*Qi He,Cheng Qian,Xiusi Chen,Bingxiang He,Yi R.,Fung,Heng Ji*

Main category: cs.CL

TL;DR: Veri-R1 is an online reinforcement learning framework that trains LLMs to interact with search engines for claim verification, improving joint accuracy by up to 30% and doubling evidence scores compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing claim verification approaches rely on prompt engineering or predefined workflows without unified training to improve essential skills like iterative evidence retrieval and reasoning.

Method: Online reinforcement learning framework where LLMs interact with search engines and receive reward signals to shape planning, retrieval, and reasoning behaviors.

Result: Improves joint accuracy by up to 30% and doubles evidence score, often outperforming larger-scale models. Ablation studies show impact of reward components and link between output logits and label accuracy.

Conclusion: Online RL is effective for precise and faithful claim verification, providing foundation for future research in LLM-powered verification systems.

Abstract: Claim verification with large language models (LLMs) has recently attracted
considerable attention, owing to their superior reasoning capabilities and
transparent verification pathways compared to traditional answer-only
judgments. Online claim verification requires iterative evidence retrieval and
reasoning, yet existing approaches mainly rely on prompt engineering or
predesigned reasoning workflows without offering a unified training paradigm to
improve necessary skills. Therefore, we introduce Veri-R1, an online
reinforcement learning (RL) framework that enables an LLM to interact with a
search engine and to receive reward signals that explicitly shape its planning,
retrieval, and reasoning behaviors. The dynamic interaction between models and
retrieval systems more accurately reflects real-world verification scenarios
and fosters comprehensive verification skills. Empirical results show that
Veri-R1 improves joint accuracy by up to 30% and doubles evidence score, often
surpassing larger-scale counterparts. Ablation studies further reveal the
impact of reward components and the link between output logits and label
accuracy. Our results highlight the effectiveness of online RL for precise and
faithful claim verification and provide a foundation for future research. We
release our code to support community progress in LLM empowered claim
verification.

</details>


### [71] [Taking a SEAT: Predicting Value Interpretations from Sentiment, Emotion, Argument, and Topic Annotations](https://arxiv.org/abs/2510.01976)
*Adina Nicola Dobrinoiu,Ana Cristiana Marcu,Amir Homayounirad,Luciano Cavalcante Siebert,Enrico Liscio*

Main category: cs.CL

TL;DR: Language models can better predict individual value interpretations by using multi-dimensional subjective annotations (SEAT: Sentiment, Emotion, Argument, Topics) as proxies for personal interpretive lenses, outperforming single dimensions and no-information baselines.


<details>
  <summary>Details</summary>
Motivation: Value interpretations are subjective and shaped by sociocultural backgrounds. Recognizing individual value perspectives is crucial for developing AI systems that align with diverse human viewpoints and avoid majority bias.

Method: Used multi-dimensional subjective annotations (SEAT dimensions: Sentiment, Emotion, Argument, Topics) as proxies for individual interpretive lenses. Evaluated language model performance across zero- and few-shot settings with different SEAT dimension combinations.

Result: Providing all SEAT dimensions simultaneously yielded superior performance compared to individual dimensions and baseline with no individual information. Individual variations across annotators highlighted the importance of accounting for subjective annotation behavior.

Conclusion: This controlled study provides the first evidence that annotation behavior (beyond demographics) impacts value prediction, establishing a foundation for future large-scale validation of personalized AI alignment with diverse human values.

Abstract: Our interpretation of value concepts is shaped by our sociocultural
background and lived experiences, and is thus subjective. Recognizing
individual value interpretations is important for developing AI systems that
can align with diverse human perspectives and avoid bias toward majority
viewpoints. To this end, we investigate whether a language model can predict
individual value interpretations by leveraging multi-dimensional subjective
annotations as a proxy for their interpretive lens. That is, we evaluate
whether providing examples of how an individual annotates Sentiment, Emotion,
Argument, and Topics (SEAT dimensions) helps a language model in predicting
their value interpretations. Our experiment across different zero- and few-shot
settings demonstrates that providing all SEAT dimensions simultaneously yields
superior performance compared to individual dimensions and a baseline where no
information about the individual is provided. Furthermore, individual
variations across annotators highlight the importance of accounting for the
incorporation of individual subjective annotators. To the best of our
knowledge, this controlled setting, although small in size, is the first
attempt to go beyond demographics and investigate the impact of annotation
behavior on value prediction, providing a solid foundation for future
large-scale validation.

</details>


### [72] [Exploring Database Normalization Effects on SQL Generation](https://arxiv.org/abs/2510.01989)
*Ryosuke Kohita*

Main category: cs.CL

TL;DR: Schema normalization significantly impacts NL2SQL performance - denormalized schemas work better for simple retrieval queries, while normalized schemas excel at aggregation queries due to better handling of data duplication and NULL values.


<details>
  <summary>Details</summary>
Motivation: To systematically study how schema design, particularly normalization levels, affects natural language to SQL systems, as most prior research evaluates models on fixed schemas without considering design impact.

Method: Evaluated eight leading LLMs on synthetic datasets with formal normalization (1NF-3NF) and real academic paper datasets with practical schemes, using both zero-shot and few-shot settings.

Result: Denormalized schemas achieved high accuracy on simple retrieval queries even with cost-effective models in zero-shot. Normalized schemas (2NF/3NF) caused base table selection and join prediction errors but were substantially improved by few-shot examples. For aggregation queries, normalized schemas performed better due to robustness against data duplication and NULL value issues.

Conclusion: Optimal schema design for NL2SQL depends on query types - denormalized for simple retrieval, normalized for aggregation. Schema design should be considered in NL2SQL development with adaptive schema selection for real-world scenarios.

Abstract: Schema design, particularly normalization, is a critical yet often overlooked
factor in natural language to SQL (NL2SQL) systems. Most prior research
evaluates models on fixed schemas, overlooking the influence of design on
performance. We present the first systematic study of schema normalization's
impact, evaluating eight leading large language models on synthetic and
real-world datasets with varied normalization levels. We construct controlled
synthetic datasets with formal normalization (1NF-3NF) and real academic paper
datasets with practical schemes. Our results show that denormalized schemas
offer high accuracy on simple retrieval queries, even with cost-effective
models in zero-shot settings. In contrast, normalized schemas (2NF/3NF)
introduce challenges such as errors in base table selection and join type
prediction; however, these issues are substantially mitigated by providing
few-shot examples. For aggregation queries, normalized schemas yielded better
performance, mainly due to their robustness against the data duplication and
NULL value issues that cause errors in denormalized schemas. These findings
suggest that the optimal schema design for NL2SQL applications depends on the
types of queries to be supported. Our study demonstrates the importance of
considering schema design when developing NL2SQL interfaces and integrating
adaptive schema selection for real-world scenarios.

</details>


### [73] [LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and Target](https://arxiv.org/abs/2510.01995)
*Md Arid Hasan,Firoj Alam,Md Fahad Hossain,Usman Naseem,Syed Ishtiaque Ahmed*

Main category: cs.CL

TL;DR: This paper introduces BanglaMultiHate, the first multi-task Bangla hate-speech dataset, and compares various models including LLMs for hate speech detection in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Online platforms need reliable hate speech detection systems, especially for low-resource languages like Bangla where existing tools are limited and mostly single-task with poor coverage of multi-facet signals.

Method: Created BanglaMultiHate dataset, then conducted comprehensive comparison of classical baselines, monolingual pretrained models, and LLMs using zero-shot prompting and LoRA fine-tuning.

Result: LoRA-tuned LLMs are competitive with BanglaBERT, but culturally and linguistically grounded pretraining remains crucial for robust performance.

Conclusion: The dataset and findings establish a stronger benchmark for developing culturally aligned moderation tools in low-resource contexts.

Abstract: Online social media platforms are central to everyday communication and
information seeking. While these platforms serve positive purposes, they also
provide fertile ground for the spread of hate speech, offensive language, and
bullying content targeting individuals, organizations, and communities. Such
content undermines safety, participation, and equity online. Reliable detection
systems are therefore needed, especially for low-resource languages where
moderation tools are limited. In Bangla, prior work has contributed resources
and models, but most are single-task (e.g., binary hate/offense) with limited
coverage of multi-facet signals (type, severity, target). We address these gaps
by introducing the first multi-task Bangla hate-speech dataset,
BanglaMultiHate, one of the largest manually annotated corpus to date. Building
on this resource, we conduct a comprehensive, controlled comparison spanning
classical baselines, monolingual pretrained models, and LLMs under zero-shot
prompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a
low-resource setting and reveal a consistent trend: although LoRA-tuned LLMs
are competitive with BanglaBERT, culturally and linguistically grounded
pretraining remains critical for robust performance. Together, our dataset and
findings establish a stronger benchmark for developing culturally aligned
moderation tools in low-resource contexts. For reproducibility, we will release
the dataset and all related scripts.

</details>


### [74] [Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models](https://arxiv.org/abs/2510.02025)
*Donghoon Jung,Jiwoo Choi,Songeun Chae,Seohyon Jung*

Main category: cs.CL

TL;DR: This paper evaluates LLMs' creativity through a process-oriented approach using narratology, focusing on constraint-based decision-making and authorial personas to analyze creative preferences.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLM creativity focus mainly on output quality rather than the creative processes that generate them, creating a gap in understanding how LLMs function as computational authors.

Method: Using controlled prompting to assign authorial personas and constraint-based decision-making as analytical framework, examining LLMs' creative preferences across Style, Character, Event, and Setting elements.

Result: LLMs consistently prioritize Style over other narrative elements (Character, Event, Setting), and distinctive creative profiles emerge across different models based on their reasoning patterns.

Conclusion: The approach provides a novel systematic tool for analyzing AI's authorial creativity, revealing consistent patterns in how LLMs make creative decisions and demonstrating the value of process-oriented evaluation.

Abstract: Evaluations of large language models (LLMs)' creativity have focused
primarily on the quality of their outputs rather than the processes that shape
them. This study takes a process-oriented approach, drawing on narratology to
examine LLMs as computational authors. We introduce constraint-based
decision-making as a lens for authorial creativity. Using controlled prompting
to assign authorial personas, we analyze the creative preferences of the
models. Our findings show that LLMs consistently emphasize Style over other
elements, including Character, Event, and Setting. By also probing the
reasoning the models provide for their choices, we show that distinctive
profiles emerge across models and argue that our approach provides a novel
systematic tool for analyzing AI's authorial creativity.

</details>


### [75] [Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage](https://arxiv.org/abs/2510.02044)
*Siddhant Arora,Haidar Khan,Kai Sun,Xin Luna Dong,Sajal Choudhary,Seungwhan Moon,Xinyuan Zhang,Adithya Sagar,Surya Teja Appini,Kaushik Patnaik,Sanat Sharma,Shinji Watanabe,Anuj Kumar,Ahmed Aly,Yue Liu,Florian Metze,Zhaojiang Lin*

Main category: cs.CL

TL;DR: Streaming RAG enables speech-in speech-out dialogue systems to use tools with reduced latency by predicting tool queries during ongoing user speech and fusing retrieved results with audio responses.


<details>
  <summary>Details</summary>
Motivation: End-to-end speech dialogue systems suffer from hallucinations due to limited factual grounding, and tool integration increases response latency, disrupting conversational flow.

Method: Proposed Streaming Retrieval-Augmented Generation (Streaming RAG) framework that predicts tool queries in parallel with user speech and generates spoken summaries fusing audio queries with retrieved text results.

Result: Increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and reduces tool use latency by 20% on AudioCRAG benchmark.

Conclusion: Streaming RAG approach is modality-agnostic and enables more agentic, real-time AI assistants by improving both accuracy and responsiveness in speech dialogue systems.

Abstract: End-to-end speech-in speech-out dialogue systems are emerging as a powerful
alternative to traditional ASR-LLM-TTS pipelines, generating more natural,
expressive responses with significantly lower latency. However, these systems
remain prone to hallucinations due to limited factual grounding. While
text-based dialogue systems address this challenge by integrating tools such as
web search and knowledge graph APIs, we introduce the first approach to extend
tool use directly into speech-in speech-out systems. A key challenge is that
tool integration substantially increases response latency, disrupting
conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented
Generation (Streaming RAG), a novel framework that reduces user-perceived
latency by predicting tool queries in parallel with user speech, even before
the user finishes speaking. Specifically, we develop a post-training pipeline
that teaches the model when to issue tool calls during ongoing speech and how
to generate spoken summaries that fuse audio queries with retrieved text
results, thereby improving both accuracy and responsiveness. To evaluate our
approach, we construct AudioCRAG, a benchmark created by converting queries
from the publicly available CRAG dataset into speech form. Experimental results
demonstrate that our streaming RAG approach increases QA accuracy by up to 200%
relative (from 11.1% to 34.2% absolute) and further enhances user experience by
reducing tool use latency by 20%. Importantly, our streaming RAG approach is
modality-agnostic and can be applied equally to typed input, paving the way for
more agentic, real-time AI assistants.

</details>


### [76] [Chain-of-Thought Reasoning in Streaming Full-Duplex End-to-End Spoken Dialogue Systems](https://arxiv.org/abs/2510.02066)
*Siddhant Arora,Jinchuan Tian,Hayato Futami,Jiatong Shi,Yosuke Kashiwagi,Emiru Tsunoo,Shinji Watanabe*

Main category: cs.CL

TL;DR: SCoT is a Streaming Chain-of-Thought framework for Duplex spoken dialogue systems that processes user input in blocks and generates responses continuously, eliminating the need for voice activity detection while improving coherence and reducing latency.


<details>
  <summary>Details</summary>
Motivation: Traditional E2E spoken dialogue systems rely on VAD for turn-taking, but VAD cannot distinguish between pauses and turn completions. Existing duplex SDS models have complex architectures and lag in semantic reasoning compared to cascaded models.

Method: Propose SCoT framework that alternates between processing fixed-duration user input blocks and generating responses in a blockwise manner. Uses frame-level alignments to create intermediate targets (aligned user transcripts and system responses) for each block.

Result: Produces more coherent and interpretable responses than existing duplex methods while supporting lower-latency and overlapping interactions compared to turn-by-turn systems.

Conclusion: SCoT framework effectively addresses limitations of both VAD-based systems and existing duplex models by providing continuous processing with improved semantic reasoning and reduced latency.

Abstract: Most end-to-end (E2E) spoken dialogue systems (SDS) rely on voice activity
detection (VAD) for turn-taking, but VAD fails to distinguish between pauses
and turn completions. Duplex SDS models address this by predicting output
continuously, including silence tokens, thus removing the need for explicit
VAD. However, they often have complex dual-channel architecture and lag behind
cascaded models in semantic reasoning. To overcome these challenges, we propose
SCoT: a Streaming Chain-of-Thought (CoT) framework for Duplex SDS, alternating
between processing fixed-duration user input and generating responses in a
blockwise manner. Using frame-level alignments, we create intermediate
targets-aligned user transcripts and system responses for each block.
Experiments show that our approach produces more coherent and interpretable
responses than existing duplex methods while supporting lower-latency and
overlapping interactions compared to turn-by-turn systems.

</details>


### [77] [The Disparate Impacts of Speculative Decoding](https://arxiv.org/abs/2510.02128)
*Jameson Sandler,Ahmet ÃstÃ¼n,Marco Romanelli,Sara Hooker,Ferdinando Fioretto*

Main category: cs.CL

TL;DR: Speculative decoding provides uneven speed-up across tasks, with under-fit and underrepresented tasks benefiting less. The paper analyzes this unfairness and proposes a mitigation strategy that improves fairness by 12% on average.


<details>
  <summary>Details</summary>
Motivation: To understand and address the disparate speed-up rates in speculative decoding, where smaller tasks get less benefit, potentially exacerbating performance gaps.

Method: Derived analytical framework to quantify unfairness, identified key factors causing disparities, and proposed a targeted mitigation strategy tested across multiple model pairs.

Result: Found consistent speed-up reduction for under-fit tasks, validated mitigation approach showing 12% average improvement in fairness metric across various model configurations.

Conclusion: Speculative decoding inherently creates unfair speed-up distribution, but targeted interventions can significantly reduce these disparities and improve overall system fairness.

Abstract: The practice of speculative decoding, whereby inference is probabilistically
supported by a smaller, cheaper, ``drafter'' model, has become a standard
technique for systematically reducing the decoding time of large language
models. This paper conducts an analysis of speculative decoding through the
lens of its potential disparate speed-up rates across tasks. Crucially, the
paper shows that speed-up gained from speculative decoding is not uniformly
distributed across tasks, consistently diminishing for under-fit, and often
underrepresented tasks. To better understand this phenomenon, we derive an
analysis to quantify this observed ``unfairness'' and draw attention to the
factors that motivate such disparate speed-ups to emerge. Further, guided by
these insights, the paper proposes a mitigation strategy designed to reduce
speed-up disparities and validates the approach across several model pairs,
revealing on average a 12% improvement in our fairness metric.

</details>


### [78] [RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization](https://arxiv.org/abs/2510.02172)
*Zhaoning Yu,Will Su,Leitian Tao,Haozhu Wang,Aashu Singh,Hanchao Yu,Jianyu Wang,Hongyang Gao,Weizhe Yuan,Jason Weston,Ping Yu,Jing Xu*

Main category: cs.CL

TL;DR: RESTRAIN is a self-penalizing RL framework that enables models to improve reasoning without gold labels by penalizing overconfident predictions and low-consistency examples while preserving promising reasoning chains.


<details>
  <summary>Details</summary>
Motivation: To address the high costs of human-annotated data in reinforcement learning for chain-of-thought reasoning and the limitations on harder tasks, enabling experience-driven learning without curated labels.

Method: A self-penalizing RL framework that converts absence of gold labels into learning signals, penalizing overconfident rollouts and low-consistency examples while integrating with policy optimization methods like GRPO for continual self-improvement.

Result: Achieves significant improvements: +140.7% Pass@1 on AIME25, +36.2% on MMLU_STEM, and +19.6% on GPQA-Diamond using Qwen3-4B-Base and OctoThinker Hybrid-8B-Base models, nearly matching gold-label training performance without using gold labels.

Conclusion: RESTRAIN establishes a scalable path toward stronger reasoning capabilities without requiring gold labels, demonstrating effective self-improvement through self-penalization mechanisms.

Abstract: Reinforcement learning with human-annotated data has boosted chain-of-thought
reasoning in large reasoning models, but these gains come at high costs in
labeled data while faltering on harder tasks. A natural next step is
experience-driven learning, where models improve without curated labels by
adapting to unlabeled data. We introduce RESTRAIN (REinforcement learning with
Self-restraint), a self-penalizing RL framework that converts the absence of
gold labels into a useful learning signal. Instead of overcommitting to
spurious majority votes, RESTRAIN exploits signals from the model's entire
answer distribution: penalizing overconfident rollouts and low-consistency
examples while preserving promising reasoning chains. The self-penalization
mechanism integrates seamlessly into policy optimization methods such as GRPO,
enabling continual self-improvement without supervision. On challenging
reasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data.
With Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to
+140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent on
GPQA-Diamond, nearly matching gold-label training while using no gold labels.
These results demonstrate that RESTRAIN establishes a scalable path toward
stronger reasoning without gold labels.

</details>


### [79] [Learning to Reason for Hallucination Span Detection](https://arxiv.org/abs/2510.02173)
*Hsuan Su,Ting-Yao Hu,Hema Swetha Koppula,Kundan Krishna,Hadi Pouransari,Cheng-Yu Hsieh,Cem Koc,Joseph Yitan Cheng,Oncel Tuzel,Raviteja Vemulapalli*

Main category: cs.CL

TL;DR: RL4HS is a reinforcement learning framework that uses span-level rewards and explicit reasoning to detect hallucinated spans in LLM outputs, outperforming pretrained models and supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate hallucinations that undermine reliability. While most prior work treats hallucination detection as binary classification, real applications need to identify specific hallucinated spans, which requires multi-step reasoning.

Method: Proposed RL4HS framework using reinforcement learning with span-level reward function. Built on Group Relative Policy Optimization and introduced Class-Aware Policy Optimization to address reward imbalance. Uses Chain-of-Thought reasoning to help detect hallucination spans.

Result: Experiments on RAGTruth benchmark (summarization, QA, data-to-text) show RL4HS surpasses pretrained reasoning models and supervised fine-tuning methods.

Conclusion: Reinforcement learning with span-level rewards is necessary for effectively detecting hallucination spans in LLM outputs, and explicit reasoning helps this complex multi-step decision process.

Abstract: Large language models (LLMs) often generate hallucinations -- unsupported
content that undermines reliability. While most prior works frame hallucination
detection as a binary task, many real-world applications require identifying
hallucinated spans, which is a multi-step decision making process. This
naturally raises the question of whether explicit reasoning can help the
complex task of detecting hallucination spans. To answer this question, we
first evaluate pretrained models with and without Chain-of-Thought (CoT)
reasoning, and show that CoT reasoning has the potential to generate at least
one correct answer when sampled multiple times. Motivated by this, we propose
RL4HS, a reinforcement learning framework that incentivizes reasoning with a
span-level reward function. RL4HS builds on Group Relative Policy Optimization
and introduces Class-Aware Policy Optimization to mitigate reward imbalance
issue. Experiments on the RAGTruth benchmark (summarization, question
answering, data-to-text) show that RL4HS surpasses pretrained reasoning models
and supervised fine-tuning, demonstrating the necessity of reinforcement
learning with span-level rewards for detecting hallucination spans.

</details>


### [80] [ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities](https://arxiv.org/abs/2510.02200)
*Felix Brei,Lorenz BÃ¼hmann,Johannes Frey,Daniel Gerber,Lars-Peter Meyer,Claus Stadler,Kirill Bulert*

Main category: cs.CL

TL;DR: The paper introduces a generalized method based on SPINACH, an LLM-backed agent that translates natural language questions to SPARQL queries through iterative exploration and execution, addressing the high barrier of SPARQL for non-experts.


<details>
  <summary>Details</summary>
Motivation: To lower the barrier of entry for interacting with knowledge graphs by providing Text2SPARQL translation support using large language models, motivated by the Text2SPARQL challenge.

Method: Uses SPINACH, an LLM-backed agent that translates natural language to SPARQL queries not in a single shot but as an iterative process of exploration and execution.

Result: The paper describes the overall architecture and design decisions, and conducts a thorough analysis of agent behavior to identify areas for future improvements.

Conclusion: The iterative approach to Text2SPARQL translation shows promise for making knowledge graph interaction more accessible, with identified areas for targeted improvements.

Abstract: Interacting with knowledge graphs can be a daunting task for people without a
background in computer science since the query language that is used (SPARQL)
has a high barrier of entry. Large language models (LLMs) can lower that
barrier by providing support in the form of Text2SPARQL translation. In this
paper we introduce a generalized method based on SPINACH, an LLM backed agent
that translates natural language questions to SPARQL queries not in a single
shot, but as an iterative process of exploration and execution. We describe the
overall architecture and reasoning behind our design decisions, and also
conduct a thorough analysis of the agent behavior to gain insights into future
areas for targeted improvements. This work was motivated by the Text2SPARQL
challenge, a challenge that was held to facilitate improvements in the
Text2SPARQL domain.

</details>


### [81] [Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents](https://arxiv.org/abs/2510.02204)
*Lingzhong Dong,Ziqi Zhou,Shuaibo Yang,Haiyue Sheng,Pengzhou Cheng,Zongru Wu,Zheng Wu,Gongshen Liu,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: This paper introduces a new evaluation framework to diagnose reasoning-execution gaps in mobile-use agents powered by vision-language models, focusing on whether chain-of-thought reasoning aligns with ground-truth actions.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations emphasize execution accuracy but neglect whether CoT reasoning aligns with ground-truth actions, potentially fostering over-trust where users may unknowingly authorize harmful actions based on seemingly plausible but misaligned reasoning.

Method: Proposes Ground-Truth Alignment (GTA) metric to measure whether the action implied by CoT matches ground-truth action, combined with Exact Match (EM) metric to jointly assess reasoning and execution accuracy, identifying Execution Gap (EG) and Reasoning Gap (RG).

Result: Experimental results show reasoning-execution gaps are prevalent across mobile interaction tasks, with execution gaps occurring more frequently than reasoning gaps. Scaling up model size reduces overall gap but sizable execution gaps persist even in largest models.

Conclusion: The framework reliably reflects systematic EG/RG patterns in state-of-the-art models, offering concrete diagnostics to support development of more trustworthy mobile-use agents.

Abstract: Mobile-use agents powered by vision-language models (VLMs) have shown great
potential in interpreting natural language instructions and generating
corresponding actions based on mobile graphical user interface. Recent studies
suggest that incorporating chain-of-thought (CoT) reasoning tends to improve
the execution accuracy. However, existing evaluations emphasize execution
accuracy while neglecting whether CoT reasoning aligns with ground-truth
actions. This oversight fails to assess potential reasoning-execution gaps,
which in turn foster over-trust: users relying on seemingly plausible CoTs may
unknowingly authorize harmful actions, potentially resulting in financial loss
or trust crisis. In this work, we introduce a new evaluation framework to
diagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment
(GTA), which measures whether the action implied by a CoT matches the
ground-truth action. By combining GTA with the standard Exact Match (EM)
metric, we jointly assess both the reasoning accuracy and execution accuracy.
This joint perspective reveals two types of reasoning-execution gaps: (i)
Execution Gap (EG), where the reasoning correctly identifies the correct action
but execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but
reasoning process conflicts with the actual execution. Experimental results
across a wide range of mobile interaction tasks reveal that reasoning-execution
gaps are prevalent, with execution gaps occurring more frequently than
reasoning gaps. Moreover, while scaling up model size reduces the overall gap,
sizable execution gaps persist even in the largest models. Further analysis
shows that our framework reliably reflects systematic EG/RG patterns in
state-of-the-art models. These findings offer concrete diagnostics and support
the development of more trustworthy mobile-use agents.

</details>


### [82] [More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration](https://arxiv.org/abs/2510.02227)
*Xiaoyang Yuan,Yujuan Ding,Yi Bin,Wenqi Shao,Jinyu Cai,Jingkuan Song,Yang Yang,Hengtao Shen*

Main category: cs.CL

TL;DR: AMPO introduces adaptive multi-teacher guidance for RLVR in LLMs, improving reasoning diversity and performance by selectively using teacher guidance only when needed and focusing on comprehensible reasoning paths.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods rely on self-exploration or single teachers, which can introduce model biases and limit reasoning diversity. Multi-teacher strategies from knowledge distillation can help overcome these limitations.

Method: AMPO adaptively leverages multiple teacher models' guidance only when the on-policy model fails, using a "guidance-on-demand" approach with comprehension-based selection to balance exploration and exploitation.

Result: AMPO achieves 4.3% improvement on mathematical reasoning and 12.2% on out-of-distribution tasks vs GRPO baseline, boosts Pass@k performance, enables more diverse exploration, and matches single powerful teacher approaches with peer-sized teachers.

Conclusion: AMPO provides a more efficient and scalable path to superior reasoning and generalizability in LLMs through adaptive multi-teacher guidance.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm
for enhancing the reasoning ability in Large Language Models (LLMs). However,
prevailing methods primarily rely on self-exploration or a single off-policy
teacher to elicit long chain-of-thought (LongCoT) reasoning, which may
introduce intrinsic model biases and restrict exploration, ultimately limiting
reasoning diversity and performance. Drawing inspiration from multi-teacher
strategies in knowledge distillation, we introduce Adaptive Multi-Guidance
Policy Optimization (AMPO), a novel framework that adaptively leverages
guidance from multiple proficient teacher models, but only when the on-policy
model fails to generate correct solutions. This "guidance-on-demand" approach
expands exploration while preserving the value of self-discovery. Moreover,
AMPO incorporates a comprehension-based selection mechanism, prompting the
student to learn from the reasoning paths that it is most likely to comprehend,
thus balancing broad exploration with effective exploitation. Extensive
experiments show AMPO substantially outperforms a strong baseline (GRPO), with
a 4.3% improvement on mathematical reasoning tasks and 12.2% on
out-of-distribution tasks, while significantly boosting Pass@k performance and
enabling more diverse exploration. Notably, using four peer-sized teachers, our
method achieves comparable results to approaches that leverage a single, more
powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate
a more efficient and scalable path to superior reasoning and generalizability.
Our code is available at https://github.com/SII-Enigma/AMPO.

</details>


### [83] [Enhanced Arabic-language cyberbullying detection: deep embedding and transformer (BERT) approaches](https://arxiv.org/abs/2510.02232)
*Ebtesam Jaber Aljohani,Wael M. S. Yafoo*

Main category: cs.CL

TL;DR: This paper proposes deep learning methods for detecting cyberbullying in Arabic-language content, achieving up to 98% accuracy using Bi-LSTM with FastText embeddings.


<details>
  <summary>Details</summary>
Motivation: There is a scarcity of methods for detecting Arabic-language cyberbullying, while most existing approaches focus on English. The growth of social media platforms like X (Twitter) exposes young people to cyberbullying, necessitating effective detection methods for Arabic content.

Method: Assembled a dataset of 10,662 X posts, pre-processed data, and used kappa tool for annotation quality. Tested four experiments with deep learning models: LSTM and Bi-LSTM with various word embeddings, and combined LSTM-BERT and Bi-LSTM-BERT models.

Result: LSTM-BERT and Bi-LSTM-BERT achieved 97% accuracy. Bi-LSTM with FastText embedding performed even better, achieving 98% accuracy. The results demonstrate good generalization.

Conclusion: The proposed deep learning methods effectively detect Arabic-language cyberbullying, with Bi-LSTM with FastText embeddings showing the best performance at 98% accuracy, addressing the scarcity of Arabic cyberbullying detection methods.

Abstract: Recent technological advances in smartphones and communications, including
the growth of such online platforms as massive social media networks such as X
(formerly known as Twitter) endangers young people and their emotional
well-being by exposing them to cyberbullying, taunting, and bullying content.
Most proposed approaches for automatically detecting cyberbullying have been
developed around the English language, and methods for detecting
Arabic-language cyberbullying are scarce. Methods for detecting Arabic-language
cyberbullying are especially scarce. This paper aims to enhance the
effectiveness of methods for detecting cyberbullying in Arabic-language
content. We assembled a dataset of 10,662 X posts, pre-processed the data, and
used the kappa tool to verify and enhance the quality of our annotations. We
conducted four experiments to test numerous deep learning models for
automatically detecting Arabic-language cyberbullying. We first tested a long
short-term memory (LSTM) model and a bidirectional long short-term memory
(Bi-LSTM) model with several experimental word embeddings. We also tested the
LSTM and Bi-LSTM models with a novel pre-trained bidirectional encoder from
representations (BERT) and then tested them on a different experimental models
BERT again. LSTM-BERT and Bi-LSTM-BERT demonstrated a 97% accuracy. Bi-LSTM
with FastText embedding word performed even better, achieving 98% accuracy. As
a result, the outcomes are generalize

</details>


### [84] [AccurateRAG: A Framework for Building Accurate Retrieval-Augmented Question-Answering Applications](https://arxiv.org/abs/2510.02243)
*Linh The Nguyen,Chi Tran,Dung Ngoc Nguyen,Van-Cuong Pham,Hoang Ngo,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: AccurateRAG is a novel framework for building high-performance question-answering applications using retrieval-augmented generation (RAG), featuring a complete development pipeline and achieving state-of-the-art results on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive framework that enables efficient development of high-performance question-answering applications based on RAG technology, addressing the need for better tools and pipelines in this domain.

Method: The framework offers a complete pipeline including raw dataset processing, fine-tuning data generation, text embedding & LLM fine-tuning, output evaluation, and local RAG system building.

Result: Experimental results demonstrate that AccurateRAG outperforms previous strong baselines and achieves new state-of-the-art question-answering performance on benchmark datasets.

Conclusion: AccurateRAG provides an effective framework for developing high-performance RAG-based question-answering systems, establishing new benchmarks in the field.

Abstract: We introduce AccurateRAG -- a novel framework for constructing
high-performance question-answering applications based on retrieval-augmented
generation (RAG). Our framework offers a pipeline for development efficiency
with tools for raw dataset processing, fine-tuning data generation, text
embedding & LLM fine-tuning, output evaluation, and building RAG systems
locally. Experimental results show that our framework outperforms previous
strong baselines and obtains new state-of-the-art question-answering
performance on benchmark datasets.

</details>


### [85] [Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation](https://arxiv.org/abs/2510.02249)
*Tianyi Jiang,Yi Bin,Yujuan Ding,Kainian Zhu,Fei Ma,Jingkuan Song,Heng Tao Shen*

Main category: cs.CL

TL;DR: This paper introduces TECA metric and CER mechanism to address LLM overthinking by dynamically determining optimal reasoning depth, reducing response length by up to 71% on simpler problems while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate unnecessarily lengthy reasoning steps for simpler problems (overthinking), which degrades efficiency and makes it difficult to adapt reasoning depth to problem complexity.

Method: Proposes Token Entropy Cumulative Average (TECA) metric to measure exploration extent, and Cumulative Entropy Regulation (CER) mechanism with "Explore Briefly, Then Decide" paradigm to dynamically determine optimal reasoning stopping point.

Result: Experimental results across mathematical benchmarks show substantial mitigation of overthinking without sacrificing problem-solving ability. Average response length decreases by up to 71% on simpler datasets.

Conclusion: The proposed approach creates a more efficient and adaptive reasoning process for LLMs by preventing overthinking while maintaining reasoning quality.

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
on complex problems using long Chain-of-Thought (CoT) reasoning. However, they
often suffer from overthinking, meaning generating unnecessarily lengthy
reasoning steps for simpler problems. This issue may degrade the efficiency of
the models and make them difficult to adapt the reasoning depth to the
complexity of problems. To address this, we introduce a novel metric Token
Entropy Cumulative Average (TECA), which measures the extent of exploration
throughout the reasoning process. We further propose a novel reasoning paradigm
-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy
Regulation (CER) mechanism. This paradigm leverages TECA to help the model
dynamically determine the optimal point to conclude its thought process and
provide a final answer, thus achieving efficient reasoning. Experimental
results across diverse mathematical benchmarks show that our approach
substantially mitigates overthinking without sacrificing problem-solving
ability. With our thinking paradigm, the average response length decreases by
up to 71% on simpler datasets, demonstrating the effectiveness of our method in
creating a more efficient and adaptive reasoning process.

</details>


### [86] [InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents](https://arxiv.org/abs/2510.02271)
*Yaxin Du,Yuanshuo Zhang,Xiyuan Yang,Yifan Zhou,Cheng Wang,Gongyi Zou,Xianghe Pang,Wenhao Wang,Menglan Chen,Shuo Tang,Zhiyu Li,Siheng Chen*

Main category: cs.CL

TL;DR: InfoMosaic-Bench is the first benchmark for multi-source information seeking in tool-augmented agents, requiring agents to combine general-purpose search with domain-specific tools across six domains.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agents rely heavily on open-web search which has limitations: noisy/unreliable content and lack of precise domain-specific knowledge. The emergence of Model Context Protocol (MCP) enables agents to access specialized tools, but it's unclear if agents can effectively leverage these tools and integrate them with general search.

Method: Created InfoMosaic-Bench benchmark covering six domains (medicine, finance, maps, video, web, and multi-domain integration) with tasks synthesized using InfoMosaic-Flow pipeline that grounds task conditions in verified tool outputs, enforces cross-source dependencies, and filters out trivial lookup cases.

Result: Experiments with 14 state-of-the-art LLM agents show: web information alone achieves only 38.2% accuracy and 67.5% pass rate; domain tools provide selective but inconsistent benefits; 22.4% of failures come from incorrect tool usage or selection.

Conclusion: Current LLM agents still struggle with basic tool handling and effectively integrating multiple information sources, highlighting the need for improved tool-augmented agent capabilities.

Abstract: Information seeking is a fundamental requirement for humans. However,
existing LLM agents rely heavily on open-web search, which exposes two
fundamental weaknesses: online content is noisy and unreliable, and many
real-world tasks require precise, domain-specific knowledge unavailable from
the web. The emergence of the Model Context Protocol (MCP) now allows agents to
interface with thousands of specialized tools, seemingly resolving this
limitation. Yet it remains unclear whether agents can effectively leverage such
tools -- and more importantly, whether they can integrate them with
general-purpose search to solve complex tasks. Therefore, we introduce
InfoMosaic-Bench, the first benchmark dedicated to multi-source information
seeking in tool-augmented agents. Covering six representative domains
(medicine, finance, maps, video, web, and multi-domain integration),
InfoMosaic-Bench requires agents to combine general-purpose search with
domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable
pipeline that grounds task conditions in verified tool outputs, enforces
cross-source dependencies, and filters out shortcut cases solvable by trivial
lookup. This design guarantees both reliability and non-triviality. Experiments
with 14 state-of-the-art LLM agents reveal three findings: (i) web information
alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass
rate; (ii) domain tools provide selective but inconsistent benefits, improving
some domains while degrading others; and (iii) 22.4% of failures arise from
incorrect tool usage or selection, highlighting that current LLMs still
struggle with even basic tool handling.

</details>


### [87] [Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective](https://arxiv.org/abs/2510.02272)
*Wen Yang,Junhong Wu,Chong Li,Chengqing Zong,Jiajun Zhang*

Main category: cs.CL

TL;DR: This paper investigates cross-lingual transfer of reasoning capabilities in Large Reasoning Models (LRMs) after English Reinforcement Post-Training (RPT), revealing significant variation in transferability and identifying key patterns including Parallel Leap, Parallel Scaling Law, and Monolingual Generalization Gap.


<details>
  <summary>Details</summary>
Motivation: To address whether reasoning capabilities achieved from English RPT effectively transfer to other languages, challenging the assumption that LRM reasoning mirrors human cognition.

Method: Systematically evaluate English-centric LRMs on multilingual reasoning benchmarks, introduce cross-lingual transferability metric, conduct interventional studies, and perform parallel training experiments.

Result: Cross-lingual transferability varies significantly; models with stronger initial English capabilities over-rely on English patterns, diminishing cross-lingual generalization; parallel training shows substantial performance leap and follows power-law scaling.

Conclusion: English-centric LRMs fail to fully generalize across languages, challenging assumptions about LRM reasoning mirroring human cognition, with implications for developing more language-agnostic models.

Abstract: Recent advancements in Reinforcement Post-Training (RPT) have significantly
enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased
interest in the generalization of RL-based reasoning. While existing work has
primarily focused on investigating its generalization across tasks or
modalities, this study proposes a novel cross-linguistic perspective to
investigate reasoning generalization. This raises a crucial question:
$\textit{Does the reasoning capability achieved from English RPT effectively
transfer to other languages?}$ We address this by systematically evaluating
English-centric LRMs on multilingual reasoning benchmarks and introducing a
metric to quantify cross-lingual transferability. Our findings reveal that
cross-lingual transferability varies significantly across initial model, target
language, and training paradigm. Through interventional studies, we find that
models with stronger initial English capabilities tend to over-rely on
English-specific patterns, leading to diminished cross-lingual generalization.
To address this, we conduct a thorough parallel training study. Experimental
results yield three key findings: $\textbf{First-Parallel Leap}$, a substantial
leap in performance when transitioning from monolingual to just a single
parallel language, and a predictable $\textbf{Parallel Scaling Law}$, revealing
that cross-lingual reasoning transfer follows a power-law with the number of
training parallel languages. Moreover, we identify the discrepancy between
actual monolingual performance and the power-law prediction as
$\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs
fail to fully generalize across languages. Our study challenges the assumption
that LRM reasoning mirrors human cognition, providing critical insights for the
development of more language-agnostic LRMs.

</details>


### [88] [From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens](https://arxiv.org/abs/2510.02292)
*Hala Sheta,Eric Huang,Shuyu Wu,Ilia Alenabi,Jiajun Hong,Ryker Lin,Ruoxi Ning,Daniel Wei,Jialin Yang,Jiawei Zhou,Ziqiao Ma,Freda Shi*

Main category: cs.CL

TL;DR: VLM-Lens is a toolkit for systematic benchmarking and analysis of vision-language models, enabling extraction of intermediate outputs from any layer during forward pass of open-source VLMs.


<details>
  <summary>Details</summary>
Motivation: To provide a unified interface for analyzing and interpreting vision-language models, abstracting away model-specific complexities and supporting diverse VLMs.

Method: Provides a YAML-configurable interface that supports extraction of intermediate outputs from any layer, currently supporting 16 base VLMs and over 30 variants, with extensible architecture.

Result: The toolkit enables systematic analysis revealing differences in hidden representations across layers and target concepts, and integrates easily with various interpretability methods.

Conclusion: VLM-Lens is released as open-source to accelerate community efforts in understanding and improving vision-language models.

Abstract: We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,
analysis, and interpretation of vision-language models (VLMs) by supporting the
extraction of intermediate outputs from any layer during the forward pass of
open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that
abstracts away model-specific complexities and supports user-friendly operation
across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and
their over 30 variants, and is extensible to accommodate new models without
changing the core logic.
  The toolkit integrates easily with various interpretability and analysis
methods. We demonstrate its usage with two simple analytical experiments,
revealing systematic differences in the hidden representations of VLMs across
layers and target concepts. VLM-Lens is released as an open-sourced project to
accelerate community efforts in understanding and improving VLMs.

</details>


### [89] [F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data](https://arxiv.org/abs/2510.02294)
*Ziyin Zhang,Zihan Liao,Hang Yu,Peng Di,Rui Wang*

Main category: cs.CL

TL;DR: F2LLM is a suite of embedding models (0.6B, 1.7B, 4B) that achieves state-of-the-art performance by directly finetuning foundation models on 6M open-source query-document pairs, avoiding costly contrastive pretraining and synthetic data.


<details>
  <summary>Details</summary>
Motivation: To create high-performance embedding models that avoid the massive computational costs, complex training pipelines, and expensive synthetic data requirements of previous top-ranking embedding models.

Method: Directly finetune foundation models on 6 million query-document-negative tuples curated from open-source, non-synthetic datasets, without requiring contrastive pretraining.

Result: F2LLM-4B ranks 2nd among 4B-parameter models and 7th overall on MTEB English leaderboard; F2LLM-1.7B ranks 1st in the 1B-2B size range.

Conclusion: F2LLM provides a strong, reproducible, and budget-friendly baseline for embedding models, achieving excellent performance-cost balance with released models, dataset, and code.

Abstract: We introduce F2LLM - Foundation to Feature Large Language Models, a suite of
state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike
previous top-ranking embedding models that require massive contrastive
pretraining, sophisticated training pipelines, and costly synthetic training
data, F2LLM is directly finetuned from foundation models on 6 million
query-document-negative tuples curated from open-source, non-synthetic
datasets, striking a strong balance between training cost, model size, and
embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd
among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B
ranks 1st among models in the 1B-2B size range. To facilitate future research
in the field, we release the models, training dataset, and code, positioning
F2LLM as a strong, reproducible, and budget-friendly baseline for future works.

</details>


### [90] [Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation](https://arxiv.org/abs/2510.02306)
*Raphael Tang,Crystina Zhang,Wenyan Li,Carmen Lai,Pontus Stenetorp,Yao Lu*

Main category: cs.CL

TL;DR: The paper challenges the conventional use of Elo rating system for LLM arena evaluations, arguing that draws indicate query difficulty rather than model equality, and shows that ignoring draws improves prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: To critically examine whether draws in LLM arena evaluations truly mean model equality, and to investigate if draws are more indicative of query difficulty.

Method: Analyzed three real-world arena datasets, tested four rating systems, and examined how ignoring rating updates for draws affects battle outcome prediction accuracy.

Result: Ignoring rating updates for draws yields 1-3% relative increase in battle outcome prediction accuracy across all four rating systems studied. Draws occur more for very easy queries (risk ratio 1.37) and highly objective queries (risk ratio 1.35).

Conclusion: Future rating systems should reconsider existing draw semantics and account for query properties in rating updates, as draws are more indicative of query difficulty than model equality.

Abstract: In arena-style evaluation of large language models (LLMs), two LLMs respond
to a user query, and the user chooses the winning response or deems the
"battle" a draw, resulting in an adjustment to the ratings of both models. The
prevailing approach for modeling these rating dynamics is to view battles as
two-player game matches, as in chess, and apply the Elo rating system and its
derivatives. In this paper, we critically examine this paradigm. Specifically,
we question whether a draw genuinely means that the two models are equal and
hence whether their ratings should be equalized. Instead, we conjecture that
draws are more indicative of query difficulty: if the query is too easy, then
both models are more likely to succeed equally. On three real-world arena
datasets, we show that ignoring rating updates for draws yields a 1-3% relative
increase in battle outcome prediction accuracy (which includes draws) for all
four rating systems studied. Further analyses suggest that draws occur more for
queries rated as very easy and those as highly objective, with risk ratios of
1.37 and 1.35, respectively. We recommend future rating systems to reconsider
existing draw semantics and to account for query properties in rating updates.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [91] [Accelerating Long-Term Molecular Dynamics with Physics-Informed Time-Series Forecasting](https://arxiv.org/abs/2510.01206)
*Hung Le,Sherif Abbas,Minh Hoang Nguyen,Van Dai Do,Huu Hiep Nguyen,Dung Nguyen*

Main category: cs.LG

TL;DR: Proposes a novel MD simulation approach using time-series forecasting with physics-informed constraints, enabling efficient and accurate atomic trajectory prediction.


<details>
  <summary>Details</summary>
Motivation: Traditional DFT methods are computationally expensive for long-term MD simulations, limiting their practical feasibility in materials science and biophysics.

Method: Formulates MD simulation as time-series forecasting problem, uses displacement-based trajectory prediction with physics-informed loss and DFT-parametrized Morse potential constraints.

Result: Consistently surpasses standard baselines in simulation accuracy across diverse materials, enables stable modeling of thousands of MD steps in minutes.

Conclusion: Physics-informed forecasting provides a scalable alternative to costly DFT simulations, enhancing reliability and precision of atomic trajectory prediction.

Abstract: Efficient molecular dynamics (MD) simulation is vital for understanding
atomic-scale processes in materials science and biophysics. Traditional density
functional theory (DFT) methods are computationally expensive, which limits the
feasibility of long-term simulations. We propose a novel approach that
formulates MD simulation as a time-series forecasting problem, enabling
advanced forecasting models to predict atomic trajectories via displacements
rather than absolute positions. We incorporate a physics-informed loss and
inference mechanism based on DFT-parametrised pair-wise Morse potential
functions that penalize unphysical atomic proximity to enforce physical
plausibility. Our method consistently surpasses standard baselines in
simulation accuracy across diverse materials. The results highlight the
importance of incorporating physics knowledge to enhance the reliability and
precision of atomic trajectory forecasting. Remarkably, it enables stable
modeling of thousands of MD steps in minutes, offering a scalable alternative
to costly DFT simulations.

</details>


### [92] [Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs](https://arxiv.org/abs/2510.01218)
*Sergey Troshin,Wafaa Mohammed,Yan Meng,Christof Monz,Antske Fokkens,Vlad Niculae*

Main category: cs.LG

TL;DR: Selective sampling method that dynamically switches between greedy and high-temperature sampling based on sampling risk to improve quality-diversity trade-off in mathematical reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Temperature-based sampling increases diversity but degrades reasoning quality in precision-required tasks like mathematical reasoning due to sampling incorrect continuations in sensitive positions.

Method: Propose selective sampling with a sampling risk metric that estimates error likelihood when using high-temperature sampling. Train lightweight classifier on verifiable problems to predict sampling risk with minimal latency overhead.

Result: Experiments on mathematical reasoning tasks show selective sampling enhances quality-diversity trade-off even in high-temperature settings.

Conclusion: Selective sampling effectively addresses the accuracy loss caused by uncontrolled high-temperature sampling while maintaining diversity in precision-critical tasks.

Abstract: Diversity is an essential metric for evaluating the creativity of outputs
generated by language models. Temperature-based sampling is a common strategy
to increase diversity. However, for tasks that require high precision, e.g.,
mathematical reasoning, uncontrolled high temperature sampling, e.g., min-$p$
or top-$p$, degrades reasoning quality. We demonstrate that the loss of
accuracy is caused by sampling incorrect continuations in sensitive decoding
positions. To address this, in this paper, we propose \textbf{selective
sampling}, a method that dynamically switches between greedy and
high-temperature sampling based on a sampling risk metric. This risk metric
estimates the likelihood of output errors when applying high-temperature
sampling on the current token position. To predict sampling risk, we train a
lightweight classifier on a small subset of verifiable problems. The trained
classifier can be integrated with the base language model with minimal latency
overhead. Experiments on mathematical reasoning tasks demonstrate that
selective sampling enhances the quality-diversity trade-off, even in
high-temperature settings.

</details>


### [93] [Automated Extraction of Material Properties using LLM-based AI Agents](https://arxiv.org/abs/2510.01235)
*Subham Ghosh,Abhishek Tewari*

Main category: cs.LG

TL;DR: LLM-driven workflow extracts thermoelectric and structural properties from 10,000 scientific articles, creating the largest LLM-curated thermoelectric dataset with 27,822 records.


<details>
  <summary>Details</summary>
Motivation: Address the constraint in materials discovery due to lack of large, machine-readable datasets that couple performance metrics with structural context, as existing databases are small, manually curated, or biased toward first principles results.

Method: Agentic LLM-driven workflow integrating dynamic token allocation, zero-shot multi-agent extraction, and conditional table parsing to balance accuracy against computational cost.

Result: Created 27,822 temperature-resolved property records with normalized units, achieving F1 scores of 0.91 for thermoelectric properties and 0.82 for structural fields using GPT-4.1.

Conclusion: Provides the largest LLM-curated thermoelectric dataset, establishes a reproducible extraction pipeline, and creates foundation for scalable data-driven materials discovery beyond thermoelectrics.

Abstract: The rapid discovery of materials is constrained by the lack of large,
machine-readable datasets that couple performance metrics with structural
context. Existing databases are either small, manually curated, or biased
toward first principles results, leaving experimental literature
underexploited. We present an agentic, large language model (LLM)-driven
workflow that autonomously extracts thermoelectric and structural-properties
from about 10,000 full-text scientific articles. The pipeline integrates
dynamic token allocation, zeroshot multi-agent extraction, and conditional
table parsing to balance accuracy against computational cost. Benchmarking on
50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91
for thermoelectric properties and 0.82 for structural fields), while GPT-4.1
Mini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction
of the cost, enabling practical large scale deployment. Applying this workflow,
we curated 27,822 temperature resolved property records with normalized units,
spanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity,
power factor, and thermal conductivity, together with structural attributes
such as crystal class, space group, and doping strategy. Dataset analysis
reproduces known thermoelectric trends, such as the superior performance of
alloys over oxides and the advantage of p-type doping, while also surfacing
broader structure-property correlations. To facilitate community access, we
release an interactive web explorer with semantic filters, numeric queries, and
CSV export. This study delivers the largest LLM-curated thermoelectric dataset
to date, provides a reproducible and cost-profiled extraction pipeline, and
establishes a foundation for scalable, data-driven materials discovery beyond
thermoelectrics.

</details>


### [94] [RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models](https://arxiv.org/abs/2510.01240)
*Zukang Xu,Xing Hu,Qiang Wu,Dawei Yang*

Main category: cs.LG

TL;DR: RSAVQ is a novel vector quantization framework for extremely low-bit LLM compression that uses geometry-driven innovations to address direction error and bit allocation challenges, achieving superior performance in 2-bit quantization.


<details>
  <summary>Details</summary>
Motivation: LLMs face deployment challenges on resource-constrained devices due to exponentially increasing parameters. Existing VQ methods suffer from unconstrained direction error and suboptimal bit allocation in low-bit quantization.

Method: RSAVQ introduces two key innovations: 1) Error Direction Sensitivity Guidance (EDSG) using Fisher Information Matrix-induced Riemannian metric to project quantization errors along negative natural gradient direction; 2) Weight Channel Sensitivity Guidance (WCSG) using FIM curvature analysis for dynamic bit allocation.

Result: RSAVQ outperforms existing methods, achieving 0.4 lower perplexity and 1.5 higher zero-shot accuracy in 2-bit quantization of LLaMA-3 8B compared to baselines like VPTQ and QuIP#.

Conclusion: The work provides a practical solution for constrained environments and establishes a theoretical bridge between information geometry and neural network quantization, advancing efficient deep learning.

Abstract: Large language models (LLMs) have demonstrated remarkable performance across
a wide range of natural language processing tasks. However, their exponentially
increasing parameters pose significant challenges for deployment on
resource-constrained devices. Vector Quantization (VQ) shows great promise for
low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key
challenges: unconstrained direction error and suboptimal bit allocation. In
this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit
quantization for LLMs. RSAVQ introduces two geometry-driven innovations that
effectively mitigate above limitations: (1) Error Direction Sensitivity
Guidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced
Riemannian metric to project quantization errors onto low-sensitivity
directions in the parameter space. Specifically, this projection is performed
along the negative natural gradient direction, which effectively suppresses
error expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which
constructs a channel-wise sensitivity metric via FIM curvature analysis to
dynamically guide bit resource allocation. The approach facilitates a globally
optimal quantization solution within prescribed bit constraints. Experiments
demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in
2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by
0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a
practical solution for constrained environments and a theoretical bridge
between information geometry and the quantization of neural networks, advancing
efficient deep learning.

</details>


### [95] [Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks](https://arxiv.org/abs/2510.01261)
*Vedant Palit*

Main category: cs.LG

TL;DR: A trust-aware Deep Q-Network defense for federated learning that integrates multi-signal evidence to mitigate poisoning and backdoor attacks under partial observability, achieving optimal robustness-accuracy trade-off.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to poisoning and backdoor attacks under partial observability, requiring effective defense mechanisms.

Method: Formulate defense as a partially observable sequential decision problem and introduce a trust-aware Deep Q-Network that integrates multi-signal evidence into client trust updates while optimizing long-horizon robustness-accuracy objective.

Result: On CIFAR-10: (i) baseline shows steadily improving accuracy; (ii) increased client overlap improves accuracy and reduces ASR with stable detection; (iii) accuracy remains steady while ASR increases and ROC-AUC declines as observability reduces; DQN achieves best robustness-accuracy trade-off compared to random, linear-Q, and policy gradient controllers.

Conclusion: The trust-aware DQN approach effectively mitigates federated learning vulnerabilities through sequential belief updates and achieves superior robustness-accuracy balance.

Abstract: Federated learning is vulnerable to poisoning and backdoor attacks under
partial observability. We formulate defence as a partially observable
sequential decision problem and introduce a trust-aware Deep Q-Network that
integrates multi-signal evidence into client trust updates while optimizing a
long-horizon robustness--accuracy objective. On CIFAR-10, we (i) establish a
baseline showing steadily improving accuracy, (ii) show through a Dirichlet
sweep that increased client overlap consistently improves accuracy and reduces
ASR with stable detection, and (iii) demonstrate in a signal-budget study that
accuracy remains steady while ASR increases and ROC-AUC declines as
observability is reduced, which highlights that sequential belief updates
mitigate weaker signals. Finally, a comparison with random, linear-Q, and
policy gradient controllers confirms that DQN achieves the best
robustness--accuracy trade-off.

</details>


### [96] [RSTGCN: Railway-centric Spatio-Temporal Graph Convolutional Network for Train Delay Prediction](https://arxiv.org/abs/2510.01262)
*Koyena Chowdhury,Paramita Koley,Abhijnan Chakraborty,Saptarshi Ghosh*

Main category: cs.LG

TL;DR: The paper proposes RSTGCN, a novel spatio-temporal graph neural network for predicting average train arrival delays at railway stations, and releases the largest railway dataset for the Indian Railway Network.


<details>
  <summary>Details</summary>
Motivation: Accurate train delay prediction is critical for efficient railway operations and scheduling decisions. While previous work focused on individual train delays, there's a need for station-level delay prediction to support higher-level traffic management.

Method: Proposed Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN) with architectural innovations including train frequency-aware spatial attention and novel feature integrations to forecast average arrival delays at stations.

Result: Extensive experiments show consistent improvements across standard metrics compared to multiple state-of-the-art baselines. The method significantly enhances predictive performance.

Conclusion: The work advances average delay prediction modeling in large-scale rail networks and provides an open dataset (covering 4,735 stations across 17 zones) to encourage further research in railway delay prediction.

Abstract: Accurate prediction of train delays is critical for efficient railway
operations, enabling better scheduling and dispatching decisions. While earlier
approaches have largely focused on forecasting the exact delays of individual
trains, recent studies have begun exploring station-level delay prediction to
support higher-level traffic management. In this paper, we propose the
Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN), designed
to forecast average arrival delays of all the incoming trains at railway
stations for a particular time period. Our approach incorporates several
architectural innovations and novel feature integrations, including train
frequency-aware spatial attention, which significantly enhances predictive
performance. To support this effort, we curate and release a comprehensive
dataset for the entire Indian Railway Network (IRN), spanning 4,735 stations
across 17 zones - the largest and most diverse railway network studied to date.
We conduct extensive experiments using multiple state-of-the-art baselines,
demonstrating consistent improvements across standard metrics. Our work not
only advances the modeling of average delay prediction in large-scale rail
networks but also provides an open dataset to encourage further research in
this critical domain.

</details>


### [97] [Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency](https://arxiv.org/abs/2510.01263)
*Yaron Meirovitch,Fuming Yang,Jeff Lichtman,Nir Shavit*

Main category: cs.LG

TL;DR: Budgeted Broadcast (BB) is a pruning method that uses local traffic budgets to maximize coding entropy, achieving better accuracy and efficiency than traditional pruning methods across various neural network architectures.


<details>
  <summary>Details</summary>
Motivation: Traditional pruning methods remove parameters based on loss impact metrics like magnitude or gradient, but these approaches may not optimize for network efficiency and representation diversity.

Method: BB assigns each unit a local traffic budget (product of long-term on-rate and fan-out), uses constrained-entropy analysis to derive a selectivity-audience balance equation, and enforces this balance with local actuators that prune either fan-in or fan-out connections.

Result: BB increases coding entropy and decorrelation, improves accuracy at matched sparsity levels across Transformers for ASR, ResNets for face identification, and 3D U-Nets for synapse prediction. On electron microscopy images, it achieves state-of-the-art F1 and PR-AUC scores.

Conclusion: BB is easy to integrate and provides a path toward learning more diverse and efficient representations through principled pruning based on traffic budgets and entropy maximization.

Abstract: Most pruning methods remove parameters ranked by impact on loss (e.g.,
magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each
unit a local traffic budget (the product of its long-term on-rate $a_i$ and
fan-out $k_i$). A constrained-entropy analysis shows that maximizing coding
entropy under a global traffic budget yields a selectivity-audience balance,
$\log\frac{1-a_i}{a_i}=\beta k_i$. BB enforces this balance with simple local
actuators that prune either fan-in (to lower activity) or fan-out (to reduce
broadcast). In practice, BB increases coding entropy and decorrelation and
improves accuracy at matched sparsity across Transformers for ASR, ResNets for
face identification, and 3D U-Nets for synapse prediction, sometimes exceeding
dense baselines. On electron microscopy images, it attains state-of-the-art F1
and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests
a path toward learning more diverse and efficient representations.

</details>


### [98] [A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab](https://arxiv.org/abs/2510.01264)
*Isaac Peterson,Christopher Allred,Jacob Morrey,Mario Harper*

Main category: cs.LG

TL;DR: Extends IsaacLab framework for adversarial MARL training in physics simulations, introducing competitive environments with heterogeneous agents and asymmetric goals.


<details>
  <summary>Details</summary>
Motivation: Address the gap in MARL research by focusing on adversarial interactions critical for real-world applications like pursuit-evasion, security, and competitive manipulation.

Method: Develops a competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal Policy Optimization (HAPPO) integrated into IsaacLab framework for scalable adversarial policy training.

Result: Successfully demonstrates framework's ability to model and train robust policies for morphologically diverse multi-agent competition while maintaining high throughput and simulation realism.

Conclusion: The extended IsaacLab framework provides an effective platform for scalable adversarial MARL training with heterogeneous agents, enabling robust policy development for competitive multi-agent scenarios.

Abstract: Multi-Agent Reinforcement Learning (MARL) is central to robotic systems
cooperating in dynamic environments. While prior work has focused on these
collaborative settings, adversarial interactions are equally critical for
real-world applications such as pursuit-evasion, security, and competitive
manipulation. In this work, we extend the IsaacLab framework to support
scalable training of adversarial policies in high-fidelity physics simulations.
We introduce a suite of adversarial MARL environments featuring heterogeneous
agents with asymmetric goals and capabilities. Our platform integrates a
competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal
Policy Optimization (HAPPO), enabling efficient training and evaluation under
adversarial dynamics. Experiments across several benchmark scenarios
demonstrate the framework's ability to model and train robust policies for
morphologically diverse multi-agent competition while maintaining high
throughput and simulation realism. Code and benchmarks are available at:
https://github.com/DIRECTLab/IsaacLab-HARL .

</details>


### [99] [RLP: Reinforcement as a Pretraining Objective](https://arxiv.org/abs/2510.01265)
*Ali Hatamizadeh,Syeda Nahida Akter,Shrimai Prabhumoye,Jan Kautz,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Yejin Choi*

Main category: cs.LG

TL;DR: RLP introduces reinforcement learning during pretraining by treating chain-of-thought as exploratory actions, with rewards based on information gain for predicting future tokens, improving reasoning capabilities without requiring verifiers.


<details>
  <summary>Details</summary>
Motivation: Current training paradigm delays reinforcement learning to post-training phase, missing opportunities to teach independent thinking behavior earlier in pretraining.

Method: RLP uses information-driven reinforcement pretraining where chain-of-thought reasoning is treated as exploratory actions, with rewards computed from the increase in log-likelihood of next tokens when conditioning on reasoning chains vs. context alone.

Result: RLP pretraining on Qwen3-1.7B-Base improved average performance across eight math-and-science benchmarks by 19%, with largest gains on reasoning-heavy tasks like AIME25 and MMLU-Pro. Applied to Nemotron-Nano-12B-v2, it increased overall average from 42.81% to 61.32% and scientific reasoning by 23%.

Conclusion: RLP successfully bridges the gap between next-token prediction and chain-of-thought reasoning emergence, demonstrating that reinforcement learning can be effectively integrated during pretraining to enhance reasoning capabilities across different architectures and model sizes.

Abstract: The dominant paradigm for training large reasoning models starts with
pre-training using next-token prediction loss on vast amounts of data.
Reinforcement learning, while powerful in scaling reasoning, is introduced only
as the very last phase of post-training, preceded by supervised fine-tuning.
While dominant, is this an optimal way of training? In this paper, we present
RLP, an information-driven reinforcement pretraining objective, that brings the
core spirit of reinforcement learning -- exploration -- to the last phase of
pretraining. The key idea is to treat chain-of-thought as an exploratory
action, with rewards computed based on the information gain it provides for
predicting future tokens. This training objective essentially encourages the
model to think for itself before predicting what comes next, thus teaching an
independent thinking behavior earlier in the pretraining. More concretely, the
reward signal measures the increase in log-likelihood of the next token when
conditioning on both context and a sampled reasoning chain, compared to
conditioning on context alone. This approach yields a verifier-free dense
reward signal, allowing for efficient training for the full document stream
during pretraining. Specifically, RLP reframes reinforcement learning for
reasoning as a pretraining objective on ordinary text, bridging the gap between
next-token prediction and the emergence of useful chain-of-thought reasoning.
Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an
eight-benchmark math-and-science suite by 19%. With identical post-training,
the gains compound, with the largest improvements on reasoning-heavy tasks such
as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2
increases the overall average from 42.81% to 61.32% and raises the average on
scientific reasoning by 23%, demonstrating scalability across architectures and
model sizes.

</details>


### [100] [Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance](https://arxiv.org/abs/2510.01269)
*Rohan Vitthal Thorat,Juhi Singh,Rajdip Nayek*

Main category: cs.LG

TL;DR: Proposes a hybrid LQR-RL framework for structural vibration control that eliminates model dependency while ensuring training safety by using LQR to guide RL during initial learning phases.


<details>
  <summary>Details</summary>
Motivation: To address safety risks during RL controller training on physical structures, where random control actions could damage the system, while maintaining model-free operation.

Method: Hybrid control combining LQR and RL controllers, where LQR uses randomly selected model parameters to provide safe guidance during RL training, eliminating need for accurate system identification.

Result: The framework achieves model-free vibration control while minimizing exploration risks, with LQR based on incorrect models still outperforming uncontrolled scenarios.

Conclusion: First validated solution addressing RL training safety in vibration control, providing a practical model-free approach that ensures structural safety during controller learning.

Abstract: Structural vibrations induced by external excitations pose significant risks,
including safety hazards for occupants, structural damage, and increased
maintenance costs. While conventional model-based control strategies, such as
Linear Quadratic Regulator (LQR), effectively mitigate vibrations, their
reliance on accurate system models necessitates tedious system identification.
This tedious system identification process can be avoided by using a model-free
Reinforcement learning (RL) method. RL controllers derive their policies solely
from observed structural behaviour, eliminating the requirement for an explicit
structural model. For an RL controller to be truly model-free, its training
must occur on the actual physical system rather than in simulation. However,
during this training phase, the RL controller lacks prior knowledge and it
exerts control force on the structure randomly, which can potentially harm the
structure. To mitigate this risk, we propose guiding the RL controller using a
Linear Quadratic Regulator (LQR) controller. While LQR control typically relies
on an accurate structural model for optimal performance, our observations
indicate that even an LQR controller based on an entirely incorrect model
outperforms the uncontrolled scenario. Motivated by this finding, we introduce
a hybrid control framework that integrates both LQR and RL controllers. In this
approach, the LQR policy is derived from a randomly selected model and its
parameters. As this LQR policy does not require knowledge of the true or an
approximate structural model the overall framework remains model-free. This
hybrid approach eliminates dependency on explicit system models while
minimizing exploration risks inherent in naive RL implementations. As per our
knowledge, this is the first study to address the critical training safety
challenge of RL-based vibration control and provide a validated solution.

</details>


### [101] [Identifying Information-Transfer Nodes in a Recurrent Neural Network Reveals Dynamic Representations](https://arxiv.org/abs/2510.01271)
*Arend Hintze,Asadullah Najam,Jory Schossau*

Main category: cs.LG

TL;DR: The paper introduces an information-theoretic method to identify information-transfer nodes (information relays) in RNNs by quantifying mutual information between input and output vectors, revealing critical information pathways in various RNN architectures.


<details>
  <summary>Details</summary>
Motivation: Understanding the internal dynamics of RNNs is crucial for advancing their interpretability and improving their design, as current methods lack clear insights into how information flows through these networks.

Method: An innovative information-theoretic approach that quantifies mutual information between input and output vectors across nodes to identify information relays, applied to synthetic and real-world time series classification tasks using LSTM and GRU architectures, with node knockout experiments to assess functional importance.

Result: The method reveals distinct patterns of information relay across different RNN architectures, showing how information is processed and maintained over time, and identifies critical nodes that significantly influence network behavior.

Conclusion: This study enhances understanding of RNN mechanisms and provides a valuable tool for designing more robust and interpretable neural networks, contributing significantly to explainable AI by elucidating how specific nodes influence overall network behavior.

Abstract: Understanding the internal dynamics of Recurrent Neural Networks (RNNs) is
crucial for advancing their interpretability and improving their design. This
study introduces an innovative information-theoretic method to identify and
analyze information-transfer nodes within RNNs, which we refer to as
\textit{information relays}. By quantifying the mutual information between
input and output vectors across nodes, our approach pinpoints critical pathways
through which information flows during network operations. We apply this
methodology to both synthetic and real-world time series classification tasks,
employing various RNN architectures, including Long Short-Term Memory (LSTM)
networks and Gated Recurrent Units (GRUs). Our results reveal distinct patterns
of information relay across different architectures, offering insights into how
information is processed and maintained over time. Additionally, we conduct
node knockout experiments to assess the functional importance of identified
nodes, significantly contributing to explainable artificial intelligence by
elucidating how specific nodes influence overall network behavior. This study
not only enhances our understanding of the complex mechanisms driving RNNs but
also provides a valuable tool for designing more robust and interpretable
neural networks.

</details>


### [102] [Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning](https://arxiv.org/abs/2510.01278)
*Hengwei Zhao,Zhengzhong Tu,Zhuo Zheng,Wei Wang,Junjue Wang,Rusty Feagin,Wenzhe Jiao*

Main category: cs.LG

TL;DR: NcPU is a non-contrastive PU learning framework that addresses representation learning challenges in Positive-Unlabeled classification by combining noisy-pair robust supervised non-contrastive loss with phantom label disambiguation, achieving state-of-the-art performance without auxiliary information.


<details>
  <summary>Details</summary>
Motivation: Current PU learning methods significantly underperform supervised methods on complex datasets (e.g., 14.26% gap on CIFAR-100) due to challenges in learning discriminative representations under unreliable supervision, especially without auxiliary negatives or pre-estimated parameters.

Method: Proposes NcPU framework with two key components: 1) NoiSNCL - noisy-pair robust supervised non-contrastive loss that aligns intra-class representations despite unreliable supervision; 2) PLD - phantom label disambiguation scheme that provides conservative negative supervision via regret-based label updates. Theoretically, these components iteratively benefit each other under the Expectation-Maximization framework.

Result: Extensive experiments show: 1) NoiSNCL enables simple PU methods to achieve competitive performance; 2) NcPU achieves substantial improvements over state-of-the-art PU methods across diverse datasets, including challenging post-disaster building damage mapping applications.

Conclusion: NcPU effectively addresses the representation learning bottleneck in PU learning, demonstrating strong performance on complex datasets and real-world applications without requiring auxiliary information.

Abstract: Positive-Unlabeled (PU) learning aims to train a binary classifier (positive
vs. negative) where only limited positive data and abundant unlabeled data are
available. While widely applicable, state-of-the-art PU learning methods
substantially underperform their supervised counterparts on complex datasets,
especially without auxiliary negatives or pre-estimated parameters (e.g., a
14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the
challenge of learning discriminative representations under unreliable
supervision. To tackle this challenge, we propose NcPU, a non-contrastive PU
learning framework that requires no auxiliary information. NcPU combines a
noisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns
intra-class representations despite unreliable supervision, with a phantom
label disambiguation (PLD) scheme that supplies conservative negative
supervision via regret-based label updates. Theoretically, NoiSNCL and PLD can
iteratively benefit each other from the perspective of the
Expectation-Maximization framework. Empirically, extensive experiments
demonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive
performance; and (2) NcPU achieves substantial improvements over
state-of-the-art PU methods across diverse datasets, including challenging
datasets on post-disaster building damage mapping, highlighting its promise for
real-world applications. Code: Code will be open-sourced after review.

</details>


### [103] [Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours](https://arxiv.org/abs/2510.01288)
*Rui Melo,Rui Abreu,Corina S. Pasareanu*

Main category: cs.LG

TL;DR: The paper proposes a microsaccade-inspired probing method using lightweight position encoding perturbations to detect LLM misbehaviors without fine-tuning or supervision.


<details>
  <summary>Details</summary>
Motivation: Drawing inspiration from microsaccades in human vision that reveal perceptual dynamics, the authors aim to develop an analogous method to expose hidden failure signals in LLMs.

Method: Uses lightweight position encoding perturbations as probes to elicit latent signals indicating model misbehavior, requiring no fine-tuning or task-specific supervision.

Result: Successfully detects failures across diverse settings including factuality, safety, toxicity, and backdoor attacks in multiple state-of-the-art LLMs, while remaining computationally efficient.

Conclusion: Pretrained LLMs already encode internal evidence to flag their own failures, and microsaccade-inspired interventions provide an effective pathway for detecting and mitigating undesirable behaviors.

Abstract: We draw inspiration from microsaccades, tiny involuntary eye movements that
reveal hidden dynamics of human perception, to propose an analogous probing
method for large language models (LLMs). Just as microsaccades expose subtle
but informative shifts in vision, we show that lightweight position encoding
perturbations elicit latent signals that indicate model misbehaviour. Our
method requires no fine-tuning or task-specific supervision, yet detects
failures across diverse settings including factuality, safety, toxicity, and
backdoor attacks. Experiments on multiple state-of-the-art LLMs demonstrate
that these perturbation-based probes surface misbehaviours while remaining
computationally efficient. These findings suggest that pretrained LLMs already
encode the internal evidence needed to flag their own failures, and that
microsaccade-inspired interventions provide a pathway for detecting and
mitigating undesirable behaviours.

</details>


### [104] [ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models](https://arxiv.org/abs/2510.01290)
*Akshat Ramachandran,Marina Neseem,Charbel Sakr,Rangharajan Venkatesan,Brucek Khailany,Tushar Krishna*

Main category: cs.LG

TL;DR: ThinKV is a KV cache compression framework that uses attention sparsity to identify important thoughts in chain-of-thought reasoning, applying hybrid quantization-eviction to reduce KV cache to <5% of original size while maintaining near-lossless accuracy and improving throughput up to 5.8x.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models generate long CoT outputs that cause KV cache to grow rapidly and overwhelm GPU memory, creating a memory bottleneck for inference.

Method: Uses attention sparsity to identify thought types with varying importance, applies hybrid quantization-eviction strategy based on thought importance, and implements efficient kernel with PagedAttention extension for memory slot reuse.

Result: Achieves near-lossless accuracy with <5% of original KV cache, improves inference throughput up to 5.8x over SOTA baselines on mathematics and coding benchmarks.

Conclusion: ThinKV effectively addresses KV cache memory bottleneck through thought-adaptive compression, enabling efficient long-context reasoning with minimal accuracy loss.

Abstract: The long-output context generation of large reasoning models enables extended
chain of thought (CoT) but also drives rapid growth of the key-value (KV)
cache, quickly overwhelming GPU memory. To address this challenge, we propose
ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on
the observation that attention sparsity reveals distinct thought types with
varying importance within the CoT. It applies a hybrid quantization-eviction
strategy, assigning token precision by thought importance and progressively
evicting tokens from less critical thoughts as reasoning trajectories evolve.
Furthermore, to implement ThinKV, we design a kernel that extends
PagedAttention to enable efficient reuse of evicted tokens' memory slots,
eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,
GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show
that ThinKV achieves near-lossless accuracy with less than 5% of the original
KV cache, while improving performance with up to 5.8x higher inference
throughput over state-of-the-art baselines.

</details>


### [105] [Network-Level Vehicle Delay Estimation at Heterogeneous Signalized Intersections](https://arxiv.org/abs/2510.01292)
*Xiaobo Ma,Hyunsoo Noh,James Tokishi,Ryan Hatch*

Main category: cs.LG

TL;DR: This paper introduces a domain adaptation framework using Gradient Boosting with Balanced Weighting (GBBW) to improve vehicle delay estimation across diverse intersections by addressing distribution shifts between training and testing data.


<details>
  <summary>Details</summary>
Motivation: Conventional ML models for vehicle delay estimation assume same data distribution between training and testing, which rarely holds in real-world due to variations in road geometry, signal timing, and driver behavior across intersections, leading to poor generalization.

Method: Proposes a domain adaptation framework that separates data into source/target domains, extracts traffic features, and fine-tunes using small labeled target data. Introduces GBBW model that reweights source data based on similarity to target domain.

Result: Tested on 57 heterogeneous intersections in Pima County, Arizona. GBBW outperformed 8 state-of-the-art ML regression models and 7 instance-based domain adaptation methods, providing more accurate and robust delay estimates.

Conclusion: The framework enhances model transferability, supporting more reliable traffic signal optimization, congestion management, and broader deployment of ML techniques in real-world transportation systems.

Abstract: Accurate vehicle delay estimation is essential for evaluating the performance
of signalized intersections and informing traffic management strategies. Delay
reflects congestion levels and affects travel time reliability, fuel use, and
emissions. Machine learning (ML) offers a scalable, cost-effective alternative;
However, conventional models typically assume that training and testing data
follow the same distribution, an assumption that is rarely satisfied in
real-world applications. Variations in road geometry, signal timing, and driver
behavior across intersections often lead to poor generalization and reduced
model accuracy. To address this issue, this study introduces a domain
adaptation (DA) framework for estimating vehicle delays across diverse
intersections. The framework separates data into source and target domains,
extracts key traffic features, and fine-tunes the model using a small, labeled
subset from the target domain. A novel DA model, Gradient Boosting with
Balanced Weighting (GBBW), reweights source data based on similarity to the
target domain, improving adaptability. The framework is tested using data from
57 heterogeneous intersections in Pima County, Arizona. Performance is
evaluated against eight state-of-the-art ML regression models and seven
instance-based DA methods. Results demonstrate that the GBBW framework provides
more accurate and robust delay estimates. This approach supports more reliable
traffic signal optimization, congestion management, and performance-based
planning. By enhancing model transferability, the framework facilitates broader
deployment of machine learning techniques in real-world transportation systems.

</details>


### [106] [From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review](https://arxiv.org/abs/2510.01296)
*Emma McMillian,Abhirup Banerjee,Alfonso Bueno-Orovio*

Main category: cs.LG

TL;DR: This review paper surveys deep learning methods for 3D shape reconstruction from 2D MRI, covering four main approaches: point cloud, mesh-based, shape-aware, and volumetric models, with analysis of techniques, limitations, applications, datasets, and future directions.


<details>
  <summary>Details</summary>
Motivation: 3D shape reconstruction from 2D MRI is crucial for medical diagnosis, treatment planning, and computational modeling, but requires comprehensive understanding of current methodologies and their clinical applicability.

Method: The review systematically analyzes four primary reconstruction approaches, examining state-of-the-art techniques, methodological foundations, limitations, applications across anatomical structures, datasets, computational demands, and evaluation metrics.

Result: The paper provides a structured overview of current 3D reconstruction methodologies, highlighting their performance across different anatomical regions and clinical scenarios, including diseased anatomy applications.

Conclusion: The review identifies opportunities for advancing deep learning towards more robust, generalizable, and clinically impactful 3D reconstruction solutions, with emerging directions including multimodal integration and cross-modality frameworks.

Abstract: Deep learning-based 3-dimensional (3D) shape reconstruction from
2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly
important in medical disease diagnosis, treatment planning, and computational
modeling. This review surveys the methodological landscape of 3D MRI
reconstruction, focusing on 4 primary approaches: point cloud, mesh-based,
shape-aware, and volumetric models. For each category, we analyze the current
state-of-the-art techniques, their methodological foundation, limitations, and
applications across anatomical structures. We provide an extensive overview
ranging from cardiac to neurological to lung imaging. We also focus on the
clinical applicability of models to diseased anatomy, and the influence of
their training and testing data. We examine publicly available datasets,
computational demands, and evaluation metrics. Finally, we highlight the
emerging research directions including multimodal integration and
cross-modality frameworks. This review aims to provide researchers with a
structured overview of current 3D reconstruction methodologies to identify
opportunities for advancing deep learning towards more robust, generalizable,
and clinically impactful solutions.

</details>


### [107] [Low Rank Gradients and Where to Find Them](https://arxiv.org/abs/2510.01303)
*Rishi Sonthalia,Michael Murray,Guido MontÃºfar*

Main category: cs.LG

TL;DR: This paper shows that gradients in two-layer neural networks have low-rank structure dominated by two rank-one components: one aligned with bulk data-residue and another with data spikes, with their balance controlled by data properties, scaling regimes, and activation functions.


<details>
  <summary>Details</summary>
Motivation: To understand gradient structure in neural networks without relying on restrictive isotropy assumptions about data and parameters, and to investigate how different factors affect gradient composition.

Method: Theoretical analysis using spiked data models with anisotropic bulk, examining both mean-field and neural-tangent-kernel scalings, and experimental validation on synthetic and real data.

Result: Gradient with respect to input weights is approximately low-rank and dominated by two rank-one terms aligned with bulk data-residue and data spikes, with their relative importance governed by data properties, scaling, and activation functions.

Conclusion: Standard regularizers like weight decay, input noise, and Jacobian penalties selectively modulate the two gradient components, providing insights into gradient structure and regularization effects in neural network training.

Abstract: This paper investigates low-rank structure in the gradients of the training
loss for two-layer neural networks while relaxing the usual isotropy
assumptions on the training data and parameters. We consider a spiked data
model in which the bulk can be anisotropic and ill-conditioned, we do not
require independent data and weight matrices and we also analyze both the
mean-field and neural-tangent-kernel scalings. We show that the gradient with
respect to the input weights is approximately low rank and is dominated by two
rank-one terms: one aligned with the bulk data-residue , and another aligned
with the rank one spike in the input data. We characterize how properties of
the training data, the scaling regime and the activation function govern the
balance between these two components. Additionally, we also demonstrate that
standard regularizers, such as weight decay, input noise and Jacobian
penalties, also selectively modulate these components. Experiments on synthetic
and real data corroborate our theoretical predictions.

</details>


### [108] [Quantum-inspired Benchmark for Estimating Intrinsic Dimension](https://arxiv.org/abs/2510.01335)
*Aritra Das,Joseph T. Iosue,Victor V. Albert*

Main category: cs.LG

TL;DR: Proposes QuIIEst benchmark for intrinsic dimension estimation using quantum-inspired manifolds with known ID, showing existing methods perform worse on these complex manifolds than on traditional benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing intrinsic dimension estimation methods give varying results, and current benchmarks use overly simple manifolds, so a more complex benchmark is needed to properly evaluate these methods.

Method: Creates a benchmark using quantum-optical methods to embed arbitrary homogeneous spaces with controllable curvature and additive noise, generating infinite families of topologically non-trivial manifolds.

Result: IDE methods were generally less accurate on QuIIEst manifolds than existing benchmarks, with minimal performance degradation despite increasingly non-uniform curvature, showing the benchmark's difficulty.

Conclusion: QuIIEst provides a challenging benchmark for IDE methods, and some methods can extract effective dimension from non-manifold spaces like fractals.

Abstract: Machine learning models can generalize well on real-world datasets. According
to the manifold hypothesis, this is possible because datasets lie on a latent
manifold with small intrinsic dimension (ID). There exist many methods for ID
estimation (IDE), but their estimates vary substantially. This warrants
benchmarking IDE methods on manifolds that are more complex than those in
existing benchmarks. We propose a Quantum-Inspired Intrinsic-dimension
Estimation (QuIIEst) benchmark consisting of infinite families of topologically
non-trivial manifolds with known ID. Our benchmark stems from a quantum-optical
method of embedding arbitrary homogeneous spaces while allowing for curvature
modification and additive noise. The IDE methods tested were generally less
accurate on QuIIEst manifolds than on existing benchmarks under identical
resource allocation. We also observe minimal performance degradation with
increasingly non-uniform curvature, underscoring the benchmark's inherent
difficulty. As a result of independent interest, we perform IDE on the fractal
Hofstadter's butterfly and identify which methods are capable of extracting the
effective dimension of a space that is not a manifold.

</details>


### [109] [On the Identifiability of Latent Action Policies](https://arxiv.org/abs/2510.01337)
*SÃ©bastien Lachapelle*

Main category: cs.LG

TL;DR: The paper analyzes identifiability in latent action policy learning (LAPO), proving that an entropy-regularized objective can identify meaningful action representations under certain conditions, explaining why discrete representations work well in practice.


<details>
  <summary>Details</summary>
Motivation: To understand why latent action representations from video data work well in practice and to formally establish conditions under which such representations can be identified.

Method: The authors formally describe desiderata for action representations, analyze statistical benefits and sources of unidentifiability, and prove identifiability results for an entropy-regularized LAPO objective.

Result: The paper proves that under suitable conditions, the entropy-regularized LAPO objective identifies action representations that satisfy the established desiderata.

Conclusion: The analysis provides theoretical justification for the empirical success of discrete action representations in practice, establishing identifiability conditions for latent action policy learning.

Abstract: We study the identifiability of latent action policy learning (LAPO), a
framework introduced recently to discover representations of actions from video
data. We formally describe desiderata for such representations, their
statistical benefits and potential sources of unidentifiability. Finally, we
prove that an entropy-regularized LAPO objective identifies action
representations satisfying our desiderata, under suitable conditions. Our
analysis provides an explanation for why discrete action representations
perform well in practice.

</details>


### [110] [Self-Supervised Representation Learning as Mutual Information Maximization](https://arxiv.org/abs/2510.01345)
*Akhlaqur Rahman Sabby,Yi Sui,Tongzi Wu,Jesse C. Cresswell,Ga Wu*

Main category: cs.LG

TL;DR: This paper provides a theoretical framework that explains architectural choices in self-supervised representation learning (SSRL) by deriving two training paradigms from variational mutual information objectives.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying principles of SSRL methods, which have shown empirical success but lack theoretical explanations for their architectural components like predictor networks and stop-gradient operations.

Method: Starting from a variational mutual information lower bound, the authors derive two training paradigms: Self-Distillation MI (SDMI) requiring alternating optimization with stop-gradient, and Joint MI (JMI) allowing joint optimization with symmetric architectures.

Result: The framework shows that predictor networks in SDMI and statistical regularizers in JMI emerge as tractable surrogates for mutual information objectives, explaining many existing SSRL methods as specific instances of these paradigms.

Conclusion: The paper provides a theoretical foundation explaining why different SSRL methods use specific architectural components, moving beyond heuristic justifications to principled derivations from mutual information objectives.

Abstract: Self-supervised representation learning (SSRL) has demonstrated remarkable
empirical success, yet its underlying principles remain insufficiently
understood. While recent works attempt to unify SSRL methods by examining their
information-theoretic objectives or summarizing their heuristics for preventing
representation collapse, architectural elements like the predictor network,
stop-gradient operation, and statistical regularizer are often viewed as
empirically motivated additions. In this paper, we adopt a first-principles
approach and investigate whether the learning objective of an SSRL algorithm
dictates its possible optimization strategies and model design choices. In
particular, by starting from a variational mutual information (MI) lower bound,
we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint
MI (JMI), each imposing distinct structural constraints and covering a set of
existing SSRL algorithms. SDMI inherently requires alternating optimization,
making stop-gradient operations theoretically essential. In contrast, JMI
admits joint optimization through symmetric architectures without such
components. Under the proposed formulation, predictor networks in SDMI and
statistical regularizers in JMI emerge as tractable surrogates for the MI
objective. We show that many existing SSRL methods are specific instances or
approximations of these two paradigms. This paper provides a theoretical
explanation behind the choices of different architectural components of
existing SSRL methods, beyond heuristic conveniences.

</details>


### [111] [To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking](https://arxiv.org/abs/2510.01349)
*Hannah Lawrence,Elyssa Hofgard,Vasco Portilheiro,Yuxuan Chen,Tess Smidt,Robin Walters*

Main category: cs.LG

TL;DR: This paper proposes a metric to quantify dataset anisotropy (symmetry-breaking) and shows that distributional symmetry-breaking can prevent invariant methods from performing optimally, even when labels are truly invariant.


<details>
  <summary>Details</summary>
Motivation: To critically evaluate the assumption that transformed datapoints are "important" under test distribution, which underlies symmetry-aware methods like data augmentation and equivariant architectures.

Method: Developed a two-sample neural classifier test metric to distinguish between original datasets and their randomly augmented equivalents, validated on synthetic datasets and applied to benchmark point cloud datasets.

Result: Uncovered surprisingly high degrees of alignment in several benchmark point cloud datasets. Showed theoretically that distributional symmetry-breaking prevents invariant methods from optimal performance even with truly invariant labels. Empirical results show equivariant methods' benefits are dataset-dependent.

Conclusion: Understanding equivariance requires rethinking symmetry biases in data, as distributional symmetry-breaking can undermine the effectiveness of symmetry-aware methods.

Abstract: Symmetry-aware methods for machine learning, such as data augmentation and
equivariant architectures, encourage correct model behavior on all
transformations (e.g. rotations or permutations) of the original dataset. These
methods can improve generalization and sample efficiency, under the assumption
that the transformed datapoints are highly probable, or "important", under the
test distribution. In this work, we develop a method for critically evaluating
this assumption. In particular, we propose a metric to quantify the amount of
anisotropy, or symmetry-breaking, in a dataset, via a two-sample neural
classifier test that distinguishes between the original dataset and its
randomly augmented equivalent. We validate our metric on synthetic datasets,
and then use it to uncover surprisingly high degrees of alignment in several
benchmark point cloud datasets. We show theoretically that distributional
symmetry-breaking can actually prevent invariant methods from performing
optimally even when the underlying labels are truly invariant, as we show for
invariant ridge regression in the infinite feature limit. Empirically, we find
that the implication for symmetry-aware methods is dataset-dependent:
equivariant methods still impart benefits on some anisotropic datasets, but not
others. Overall, these findings suggest that understanding equivariance -- both
when it works, and why -- may require rethinking symmetry biases in the data.

</details>


### [112] [Bypassing Prompt Guards in Production with Controlled-Release Prompting](https://arxiv.org/abs/2510.01529)
*Jaiden Fairoze,Sanjam Garg,Keewoo Lee,Mingyuan Wang*

Main category: cs.LG

TL;DR: The paper introduces a new attack method that bypasses prompt guards in LLMs by exploiting resource asymmetry between lightweight guards and main models, successfully jailbreaking production models while maintaining response quality.


<details>
  <summary>Details</summary>
Motivation: To highlight the limitations of current prompt guard approaches in ensuring AI safety and alignment, demonstrating their vulnerability to sophisticated attacks.

Method: The attack exploits resource asymmetry by encoding jailbreak prompts that lightweight guards cannot decode but the main LLM can process, targeting production models like Google Gemini, DeepSeek Chat, Grok, and Mistral Le Chat.

Result: The method consistently jailbreaks production models while maintaining response quality, revealing critical alignment issues including copyrighted data extraction, training data extraction, and malicious response leakage during thinking.

Conclusion: Lightweight prompt guards have inherent vulnerabilities in modern LLM architectures, necessitating a shift in defense strategies from blocking malicious inputs to preventing malicious outputs.

Abstract: As large language models (LLMs) advance, ensuring AI safety and alignment is
paramount. One popular approach is prompt guards, lightweight mechanisms
designed to filter malicious queries while being easy to implement and update.
In this work, we introduce a new attack that circumvents such prompt guards,
highlighting their limitations. Our method consistently jailbreaks production
models while maintaining response quality, even under the highly protected chat
interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok
(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry
between the prompt guard and the main LLM, encoding a jailbreak prompt that
lightweight guards cannot decode but the main model can. This reveals an attack
surface inherent to lightweight prompt guards in modern LLM architectures and
underscores the need to shift defenses from blocking malicious inputs to
preventing malicious outputs. We additionally identify other critical alignment
issues, such as copyrighted data extraction, training data extraction, and
malicious response leakage during thinking.

</details>


### [113] [RheOFormer: A generative transformer model for simulation of complex fluids and flows](https://arxiv.org/abs/2510.01365)
*Maedeh Saberi,Amir Barati Farimani,Safa Jamali*

Main category: cs.LG

TL;DR: RheOFormer is a generative operator learning method using self-attention to efficiently model complex fluid flows, accurately predicting spatio-temporal evolution of nonlinear mechanics with strong generalization and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for non-Newtonian fluid dynamics are computationally demanding and poorly scalable, while existing data-driven methods require retraining across varied physical conditions.

Method: Rheological Operator Transformer (RheOFormer) leverages self-attention to learn spatial interactions and features of complex fluid flows, using generative operator learning.

Result: RheOFormer accurately learns scalar and tensorial nonlinear mechanics of complex fluids and predicts spatio-temporal flow evolution, even with limited training data, demonstrating strong generalization capabilities.

Conclusion: RheOFormer establishes as a robust neural surrogate for accelerating predictive complex fluid simulations, advancing data-driven experimentation, and enabling real-time process optimization.

Abstract: The ability to model mechanics of soft materials under flowing conditions is
key in designing and engineering processes and materials with targeted
properties. This generally requires solution of internal stress tensor, related
to the deformation tensor through nonlinear and history-dependent constitutive
models. Traditional numerical methods for non-Newtonian fluid dynamics often
suffer from prohibitive computational demands and poor scalability to new
problem instances. Developments in data-driven methods have mitigated some
limitations but still require retraining across varied physical conditions. In
this work, we introduce Rheological Operator Transformer (RheOFormer), a
generative operator learning method leveraging self-attention to efficiently
learn different spatial interactions and features of complex fluid flows. We
benchmark RheOFormer across a range of different viscometric and
non-viscometric flows with different types of viscoelastic and
elastoviscoplastic mechanics in complex domains against ground truth solutions.
Our results demonstrate that RheOFormer can accurately learn both scalar and
tensorial nonlinear mechanics of different complex fluids and predict the
spatio-temporal evolution of their flows, even when trained on limited
datasets. Its strong generalization capabilities and computational efficiency
establish RheOFormer as a robust neural surrogate for accelerating predictive
complex fluid simulations, advancing data-driven experimentation, and enabling
real-time process optimization across a wide range of applications.

</details>


### [114] [Selective Underfitting in Diffusion Models](https://arxiv.org/abs/2510.01378)
*Kiwhan Song,Jaeyeon Kim,Sitan Chen,Yilun Du,Sham Kakade,Vincent Sitzmann*

Main category: cs.LG

TL;DR: The paper introduces 'selective underfitting' as a refined perspective on how diffusion models learn score functions - they accurately approximate scores in certain regions while underfitting in others, which is essential for understanding their generalization and generative performance.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental question of which score diffusion models actually learn, since perfectly matching the empirical score would simply reproduce training data and fail to generate novel samples.

Method: Characterizing regions where diffusion models accurately approximate scores versus where they underfit, and designing empirical interventions to validate this selective underfitting perspective.

Result: Established that selective underfitting is essential for understanding diffusion models, providing new testable insights into their generalization and generative performance.

Conclusion: Selective underfitting - where diffusion models accurately approximate scores in certain regions while underfitting in others - is crucial for understanding how these models generate novel samples and achieve good performance.

Abstract: Diffusion models have emerged as the principal paradigm for generative
modeling across various domains. During training, they learn the score
function, which in turn is used to generate samples at inference. They raise a
basic yet unsolved question: which score do they actually learn? In principle,
a diffusion model that matches the empirical score in the entire data space
would simply reproduce the training data, failing to generate novel samples.
Recent work addresses this question by arguing that diffusion models underfit
the empirical score due to training-time inductive biases. In this work, we
refine this perspective, introducing the notion of selective underfitting:
instead of underfitting the score everywhere, better diffusion models more
accurately approximate the score in certain regions of input space, while
underfitting it in others. We characterize these regions and design empirical
interventions to validate our perspective. Our results establish that selective
underfitting is essential for understanding diffusion models, yielding new,
testable insights into their generalization and generative performance.

</details>


### [115] [Fine-Tuning Masked Diffusion for Provable Self-Correction](https://arxiv.org/abs/2510.01384)
*Jaeyeon Kim,Seunggeun Kim,Taekyun Lee,David Z. Pan,Hyeji Kim,Sham Kakade,Sitan Chen*

Main category: cs.LG

TL;DR: PRISM is a lightweight, model-agnostic approach for self-correction in Masked Diffusion Models that learns per-token quality scores without RL or verifier, improving inference across domains.


<details>
  <summary>Details</summary>
Motivation: Current approaches for self-correction in Masked Diffusion Models either require architectural changes or rely on imprecise proxies for token quality, limiting applicability.

Method: PRISM uses plug-in remasking to define a self-correction loss that learns per-token quality scores in the same forward pass with MDM, enabling detection of low-quality tokens.

Result: Empirical results show PRISM advances MDM inference across domains including Sudoku, unconditional text (170M), and code with LLaDA (8B).

Conclusion: PRISM provides an effective, lightweight solution for self-correction in Masked Diffusion Models without requiring model retraining or architectural changes.

Abstract: A natural desideratum for generative models is self-correction--detecting and
revising low-quality tokens at inference. While Masked Diffusion Models (MDMs)
have emerged as a promising approach for generative modeling in discrete
spaces, their capacity for self-correction remains poorly understood. Prior
attempts to incorporate self-correction into MDMs either require overhauling
MDM architectures/training or rely on imprecise proxies for token quality,
limiting their applicability. Motivated by this, we introduce PRISM--Plug-in
Remasking for Inference-time Self-correction of Masked Diffusions--a
lightweight, model-agnostic approach that applies to any pretrained MDM.
Theoretically, PRISM defines a self-correction loss that provably learns
per-token quality scores, without RL or a verifier. These quality scores are
computed in the same forward pass with MDM and used to detect low-quality
tokens. Empirically, PRISM advances MDM inference across domains and scales:
Sudoku; unconditional text (170M); and code with LLaDA (8B).

</details>


### [116] [Optimal Stopping vs Best-of-$N$ for Inference Time Optimization](https://arxiv.org/abs/2510.01394)
*Yusuf Kalayci,Vinod Raman,Shaddin Dughmi*

Main category: cs.LG

TL;DR: A new inference-time optimization framework for LLMs based on Pandora's Box problem, using UCB-style algorithms to decide when to stop generating, achieving 15-35% fewer generations while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Balance LLM output quality against inference cost when using multiple generations, addressing the trade-off between generation quality and computational expense.

Method: Developed UCB-style Pandora's Box algorithm that learns stopping thresholds adaptively, with Bradley-Terry inspired transformation for reward scaling across prompts.

Result: Achieved same performance as non-adaptive Best-of-N sampling with 15-35% fewer generations on average in AlpacaFarm and HH-RLHF datasets.

Conclusion: Established principled bridge between optimal stopping theory and inference-time scaling, providing both theoretical performance bounds and practical efficiency gains for LLM deployment.

Abstract: Large language model (LLM) generation often requires balancing output quality
against inference cost, especially when using multiple generations. We
introduce a new framework for inference-time optimization based on the
classical Pandora's Box problem. Viewing each generation as opening a costly
"box" with random reward, we develop algorithms that decide when to stop
generating without knowing the underlying reward distribution. Our first
contribution is a UCB-style Pandora's Box algorithm, which achieves performance
that is provably close to Weitzman's algorithm, the optimal strategy when the
distribution is known. We further adapt this method to practical LLM settings
by addressing reward scaling across prompts via a Bradley-Terry inspired
transformation. This leads to an adaptive inference-time optimization method
that normalizes rewards and learns stopping thresholds on the fly. Experiments
on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,
show that our adaptive strategy can obtain the same performance as non-adaptive
Best-of-N sampling while requiring 15-35 percent fewer generations on average.
Our results establish a principled bridge between optimal stopping theory and
inference-time scaling, providing both theoretical performance bounds and
practical efficiency gains for LLM deployment.

</details>


### [117] [Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems](https://arxiv.org/abs/2510.01396)
*Wasut Pornpatcharapong*

Main category: cs.LG

TL;DR: A neural network framework that learns collective variables from Cartesian coordinates and provides Jacobians via automatic differentiation, enabling gradient-based free energy methods to use complex CVs without analytical forms.


<details>
  <summary>Details</summary>
Motivation: Existing free energy reconstruction methods like Gaussian Process Regression require Jacobians of collective variables, which restricts the use of complex or machine-learned CVs due to the analytical bottleneck.

Method: Neural network surrogate framework that learns CVs directly from Cartesian coordinates and uses automatic differentiation to compute Jacobians, bypassing the need for analytical forms.

Result: Achieved high accuracy on MgCl2 ion-pairing system for both simple distance CV and complex coordination-number CV. Jacobian errors followed near-Gaussian distribution suitable for GPR pipelines.

Conclusion: This framework enables gradient-based free energy methods to incorporate complex and machine-learned CVs, broadening the scope of biochemistry and materials simulations.

Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR)
require Jacobians of the collective variables (CVs), a bottleneck that
restricts the use of complex or machine-learned CVs. We introduce a neural
network surrogate framework that learns CVs directly from Cartesian coordinates
and uses automatic differentiation to provide Jacobians, bypassing analytical
forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for
both a simple distance CV and a complex coordination-number CV. Moreover,
Jacobian errors also followed a near-Gaussian distribution, making them
suitable for GPR pipelines. This framework enables gradient-based free energy
methods to incorporate complex and machine-learned CVs, broadening the scope of
biochemistry and materials simulations.

</details>


### [118] [Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction](https://arxiv.org/abs/2510.01407)
*Ethan G. Rogers,Cheng Wang*

Main category: cs.LG

TL;DR: A new neural compression framework using low-rank autoencoder with vector quantization that dramatically reduces decoder computational costs while maintaining high-quality image reconstruction.


<details>
  <summary>Details</summary>
Motivation: Current neural compression methods suffer from high complexity and large computational costs in convolution-based decoders, which hinders their practical adoption.

Method: Incorporating low-rank representation in an autoencoder with vector quantization, performing computationally efficient low-rank operations on learned latent representations.

Result: The approach dramatically reduces computational overhead in decoding phase, essentially eliminating decoder compute bottleneck while maintaining high fidelity of image outputs.

Conclusion: The proposed framework successfully addresses the decoder bottleneck in neural compression through efficient low-rank operations, enabling practical adoption of neural compression technology.

Abstract: Image compression and reconstruction are crucial for various digital
applications. While contemporary neural compression methods achieve impressive
compression rates, the adoption of such technology has been largely hindered by
the complexity and large computational costs of the convolution-based decoders
during data reconstruction. To address the decoder bottleneck in neural
compression, we develop a new compression-reconstruction framework based on
incorporating low-rank representation in an autoencoder with vector
quantization. We demonstrated that performing a series of computationally
efficient low-rank operations on the learned latent representation of images
can efficiently reconstruct the data with high quality. Our approach
dramatically reduces the computational overhead in the decoding phase of neural
compression/reconstruction, essentially eliminating the decoder compute
bottleneck while maintaining high fidelity of image outputs.

</details>


### [119] [Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons](https://arxiv.org/abs/2510.01439)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.LG

TL;DR: This review paper systematically examines Edge AI's evolution, current landscape, and future directions through a multi-dimensional taxonomy covering deployment, processing capabilities, applications, and hardware, while identifying key challenges and emerging opportunities.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive framework for understanding Edge AI by systematically examining its evolution from early content delivery networks to modern on-device intelligence, addressing the need for organized knowledge in this rapidly developing field.

Method: Following PRISMA guidelines, the authors conducted a systematic review using a multi-dimensional taxonomy approach that includes deployment location, processing capabilities (TinyML, federated learning), application domains, and hardware types.

Result: The analysis traces Edge AI's development, explores core enabling technologies (hardware accelerators, optimized software, communication protocols), and critically assesses challenges like resource limitations, security, model management, power consumption, and connectivity.

Conclusion: The review highlights emerging opportunities in neuromorphic hardware, continual learning algorithms, edge-cloud collaboration, and trustworthiness integration, providing a comprehensive framework to guide researchers and practitioners in the Edge AI field.

Abstract: Edge Artificial Intelligence (Edge AI) embeds intelligence directly into
devices at the network edge, enabling real-time processing with improved
privacy and reduced latency by processing data close to its source. This review
systematically examines the evolution, current landscape, and future directions
of Edge AI through a multi-dimensional taxonomy including deployment location,
processing capabilities such as TinyML and federated learning, application
domains, and hardware types. Following PRISMA guidelines, the analysis traces
the field from early content delivery networks and fog computing to modern
on-device intelligence. Core enabling technologies such as specialized hardware
accelerators, optimized software, and communication protocols are explored.
Challenges including resource limitations, security, model management, power
consumption, and connectivity are critically assessed. Emerging opportunities
in neuromorphic hardware, continual learning algorithms, edge-cloud
collaboration, and trustworthiness integration are highlighted, providing a
comprehensive framework for researchers and practitioners.

</details>


### [120] [SoftAdaClip: A Smooth Clipping Strategy for Fair and Private Model Training](https://arxiv.org/abs/2510.01447)
*Dorsa Soleymani,Ali Dadsetan,Frank Rudzicz*

Main category: cs.LG

TL;DR: SoftAdaClip is a differentially private training method that replaces hard gradient clipping with a smooth tanh-based transformation to improve fairness while maintaining privacy, reducing subgroup disparities by up to 87% compared to DP-SGD.


<details>
  <summary>Details</summary>
Motivation: Differential privacy (DP) often reduces model performance and fairness, especially for underrepresented groups, due to gradient clipping in DP-SGD which disproportionately suppresses learning signals for minority subpopulations.

Method: SoftAdaClip replaces hard clipping with a smooth, tanh-based transformation to preserve relative gradient magnitudes while bounding sensitivity, combined with adaptive mechanisms for private training.

Result: SoftAdaClip reduces subgroup disparities by up to 87% compared to DP-SGD and up to 48% compared to Adaptive-DPSGD across datasets including MIMIC-III, GOSSIS-eICU, and Adult Income, with statistically significant improvements.

Conclusion: Integrating smooth transformations with adaptive mechanisms is crucial for achieving fair and private model training, as demonstrated by SoftAdaClip's ability to significantly reduce subgroup disparities while maintaining differential privacy.

Abstract: Differential privacy (DP) provides strong protection for sensitive data, but
often reduces model performance and fairness, especially for underrepresented
groups. One major reason is gradient clipping in DP-SGD, which can
disproportionately suppress learning signals for minority subpopulations.
Although adaptive clipping can enhance utility, it still relies on uniform hard
clipping, which may restrict fairness. To address this, we introduce
SoftAdaClip, a differentially private training method that replaces hard
clipping with a smooth, tanh-based transformation to preserve relative gradient
magnitudes while bounding sensitivity. We evaluate SoftAdaClip on various
datasets, including MIMIC-III (clinical text), GOSSIS-eICU (structured
healthcare), and Adult Income (tabular data). Our results show that SoftAdaClip
reduces subgroup disparities by up to 87% compared to DP-SGD and up to 48%
compared to Adaptive-DPSGD, and these reductions in subgroup disparities are
statistically significant. These findings underscore the importance of
integrating smooth transformations with adaptive mechanisms to achieve fair and
private model training.

</details>


### [121] [Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression](https://arxiv.org/abs/2510.01450)
*Yifei Zuo,Yutong Yin,Zhichen Zeng,Ang Li,Banghua Zhu,Zhaoran Wang*

Main category: cs.LG

TL;DR: Proposes Local Linear Attention (LLA), a novel attention mechanism derived from nonparametric statistics that offers theoretical advantages over existing attention mechanisms and addresses computational challenges with memory-efficient implementations.


<details>
  <summary>Details</summary>
Motivation: While efficient alternatives to Softmax Attention have been widely studied, there's a gap in exploring more expressive mechanisms grounded in theoretical insight, even at greater computational cost.

Method: Derives LLA from nonparametric statistics through test-time regression, proposes two memory-efficient primitives to tackle computational complexity, and introduces FlashLLA - a hardware-efficient blockwise algorithm with customized inference kernel.

Result: LLA effectively adapts to non-stationarity, outperforming strong baselines in test-time training and in-context learning, and shows promising evidence for scalability in large-scale models.

Conclusion: LLA bridges the gap between theoretical insight and practical implementation, offering a more expressive attention mechanism with demonstrated advantages in various tasks while addressing computational challenges.

Abstract: Transformer architectures have achieved remarkable success in various
domains. While efficient alternatives to Softmax Attention have been widely
studied, the search for more expressive mechanisms grounded in theoretical
insight-even at greater computational cost-has been relatively underexplored.
In this work, we bridge this gap by proposing Local Linear Attention (LLA), a
novel attention mechanism derived from nonparametric statistics through the
lens of test-time regression. First, we show that LLA offers theoretical
advantages over Linear and Softmax Attention for associative memory via a
bias-variance trade-off analysis. Next, we address its computational challenges
and propose two memory-efficient primitives to tackle the $\Theta(n^2 d)$ and
$\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient,
blockwise algorithm that enables scalable and parallel computation on modern
accelerators. In addition, we implement and profile a customized inference
kernel that significantly reduces memory overheads. Finally, we empirically
validate the advantages and limitations of LLA on test-time regression,
in-context regression, associative recall and state tracking tasks. Experiment
results demonstrate that LLA effectively adapts to non-stationarity,
outperforming strong baselines in test-time training and in-context learning,
and exhibiting promising evidence for its scalability and applicability in
large-scale models. Code is available at
https://github.com/Yifei-Zuo/Flash-LLA.

</details>


### [122] [SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion](https://arxiv.org/abs/2510.01456)
*Brett Barkley,Preston Culbertson,David Fridovich-Keil*

Main category: cs.LG

TL;DR: SCOPED is a fast OOD detection method for diffusion models that reduces forward passes by 10x, combining Jacobian trace and score function norm into a single statistic with kernel density estimation for flexible testing.


<details>
  <summary>Details</summary>
Motivation: OOD detection is crucial for reliable ML deployment across vision, robotics, and RL. Current diffusion-based methods require many forward passes, making them computationally expensive.

Method: Combines Jacobian trace and squared norm of score function into SCOPED statistic, uses kernel density estimation for density estimation, requires only single forward pass and one JVP with Hutchinson's trace estimator.

Result: Achieves competitive/state-of-the-art precision-recall scores on 4 vision benchmarks with low computational cost, generalizes to robotic control tasks across reward functions and training regimes.

Conclusion: SCOPED provides practical foundation for fast and reliable OOD detection in real-world domains including vision artifacts, outlier detection, RL exploration, and dataset curation.

Abstract: Out-of-distribution (OOD) detection is essential for reliable deployment of
machine learning systems in vision, robotics, reinforcement learning, and
beyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator
for Diffusion (SCOPED), a fast and general-purpose OOD detection method for
diffusion models that reduces the number of forward passes on the trained model
by an order of magnitude compared to prior methods, outperforming most
diffusion-based baselines and closely approaching the accuracy of the strongest
ones. SCOPED is computed from a single diffusion model trained once on a
diverse dataset, and combines the Jacobian trace and squared norm of the
model's score function into a single test statistic. Rather than thresholding
on a fixed value, we estimate the in-distribution density of SCOPED scores
using kernel density estimation, enabling a flexible, unsupervised test that,
in the simplest case, only requires a single forward pass and one
Jacobian-vector product (JVP), made efficient by Hutchinson's trace estimator.
On four vision benchmarks, SCOPED achieves competitive or state-of-the-art
precision-recall scores despite its low computational cost. The same method
generalizes to robotic control tasks with shared state and action spaces,
identifying distribution shifts across reward functions and training regimes.
These results position SCOPED as a practical foundation for fast and reliable
OOD detection in real-world domains, including perceptual artifacts in vision,
outlier detection in autoregressive models, exploration in reinforcement
learning, and dataset curation for unsupervised training.

</details>


### [123] [Fixing That Free Lunch: When, Where, and Why Synthetic Data Fails in Model-Based Policy Optimization](https://arxiv.org/abs/2510.01457)
*Brett Barkley,David Fridovich-Keil*

Main category: cs.LG

TL;DR: MBPO's synthetic data can degrade performance in DMC tasks despite success in Gym. Two failure modes identified: scale mismatches between dynamics/reward models causing critic underestimation, and poor target representation inflating model variance. Fixing these enables MBPO to outperform SAC in 5/7 DMC tasks.


<details>
  <summary>Details</summary>
Motivation: To understand why MBPO underperforms SAC in DeepMind Control Suite despite strong results in OpenAI Gym, and identify the specific failure modes causing this performance degradation.

Method: Analyzed MBPO's performance across seven challenging DMC tasks, identified two coupled failure modes: scale mismatches between dynamics and reward models, and poor target representation choice. Addressed these issues to enable policy improvement.

Result: After addressing the identified failure modes, MBPO outperformed SAC in five out of seven DMC tasks while maintaining strong performance in OpenAI Gym benchmarks.

Conclusion: Environment-specific assumptions become implicitly encoded into algorithm design when evaluation is limited. Community should develop taxonomies linking MDP structure to algorithmic failure modes and clarify how benchmark choices shape algorithm generalization.

Abstract: Synthetic data is a core component of data-efficient Dyna-style model-based
reinforcement learning, yet it can also degrade performance. We study when it
helps, where it fails, and why, and we show that addressing the resulting
failure modes enables policy improvement that was previously unattainable. We
focus on Model-Based Policy Optimization (MBPO), which performs actor and
critic updates using synthetic action counterfactuals. Despite reports of
strong and generalizable sample-efficiency gains in OpenAI Gym, recent work
shows that MBPO often underperforms its model-free counterpart, Soft
Actor-Critic (SAC), in the DeepMind Control Suite (DMC). Although both suites
involve continuous control with proprioceptive robots, this shift leads to
sharp performance losses across seven challenging DMC tasks, with MBPO failing
in cases where claims of generalization from Gym would imply success. This
reveals how environment-specific assumptions can become implicitly encoded into
algorithm design when evaluation is limited. We identify two coupled issues
behind these failures: scale mismatches between dynamics and reward models that
induce critic underestimation and hinder policy improvement during model-policy
coevolution, and a poor choice of target representation that inflates model
variance and produces error-prone rollouts. Addressing these failure modes
enables policy improvement where none was previously possible, allowing MBPO to
outperform SAC in five of seven tasks while preserving the strong performance
previously reported in OpenAI Gym. Rather than aiming only for incremental
average gains, we hope our findings motivate the community to develop
taxonomies that tie MDP task- and environment-level structure to algorithmic
failure modes, pursue unified solutions where possible, and clarify how
benchmark choices ultimately shape the conditions under which algorithms
generalize.

</details>


### [124] [How Well Can Preference Optimization Generalize Under Noisy Feedback?](https://arxiv.org/abs/2510.01458)
*Shawn Im,Yixuan Li*

Main category: cs.LG

TL;DR: This paper analyzes the impact of noisy human feedback on preference optimization for large language models, providing generalization guarantees under realistic noise conditions like mislabeling and uncertainty.


<details>
  <summary>Details</summary>
Motivation: Most existing preference optimization works assume noise-free human feedback, which is unrealistic due to inherent errors and inconsistencies in human judgments. The paper aims to address this gap by studying how noisy feedback affects model alignment.

Method: The authors analyze finite-step preference optimization under various noise models (mislabeling, uncertainty) that correspond to real-world noise sources. They provide theoretical generalization guarantees and analyze how generalization decays with different noise types and rates based on data distribution and sample size.

Result: The analysis shows how generalization performance deteriorates with different types and levels of noise in preference data. The findings apply broadly to various preference optimization losses including DPO, IPO, SLiC, etc. Empirical validation on contemporary LLMs confirms the practical relevance of the theoretical results.

Conclusion: The paper provides valuable insights for developing AI systems that better align with human preferences under realistic noisy feedback conditions, offering theoretical guarantees that are more aligned with practical LLM training scenarios than traditional convergence-based analyses.

Abstract: As large language models (LLMs) advance their capabilities, aligning these
models with human preferences has become crucial. Preference optimization,
which trains models to distinguish between preferred and non-preferred
responses based on human feedback, has become a crucial component for aligning
LLMs. However, most existing works assume noise-free feedback, which is
unrealistic due to the inherent errors and inconsistencies in human judgments.
This paper addresses the impact of noisy feedback on preference optimization,
providing generalization guarantees under these conditions. In particular, we
consider noise models that correspond to common real-world sources of noise,
such as mislabeling and uncertainty. Unlike traditional analyses that assume
convergence, our work focuses on finite-step preference optimization, offering
new insights that are more aligned with practical LLM training. We describe how
generalization decays with different types of noise across levels of noise
rates based on the preference data distribution and number of samples. Our
analysis for noisy preference learning applies to a broad family of preference
optimization losses such as DPO, IPO, SLiC, etc. Empirical validation on
contemporary LLMs confirms the practical relevance of our findings, offering
valuable insights for developing AI systems that align with human preferences.

</details>


### [125] [LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning](https://arxiv.org/abs/2510.01459)
*Weizhe Chen,Sven Koenig,Bistra Dilkina*

Main category: cs.LG

TL;DR: LSPO is a meta-RLVR algorithm that dynamically selects training data based on average response length to improve learning effectiveness in LLMs.


<details>
  <summary>Details</summary>
Motivation: Motivated by studies of overthinking in LLMs, the authors aim to address efficiency and effectiveness issues in RLVR training by incorporating length signals.

Method: Proposed Length-aware Sampling for Policy Optimization (LSPO), a meta-RLVR algorithm that dynamically selects training data at each step based on average response length.

Result: LSPO consistently improves learning effectiveness across multiple base models and datasets, with ablation studies providing insights on length signal incorporation.

Conclusion: LSPO offers an effective approach for RLVR training and highlights promising directions for future research in dynamic sampling methods.

Abstract: Since the release of Deepseek-R1, reinforcement learning with verifiable
rewards (RLVR) has become a central approach for training large language models
(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss
functions to make RLVR more efficient and effective. In this paper, motivated
by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy
Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects
training data at each step based on the average response length. We evaluate
LSPO across multiple base models and datasets, demonstrating that it
consistently improves learning effectiveness. In addition, we conduct a
detailed ablation study to examine alternative ways of incorporating length
signals into dynamic sampling, offering further insights and highlighting
promising directions for future research.

</details>


### [126] [The Three Regimes of Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2510.01460)
*Lu Li,Tianwei Ni,Yihao Sun,Pierre-Luc Bacon*

Main category: cs.LG

TL;DR: The paper proposes a stability-plasticity principle for offline-to-online RL, identifying three regimes of online fine-tuning based on relative performance of offline dataset vs pretrained policy.


<details>
  <summary>Details</summary>
Motivation: Address the inconsistent empirical behavior in offline-to-online RL where design choices that work well in one setting fail in another.

Method: Propose a stability-plasticity principle framework and validate through large-scale empirical study across 63 cases.

Result: The framework strongly aligns with empirical results in 45 of 63 cases, successfully predicting appropriate fine-tuning regimes.

Conclusion: Provides a principled framework for guiding design choices in offline-to-online RL based on relative performance of offline dataset and pretrained policy.

Abstract: Offline-to-online reinforcement learning (RL) has emerged as a practical
paradigm that leverages offline datasets for pretraining and online
interactions for fine-tuning. However, its empirical behavior is highly
inconsistent: design choices of online-fine tuning that work well in one
setting can fail completely in another. We propose a stability--plasticity
principle that can explain this inconsistency: we should preserve the knowledge
of pretrained policy or offline dataset during online fine-tuning, whichever is
better, while maintaining sufficient plasticity. This perspective identifies
three regimes of online fine-tuning, each requiring distinct stability
properties. We validate this framework through a large-scale empirical study,
finding that the results strongly align with its predictions in 45 of 63 cases.
This work provides a principled framework for guiding design choices in
offline-to-online RL based on the relative performance of the offline dataset
and the pretrained policy.

</details>


### [127] [Fine-tuning LLMs with variational Bayesian last layer for high-dimensional Bayesian optimzation](https://arxiv.org/abs/2510.01471)
*Haotian Xiang,Jinwen Xu,Qin Lu*

Main category: cs.LG

TL;DR: This paper proposes a novel Bayesian optimization approach using LLMs as surrogates for high-dimensional black-box optimization with irregular variables, featuring LoRA fine-tuning and variational Bayesian last layer for computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional Gaussian processes struggle with high-dimensional optimization problems involving irregular variables (categorical, ordinal), while existing neural network surrogates are computationally expensive. LLMs offer powerful modeling capabilities but need adaptation for efficient Bayesian optimization.

Method: Use LLM as surrogate model with LoRA fine-tuning combined with variational Bayesian last layer (VBLL) framework. Develop weighted ensemble (ENS) of LoRA-VBLL surrogates for automated hyperparameter selection and recursive updates.

Result: Extensive experiments show compelling performance on high-dimensional benchmarks and real-world molecular optimization tasks, outperforming existing alternatives with computational efficiency.

Conclusion: The proposed (ENS-)LoRA-VBLL approach effectively addresses high-dimensional black-box optimization with irregular variables, offering both superior performance and computational efficiency through LLM-based surrogate modeling and Bayesian adaptation techniques.

Abstract: A plethora of applications entail solving black-box optimization problems
with high evaluation costs, including drug discovery, material design, as well
as hyperparameter tuning. Toward finding the global optimum of such black-box
optimization problems with sample efficiency, Bayesian optimization (BO) is a
theoretically elegant framework that relies on a probabilistic surrogate model
so as to iteratively select the query point with well-balanced
exploration-exploitation tradeoffs. The Gaussian process (GP), as the de-facto
choice for surrogate modeling, has achieved compelling performances for vanilla
BO with low-dimensional continuous variables. However, GPs fall short in coping
with high-dimensional counterparts with {\it irregular} variables (e.g.,
categorical, ordinal, etc.). To alleviate this, neural network-based surrogates
have been explored. Inspired by the powerful capabilities of LLMs, we adopt the
LLM as the surrogate to model the mapping from the high-dimensional input
variables to the objective function. To adapt to the current problem, we
leverage the low-rank adaptation (LoRA) to fine-tune the LLM parameters
together with the posterior of a linear regression head via the variational
Bayesian last layer (VBLL) framework. The resulting LoRA-VBLL is not only
computationally light compared to existing alternatives, but also admits
recursive updates. To automate the critical selection of the LoRA rank as well
as other hyperparameters, a weighted ensemble (ENS) of LoRA-VBLL surrogates has
been devised, which further accommodates continual update of the per-model
weight and individual LoRA-VBLL parameters via recursive Bayes. Extensive
experimental results demonstrate the compelling performance of the proposed
(ENS-)LoRA-VBLL approaches on various high-dimensional benchmarks and the
real-world molecular optimization tasks.

</details>


### [128] [PEL-NAS: Search Space Partitioned Architecture Prompt Co-Evolutionary LLM-driven Hardware-Aware Neural Architecture Search](https://arxiv.org/abs/2510.01472)
*Hengyi Zhu,Grace Li Zhang,Shaoyi Huang*

Main category: cs.LG

TL;DR: PEL-NAS is a novel HW-NAS method that uses LLM-driven co-evolution with search space partitioning to efficiently generate high-accuracy, low-latency neural architectures, reducing search time from days to minutes.


<details>
  <summary>Details</summary>
Motivation: Traditional supernet-based HW-NAS methods are too slow (multiple GPU days), while LLM-driven approaches suffer from exploration bias and limited search space coverage.

Method: Three key components: 1) complexity-driven search space partitioning for diversity, 2) LLM-powered architecture prompt co-evolution with knowledge base updates, 3) zero-cost predictor to avoid training candidates from scratch.

Result: On HW-NAS-Bench, achieves higher HV, lower IGD, and up to 54% lower latency than baselines at similar accuracy, with search cost dropping from days to minutes.

Conclusion: PEL-NAS effectively addresses exploration bias in LLM-driven NAS, enabling efficient discovery of diverse, high-performance neural architectures across different latency ranges.

Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) requires joint
optimization of accuracy and latency under device constraints. Traditional
supernet-based methods require multiple GPU days per dataset. Large Language
Model (LLM)-driven approaches avoid training a large supernet and can provide
quick feedback, but we observe an exploration bias: the LLM repeatedly proposes
neural network designs within limited search space and fails to discover
architectures across different latency ranges in the entire search space. To
address this issue, we propose PEL-NAS: a search space Partitioned,
architecture prompt co-Evolutionary and LLM-driven Neural Architecture Search
that can generate neural networks with high accuracy and low latency with
reduced search cost. Our proposed PEL-NAS has three key components: 1) a
complexity-driven partitioning engine that divides the search space by
complexity to enforce diversity and mitigate exploration bias; 2) an
LLM-powered architecture prompt co-evolution operator, in which the LLM first
updates a knowledge base of design heuristics based on results from the
previous round, then performs a guided evolution algorithm on architectures
with prompts that incorporate this knowledge base. Prompts and designs improve
together across rounds which avoids random guesswork and improve efficiency; 3)
a zero-cost predictor to avoid training a large number of candidates from
scratch. Experimental results show that on HW-NAS-Bench, PEL-NAS can achieve
overall higher HV, lower IGD, and up to 54% lower latency than baselines at
similar accuracy. Meanwhile, the search cost drops from days to minutes
compared with traditional supernet baselines.

</details>


### [129] [Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets](https://arxiv.org/abs/2510.01479)
*Shriram Karpoora Sundara Pandian,Ali Baheri*

Main category: cs.LG

TL;DR: Weighted BC is a robust offline RL method that uses density ratio weighting from a clean reference set to handle contaminated datasets, achieving near-optimal performance even with high contamination rates.


<details>
  <summary>Details</summary>
Motivation: Offline RL datasets often contain corrupted samples from adversarial poisoning or system errors, which degrade policy performance in standard methods. There's a need for robust approaches that can handle contaminated data without knowing the contamination mechanism.

Method: Uses a small verified clean reference set to estimate trajectory-level density ratios via a binary discriminator. These clipped density ratios are used as weights in the behavioral cloning objective to prioritize clean expert behavior while down-weighting corrupted data.

Result: Maintains near-optimal performance even at high contamination ratios, outperforming baselines like traditional BC, BCQ, and BRAC across various poisoning protocols (reward, state, transition, action) on continuous control benchmarks.

Conclusion: Weighted BC provides a practical and theoretically grounded solution for robust offline RL with contaminated datasets, converging to the clean expert policy with finite-sample bounds independent of contamination rate.

Abstract: Offline reinforcement learning (RL) enables policy optimization from fixed
datasets, making it suitable for safety-critical applications where online
exploration is infeasible. However, these datasets are often contaminated by
adversarial poisoning, system errors, or low-quality samples, leading to
degraded policy performance in standard behavioral cloning (BC) and offline RL
methods. This paper introduces Density-Ratio Weighted Behavioral Cloning
(Weighted BC), a robust imitation learning approach that uses a small, verified
clean reference set to estimate trajectory-level density ratios via a binary
discriminator. These ratios are clipped and used as weights in the BC objective
to prioritize clean expert behavior while down-weighting or discarding
corrupted data, without requiring knowledge of the contamination mechanism. We
establish theoretical guarantees showing convergence to the clean expert policy
with finite-sample bounds that are independent of the contamination rate. A
comprehensive evaluation framework is established, which incorporates various
poisoning protocols (reward, state, transition, and action) on continuous
control benchmarks. Experiments demonstrate that Weighted BC maintains
near-optimal performance even at high contamination ratios outperforming
baselines such as traditional BC, batch-constrained Q-learning (BCQ) and
behavior regularized actor-critic (BRAC).

</details>


### [130] [Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed](https://arxiv.org/abs/2510.01494)
*Isha Gupta,Rylan Schaeffer,Joshua Kazdan,Ken Liu,Sanmi Koyejo*

Main category: cs.LG

TL;DR: This paper explains why adversarial attacks transfer between some models but not others, showing that data-space attacks transfer while representation-space attacks generally don't, unless there's geometric alignment of representations.


<details>
  <summary>Details</summary>
Motivation: To explain why image jailbreaks don't transfer between vision-language models (VLMs) while adversarial examples transfer between image classifiers and text jailbreaks transfer between language models.

Method: Theoretical analysis and empirical evidence across four settings: mathematical proof in simple networks, representation-space attacks on image classifiers, representation-space attacks on language models, and data-space attacks on VLMs with geometric alignment analysis.

Result: Data-space attacks successfully transfer between models, while representation-space attacks fail to transfer unless there's sufficient geometric alignment of representations in the latent space. Representation-space attacks can be as effective as data-space attacks but lack transferability.

Conclusion: Adversarial transfer is not inherent to all attacks but depends on their operational domain - shared data-space versus unique representation spaces - which is crucial for building more robust models.

Abstract: The field of adversarial robustness has long established that adversarial
examples can successfully transfer between image classifiers and that text
jailbreaks can successfully transfer between language models (LMs). However, a
pair of recent studies reported being unable to successfully transfer image
jailbreaks between vision-language models (VLMs). To explain this striking
difference, we propose a fundamental distinction regarding the transferability
of attacks against machine learning models: attacks in the input data-space can
transfer, whereas attacks in model representation space do not, at least not
without geometric alignment of representations. We then provide theoretical and
empirical evidence of this hypothesis in four different settings. First, we
mathematically prove this distinction in a simple setting where two networks
compute the same input-output map but via different representations. Second, we
construct representation-space attacks against image classifiers that are as
successful as well-known data-space attacks, but fail to transfer. Third, we
construct representation-space attacks against LMs that successfully jailbreak
the attacked models but again fail to transfer. Fourth, we construct data-space
attacks against VLMs that successfully transfer to new VLMs, and we show that
representation space attacks \emph{can} transfer when VLMs' latent geometries
are sufficiently aligned in post-projector space. Our work reveals that
adversarial transfer is not an inherent property of all attacks but contingent
on their operational domain - the shared data-space versus models' unique
representation spaces - a critical insight for building more robust models.

</details>


### [131] [Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information](https://arxiv.org/abs/2510.01499)
*Rui Ai,Yuqi Pan,David Simchi-Levi,Milind Tambe,Haifeng Xu*

Main category: cs.LG

TL;DR: The paper proposes two new aggregation algorithms (Optimal Weight and Inverse Surprising Popularity) for multi-agent LLM reasoning that outperform standard majority voting by leveraging first-order and second-order information.


<details>
  <summary>Details</summary>
Motivation: Standard majority voting treats all LLM answers equally, failing to account for latent heterogeneity and correlation across models, which limits the reliability of collective decisions.

Method: Designed two aggregation algorithms: Optimal Weight (OW) and Inverse Surprising Popularity (ISP) that utilize both first-order and second-order information from multiple LLMs.

Result: Empirical validation on synthetic datasets, LLM benchmarks (UltraFeedback, MMLU), and real-world healthcare setting (ARMMAN) shows consistent outperformance over majority voting.

Conclusion: The proposed methods provide both practical performance gains and conceptual insights for designing robust multi-agent LLM pipelines, effectively mitigating limitations of majority voting.

Abstract: With the rapid progress of multi-agent large language model (LLM) reasoning,
how to effectively aggregate answers from multiple LLMs has emerged as a
fundamental challenge. Standard majority voting treats all answers equally,
failing to consider latent heterogeneity and correlation across models. In this
work, we design two new aggregation algorithms called Optimal Weight (OW) and
Inverse Surprising Popularity (ISP), leveraging both first-order and
second-order information. Our theoretical analysis shows these methods provably
mitigate inherent limitations of majority voting under mild assumptions,
leading to more reliable collective decisions. We empirically validate our
algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as
UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all
cases, our methods consistently outperform majority voting, offering both
practical performance gains and conceptual insights for the design of robust
multi-agent LLM pipelines.

</details>


### [132] [Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control](https://arxiv.org/abs/2510.01508)
*Will Y. Zou,Jean Feng,Alexandre Kalimouttou,Jennifer Yuntong Zhang,Christopher W. Seymour,Romain Pirracchio*

Main category: cs.LG

TL;DR: This paper presents an end-to-end RL approach for dual vasopressor dosing in ICU septic shock patients, using designed action spaces to improve interpretability and clinical adoption while achieving 15% survival improvement.


<details>
  <summary>Details</summary>
Motivation: Address skepticism from practitioners about inoperable RL dosing decisions in Clinical Decision Support Systems by making policies more interpretable and clinically adoptable.

Method: Combines offline conservative Q-learning with novel recurrent modeling in replay buffer to capture temporal dependencies in ICU data, using designed action spaces that accommodate discrete, continuous, and directional dosing strategies.

Result: Action space design significantly influences learned behavioral policies, achieving over 15% improvement in survival probability while aligning with established clinical protocols on eICU and MIMIC datasets.

Conclusion: Designed action spaces improve interpretability and facilitate clinical adoption of RL-based dosing policies while preserving efficacy, demonstrating the importance of action space formulation in medical RL applications.

Abstract: Reinforcement learning (RL) applications in Clinical Decision Support Systems
(CDSS) frequently encounter skepticism from practitioners regarding inoperable
dosing decisions. We address this challenge with an end-to-end approach for
learning optimal drug dosing and control policies for dual vasopressor
administration in intensive care unit (ICU) patients with septic shock. For
realistic drug dosing, we apply action space design that accommodates discrete,
continuous, and directional dosing strategies in a system that combines offline
conservative Q-learning with a novel recurrent modeling in a replay buffer to
capture temporal dependencies in ICU time-series data. Our comparative analysis
of norepinephrine dosing strategies across different action space formulations
reveals that the designed action spaces improve interpretability and facilitate
clinical adoption while preserving efficacy. Empirical results1 on eICU and
MIMIC demonstrate that action space design profoundly influences learned
behavioral policies. The proposed methods achieve improved patient outcomes of
over 15% in survival improvement probability, while aligning with established
clinical protocols.

</details>


### [133] [Flock: A Knowledge Graph Foundation Model via Learning on Random Walks](https://arxiv.org/abs/2510.01510)
*Jinwoo Kim,Xingyue Huang,Krzysztof Olejniczak,Kyungbin Min,Michael Bronstein,Seunghoon Hong,Ä°smail Ä°lkan Ceylan*

Main category: cs.LG

TL;DR: Flock introduces probabilistic node-relation equivariance to overcome limitations of deterministic equivariance in knowledge graph foundation models, enabling better zero-shot link prediction on novel entities and relations.


<details>
  <summary>Details</summary>
Motivation: Current knowledge graph foundation models (KGFMs) use deterministic equivariance which limits their expressive power, preventing them from distinguishing structurally similar but semantically distinct relations in zero-shot link prediction scenarios.

Method: Flock uses probabilistic node-relation equivariance that preserves equivariance in distribution while incorporating principled randomization. It iteratively samples random walks, encodes them via recording protocol, embeds with sequence model, and aggregates representations via learned pooling.

Result: Flock perfectly solves the new diagnostic dataset Petals where current KGFMs fail, and achieves state-of-the-art performance on entity and relation prediction tasks across 54 diverse knowledge graphs.

Conclusion: Probabilistic node-relation equivariance enables KGFMs to be universal approximators for isomorphism-invariant link-level functions, overcoming limitations of deterministic approaches and achieving superior zero-shot link prediction performance.

Abstract: We study the problem of zero-shot link prediction on knowledge graphs (KGs),
which requires models to generalize over novel entities and novel relations.
Knowledge graph foundation models (KGFMs) address this task by enforcing
equivariance over both nodes and relations, learning from structural properties
of nodes and relations, which are then transferable to novel graphs with
similar structural properties. However, the conventional notion of
deterministic equivariance imposes inherent limits on the expressive power of
KGFMs, preventing them from distinguishing structurally similar but
semantically distinct relations. To overcome this limitation, we introduce
probabilistic node-relation equivariance, which preserves equivariance in
distribution while incorporating a principled randomization to break symmetries
during inference. Building on this principle, we present Flock, a KGFM that
iteratively samples random walks, encodes them into sequences via a recording
protocol, embeds them with a sequence model, and aggregates representations of
nodes and relations via learned pooling. Crucially, Flock respects
probabilistic node-relation equivariance and is a universal approximator for
isomorphism-invariant link-level functions over KGs. Empirically, Flock
perfectly solves our new diagnostic dataset Petals where current KGFMs fail,
and achieves state-of-the-art performances on entity- and relation prediction
tasks on 54 KGs from diverse domains.

</details>


### [134] [Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties](https://arxiv.org/abs/2510.01520)
*Hossein Sholehrasa,Xuan Xu,Doina Caragea,Jim E. Riviere,Majid Jaberi-Douraki*

Main category: cs.LG

TL;DR: This study develops a predictive framework using machine learning to classify veterinary drug adverse event outcomes (Death vs. Recovery) from FDA data, achieving high performance with ensemble methods and providing interpretable insights into risk factors.


<details>
  <summary>Details</summary>
Motivation: To protect animal welfare and human food safety by predicting adverse events from pharmaceuticals in food-producing animals, which may signal unexpected pharmacokinetic effects and increase the risk of violative residues in the food chain.

Method: Used ~1.28 million FDA reports with preprocessing pipeline including data normalization, missing value imputation, feature reduction, and integration of physicochemical drug properties. Evaluated multiple supervised models (Random Forest, CatBoost, XGBoost, ExcelFormer, LLMs) with class imbalance handling techniques and ensemble methods.

Result: Ensemble methods and CatBoost performed best with precision, recall, and F1-scores of 0.95. AUM-based pseudo-labeling improved minority-class detection. SHAP analysis identified biologically plausible predictors including lung/heart disorders, animal demographics, and drug properties strongly linked to fatal outcomes.

Conclusion: The framework demonstrates that combining rigorous data engineering, advanced machine learning, and explainable AI enables accurate, interpretable predictions of veterinary safety outcomes, supporting regulatory decision-making and residue risk assessment.

Abstract: The safe use of pharmaceuticals in food-producing animals is vital to protect
animal welfare and human food safety. Adverse events (AEs) may signal
unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of
violative residues in the food chain. This study introduces a predictive
framework for classifying outcomes (Death vs. Recovery) using ~1.28 million
reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary
Medicine. A preprocessing pipeline merged relational tables and standardized
AEs through VeDDRA ontologies. Data were normalized, missing values imputed,
and high-cardinality features reduced; physicochemical drug properties were
integrated to capture chemical-residue links. We evaluated supervised models,
including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language
models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as
undersampling and oversampling, with a focus on prioritizing recall for fatal
outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,
achieving precision, recall, and F1-scores of 0.95. Incorporating Average
Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved
minority-class detection, particularly in ExcelFormer and XGBoost.
Interpretability via SHAP identified biologically plausible predictors,
including lung, heart, and bronchial disorders, animal demographics, and drug
physicochemical properties. These features were strongly linked to fatal
outcomes. Overall, the framework shows that combining rigorous data
engineering, advanced machine learning, and explainable AI enables accurate,
interpretable predictions of veterinary safety outcomes. The approach supports
FARAD's mission by enabling early detection of high-risk drug-event profiles,
strengthening residue risk assessment, and informing regulatory and clinical
decision-making.

</details>


### [135] [CarbonX: An Open-Source Tool for Computational Decarbonization Using Time Series Foundation Models](https://arxiv.org/abs/2510.01521)
*Diptyaroop Maji,Kang Yang,Prashant Shenoy,Ramesh K Sitaraman,Mani Srivastava*

Main category: cs.LG

TL;DR: CarbonX is an open-source tool using Time Series Foundation Models for carbon intensity forecasting across global grids, achieving 15.82% MAPE in zero-shot forecasting across 214 grids and providing uncertainty estimates.


<details>
  <summary>Details</summary>
Motivation: Existing carbon intensity forecasting tools have limitations: require grid-specific electricity mix data, depend on separate grid-specific models limiting global coverage, and lack uncertainty estimates, which restricts reliability for carbon-aware applications.

Method: Leverages Time Series Foundation Models (TSFMs) using only historical carbon intensity data and a single general model, providing zero-shot forecasting, imputation capabilities, and prediction intervals with 95% coverage.

Result: Achieves zero-shot forecasting MAPE of 15.82% across 214 grids worldwide, comparable performance with state-of-the-art on 13 benchmark grids (average MAPE 9.59%, tail forecasting MAPE 16.54%), and 1.2-3.9X improvement over statistical baselines on imputation task when fine-tuned.

Conclusion: CarbonX provides a practical tool for global-scale decarbonization that can be easily used on any grid with limited data while delivering strong performance and uncertainty estimates.

Abstract: Computational decarbonization aims to reduce carbon emissions in computing
and societal systems such as data centers, transportation, and built
environments. This requires accurate, fine-grained carbon intensity forecasts,
yet existing tools have several key limitations: (i) they require grid-specific
electricity mix data, restricting use where such information is unavailable;
(ii) they depend on separate grid-specific models that make it challenging to
provide global coverage; and (iii) they provide forecasts without uncertainty
estimates, limiting reliability for downstream carbon-aware applications.
  In this paper, we present CarbonX, an open-source tool that leverages Time
Series Foundation Models (TSFMs) for a range of decarbonization tasks. CarbonX
utilizes the versatility of TSFMs to provide strong performance across multiple
tasks, such as carbon intensity forecasting and imputation, and across diverse
grids. Using only historical carbon intensity data and a single general model,
our tool achieves a zero-shot forecasting Mean Absolute Percentage Error (MAPE)
of 15.82% across 214 grids worldwide. Across 13 benchmark grids, CarbonX
performance is comparable with the current state-of-the-art, with an average
MAPE of 9.59% and tail forecasting MAPE of 16.54%, while also providing
prediction intervals with 95% coverage. CarbonX can provide forecasts for up to
21 days with minimal accuracy degradation. Further, when fully fine-tuned,
CarbonX outperforms the statistical baselines by 1.2--3.9X on the imputation
task. Overall, these results demonstrate that CarbonX can be used easily on any
grid with limited data and still deliver strong performance, making it a
practical tool for global-scale decarbonization.

</details>


### [136] [On Integer Programming for the Binarized Neural Network Verification Problem](https://arxiv.org/abs/2510.01525)
*Woojin Kim,James R. Luedtke*

Main category: cs.LG

TL;DR: This paper presents improved integer programming (IP) formulations for verifying binarized neural networks (BNNs) by introducing a new linear objective method for multi-class classification and generating valid inequalities from BNNs' recursive structure.


<details>
  <summary>Details</summary>
Motivation: The verification problem for BNNs seeks to determine if small input perturbations can cause misclassification, but existing IP formulations face challenges due to large integrality gaps from big-M constraints.

Method: Two main techniques: 1) A new method for obtaining linear objectives in multi-class settings, 2) A technique for generating valid inequalities that exploit BNNs' recursive structure.

Result: The proposed techniques enable verifying BNNs against higher ranges of input perturbations than existing IP approaches within limited time constraints.

Conclusion: The improved IP formulation with linear objectives and valid inequalities derived from BNN structure significantly enhances verification capabilities for binarized neural networks.

Abstract: Binarized neural networks (BNNs) are feedforward neural networks with binary
weights and activation functions. In the context of using a BNN for
classification, the verification problem seeks to determine whether a small
perturbation of a given input can lead it to be misclassified by the BNN, and
the robustness of the BNN can be measured by solving the verification problem
over multiple inputs. The BNN verification problem can be formulated as an
integer programming (IP) problem. However, the natural IP formulation is often
challenging to solve due to a large integrality gap induced by big-$M$
constraints. We present two techniques to improve the IP formulation. First, we
introduce a new method for obtaining a linear objective for the multi-class
setting. Second, we introduce a new technique for generating valid inequalities
for the IP formulation that exploits the recursive structure of BNNs. We find
that our techniques enable verifying BNNs against a higher range of input
perturbation than existing IP approaches within a limited time.

</details>


### [137] [Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs](https://arxiv.org/abs/2510.01527)
*Lecheng Kong,Xiyuan Wang,Yixin Chen,Muhan Zhang*

Main category: cs.LG

TL;DR: RTRL framework trains LLMs for round-trip consistency in chemistry tasks using reinforcement learning with round-trip success as reward, improving performance and reliability.


<details>
  <summary>Details</summary>
Motivation: LLMs in computational chemistry lack round-trip consistency - they can predict reactions but fail to reconstruct original structures from their own generated text, indicating memorization rather than true understanding.

Method: Round-Trip Reinforcement Learning (RTRL) uses round-trip transformation success as reward signal, with iterative variant where forward and reverse mappings alternately train each other in self-improvement loop.

Result: RTRL significantly boosts performance and consistency over strong baselines across supervised, self-supervised, and synthetic data regimes, showing strong correlation between consistency and primary task performance.

Conclusion: Round-trip consistency is a trainable objective that offers new path toward more robust and reliable foundation models in computational chemistry.

Abstract: Large Language Models (LLMs) are emerging as versatile foundation models for
computational chemistry, handling bidirectional tasks like reaction prediction
and retrosynthesis. However, these models often lack round-trip consistency.
For instance, a state-of-the-art chemical LLM may successfully caption a
molecule, yet be unable to accurately reconstruct the original structure from
its own generated text. This inconsistency suggests that models are learning
unidirectional memorization rather than flexible mastery. Indeed, recent work
has demonstrated a strong correlation between a model's round-trip consistency
and its performance on the primary tasks. This strong correlation reframes
consistency into a direct target for model improvement. We therefore introduce
Round-Trip Reinforcement Learning (RTRL), a novel framework that trains a model
to improve its consistency by using the success of a round-trip transformation
as a reward signal. We further propose an iterative variant where forward and
reverse mappings alternately train each other in a self-improvement loop, a
process that is highly data-efficient and notably effective with the massive
amount of unlabelled data common in chemistry. Experiments demonstrate that
RTRL significantly \textbf{boosts performance and consistency} over strong
baselines across supervised, self-supervised, and synthetic data regimes. This
work shows that round-trip consistency is not just a desirable property but a
trainable objective, offering a new path toward more robust and reliable
foundation models.

</details>


### [138] [NVIDIA AI Aerial: AI-Native Wireless Communications](https://arxiv.org/abs/2510.01533)
*Kobi Cohen-Arazi,Michael Roe,Zhen Hu,Rohan Chavan,Anna Ptasznik,Joanna Lin,Joao Morais,Joseph Boccuzzi,Tommaso Balercia*

Main category: cs.LG

TL;DR: A framework for compiling Python-based AI algorithms into GPU-runnable blobs, enabling efficient integration of DSP and ML in 6G networks, demonstrated through CNN-based channel estimation.


<details>
  <summary>Details</summary>
Motivation: 6G requires seamless integration of digital signal processing and machine learning in cellular networks, bringing network life cycles closer to AI systems with iterative training, simulation, and deployment.

Method: Proposes a framework that compiles Python-based algorithms into GPU-runnable blobs, using convolutional neural networks for channel estimation in PUSCH receivers, tested in digital twins and real-time testbeds via NVIDIA AI Aerial platform.

Result: Achieves a unified approach ensuring efficiency, flexibility, and high performance on NVIDIA GPUs, successfully demonstrating CNN-based channel estimation.

Conclusion: The methodology lays the foundation for scalable AI/ML integration in next-generation cellular systems, essential for realizing natively intelligent 6G networks.

Abstract: 6G brings a paradigm shift towards AI-native wireless systems, necessitating
the seamless integration of digital signal processing (DSP) and machine
learning (ML) within the software stacks of cellular networks. This
transformation brings the life cycle of modern networks closer to AI systems,
where models and algorithms are iteratively trained, simulated, and deployed
across adjacent environments. In this work, we propose a robust framework that
compiles Python-based algorithms into GPU-runnable blobs. The result is a
unified approach that ensures efficiency, flexibility, and the highest possible
performance on NVIDIA GPUs. As an example of the capabilities of the framework,
we demonstrate the efficacy of performing the channel estimation function in
the PUSCH receiver through a convolutional neural network (CNN) trained in
Python. This is done in a digital twin first, and subsequently in a real-time
testbed. Our proposed methodology, realized in the NVIDIA AI Aerial platform,
lays the foundation for scalable integration of AI/ML models into
next-generation cellular systems, and is essential for realizing the vision of
natively intelligent 6G networks.

</details>


### [139] [TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis](https://arxiv.org/abs/2510.01538)
*Haokun Zhao,Xiang Zhang,Jiaqi Wei,Yiwei Xu,Yuting He,Siqi Sun,Chenyu You*

Main category: cs.LG

TL;DR: TSci is an LLM-driven agentic framework for general time series forecasting that uses four specialized agents (Curator, Planner, Forecaster, Reporter) to automate preprocessing, model selection, validation, and reporting, achieving state-of-the-art performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current time series forecasting faces challenges with thousands of short, noisy series across different domains, requiring labor-intensive preprocessing and validation. Existing models are domain-specific and generalize poorly, creating demand for a domain-agnostic framework that minimizes human intervention.

Method: The framework uses four specialized agents: Curator performs LLM-guided diagnostics and preprocessing; Planner narrows model choices using multi-modal diagnostics; Forecaster handles model fitting, validation, and ensemble selection; Reporter synthesizes the process into transparent reports.

Result: Empirical results on eight benchmarks show TSci consistently outperforms both statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2% respectively, while producing transparent and interpretable reports.

Conclusion: TSci transforms time series forecasting into a white-box system that is both interpretable and extensible across tasks, demonstrating superior performance and transparency compared to existing approaches.

Abstract: Time series forecasting is central to decision-making in domains as diverse
as energy, finance, climate, and public health. In practice, forecasters face
thousands of short, noisy series that vary in frequency, quality, and horizon,
where the dominant cost lies not in model fitting, but in the labor-intensive
preprocessing, validation, and ensembling required to obtain reliable
predictions. Prevailing statistical and deep learning models are tailored to
specific datasets or domains and generalize poorly. A general, domain-agnostic
framework that minimizes human intervention is urgently in demand. In this
paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic
framework for general time series forecasting. The framework comprises four
specialized agents: Curator performs LLM-guided diagnostics augmented by
external tools that reason over data statistics to choose targeted
preprocessing; Planner narrows the hypothesis space of model choice by
leveraging multi-modal diagnostics and self-planning over the input; Forecaster
performs model fitting and validation and, based on the results, adaptively
selects the best model configuration as well as ensemble strategy to make final
predictions; and Reporter synthesizes the whole process into a comprehensive,
transparent report. With transparent natural-language rationales and
comprehensive reports, TSci transforms the forecasting workflow into a
white-box system that is both interpretable and extensible across tasks.
Empirical results on eight established benchmarks demonstrate that TSci
consistently outperforms both statistical and LLM-based baselines, reducing
forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci
produces a clear and rigorous report that makes the forecasting workflow more
transparent and interpretable.

</details>


### [140] [Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code](https://arxiv.org/abs/2510.01539)
*Aniket Vashishtha,Qirun Dai,Hongyuan Mei,Amit Sharma,Chenhao Tan,Hao Peng*

Main category: cs.LG

TL;DR: The paper introduces 'executable counterfactuals' framework to evaluate LLMs' full counterfactual reasoning (abduction, intervention, prediction), revealing 25-40% performance drop in SOTA models when abduction is required. RL training shows better generalization than supervised finetuning.


<details>
  <summary>Details</summary>
Motivation: Existing LLM evaluations skip the abduction step in counterfactual reasoning, leading to overestimation of capabilities. This gap is critical for applications in high-stakes domains like scientific research.

Method: Proposed executable counterfactuals framework using code and math problems that explicitly require all three counterfactual reasoning steps. Compared supervised finetuning vs reinforcement learning for training.

Result: SOTA models show 25-40% accuracy drop from interventional to full counterfactual reasoning. RL training achieved 1.5x-2x improvement on code problems and better generalization to math problems, while supervised finetuning degraded OOD performance.

Conclusion: RL induces core cognitive behaviors for counterfactual reasoning and generalizes better than supervised approaches. The framework provides scalable evaluation and training for improving LLMs' causal understanding.

Abstract: Counterfactual reasoning, a hallmark of intelligence, consists of three
steps: inferring latent variables from observations (abduction), constructing
alternatives (interventions), and predicting their outcomes (prediction). This
skill is essential for advancing LLMs' causal understanding and expanding their
applications in high-stakes domains such as scientific research. However,
existing efforts in assessing LLM's counterfactual reasoning capabilities tend
to skip the abduction step, effectively reducing to interventional reasoning
and leading to overestimation of LLM performance. To address this, we introduce
executable counterfactuals, a novel framework that operationalizes causal
reasoning through code and math problems. Our framework explicitly requires all
three steps of counterfactual reasoning and enables scalable synthetic data
creation with varying difficulty, creating a frontier for evaluating and
improving LLM's reasoning. Our results reveal substantial drop in accuracy
(25-40%) from interventional to counterfactual reasoning for SOTA models like
o4-mini and Claude-4-Sonnet. To address this gap, we construct a training set
comprising counterfactual code problems having if-else condition and test on
out-of-domain code structures (e.g. having while-loop); we also test whether a
model trained on code would generalize to counterfactual math word problems.
While supervised finetuning on stronger models' reasoning traces improves
in-domain performance of Qwen models, it leads to a decrease in accuracy on OOD
tasks such as counterfactual math problems. In contrast, reinforcement learning
induces the core cognitive behaviors and generalizes to new domains, yielding
gains over the base model on both code (improvement of 1.5x-2x) and math
problems. Analysis of the reasoning traces reinforces these findings and
highlights the promise of RL for improving LLMs' counterfactual reasoning.

</details>


### [141] [Predictive Preference Learning from Human Interventions](https://arxiv.org/abs/2510.01545)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.LG

TL;DR: PPL is a predictive preference learning method that propagates human interventions into future states to improve agent safety and learning efficiency in interactive imitation learning.


<details>
  <summary>Details</summary>
Motivation: Existing interactive imitation learning methods only correct agent actions at current states but fail to adjust future potentially hazardous actions, limiting their effectiveness in safety-critical scenarios.

Method: PPL bootstraps each human intervention into L future time steps (preference horizon), assuming the agent follows the same action and human makes the same intervention, then applies preference optimization on these future states.

Result: The method significantly improves learning efficiency and reduces human demonstrations needed in autonomous driving and robotic manipulation benchmarks.

Conclusion: PPL effectively propagates expert corrections to safety-critical regions, with theoretical analysis showing appropriate preference horizon selection balances risky state coverage and label correctness to bound optimality gap.

Abstract: Learning from human involvement aims to incorporate the human subject to
monitor and correct agent behavior errors. Although most interactive imitation
learning methods focus on correcting the agent's action at the current state,
they do not adjust its actions in future states, which may be potentially more
hazardous. To address this, we introduce Predictive Preference Learning from
Human Interventions (PPL), which leverages the implicit preference signals
contained in human interventions to inform predictions of future rollouts. The
key idea of PPL is to bootstrap each human intervention into L future time
steps, called the preference horizon, with the assumption that the agent
follows the same action and the human makes the same intervention in the
preference horizon. By applying preference optimization on these future states,
expert corrections are propagated into the safety-critical regions where the
agent is expected to explore, significantly improving learning efficiency and
reducing human demonstrations needed. We evaluate our approach with experiments
on both autonomous driving and robotic manipulation benchmarks and demonstrate
its efficiency and generality. Our theoretical analysis further shows that
selecting an appropriate preference horizon L balances coverage of risky states
with label correctness, thereby bounding the algorithmic optimality gap. Demo
and code are available at: https://metadriverse.github.io/ppl

</details>


### [142] [MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models](https://arxiv.org/abs/2510.01549)
*Kevin Zhai,Utsav Singh,Anirudh Thatipelli,Souradip Chakraborty,Anit Kumar Sahu,Furong Huang,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.LG

TL;DR: MIRA is a training-free inference-time alignment method that prevents reward hacking in diffusion models by using image-space KL regularization to maintain prompt adherence while optimizing for rewards.


<details>
  <summary>Details</summary>
Motivation: Diffusion models often fail to satisfy user-specific criteria measured by scalar rewards, and existing inference-time alignment methods suffer from reward hacking where images score highly but deviate from the original prompt.

Method: MIRA introduces an image-space, score-based KL surrogate that regularizes the sampling trajectory with a frozen backbone, constraining the output distribution to prevent off-distribution drift while increasing rewards.

Result: Across SDv1.5 and SDXL, multiple rewards (Aesthetic, HPSv2, PickScore), and public datasets, MIRA achieves >60% win rate vs. strong baselines while preserving prompt adherence, with reward gains and near-zero drift.

Conclusion: MIRA effectively mitigates reward hacking in diffusion models through image-space constraints, and MIRA-DPO extends this approach to non-differentiable rewards without fine-tuning.

Abstract: Diffusion models excel at generating images conditioned on text prompts, but
the resulting images often do not satisfy user-specific criteria measured by
scalar rewards such as Aesthetic Scores. This alignment typically requires
fine-tuning, which is computationally demanding. Recently, inference-time
alignment via noise optimization has emerged as an efficient alternative,
modifying initial input noise to steer the diffusion denoising process towards
generating high-reward images. However, this approach suffers from reward
hacking, where the model produces images that score highly, yet deviate
significantly from the original prompt. We show that noise-space regularization
is insufficient and that preventing reward hacking requires an explicit
image-space constraint. To this end, we propose MIRA (MItigating Reward
hAcking), a training-free, inference-time alignment method. MIRA introduces an
image-space, score-based KL surrogate that regularizes the sampling trajectory
with a frozen backbone, constraining the output distribution so reward can
increase without off-distribution drift (reward hacking). We derive a tractable
approximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple
rewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g.,
Animal-Animal, HPDv2), MIRA achieves >60\% win rate vs. strong baselines while
preserving prompt adherence; mechanism plots show reward gains with near-zero
drift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO,
mapping preference optimization to inference time with a frozen backbone,
extending MIRA to non-differentiable rewards without fine-tuning.

</details>


### [143] [Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization](https://arxiv.org/abs/2510.01555)
*Kezhao Liu,Jason Klein Liu,Mingtao Chen,Yiming Liu*

Main category: cs.LG

TL;DR: This paper establishes a unified framework for understanding KL divergence regularization in RLHF, showing that '$k_1$ in reward' (PPO-style) and '$k_2$ as loss' are gradient-equivalent and theoretically sound, while '$k_3$ as loss' (GRPO-style) is a biased approximation.


<details>
  <summary>Details</summary>
Motivation: To address the confusion in RLHF implementations where KL divergence is sometimes treated as a numerical coefficient rather than an optimization loss, and to provide principled guidance for correct KL regularization.

Method: Established a unified framework connecting two implementation styles: '$k_n$ in reward' (detached coefficient) and '$k_n$ as loss' (direct loss function). Proved gradient equivalence between '$k_1$ in reward' and '$k_2$ as loss' under on-policy conditions, and identified biases in '$k_3$ as loss' implementations.

Result: Showed that '$k_1$ in reward' is the principled loss for Reverse KL regularization, and '$k_2$ as loss' is gradient-equivalent to it. Demonstrated that '$k_3$ as loss' is a first-order biased approximation. Proposed principled correction for off-policy implementations.

Conclusion: The work provides a comprehensive gradient-based rationale for choosing and correctly implementing KL regularization in RLHF, identifying theoretically sound approaches and exposing biased approximations, leading to more robust RLHF systems.

Abstract: Reinforcement Learning from Human Feedback (RLHF) leverages a
Kullback-Leibler (KL) divergence loss to stabilize training and prevent
overfitting. However, in methods such as GRPO, its implementation may be guided
by principles from numerical value estimation-a practice that overlooks the
term's functional role as an optimization loss. To analyze this issue, we
establish a unified framework that connects two seemingly distinct
implementation styles: using the mathematical term $k_n$ as a detached
coefficient for the policy's score function ('$k_n$ in reward') or as a direct
loss function through which gradients are propagated ('$k_n$ as loss'). We show
that the latter can always be analyzed via an equivalent gradient coefficient
in the former, unifying the two perspectives. Through this framework, we prove
that the conventional '$k_1$ in reward' (like in PPO) is the principled loss
for Reverse KL (RKL) regularization. We further establish a key finding: under
on-policy conditions, the '$k_2$ as loss' formulation is, in fact,
gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our
work, identifies both as the theoretically sound implementations of the RKL
objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like
in GRPO) is merely a first-order, biased approximation of the principled loss.
Furthermore, we argue that common off-policy implementations of '$k_n$ as loss'
methods are biased due to neglected importance sampling, and we propose a
principled correction. Our findings provide a comprehensive, gradient-based
rationale for choosing and correctly implementing KL regularization, paving the
way for more robust and effective RLHF systems.

</details>


### [144] [Large-Scale Bayesian Causal Discovery with Interventional Data](https://arxiv.org/abs/2510.01562)
*Seong Woo Han,Daniel Duy Vo,Brielin C. Brown*

Main category: cs.LG

TL;DR: IBCD is an empirical Bayesian framework for causal discovery using interventional data that models total causal effects with matrix normal distribution, uses spike-and-slab horseshoe priors, and provides uncertainty quantification through edge posterior inclusion probabilities.


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods perform poorly on large-scale tasks and lack uncertainty quantification, despite advancements in genomic perturbation screens that provide interventional data.

Method: Models likelihood of total causal effects matrix using matrix normal distribution, places spike-and-slab horseshoe prior on edges, learns data-driven weights for scale-free and ErdÅs-RÃ©nyi structures from observational data, and treats edges as latent variables.

Result: IBCD achieves superior structure recovery compared to existing baselines in simulations and successfully identifies robust graph structures when applied to CRISPR perturbation data on 521 genes.

Conclusion: The proposed framework enables uncertainty-aware causal discovery with interventional data and demonstrates strong performance on large-scale genomic applications.

Abstract: Inferring the causal relationships among a set of variables in the form of a
directed acyclic graph (DAG) is an important but notoriously challenging
problem. Recently, advancements in high-throughput genomic perturbation screens
have inspired development of methods that leverage interventional data to
improve model identification. However, existing methods still suffer poor
performance on large-scale tasks and fail to quantify uncertainty. Here, we
propose Interventional Bayesian Causal Discovery (IBCD), an empirical Bayesian
framework for causal discovery with interventional data. Our approach models
the likelihood of the matrix of total causal effects, which can be approximated
by a matrix normal distribution, rather than the full data matrix. We place a
spike-and-slab horseshoe prior on the edges and separately learn data-driven
weights for scale-free and Erd\H{o}s-R\'enyi structures from observational
data, treating each edge as a latent variable to enable uncertainty-aware
inference. Through extensive simulation, we show that IBCD achieves superior
structure recovery compared to existing baselines. We apply IBCD to CRISPR
perturbation (Perturb-seq) data on 521 genes, demonstrating that edge posterior
inclusion probabilities enable identification of robust graph structures.

</details>


### [145] [TetriServe: Efficient DiT Serving for Heterogeneous Image Generation](https://arxiv.org/abs/2510.01565)
*Runyu Lu,Shiqi He,Wenxuan Tan,Shenggui Li,Ruofan Wu,Jeff J. Ma,Ang Chen,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: TetriServe is a DiT serving system that uses step-level sequence parallelism and round-based scheduling to improve SLO attainment by up to 32% compared to existing solutions, while maintaining image quality.


<details>
  <summary>Details</summary>
Motivation: Existing DiT serving systems use fixed parallelism, which is inefficient for heterogeneous workloads with mixed resolutions and deadlines, leading to poor GPU utilization and low SLO attainment.

Method: Proposes step-level sequence parallelism to dynamically adjust parallel degree per request deadline, and introduces round-based scheduling with time discretization, adaptive parallelism at step level, and joint request packing to minimize late completions.

Result: Achieves up to 32% higher SLO attainment compared to existing solutions without degrading image quality.

Conclusion: TetriServe provides an efficient solution for DiT model serving that significantly improves SLO attainment through dynamic parallelism adjustment and intelligent scheduling.

Abstract: Diffusion Transformer (DiT) models excel at generating highquality images
through iterative denoising steps, but serving them under strict Service Level
Objectives (SLOs) is challenging due to their high computational cost,
particularly at large resolutions. Existing serving systems use fixed degree
sequence parallelism, which is inefficient for heterogeneous workloads with
mixed resolutions and deadlines, leading to poor GPU utilization and low SLO
attainment.
  In this paper, we propose step-level sequence parallelism to dynamically
adjust the parallel degree of individual requests according to their deadlines.
We present TetriServe, a DiT serving system that implements this strategy for
highly efficient image generation. Specifically, TetriServe introduces a novel
round-based scheduling mechanism that improves SLO attainment: (1) discretizing
time into fixed rounds to make deadline-aware scheduling tractable, (2)
adapting parallelism at the step level and minimize GPU hour consumption, and
(3) jointly packing requests to minimize late completions. Extensive evaluation
on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher
SLO attainment compared to existing solutions without degrading image quality.

</details>


### [146] [From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?](https://arxiv.org/abs/2510.01571)
*Hanqun Cao,Hongrui Zhang,Junde Xu,Zhou Zhang,Lingdong Shen,Minghao Sun,Ge Liu,Jinbo Xu,Wu-Jun Li,Jinren Ni,Cesar de la Fuente-Nunez,Tianfan Fu,Yejin Choi,Pheng-Ann Heng,Fang Wu*

Main category: cs.LG

TL;DR: RL combined with protein language models improves protein design success rates and efficiency across multiple domains, with performance gains depending on task headroom, reward fidelity, and policy capacity.


<details>
  <summary>Details</summary>
Motivation: To determine if reinforcement learning can push protein language models beyond their pretraining priors to uncover latent sequence-structure-function rules that supervised learning might miss.

Method: Pairing RL with PLMs across four domains (antimicrobial peptide design, kinase variant optimization, antibody engineering, inverse folding) using diverse RL algorithms and model classes.

Result: RL consistently boosts success rates and sample efficiency across benchmarks. Performance follows a three-factor interaction: task headroom, reward fidelity, and policy capacity jointly determine gains.

Conclusion: Practical guidance for RL in protein design: prioritize reward modeling and calibration before scaling policy size, match algorithm and regularization strength to task difficulty, and allocate capacity where marginal gains are largest.

Abstract: Protein language models (PLMs) have advanced computational protein science
through large-scale pretraining and scalable architectures. In parallel,
reinforcement learning (RL) has broadened exploration and enabled precise
multi-objective optimization in protein design. Yet whether RL can push PLMs
beyond their pretraining priors to uncover latent sequence-structure-function
rules remains unclear. We address this by pairing RL with PLMs across four
domains: antimicrobial peptide design, kinase variant optimization, antibody
engineering, and inverse folding. Using diverse RL algorithms and model
classes, we ask if RL improves sampling efficiency and, more importantly, if it
reveals capabilities not captured by supervised learning. Across benchmarks, RL
consistently boosts success rates and sample efficiency. Performance follows a
three-factor interaction: task headroom, reward fidelity, and policy capacity
jointly determine gains. When rewards are accurate and informative, policies
have sufficient capacity, and tasks leave room beyond supervised baselines,
improvements scale; when rewards are noisy or capacity is constrained, gains
saturate despite exploration. This view yields practical guidance for RL in
protein design: prioritize reward modeling and calibration before scaling
policy size, match algorithm and regularization strength to task difficulty,
and allocate capacity where marginal gains are largest. Implementation is
available at https://github.com/chq1155/RL-PLM.

</details>


### [147] [Gradient Shaping Beyond Clipping: A Functional Perspective on Update Magnitude Control](https://arxiv.org/abs/2510.01578)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: SPAMP is a unified framework that generalizes gradient clipping into smooth, per-layer gradient shaping using statistical tracking and adaptive modulation.


<details>
  <summary>Details</summary>
Motivation: Traditional gradient clipping uses hard, fixed thresholds that lack flexibility and ignore gradient distribution dynamics, limiting its effectiveness in stabilizing deep network training.

Method: SPAMP tracks local gradient statistics, dynamically estimates thresholds, and applies power-based transformations to modulate update magnitudes in a differentiable manner, treating clipping and warmup as dual mechanisms for controlling effective update scale.

Result: Extensive experiments across image and language tasks show that SPAMP improves stability, convergence, and robustness over existing gradient clipping methods.

Conclusion: SPAMP provides a principled alternative to rigid heuristics by offering a unified framework for adaptive gradient shaping that outperforms traditional clipping approaches.

Abstract: Gradient clipping is widely used to stabilize deep network training, but its
formulation as a hard, fixed threshold limits flexibility and ignores gradient
distribution dynamics. We propose SPAMP (Statistical Per-layer Adaptive
Modulation and Projection), a unified framework that generalizes clipping into
smooth, per-layer gradient shaping. SPAMP tracks local gradient statistics,
dynamically estimates thresholds, and applies power-based transformations to
modulate update magnitudes in a differentiable manner. This perspective recasts
clipping and warmup as dual mechanisms for controlling the effective update
scale $\eta_t \|g_t\|$, offering a principled alternative to rigid heuristics.
Extensive experiments across image and language tasks demonstrate that SPAMP
improves stability, convergence, and robustness over existing methods.

</details>


### [148] [Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression](https://arxiv.org/abs/2510.01581)
*Joykirat Singh,Justin Chih-Yao Chen,Archiki Prasad,Elias Stengel-Eskin,Akshay Nambi,Mohit Bansal*

Main category: cs.LG

TL;DR: TRAAC is an RL method that adaptively adjusts reasoning length based on problem difficulty, using self-attention to compress reasoning steps and improve efficiency.


<details>
  <summary>Details</summary>
Motivation: Current models suffer from under-adaptivity - they either underthink (too short reasoning for hard problems) or overthink (too long reasoning even after solving), leading to inefficiency.

Method: TRAAC uses online post-training RL with self-attention over reasoning trajectories to identify important steps and prune redundant ones, while incorporating difficulty estimation into rewards.

Result: TRAAC achieves 8.4% accuracy gain with 36.8% reasoning length reduction vs base model, and 7.9% accuracy gain with 29.4% length reduction vs best RL baseline across multiple tasks.

Conclusion: TRAAC enables adaptive thinking by combining difficulty calibration and attention-based compression, showing strong generalization to out-of-distribution tasks.

Abstract: Recent thinking models solve complex reasoning tasks by scaling test-time
compute, but this scaling must be allocated in line with task difficulty. On
one hand, short reasoning (underthinking) leads to errors on harder problems
that require extended reasoning steps; but, excessively long reasoning
(overthinking) can be token-inefficient, generating unnecessary steps even
after reaching a correct intermediate solution. We refer to this as
under-adaptivity, where the model fails to modulate its response length
appropriately given problems of varying difficulty. To address under-adaptivity
and strike a balance between under- and overthinking, we propose TRAAC (Think
Right with Adaptive, Attentive Compression), an online post-training RL method
that leverages the model's self-attention over a long reasoning trajectory to
identify important steps and prune redundant ones. TRAAC also estimates
difficulty and incorporates it into training rewards, thereby learning to
allocate reasoning budget commensurate with example difficulty. Our approach
improves accuracy, reduces reasoning steps, and enables adaptive thinking
compared to base models and other RL baselines. Across a variety of tasks
(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute
accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%
compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length
drop compared to the best RL baseline. TRAAC also shows strong generalization:
although our models are trained on math datasets, they show accuracy and
efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,
and OptimalThinkingBench. Our analysis further verifies that TRAAC provides
fine-grained adjustments to thinking budget based on difficulty and that a
combination of task-difficulty calibration and attention-based compression
yields gains across diverse tasks.

</details>


### [149] [Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation](https://arxiv.org/abs/2510.01588)
*Ziming Tang,Chengbin Hou,Tianyu Zhang,Bangxu Tian,Jinbao Wang,Hairong Lv*

Main category: cs.LG

TL;DR: NoRo is a noise-robust framework that enhances UPDRS prediction accuracy in Parkinson's disease telemonitoring by using contrastive learning to generate robust features against patient-induced inaccuracies, environmental noise, and data transmission loss.


<details>
  <summary>Details</summary>
Motivation: Parkinson's disease telemonitoring faces challenges from three types of noise: patient-induced measurement inaccuracies, environmental noise, and data packet loss during transmission, which increase prediction errors in UPDRS scoring.

Method: The framework groups speech features into ordered bins based on selected feature values to create contrastive pairs, trains a multilayer perceptron encoder using these pairs to generate noise-robust features, then concatenates these with original features as augmented input for UPDRS prediction models.

Result: Extensive experiments with customizable noise injection show that NoRo successfully enhances noise robustness across various downstream prediction models under different noisy environments.

Conclusion: NoRo effectively addresses noise challenges in PD telemonitoring and improves the reliability of at-home UPDRS assessments through noise-robust feature learning.

Abstract: Parkinson's disease (PD) is one of the most common neurodegenerative
disorder. PD telemonitoring emerges as a novel assessment modality enabling
self-administered at-home tests of Unified Parkinson's Disease Rating Scale
(UPDRS) scores, enhancing accessibility for PD patients. However, three types
of noise would occur during measurements: (1) patient-induced measurement
inaccuracies, (2) environmental noise, and (3) data packet loss during
transmission, resulting in higher prediction errors. To address these
challenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,
the original speech features are grouped into ordered bins, based on the
continuous values of a selected feature, to construct contrastive pairs.
Second, the contrastive pairs are employed to train a multilayer perceptron
encoder for generating noise-robust features. Finally, these features are
concatenated with the original features as the augmented features, which are
then fed into the UPDRS prediction models. Notably, we further introduces a
novel evaluation approach with customizable noise injection module, and
extensive experiments show that NoRo can successfully enhance the noise
robustness of UPDRS prediction across various downstream prediction models
under different noisy environments.

</details>


### [150] [Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness](https://arxiv.org/abs/2510.01598)
*Youwei Bao,Shuhan Yang,Hyunsoo Yang*

Main category: cs.LG

TL;DR: This paper proposes using spin-transfer torque magnetic tunnel junctions (STT-MTJs) to generate true random numbers for generative AI security, reducing vulnerable outputs by 18.6x compared to conventional PRNGs.


<details>
  <summary>Details</summary>
Motivation: Deterministic pseudo random number generators (PRNGs) in generative AI models create predictable patterns that are vulnerable to attacks, while conventional defenses have significant energy and latency overhead.

Method: Embed hardware-generated true random bits from STT-MTJs in a highly parallel FPGA-assisted computing system, integrating these hardware random bits into generative adversarial networks (GANs).

Result: The system delivers megabit-per-second true random numbers passing NIST tests, reduces insecure GAN outputs by up to 18.6 times, and has potential for gigabit-per-second throughput with scalability beyond 10^6 parallel cells.

Conclusion: STT-MTJ-based spintronic RNGs are practical security components for next-generation generative AI systems, offering nanosecond switching speed, high energy efficiency, and established scalability.

Abstract: Deterministic pseudo random number generators (PRNGs) used in generative
artificial intelligence (GAI) models produce predictable patterns vulnerable to
exploitation by attackers. Conventional defences against the vulnerabilities
often come with significant energy and latency overhead. Here, we embed
hardware-generated true random bits from spin-transfer torque magnetic tunnel
junctions (STT-MTJs) to address the challenges. A highly parallel,
FPGA-assisted prototype computing system delivers megabit-per-second true
random numbers, passing NIST randomness tests after in-situ operations with
minimal overhead. Integrating the hardware random bits into a generative
adversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to
18.6 times compared to the low-quality random number generators (RNG) baseline.
With nanosecond switching speed, high energy efficiency, and established
scalability, our STT-MTJ-based system holds the potential to scale beyond 106
parallel cells, achieving gigabit-per-second throughput suitable for large
language model sampling. This advancement highlights spintronic RNGs as
practical security components for next-generation GAI systems.

</details>


### [151] [Posterior Collapse as a Phase Transition in Variational Autoencoders](https://arxiv.org/abs/2510.01621)
*Zhen Li,Fan Zhang,Zheng Zhang,Yu Chen*

Main category: cs.LG

TL;DR: Posterior collapse in VAEs is a phase transition governed by data structure and model hyper-parameters, characterized by a critical threshold in KL divergence between posterior and prior.


<details>
  <summary>Details</summary>
Motivation: To understand posterior collapse in variational autoencoders from a statistical physics perspective, revealing it as a phase transition rather than just optimization failure.

Method: Analyzed stability of trivial solution associated with posterior collapse, identified critical hyper-parameter threshold, and validated on synthetic and real-world datasets.

Result: Discovered a critical boundary separating meaningful latent inference from collapse, characterized by discontinuity in KL divergence, confirming phase transition behavior.

Conclusion: Posterior collapse is an emerging phase transition from interplay between data structure and variational constraints, providing new insights into deep generative model trainability and capacity.

Abstract: We investigate the phenomenon of posterior collapse in variational
autoencoders (VAEs) from the perspective of statistical physics, and reveal
that it constitutes a phase transition governed jointly by data structure and
model hyper-parameters. By analyzing the stability of the trivial solution
associated with posterior collapse, we identify a critical hyper-parameter
threshold. This critical boundary, separating meaningful latent inference from
collapse, is characterized by a discontinuity in the KL divergence between the
approximate posterior and the prior distribution. We validate this critical
behavior on both synthetic and real-world datasets, confirming the existence of
a phase transition. Our results demonstrate that posterior collapse is not
merely an optimization failure, but rather an emerging phase transition arising
from the interplay between data structure and variational constraints. This
perspective offers new insights into the trainability and representational
capacity of deep generative models.

</details>


### [152] [Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead](https://arxiv.org/abs/2510.01624)
*Feiyang Kang,Michael Kuchnik,Karthik Padthe,Marin Vlastelica,Ruoxi Jia,Carole-Jean Wu,Newsha Ardalani*

Main category: cs.LG

TL;DR: High SFT scores don't reliably predict RL success; generalization loss and Pass@large k are better proxies for RL outcomes.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that high SFT scores translate to improved performance after RL training, as current practice treats SFT and RL as independent stages.

Method: Trained hundreds of models up to 12B parameters with SFT and RLVR via GRPO, evaluated on 7 math benchmarks with up to 256 repetitions using $>1M GPU hours.

Result: Found high SFT scores can be biased and don't predict RL gains; generalization loss and Pass@large k improve prediction accuracy (RÂ² and Spearman correlation up by 0.5, 2x improvement).

Conclusion: Alternative metrics like generalization loss and Pass@large k provide better proxies for RL outcomes than SFT scores alone, with practical implications for training strategies.

Abstract: In post-training for reasoning Large Language Models (LLMs), the current
state of practice trains LLMs in two independent stages: Supervised Fine-Tuning
(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as
``RL'' below). In this work, we challenge whether high SFT scores translate to
improved performance after RL. We provide extensive counter-examples where this
is not true. We find high SFT scores can be biased toward simpler or more
homogeneous data and are not reliably predictive of subsequent RL gains or
scaled-up post-training effectiveness. In some cases, RL training on models
with improved SFT performance could lead to substantially worse outcome
compared to RL on the base model without SFT. We study alternative metrics and
identify generalization loss on held-out reasoning examples and Pass@large k
performance to provide strong proxies for the RL outcome. We trained hundreds
of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive
evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU
hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple
state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL
performance, prediction based on generalization loss and Pass@large k achieves
substantial higher precision, improving $R^2$ coefficient and Spearman's rank
correlation coefficient by up to 0.5 (2x). This provides strong utility for
broad use cases. For example, in most experiments, we find SFT training on
unique examples for a one epoch underperforms training on half examples for two
epochs, either after SFT or SFT-then-RL; With the same SFT budget, training
only on short examples may lead to better SFT performance, though, it often
leads to worse outcome after RL compared to training on examples with varying
lengths. Evaluation tool will be open-sourced.

</details>


### [153] [Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls](https://arxiv.org/abs/2510.01631)
*Feiyang Kang,Newsha Ardalani,Michael Kuchnik,Youssef Emad,Mostafa Elhoushi,Shubhabrata Sengupta,Shang-Wen Li,Ramya Raghavendra,Ruoxi Jia,Carole-Jean Wu*

Main category: cs.LG

TL;DR: Synthetic data alone isn't better than natural web data for LLM pre-training, but mixing 1/3 rephrased synthetic data with 2/3 natural data can speed up training 5-10x. Optimal synthetic data ratio is ~30% and depends on model size and data budget.


<details>
  <summary>Details</summary>
Motivation: High-quality training data is limited for LLM scaling, and synthetic data offers a potential solution, but its effectiveness needs empirical validation.

Method: Large-scale empirical investigation (>1000 LLMs, >100k GPU hours) using unified protocol and scaling laws, comparing natural web data, diverse synthetic types (rephrased text, generated textbooks), and mixtures.

Result: Rephrased synthetic data alone isn't faster than natural data; 1/3 rephrased synthetic + 2/3 natural speeds up training 5-10x; textbook-style synthetic alone performs worse; optimal synthetic ratio ~30%; larger generators don't necessarily yield better data.

Conclusion: Synthetic data has conditional benefits in pre-training, with rephrased synthetic showing no model collapse while textbook-style synthetic shows collapse patterns, providing practical guidance for synthetic data usage.

Abstract: Training data plays a crucial role in Large Language Models (LLM) scaling,
yet high quality data is of limited supply. Synthetic data techniques offer a
potential path toward sidestepping these limitations. We conduct a large-scale
empirical investigation (>1000 LLMs with >100k GPU hours) using a unified
protocol and scaling laws, comparing natural web data, diverse synthetic types
(rephrased text, generated textbooks), and mixtures of natural and synthetic
data. Specifically, we found pre-training on rephrased synthetic data
\textit{alone} is not faster than pre-training on natural web texts; while
pre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts
can speed up 5-10x (to reach the same validation loss) at larger data budgets.
Pre-training on textbook-style synthetic data \textit{alone} results in notably
higher loss on many downstream domains especially at small data budgets. "Good"
ratios of synthetic data in training data mixtures depend on the model size and
data budget, empirically converging to ~30% for rephrased synthetic data.
Larger generator models do not necessarily yield better pre-training data than
~8B-param models. These results contribute mixed evidence on "model collapse"
during large-scale single-round (n=1) model training on synthetic
data--training on rephrased synthetic data shows no degradation in performance
in foreseeable scales whereas training on mixtures of textbook-style
pure-generated synthetic data shows patterns predicted by "model collapse". Our
work demystifies synthetic data in pre-training, validates its conditional
benefits, and offers practical guidance.

</details>


### [154] [CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning](https://arxiv.org/abs/2510.01634)
*Ryan Y. Lin,Siddhartha Ojha,Nicholas Bai*

Main category: cs.LG

TL;DR: CAT is a transformer architecture that dynamically routes tokens across Euclidean, hyperbolic, and spherical attention branches using a learnable gating mechanism, enabling adaptive geometric specialization for mixed-geometry data.


<details>
  <summary>Details</summary>
Motivation: Existing transformers assume Euclidean geometry, limiting effectiveness on non-Euclidean data. Fixed-geometry extensions (hyperbolic/spherical) lack flexibility for mixed geometric properties in real-world data.

Method: Introduces Curvature-Adaptive Transformer with three geometric attention branches and a lightweight differentiable gating mechanism that learns per-token routing across Euclidean, hyperbolic, and spherical spaces.

Result: Achieves ~10% improvements in MRR and Hits@10 on knowledge graph benchmarks (FB15k-237, WN18RR) over fixed-geometry baselines with only 5% parameter increase and comparable inference time.

Conclusion: Learned geometric adaptation outperforms any single fixed geometry for complex relational reasoning, establishing CAT as a scalable and interpretable foundation for mixture-of-geometry architectures across domains.

Abstract: Transformers achieve strong performance across diverse domains but implicitly
assume Euclidean geometry in their attention mechanisms, limiting their
effectiveness on data with non-Euclidean structure. While recent extensions to
hyperbolic and spherical spaces show promise for hierarchical and cyclical
patterns, respectively, they require committing to a single geometry a priori,
reducing flexibility when data exhibits mixed geometric properties. We
introduce the Curvature-Adaptive Transformer (CAT), a novel architecture that
dynamically learns per-token routing across three geometric attention branches
through a lightweight, differentiable gating mechanism. Unlike fixed-geometry
approaches, CAT enables adaptive geometric specialization, routing tokens to
the appropriate curvature based on their local relational structure. The
routing network provides interpretable curvature preferences while each branch
employs geometry-specific operations optimized for its respective manifold. On
knowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves
approximately 10% improvements in MRR and Hits@10 over fixed-geometry baselines
with minimal overhead (5% parameter increase, comparable inference time). These
results demonstrate that learned geometric adaptation outperforms any single
fixed geometry for complex relational reasoning, establishing CAT as a scalable
and interpretable foundation for mixture-of-geometry architectures across
language, vision, and multimodal domains.

</details>


### [155] [Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking](https://arxiv.org/abs/2510.01637)
*Liyan Xie,Muhammad Siddeek,Mohamed Seif,Andrea J. Goldsmith,Mengdi Wang*

Main category: cs.LG

TL;DR: This paper introduces a combinatorial pattern-based watermarking framework to detect and localize post-generation edits in LLM outputs, addressing the challenge of modified AI-generated content.


<details>
  <summary>Details</summary>
Motivation: Watermarking is crucial for distinguishing AI-generated text, but real-world scenarios often involve post-generation edits (human revisions or spoofing attacks), making it essential to detect and localize such modifications.

Method: Proposes a combinatorial pattern-based watermarking framework that partitions vocabulary into disjoint subsets and embeds watermarks by enforcing deterministic combinatorial patterns during generation, with global statistics for detection and lightweight local statistics for edit localization.

Result: The method was evaluated on open-source LLMs across various editing scenarios using Type-I error rate and detection accuracy metrics, demonstrating strong empirical performance in edit localization.

Conclusion: The combinatorial watermarking approach effectively addresses the challenge of detecting and localizing post-generation edits in watermarked LLM outputs, showing promising results across different editing scenarios.

Abstract: Watermarking has become a key technique for proprietary language models,
enabling the distinction between AI-generated and human-written text. However,
in many real-world scenarios, LLM-generated content may undergo post-generation
edits, such as human revisions or even spoofing attacks, making it critical to
detect and localize such modifications. In this work, we introduce a new task:
detecting post-generation edits locally made to watermarked LLM outputs. To
this end, we propose a combinatorial pattern-based watermarking framework,
which partitions the vocabulary into disjoint subsets and embeds the watermark
by enforcing a deterministic combinatorial pattern over these subsets during
generation. We accompany the combinatorial watermark with a global statistic
that can be used to detect the watermark. Furthermore, we design lightweight
local statistics to flag and localize potential edits. We introduce two
task-specific evaluation metrics, Type-I error rate and detection accuracy, and
evaluate our method on open-source LLMs across a variety of editing scenarios,
demonstrating strong empirical performance in edit localization.

</details>


### [156] [Support Basis: Fast Attention Beyond Bounded Entries](https://arxiv.org/abs/2510.01643)
*Maryam Aliakbarpour,Vladimir Braverman,Junze Yin,Haochen Zhang*

Main category: cs.LG

TL;DR: This paper introduces support-basis decomposition, a new framework for efficient attention approximation that overcomes the limitations of previous bounded-entry assumptions, enabling sub-quadratic runtime for softmax attention in LLMs.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of softmax attention is a major bottleneck in scaling LLMs. Previous sub-quadratic approximation algorithms only work under restrictive bounded-entry assumptions, which rarely hold in practice, limiting their applicability to modern LLMs.

Method: The authors propose support-basis decomposition, which leverages the empirical observation that query and key matrix entries exhibit sub-Gaussian behavior. The method splits large and small entries, enabling exact computation on sparse components and polynomial approximation on dense components. They also extend to a multi-threshold setting that eliminates all distributional assumptions.

Result: The approach achieves sub-quadratic runtime for attention approximation and provides theoretical guarantees. It also offers the first theoretical justification for polynomial attention methods, showing softmax attention can be closely approximated by multiple polynomial attentions with sketching.

Conclusion: Support-basis decomposition provides a practical and theoretically sound framework for efficient attention approximation in LLMs, overcoming the limitations of previous bounded-entry assumptions and enabling scalable sub-quadratic attention computation.

Abstract: The quadratic complexity of softmax attention remains a central bottleneck in
scaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a
sub-quadratic attention approximation algorithm, but it works only under the
restrictive bounded-entry assumption. Since this assumption rarely holds in
practice, its applicability to modern LLMs is limited.
  In this paper, we introduce support-basis decomposition, a new framework for
efficient attention approximation beyond bounded entries. We empirically
demonstrate that the entries of the query and key matrices exhibit sub-Gaussian
behavior. Our approach uses this property to split large and small entries,
enabling exact computation on sparse components and polynomial approximation on
dense components. We establish rigorous theoretical guarantees, proving a
sub-quadratic runtime, and extend the method to a multi-threshold setting that
eliminates all distributional assumptions. Furthermore, we provide the first
theoretical justification for the empirical success of polynomial attention
[Kacham, Mirrokni, and Zhong, ICML 2024], showing that softmax attention can be
closely approximated by a combination of multiple polynomial attentions with
sketching.

</details>


### [157] [Source-Free Cross-Domain Continual Learning](https://arxiv.org/abs/2510.01649)
*Muhammad Tanzil Furqon,Mahardhika Pratama,Igor Å krjanc,Lin Liu,Habibullah Habibullah,Kutluyil Dogancay*

Main category: cs.LG

TL;DR: REFEREE is a source-free cross-domain continual learning method that uses frequency-aware prompting and uncertainty-aware weighting to handle domain shifts and noisy pseudo labels without accessing source domain samples, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing cross-domain continual learning methods require fully labeled source domains, which is impractical in privacy-constrained environments. This paper aims to solve source-free cross-domain continual learning where source-domain samples are completely prohibited.

Method: Proposes REFEREE with frequency-aware prompting to handle domain shifts by encouraging low-frequency components, uncertainty-aware weighting to mitigate noisy pseudo labels, and kernel linear discriminant analysis (KLDA) to prevent catastrophic forgetting while freezing the backbone network.

Result: The approach outperforms prior methods that have access to source domain samples by significant margins in numerical studies.

Conclusion: REFEREE successfully addresses source-free cross-domain continual learning by combining source-pre-trained and vision-language models with frequency-aware techniques, demonstrating superior performance without requiring source domain access.

Abstract: Although existing cross-domain continual learning approaches successfully
address many streaming tasks having domain shifts, they call for a fully
labeled source domain hindering their feasibility in the privacy constrained
environments. This paper goes one step ahead with the problem of source-free
cross-domain continual learning where the use of source-domain samples are
completely prohibited. We propose the idea of rehearsal-free frequency-aware
dynamic prompt collaborations (REFEREE) to cope with the absence of labeled
source-domain samples in realm of cross-domain continual learning. REFEREE is
built upon a synergy between a source-pre-trained model and a large-scale
vision-language model, thus overcoming the problem of sub-optimal
generalizations when relying only on a source pre-trained model. The domain
shift problem between the source domain and the target domain is handled by a
frequency-aware prompting technique encouraging low-frequency components while
suppressing high-frequency components. This strategy generates frequency-aware
augmented samples, robust against noisy pseudo labels. The noisy pseudo-label
problem is further addressed with the uncertainty-aware weighting strategy
where the mean and covariance matrix are weighted by prediction uncertainties,
thus mitigating the adverse effects of the noisy pseudo label. Besides, the
issue of catastrophic forgetting (CF) is overcome by kernel linear discriminant
analysis (KLDA) where the backbone network is frozen while the classification
is performed using the linear discriminant analysis approach guided by the
random kernel method. Our rigorous numerical studies confirm the advantage of
our approach where it beats prior arts having access to source domain samples
with significant margins.

</details>


### [158] [The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM](https://arxiv.org/abs/2510.01650)
*Kwanhee Lee,Hyeondo Jang,Dongyeop Lee,Dan Alistarh,Namhoon Lee*

Main category: cs.LG

TL;DR: Elsa is a novel neural network pruning method that achieves extreme sparsity (up to 90%) in large language models while maintaining high accuracy, overcoming limitations of conventional methods that plateau at 50-60% sparsity.


<details>
  <summary>Details</summary>
Motivation: Current neural network pruning methods for LLMs face limitations in achieving high sparsity levels without significant accuracy degradation, creating a research bottleneck.

Method: Elsa uses constrained optimization techniques based on ADMM to directly address limitations in surrogate objective formulations used by conventional pruning methods.

Result: Elsa achieves 7.8Ã less perplexity than existing methods on LLaMA-2-7B at 90% sparsity, and scales to extremely large models (27B) with theoretical convergence guarantees.

Conclusion: The method represents meaningful progress in LLM sparsity and suggests significant opportunities remain in under-explored research directions.

Abstract: Neural network pruning is a promising technique to mitigate the excessive
computational and memory requirements of large language models (LLMs). Despite
its promise, however, progress in this area has diminished, as conventional
methods are seemingly unable to surpass moderate sparsity levels (50-60%)
without severely degrading model accuracy. This work breaks through the current
impasse, presenting a principled and effective method called $\texttt{Elsa}$,
which achieves extreme sparsity levels of up to 90% while retaining high model
fidelity. This is done by identifying several limitations in current practice,
all of which can be traced back to their reliance on a surrogate objective
formulation. $\texttt{Elsa}$ tackles this issue directly and effectively via
standard and well-established constrained optimization techniques based on
ADMM. Our extensive experiments across a wide range of models and scales show
that $\texttt{Elsa}$ achieves substantial improvements over existing methods;
e.g., it achieves 7.8$\times$ less perplexity than the best existing method on
LLaMA-2-7B at 90% sparsity. Furthermore, we present
$\texttt{Elsa}_{\text{-L}}$, a quantized variant that scales to extremely large
models (27B), and establish its theoretical convergence guarantees. These
results highlight meaningful progress in advancing the frontier of LLM
sparsity, while promising that significant opportunities for further
advancement may remain in directions that have so far attracted limited
exploration.

</details>


### [159] [Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning](https://arxiv.org/abs/2510.01656)
*Jiashun Liu,Johan Obando-Ceron,Han Lu,Yancheng He,Weixun Wang,Wenbo Su,Bo Zheng,Pablo Samuel Castro,Aaron Courville,Ling Pan*

Main category: cs.LG

TL;DR: AsyPPO introduces lightweight mini-critics trained on disjoint prompt shards to restore critics in RL for LLMs, improving value estimation and policy updates through uncertainty-based filtering.


<details>
  <summary>Details</summary>
Motivation: Conventional value functions are computationally expensive at LLM scale and fail under sparse rewards and long reasoning horizons, leading recent RL4LLM methods to avoid explicit critics.

Method: Asymmetric PPO with lightweight mini-critics trained on disjoint prompt shards, using inter-critic uncertainty to mask advantages in agreed states and filter high-divergence states from entropy regularization.

Result: Consistent improvement in learning stability and performance across benchmarks, achieving >6% gain on Qwen3-4b-Base and ~3% gains on Qwen3-8b/14b-Base over classic PPO with only 5,000 training samples.

Conclusion: Architectural innovations like AsyPPO enable scalable, efficient algorithms by restoring critics' role while maintaining efficiency in large-model settings.

Abstract: Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing
them with average advantage baselines. This shift is largely pragmatic:
conventional value functions are computationally expensive to train at LLM
scale and often fail under sparse rewards and long reasoning horizons. We
revisit this bottleneck from an architectural perspective and introduce
Asymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable
framework that restores the critics role while remaining efficient in
large-model settings. AsyPPO employs a set of lightweight mini-critics, each
trained on disjoint prompt shards. This design encourages diversity while
preserving calibration, reducing value-estimation bias. Beyond robust
estimation, AsyPPO leverages inter-critic uncertainty to refine the policy
update: (i) masking advantages in states where critics agree and gradients add
little learning signal, and (ii) filtering high-divergence states from entropy
regularization, suppressing spurious exploration. After training on open-source
data with only 5,000 samples, AsyPPO consistently improves learning stability
and performance across multiple benchmarks over strong baselines, such as GRPO,
achieving performance gains of more than six percent on Qwen3-4b-Base and about
three percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without
additional tricks. These results highlight the importance of architectural
innovations for scalable, efficient algorithms.

</details>


### [160] [Learning Time-Series Representations by Hierarchical Uniformity-Tolerance Latent Balancing](https://arxiv.org/abs/2510.01658)
*Amin Jalali,Milad Soltany,Michael Greenspan,Ali Etemad*

Main category: cs.LG

TL;DR: TimeHUT is a novel time-series representation learning method that balances uniformity and tolerance through hierarchical contrastive learning with temperature scheduling and angular margin losses.


<details>
  <summary>Details</summary>
Motivation: To learn effective time-series representations by striking a balance between uniformity (spreading embeddings evenly) and tolerance (allowing similar patterns to be close) in the embedding space.

Method: Uses hierarchical contrastive learning with instance-wise and temporal information, integrates temperature scheduler in contrastive loss, and applies hierarchical angular margin loss to create geometric margins between positive and negative pairs.

Result: Outperforms prior methods on 128 UCR and 30 UAE datasets for classification, achieves competitive results on Yahoo and KPI datasets for anomaly detection.

Conclusion: TimeHUT effectively balances uniformity and tolerance in time-series representations, demonstrating superior performance in classification tasks and competitive results in anomaly detection.

Abstract: We propose TimeHUT, a novel method for learning time-series representations
by hierarchical uniformity-tolerance balancing of contrastive representations.
Our method uses two distinct losses to learn strong representations with the
aim of striking an effective balance between uniformity and tolerance in the
embedding space. First, TimeHUT uses a hierarchical setup to learn both
instance-wise and temporal information from input time-series. Next, we
integrate a temperature scheduler within the vanilla contrastive loss to
balance the uniformity and tolerance characteristics of the embeddings.
Additionally, a hierarchical angular margin loss enforces instance-wise and
temporal contrast losses, creating geometric margins between positive and
negative pairs of temporal sequences. This approach improves the coherence of
positive pairs and their separation from the negatives, enhancing the capture
of temporal dependencies within a time-series sample. We evaluate our approach
on a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and
multivariate classification, as well as Yahoo and KPI datasets for anomaly
detection. The results demonstrate that TimeHUT outperforms prior methods by
considerable margins on classification, while obtaining competitive results for
anomaly detection. Finally, detailed sensitivity and ablation studies are
performed to evaluate different components and hyperparameters of our method.

</details>


### [161] [Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value](https://arxiv.org/abs/2510.01663)
*Wangxuan Fan,Ching Wang,Siqi Li,Nan Liu*

Main category: cs.LG

TL;DR: ShapKAN is a pruning framework that uses Shapley values to assess node importance in Kolmogorov-Arnold Networks (KANs) in a shift-invariant manner, enabling effective network compression while preserving interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional neural networks are black-box models that obscure feature-outcome relationships. KANs address this with learnable spline-based activations but face pruning challenges due to sensitivity to input coordinate shifts, making conventional magnitude-based methods unreliable.

Method: Proposed ShapKAN framework uses Shapley value attribution to quantify each node's actual contribution, ensuring consistent importance rankings regardless of input parameterization. This provides shift-invariant pruning for KANs.

Result: Extensive experiments on synthetic and real-world datasets show ShapKAN preserves true node importance while enabling effective network compression, maintaining competitive performance.

Conclusion: ShapKAN improves KAN's interpretability advantages and facilitates deployment in resource-constrained environments by providing reliable, shift-invariant pruning that maintains functional relationship understanding.

Abstract: For many real-world applications, understanding feature-outcome relationships
is as crucial as achieving high predictive accuracy. While traditional neural
networks excel at prediction, their black-box nature obscures underlying
functional relationships. Kolmogorov--Arnold Networks (KANs) address this by
employing learnable spline-based activation functions on edges, enabling
recovery of symbolic representations while maintaining competitive performance.
However, KAN's architecture presents unique challenges for network pruning.
Conventional magnitude-based methods become unreliable due to sensitivity to
input coordinate shifts. We propose \textbf{ShapKAN}, a pruning framework using
Shapley value attribution to assess node importance in a shift-invariant
manner. Unlike magnitude-based approaches, ShapKAN quantifies each node's
actual contribution, ensuring consistent importance rankings regardless of
input parameterization. Extensive experiments on synthetic and real-world
datasets demonstrate that ShapKAN preserves true node importance while enabling
effective network compression. Our approach improves KAN's interpretability
advantages, facilitating deployment in resource-constrained environments.

</details>


### [162] [Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.01677)
*Han Wu,Yanming Sun,Yunhe Yang,Derek F. Wong*

Main category: cs.LG

TL;DR: AGFN is a multimodal sentiment analysis model that uses adaptive gated fusion with entropy-based weighting to handle noisy/conflicting modalities and improve emotion recognition.


<details>
  <summary>Details</summary>
Motivation: Standard multimodal fusion fails to account for modality quality variations (noise, missing data, conflicts), leading to poor performance on subtle emotional nuances.

Method: Dual gate fusion mechanism using information entropy and modality importance to adaptively weight features, reducing noisy modality influence while prioritizing informative cues after unimodal encoding and cross-modal interaction.

Result: Significantly outperforms baselines on CMU-MOSI and CMU-MOSEI datasets in accuracy, effectively discerning subtle emotions with robust performance.

Conclusion: AGFN enhances generalization by learning broader feature distributions, reducing correlation between feature location and prediction error, creating more robust multimodal representations.

Abstract: Multimodal sentiment analysis (MSA) leverages information fusion from diverse
modalities (e.g., text, audio, visual) to enhance sentiment prediction.
However, simple fusion techniques often fail to account for variations in
modality quality, such as those that are noisy, missing, or semantically
conflicting. This oversight leads to suboptimal performance, especially in
discerning subtle emotional nuances. To mitigate this limitation, we introduce
a simple yet efficient \textbf{A}daptive \textbf{G}ated \textbf{F}usion
\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion
mechanism based on information entropy and modality importance. This mechanism
mitigates the influence of noisy modalities and prioritizes informative cues
following unimodal encoding and cross-modal interaction. Experiments on
CMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong
baselines in accuracy, effectively discerning subtle emotions with robust
performance. Visualization analysis of feature representations demonstrates
that AGFN enhances generalization by learning from a broader feature
distribution, achieved by reducing the correlation between feature location and
prediction error, thereby decreasing reliance on specific locations and
creating more robust multimodal feature representations.

</details>


### [163] [PASTA: A Unified Framework for Offline Assortment Learning](https://arxiv.org/abs/2510.01693)
*Juncheng Dong,Weibin Mo,Zhengling Qi,Cong Shi,Ethan X. Fang,Vahid Tarokh*

Main category: cs.LG

TL;DR: PASTA framework for offline assortment optimization using pessimism principle to handle insufficient data coverage, achieving optimal revenue under general choice models with finite-sample regret bounds.


<details>
  <summary>Details</summary>
Motivation: Firms lack prior knowledge of choice models and face insufficient data coverage in assortment optimization, making it challenging to design effective solutions from historical customer choice data.

Method: Proposed Pessimistic Assortment Optimization (PASTA) framework that leverages pessimism principle, requiring only that offline data distribution contains an optimal assortment rather than full coverage of all assortments.

Result: Established first finite-sample regret bounds for offline assortment optimization across multinomial logit and nested logit models, proved minimax optimality in sample and model complexity, and demonstrated superior performance over baselines in experiments.

Conclusion: PASTA provides an effective data-driven solution for assortment optimization with provable guarantees under insufficient data coverage, achieving minimax optimal performance across various choice models.

Abstract: We study a broad class of assortment optimization problems in an offline and
data-driven setting. In such problems, a firm lacks prior knowledge of the
underlying choice model, and aims to determine an optimal assortment based on
historical customer choice data. The combinatorial nature of assortment
optimization often results in insufficient data coverage, posing a significant
challenge in designing provably effective solutions. To address this, we
introduce a novel Pessimistic Assortment Optimization (PASTA) framework that
leverages the principle of pessimism to achieve optimal expected revenue under
general choice models. Notably, PASTA requires only that the offline data
distribution contains an optimal assortment, rather than providing the full
coverage of all feasible assortments. Theoretically, we establish the first
finite-sample regret bounds for offline assortment optimization across several
widely used choice models, including the multinomial logit and nested logit
models. Additionally, we derive a minimax regret lower bound, proving that
PASTA is minimax optimal in terms of sample and model complexity. Numerical
experiments further demonstrate that our method outperforms existing baseline
approaches.

</details>


### [164] [Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport](https://arxiv.org/abs/2510.01706)
*Shaan Shah,Meenakshi Khosla*

Main category: cs.LG

TL;DR: HOT is a unified framework using hierarchical optimal transport to compare neural network representations, handling depth mismatches and providing global alignment scores through soft layer-to-layer couplings.


<details>
  <summary>Details</summary>
Motivation: Standard representational similarity methods have limitations: asymmetric results, no global alignment score, and difficulty handling networks of different depths due to ignoring global activation structure and rigid one-to-one layer mappings.

Method: Hierarchical Optimal Transport (HOT) jointly infers soft, globally consistent layer-to-layer couplings and neuron-level transport plans, allowing source neurons to distribute mass across multiple target layers while minimizing total transport cost under marginal constraints.

Result: HOT matches or surpasses standard pairwise matching in alignment quality across vision models, large language models, and human visual cortex recordings. It reveals smooth, fine-grained hierarchical correspondences that emerge naturally from global optimization.

Conclusion: HOT enables richer, more interpretable comparisons between representations, particularly when networks differ in architecture or depth, by providing a unified framework that handles depth mismatches through mass distribution and reveals structured patterns absent in greedy layer-wise methods.

Abstract: Standard representational similarity methods align each layer of a network to
its best match in another independently, producing asymmetric results, lacking
a global alignment score, and struggling with networks of different depths.
These limitations arise from ignoring global activation structure and
restricting mappings to rigid one-to-one layer correspondences. We propose
Hierarchical Optimal Transport (HOT), a unified framework that jointly infers
soft, globally consistent layer-to-layer couplings and neuron-level transport
plans. HOT allows source neurons to distribute mass across multiple target
layers while minimizing total transport cost under marginal constraints. This
yields both a single alignment score for the entire network comparison and a
soft transport plan that naturally handles depth mismatches through mass
distribution. We evaluate HOT on vision models, large language models, and
human visual cortex recordings. Across all domains, HOT matches or surpasses
standard pairwise matching in alignment quality. Moreover, it reveals smooth,
fine-grained hierarchical correspondences: early layers map to early layers,
deeper layers maintain relative positions, and depth mismatches are resolved by
distributing representations across multiple layers. These structured patterns
emerge naturally from global optimization without being imposed, yet are absent
in greedy layer-wise methods. HOT thus enables richer, more interpretable
comparisons between representations, particularly when networks differ in
architecture or depth.

</details>


### [165] [ActiNet: Activity intensity classification of wrist-worn accelerometers using self-supervised deep learning](https://arxiv.org/abs/2510.01712)
*Aidan Acquah,Shing Chan,Aiden Doherty*

Main category: cs.LG

TL;DR: The paper proposes ActiNet, a self-supervised ResNet-V2 model with HMM smoothing, which outperforms baseline RF+HMM in classifying activity intensity from wrist-accelerometer data, achieving mean F1 score of 0.82 and Cohen's kappa of 0.86.


<details>
  <summary>Details</summary>
Motivation: To improve human activity recognition (HAR) models for wrist-accelerometer data in epidemiological studies investigating physical activity and health outcomes, by exploring the potential of self-supervised learning combined with HMMs.

Method: Used 151 CAPTURE-24 participants' data to train ActiNet (18-layer modified ResNet-V2 with self-supervised learning) followed by HMM smoothing. Compared performance against baseline RF+HMM using 5-fold stratified group cross-validation, and analyzed subgroups by age and sex.

Result: ActiNet achieved mean macro F1 score of 0.82 and Cohen's kappa of 0.86, outperforming RF+HMM (0.77 F1, 0.81 kappa). Performance improvements were consistent across age and sex subgroups.

Conclusion: ActiNet is recommended for extracting activity intensity labels from wrist-accelerometer data in future epidemiological studies due to its superior performance over existing methods.

Abstract: The use of reliable and accurate human activity recognition (HAR) models on
passively collected wrist-accelerometer data is essential in large-scale
epidemiological studies that investigate the association between physical
activity and health outcomes. While the use of self-supervised learning has
generated considerable excitement in improving HAR, it remains unknown the
extent to which these models, coupled with hidden Markov models (HMMs), would
make a tangible improvement to classification performance, and the effect this
may have on the predicted daily activity intensity compositions. Using 151
CAPTURE-24 participants' data, we trained the ActiNet model, a self-supervised,
18-layer, modified ResNet-V2 model, followed by hidden Markov model (HMM)
smoothing to classify labels of activity intensity. The performance of this
model, evaluated using 5-fold stratified group cross-validation, was then
compared to a baseline random forest (RF) + HMM, established in existing
literature. Differences in performance and classification outputs were compared
with different subgroups of age and sex within the Capture-24 population. The
ActiNet model was able to distinguish labels of activity intensity with a mean
macro F1 score of 0.82, and mean Cohen's kappa score of 0.86. This exceeded the
performance of the RF + HMM, trained and validated on the same dataset, with
mean scores of 0.77 and 0.81, respectively. These findings were consistent
across subgroups of age and sex. These findings encourage the use of ActiNet
for the extraction of activity intensity labels from wrist-accelerometer data
in future epidemiological studies.

</details>


### [166] [Latency-aware Multimodal Federated Learning over UAV Networks](https://arxiv.org/abs/2510.01717)
*Shaba Shaon,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: This paper proposes a UAV-assisted federated multimodal learning framework that optimizes system latency through joint optimization of sensing scheduling, power control, trajectory planning, and resource allocation, with theoretical convergence analysis.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of unimodal systems and enhance model accuracy and generalization in federated learning by leveraging UAVs for multimodal data collection and processing, while minimizing system latency in UAV networks.

Method: Proposes an efficient iterative optimization algorithm combining block coordinate descent and successive convex approximation techniques to solve the complex latency minimization problem, addressing UAV sensing scheduling, power control, trajectory planning, resource allocation, and BS resource management.

Result: Numerical experiments show the proposed FML framework outperforms existing approaches in system latency and model training performance across different data settings.

Conclusion: The UAV-assisted federated multimodal learning framework effectively minimizes system latency while providing theoretical convergence guarantees and improved model performance compared to existing methods.

Abstract: This paper investigates federated multimodal learning (FML) assisted by
unmanned aerial vehicles (UAVs) with a focus on minimizing system latency and
providing convergence analysis. In this framework, UAVs are distributed
throughout the network to collect data, participate in model training, and
collaborate with a base station (BS) to build a global model. By utilizing
multimodal sensing, the UAVs overcome the limitations of unimodal systems,
enhancing model accuracy, generalization, and offering a more comprehensive
understanding of the environment. The primary objective is to optimize FML
system latency in UAV networks by jointly addressing UAV sensing scheduling,
power control, trajectory planning, resource allocation, and BS resource
management. To address the computational complexity of our latency minimization
problem, we propose an efficient iterative optimization algorithm combining
block coordinate descent and successive convex approximation techniques, which
provides high-quality approximate solutions. We also present a theoretical
convergence analysis for the UAV-assisted FML framework under a non-convex loss
function. Numerical experiments demonstrate that our FML framework outperforms
existing approaches in terms of system latency and model training performance
under different data settings.

</details>


### [167] [Accelerating Attention with Basis Decomposition](https://arxiv.org/abs/2510.01718)
*Jialin Zhao*

Main category: cs.LG

TL;DR: BD Attention (BDA) is a lossless algorithmic reformulation of attention using Basis Decomposition, providing mathematically guaranteed acceleration without changing model outputs.


<details>
  <summary>Details</summary>
Motivation: Attention is computationally expensive in LLMs and VLMs, and existing optimizations like FlashAttention are system-level rather than algorithmic. BDA aims to provide a mathematically exact acceleration method.

Method: Uses Basis Decomposition matrix identity to restructure multi-head projections into a compact form, reducing computation while preserving exact outputs. Requires only 4s offline preparation with no retraining.

Result: Achieves 32% faster key/value projections, 25% smaller weights, with negligible impact on perplexity (0.02% increase in FP16, 0.0004% in FP32).

Conclusion: BDA is the first theoretically exact method for lossless attention acceleration, complementary to existing engineering optimizations.

Abstract: Attention is a core operation in large language models (LLMs) and
vision-language models (VLMs). We present BD Attention (BDA), the first
lossless algorithmic reformulation of attention. BDA is enabled by a simple
matrix identity from Basis Decomposition (BD), which restructures multi-head
projections into a compact form while preserving exact outputs. Unlike
I/O-aware system optimizations such as FlashAttention, BDA provides a
mathematically guaranteed acceleration that is architecture-agnostic. On
DeepSeek-V2-Lite (16B, FP16), BDA requires only 4s of offline preparation with
no retraining required and, on modern GPUs, achieves 32% faster key/value
projections and 25% smaller weights, while increasing end-to-end perplexity
(PPL) by just 0.02% (FP16) or 0.0004% (FP32), a negligible effect on model
performance. These results position BDA as the first theoretically exact method
for lossless attention acceleration that is complementary to existing
engineering-level optimizations. Our code is available at
https://github.com/abcbdf/basis-decomposition-official.

</details>


### [168] [Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation](https://arxiv.org/abs/2510.01721)
*Saptarshi Mandal,Yashaswini Murthy,R. Srikant*

Main category: cs.LG

TL;DR: This paper presents the first robust TD learning algorithm with linear function approximation for distributionally robust RL, achieving O(1/ÎµÂ²) sample complexity without requiring generative access to the MDP.


<details>
  <summary>Details</summary>
Motivation: Existing robust TD learning guarantees are limited to tabular MDPs or require restrictive discount-factor assumptions with function approximation, creating a gap between empirical success and theoretical guarantees.

Method: Combines two-time-scale stochastic approximation with outer-loop target-network updates, using total-variation and Wasserstein distance uncertainty sets, and is model-free.

Result: Achieves Ã(1/ÎµÂ²) sample complexity for Îµ-accurate value estimation, bridging the theoretical gap between robust and non-robust RL algorithms.

Conclusion: The proposed robust TD learning with linear function approximation provides the first non-asymptotic convergence guarantees and extends naturally to robust Q-learning with function approximation.

Abstract: Distributionally robust reinforcement learning (DRRL) focuses on designing
policies that achieve good performance under model uncertainties. In
particular, we are interested in maximizing the worst-case long-term discounted
reward, where the data for RL comes from a nominal model while the deployed
environment can deviate from the nominal model within a prescribed uncertainty
set. Existing convergence guarantees for robust temporal-difference (TD)
learning for policy evaluation are limited to tabular MDPs or are dependent on
restrictive discount-factor assumptions when function approximation is used. We
present the first robust TD learning with linear function approximation, where
robustness is measured with respect to the total-variation distance and
Wasserstein-l distance uncertainty set. Additionally, our algorithm is both
model-free and does not require generative access to the MDP. Our algorithm
combines a two-time-scale stochastic-approximation update with an outer-loop
target-network update. We establish an $\tilde{O}(1/\epsilon^2)$ sample
complexity to obtain an $\epsilon$-accurate value estimate. Our results close a
key gap between the empirical success of robust RL algorithms and the
non-asymptotic guarantees enjoyed by their non-robust counterparts. The key
ideas in the paper also extend in a relatively straightforward fashion to
robust Q-learning with function approximation.

</details>


### [169] [Workplace Location Choice Model based on Deep Neural Network](https://arxiv.org/abs/2510.01723)
*Tanay Rastogi,Anders KarlstrÃ¶m*

Main category: cs.LG

TL;DR: This paper compares deep neural networks (DNNs) with traditional discrete choice models (DCMs) for workplace location choice modeling, finding DNNs offer better performance in some aspects while DCMs excel in others, highlighting the need for model selection based on specific requirements.


<details>
  <summary>Details</summary>
Motivation: Traditional discrete choice models face challenges in accurately representing individual decision-making processes for workplace location choices, motivating the exploration of deep neural networks as an alternative approach.

Method: The study uses deep neural networks (DNNs) to model workplace location choices and compares their performance with traditional discrete choice models (DCMs).

Result: DNNs outperform DCMs in certain aspects and show significant potential as an alternative. Both models effectively capture job opportunity impacts, but DCMs better align with data for individual attribute influences on workplace distance. DCMs excel at shorter distances, while DNNs perform comparably for longer distances.

Conclusion: The findings emphasize the importance of selecting appropriate models based on specific application requirements in workplace location choice analysis, as both DNNs and DCMs have distinct strengths in different scenarios.

Abstract: Discrete choice models (DCMs) have long been used to analyze workplace
location decisions, but they face challenges in accurately mirroring individual
decision-making processes. This paper presents a deep neural network (DNN)
method for modeling workplace location choices, which aims to better understand
complex decision patterns and provides better results than traditional discrete
choice models (DCMs). The study demonstrates that DNNs show significant
potential as a robust alternative to DCMs in this domain. While both models
effectively replicate the impact of job opportunities on workplace location
choices, the DNN outperforms the DCM in certain aspects. However, the DCM
better aligns with data when assessing the influence of individual attributes
on workplace distance. Notably, DCMs excel at shorter distances, while DNNs
perform comparably to both data and DCMs for longer distances. These findings
underscore the importance of selecting the appropriate model based on specific
application requirements in workplace location choice analysis.

</details>


### [170] [Private and Fair Machine Learning: Revisiting the Disparate Impact of Differentially Private SGD](https://arxiv.org/abs/2510.01744)
*Lea Demelius,Dominik Kowald,Simone Kopeinik,Roman Kern,Andreas TrÃ¼gler*

Main category: cs.LG

TL;DR: DPSGD's impact on fairness varies across metrics; hyperparameter tuning on private models improves utility-fairness trade-offs but increases privacy leakage.


<details>
  <summary>Details</summary>
Motivation: To analyze whether optimizing hyperparameters directly on differentially private models can mitigate DPSGD's disparate impact on fairness, and to examine this across different performance metrics and hyperparameter settings.

Method: 1) Compare disparate impact of DPSGD on different performance metrics, 2) Analyze impact over wide range of hyperparameter settings, 3) Extend analysis to DPSGD-Global-Adapt variant.

Result: Disparate impact on one metric doesn't imply impact on others; hyperparameter tuning on private models doesn't reliably mitigate disparate impact but improves utility-fairness trade-offs compared to reusing non-private hyperparameters.

Conclusion: Hyperparameter tuning on private models offers better utility-fairness balance but increases privacy leakage; DPSGD-Global-Adapt is not robust to hyperparameter choices.

Abstract: Differential privacy (DP) is a prominent method for protecting information
about individuals during data analysis. Training neural networks with
differentially private stochastic gradient descent (DPSGD) influences the
model's learning dynamics and, consequently, its output. This can affect the
model's performance and fairness. While the majority of studies on the topic
report a negative impact on fairness, it has recently been suggested that
fairness levels comparable to non-private models can be achieved by optimizing
hyperparameters for performance directly on differentially private models
(rather than re-using hyperparameters from non-private models, as is common
practice). In this work, we analyze the generalizability of this claim by 1)
comparing the disparate impact of DPSGD on different performance metrics, and
2) analyzing it over a wide range of hyperparameter settings. We highlight that
a disparate impact on one metric does not necessarily imply a disparate impact
on another. Most importantly, we show that while optimizing hyperparameters
directly on differentially private models does not mitigate the disparate
impact of DPSGD reliably, it can still lead to improved utility-fairness
trade-offs compared to re-using hyperparameters from non-private models. We
stress, however, that any form of hyperparameter tuning entails additional
privacy leakage, calling for careful considerations of how to balance privacy,
utility and fairness. Finally, we extend our analyses to DPSGD-Global-Adapt, a
variant of DPSGD designed to mitigate the disparate impact on accuracy, and
conclude that this alternative may not be a robust solution with respect to
hyperparameter choice.

</details>


### [171] [Learning Regularization Functionals for Inverse Problems: A Comparative Study](https://arxiv.org/abs/2510.01755)
*Johannes Hertrich,Hok Shing Wong,Alexander Denker,Stanislas Ducotterd,Zhenghan Fang,Markus Haltmeier,Å½eljko Kereta,Erich Kobler,Oscar Leong,Mohammad Sadegh Salehi,Carola-Bibiane SchÃ¶nlieb,Johannes Schwab,Zakhar Shumaylov,Jeremias Sulam,German ShÃ¢ma Wache,Martin Zach,Yasi Zhang,Matthias J. Ehrhardt,Sebastian Neumayer*

Main category: cs.LG

TL;DR: This paper presents a unified framework for comparing learned regularization methods in inverse imaging problems, addressing implementation inconsistencies through code collection and systematic analysis.


<details>
  <summary>Details</summary>
Motivation: The proliferation of diverse learned regularization methods for inverse problems in imaging has created challenges in direct comparison due to non-modular implementations and varying architectural designs.

Method: The authors collect and unify available code into a common framework, enabling systematic comparison of different approaches and providing practical guidelines for each method.

Result: The unified framework allows comprehensive comparison of learned regularization methods, revealing their respective strengths and limitations while offering valuable insights into future development potential.

Conclusion: This work provides a standardized platform for evaluating learned regularization techniques in imaging inverse problems, facilitating better understanding and comparison of existing methods while guiding future research directions.

Abstract: In recent years, a variety of learned regularization frameworks for solving
inverse problems in imaging have emerged. These offer flexible modeling
together with mathematical insights. The proposed methods differ in their
architectural design and training strategies, making direct comparison
challenging due to non-modular implementations. We address this gap by
collecting and unifying the available code into a common framework. This
unified view allows us to systematically compare the approaches and highlight
their strengths and limitations, providing valuable insights into their future
potential. We also provide concise descriptions of each method, complemented by
practical guidelines.

</details>


### [172] [Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks](https://arxiv.org/abs/2510.01758)
*Bruno Corcuera,Carlos Eiras-Franco,Brais Cancela*

Main category: cs.LG

TL;DR: This paper presents an unsupervised Dynamic Feature Selection (DFS) method to enhance latent representations by removing noisy or irrelevant features in images, improving model performance and generalization without labeled data.


<details>
  <summary>Details</summary>
Motivation: Latent representations in vision tasks are often degraded by noisy or irrelevant features, which negatively impact model performance and generalization capabilities.

Method: The proposed unsupervised Dynamic Feature Selection (DFS) method identifies and removes misleading or redundant information in images for each instance, ensuring only relevant features contribute to the latent space.

Result: Experiments on image datasets show models with unsupervised DFS achieve significant improvements in generalization performance for tasks like clustering and image generation, with minimal computational cost increase.

Conclusion: Unsupervised DFS effectively enhances latent representations by filtering out irrelevant features, leading to better model performance and generalization across various vision tasks without requiring labeled data.

Abstract: Latent representations are critical for the performance and robustness of
machine learning models, as they encode the essential features of data in a
compact and informative manner. However, in vision tasks, these representations
are often affected by noisy or irrelevant features, which can degrade the
model's performance and generalization capabilities. This paper presents a
novel approach for enhancing latent representations using unsupervised Dynamic
Feature Selection (DFS). For each instance, the proposed method identifies and
removes misleading or redundant information in images, ensuring that only the
most relevant features contribute to the latent space. By leveraging an
unsupervised framework, our approach avoids reliance on labeled data, making it
broadly applicable across various domains and datasets. Experiments conducted
on image datasets demonstrate that models equipped with unsupervised DFS
achieve significant improvements in generalization performance across various
tasks, including clustering and image generation, while incurring a minimal
increase in the computational cost.

</details>


### [173] [Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX](https://arxiv.org/abs/2510.01764)
*Waris Radji,Thomas Michel,Hector Piteau*

Main category: cs.LG

TL;DR: Octax is a JAX-based GPU-accelerated suite of classic arcade game environments for reinforcement learning research, offering massive speedups over CPU emulators while maintaining perfect game fidelity.


<details>
  <summary>Details</summary>
Motivation: Modern video games are computationally expensive and CPU-bound, poorly suited for large-scale RL experimentation. There's a need for diverse, challenging environments that are both tractable and scalable on modern hardware.

Method: Implemented classic arcade game environments in JAX based on CHIP-8 emulation (predecessor to Atari), providing GPU-accelerated execution with modular design for easy extension.

Result: Achieved orders-of-magnitude speedups over traditional CPU emulators while maintaining perfect fidelity to original game mechanics. Demonstrated significant improvements in RL training speed and scalability across multiple games.

Conclusion: Octax provides the RL community with a long-awaited GPU alternative to Atari benchmark, enabling large-scale experimentation with easy extensibility for new games and novel environment generation.

Abstract: Reinforcement learning (RL) research requires diverse, challenging
environments that are both tractable and scalable. While modern video games may
offer rich dynamics, they are computationally expensive and poorly suited for
large-scale experimentation due to their CPU-bound execution. We introduce
Octax, a high-performance suite of classic arcade game environments implemented
in JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely
adopted as a benchmark in RL research. Octax provides the JAX community with a
long-awaited end-to-end GPU alternative to the Atari benchmark, offering
image-based environments, spanning puzzle, action, and strategy genres, all
executable at massive scale on modern GPUs. Our JAX-based implementation
achieves orders-of-magnitude speedups over traditional CPU emulators while
maintaining perfect fidelity to the original game mechanics. We demonstrate
Octax's capabilities by training RL agents across multiple games, showing
significant improvements in training speed and scalability compared to existing
solutions. The environment's modular design enables researchers to easily
extend the suite with new games or generate novel environments using large
language models, making it an ideal platform for large-scale RL
experimentation.

</details>


### [174] [Neural non-canonical Hamiltonian dynamics for long-time simulations](https://arxiv.org/abs/2510.01788)
*ClÃ©mentine CourtÃ¨s,Emmanuel Franck,Michael Kraus,Laurent Navoret,LÃ©opold TrÃ©mant*

Main category: cs.LG

TL;DR: This paper addresses numerical instability in learning non-canonical Hamiltonian dynamics by proposing two training strategies to overcome gauge dependency issues when combining potential-based architectures with degenerate variational integrators.


<details>
  <summary>Details</summary>
Motivation: Previous methods for learning non-canonical Hamiltonian dynamics focused separately on model structure preservation (potential-based architecture) and numerical scheme preservation (degenerate variational integrators), but combining both caused numerical instability due to gauge dependency, making long-term simulations impossible.

Method: Proposed two training strategies: 1) directly learning the vector field, and 2) learning time-discrete dynamics through the numerical scheme. Both approaches aim to address the gauge dependency problem that causes instability.

Result: The methods were tested on complex physical dynamics including guiding center from gyrokinetic plasma physics, demonstrating improved ability to learn and simulate these systems.

Conclusion: The proposed training strategies successfully address the numerical instability problem in learning non-canonical Hamiltonian dynamics, enabling stable long-term simulations by overcoming gauge dependency issues when combining structure-preserving models and numerical schemes.

Abstract: This work focuses on learning non-canonical Hamiltonian dynamics from data,
where long-term predictions require the preservation of structure both in the
learned model and in numerical schemes. Previous research focused on either
facet, respectively with a potential-based architecture and with degenerate
variational integrators, but new issues arise when combining both. In
experiments, the learnt model is sometimes numerically unstable due to the
gauge dependency of the scheme, rendering long-time simulations impossible. In
this paper, we identify this problem and propose two different training
strategies to address it, either by directly learning the vector field or by
learning a time-discrete dynamics through the scheme. Several numerical test
cases assess the ability of the methods to learn complex physical dynamics,
like the guiding center from gyrokinetic plasma physics.

</details>


### [175] [Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy Filters for Synthetic Data Generation](https://arxiv.org/abs/2510.01793)
*Adil Koeken,Alexander Ziller,Moritz Knolle,Daniel Rueckert*

Main category: cs.LG

TL;DR: Post-hoc privacy filters for synthetic medical datasets show limited effectiveness, failing to reliably detect near-duplicates from training data while providing false security.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate the effectiveness of post-hoc privacy filtering techniques for synthetic medical datasets, which are claimed to remove personally identifiable information but remain largely unverified.

Method: Applied a filtering pipeline to chest X-ray synthesis and evaluated its performance in detecting near-duplicates and protecting patient privacy.

Result: Current filters exhibit limited specificity and consistency, achieving high sensitivity only for real images but failing to reliably detect near-duplicates generated from training data.

Conclusion: Substantial advances in filter design are needed before post-hoc privacy filtering methods can be confidently deployed in sensitive medical applications, as current methods provide false security while leaving patient information exposed.

Abstract: The generation of privacy-preserving synthetic datasets is a promising avenue
for overcoming data scarcity in medical AI research. Post-hoc privacy filtering
techniques, designed to remove samples containing personally identifiable
information, have recently been proposed as a solution. However, their
effectiveness remains largely unverified. This work presents a rigorous
evaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary
to claims from the original publications, our results demonstrate that current
filters exhibit limited specificity and consistency, achieving high sensitivity
only for real images while failing to reliably detect near-duplicates generated
from training data. These results demonstrate a critical limitation of post-hoc
filtering: rather than effectively safeguarding patient privacy, these methods
may provide a false sense of security while leaving unacceptable levels of
patient information exposed. We conclude that substantial advances in filter
design are needed before these methods can be confidently deployed in sensitive
applications.

</details>


### [176] [Rethinking the shape convention of an MLP](https://arxiv.org/abs/2510.01796)
*Meng-Hsi Chen,Yu-Ang Lee,Feng-Ting Liao,Da-shan Shiu*

Main category: cs.LG

TL;DR: The paper proposes Hourglass MLP blocks with wide-narrow-wide architecture where skip connections operate at expanded dimensions, challenging conventional narrow-wide-narrow MLP designs. This approach achieves superior performance-parameter Pareto frontiers in generative tasks.


<details>
  <summary>Details</summary>
Motivation: To challenge the conventional narrow-wide-narrow MLP design and leverage higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs.

Method: Propose Hourglass MLP blocks with wide-narrow-wide architecture where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. Use fixed random initialization for the initial projection throughout training.

Result: Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal configurations favor deeper networks with wider skip connections and narrower bottlenecks.

Conclusion: The findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.

Abstract: Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow
design where skip connections operate at the input/output dimensions while
processing occurs in expanded hidden spaces. We challenge this convention by
proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections
operate at expanded dimensions while residual computation flows through narrow
bottlenecks. This inversion leverages higher-dimensional spaces for incremental
refinement while maintaining computational efficiency through parameter-matched
designs. Implementing Hourglass MLPs requires an initial projection to lift
input signals to expanded dimensions. We propose that this projection can
remain fixed at random initialization throughout training, enabling efficient
training and inference implementations. We evaluate both architectures on
generative tasks over popular image datasets, characterizing
performance-parameter Pareto frontiers through systematic architectural search.
Results show that Hourglass architectures consistently achieve superior Pareto
frontiers compared to conventional designs. As parameter budgets increase,
optimal Hourglass configurations favor deeper networks with wider skip
connections and narrower bottlenecks-a scaling pattern distinct from
conventional MLPs. Our findings suggest reconsidering skip connection placement
in modern architectures, with potential applications extending to Transformers
and other residual networks.

</details>


### [177] [Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction](https://arxiv.org/abs/2510.01817)
*Adam Filipek*

Main category: cs.LG

TL;DR: Sparse Query Attention (SQA) reduces computational complexity by decreasing Query heads instead of Key/Value heads, achieving up to 3x throughput improvements in long-sequence processing with minimal quality impact.


<details>
  <summary>Details</summary>
Motivation: Address the quadratic computational complexity of Multi-Head Attention (MHA) that limits scaling for long contexts, where existing methods like MQA and GQA reduce memory bandwidth but not FLOPs.

Method: Introduces Sparse Query Attention architecture that reduces the number of Query heads, directly lowering attention computation FLOPs by a factor proportional to query head reduction.

Result: Empirical benchmarks on 32k-200k token sequences show up to 3x throughput improvements in computation-bound scenarios (pre-training, fine-tuning, encoder tasks) with minimal model quality degradation.

Conclusion: SQA provides an alternative optimization path to existing attention mechanisms, offering significant computational efficiency gains for long-sequence processing while maintaining model quality.

Abstract: The Transformer architecture, underpinned by the Multi-Head Attention (MHA)
mechanism, has become the de facto standard for state-of-the-art models in
artificial intelligence. However, the quadratic computational complexity of MHA
with respect to sequence length presents a significant barrier to scaling,
particularly for applications involving long contexts. Prevailing solutions,
such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have
effectively addressed the memory bandwidth bottleneck that dominates
autoregressive inference latency by sharing Key and Value projections. While
highly successful, these methods do not reduce the fundamental number of
floating-point operations (FLOPs) required for the attention score computation,
which remains a critical bottleneck for training and full-sequence processing.
This paper introduces Sparse Query Attention (SQA), a novel attention
architecture that pursues an alternative and complementary optimization path.
Instead of reducing Key/Value heads, SQA reduces the number of Query heads.
This architectural modification directly decreases the computational complexity
of the attention mechanism by a factor proportional to the reduction in query
heads, thereby lowering the overall FLOPs. This work presents the theoretical
foundation of SQA, its mathematical formulation, and a family of architectural
variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate
that SQA can achieve significant throughput improvements of up to 3x in
computation-bound scenarios such as model pre-training, fine-tuning, and
encoder-based tasks, with only a minimal impact on model quality in preliminary
smallscale experiments. SQA was discovered serendipitously during the
development of the upcoming Reactive Transformer architecture, suggesting its
potential as a powerful tool for building more efficient and scalable models

</details>


### [178] [Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning](https://arxiv.org/abs/2510.01824)
*Olivier Goudet,Quentin Suire,Adrien GoÃ«ffon,FrÃ©dÃ©ric Saubion,Sylvain Lamprier*

Main category: cs.LG

TL;DR: An order-invariant reinforcement learning framework for black-box combinatorial optimization that uses random generation orders during training to improve sample efficiency and avoid catastrophic failures.


<details>
  <summary>Details</summary>
Motivation: Classical estimation-of-distribution algorithms (EDAs) often rely on learning explicit variable dependency graphs, which can be costly and fail to capture complex interactions efficiently.

Method: Parameterizes a multivariate autoregressive generative model trained without fixed variable ordering, using random generation orders during training as information-preserving dropout to encourage order invariance.

Result: Across a wide range of benchmark algorithms and problem instances of varying sizes, the method frequently achieves the best performance and consistently avoids catastrophic failures.

Conclusion: The order-invariant reinforcement learning framework with random generation orders improves search-space diversity, focuses on relevant variable dependencies, and provides stable policy-gradient updates through adapted GRPO.

Abstract: We introduce an order-invariant reinforcement learning framework for
black-box combinatorial optimization. Classical estimation-of-distribution
algorithms (EDAs) often rely on learning explicit variable dependency graphs,
which can be costly and fail to capture complex interactions efficiently. In
contrast, we parameterize a multivariate autoregressive generative model
trained without a fixed variable ordering. By sampling random generation orders
during training - a form of information-preserving dropout - the model is
encouraged to be invariant to variable order, promoting search-space diversity
and shaping the model to focus on the most relevant variable dependencies,
improving sample efficiency. We adapt Generalized Reinforcement Policy
Optimization (GRPO) to this setting, providing stable policy-gradient updates
from scale-invariant advantages. Across a wide range of benchmark algorithms
and problem instances of varying sizes, our method frequently achieves the best
performance and consistently avoids catastrophic failures.

</details>


### [179] [Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets](https://arxiv.org/abs/2510.01842)
*Yannis Belkhiter,Seshu Tirupathi,Giulio Zizzo,Sachin Sharma,John D. Kelleher*

Main category: cs.LG

TL;DR: This paper proposes using pre-hoc model selection with traditional models and LLM agents to reduce AutoML search space, achieving computational efficiency while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: Current AutoML methods rely on exhaustive hyperparameter searches which are computationally expensive. Pre-hoc prediction offers a promising alternative but remains under-explored.

Method: Leverage traditional models and LLM agents using dataset descriptions and statistical information to pre-select models, reducing the AutoML search space. Applied to AWS AutoGluon portfolio dataset with 175 tabular classification datasets.

Result: The approach significantly reduces computational overhead while still selecting the best model for the given dataset.

Conclusion: Pre-hoc model selection provides an effective shift in AutoML workflows, balancing computational efficiency with model performance.

Abstract: The field of AutoML has made remarkable progress in post-hoc model selection,
with libraries capable of automatically identifying the most performing models
for a given dataset. Nevertheless, these methods often rely on exhaustive
hyperparameter searches, where methods automatically train and test different
types of models on the target dataset. Contrastingly, pre-hoc prediction
emerges as a promising alternative, capable of bypassing exhaustive search
through intelligent pre-selection of models. Despite its potential, pre-hoc
prediction remains under-explored in the literature. This paper explores the
intersection of AutoML and pre-hoc model selection by leveraging traditional
models and Large Language Model (LLM) agents to reduce the search space of
AutoML libraries. By relying on dataset descriptions and statistical
information, we reduce the AutoML search space. Our methodology is applied to
the AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark
containing 175 tabular classification datasets available on OpenML. The
proposed approach offers a shift in AutoML workflows, significantly reducing
computational overhead, while still selecting the best model for the given
dataset.

</details>


### [180] [Learning Representations Through Contrastive Neural Model Checking](https://arxiv.org/abs/2510.01853)
*Vladimir Krsmanovic,Matthias Cosler,Mohamed Ghanem,Bernd Finkbeiner*

Main category: cs.LG

TL;DR: CNML is a novel contrastive learning method that embeds logical specifications and systems into a shared latent space using model checking as a guiding signal, outperforming baselines on retrieval tasks and showing effective transfer to downstream applications.


<details>
  <summary>Details</summary>
Motivation: Representation learning remains underexplored in formal verification despite its success in vision and language domains. The paper aims to leverage model checking as a guiding signal for learning aligned representations of logical specifications and systems.

Method: Contrastive Neural Model Checking (CNML) jointly embeds logical specifications and systems into a shared latent space through a self-supervised contrastive objective, using model checking as the learning signal.

Result: CNML considerably outperforms both algorithmic and neural baselines on industry-inspired retrieval tasks in cross-modal and intra-modal settings. The learned representations effectively transfer to downstream tasks and generalize to more complex formulas.

Conclusion: Model checking can serve as an effective objective for learning representations for formal languages, demonstrating the potential of representation learning in formal verification.

Abstract: Model checking is a key technique for verifying safety-critical systems
against formal specifications, where recent applications of deep learning have
shown promise. However, while ubiquitous for vision and language domains,
representation learning remains underexplored in formal verification. We
introduce Contrastive Neural Model Checking (CNML), a novel method that
leverages the model checking task as a guiding signal for learning aligned
representations. CNML jointly embeds logical specifications and systems into a
shared latent space through a self-supervised contrastive objective. On
industry-inspired retrieval tasks, CNML considerably outperforms both
algorithmic and neural baselines in cross-modal and intra-modal settings.We
further show that the learned representations effectively transfer to
downstream tasks and generalize to more complex formulas. These findings
demonstrate that model checking can serve as an objective for learning
representations for formal languages.

</details>


### [181] [Explicit Discovery of Nonlinear Symmetries from Dynamic Data](https://arxiv.org/abs/2510.01855)
*Lexiang Hu,Yikang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: LieNLSD is the first method that can discover nonlinear symmetries by determining the number and explicit expressions of infinitesimal generators with nonlinear terms, improving neural PDE solver accuracy by over 20%.


<details>
  <summary>Details</summary>
Motivation: Symmetry is important for equivariant networks and governing equation discovery, but existing methods are limited to linear symmetries and fail to explicitly obtain Lie algebra subspaces for nonlinear symmetries.

Method: Specify a function library for infinitesimal group action, prove its prolongation formula is linear with respect to coefficient matrix, substitute central differences and neural network Jacobian into infinitesimal criterion to get linear equations, then solve using SVD.

Result: LieNLSD shows qualitative advantages over existing methods on top quark tagging and dynamic systems, improves long rollout accuracy of neural PDE solvers by over 20% when used for data augmentation.

Conclusion: LieNLSD successfully discovers nonlinear symmetries and their explicit Lie algebra expressions, demonstrating practical benefits in improving neural PDE solver performance.

Abstract: Symmetry is widely applied in problems such as the design of equivariant
networks and the discovery of governing equations, but in complex scenarios, it
is not known in advance. Most previous symmetry discovery methods are limited
to linear symmetries, and recent attempts to discover nonlinear symmetries fail
to explicitly get the Lie algebra subspace. In this paper, we propose LieNLSD,
which is, to our knowledge, the first method capable of determining the number
of infinitesimal generators with nonlinear terms and their explicit
expressions. We specify a function library for the infinitesimal group action
and aim to solve for its coefficient matrix, proving that its prolongation
formula for differential equations, which governs dynamic data, is also linear
with respect to the coefficient matrix. By substituting the central differences
of the data and the Jacobian matrix of the trained neural network into the
infinitesimal criterion, we get a system of linear equations for the
coefficient matrix, which can then be solved using SVD. On top quark tagging
and a series of dynamic systems, LieNLSD shows qualitative advantages over
existing methods and improves the long rollout accuracy of neural PDE solvers
by over 20% while applying to guide data augmentation. Code and data are
available at https://github.com/hulx2002/LieNLSD.

</details>


### [182] [Compositional meta-learning through probabilistic task inference](https://arxiv.org/abs/2510.01858)
*Jacob J. W. Bakermans,Pablo Tano,Reidar Riveland,Charles Findling,Alexandre Pouget*

Main category: cs.LG

TL;DR: A compositional meta-learning model that represents tasks as structured combinations of reusable computations, enabling rapid learning of new tasks from minimal examples through probabilistic inference without parameter updates.


<details>
  <summary>Details</summary>
Motivation: To enable effective knowledge reuse from previous tasks for solving new tasks with minimal experience, particularly focusing on compositional solutions where common computational elements can be flexibly recombined.

Method: Learn a generative model that captures underlying components and their statistics shared across task families, transforming new task learning into probabilistic inference through constrained hypothesis testing without parameter updates.

Result: Successfully recovers ground truth components and statistics in rule learning and motor learning tasks, and demonstrates ability to quickly infer new solutions from single examples.

Conclusion: The framework combines neural network expressivity with probabilistic inference data-efficiency to achieve rapid compositional meta-learning.

Abstract: To solve a new task from minimal experience, it is essential to effectively
reuse knowledge from previous tasks, a problem known as meta-learning.
Compositional solutions, where common elements of computation are flexibly
recombined into new configurations, are particularly well-suited for
meta-learning. Here, we propose a compositional meta-learning model that
explicitly represents tasks as structured combinations of reusable
computations. We achieve this by learning a generative model that captures the
underlying components and their statistics shared across a family of tasks.
This approach transforms learning a new task into a probabilistic inference
problem, which allows for finding solutions without parameter updates through
highly constrained hypothesis testing. Our model successfully recovers ground
truth components and statistics in rule learning and motor learning tasks. We
then demonstrate its ability to quickly infer new solutions from just single
examples. Together, our framework joins the expressivity of neural networks
with the data-efficiency of probabilistic inference to achieve rapid
compositional meta-learning.

</details>


### [183] [Universal Dynamic Regret and Constraint Violation Bounds for Constrained Online Convex Optimization](https://arxiv.org/abs/2510.01867)
*Subhamon Supantha,Abhishek Sinha*

Main category: cs.LG

TL;DR: This paper presents two modular algorithms for Online Convex Optimization with adversarial constraints, achieving improved universal dynamic regret and cumulative constraint violation bounds in the most general adversarial setting.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing OCO frameworks by handling adversarial constraints where both cost and constraint functions are chosen arbitrarily by an adversary, without requiring common feasible points in constraint functions.

Method: Reduces the constrained learning problem to standard OCO by constructing special surrogate cost functions, using two modular algorithms with simple structures.

Result: Achieves improved universal dynamic regret and cumulative constraint violation bounds compared to state-of-the-art methods, working in the most general adversarial setting.

Conclusion: The proposed modular approach successfully handles the most challenging case of OCO with adversarial constraints, providing strong theoretical guarantees without requiring common feasibility assumptions.

Abstract: We consider a generalization of the celebrated Online Convex Optimization
(OCO) framework with online adversarial constraints. We present two algorithms
having simple modular structures that yield universal dynamic regret and
cumulative constraint violation bounds, improving upon the state-of-the-art
results. Our results hold in the most general case when both the cost and
constraint functions are chosen arbitrarily by an adversary, and the constraint
functions need not contain any common feasible point. The results are
established by reducing the constrained learning problem to an instance of the
standard OCO problem with specially constructed surrogate cost functions.

</details>


### [184] [Randomized Gradient Subspaces for Efficient Large Language Model Training](https://arxiv.org/abs/2510.01878)
*Sahar Rajabi,Nayeema Nonta,Samanvay Vajpayee,Sirisha Rambhatla*

Main category: cs.LG

TL;DR: Analyzing gradient space dynamics reveals that while a small subspace captures most gradient energy, significant residual bulk remains and core subspace influence diminishes over time. New algorithms GrassWalk and GrassJump exploit this structure for state-of-the-art memory savings in LLM training.


<details>
  <summary>Details</summary>
Motivation: Training large language models is bottlenecked by extreme memory demands, particularly from optimizer states. Recent approaches use gradient projection into low-dimensional subspaces, but the dynamics of gradient space and its underlying subspaces need deeper analysis to develop more effective memory-efficient training methods.

Method: Analyzed gradient space dynamics and subspace properties, then introduced randomized algorithms GrassWalk and GrassJump that exploit subspace structure. These algorithms explicitly account for the near-flat curvature of gradient space geometry.

Result: Found that while small subspace captures most gradient energy, significant residual bulk remains; core subspace influence diminishes over time and in deeper layers. GrassWalk and GrassJump achieved state-of-the-art memory savings while improving performance on LLaMA-1B and LLaMA-7B pretraining.

Conclusion: Understanding gradient space dynamics enables development of more effective memory-efficient training algorithms. The proposed subspace-exploiting methods provide significant memory savings while maintaining or improving model performance in large-scale language model pretraining.

Abstract: Training large language models (LLMs) is often bottlenecked by extreme memory
demands, with optimizer states dominating the footprint. Recent works mitigates
this cost by projecting gradients into low-dimensional subspaces using
sophisticated update strategies. In this paper, we analyze the dynamics of
gradient space and its underlying subspaces. We find that while a small
subspace captures most gradient energy, a significant portion still resides in
the residual bulk; moreover, the influence of the core subspace diminishes over
time and in deeper layers. We also observe that the gradient space exhibits
near-flat curvature, calling for algorithms that explicitly account for this
geometry. Motivated by these insights, we introduce a suite of randomized
algorithms, GrassWalk and GrassJump, which exploit subspace and achieve
state-of-the-art memory savings while improving performance on LLaMA-1B and
LLaMA-7B pretraining.

</details>


### [185] [Multi-marginal temporal SchrÃ¶dinger Bridge Matching for video generation from unpaired data](https://arxiv.org/abs/2510.01894)
*Thomas Gravier,Thomas Boyer,Auguste Genovesio*

Main category: cs.LG

TL;DR: MMtSBM is a novel method for reconstructing temporal dynamics from static snapshots using multi-marginal SchrÃ¶dinger bridges, achieving state-of-the-art performance in high-dimensional settings like transcriptomics and image data.


<details>
  <summary>Details</summary>
Motivation: Many natural dynamic processes can only be observed through static snapshots, making it challenging to reconstruct temporal evolution. Existing methods have scalability issues in high dimensions and require restrictive assumptions.

Method: Extends Diffusion SchrÃ¶dinger Bridge Matching by deriving Iterative Markovian Fitting algorithm for multiple marginals in a factorized fashion, enabling video generation from unpaired data.

Result: Retains theoretical properties on toy examples, achieves state-of-the-art performance on real datasets including transcriptomic trajectory inference in 100D, and recovers couplings/dynamics in very high-dimensional image settings.

Conclusion: Establishes multi-marginal SchrÃ¶dinger bridges as a practical and principled approach for recovering hidden dynamics from static data.

Abstract: Many natural dynamic processes -- such as in vivo cellular differentiation or
disease progression -- can only be observed through the lens of static sample
snapshots. While challenging, reconstructing their temporal evolution to
decipher underlying dynamic properties is of major interest to scientific
research. Existing approaches enable data transport along a temporal axis but
are poorly scalable in high dimension and require restrictive assumptions to be
met. To address these issues, we propose \textit{\textbf{Multi-Marginal
temporal Schr\"odinger Bridge Matching}} (\textbf{MMtSBM}) \textit{for video
generation from unpaired data}, extending the theoretical guarantees and
empirical efficiency of Diffusion Schr\"odinger Bridge Matching
(arXiv:archive/2303.16852) by deriving the Iterative Markovian Fitting
algorithm to multiple marginals in a novel factorized fashion. Experiments show
that MMtSBM retains theoretical properties on toy examples, achieves
state-of-the-art performance on real world datasets such as transcriptomic
trajectory inference in 100 dimensions, and for the first time recovers
couplings and dynamics in very high dimensional image settings. Our work
establishes multi-marginal Schr\"odinger bridges as a practical and principled
approach for recovering hidden dynamics from static data.

</details>


### [186] [Multimodal Foundation Models for Early Disease Detection](https://arxiv.org/abs/2510.01899)
*Md Talha Mohsin,Ismail Abdulrashid*

Main category: cs.LG

TL;DR: A multimodal foundation model using attention-based transformers to integrate diverse healthcare data (EHR, imaging, genetics, wearables) for early disease diagnosis across oncology, cardiology, and neurology.


<details>
  <summary>Details</summary>
Motivation: Traditional diagnostic models analyze healthcare data sources in isolation, limiting their ability to identify cross-modal correlations essential for early disease detection.

Method: Uses dedicated encoders to map each modality into a shared latent space, then combines them using multi-head attention and residual normalization. Designed for pretraining on multiple tasks for easy adaptation to new diseases and datasets.

Result: Experimental evaluation on benchmark datasets in oncology, cardiology, and neurology for early detection tasks, with additional focus on data governance and model management tools.

Conclusion: The proposed method works toward a unified foundation model for precision diagnostics that could improve prediction accuracy and support clinical decision-making with enhanced transparency and interpretability.

Abstract: Healthcare generates diverse streams of data, including electronic health
records (EHR), medical imaging, genetics, and ongoing monitoring from wearable
devices. Traditional diagnostic models frequently analyze these sources in
isolation, which constrains their capacity to identify cross-modal correlations
essential for early disease diagnosis. Our research presents a multimodal
foundation model that consolidates diverse patient data through an
attention-based transformer framework. At first, dedicated encoders put each
modality into a shared latent space. Then, they combine them using multi-head
attention and residual normalization. The architecture is made for pretraining
on many tasks, which makes it easy to adapt to new diseases and datasets with
little extra work. We provide an experimental strategy that uses benchmark
datasets in oncology, cardiology, and neurology, with the goal of testing early
detection tasks. The framework includes data governance and model management
tools in addition to technological performance to improve transparency,
reliability, and clinical interpretability. The suggested method works toward a
single foundation model for precision diagnostics, which could improve the
accuracy of predictions and help doctors make decisions.

</details>


### [187] [A Methodology for Transparent Logic-Based Classification Using a Multi-Task Convolutional Tsetlin Machine](https://arxiv.org/abs/2510.01906)
*Mayur Kishor Shende,Ole-Christoffer Granmo,Runar Helin,Vladimir I. Zadorozhny,Rishad Shafik*

Main category: cs.LG

TL;DR: This paper explores applying Tsetlin Machine (TM) architecture to large-scale multi-channel image classification, proposing methods for local and global interpretations while maintaining competitive performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: To extend the Tsetlin Machine's applicability to complex RGB image classification while preserving its inherent interpretability advantages over neural networks.

Method: Proposed methodology to generate local interpretations (explaining model predictions) and global class representations (aggregating important patterns per class) using convolutional clauses that can be visualized as images.

Result: Achieved 98.5% accuracy on MNIST and 86.56% F1-score on CelebA (compared to 88.07% for ResNet50), demonstrating competitive performance while maintaining interpretability.

Conclusion: TM performs competitively with deep learning models like ResNet50 while preserving interpretability, providing better understanding of TM clauses and enabling application to more complex datasets.

Abstract: The Tsetlin Machine (TM) is a novel machine learning paradigm that employs
finite-state automata for learning and utilizes propositional logic to
represent patterns. Due to its simplistic approach, TMs are inherently more
interpretable than learning algorithms based on Neural Networks. The
Convolutional TM has shown comparable performance on various datasets such as
MNIST, K-MNIST, F-MNIST and CIFAR-2. In this paper, we explore the
applicability of the TM architecture for large-scale multi-channel (RGB) image
classification. We propose a methodology to generate both local interpretations
and global class representations. The local interpretations can be used to
explain the model predictions while the global class representations aggregate
important patterns for each class. These interpretations summarize the
knowledge captured by the convolutional clauses, which can be visualized as
images. We evaluate our methods on MNIST and CelebA datasets, using models that
achieve 98.5\% accuracy on MNIST and 86.56\% F1-score on CelebA (compared to
88.07\% for ResNet50) respectively. We show that the TM performs competitively
to this deep learning model while maintaining its interpretability, even in
large-scale complex training environments. This contributes to a better
understanding of TM clauses and provides insights into how these models can be
applied to more complex and diverse datasets.

</details>


### [188] [Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement](https://arxiv.org/abs/2510.01910)
*Zhaoyan Wang,Zheng Gao,Arogya Kharel,In-Young Ko*

Main category: cs.LG

TL;DR: This paper introduces RoGRAD, a novel framework for robust graph learning that addresses compound deficiencies in graph-structured data through iterative retrieval-augmented contrastive refinement, outperforming both conventional GNN and LLM-enhanced methods.


<details>
  <summary>Details</summary>
Motivation: Current GNNs struggle with real-world graph deficiencies, and there's a lack of systematic understanding of how graph-native and LLM-enhanced methods perform under compound deficiencies. The assumption that LLM augmentation is consistently superior needs validation.

Method: Proposed RoGRAD framework - an iterative paradigm using Retrieval-Augmented Generation (RAG) to inject retrieval-grounded augmentations, providing class-consistent diverse augmentations and enforcing discriminative representations through iterative graph contrastive learning.

Result: Extensive experiments show RoGRAD's superiority over conventional GNN- and LLM-enhanced baselines, achieving up to 82.43% average improvement. The study reveals overlooked vulnerabilities and challenges the assumption that LLM augmentation is consistently superior.

Conclusion: RoGRAD transforms LLM augmentation for graphs from static signal injection into dynamic refinement, demonstrating significant performance improvements and providing a more robust solution for handling compound graph deficiencies.

Abstract: Graph Neural Networks (GNNs) are widely adopted in Web-related applications,
serving as a core technique for learning from graph-structured data, such as
text-attributed graphs. Yet in real-world scenarios, such graphs exhibit
deficiencies that substantially undermine GNN performance. While prior
GNN-based augmentation studies have explored robustness against individual
imperfections, a systematic understanding of how graph-native and Large
Language Models (LLMs) enhanced methods behave under compound deficiencies is
still missing. Specifically, there has been no comprehensive investigation
comparing conventional approaches and recent LLM-on-graph frameworks, leaving
their merits unclear. To fill this gap, we conduct the first empirical study
that benchmarks these two lines of methods across diverse graph deficiencies,
revealing overlooked vulnerabilities and challenging the assumption that LLM
augmentation is consistently superior. Building on empirical findings, we
propose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement
(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is
the first iterative paradigm that leverages Retrieval-Augmented Generation
(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,
diverse augmentations and enforcing discriminative representations through
iterative graph contrastive learning. It transforms LLM augmentation for graphs
from static signal injection into dynamic refinement. Extensive experiments
demonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced
baselines, achieving up to 82.43% average improvement.

</details>


### [189] [StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold](https://arxiv.org/abs/2510.01938)
*Zhizhong Li,Sina Sajadmanesh,Jingtao Li,Lingjuan Lyu*

Main category: cs.LG

TL;DR: Proposes a geometry-aware LoRA extension using three-factor decomposition (USVâ¤) with Stiefel manifold constraints on U and V for improved parameter-efficient fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Standard LoRA lags behind full fine-tuning due to insufficient exploitation of geometric structure in low-rank manifolds.

Method: Uses three-factor decomposition USVâ¤ similar to SVD, constrains U and V to Stiefel manifold for orthonormality, employs geometric optimization to convert Euclidean optimizers to Riemannian ones.

Result: Superior performance across commonsense reasoning, math/code generation, image classification, and image generation tasks compared to state-of-the-art LoRA variants.

Conclusion: Geometry-aware LoRA with Stiefel manifold constraints enables more efficient subspace learning and achieves better performance while maintaining compatibility with existing fine-tuning pipelines.

Abstract: Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient
technique for fine-tuning large-scale pre-trained models. However, it still
lags behind full fine-tuning in performance, partly due to its insufficient
exploitation of the geometric structure underlying low-rank manifolds. In this
paper, we propose a geometry-aware extension of LoRA that uses a three-factor
decomposition $U\!SV^\top$. Analogous to the structure of singular value
decomposition (SVD), it separates the adapter's input and output subspaces, $V$
and $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie
on the Stiefel manifold, ensuring their orthonormality throughout the training.
To optimize on the Stiefel manifold, we employ a flexible and modular geometric
optimization design that converts any Euclidean optimizer to a Riemannian one.
It enables efficient subspace learning while remaining compatible with existing
fine-tuning pipelines. Empirical results across a wide range of downstream
tasks, including commonsense reasoning, math and code generation, image
classification, and image generation, demonstrate the superior performance of
our approach against the recent state-of-the-art variants of LoRA. Code is
available at https://github.com/SonyResearch/stella.

</details>


### [190] [Lower Bounds on Adversarial Robustness for Multiclass Classification with General Loss Functions](https://arxiv.org/abs/2510.01969)
*Camilo AndrÃ©s GarcÃ­a Trillos,NicolÃ¡s GarcÃ­a Trillos*

Main category: cs.LG

TL;DR: This paper provides dual and barycentric reformulations for adversarially robust multiclass classification under arbitrary loss functions, extending beyond 0-1 loss to include cross-entropy, power form losses, and quadratic loss.


<details>
  <summary>Details</summary>
Motivation: To enable efficient computation of sharp lower bounds for adversarial risks and facilitate robust classifier design beyond the limited 0-1 loss setting.

Method: Derived dual and barycentric reformulations of robust risk minimization, with explicit characterizations for cross-entropy loss, power form losses, and quadratic loss.

Result: Established connections between adversarial robustness, Î±-fair packing problems, and generalized barycenter problems using KL and Tsallis entropies as penalties. Numerical experiments showed tighter lower bounds for adversarial risks with cross-entropy loss.

Conclusion: The reformulations enable efficient computation and reveal fundamental connections between adversarial robustness and optimization problems, extending robust classification capabilities to diverse loss functions.

Abstract: We consider adversarially robust classification in a multiclass setting under
arbitrary loss functions and derive dual and barycentric reformulations of the
corresponding learner-agnostic robust risk minimization problem. We provide
explicit characterizations for important cases such as the cross-entropy loss,
loss functions with a power form, and the quadratic loss, extending in this way
available results for the 0-1 loss. These reformulations enable efficient
computation of sharp lower bounds for adversarial risks and facilitate the
design of robust classifiers beyond the 0-1 loss setting. Our paper uncovers
interesting connections between adversarial robustness, $\alpha$-fair packing
problems, and generalized barycenter problems for arbitrary positive measures
where Kullback-Leibler and Tsallis entropies are used as penalties. Our
theoretical results are accompanied with illustrative numerical experiments
where we obtain tighter lower bounds for adversarial risks with the
cross-entropy loss function.

</details>


### [191] [Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.01970)
*Yuanyuan Yao,Yuhan Shi,Lu Chen,Ziquan Fang,Yunjun Gao,Leong Hou U,Yushuai Li,Tianyi Li*

Main category: cs.LG

TL;DR: Moon is a supervised modality conversion framework for multivariate time series anomaly detection that converts numeric time series to images using MV-MTF, integrates multimodal data through parameter-shared CNN, and provides interpretable anomaly explanations using SHAP.


<details>
  <summary>Details</summary>
Motivation: Existing MTS anomaly detection methods face challenges: unsupervised methods rely on error thresholds causing inaccuracies, semi-supervised methods underuse anomaly labels, and supervised methods fail to capture local relationships with high computational costs and data scarcity.

Method: Proposes Moon framework with three components: (1) MV-MTF converts numeric time series to image representations capturing cross-variable relationships; (2) Multimodal-CNN integrates numeric and image data through feature fusion with parameter sharing; (3) SHAP-based explainer identifies key anomaly-contributing variables.

Result: Extensive experiments on six real-world MTS datasets show Moon outperforms six state-of-the-art methods by up to 93% in efficiency, 4% in accuracy, and 10.8% in interpretation performance.

Conclusion: Moon effectively addresses limitations of existing MTS anomaly detection methods by combining modality conversion, multimodal integration, and interpretable analysis, achieving superior performance in efficiency, accuracy, and interpretability.

Abstract: Multivariate time series (MTS) anomaly detection identifies abnormal patterns
where each timestamp contains multiple variables. Existing MTS anomaly
detection methods fall into three categories: reconstruction-based,
prediction-based, and classifier-based methods. However, these methods face two
key challenges: (1) Unsupervised learning methods, such as reconstruction-based
and prediction-based methods, rely on error thresholds, which can lead to
inaccuracies; (2) Semi-supervised methods mainly model normal data and often
underuse anomaly labels, limiting detection of subtle anomalies;(3) Supervised
learning methods, such as classifier-based approaches, often fail to capture
local relationships, incur high computational costs, and are constrained by the
scarcity of labeled data. To address these limitations, we propose Moon, a
supervised modality conversion-based multivariate time series anomaly detection
framework. Moon enhances the efficiency and accuracy of anomaly detection while
providing detailed anomaly analysis reports. First, Moon introduces a novel
multivariate Markov Transition Field (MV-MTF) technique to convert numeric time
series data into image representations, capturing relationships across
variables and timestamps. Since numeric data retains unique patterns that
cannot be fully captured by image conversion alone, Moon employs a
Multimodal-CNN to integrate numeric and image data through a feature fusion
model with parameter sharing, enhancing training efficiency. Finally, a
SHAP-based anomaly explainer identifies key variables contributing to
anomalies, improving interpretability. Extensive experiments on six real-world
MTS datasets demonstrate that Moon outperforms six state-of-the-art methods by
up to 93% in efficiency, 4% in accuracy and, 10.8% in interpretation
performance.

</details>


### [192] [$\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models](https://arxiv.org/abs/2510.01982)
*Yujie Zhou,Pengyang Ling,Jiazi Bu,Yibin Wang,Yuhang Zang,Jiaqi Wang,Li Niu,Guangtao Zhai*

Main category: cs.LG

TL;DR: G^2RPO is a novel framework that improves preference alignment in flow models by enabling granular reward assessments through singular stochastic sampling and multi-granularity advantage integration.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for aligning diffusion models with human preferences suffer from sub-optimal alignment due to sparse and narrow reward signals during stochastic sampling.

Method: Proposes Granular-GRPO framework with: 1) Singular Stochastic Sampling for step-wise exploration with high reward-noise correlation, 2) Multi-Granularity Advantage Integration to eliminate bias from fixed-granularity denoising.

Result: Experiments show G^2RPO significantly outperforms existing flow-based GRPO baselines across various reward models in both in-domain and out-of-domain evaluations.

Conclusion: G^2RPO achieves more precise and comprehensive reward assessments, demonstrating effectiveness and robustness in aligning flow models with human preferences.

Abstract: The integration of online reinforcement learning (RL) into diffusion and flow
models has recently emerged as a promising approach for aligning generative
models with human preferences. Stochastic sampling via Stochastic Differential
Equations (SDE) is employed during the denoising process to generate diverse
denoising directions for RL exploration. While existing methods effectively
explore potential high-value samples, they suffer from sub-optimal preference
alignment due to sparse and narrow reward signals. To address these challenges,
we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves
precise and comprehensive reward assessments of sampling directions in
reinforcement learning of flow models. Specifically, a Singular Stochastic
Sampling strategy is introduced to support step-wise stochastic exploration
while enforcing a high correlation between the reward and the injected noise,
thereby facilitating a faithful reward for each SDE perturbation. Concurrently,
to eliminate the bias inherent in fixed-granularity denoising, we introduce a
Multi-Granularity Advantage Integration module that aggregates advantages
computed at multiple diffusion scales, producing a more comprehensive and
robust evaluation of the sampling directions. Experiments conducted on various
reward models, including both in-domain and out-of-domain evaluations,
demonstrate that our $\text{G}^2$RPO significantly outperforms existing
flow-based GRPO baselines,highlighting its effectiveness and robustness.

</details>


### [193] [Private Federated Multiclass Post-hoc Calibration](https://arxiv.org/abs/2510.01987)
*Samuel Maddock,Graham Cormode,Carsten Maple*

Main category: cs.LG

TL;DR: This paper introduces federated calibration methods for machine learning models in privacy-sensitive FL settings, adapting traditional calibration techniques like histogram binning and temperature scaling to work under client heterogeneity and differential privacy constraints.


<details>
  <summary>Details</summary>
Motivation: Calibration is crucial for reliable decision-making in applications like healthcare and finance, but federated private calibration has been largely overlooked despite FL's use in these domains where calibration is strongly required.

Method: Transferred traditional centralized calibration methods (histogram binning and temperature scaling) into federated environments and defined new methods to operate them under strong client heterogeneity, studying both federated and user-level differential privacy settings.

Result: Demonstrated how both federation and differential privacy impact calibration accuracy, with federated temperature scaling working best for DP-FL and weighted binning approach performing best when DP is not required.

Conclusion: Proposed strategies to mitigate calibration degradation under heterogeneity, highlighting the effectiveness of different calibration approaches depending on whether differential privacy is required in the federated learning setting.

Abstract: Calibrating machine learning models so that predicted probabilities better
reflect the true outcome frequencies is crucial for reliable decision-making
across many applications. In Federated Learning (FL), the goal is to train a
global model on data which is distributed across multiple clients and cannot be
centralized due to privacy concerns. FL is applied in key areas such as
healthcare and finance where calibration is strongly required, yet federated
private calibration has been largely overlooked. This work introduces the
integration of post-hoc model calibration techniques within FL. Specifically,
we transfer traditional centralized calibration methods such as histogram
binning and temperature scaling into federated environments and define new
methods to operate them under strong client heterogeneity. We study (1) a
federated setting and (2) a user-level Differential Privacy (DP) setting and
demonstrate how both federation and DP impacts calibration accuracy. We propose
strategies to mitigate degradation commonly observed under heterogeneity and
our findings highlight that our federated temperature scaling works best for
DP-FL whereas our weighted binning approach is best when DP is not required.

</details>


### [194] [PepCompass: Navigating peptide embedding spaces using Riemannian Geometry](https://arxiv.org/abs/2510.01988)
*Marcin MoÅ¼ejko,Adam Bielecki,Jurand PrÄdzyÅski,Marcin Traskowski,Antoni Janowski,Karol Jurasz,MichaÅ Kucharczyk,Hyun-Su Lee,Marcelo Der Torossian Torres,Cesar de la Fuente-Nunez,Paulina Szymczak,MichaÅ Kmicikiewicz,Ewa Szczurek*

Main category: cs.LG

TL;DR: PepCompass is a geometry-aware framework for antimicrobial peptide discovery that uses Riemannian manifolds to model peptide space, enabling efficient exploration and optimization through local sampling methods and geodesic search.


<details>
  <summary>Details</summary>
Motivation: Traditional generative models for antimicrobial peptides use flat Euclidean metrics that distort peptide space geometry, making exploration inefficient. The astronomical size of peptide space and scarcity of active peptides require better geometric modeling.

Method: PepCompass defines Union of Îº-Stable Riemannian Manifolds to capture local geometry. It uses Second-Order Riemannian Brownian Efficient Sampling for local exploration and Mutation Enumeration in Tangent Space for discrete substitutions. Combines these in LE-BO algorithm for local optimization, and PoGS for geodesic interpolation between prototype embeddings.

Result: In-vitro validation showed PoGS discovered 4 novel seed peptides, and subsequent LE-BO optimization found 25 highly active peptides with broad-spectrum activity, including against resistant bacterial strains.

Conclusion: Geometry-informed exploration provides a powerful new paradigm for antimicrobial peptide design, overcoming limitations of traditional Euclidean approaches.

Abstract: Antimicrobial peptide discovery is challenged by the astronomical size of
peptide space and the relative scarcity of active peptides. Generative models
provide continuous latent "maps" of peptide space, but conventionally ignore
decoder-induced geometry and rely on flat Euclidean metrics, rendering
exploration and optimization distorted and inefficient. Prior manifold-based
remedies assume fixed intrinsic dimensionality, which critically fails in
practice for peptide data. Here, we introduce PepCompass, a geometry-aware
framework for peptide exploration and optimization. At its core, we define a
Union of $\kappa$-Stable Riemannian Manifolds $\mathbb{M}^{\kappa}$, a family
of decoder-induced manifolds that captures local geometry while ensuring
computational stability. We propose two local exploration methods: Second-Order
Riemannian Brownian Efficient Sampling, which provides a convergent
second-order approximation to Riemannian Brownian motion, and Mutation
Enumeration in Tangent Space, which reinterprets tangent directions as discrete
amino-acid substitutions. Combining these yields Local Enumeration Bayesian
Optimization (LE-BO), an efficient algorithm for local activity optimization.
Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which
interpolates between prototype embeddings along property-enriched geodesics,
biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro
validation confirms the effectiveness of PepCompass: PoGS yields four novel
seeds, and subsequent optimization with LE-BO discovers 25 highly active
peptides with broad-spectrum activity, including against resistant bacterial
strains. These results demonstrate that geometry-informed exploration provides
a powerful new paradigm for antimicrobial peptide design.

</details>


### [195] [Normality Calibration in Semi-supervised Graph Anomaly Detection](https://arxiv.org/abs/2510.02014)
*Guolei Zeng,Hezhe Qiao,Guoguo Ai,Jinsong Guo,Guansong Pang*

Main category: cs.LG

TL;DR: GraphNC is a graph normality calibration framework that improves semi-supervised graph anomaly detection by leveraging both labeled and unlabeled data to calibrate normality in anomaly score and representation spaces, addressing overfitting issues in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised GAD methods tend to overfit the limited labeled normal nodes, leading to high false positives and detection errors. The learned normality is constrained to the labeled patterns.

Method: GraphNC uses two components: 1) ScoreDA aligns anomaly scores with teacher model's distribution to separate normal/abnormal classes, and 2) NormReg regularizes representations by minimizing perturbation-guided consistency loss on labeled nodes to make normal representations more compact.

Result: The framework effectively pulls anomaly scores of normal and abnormal classes toward opposite ends, resulting in more separable anomaly scores while making normal node representations more compact.

Conclusion: GraphNC successfully addresses the overfitting limitation in semi-supervised GAD by calibrating normality across both score and representation spaces using both labeled and unlabeled data, leading to improved anomaly detection performance.

Abstract: Graph anomaly detection (GAD) has attracted growing interest for its crucial
ability to uncover irregular patterns in broad applications. Semi-supervised
GAD, which assumes a subset of annotated normal nodes available during
training, is among the most widely explored application settings. However, the
normality learned by existing semi-supervised GAD methods is limited to the
labeled normal nodes, often inclining to overfitting the given patterns. These
can lead to high detection errors, such as high false positives. To overcome
this limitation, we propose GraphNC , a graph normality calibration framework
that leverages both labeled and unlabeled data to calibrate the normality from
a teacher model (a pre-trained semi-supervised GAD model) jointly in anomaly
score and node representation spaces. GraphNC includes two main components,
anomaly score distribution alignment (ScoreDA) and perturbation-based normality
regularization (NormReg). ScoreDA optimizes the anomaly scores of our model by
aligning them with the score distribution yielded by the teacher model. Due to
accurate scores in most of the normal nodes and part of the anomaly nodes in
the teacher model, the score alignment effectively pulls the anomaly scores of
the normal and abnormal classes toward the two ends, resulting in more
separable anomaly scores. Nevertheless, there are inaccurate scores from the
teacher model. To mitigate the misleading by these scores, NormReg is designed
to regularize the graph normality in the representation space, making the
representations of normal nodes more compact by minimizing a
perturbation-guided consistency loss solely on the labeled nodes.

</details>


### [196] [FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data](https://arxiv.org/abs/2510.02017)
*Aida Tayebi,Ali Khodabandeh Yalabadi,Mehdi Yazdani-Jahromi,Ozlem Ozmen Garibay*

Main category: cs.LG

TL;DR: A contrastive learning framework for learning fair representations in tabular data, reducing bias while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: AI systems embedded in everyday life require fair and unbiased models. Fair representation learning is crucial for debiasing algorithms, but fairness in tabular data representations is underexplored.

Method: Contrastive learning framework with strategic positive pair selection, using both supervised and self-supervised contrastive learning for tabular datasets.

Result: Significantly reduces bias compared to state-of-the-art contrastive learning models for tabular data, with minimal accuracy trade-off.

Conclusion: The approach effectively mitigates bias in tabular data representations and enables use of fair representations in downstream tasks.

Abstract: As AI systems become more embedded in everyday life, the development of fair
and unbiased models becomes more critical. Considering the social impact of AI
systems is not merely a technical challenge but a moral imperative. As
evidenced in numerous research studies, learning fair and robust
representations has proven to be a powerful approach to effectively debiasing
algorithms and improving fairness while maintaining essential information for
prediction tasks. Representation learning frameworks, particularly those that
utilize self-supervised and contrastive learning, have demonstrated superior
robustness and generalizability across various domains. Despite the growing
interest in applying these approaches to tabular data, the issue of fairness in
these learned representations remains underexplored. In this study, we
introduce a contrastive learning framework specifically designed to address
bias and learn fair representations in tabular datasets. By strategically
selecting positive pair samples and employing supervised and self-supervised
contrastive learning, we significantly reduce bias compared to existing
state-of-the-art contrastive learning models for tabular data. Our results
demonstrate the efficacy of our approach in mitigating bias with minimum
trade-off in accuracy and leveraging the learned fair representations in
various downstream tasks.

</details>


### [197] [Mathematical Modeling and Convergence Analysis of Deep Neural Networks with Dense Layer Connectivities in Deep Learning](https://arxiv.org/abs/2510.02049)
*Jinshu Huang,Haibin Su,Xue-Cheng Tai,Chunlin Wu*

Main category: cs.LG

TL;DR: This paper provides a mathematical framework for analyzing densely connected deep neural networks (DNNs) using nonlinear integral equations and optimal control theory, proving convergence from discrete network learning to continuous-time counterparts.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous mathematical foundation for understanding densely connected DNNs and analyze their learning problems in the deep-layer limit, addressing the need for theoretical insights beyond the common ordinary differential equation viewpoint.

Method: Developed a dense non-local (DNL) framework modeling densely connected DNNs as nonlinear integral equations, studied training problems from optimal control perspective, and used piecewise linear extension with Î-convergence analysis to prove convergence results.

Result: Proved convergence of optimal values and subsequence convergence of minimizers from network learning problems to their continuous-time counterparts, demonstrating that densely connected architectures can offer training stability for deep models.

Conclusion: The work provides a solid mathematical foundation for densely connected DNNs and suggests these architectures enhance training stability in deep learning models, offering theoretical support for their empirical success.

Abstract: In deep learning, dense layer connectivity has become a key design principle
in deep neural networks (DNNs), enabling efficient information flow and strong
performance across a range of applications. In this work, we model densely
connected DNNs mathematically and analyze their learning problems in the
deep-layer limit. For a broad applicability, we present our analysis in a
framework setting of DNNs with densely connected layers and general non-local
feature transformations (with local feature transformations as special cases)
within layers, which is called dense non-local (DNL) framework and includes
standard DenseNets and variants as special examples. In this formulation, the
densely connected networks are modeled as nonlinear integral equations, in
contrast to the ordinary differential equation viewpoint commonly adopted in
prior works. We study the associated training problems from an optimal control
perspective and prove convergence results from the network learning problem to
its continuous-time counterpart. In particular, we show the convergence of
optimal values and the subsequence convergence of minimizers, using a piecewise
linear extension and $\Gamma$-convergence analysis. Our results provide a
mathematical foundation for understanding densely connected DNNs and further
suggest that such architectures can offer stability of training deep models.

</details>


### [198] [Adaptive Heterogeneous Mixtures of Normalising Flows for Robust Variational Inference](https://arxiv.org/abs/2510.02056)
*Benjamin Wiriyapong,Oktay KarakuÅ,Kirill Sidorov*

Main category: cs.LG

TL;DR: AMF-VI is a two-stage adaptive mixture of complementary normalizing flows (MAF, RealNVP, RBIG) that achieves robust variational inference across diverse posterior distributions by combining sequential expert training with likelihood-driven global weight estimation.


<details>
  <summary>Details</summary>
Motivation: Single-flow variational inference models often behave inconsistently across qualitatively different posterior distributions, lacking robustness across various shapes and modalities.

Method: Two-stage approach: (1) sequential expert training of individual flows (MAF, RealNVP, RBIG), (2) adaptive global weight estimation via likelihood-driven updates without per-sample gating or architectural changes.

Result: Consistently lower negative log-likelihood than single-flow baselines on six canonical posterior families; improved robustness in transport metrics (Wasserstein-2) and maximum mean discrepancy across various shapes and modalities.

Conclusion: Adaptive mixtures of diverse flows provide reliable robust variational inference across diverse posterior families while preserving each expert's inductive bias, with minimal computational overhead.

Abstract: Normalising-flow variational inference (VI) can approximate complex
posteriors, yet single-flow models often behave inconsistently across
qualitatively different distributions. We propose Adaptive Mixture Flow
Variational Inference (AMF-VI), a heterogeneous mixture of complementary flows
(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of
individual flows, and (ii) adaptive global weight estimation via
likelihood-driven updates, without per-sample gating or architectural changes.
Evaluated on six canonical posterior families of banana, X-shape, two-moons,
rings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower
negative log-likelihood than each single-flow baseline and delivers stable
gains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),
indicating improved robustness across shapes and modalities. The procedure is
efficient and architecture-agnostic, incurring minimal overhead relative to
standard flow training, and demonstrates that adaptive mixtures of diverse
flows provide a reliable route to robust VI across diverse posterior families
whilst preserving each expert's inductive bias.

</details>


### [199] [Inferring Optical Tissue Properties from Photoplethysmography using Hybrid Amortized Inference](https://arxiv.org/abs/2510.02073)
*Jens Behrmann,Maria R. Cervera,Antoine Wehenkel,Andrew C. Miller,Albert Cerussi,Pranay Jain,Vivek Venugopal,Shijie Yan,Guillermo Sapiro,Luca Pegolotti,JÃ¶rn-Henrik Jacobsen*

Main category: cs.LG

TL;DR: PPGen is a biophysical model that interprets PPG signals using physiological parameters, combined with hybrid amortized inference for robust parameter estimation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between deep learning's predictive power and the need for clinical interpretability in PPG signal analysis.

Method: Developed PPGen biophysical model and hybrid amortized inference (HAI) for parameter estimation from PPG signals.

Result: HAI accurately infers physiological parameters under various noise and sensor conditions in in-silico experiments.

Conclusion: PPGen enables interpretable PPG analysis while maintaining fidelity, supporting clinical use and hardware design.

Abstract: Smart wearables enable continuous tracking of established biomarkers such as
heart rate, heart rate variability, and blood oxygen saturation via
photoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer
physiological information, as recent deep learning (DL) studies demonstrate.
However, DL models often rely on features with unclear physiological meaning,
creating a tension between predictive power, clinical interpretability, and
sensor design. We address this gap by introducing PPGen, a biophysical model
that relates PPG signals to interpretable physiological and optical parameters.
Building on PPGen, we propose hybrid amortized inference (HAI), enabling fast,
robust, and scalable estimation of relevant physiological parameters from PPG
signals while correcting for model misspecification. In extensive in-silico
experiments, we show that HAI can accurately infer physiological parameters
under diverse noise and sensor conditions. Our results illustrate a path toward
PPG models that retain the fidelity needed for DL-based features while
supporting clinical interpretation and informed hardware design.

</details>


### [200] [Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions](https://arxiv.org/abs/2510.02081)
*Zhaoyi Li,Jingtao Ding,Yong Li,Shihua Li*

Main category: cs.LG

TL;DR: This paper addresses the train-inference gap in Flow Matching (FM) algorithms by proposing a fine-tuning method using Maximum Likelihood Estimation of reconstructions, which improves inference performance in image generation and robotic manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Flow Matching algorithms have a train-inference gap where model outputs cannot be assessed during training, unlike other generative models that directly optimize reconstruction loss. This gap is problematic in precision-demanding scenarios like robotic manipulation, and FM's pursuit of straight predefined paths can introduce stiffness issues.

Method: The paper proposes fine-tuning FM via Maximum Likelihood Estimation of reconstructions, including straightforward fine-tuning and residual-based fine-tuning approaches. The residual-based method incorporates contraction properties through specific architectures to enhance robustness and interpretability.

Result: Experimental results in both image generation and robotic manipulation tasks demonstrate that the proposed method reliably improves the inference performance of Flow Matching algorithms.

Conclusion: The Maximum Likelihood Estimation-based fine-tuning approach effectively bridges the train-inference gap in FM, addressing issues like stiffness and improving performance in high-precision applications while maintaining the benefits of FM's smooth ODE formulation.

Abstract: Flow Matching (FM) algorithm achieves remarkable results in generative tasks
especially in robotic manipulation. Building upon the foundations of diffusion
models, the simulation-free paradigm of FM enables simple and efficient
training, but inherently introduces a train-inference gap. Specifically, we
cannot assess the model's output during the training phase. In contrast, other
generative models including Variational Autoencoder (VAE), Normalizing Flow and
Generative Adversarial Networks (GANs) directly optimize on the reconstruction
loss. Such a gap is particularly evident in scenarios that demand high
precision, such as robotic manipulation. Moreover, we show that FM's
over-pursuit of straight predefined paths may introduce some serious problems
such as stiffness into the system. These motivate us to fine-tune FM via
Maximum Likelihood Estimation of reconstructions - an approach made feasible by
FM's underlying smooth ODE formulation, in contrast to the stochastic
differential equations (SDEs) used in diffusion models. This paper first
theoretically analyzes the relation between training loss and inference error
in FM. Then we propose a method of fine-tuning FM via Maximum Likelihood
Estimation of reconstructions, which includes both straightforward fine-tuning
and residual-based fine-tuning approaches. Furthermore, through specifically
designed architectures, the residual-based fine-tuning can incorporate the
contraction property into the model, which is crucial for the model's
robustness and interpretability. Experimental results in image generation and
robotic manipulation verify that our method reliably improves the inference
performance of FM.

</details>


### [201] [KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting](https://arxiv.org/abs/2510.02084)
*Kuiye Ding,Fanda Fan,Zheya Wang,Hongxiao Li,Yifan Wang,Lei Wang,Chunjie Luo,Jianfeng Zhan*

Main category: cs.LG

TL;DR: KAIROS is a non-autoregressive time series forecasting framework that models segment-level multi-peak distributions, enabling fast inference without error accumulation while maintaining accuracy comparable to state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Web applications require fast time series forecasting for real-time decision making in resource planning, cache placement, and anomaly response, but existing approaches suffer from error accumulation (autoregressive) or over-smoothing (non-autoregressive).

Method: Directly models segment-level multi-peak distributions using a non-autoregressive framework, avoiding sequential prediction and enabling just-in-time inference.

Result: Demonstrates strong zero-shot generalization on six benchmarks, achieving forecasting performance comparable to state-of-the-art foundation models with similar scale, but at much lower inference cost.

Conclusion: KAIROS highlights non-autoregressive design as a scalable paradigm for foundation models in time series, offering efficient and accurate forecasting suitable for web applications.

Abstract: In the World Wide Web, reliable time series forecasts provide the
forward-looking signals that drive resource planning, cache placement, and
anomaly response, enabling platforms to operate efficiently as user behavior
and content distributions evolve. Compared with other domains, time series
forecasting for Web applications requires much faster responsiveness to support
real-time decision making. We present KAIROS, a non-autoregressive time series
forecasting framework that directly models segment-level multi-peak
distributions. Unlike autoregressive approaches, KAIROS avoids error
accumulation and achieves just-in-time inference, while improving over existing
non-autoregressive models that collapse to over-smoothed predictions. Trained
on the large-scale corpus, KAIROS demonstrates strong zero-shot generalization
on six widely used benchmarks, delivering forecasting performance comparable to
state-of-the-art foundation models with similar scale, at a fraction of their
inference cost. Beyond empirical results, KAIROS highlights the importance of
non-autoregressive design as a scalable paradigm for foundation models in time
series.

</details>


### [202] [Learning Model Representations Using Publicly Available Model Hubs](https://arxiv.org/abs/2510.02096)
*Damian Falk,Konstantin SchÃ¼rholt,Konstantinos Tzevelekakis,LÃ©o Meynent,Damian Borth*

Main category: cs.LG

TL;DR: This paper proposes training weight space learning backbones on heterogeneous models from unstructured repositories like Hugging Face, eliminating the need for curated model zoos.


<details>
  <summary>Details</summary>
Motivation: Current weight space learning requires large, curated model zoos that are computationally expensive to create, limiting scale and flexibility.

Method: Develop a new weight space backbone designed to handle unstructured model populations from repositories with varying architectures and datasets.

Result: Weight space representations trained on Hugging Face models achieve strong performance, often outperforming those trained on laboratory-generated model zoos, and generalize to unseen data modalities.

Conclusion: Curated model zoos are not indispensable for weight space learning, as high-quality representations can be learned from unstructured model repositories.

Abstract: The weights of neural networks have emerged as a novel data modality, giving
rise to the field of weight space learning. A central challenge in this area is
that learning meaningful representations of weights typically requires large,
carefully constructed collections of trained models, typically referred to as
model zoos. These model zoos are often trained ad-hoc, requiring large
computational resources, constraining the learned weight space representations
in scale and flexibility. In this work, we drop this requirement by training a
weight space learning backbone on arbitrary models downloaded from large,
unstructured model repositories such as Hugging Face. Unlike curated model
zoos, these repositories contain highly heterogeneous models: they vary in
architecture and dataset, and are largely undocumented. To address the
methodological challenges posed by such heterogeneity, we propose a new weight
space backbone designed to handle unstructured model populations. We
demonstrate that weight space representations trained on models from Hugging
Face achieve strong performance, often outperforming backbones trained on
laboratory-generated model zoos. Finally, we show that the diversity of the
model weights in our training set allows our weight space model to generalize
to unseen data modalities. By demonstrating that high-quality weight space
representations can be learned in the wild, we show that curated model zoos are
not indispensable, thereby overcoming a strong limitation currently faced by
the weight space learning community.

</details>


### [203] [PENEX: AdaBoost-Inspired Neural Network Regularization](https://arxiv.org/abs/2510.02107)
*Klaus-Rudolf Kladny,Bernhard SchÃ¶lkopf,Michael Muehlebach*

Main category: cs.LG

TL;DR: PENEX is a new multi-class exponential loss formulation that enables first-order optimization, implicitly maximizes margins, and shows better regularization than established methods in computer vision and language tasks.


<details>
  <summary>Details</summary>
Motivation: AdaBoost uses exponential loss that penalizes mislabeled points severely but generalizes well with many weak learners. The existing exponential loss formulation is not amenable to first-order optimization methods.

Method: Introduce Penalized Exponential Loss (PENEX), a theoretically grounded multi-class exponential loss formulation that can be optimized via first-order methods. It implicitly maximizes margins and parameterizes weak learners through gradient increments.

Result: PENEX shows a regularizing effect often better than established methods with similar computational cost across computer vision and language tasks. It implicitly maximizes margins of data points.

Conclusion: PENEX has potential as an AdaBoost-inspired alternative for effective training and fine-tuning of deep neural networks, providing theoretical grounding and practical optimization benefits.

Abstract: AdaBoost sequentially fits so-called weak learners to minimize an exponential
loss, which penalizes mislabeled data points more severely than other loss
functions like cross-entropy. Paradoxically, AdaBoost generalizes well in
practice as the number of weak learners grows. In the present work, we
introduce Penalized Exponential Loss (PENEX), a new formulation of the
multi-class exponential loss that is theoretically grounded and, in contrast to
the existing formulation, amenable to optimization via first-order methods. We
demonstrate both empirically and theoretically that PENEX implicitly maximizes
margins of data points. Also, we show that gradient increments on PENEX
implicitly parameterize weak learners in the boosting framework. Across
computer vision and language tasks, we show that PENEX exhibits a regularizing
effect often better than established methods with similar computational cost.
Our results highlight PENEX's potential as an AdaBoost-inspired alternative for
effective training and fine-tuning of deep neural networks.

</details>


### [204] [Hybrid Deep Learning Modeling Approach to Predict Natural Gas Consumption of Home Subscribers on Limited Data](https://arxiv.org/abs/2510.02115)
*Milad Firoozeh,Nader Dashti,Mohammad Ali Hatefi*

Main category: cs.LG

TL;DR: This study analyzes and predicts residential gas consumption in Zanjan province, Iran using machine learning models including LSTM, GRU, and a hybrid BiLSTM-XGBoost model, with the hybrid model showing superior performance.


<details>
  <summary>Details</summary>
Motivation: Iran faces gas pressure drops and outages during cold seasons due to population growth and energy consumption, particularly in the residential sector which has the largest consumption share, necessitating better consumption control.

Method: Used machine learning models (LSTM, GRU, and hybrid BiLSTM-XGBoost) trained on six years of gas consumption and meteorology data (2017-2022) to predict consumption patterns.

Result: The hybrid BiLSTM-XGBoost model outperformed other models with lower RMSE, MAPE, and MPE values, and demonstrated robust performance especially with limited data.

Conclusion: Machine learning approaches, particularly hybrid models, can effectively manage and predict gas consumption, contributing to more efficient resource management and reducing seasonal shortages, with geographical and climatic factors being important considerations.

Abstract: Today, natural gas, as a clean fuel and the best alternative to crude oil,
covers a significant part of global demand. Iran is one of the largest
countries with energy resources and in terms of gas is the second-largest
country in the world. But, due to the increase in population and energy
consumption, it faces problems such as pressure drops and gas outages yearly in
cold seasons and therefore it is necessary to control gas consumption,
especially in the residential sector, which has the largest share in Iran. This
study aims to analyze and predict gas consumption for residential customers in
Zanjan province, Iran, using machine learning models, including LSTM, GRU, and
a hybrid BiLSTM-XGBoost model. The dataset consists of gas consumption and
meteorology data collected over six years, from 2017 to 2022. The models were
trained and evaluated based on their ability to accurately predict consumption
patterns. The results indicate that the hybrid BiLSTM-XGBoost model
outperformed the other models in terms of accuracy, with lower Root Mean
Squared Error (RMSE), Mean Absolute Percentage Error (MAPE) values, and Mean
Percentage Error (MPE). Additionally, the Hybrid model demonstrated robust
performance, particularly in scenarios with limited data. The findings suggest
that machine learning approaches, particularly hybrid models, can be
effectively utilized to manage and predict gas consumption, contributing to
more efficient resource management and reducing seasonal shortages. This study
highlights the importance of incorporating geographical and climatic factors in
predictive modeling, as these significantly influence gas usage across
different regions.

</details>


### [205] [Ensemble Threshold Calibration for Stable Sensitivity Control](https://arxiv.org/abs/2510.02116)
*John N. Daras*

Main category: cs.LG

TL;DR: An end-to-end framework for precise recall control in spatial conflation tasks, achieving exact recall with sub-percent variance through neural ranking, ensemble threshold estimation, and TPU-friendly processing.


<details>
  <summary>Details</summary>
Motivation: Classical confidence-interval methods overshoot recall targets and have high variance in skewed score distributions, making precise recall control difficult in large-scale spatial conflation where missing matches breaks analytics and excessive review inflates costs.

Method: Pipeline with equigrid bounding-box filter and CSR representation for efficient pair enumeration, neural ranker trained on bootstrap samples, score propagation, stratified calibration set, and ensemble of four threshold estimators (Clopper-Pearson, Jeffreys, Wilson, exact quantile) with inverse-variance weighting across multiple subsamples.

Result: Consistently hits recall targets within small error on two real cadastral datasets (6.31M and 67.34M pairs), reduces redundant verifications compared to other calibrations, and runs end-to-end on single TPU v3 core.

Conclusion: The framework achieves precise recall control with minimal variance while being computationally efficient and TPU-friendly, addressing limitations of classical methods in large-scale spatial matching tasks.

Abstract: Precise recall control is critical in large-scale spatial conflation and
entity-matching tasks, where missing even a few true matches can break
downstream analytics, while excessive manual review inflates cost. Classical
confidence-interval cuts such as Clopper-Pearson or Wilson provide lower bounds
on recall, but they routinely overshoot the target by several percentage points
and exhibit high run-to-run variance under skewed score distributions. We
present an end-to-end framework that achieves exact recall with sub-percent
variance over tens of millions of geometry pairs, while remaining TPU-friendly.
Our pipeline starts with an equigrid bounding-box filter and compressed sparse
row (CSR) candidate representation, reducing pair enumeration by two orders of
magnitude. A deterministic xxHash bootstrap sample trains a lightweight neural
ranker; its scores are propagated to all remaining pairs via a single forward
pass and used to construct a reproducible, score-decile-stratified calibration
set. Four complementary threshold estimators - Clopper-Pearson, Jeffreys,
Wilson, and an exact quantile - are aggregated via inverse-variance weighting,
then fused across nine independent subsamples. This ensemble reduces threshold
variance compared to any single method. Evaluated on two real cadastral
datasets (approximately 6.31M and 67.34M pairs), our approach consistently hits
a recall target within a small error, decreases redundant verifications
relative to other calibrations, and runs end-to-end on a single TPU v3 core.

</details>


### [206] [DAG DECORation: Continuous Optimization for Structure Learning under Hidden Confounding](https://arxiv.org/abs/2510.02117)
*Samhita Pal,James O'quinn,Kaveh Aryan,Heather Pua,James P. Long,Amir Asiaee*

Main category: cs.LG

TL;DR: DECOR is a differentiable estimator that jointly learns DAGs and correlated noise models for linear Gaussian SEMs with latent confounding, providing identifiability under bow-free graphs and uniform eigenvalue conditions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with latent confounding in linear Gaussian SEMs - continuous methods need independent errors, while deconfounding pipelines require pervasive factors or nonlinearity.

Method: DECOR uses a single likelihood-based differentiable estimator that alternates between smooth-acyclic graph updates and convex noise updates, with optional bow complementarity penalty or post hoc reconciliation.

Result: DECOR matches or outperforms baselines on synthetic benchmarks across varying confounding density, graph density, latent rank, and dimension, especially robust under non-pervasive confounding.

Conclusion: DECOR provides a unified approach for structure learning in linear Gaussian SEMs with latent confounding, with theoretical identifiability guarantees and strong empirical performance.

Abstract: We study structure learning for linear Gaussian SEMs in the presence of
latent confounding. Existing continuous methods excel when errors are
independent, while deconfounding-first pipelines rely on pervasive factor
structure or nonlinearity. We propose \textsc{DECOR}, a single likelihood-based
and fully differentiable estimator that jointly learns a DAG and a correlated
noise model. Our theory gives simple sufficient conditions for global parameter
identifiability: if the mixed graph is bow free and the noise covariance has a
uniform eigenvalue margin, then the map from $(\B,\OmegaMat)$ to the
observational covariance is injective, so both the directed structure and the
noise are uniquely determined. The estimator alternates a smooth-acyclic graph
update with a convex noise update and can include a light bow complementarity
penalty or a post hoc reconciliation step. On synthetic benchmarks that vary
confounding density, graph density, latent rank, and dimension with $n<p$,
\textsc{DECOR} matches or outperforms strong baselines and is especially robust
when confounding is non-pervasive, while remaining competitive under
pervasiveness.

</details>


### [207] [Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study](https://arxiv.org/abs/2510.02142)
*Lena Podina,Christina Humer,Alexandre Duval,Victor Schmidt,Ali Ramlaoui,Shahana Chatterjee,Yoshua Bengio,Alex Hernandez-Garcia,David Rolnick,FÃ©lix Therrien*

Main category: cs.LG

TL;DR: Catalyst GFlowNet is a generative model that uses ML predictors to design efficient catalyst surfaces for hydrogen energy storage, successfully identifying platinum as the best catalyst for hydrogen evolution reaction.


<details>
  <summary>Details</summary>
Motivation: To address the need for affordable and high-performance catalysts in hydrogen energy storage systems, which are essential for renewable energy adoption and stable energy supply despite fluctuations in wind and solar sources.

Method: Developed Catalyst GFlowNet, a generative model that leverages machine learning-based predictors of formation and adsorption energy to design crystal surfaces that act as efficient catalysts.

Result: Successfully demonstrated the model's performance through proof-of-concept application to hydrogen evolution reaction, identifying platinum as the most efficient known catalyst.

Conclusion: The generative modeling framework offers a promising pathway for accelerating the search for novel and efficient catalysts, with future plans to extend to oxygen evolution reaction and explore new materials.

Abstract: Efficient and inexpensive energy storage is essential for accelerating the
adoption of renewable energy and ensuring a stable supply, despite fluctuations
in sources such as wind and solar. Electrocatalysts play a key role in hydrogen
energy storage (HES), allowing the energy to be stored as hydrogen. However,
the development of affordable and high-performance catalysts for this process
remains a significant challenge. We introduce Catalyst GFlowNet, a generative
model that leverages machine learning-based predictors of formation and
adsorption energy to design crystal surfaces that act as efficient catalysts.
We demonstrate the performance of the model through a proof-of-concept
application to the hydrogen evolution reaction, a key reaction in HES, for
which we successfully identified platinum as the most efficient known catalyst.
In future work, we aim to extend this approach to the oxygen evolution
reaction, where current optimal catalysts are expensive metal oxides, and open
the search space to discover new materials. This generative modeling framework
offers a promising pathway for accelerating the search for novel and efficient
catalysts.

</details>


### [208] [Policy Gradient Guidance Enables Test Time Control](https://arxiv.org/abs/2510.02148)
*Jianing Qi,Hao Tang,Zhigang Zhu*

Main category: cs.LG

TL;DR: PGG extends classifier-free guidance from diffusion models to policy gradient methods, enabling test-time behavior modulation without retraining through interpolation of conditional and unconditional policy branches.


<details>
  <summary>Details</summary>
Motivation: To bring the benefits of guidance mechanisms from diffusion models to reinforcement learning, enabling controllable behavior modulation at test time without requiring additional training.

Method: Augments policy gradient with unconditional branch and interpolates conditional/unconditional branches, with theoretical derivation showing normalization term vanishes under advantage estimation.

Result: PGG improves stability, sample efficiency, and controllability in discrete/continuous control tasks, with modest guidance (Î³>1) providing consistent benefits.

Conclusion: Guidance mechanisms can be successfully adapted from diffusion policies to standard on-policy RL methods, opening new directions for controllable online reinforcement learning.

Abstract: We introduce Policy Gradient Guidance (PGG), a simple extension of
classifier-free guidance from diffusion models to classical policy gradient
methods. PGG augments the policy gradient with an unconditional branch and
interpolates conditional and unconditional branches, yielding a test-time
control knob that modulates behavior without retraining. We provide a
theoretical derivation showing that the additional normalization term vanishes
under advantage estimation, leading to a clean guided policy gradient update.
Empirically, we evaluate PGG on discrete and continuous control benchmarks. We
find that conditioning dropout-central to diffusion guidance-offers gains in
simple discrete tasks and low sample regimes, but dropout destabilizes
continuous control. Training with modestly larger guidance ($\gamma>1$)
consistently improves stability, sample efficiency, and controllability. Our
results show that guidance, previously confined to diffusion policies, can be
adapted to standard on-policy methods, opening new directions for controllable
online reinforcement learning.

</details>


### [209] [Reinforcement Learning with Action-Triggered Observations](https://arxiv.org/abs/2510.02149)
*Alexander Ryabchenko,Wenlong Mou*

Main category: cs.LG

TL;DR: This paper introduces ATST-MDPs for reinforcement learning with action-triggered state observations, develops Bellman optimality equations and action-sequence learning, and proposes ST-LSVI-UCB algorithm with regret guarantees.


<details>
  <summary>Details</summary>
Motivation: Many real-world applications have stochastically triggered state observations by actions, requiring specialized RL frameworks beyond standard MDPs.

Method: Formulates ATST-MDPs with action-triggered observations, derives tailored Bellman equations, introduces action-sequence learning paradigm, and develops ST-LSVI-UCB algorithm under linear MDP assumption.

Result: ST-LSVI-UCB achieves regret O~(âKdÂ³(1-Î³)â»Â³), demonstrating efficient learning is feasible with sporadic observations.

Conclusion: Establishes theoretical foundation for learning with action-triggered sporadic observations and shows efficient learning remains possible under such constraints.

Abstract: We study reinforcement learning problems where state observations are
stochastically triggered by actions, a constraint common in many real-world
applications. This framework is formulated as Action-Triggered Sporadically
Traceable Markov Decision Processes (ATST-MDPs), where each action has a
specified probability of triggering a state observation. We derive tailored
Bellman optimality equations for this framework and introduce the
action-sequence learning paradigm in which agents commit to executing a
sequence of actions until the next observation arrives. Under the linear MDP
assumption, value-functions are shown to admit linear representations in an
induced action-sequence feature map. Leveraging this structure, we propose
off-policy estimators with statistical error guarantees for such feature maps
and introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered
settings. ST-LSVI-UCB achieves regret $\widetilde
O(\sqrt{Kd^3(1-\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the
feature dimension, and $\gamma$ the discount factor (per-step episode
non-termination probability). Crucially, this work establishes the theoretical
foundation for learning with sporadic, action-triggered observations while
demonstrating that efficient learning remains feasible under such observation
constraints.

</details>


### [210] [StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?](https://arxiv.org/abs/2510.02209)
*Yanxu Chen,Zijun Yao,Yantao Liu,Jin Ye,Jianing Yu,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: StockBench is a contamination-free benchmark for evaluating LLM agents in realistic multi-month stock trading environments, where agents make sequential buy/sell/hold decisions based on daily market signals.


<details>
  <summary>Details</summary>
Motivation: Existing financial benchmarks primarily test static knowledge through QA but fail to capture the dynamic and iterative nature of trading, leaving the finance domain underexplored for LLM agents despite its economic importance.

Method: Introduced StockBench benchmark where LLM agents receive daily market signals (prices, fundamentals, news) and make sequential trading decisions, evaluated using financial metrics like cumulative return, maximum drawdown, and Sortino ratio.

Result: Evaluation shows most LLM agents struggle to outperform buy-and-hold baseline, but several models demonstrate potential for higher returns and better risk management, indicating that static financial knowledge doesn't necessarily translate to successful trading strategies.

Conclusion: StockBench highlights both challenges and opportunities in developing LLM-powered financial agents, and is released as open-source to support reproducibility and advance future research in this domain.

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
as autonomous agents, showing promise in reasoning, tool use, and sequential
decision-making. While prior benchmarks have evaluated LLM agents in domains
such as software engineering and scientific discovery, the finance domain
remains underexplored, despite its direct relevance to economic value and
high-stakes decision-making. Existing financial benchmarks primarily test
static knowledge through question answering, but they fall short of capturing
the dynamic and iterative nature of trading. To address this gap, we introduce
StockBench, a contamination-free benchmark designed to evaluate LLM agents in
realistic, multi-month stock trading environments. Agents receive daily market
signals -- including prices, fundamentals, and news -- and must make sequential
buy, sell, or hold decisions. Performance is assessed using financial metrics
such as cumulative return, maximum drawdown, and the Sortino ratio. Our
evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and
open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM
agents struggle to outperform the simple buy-and-hold baseline, several models
demonstrate the potential to deliver higher returns and manage risk more
effectively. These findings highlight both the challenges and opportunities in
developing LLM-powered financial agents, showing that excelling at static
financial knowledge tasks does not necessarily translate into successful
trading strategies. We release StockBench as an open-source resource to support
reproducibility and advance future research in this domain.

</details>


### [211] [Flatness-Aware Stochastic Gradient Langevin Dynamics](https://arxiv.org/abs/2510.02174)
*Stefano Bruno,Youngsik Hwang,Jaehyeon An,Sotirios Sabanis,Dong-Young Lim*

Main category: cs.LG

TL;DR: fSGLD is a novel optimization method that combines Stochastic Gradient Langevin Dynamics with Random Weight Perturbation to efficiently find flat minima in deep learning, providing theoretical guarantees and empirical improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Classical SGLD lacks mechanisms to bias optimization toward flat minima, which are crucial for generalization in deep learning. The authors aim to develop a method that can provably seek flat minima while maintaining computational efficiency.

Method: fSGLD uses stochastic gradients evaluated at parameters perturbed by isotropic Gaussian noise (Random Weight Perturbation), optimizing a randomized-smoothing objective that captures curvature information. It properly couples inverse temperature and perturbation scale parameters.

Result: Theoretical analysis shows fSGLD's invariant measure concentrates on global minimizers of a Hessian-trace regularized loss. Experiments on noisy-label and large-scale vision tasks demonstrate superior generalization and robustness compared to baselines, with computational cost similar to SGD (about half of SAM). Hessian-spectrum analysis confirms convergence to flatter minima.

Conclusion: fSGLD provides a theoretically grounded and computationally efficient approach to finding flat minima, explaining the benefits of random weight perturbation and achieving strong empirical performance across various deep learning scenarios.

Abstract: Generalization in deep learning is closely tied to the pursuit of flat minima
in the loss landscape, yet classical Stochastic Gradient Langevin Dynamics
(SGLD) offers no mechanism to bias its dynamics toward such low-curvature
solutions. This work introduces Flatness-Aware Stochastic Gradient Langevin
Dynamics (fSGLD), designed to efficiently and provably seek flat minima in
high-dimensional nonconvex optimization problems. At each iteration, fSGLD uses
the stochastic gradient evaluated at parameters perturbed by isotropic Gaussian
noise, commonly referred to as Random Weight Perturbation (RWP), thereby
optimizing a randomized-smoothing objective that implicitly captures curvature
information. Leveraging these properties, we prove that the invariant measure
of fSGLD stays close to a stationary measure concentrated on the global
minimizers of a loss function regularized by the Hessian trace whenever the
inverse temperature and the scale of random weight perturbation are properly
coupled. This result provides a rigorous theoretical explanation for the
benefits of random weight perturbation. In particular, we establish
non-asymptotic convergence guarantees in Wasserstein distance with the best
known rate and derive an excess-risk bound for the Hessian-trace regularized
objective. Extensive experiments on noisy-label and large-scale vision tasks,
in both training-from-scratch and fine-tuning settings, demonstrate that fSGLD
achieves superior or comparable generalization and robustness to baseline
algorithms while maintaining the computational cost of SGD, about half that of
SAM. Hessian-spectrum analysis further confirms that fSGLD converges to
significantly flatter minima.

</details>


### [212] [GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning](https://arxiv.org/abs/2510.02180)
*Silvia Sapora,Devon Hjelm,Alexander Toshev,Omar Attia,Bogdan Mazoure*

Main category: cs.LG

TL;DR: GRACE uses LLMs in evolutionary search to generate interpretable code-based reward functions from expert demonstrations, outperforming traditional IRL and imitation learning methods.


<details>
  <summary>Details</summary>
Motivation: Traditional Inverse Reinforcement Learning produces black-box reward models that are hard to interpret and debug, limiting practical applications.

Method: GRACE employs Large Language Models within evolutionary search to reverse-engineer executable code-based reward functions directly from expert trajectories.

Result: GRACE efficiently learns highly accurate rewards on BabyAI and AndroidWorld benchmarks, even in complex multi-task settings, and produces rewards that lead to strong policies comparable to ground-truth rewards.

Conclusion: GRACE successfully generates interpretable, code-based reward functions that can be inspected and verified, demonstrating effectiveness in complex multi-task environments and building complex reward APIs.

Abstract: Inverse Reinforcement Learning aims to recover reward models from expert
demonstrations, but traditional methods yield "black-box" models that are
difficult to interpret and debug. In this work, we introduce GRACE (Generating
Rewards As CodE), a method for using Large Language Models within an
evolutionary search to reverse-engineer an interpretable, code-based reward
function directly from expert trajectories. The resulting reward function is
executable code that can be inspected and verified. We empirically validate
GRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns
highly accurate rewards, even in complex, multi-task settings. Further, we
demonstrate that the resulting reward leads to strong policies, compared to
both competitive Imitation Learning and online RL approaches with ground-truth
rewards. Finally, we show that GRACE is able to build complex reward APIs in
multi-task setups.

</details>


### [213] [ExGRPO: Learning to Reason from Experience](https://arxiv.org/abs/2510.02245)
*Runzhe Zhan,Yafu Li,Zhi Wang,Xiaoye Qu,Dongrui Liu,Jing Shao,Derek F. Wong,Yu Cheng*

Main category: cs.LG

TL;DR: ExGRPO is a new RLVR framework that identifies rollout correctness and entropy as key indicators of valuable reasoning experiences, organizes and prioritizes these experiences, and uses a mixed-policy objective to balance exploration with experience exploitation, achieving significant performance gains over on-policy methods.


<details>
  <summary>Details</summary>
Motivation: Standard on-policy RLVR training discards rollout experiences after single use, leading to computational inefficiency and instability. The role of experience characteristics in shaping learning dynamics of large reasoning models remains underexplored.

Method: Proposed ExGRPO framework that: 1) identifies rollout correctness and entropy as effective indicators of experience value, 2) organizes and prioritizes valuable experiences, and 3) employs mixed-policy objective to balance exploration with experience exploitation.

Result: Experiments on five backbone models (1.5B-8B parameters) show consistent reasoning performance improvements on mathematical/general benchmarks, with average gains of +3.5/7.6 points over on-policy RLVR. Also stabilizes training on both stronger and weaker models where on-policy methods fail.

Conclusion: Principled experience management is a key ingredient for efficient and scalable RLVR. ExGRPO demonstrates that properly organizing and prioritizing valuable reasoning experiences can significantly improve performance and training stability.

Abstract: Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm
for improving the reasoning ability of large language models. However, standard
on-policy training discards rollout experiences after a single update, leading
to computational inefficiency and instability. While prior work on RL has
highlighted the benefits of reusing past experience, the role of experience
characteristics in shaping learning dynamics of large reasoning models remains
underexplored. In this paper, we are the first to investigate what makes a
reasoning experience valuable and identify rollout correctness and entropy as
effective indicators of experience value. Based on these insights, we propose
ExGRPO (Experiential Group Relative Policy Optimization), a framework that
organizes and prioritizes valuable experiences, and employs a mixed-policy
objective to balance exploration with experience exploitation. Experiments on
five backbone models (1.5B-8B parameters) show that ExGRPO consistently
improves reasoning performance on mathematical/general benchmarks, with an
average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO
stabilizes training on both stronger and weaker models where on-policy methods
fail. These results highlight principled experience management as a key
ingredient for efficient and scalable RLVR.

</details>


### [214] [Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet Challenge 2025](https://arxiv.org/abs/2510.02202)
*Matthew A. Reyna,Zuzana Koscova,Jan Pavlus,Soheil Saghafi,James Weigle,Andoni Elola,Salman Seyedi,Kiersten Campbell,Qiao Li,Ali Bahrami Rad,AntÃ´nio H. Ribeiro,Antonio Luiz P. Ribeiro,Reza Sameni,Gari D. Clifford*

Main category: cs.LG

TL;DR: The paper describes the George B. Moody PhysioNet Challenge 2025 focused on developing algorithms to identify Chagas disease from ECGs, using multiple datasets with weak and strong labels, data augmentation, and a triage-based evaluation metric.


<details>
  <summary>Details</summary>
Motivation: Chagas disease is a parasitic infection causing cardiovascular and digestive problems, with limited serological testing capacity. Since Chagas cardiomyopathy often manifests in ECGs, there's an opportunity to prioritize patients for testing and treatment using ECG analysis.

Method: The Challenge provided multiple datasets with labels from patient reports and serological testing, including a large dataset with weak labels and smaller datasets with strong labels. Data augmentation was used to support model robustness and generalizability. An evaluation metric was applied that captured local serological testing capacity, framing the problem as a triage task.

Result: Over 630 participants from 111 teams submitted more than 1300 entries during the Challenge, representing diverse approaches from academia and industry worldwide.

Conclusion: The Challenge successfully developed algorithmic approaches for identifying Chagas disease from ECGs, leveraging multiple datasets and data augmentation techniques to create robust models for prioritizing patients for testing and treatment.

Abstract: Objective: Chagas disease is a parasitic infection that is endemic to South
America, Central America, and, more recently, the U.S., primarily transmitted
by insects. Chronic Chagas disease can cause cardiovascular diseases and
digestive problems. Serological testing capacities for Chagas disease are
limited, but Chagas cardiomyopathy often manifests in ECGs, providing an
opportunity to prioritize patients for testing and treatment. Approach: The
George B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic
approaches for identifying Chagas disease from electrocardiograms (ECGs). Main
results: This Challenge provides multiple innovations. First, we leveraged
several datasets with labels from patient reports and serological testing,
provided a large dataset with weak labels and smaller datasets with strong
labels. Second, we augmented the data to support model robustness and
generalizability to unseen data sources. Third, we applied an evaluation metric
that captured the local serological testing capacity for Chagas disease to
frame the machine learning problem as a triage task. Significance: Over 630
participants from 111 teams submitted over 1300 entries during the Challenge,
representing diverse approaches from academia and industry worldwide.

</details>


### [215] [Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling](https://arxiv.org/abs/2510.02206)
*Daniel Gallo FernÃ¡ndez*

Main category: cs.LG

TL;DR: Poolformer replaces self-attention with recurrent layers and pooling operations to handle long sequences efficiently, outperforming state-of-the-art models on raw audio tasks.


<details>
  <summary>Details</summary>
Motivation: Self-attention scales quadratically with sequence length, limiting practicality for very long sequences. Need efficient alternative for sequence-to-sequence models.

Method: Uses recurrent layers instead of self-attention, incorporates pooling operations to reduce sequence length. Architecture defined recursively using SkipBlocks with residual blocks, down-pooling, nested SkipBlock, up-pooling, and additional residual blocks.

Result: Pooling accelerates training, improves perceptual metrics (FID and IS), prevents overfitting. Outperforms SaShiMi and Mamba on raw audio tasks. Deep layers handle long-range dependencies, shallow layers handle short-term features.

Conclusion: Poolformer effectively handles long sequences and shows promise for future applications in text, vision, and multi-modal scenarios where Poolformer-based LLMs could process dense representations of images and videos.

Abstract: Sequence-to-sequence models have become central in Artificial Intelligence,
particularly following the introduction of the transformer architecture. While
initially developed for Natural Language Processing, these models have
demonstrated utility across domains, including Computer Vision. Such models
require mechanisms to exchange information along the time dimension, typically
using recurrent or self-attention layers. However, self-attention scales
quadratically with sequence length, limiting its practicality for very long
sequences.
  We introduce Poolformer, a sequence-to-sequence model that replaces
self-attention with recurrent layers and incorporates pooling operations to
reduce sequence length. Poolformer is defined recursively using SkipBlocks,
which contain residual blocks, a down-pooling layer, a nested SkipBlock, an
up-pooling layer, and additional residual blocks. We conduct extensive
experiments to support our architectural choices.
  Our results show that pooling greatly accelerates training, improves
perceptual metrics (FID and IS), and prevents overfitting. Our experiments also
suggest that long-range dependencies are handled by deep layers, while shallow
layers take care of short-term features.
  Evaluated on raw audio, which naturally features long sequence lengths,
Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba.
Future directions include applications to text and vision, as well as
multi-modal scenarios, where a Poolformer-based LLM could effectively process
dense representations of images and videos.

</details>


### [216] [Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks](https://arxiv.org/abs/2510.02286)
*Ruohao Guo,Afshin Oroojlooy,Roshan Sridhar,Miguel Ballesteros,Alan Ritter,Dan Roth*

Main category: cs.LG

TL;DR: DialTree-RPO is an RL framework with tree search that autonomously discovers diverse multi-turn attack strategies against LLMs, achieving 25.9% higher attack success rate than previous methods.


<details>
  <summary>Details</summary>
Motivation: Current LLMs remain vulnerable to multi-turn adversarial attacks, but existing methods either rely on manual red-teaming or automated single-turn attacks, failing to explore the vast space of possible multi-turn attack trajectories emerging from complex dialogue dynamics.

Method: An on-policy reinforcement learning framework integrated with tree search that treats dialogue as sequential decision-making problem, enabling systematic exploration without manually curated data.

Result: Achieves more than 25.9% higher attack success rate across 10 target models compared to previous state-of-the-art approaches, and effectively uncovers new attack strategies.

Conclusion: The framework successfully discovers optimal dialogue policies that maximize attack success across multiple turns, demonstrating superior performance in identifying LLM vulnerabilities in multi-turn interaction settings.

Abstract: Despite recent rapid progress in AI safety, current large language models
remain vulnerable to adversarial attacks in multi-turn interaction settings,
where attackers strategically adapt their prompts across conversation turns and
pose a more critical yet realistic challenge. Existing approaches that discover
safety vulnerabilities either rely on manual red-teaming with human experts or
employ automated methods using pre-defined templates and human-curated attack
data, with most focusing on single-turn attacks. However, these methods did not
explore the vast space of possible multi-turn attacks, failing to consider
novel attack trajectories that emerge from complex dialogue dynamics and
strategic conversation planning. This gap is particularly critical given recent
findings that LLMs exhibit significantly higher vulnerability to multi-turn
attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy
reinforcement learning framework integrated with tree search that autonomously
discovers diverse multi-turn attack strategies by treating the dialogue as a
sequential decision-making problem, enabling systematic exploration without
manually curated data. Through extensive experiments, our approach not only
achieves more than 25.9% higher ASR across 10 target models compared to
previous state-of-the-art approaches, but also effectively uncovers new attack
strategies by learning optimal dialogue policies that maximize attack success
across multiple turns.

</details>


### [217] [DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning](https://arxiv.org/abs/2510.02212)
*Hanyang Zhao,Dawen Liang,Wenpin Tang,David Yao,Nathan Kallus*

Main category: cs.LG

TL;DR: DiFFPO is a unified RL framework for training masked diffusion LLMs to improve reasoning quality and speed by training surrogate policies and joint samplers.


<details>
  <summary>Details</summary>
Motivation: To enhance both the reasoning capability (furious) and inference speed (fast) of diffusion large language models through reinforcement learning, addressing the trade-off between performance and computational efficiency.

Method: 1. Train surrogate policies via off-policy RL with tractable likelihood approximation; 2. Use two-stage likelihood approximation with importance sampling; 3. Jointly train efficient samplers/controllers to adaptively allocate inference thresholds per prompt.

Result: Achieves better sample efficiency, superior task performance, and improved Pareto frontier of inference-time compute with lower number of function evaluations while maintaining high accuracy.

Conclusion: DiFFPO effectively improves both reasoning quality and speed of diffusion LLMs through unified RL training of policies and samplers, demonstrating strong performance on math and planning tasks.

Abstract: We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified
framework for training masked diffusion large language models (dLLMs) to reason
not only better (furious), but also faster via reinforcement learning (RL). We
first unify the existing baseline approach such as d1 by proposing to train
surrogate policies via off-policy RL, whose likelihood is much more tractable
as an approximation to the true dLLM policy. This naturally motivates a more
accurate and informative two-stage likelihood approximation combined with
importance sampling correction, which leads to generalized RL algorithms with
better sample efficiency and superior task performance. Second, we propose a
new direction of joint training efficient samplers/controllers of dLLMs policy.
Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by
letting the model learn to adaptively allocate an inference threshold for each
prompt. By jointly training the sampler, we yield better accuracies with lower
number of function evaluations (NFEs) compared to training the model only,
obtaining the best performance in improving the Pareto frontier of the
inference-time compute of dLLMs. We showcase the effectiveness of our pipeline
by training open source large diffusion language models over benchmark math and
planning tasks.

</details>


### [218] [Interactive Training: Feedback-Driven Neural Network Optimization](https://arxiv.org/abs/2510.02297)
*Wentao Zhang,Yang Young Lu,Yuntian Deng*

Main category: cs.LG

TL;DR: Interactive Training is a framework enabling real-time human/AI intervention in neural network training through a control server, allowing dynamic adjustment of hyperparameters, data, and checkpoints to improve stability and adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional neural network training follows fixed recipes that lack flexibility to respond to instabilities or emerging issues during training.

Method: Uses a control server to mediate communication between users/agents and training process, enabling dynamic adjustment of optimizer hyperparameters, training data, and model checkpoints.

Result: Achieves superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving user needs through three case studies.

Conclusion: Paves the way for future training paradigm where AI agents autonomously monitor training logs, proactively resolve instabilities, and optimize training dynamics.

Abstract: Traditional neural network training typically follows fixed, predefined
optimization recipes, lacking the flexibility to dynamically respond to
instabilities or emerging training issues. In this paper, we introduce
Interactive Training, an open-source framework that enables real-time,
feedback-driven intervention during neural network training by human experts or
automated AI agents. At its core, Interactive Training uses a control server to
mediate communication between users or agents and the ongoing training process,
allowing users to dynamically adjust optimizer hyperparameters, training data,
and model checkpoints. Through three case studies, we demonstrate that
Interactive Training achieves superior training stability, reduced sensitivity
to initial hyperparameters, and improved adaptability to evolving user needs,
paving the way toward a future training paradigm where AI agents autonomously
monitor training logs, proactively resolve instabilities, and optimize training
dynamics.

</details>


### [219] [C2AL: Cohort-Contrastive Auxiliary Learning for Large-scale Recommendation Systems](https://arxiv.org/abs/2510.02215)
*Mertcan Cokbas,Ziteng Liu,Zeyi Tao,Chengkai Zhang,Elder Veliz,Qin Huang,Ellie Wen,Huayu Li,Qiang Jin,Murat Duman,Benjamin Au,Guy Lebanon,Sagar Chordia*

Main category: cs.LG

TL;DR: Proposes C2AL method to address data heterogeneity in recommendation models by using attention mechanisms for shared embedding selection and auxiliary learning with conflicting labels to preserve minority cohort information.


<details>
  <summary>Details</summary>
Motivation: Large-scale recommendation models trained under single global objective assume data homogeneity but real-world data contains heterogeneous cohorts, leading to models dominated by central patterns while neglecting head and tail regions, limiting learning ability and causing inactive attention weights or dead neurons.

Method: Leverages attention mechanism in factorization machines for shared embedding selection, analyzes dataset substructures, and uses partially conflicting auxiliary labels to regularize shared representation, customizing attention layer learning to preserve mutual information with minority cohorts.

Result: Evaluated on massive production datasets with billions of data points for six SOTA models, achieving up to 0.16% reduction in normalized entropy overall and gains exceeding 0.30% on targeted minority cohorts.

Conclusion: The proposed C2AL method effectively addresses data heterogeneity in recommendation systems by using attention-based factorization machines and auxiliary learning, improving both global performance and minority cohort representation.

Abstract: Training large-scale recommendation models under a single global objective
implicitly assumes homogeneity across user populations. However, real-world
data are composites of heterogeneous cohorts with distinct conditional
distributions. As models increase in scale and complexity and as more data is
used for training, they become dominated by central distribution patterns,
neglecting head and tail regions. This imbalance limits the model's learning
ability and can result in inactive attention weights or dead neurons. In this
paper, we reveal how the attention mechanism can play a key role in
factorization machines for shared embedding selection, and propose to address
this challenge by analyzing the substructures in the dataset and exposing those
with strong distributional contrast through auxiliary learning. Unlike previous
research, which heuristically applies weighted labels or multi-task heads to
mitigate such biases, we leverage partially conflicting auxiliary labels to
regularize the shared representation. This approach customizes the learning
process of attention layers to preserve mutual information with minority
cohorts while improving global performance. We evaluated C2AL on massive
production datasets with billions of data points each for six SOTA models.
Experiments show that the factorization machine is able to capture fine-grained
user-ad interactions using the proposed method, achieving up to a 0.16%
reduction in normalized entropy overall and delivering gains exceeding 0.30% on
targeted minority cohorts.

</details>


### [220] [Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification](https://arxiv.org/abs/2510.02216)
*Zeqi Ye,Minshuo Chen*

Main category: cs.LG

TL;DR: This paper provides theoretical analysis of diffusion-based time-series imputation methods, deriving statistical bounds and uncertainty quantification for missing values using conditional diffusion transformers.


<details>
  <summary>Details</summary>
Motivation: Despite empirical success of diffusion-based imputation methods, there's limited theoretical understanding of how well they capture complex spatio-temporal dependencies between missing and observed values.

Method: The authors derive statistical sample complexity bounds using novel approximation theory for conditional score functions with transformers, construct confidence regions for missing values, and propose mixed-masking training strategy.

Result: The study reveals that imputation efficiency and accuracy are significantly influenced by missing patterns, and validates theoretical insights through simulations.

Conclusion: The work bridges the gap between empirical success and theoretical understanding of diffusion-based imputation, providing statistical guarantees and practical improvements through mixed-masking training.

Abstract: Imputation methods play a critical role in enhancing the quality of practical
time-series data, which often suffer from pervasive missing values. Recently,
diffusion-based generative imputation methods have demonstrated remarkable
success compared to autoregressive and conventional statistical approaches.
Despite their empirical success, the theoretical understanding of how well
diffusion-based models capture complex spatial and temporal dependencies
between the missing values and observed ones remains limited. Our work
addresses this gap by investigating the statistical efficiency of conditional
diffusion transformers for imputation and quantifying the uncertainty in
missing values. Specifically, we derive statistical sample complexity bounds
based on a novel approximation theory for conditional score functions using
transformers, and, through this, construct tight confidence regions for missing
values. Our findings also reveal that the efficiency and accuracy of imputation
are significantly influenced by the missing patterns. Furthermore, we validate
these theoretical insights through simulation and propose a mixed-masking
training strategy to enhance the imputation performance.

</details>


### [221] [Efficiently Generating Correlated Sample Paths from Multi-step Time Series Foundation Models](https://arxiv.org/abs/2510.02224)
*Ethan Baron,Boris Oreshkin,Ruijun Ma,Hanyu Zhang,Kari Torkkola,Michael W. Mahoney,Andrew Gordon Wilson,Tatiana Konstantinova*

Main category: cs.LG

TL;DR: A copula-based method for efficient generation of correlated sample paths from multi-step time series foundation models, avoiding expensive autoregressive sampling.


<details>
  <summary>Details</summary>
Motivation: Existing time series foundation models only predict independent marginal distributions per time step, lacking joint predictive distributions needed for realistic sample paths with correlation structures.

Method: Uses copula-based approach to generate correlated sample paths in one forward pass from existing multi-step time series foundation models.

Result: Generates sample paths orders of magnitude faster than autoregressive sampling and improves quality by mitigating snowballing error.

Conclusion: Copula-based approach enables efficient and accurate generation of correlated forecast trajectories from time series foundation models.

Abstract: Many time series applications require access to multi-step forecast
trajectories in the form of sample paths. Recently, time series foundation
models have leveraged multi-step lookahead predictions to improve the quality
and efficiency of multi-step forecasts. However, these models only predict
independent marginal distributions for each time step, rather than a full joint
predictive distribution. To generate forecast sample paths with realistic
correlation structures, one typically resorts to autoregressive sampling, which
can be extremely expensive. In this paper, we present a copula-based approach
to efficiently generate accurate, correlated sample paths from existing
multi-step time series foundation models in one forward pass. Our copula-based
approach generates correlated sample paths orders of magnitude faster than
autoregressive sampling, and it yields improved sample path quality by
mitigating the snowballing error phenomenon.

</details>


### [222] [xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity](https://arxiv.org/abs/2510.02228)
*Maximilian Beck,Kajetan Schweighofer,Sebastian BÃ¶ck,Sebastian Lehner,Sepp Hochreiter*

Main category: cs.LG

TL;DR: Comparative scaling analysis between Transformers and xLSTM architectures reveals xLSTM scales more favorably, especially with longer training and inference contexts.


<details>
  <summary>Details</summary>
Motivation: While Transformers dominate LLMs, xLSTM offers linear complexity with context length and remains competitive at billion-parameter scale, warranting systematic comparison of scaling behavior.

Method: Three-pronged approach: 1) Scaling analysis in compute-optimal and over-training regimes using IsoFLOP and parametric fits on 80M-7B models with 2B-2T tokens; 2) Study optimal model size dependence on context length; 3) Inference-time scaling analysis.

Result: xLSTM scales favorably compared to Transformers in typical LLM scenarios, with advantages widening as training and inference contexts increase.

Conclusion: xLSTM's superior scaling characteristics, particularly with longer contexts, provide valuable insights for future model design and deployment decisions.

Abstract: Scaling laws play a central role in the success of Large Language Models
(LLMs), enabling the prediction of model performance relative to compute
budgets prior to training. While Transformers have been the dominant
architecture, recent alternatives such as xLSTM offer linear complexity with
respect to context length while remaining competitive in the billion-parameter
regime. We conduct a comparative investigation on the scaling behavior of
Transformers and xLSTM along the following lines, providing insights to guide
future model design and deployment. First, we study the scaling behavior for
xLSTM in compute-optimal and over-training regimes using both IsoFLOP and
parametric fit approaches on a wide range of model sizes (80M-7B) and number of
training tokens (2B-2T). Second, we examine the dependence of optimal model
sizes on context length, a pivotal aspect that was largely ignored in previous
work. Finally, we analyze inference-time scaling characteristics. Our findings
reveal that in typical LLM training and inference scenarios, xLSTM scales
favorably compared to Transformers. Importantly, xLSTM's advantage widens as
training and inference contexts grow.

</details>


### [223] [PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks](https://arxiv.org/abs/2510.02236)
*Ricardo Misael Ayala Molina,Hyame Assem Alameddine,Makan Pourzandi,Chadi Assi*

Main category: cs.LG

TL;DR: PUL-Inter-Slice Defender is an anomaly detection solution using Positive Unlabeled Learning with LSTM Autoencoders and K-Means to protect 5G networks against Distributed Slice Mobility DDoS attacks, achieving over 98.50% F1-score.


<details>
  <summary>Details</summary>
Motivation: 5G networks' Inter-Slice Switching capability introduces vulnerability to Distributed Slice Mobility attacks, a form of DDoS attack that exploits the ability to switch between network slices.

Method: Uses Positive Unlabeled Learning combining Long Short-Term Memory Autoencoders and K-Means clustering, leveraging 3GPP KPIs and performance measurement counters as features for machine learning models.

Result: Achieved F1-scores exceeding 98.50% on training datasets with 10% to 40% attack contamination, outperforming Inter-Slice Defender and other PUL-based solutions using OCSVM with Random Forest and XGBoost.

Conclusion: PUL-Inter-Slice Defender effectively detects DSM attack variants while maintaining robustness with contaminated training data, providing strong protection for 5G network slices.

Abstract: Network Slices (NSs) are virtual networks operating over a shared physical
infrastructure, each designed to meet specific application requirements while
maintaining consistent Quality of Service (QoS). In Fifth Generation (5G)
networks, User Equipment (UE) can connect to and seamlessly switch between
multiple NSs to access diverse services. However, this flexibility, known as
Inter-Slice Switching (ISS), introduces a potential vulnerability that can be
exploited to launch Distributed Slice Mobility (DSM) attacks, a form of
Distributed Denial of Service (DDoS) attack. To secure 5G networks and their
NSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an
anomaly detection solution that leverages Positive Unlabeled Learning (PUL) and
incorporates a combination of Long Short-Term Memory Autoencoders and K-Means
clustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership
Project (3GPP) key performance indicators and performance measurement counters
as features for its machine learning models to detect DSM attack variants while
maintaining robustness in the presence of contaminated training data. When
evaluated on data collected from our 5G testbed based on the open-source
free5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;
PUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training
datasets with 10% to 40% attack contamination, consistently outperforming its
counterpart Inter-Slice Defender and other PUL based solutions combining
One-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.

</details>


### [224] [Drop-Muon: Update Less, Converge Faster](https://arxiv.org/abs/2510.02239)
*Kaja Gruntkowska,Yassine Maziane,Zheng Qu,Peter RichtÃ¡rik*

Main category: cs.LG

TL;DR: Drop-Muon challenges conventional full-network updates by introducing randomized progressive training that updates only a subset of layers per step, achieving faster convergence with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To challenge the conventional wisdom that all layers should be updated at every optimization step, showing that full-network updates can be fundamentally suboptimal.

Method: Non-Euclidean Randomized Progressive Training (Drop-Muon) that updates only a subset of layers per step using a randomized schedule, combining progressive training efficiency with layer-specific non-Euclidean updates.

Result: Drop-Muon consistently outperforms full-network Muon, achieving same accuracy up to 1.4x faster in wall-clock time, with rigorous convergence guarantees under various smoothness conditions.

Conclusion: Full-network updates are not optimal unless specific layer smoothness relationships hold, suggesting a shift toward more efficient progressive training methods for large-scale models.

Abstract: Conventional wisdom in deep learning optimization dictates updating all
layers at every step-a principle followed by all recent state-of-the-art
optimizers such as Muon. In this work, we challenge this assumption, showing
that full-network updates can be fundamentally suboptimal, both in theory and
in practice. We introduce a non-Euclidean Randomized Progressive Training
method-Drop-Muon-a simple yet powerful framework that updates only a subset of
layers per step according to a randomized schedule, combining the efficiency of
progressive training with layer-specific non-Euclidean updates for top-tier
performance. We provide rigorous convergence guarantees under both layer-wise
smoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic and
stochastic gradient settings, marking the first such results for progressive
training in the stochastic and non-smooth regime. Our cost analysis further
reveals that full-network updates are not optimal unless a very specific
relationship between layer smoothness constants holds. Through controlled CNN
experiments, we empirically demonstrate that Drop-Muon consistently outperforms
full-network Muon, achieving the same accuracy up to $1.4\times$ faster in
wall-clock time. Together, our results suggest a shift in how large-scale
models can be efficiently trained, challenging the status quo and offering a
highly efficient, theoretically grounded alternative to full-network updates.

</details>


### [225] [Transformers Discover Molecular Structure Without Graph Priors](https://arxiv.org/abs/2510.02259)
*Tobias Kreiman,Yutong Bai,Fadi Atieh,Elizabeth Weaver,Eric Qu,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: Transformers trained directly on Cartesian coordinates can achieve competitive molecular energy and force predictions without predefined graphs or physical priors, challenging the necessity of GNNs' hard-coded inductive biases.


<details>
  <summary>Details</summary>
Motivation: GNNs rely on predefined graphs with fixed receptive fields, which limits expressivity and slows inference. The authors investigate whether Transformers without such biases can effectively model molecular energies and forces.

Method: Train pure Transformers directly on Cartesian coordinates without predefined graphs or physical priors, comparing performance against state-of-the-art equivariant GNNs under matched compute budgets on the OMol25 dataset.

Result: Transformers achieve competitive energy and force mean absolute errors, learn physically consistent patterns (e.g., attention weights decaying inversely with distance), and show predictable scaling improvements consistent with empirical scaling laws.

Conclusion: Many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and suggesting standardized, scalable architectures for molecular modeling.

Abstract: Graph Neural Networks (GNNs) are the dominant architecture for molecular
machine learning, particularly for molecular property prediction and machine
learning interatomic potentials (MLIPs). GNNs perform message passing on
predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor
scheme. While this design aligns with the locality present in many molecular
tasks, a hard-coded graph can limit expressivity due to the fixed receptive
field and slows down inference with sparse graph operations. In this work, we
investigate whether pure, unmodified Transformers trained directly on Cartesian
coordinates$\unicode{x2013}$without predefined graphs or physical
priors$\unicode{x2013}$can approximate molecular energies and forces. As a
starting point for our analysis, we demonstrate how to train a Transformer to
competitive energy and force mean absolute errors under a matched training
compute budget, relative to a state-of-the-art equivariant GNN on the OMol25
dataset. We discover that the Transformer learns physically consistent
patterns$\unicode{x2013}$such as attention weights that decay inversely with
interatomic distance$\unicode{x2013}$and flexibly adapts them across different
molecular environments due to the absence of hard-coded biases. The use of a
standard Transformer also unlocks predictable improvements with respect to
scaling training resources, consistent with empirical scaling laws observed in
other domains. Our results demonstrate that many favorable properties of GNNs
can emerge adaptively in Transformers, challenging the necessity of hard-coded
graph inductive biases and pointing toward standardized, scalable architectures
for molecular modeling.

</details>


### [226] [How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning](https://arxiv.org/abs/2510.02265)
*Yalin E. Sagduyu,Tugba Erpek,Kemal Davaslioglu,Sastry Kompella*

Main category: cs.LG

TL;DR: This paper uses reinforcement learning (Q-learning and DQN) to help transmitter-receiver pairs avoid reactive jamming by adapting transmit power, modulation, and channel selection, achieving high throughput despite dynamic jamming policies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reactive jamming where jammers dynamically select channels and sensing thresholds to detect and jam transmissions, requiring adaptive countermeasures without prior knowledge of channel conditions or jamming strategies.

Method: Employed reinforcement learning (RL) with Q-learning for discrete jamming-event states and Deep Q-Networks (DQN) for continuous states based on received power. The approach adapts transmit power, modulation, and channel selection through different reward functions and action sets.

Result: The RL-based approach can rapidly adapt to spectrum dynamics and sustain high transmission rates even as channels and jamming policies change over time.

Conclusion: Reinforcement learning provides an effective solution for mitigating reactive jamming by enabling transmitter-receiver pairs to learn and adapt their transmission strategies without prior knowledge of the environment or jamming behavior.

Abstract: This paper studies the problem of mitigating reactive jamming, where a jammer
adopts a dynamic policy of selecting channels and sensing thresholds to detect
and jam ongoing transmissions. The transmitter-receiver pair learns to avoid
jamming and optimize throughput over time (without prior knowledge of channel
conditions or jamming strategies) by using reinforcement learning (RL) to adapt
transmit power, modulation, and channel selection. Q-learning is employed for
discrete jamming-event states, while Deep Q-Networks (DQN) are employed for
continuous states based on received power. Through different reward functions
and action sets, the results show that RL can adapt rapidly to spectrum
dynamics and sustain high rates as channels and jamming policies change over
time.

</details>


### [227] [Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps](https://arxiv.org/abs/2510.02274)
*Kyoungjun Park,Yifan Yang,Changhan Ge,Lili Qiu,Shiqi Jiang*

Main category: cs.LG

TL;DR: Diffusion^2 is a diffusion-based method using 3D point clouds to model RF signal propagation across various frequencies, achieving 1.9 dB error and 27x speedup over existing methods.


<details>
  <summary>Details</summary>
Motivation: RF signal propagation modeling provides environmental insights beyond RGB cameras, supporting wireless applications, but accurate prediction in complex environments is challenging due to signal-obstacle interactions.

Method: Uses diffusion-based approach with 3D point clouds and RF-3D Encoder to capture RF-related features, applying multi-scale embedding to simulate RF signal dissemination.

Result: Achieves accurate RF signal behavior estimation across frequency bands and environmental conditions with 1.9 dB error margin and 27x faster than existing methods.

Conclusion: Diffusion^2 represents significant advancement in RF signal propagation modeling, enabling accurate and efficient prediction across diverse frequencies and environments.

Abstract: Modeling radio frequency (RF) signal propagation is essential for
understanding the environment, as RF signals offer valuable insights beyond the
capabilities of RGB cameras, which are limited by the visible-light spectrum,
lens coverage, and occlusions. It is also useful for supporting wireless
diagnosis, deployment, and optimization. However, accurately predicting RF
signals in complex environments remains a challenge due to interactions with
obstacles such as absorption and reflection. We introduce Diffusion^2, a
diffusion-based approach that uses 3D point clouds to model the propagation of
RF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.
To effectively capture RF-related features from 3D data, we present the RF-3D
Encoder, which encapsulates the complexities of 3D geometry along with
signal-specific details. These features undergo multi-scale embedding to
simulate the actual RF signal dissemination process. Our evaluation, based on
synthetic and real-world measurements, demonstrates that Diffusion^2 accurately
estimates the behavior of RF signals in various frequency bands and
environmental conditions, with an error margin of just 1.9 dB and 27x faster
than existing methods, marking a significant advancement in the field. Refer to
https://rfvision-project.github.io/ for more information.

</details>


### [228] [Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks](https://arxiv.org/abs/2510.02278)
*Fedor Velikonivtsev,Oleg Platonov,Gleb Bazhenov,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: This paper introduces new large-scale traffic forecasting datasets for city road networks and proposes a scalable GNN approach without dedicated temporal modules.


<details>
  <summary>Details</summary>
Motivation: Current traffic forecasting benchmarks have limitations including missing road connectivity information, limited road properties, small scale (insufficient road segments), and focus on intercity highways rather than complex urban networks.

Method: The authors release datasets for two major cities with up to 100,000 road segments, and propose a GNN approach without dedicated temporal sequence processing modules for better scalability.

Result: The datasets provide rich road features and fine-grained traffic volume/speed data. Most current neural spatiotemporal models struggle to scale to datasets of this size, while the proposed approach achieves better scalability and stronger forecasting performance.

Conclusion: The new datasets and modeling approach serve as a valuable resource for traffic forecasting research, addressing scalability issues and providing more realistic urban traffic scenarios.

Abstract: Traffic forecasting on road networks is a complex task of significant
practical importance that has recently attracted considerable attention from
the machine learning community, with spatiotemporal graph neural networks
(GNNs) becoming the most popular approach. The proper evaluation of traffic
forecasting methods requires realistic datasets, but current publicly available
benchmarks have significant drawbacks, including the absence of information
about road connectivity for road graph construction, limited information about
road properties, and a relatively small number of road segments that falls
short of real-world applications. Further, current datasets mostly contain
information about intercity highways with sparsely located sensors, while city
road networks arguably present a more challenging forecasting task due to much
denser roads and more complex urban traffic patterns. In this work, we provide
a more complete, realistic, and challenging benchmark for traffic forecasting
by releasing datasets representing the road networks of two major cities, with
the largest containing almost 100,000 road segments (more than a 10-fold
increase relative to existing datasets). Our datasets contain rich road
features and provide fine-grained data about both traffic volume and traffic
speed, allowing for building more holistic traffic forecasting systems. We show
that most current implementations of neural spatiotemporal models for traffic
forecasting have problems scaling to datasets of our size. To overcome this
issue, we propose an alternative approach to neural traffic forecasting that
uses a GNN without a dedicated module for temporal sequence processing, thus
achieving much better scalability, while also demonstrating stronger
forecasting performance. We hope our datasets and modeling insights will serve
as a valuable resource for research in traffic forecasting.

</details>


### [229] [Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation](https://arxiv.org/abs/2510.02279)
*Mykyta Ielanskyi,Kajetan Schweighofer,Lukas Aichberger,Sepp Hochreiter*

Main category: cs.LG

TL;DR: The paper addresses the issue of evaluating uncertainty estimation methods for detecting LLM confabulations, proposing more robust evaluation approaches including multiple risk indicators, LLM-as-a-judge variants, and Elo rating systems.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for uncertainty estimation in NLG have substantial disagreement in correctness functions, allowing performance inflation and undermining reliable assessment of confabulation detection methods.

Method: Proposes using multiple alternative risk indicators, marginalizing over LLM-as-a-judge variants for QA tasks, exploring structured tasks with robust risk indicators, and implementing Elo rating for objective summarization.

Result: The approach reduces evaluation biases in QA tasks through multiple LLM-as-a-judge variants and provides more robust assessment across different task types including out-of-distribution and perturbation detection.

Conclusion: The proposed evaluation framework provides more robust and objective assessment of uncertainty estimation methods for detecting LLM confabulations, addressing the limitations of current benchmark practices.

Abstract: Hallucinations are a common issue that undermine the reliability of large
language models (LLMs). Recent studies have identified a specific subset of
hallucinations, known as confabulations, which arise due to predictive
uncertainty of LLMs. To detect confabulations, various methods for estimating
predictive uncertainty in natural language generation (NLG) have been
developed. These methods are typically evaluated by correlating uncertainty
estimates with the correctness of generated text, with question-answering (QA)
datasets serving as the standard benchmark. However, commonly used approximate
correctness functions have substantial disagreement between each other and,
consequently, in the ranking of the uncertainty estimation methods. This allows
one to inflate the apparent performance of uncertainty estimation methods. We
propose using several alternative risk indicators for risk correlation
experiments that improve robustness of empirical assessment of UE algorithms
for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge
variants leads to reducing the evaluation biases. Furthermore, we explore
structured tasks as well as out of distribution and perturbation detection
tasks which provide robust and controllable risk indicators. Finally, we
propose to use an Elo rating of uncertainty estimation methods to give an
objective summarization over extensive evaluation settings.

</details>


### [230] [Test-Time Anchoring for Discrete Diffusion Posterior Sampling](https://arxiv.org/abs/2510.02291)
*Litu Rout,Andreas Lugmayr,Yasamin Jafarian,Srivatsan Varadharajan,Constantine Caramanis,Sanjay Shakkottai,Ira Kemelmacher-Shlizerman*

Main category: cs.LG

TL;DR: APS introduces quantized expectation and anchored remasking for discrete diffusion posterior sampling, achieving SOTA performance in inverse problems without retraining.


<details>
  <summary>Details</summary>
Motivation: To enable posterior sampling using pretrained discrete diffusion models for image recovery from noisy measurements, overcoming limitations of existing methods like sparse guidance signals and dimensionality issues.

Method: Anchored Posterior Sampling (APS) with two key innovations: quantized expectation for gradient-like guidance in discrete embedding space, and anchored remasking for adaptive decoding.

Result: State-of-the-art performance among discrete diffusion samplers across linear and nonlinear inverse problems on standard benchmarks, with additional benefits in training-free stylization and text-guided editing.

Conclusion: APS effectively overcomes the challenges of discrete diffusion posterior sampling and demonstrates superior performance in various applications without requiring task-specific retraining.

Abstract: We study the problem of posterior sampling using pretrained discrete
diffusion foundation models, aiming to recover images from noisy measurements
without retraining task-specific models. While diffusion models have achieved
remarkable success in generative modeling, most advances rely on continuous
Gaussian diffusion. In contrast, discrete diffusion offers a unified framework
for jointly modeling categorical data such as text and images. Beyond
unification, discrete diffusion provides faster inference, finer control, and
principled training-free Bayesian inference, making it particularly well-suited
for posterior sampling. However, existing approaches to discrete diffusion
posterior sampling face severe challenges: derivative-free guidance yields
sparse signals, continuous relaxations limit applicability, and split Gibbs
samplers suffer from the curse of dimensionality. To overcome these
limitations, we introduce Anchored Posterior Sampling (APS) for masked
diffusion foundation models, built on two key innovations -- quantized
expectation for gradient-like guidance in discrete embedding space, and
anchored remasking for adaptive decoding. Our approach achieves
state-of-the-art performance among discrete diffusion samplers across linear
and nonlinear inverse problems on the standard benchmarks. We further
demonstrate the benefits of our approach in training-free stylization and
text-guided editing.

</details>


### [231] [Continual Personalization for Diffusion Models](https://arxiv.org/abs/2510.02296)
*Yu-Chien Liao,Jr-Jen Chen,Chi-Pin Huang,Ci-Siang Lin,Meng-Lin Wu,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: CNS is a novel approach for incremental personalization in diffusion models that identifies and fine-tunes concept-specific neurons to prevent catastrophic forgetting while maintaining zero-shot generation capabilities.


<details>
  <summary>Details</summary>
Motivation: Updating diffusion models incrementally is practical for real-world applications but computationally challenging, requiring methods that prevent catastrophic forgetting while enabling continual personalization.

Method: Concept Neuron Selection (CNS) identifies neurons related to target concepts and fine-tunes them incrementally while preserving knowledge from previous concepts through joint training.

Result: CNS achieves state-of-the-art performance on real-world datasets with minimal parameter adjustments, outperforming previous methods in single and multi-concept personalization, while being fusion-free to reduce memory and processing time.

Conclusion: CNS provides an effective solution for continual personalization in diffusion models by selectively updating concept-related neurons, enabling efficient incremental learning without catastrophic forgetting.

Abstract: Updating diffusion models in an incremental setting would be practical in
real-world applications yet computationally challenging. We present a novel
learning strategy of Concept Neuron Selection (CNS), a simple yet effective
approach to perform personalization in a continual learning scheme. CNS
uniquely identifies neurons in diffusion models that are closely related to the
target concepts. In order to mitigate catastrophic forgetting problems while
preserving zero-shot text-to-image generation ability, CNS finetunes concept
neurons in an incremental manner and jointly preserves knowledge learned of
previous concepts. Evaluation of real-world datasets demonstrates that CNS
achieves state-of-the-art performance with minimal parameter adjustments,
outperforming previous methods in both single and multi-concept personalization
works. CNS also achieves fusion-free operation, reducing memory storage and
processing time for continual personalization.

</details>


### [232] [Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models](https://arxiv.org/abs/2510.02300)
*Runqian Wang,Yilun Du*

Main category: cs.LG

TL;DR: Equilibrium Matching (EqM) is a generative modeling framework that learns the equilibrium gradient of an implicit energy landscape, enabling optimization-based sampling with adjustable parameters and achieving state-of-the-art performance on ImageNet.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional diffusion and flow-based models that rely on non-equilibrium, time-conditional dynamics, by adopting an equilibrium dynamics perspective.

Method: Discards time-conditional dynamics and learns the equilibrium gradient of an implicit energy landscape. Uses optimization-based sampling with gradient descent on the learned landscape, featuring adjustable step sizes, adaptive optimizers, and adaptive compute.

Result: Achieves FID of 1.90 on ImageNet 256Ã256, surpassing diffusion/flow models. Also handles tasks like partially noised image denoising, OOD detection, and image composition.

Conclusion: EqM provides a unified framework that bridges flow and energy-based models, offers theoretical justification for learning from data manifold, and enables flexible optimization-driven inference for various tasks.

Abstract: We introduce Equilibrium Matching (EqM), a generative modeling framework
built from an equilibrium dynamics perspective. EqM discards the
non-equilibrium, time-conditional dynamics in traditional diffusion and
flow-based generative models and instead learns the equilibrium gradient of an
implicit energy landscape. Through this approach, we can adopt an
optimization-based sampling process at inference time, where samples are
obtained by gradient descent on the learned landscape with adjustable step
sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation
performance of diffusion/flow models empirically, achieving an FID of 1.90 on
ImageNet 256$\times$256. EqM is also theoretically justified to learn and
sample from the data manifold. Beyond generation, EqM is a flexible framework
that naturally handles tasks including partially noised image denoising, OOD
detection, and image composition. By replacing time-conditional velocities with
a unified equilibrium landscape, EqM offers a tighter bridge between flow and
energy-based models and a simple route to optimization-driven inference.

</details>


### [233] [Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive](https://arxiv.org/abs/2510.02305)
*Tyler Farghly,Peter Potaptchik,Samuel Howard,George Deligiannidis,Jakiw Pidstrigach*

Main category: cs.LG

TL;DR: This paper provides evidence that diffusion models' success stems from their ability to adapt to low-dimensional geometric structure in data, through implicit regularization via score matching and smoothing.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind diffusion models' strong generalization capabilities, particularly testing the conjecture that they succeed by adapting to low-dimensional geometric structure in data.

Method: Investigates implicit regularization by analyzing the effect of smoothing minimizers of the empirical score matching objective, both theoretically and empirically.

Result: Confirms that smoothing the score function produces smoothing tangential to the data manifold, and shows that the generalization manifold can be controlled by choosing appropriate smoothing.

Conclusion: Provides evidence supporting the manifold hypothesis for diffusion models, demonstrating how geometric structure adaptation through score matching enables their strong performance.

Abstract: Diffusion models have achieved state-of-the-art performance, demonstrating
remarkable generalisation capabilities across diverse domains. However, the
mechanisms underpinning these strong capabilities remain only partially
understood. A leading conjecture, based on the manifold hypothesis, attributes
this success to their ability to adapt to low-dimensional geometric structure
within the data. This work provides evidence for this conjecture, focusing on
how such phenomena could result from the formulation of the learning problem
through score matching. We inspect the role of implicit regularisation by
investigating the effect of smoothing minimisers of the empirical score
matching objective. Our theoretical and empirical results confirm that
smoothing the score function -- or equivalently, smoothing in the log-density
domain -- produces smoothing tangential to the data manifold. In addition, we
show that the manifold along which the diffusion model generalises can be
controlled by choosing an appropriate smoothing.

</details>


### [234] [Knowledge Distillation Detection for Open-weights Models](https://arxiv.org/abs/2510.02302)
*Qin Shi,Amber Yijia Zheng,Qifan Song,Raymond A. Yeh*

Main category: cs.LG

TL;DR: Proposes knowledge distillation detection task to identify if a student model was distilled from a teacher, using only student weights and teacher API. Introduces model-agnostic framework with data-free input synthesis and statistical scoring.


<details>
  <summary>Details</summary>
Motivation: Address growing concerns about model provenance and unauthorized replication through distillation, ensuring proper attribution and preventing intellectual property violations.

Method: Model-agnostic framework combining data-free input synthesis and statistical score computation for detecting distillation, applicable to both classification and generative models.

Result: Improves detection accuracy over strongest baselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image generation across diverse architectures.

Conclusion: The proposed framework effectively detects knowledge distillation in practical settings where only student weights and teacher API are available, providing a solution for model provenance verification.

Abstract: We propose the task of knowledge distillation detection, which aims to
determine whether a student model has been distilled from a given teacher,
under a practical setting where only the student's weights and the teacher's
API are available. This problem is motivated by growing concerns about model
provenance and unauthorized replication through distillation. To address this
task, we introduce a model-agnostic framework that combines data-free input
synthesis and statistical score computation for detecting distillation. Our
approach is applicable to both classification and generative models.
Experiments on diverse architectures for image classification and text-to-image
generation show that our method improves detection accuracy over the strongest
baselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image
generation. The code is available at
https://github.com/shqii1j/distillation_detection.

</details>


### [235] [Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization](https://arxiv.org/abs/2510.02308)
*Dhruv Kohli,Sawyer J. Robertson,Gal Mishne,Alexander Cloninger*

Main category: cs.LG

TL;DR: LEGO is a spectral method that estimates tangent spaces of data manifolds by orthogonalizing gradients of low-frequency Laplacian eigenvectors, providing more robust noise resistance than standard LPCA.


<details>
  <summary>Details</summary>
Motivation: Standard Local PCA struggles in high-noise settings due to the critical trade-off in neighborhood size selection, which requires prior knowledge of data geometry and noise characteristics that are often unavailable.

Method: LEGO utilizes the global structure of data by orthogonalizing gradients of low-frequency eigenvectors of the graph Laplacian to estimate tangent spaces, rather than relying solely on local neighborhoods.

Result: LEGO yields tangent space estimates significantly more robust to noise than LPCA, with marked improvements in downstream tasks including manifold learning, boundary detection, and local intrinsic dimension estimation.

Conclusion: The proposed LEGO method provides a theoretically justified and practically effective approach for robust tangent space estimation in high-noise settings by leveraging global spectral information.

Abstract: Estimating the tangent spaces of a data manifold is a fundamental problem in
data analysis. The standard approach, Local Principal Component Analysis
(LPCA), struggles in high-noise settings due to a critical trade-off in
choosing the neighborhood size. Selecting an optimal size requires prior
knowledge of the geometric and noise characteristics of the data that are often
unavailable. In this paper, we propose a spectral method, Laplacian Eigenvector
Gradient Orthogonalization (LEGO), that utilizes the global structure of the
data to guide local tangent space estimation. Instead of relying solely on
local neighborhoods, LEGO estimates the tangent space at each data point by
orthogonalizing the gradients of low-frequency eigenvectors of the graph
Laplacian. We provide two theoretical justifications of our method. First, a
differential geometric analysis on a tubular neighborhood of a manifold shows
that gradients of the low-frequency Laplacian eigenfunctions of the tube align
closely with the manifold's tangent bundle, while an eigenfunction with high
gradient in directions orthogonal to the manifold lie deeper in the spectrum.
Second, a random matrix theoretic analysis also demonstrates that low-frequency
eigenvectors are robust to sub-Gaussian noise. Through comprehensive
experiments, we demonstrate that LEGO yields tangent space estimates that are
significantly more robust to noise than those from LPCA, resulting in marked
improvements in downstream tasks such as manifold learning, boundary detection,
and local intrinsic dimension estimation.

</details>


### [236] [KaVa: Latent Reasoning via Compressed KV-Cache Distillation](https://arxiv.org/abs/2510.02312)
*Anna Kuzina,Maciej Pioro,Paul N. Whatmough,Babak Ehteshami Bejnordi*

Main category: cs.LG

TL;DR: KaVa framework distills knowledge from compressed KV-cache of teacher LLMs into latent-reasoning students via self-distillation, enabling efficient reasoning without verbose chain-of-thought traces.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning in LLMs incurs high computational costs and memory overhead, while latent reasoning lacks effective supervision for complex natural-language reasoning tasks.

Method: Proposes KaVa framework that uses compressed KV-cache distillation to transfer knowledge from teacher models to latent-reasoning students, leveraging continuous latent tokens to align stepwise KV trajectories.

Result: Outperforms strong latent baselines, shows smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency.

Conclusion: Compressed KV-cache distillation provides scalable supervision for latent reasoning, combining CoT accuracy with latent inference efficiency and deployability.

Abstract: Large Language Models (LLMs) excel at multi-step reasoning problems with
explicit chain-of-thought (CoT), but verbose traces incur significant
computational costs and memory overhead, and often carry redundant, stylistic
artifacts. Latent reasoning has emerged as an efficient alternative that
internalizes the thought process, but it suffers from a critical lack of
supervision, limiting its effectiveness on complex, natural-language reasoning
traces. In this work, we propose KaVa, the first framework that bridges this
gap by distilling knowledge directly from a compressed KV-cache of the teacher
into a latent-reasoning student via self-distillation, leveraging the
representational flexibility of continuous latent tokens to align stepwise KV
trajectories. We show that the abstract, unstructured knowledge within
compressed KV-cache, which lacks direct token correspondence, can serve as a
rich supervisory signal for a latent reasoning student. Empirically, the
approach consistently outperforms strong latent baselines, exhibits markedly
smaller degradation from equation-only to natural-language traces, and scales
to larger backbones while preserving efficiency. These results establish
compressed KV-cache distillation as a scalable supervision signal for latent
reasoning, combining the accuracy of CoT-trained teachers with the efficiency
and deployability of latent inference.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [237] [Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge](https://arxiv.org/abs/2510.01223)
*Hui Dou,Ning Xu,Yiwen Zhang,Kaibin Wang*

Main category: cs.CR

TL;DR: RTS-Attack is a novel jailbreak method that uses semantically relevant nested scenarios with targeted toxic knowledge to bypass LLM alignment defenses while maintaining concealment.


<details>
  <summary>Details</summary>
Motivation: Existing nested scenario jailbreak methods are easily detectable due to prominent malicious intentions. The authors discovered that LLMs' alignment defenses are not sensitive to nested scenarios that are highly semantically relevant to queries and incorporate targeted toxic knowledge.

Method: Proposed RTS-Attack framework that builds scenarios highly relevant to queries and integrates targeted toxic knowledge to create adaptive and automated jailbreak prompts that bypass LLM alignment.

Result: Extensive experiments show RTS-Attack achieves superior performance in efficiency and universality compared to baselines across diverse advanced LLMs including GPT-4o, Llama3-70b, and Gemini-pro.

Conclusion: RTS-Attack demonstrates that LLMs' alignment defenses have vulnerabilities to carefully crafted nested scenarios with semantic relevance and targeted toxic knowledge, highlighting a critical security gap in current LLM safety mechanisms.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various tasks. However, they remain exposed to jailbreak attacks, eliciting
harmful responses. The nested scenario strategy has been increasingly adopted
across various methods, demonstrating immense potential. Nevertheless, these
methods are easily detectable due to their prominent malicious intentions. In
this work, we are the first to find and systematically verify that LLMs'
alignment defenses are not sensitive to nested scenarios, where these scenarios
are highly semantically relevant to the queries and incorporate targeted toxic
knowledge. This is a crucial yet insufficiently explored direction. Based on
this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with
Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs'
alignment. By building scenarios highly relevant to the queries and integrating
targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs.
Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful
queries, leading to outstanding concealment. Extensive experiments demonstrate
that RTS-Attack exhibits superior performance in both efficiency and
universality compared to the baselines across diverse advanced LLMs, including
GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the
supplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL
CONTENT.

</details>


### [238] [Fine-Tuning Jailbreaks under Highly Constrained Black-Box Settings: A Three-Pronged Approach](https://arxiv.org/abs/2510.01342)
*Xiangfang Li,Yu Wang,Bo Li*

Main category: cs.CR

TL;DR: A three-pronged jailbreak attack method that bypasses provider defenses in black-box fine-tuning settings, achieving over 97% success rates against GPT-4.1 and GPT-4o on OpenAI platform.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak attack studies focus on overly simplified scenarios with limited practical relevance to real-world defense settings, highlighting the need for more realistic attack evaluations.

Method: Combines safety-styled prefix/suffix wrappers, benign lexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism to make harmful datapoints appear innocuous while enabling models to learn harmful behaviors.

Result: Successfully jailbreaks GPT-4.1 and GPT-4o on OpenAI platform with attack success rates above 97% for both models under dataset-only black-box fine-tuning interface.

Conclusion: Demonstrates the vulnerability of current fine-tuning defenses and the need for more robust safety measures across all defense stages (pre-upload filtering, training-time defenses, and post-training audits).

Abstract: With the rapid advancement of large language models (LLMs), ensuring their
safe use becomes increasingly critical. Fine-tuning is a widely used method for
adapting models to downstream tasks, yet it is vulnerable to jailbreak attacks.
However, most existing studies focus on overly simplified attack scenarios,
limiting their practical relevance to real-world defense settings. To make this
risk concrete, we present a three-pronged jailbreak attack and evaluate it
against provider defenses under a dataset-only black-box fine-tuning interface.
In this setting, the attacker can only submit fine-tuning data to the provider,
while the provider may deploy defenses across stages: (1) pre-upload data
filtering, (2) training-time defensive fine-tuning, and (3) post-training
safety audit. Our attack combines safety-styled prefix/suffix wrappers, benign
lexical encodings (underscoring) of sensitive tokens, and a backdoor mechanism,
enabling the model to learn harmful behaviors while individual datapoints
appear innocuous. Extensive experiments demonstrate the effectiveness of our
approach. In real-world deployment, our method successfully jailbreaks GPT-4.1
and GPT-4o on the OpenAI platform with attack success rates above 97% for both
models. Our code is available at
https://github.com/lxf728/tri-pronged-ft-attack.

</details>


### [239] [Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays](https://arxiv.org/abs/2510.01350)
*Muhammad Faheemur Rahman,Wayne Burleson*

Main category: cs.CR

TL;DR: The paper proposes two security mechanisms (Keyed Permutor and Watermark Protection Columns) to protect memristive crossbar arrays from adversarial weight extraction and establish verifiable ownership, with minimal performance overhead.


<details>
  <summary>Details</summary>
Motivation: Non-volatile memristors in crossbar arrays are vulnerable to security threats where stored weights can be extracted when hardware is compromised, representing valuable intellectual property from costly training processes.

Method: Two security mechanisms: Keyed Permutor and Watermark Protection Columns that safeguard critical weights and establish verifiable ownership, integrated efficiently with existing memristive crossbar architectures without significant design modifications.

Result: Simulations across 45nm, 22nm, and 7nm CMOS nodes show both mechanisms offer robust protection with under 10% overhead in area, delay and power. Experiments with MNIST dataset confirm feasibility with minimal performance trade-offs.

Conclusion: The proposed security mechanisms effectively protect memristive in-memory computing systems from weight extraction attacks while maintaining minimal performance overhead, making them practical for real-world deployment.

Abstract: Memristive crossbar arrays enable in-memory computing by performing parallel
analog computations directly within memory, making them well-suited for machine
learning, neural networks, and neuromorphic systems. However, despite their
advantages, non-volatile memristors are vulnerable to security threats (such as
adversarial extraction of stored weights when the hardware is compromised.
Protecting these weights is essential since they represent valuable
intellectual property resulting from lengthy and costly training processes
using large, often proprietary, datasets. As a solution we propose two security
mechanisms: Keyed Permutor and Watermark Protection Columns; where both
safeguard critical weights and establish verifiable ownership (even in cases of
data leakage). Our approach integrates efficiently with existing memristive
crossbar architectures without significant design modifications. Simulations
across 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and
a large RF dataset, show that both mechanisms offer robust protection with
under 10% overhead in area, delay and power. We also present initial
experiments employing the widely known MNIST dataset; further highlighting the
feasibility of securing memristive in-memory computing systems with minimal
performance trade-offs.

</details>


### [240] [WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents](https://arxiv.org/abs/2510.01354)
*Yinuo Liu,Ruohan Xu,Xilong Wang,Yuqi Jia,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: First comprehensive benchmark study on detecting prompt injection attacks targeting web agents, evaluating both text-based and image-based detection methods across multiple attack scenarios.


<details>
  <summary>Details</summary>
Motivation: Multiple prompt injection attacks have been proposed against web agents, but existing detection methods haven't been systematically evaluated for this specific context.

Method: Created fine-grained attack categorization, constructed datasets with malicious/benign text and images, systematized detection methods, and evaluated performance across multiple scenarios.

Result: Some detectors can identify explicit textual instruction attacks or visible image perturbations with moderate-high accuracy, but fail against attacks without explicit instructions or with imperceptible perturbations.

Conclusion: Current detection methods have significant limitations, particularly against sophisticated attacks that omit explicit instructions or use imperceptible perturbations, highlighting the need for more robust defenses.

Abstract: Multiple prompt injection attacks have been proposed against web agents. At
the same time, various methods have been developed to detect general prompt
injection attacks, but none have been systematically evaluated for web agents.
In this work, we bridge this gap by presenting the first comprehensive
benchmark study on detecting prompt injection attacks targeting web agents. We
begin by introducing a fine-grained categorization of such attacks based on the
threat model. We then construct datasets containing both malicious and benign
samples: malicious text segments generated by different attacks, benign text
segments from four categories, malicious images produced by attacks, and benign
images from two categories. Next, we systematize both text-based and
image-based detection methods. Finally, we evaluate their performance across
multiple scenarios. Our key findings show that while some detectors can
identify attacks that rely on explicit textual instructions or visible image
perturbations with moderate to high accuracy, they largely fail against attacks
that omit explicit instructions or employ imperceptible perturbations. Our
datasets and code are released at:
https://github.com/Norrrrrrr-lyn/WAInjectBench.

</details>


### [241] [Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks](https://arxiv.org/abs/2510.01359)
*Shoumik Saha,Jifan Chen,Sam Mayers,Sanjay Krishna Gouda,Zijian Wang,Varun Kumar*

Main category: cs.CR

TL;DR: JAWS-BENCH is a benchmark for evaluating code-capable LLM agents against jailbreak attacks across three workspace regimes, showing that agents significantly increase vulnerability compared to text-only models, with up to 75% attack success rate and 32% instantly deployable malicious code.


<details>
  <summary>Details</summary>
Motivation: Code-capable LLM agents are increasingly used in software engineering workflows where they can execute code, raising security risks beyond text-only jailbreaks. Prior evaluations focused on refusal detection but didn't assess whether agents actually compile and run malicious programs.

Method: Created JAWS-BENCH with three escalating workspace regimes (empty, single-file, multi-file) and a hierarchical Judge Framework that tests compliance, attack success, syntactic correctness, and runtime executability. Evaluated seven LLMs from five families as backends.

Result: In prompt-only conditions, code agents accepted 61% of attacks (58% harmful, 52% parse, 27% run end-to-end). Single-file regime drove compliance to ~100% with 71% mean ASR, multi-file regime raised ASR to ~75% with 32% instantly deployable code. Agent wrapping increased vulnerability by 1.6x.

Conclusion: Code agents substantially increase vulnerability as initial refusals are overturned during planning/tool-use steps. Findings motivate execution-aware defenses, code-contextual safety filters, and mechanisms to preserve refusal decisions throughout multi-step reasoning.

Abstract: Code-capable large language model (LLM) agents are increasingly embedded into
software engineering workflows where they can read, write, and execute code,
raising the stakes of safety-bypass ("jailbreak") attacks beyond text-only
settings. Prior evaluations emphasize refusal or harmful-text detection,
leaving open whether agents actually compile and run malicious programs. We
present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three
escalating workspace regimes that mirror attacker capability: empty (JAWS-0),
single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a
hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii)
attack success, (iii) syntactic correctness, and (iv) runtime executability,
moving beyond refusal to measure deployable harm. Using seven LLMs from five
families as backends, we find that under prompt-only conditions in JAWS-0, code
agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27%
run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~
100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the
multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly
deployable attack code. Across models, wrapping an LLM in an agent
substantially increases vulnerability -- ASR raises by 1.6x -- because initial
refusals are frequently overturned during later planning/tool-use steps.
Category-level analyses identify which attack classes are most vulnerable and
most readily deployable, while others exhibit large execution gaps. These
findings motivate execution-aware defenses, code-contextual safety filters, and
mechanisms that preserve refusal decisions throughout the agent's multi-step
reasoning and tool use.

</details>


### [242] [E-FuzzEdge: Optimizing Embedded Device Security with Scalable In-Place Fuzzing](https://arxiv.org/abs/2510.01393)
*Davide Rusconi,Osama Yousef,Mirco Picca,Flavio Toffalini,Andrea Lanzi*

Main category: cs.CR

TL;DR: E-FuzzEdge is a novel fuzzing architecture that improves throughput for microcontroller fuzzing when scalability is unavailable, addressing hardware-in-the-loop inefficiencies through execution speed optimization.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of hardware-in-the-loop fuzzing for microcontrollers, particularly in contexts where scalability is unavailable, by optimizing execution speed.

Method: E-FuzzEdge architecture that optimizes execution speed for microcontroller fuzzing, compatible with embedded fuzzing techniques that perform on-device testing instead of firmware emulation.

Result: Significant performance improvements demonstrated against state-of-the-art benchmarks.

Conclusion: E-FuzzEdge can be integrated into broader embedded fuzzing workflows to enhance overall testing efficiency due to its compatibility with other embedded fuzzing techniques.

Abstract: In this paper we show E-FuzzEdge, a novel fuzzing architecture targeted
towards improving the throughput of fuzzing campaigns in contexts where
scalability is unavailable. E-FuzzEdge addresses the inefficiencies of
hardware-in-the-loop fuzzing for microcontrollers by optimizing execution
speed. We evaluated our system against state-of-the-art benchmarks,
demonstrating significant performance improvements. A key advantage of
E-FuzzEdgearchitecture is its compatibility with other embedded fuzzing
techniques that perform on device testing instead of firmware emulation. This
means that the broader embedded fuzzing community can integrate E-FuzzEdge into
their workflows to enhance overall testing efficiency.

</details>


### [243] [Securing IoT Devices in Smart Cities: A Review of Proposed Solutions](https://arxiv.org/abs/2510.01445)
*AndrÃ©s F. Betancur-LÃ³pez*

Main category: cs.CR

TL;DR: This paper reviews security proposals for protecting IoT devices in Smart Cities, focusing on lightweight cryptography, PUFs, and blockchain solutions, highlighting their strengths, limitations, and the need for more practical and scalable mechanisms.


<details>
  <summary>Details</summary>
Motivation: Privacy and security in Smart Cities are constantly at risk due to vulnerabilities in IoT devices. The limited computational resources of IoT devices make them susceptible to attacks, and their widespread adoption increases the potential impact of security breaches.

Method: The review was conducted by analyzing recent literature on device-level security, with particular emphasis on lightweight cryptography, physically unclonable functions (PUFs), and blockchain-based solutions.

Result: Findings highlight both the strengths and limitations of current approaches to IoT security in Smart Cities.

Conclusion: There is a need for more practical, scalable, and resource-efficient mechanisms to ensure user privacy and data protection in IoT ecosystems.

Abstract: Privacy and security in Smart Cities remain at constant risk due to the
vulnerabilities introduced by Internet of Things (IoT) devices. The limited
computational resources of these devices make them especially susceptible to
attacks, while their widespread adoption increases the potential impact of
security breaches. This article presents a review of security proposals aimed
at protecting IoT devices in Smart City environments. The review was conducted
by analyzing recent literature on device-level security, with particular
emphasis on lightweight cryptography, physically unclonable functions (PUFs),
and blockchain-based solutions. Findings highlight both the strengths and
limitations of current approaches, as well as the need for more practical,
scalable, and resource-efficient mechanisms to ensure user privacy and data
protection in IoT ecosystems.

</details>


### [244] [POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment](https://arxiv.org/abs/2510.01552)
*Luoxi Tang,Yuqiao Meng,Ankita Patra,Weicheng Ma,Muchao Ye,Zhaohan Xi*

Main category: cs.CR

TL;DR: LLMs have vulnerabilities in cyber threat intelligence including spurious correlations, contradictory knowledge, and constrained generalization that limit their practical effectiveness.


<details>
  <summary>Details</summary>
Motivation: To investigate intrinsic vulnerabilities of LLMs in cyber threat intelligence that arise from the threat landscape itself rather than model architecture, addressing performance gaps in practical deployments.

Method: Large-scale evaluations across multiple CTI benchmarks and real-world threat reports using a novel categorization methodology that integrates stratification, autoregressive refinement, and human-in-the-loop supervision.

Result: Identified three fundamental vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization that limit LLMs' effectiveness in CTI tasks.

Conclusion: Provides actionable insights for designing more robust LLM-powered CTI systems to facilitate future research and improve practical deployment.

Abstract: Large Language Models (LLMs) are intensively used to assist security analysts
in counteracting the rapid exploitation of cyber threats, wherein LLMs offer
cyber threat intelligence (CTI) to support vulnerability assessment and
incident response. While recent work has shown that LLMs can support a wide
range of CTI tasks such as threat analysis, vulnerability detection, and
intrusion defense, significant performance gaps persist in practical
deployments. In this paper, we investigate the intrinsic vulnerabilities of
LLMs in CTI, focusing on challenges that arise from the nature of the threat
landscape itself rather than the model architecture. Using large-scale
evaluations across multiple CTI benchmarks and real-world threat reports, we
introduce a novel categorization methodology that integrates stratification,
autoregressive refinement, and human-in-the-loop supervision to reliably
analyze failure instances. Through extensive experiments and human inspections,
we reveal three fundamental vulnerabilities: spurious correlations,
contradictory knowledge, and constrained generalization, that limit LLMs in
effectively supporting CTI. Subsequently, we provide actionable insights for
designing more robust LLM-powered CTI systems to facilitate future research.

</details>


### [245] [Position: Privacy Is Not Just Memorization!](https://arxiv.org/abs/2510.01645)
*Niloofar Mireshghallah,Tianshi Li*

Main category: cs.CR

TL;DR: The paper argues that LLM privacy risks extend far beyond training data memorization, identifying broader threats across the LLM lifecycle including data collection, inference-time leakage, autonomous agents, and surveillance democratization.


<details>
  <summary>Details</summary>
Motivation: Current privacy discourse disproportionately focuses on verbatim memorization while overlooking more immediate and scalable privacy threats in LLM systems.

Method: Presents a comprehensive taxonomy of privacy risks across the LLM lifecycle and conducts longitudinal analysis of 1,322 AI/ML privacy papers from 2016-2025.

Result: Reveals that memorization receives outsized attention while the most pressing privacy harms lie elsewhere, where current technical approaches offer little traction.

Conclusion: Calls for a fundamental shift in LLM privacy research beyond narrow technical solutions toward interdisciplinary approaches addressing sociotechnical threats.

Abstract: The discourse on privacy risks in Large Language Models (LLMs) has
disproportionately focused on verbatim memorization of training data, while a
constellation of more immediate and scalable privacy threats remain
underexplored. This position paper argues that the privacy landscape of LLM
systems extends far beyond training data extraction, encompassing risks from
data collection practices, inference-time context leakage, autonomous agent
capabilities, and the democratization of surveillance through deep inference
attacks. We present a comprehensive taxonomy of privacy risks across the LLM
lifecycle -- from data collection through deployment -- and demonstrate through
case studies how current privacy frameworks fail to address these multifaceted
threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers
published at leading conferences over the past decade (2016--2025), we reveal
that while memorization receives outsized attention in technical research, the
most pressing privacy harms lie elsewhere, where current technical approaches
offer little traction and viable paths forward remain unclear. We call for a
fundamental shift in how the research community approaches LLM privacy, moving
beyond the narrow focus of current technical solutions and embracing
interdisciplinary approaches that address the sociotechnical nature of these
emerging threats.

</details>


### [246] [Evaluating the Robustness of a Production Malware Detection System to Transferable Adversarial Attacks](https://arxiv.org/abs/2510.01676)
*Milad Nasr,Yanick Fratantonio,Luca Invernizzi,Ange Albertini,Loua Farah,Alex Petit-Bianco,Andreas Terzis,Kurt Thomas,Elie Bursztein,Nicholas Carlini*

Main category: cs.CR

TL;DR: Adversarial attacks on Gmail's ML-based malware detection system (Magika) can bypass detection by modifying just 13 bytes of malware, achieving 90% evasion. A defense was developed and deployed, requiring 50 bytes for only 20% success rate.


<details>
  <summary>Details</summary>
Motivation: To study how adversarial attacks on ML components can create system-level vulnerabilities in production systems, using Gmail's malware detection pipeline as a case study.

Method: Designed adversarial examples to fool Magika (Gmail's file-type identification model) by modifying malware samples, causing incorrect routing to unsuitable malware detectors.

Result: With only 13 bytes modified, achieved 90% evasion success rate against Magika, allowing malware to bypass Gmail's detection. The developed defense reduced attack success to 20% even with 50 bytes modification.

Conclusion: ML components in production systems are vulnerable to adversarial attacks that can bypass entire security pipelines. Effective defenses can be developed and deployed to significantly mitigate these risks, as demonstrated with Gmail's production system.

Abstract: As deep learning models become widely deployed as components within larger
production systems, their individual shortcomings can create system-level
vulnerabilities with real-world impact. This paper studies how adversarial
attacks targeting an ML component can degrade or bypass an entire
production-grade malware detection system, performing a case study analysis of
Gmail's pipeline where file-type identification relies on a ML model.
  The malware detection pipeline in use by Gmail contains a machine learning
model that routes each potential malware sample to a specialized malware
classifier to improve accuracy and performance. This model, called Magika, has
been open sourced. By designing adversarial examples that fool Magika, we can
cause the production malware service to incorrectly route malware to an
unsuitable malware detector thereby increasing our chance of evading detection.
Specifically, by changing just 13 bytes of a malware sample, we can
successfully evade Magika in 90% of cases and thereby allow us to send malware
files over Gmail. We then turn our attention to defenses, and develop an
approach to mitigate the severity of these types of attacks. For our defended
production model, a highly resourced adversary requires 50 bytes to achieve
just a 20% attack success rate. We implement this defense, and, thanks to a
collaboration with Google engineers, it has already been deployed in production
for the Gmail classifier.

</details>


### [247] [Towards Imperceptible Adversarial Defense: A Gradient-Driven Shield against Facial Manipulations](https://arxiv.org/abs/2510.01699)
*Yue Li,Linying Xue,Dongdong Lin,Qiushi Li,Hui Tian,Hongxia Wang*

Main category: cs.CR

TL;DR: GRASP is a gradient-projection-based adversarial proactive defense method that effectively counters facial deepfakes while maintaining high visual quality by resolving gradient conflicts between defense effectiveness and visual quality losses.


<details>
  <summary>Details</summary>
Motivation: With the proliferation of generative models, manipulated facial images pose threats to privacy and societal trust. Existing proactive defense methods struggle with balancing imperceptibility and defense effectiveness, often overlooking gradient conflicts between different loss functions.

Method: GRASP integrates structural similarity loss and low-frequency loss to enhance perturbation imperceptibility. It introduces a gradient-projection mechanism to mitigate conflicts between defense effectiveness loss and visual quality losses, enabling balanced optimization.

Result: Extensive experiments show GRASP achieves PSNR exceeding 40 dB, SSIM of 0.99, and 100% defense success rate against facial attribute manipulations, significantly outperforming existing approaches in visual quality.

Conclusion: GRASP successfully bridges the gap between defense effectiveness and visual quality in proactive deepfake defense by addressing gradient conflicts, providing a robust solution that preserves image fidelity without compromising defensive performance.

Abstract: With the flourishing prosperity of generative models, manipulated facial
images have become increasingly accessible, raising concerns regarding privacy
infringement and societal trust. In response, proactive defense strategies
embed adversarial perturbations into facial images to counter deepfake
manipulation. However, existing methods often face a tradeoff between
imperceptibility and defense effectiveness-strong perturbations may disrupt
forgeries but degrade visual fidelity. Recent studies have attempted to address
this issue by introducing additional visual loss constraints, yet often
overlook the underlying gradient conflicts among losses, ultimately weakening
defense performance. To bridge the gap, we propose a gradient-projection-based
adversarial proactive defense (GRASP) method that effectively counters facial
deepfakes while minimizing perceptual degradation. GRASP is the first approach
to successfully integrate both structural similarity loss and low-frequency
loss to enhance perturbation imperceptibility. By analyzing gradient conflicts
between defense effectiveness loss and visual quality losses, GRASP pioneers
the design of the gradient-projection mechanism to mitigate these conflicts,
enabling balanced optimization that preserves image fidelity without
sacrificing defensive performance. Extensive experiments validate the efficacy
of GRASP, achieving a PSNR exceeding 40 dB, SSIM of 0.99, and a 100% defense
success rate against facial attribute manipulations, significantly
outperforming existing approaches in visual quality.

</details>


### [248] [Constructions of Efficiently Implementable Boolean Functions with Provable Nonlinearity/Resiliency/Algebraic Immunity Trade-Offs](https://arxiv.org/abs/2510.01720)
*Palash Sarkar*

Main category: cs.CR

TL;DR: The paper presents efficient Boolean functions with provable trade-offs between resiliency, nonlinearity, and algebraic immunity, constructible with linear size in parameters.


<details>
  <summary>Details</summary>
Motivation: To design Boolean functions that simultaneously achieve high resiliency, nonlinearity, and algebraic immunity with efficient implementation.

Method: Propose families of Boolean functions parameterized by m0, x0, a0, ensuring resiliency â¥ m0, linear bias â¤ 2^{-x0}, algebraic immunity â¥ a0, with n linear in parameters.

Result: Constructed n-variable functions meet specified criteria with O(n) gate complexity, demonstrating feasible trade-offs among cryptographic properties.

Conclusion: Efficient Boolean functions with balanced cryptographic properties are achievable, enabling practical applications in secure system design.

Abstract: We describe several families of efficiently implementable Boolean functions
achieving provable trade-offs between resiliency, nonlinearity, and algebraic
immunity. In concrete terms, the following result holds for each of the
function families that we propose. Given integers $m_0\geq 0$, $x_0\geq 1$, and
$a_0\geq 1$, it is possible to construct an $n$-variable function which has
resiliency at least $m_0$, linear bias (which is an equivalent method of
expressing nonlinearity) at most $2^{-x_0}$ and algebraic immunity at least
$a_0$; further, $n$ is linear in $m_0$, $x_0$ and $a_0$, and the function can
be implemented using $O(n)$ gates.

</details>


### [249] [Secure Multi-Modal Data Fusion in Federated Digital Health Systems via MCP](https://arxiv.org/abs/2510.01780)
*Aueaphum Aueawatthanaphisut*

Main category: cs.CR

TL;DR: A novel federated learning framework using Model Context Protocol (MCP) for secure multi-modal healthcare data integration, improving diagnostic accuracy by 9.8% and reducing client dropouts by 54% while maintaining privacy.


<details>
  <summary>Details</summary>
Motivation: Secure and interoperable integration of heterogeneous medical data is challenging in digital health. Current FL frameworks lack standardized mechanisms for multi-modal data fusion across distributed and resource-constrained environments.

Method: Proposes a framework with three pillars: (i) multi-modal feature alignment for clinical imaging, EMR, and wearable IoT data; (ii) secure aggregation with differential privacy; (iii) energy-aware scheduling to mitigate mobile client dropouts. Uses MCP as schema-driven interface for adaptive orchestration.

Result: Experimental evaluation shows 9.8% improvement in diagnostic accuracy compared to baseline FL, 54% reduction in client dropout rates, and clinically acceptable privacy-utility trade-offs.

Conclusion: MCP-enabled multi-modal fusion provides a scalable and trustworthy pathway toward equitable, next-generation federated health infrastructures.

Abstract: Secure and interoperable integration of heterogeneous medical data remains a
grand challenge in digital health. Current federated learning (FL) frameworks
offer privacy-preserving model training but lack standardized mechanisms to
orchestrate multi-modal data fusion across distributed and resource-constrained
environments. This study introduces a novel framework that leverages the Model
Context Protocol (MCP) as an interoperability layer for secure, cross-agent
communication in multi-modal federated healthcare systems. The proposed
architecture unifies three pillars: (i) multi-modal feature alignment for
clinical imaging, electronic medical records, and wearable IoT data; (ii)
secure aggregation with differential privacy to protect patient-sensitive
updates; and (iii) energy-aware scheduling to mitigate dropouts in mobile
clients. By employing MCP as a schema-driven interface, the framework enables
adaptive orchestration of AI agents and toolchains while ensuring compliance
with privacy regulations. Experimental evaluation on benchmark datasets and
pilot clinical cohorts demonstrates up to 9.8\% improvement in diagnostic
accuracy compared with baseline FL, a 54\% reduction in client dropout rates,
and clinically acceptable privacy--utility trade-offs. These results highlight
MCP-enabled multi-modal fusion as a scalable and trustworthy pathway toward
equitable, next-generation federated health infrastructures.

</details>


### [250] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON is a novel system using ZK-SNARKs to watermark image generation models, enabling verifiable proof of origin without exposing sensitive model information, with selective layer circuit creation and LSB steganography embedding.


<details>
  <summary>Details</summary>
Motivation: Address concerns around authenticity, ownership, and misuse of synthetic media as image generation models become more powerful and accessible, with risks including misinformation, deepfakes, and IP violations.

Method: Uses ZK-SNARKs for watermarking, proposes Selective Layer ZK-Circuit Creation (SL-ZKCC) to convert key model layers into circuits, and embeds proofs via Least Significant Bit steganography into generated images.

Result: Demonstrated on both GAN and Diffusion models, providing a secure, model-agnostic pipeline for trustworthy AI image generation with significantly reduced proof generation time.

Conclusion: ZK-WAGON offers a secure and scalable solution for watermarking image generation models without degrading quality or requiring access to confidential model internals, addressing critical authenticity concerns in synthetic media.

Abstract: As image generation models grow increasingly powerful and accessible,
concerns around authenticity, ownership, and misuse of synthetic media have
become critical. The ability to generate lifelike images indistinguishable from
real ones introduces risks such as misinformation, deepfakes, and intellectual
property violations. Traditional watermarking methods either degrade image
quality, are easily removed, or require access to confidential model internals
- making them unsuitable for secure and scalable deployment. We are the first
to introduce ZK-WAGON, a novel system for watermarking image generation models
using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge
(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing
model weights, generation prompts, or any sensitive internal information. We
propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively
convert key layers of an image generation model into a circuit, reducing proof
generation time significantly. Generated ZK-SNARK proofs are imperceptibly
embedded into a generated image via Least Significant Bit (LSB) steganography.
We demonstrate this system on both GAN and Diffusion models, providing a
secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


### [251] [Mirage Fools the Ear, Mute Hides the Truth: Precise Targeted Adversarial Attacks on Polyphonic Sound Event Detection Systems](https://arxiv.org/abs/2510.02158)
*Junjie Su,Weifei Jin,Yuxin Cao,Derui Wang,Kai Ye,Jie Hao*

Main category: cs.CR

TL;DR: Proposed M2A framework for targeted adversarial attacks on polyphonic Sound Event Detection systems, achieving high precision with preservation loss constraints and new EP metric.


<details>
  <summary>Details</summary>
Motivation: SED systems used in safety-critical applications lack robustness against adversarial attacks, with existing methods being ineffective or imprecise due to contextual dependencies and unintended effects on non-target regions.

Method: Mirage and Mute Attack (M2A) framework with preservation loss constraints on non-target outputs to ensure precise attacks without altering model outputs for non-target regions.

Result: Achieved 94.56% and 99.11% Editing Precision on two state-of-the-art SED models, demonstrating both effectiveness and high precision.

Conclusion: M2A framework successfully addresses the precision limitations of existing audio adversarial attacks on SED systems while maintaining attack effectiveness.

Abstract: Sound Event Detection (SED) systems are increasingly deployed in
safety-critical applications such as industrial monitoring and audio
surveillance. However, their robustness against adversarial attacks has not
been well explored. Existing audio adversarial attacks targeting SED systems,
which incorporate both detection and localization capabilities, often lack
effectiveness due to SED's strong contextual dependencies or lack precision by
focusing solely on misclassifying the target region as the target event,
inadvertently affecting non-target regions. To address these challenges, we
propose the Mirage and Mute Attack (M2A) framework, which is designed for
targeted adversarial attacks on polyphonic SED systems. In our optimization
process, we impose specific constraints on the non-target output, which we
refer to as preservation loss, ensuring that our attack does not alter the
model outputs for non-target region, thus achieving precise attacks.
Furthermore, we introduce a novel evaluation metric Editing Precison (EP) that
balances effectiveness and precision, enabling our method to simultaneously
enhance both. Comprehensive experiments show that M2A achieves 94.56% and
99.11% EP on two state-of-the-art SED models, demonstrating that the framework
is sufficiently effective while significantly enhancing attack precision.

</details>


### [252] [NoMod: A Non-modular Attack on Module Learning With Errors](https://arxiv.org/abs/2510.02162)
*Cristian Bassotto,Ermes Franch,Marina KrÄek,Stjepan Picek*

Main category: cs.CR

TL;DR: NoMod ML-Attack is a hybrid white-box cryptanalytic method that recovers secrets in Module-LWE based post-quantum schemes by treating modular wrap-arounds as statistical corruption and using robust linear estimation with optimized lattice preprocessing.


<details>
  <summary>Details</summary>
Motivation: The threat of quantum computing to classical cryptography motivates the need to analyze the security of post-quantum schemes like Module-LWE based systems, which are being adopted by NIST.

Method: Combines optimized lattice preprocessing (reduced-vector saving and algebraic amplification) with robust estimators trained via Tukey's Biweight loss, treating modular wrap-arounds as statistical corruption rather than modeling them directly.

Result: Achieves full recovery of binary secrets for dimension n=350, recovery of sparse binomial secrets for n=256, and successful recovery in CRYSTALS-Kyber settings with parameters (128,3) and (256,2).

Conclusion: NoMod ML-Attack demonstrates effective secret recovery in Module-LWE based post-quantum cryptography, highlighting potential vulnerabilities in these schemes.

Abstract: The advent of quantum computing threatens classical public-key cryptography,
motivating NIST's adoption of post-quantum schemes such as those based on the
Module Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a
hybrid white-box cryptanalytic method that circumvents the challenge of
modeling modular reduction by treating wrap-arounds as statistical corruption
and casting secret recovery as robust linear estimation. Our approach combines
optimized lattice preprocessing--including reduced-vector saving and algebraic
amplification--with robust estimators trained via Tukey's Biweight loss.
Experiments show NoMod achieves full recovery of binary secrets for dimension
$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful
recovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =
(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous
repository https://anonymous.4open.science/r/NoMod-3BD4.

</details>


### [253] [Testing Stability and Robustness in Three Cryptographic Chaotic Systems](https://arxiv.org/abs/2510.02184)
*N. A. Anagnostopoulos,K. Konstantinidis,A. N. Miliou,S. G. Stavrinides*

Main category: cs.CR

TL;DR: Testing stability and robustness of three cryptographic chaotic systems for synchronization under noise conditions.


<details>
  <summary>Details</summary>
Motivation: Ensure drive-response systems remain synchronized despite noise for practical security applications.

Method: Evaluate three distinct cryptographic chaotic systems for stability and robustness.

Result: Comparison of synchronization performance across different systems under noise.

Conclusion: Identifies most robust chaotic system for secure cryptographic applications.

Abstract: In practical applications, it is crucial that the drive-response systems,
although identical in all respects, are synchronized at all times, even if
there is noise present. In this work, we test the stability and robustness of
three distinct and well-known cryptographic chaotic systems, and compare the
results in relation to the desired security.

</details>


### [254] [Authentication Security of PRF GNSS Ranging](https://arxiv.org/abs/2510.02196)
*Jason Anderson*

Main category: cs.CR

TL;DR: This paper analyzes the authentication security of pseudorandom function (PRF) GNSS ranging against various spoofing models, including SCER spoofers, and applies the analysis to Galileo's SAS using E6-C signal.


<details>
  <summary>Details</summary>
Motivation: To establish trust in GNSS pseudoranges and PNT solutions by analyzing how PRF ranging with secret keys can prevent spoofers from predicting ranging codes before broadcast.

Method: Derives authentication security bounds for PRF GNSS ranging under multiple spoofing models, applies methods to Galileo's SAS using encrypted E6-C signal, and computes required data duration for security.

Result: Shows that at most 400 ms of Galileo E6-C data is needed to achieve 128-bit authentication security under non-SCER models, and predicts adversary equipment requirements for SCER attacks.

Conclusion: The work provides a framework for designing PRF GNSS ranging protocols that meet authentication security requirements by computing probability of missed detection.

Abstract: This work derives the authentication security of pseudorandom function (PRF)
GNSS ranging under multiple GNSS spoofing models, including the Security Code
Estimation and Replay (SCER) spoofer. When GNSS ranging codes derive from a PRF
utilizing a secret known only to the broadcaster, the spoofer cannot predict
the ranging code before broadcast. Therefore, PRF ranging can be used to
establish trust in the GNSS pseudoranges and the resulting receiver position,
navigation, and timing (PNT) solution. I apply the methods herein to Galileo's
Signal Authentication Service (SAS) utilizing the encrypted Galileo E6-C signal
to compute that, at most, 400 ms of Galileo E6-C data to assert 128-bit
authentication security under non-SCER models. For the SCER adversary, I
predict the adversary's needed receiving radio equipment to break
authentication security. One can use this work to design a PRF GNSS ranging
protocol to meet useful authentication security requirements by computing the
probability of missed detection.

</details>


### [255] [An efficient quantum algorithm for computing $S$-units and its applications](https://arxiv.org/abs/2510.02280)
*Jean-Francois Biasse,Fang Song*

Main category: cs.CR

TL;DR: This paper provides detailed proofs for Biasse and Song's quantum polynomial-time algorithm for computing S-unit groups in number fields, which enables various computational number theory problems to be solved efficiently.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for the quantum polynomial-time algorithm that can solve fundamental problems in computational number theory, including class group computations and principal ideal problems.

Method: Detailed analysis and proofs of Biasse and Song's quantum algorithm for computing S-unit groups, leveraging quantum computing capabilities to achieve polynomial-time solutions.

Result: The algorithm enables polynomial-time computation of class groups, S-class groups, relative class groups, unit groups, ray class groups, principal ideal problems, norm equations, and ideal class decomposition.

Conclusion: The proven quantum algorithm provides efficient solutions to long-standing computational number theory problems and enables applications in cryptography, including finding short generators in principal ideals and mildly short vectors in ideal lattices.

Abstract: In this paper, we provide details on the proofs of the quantum polynomial
time algorithm of Biasse and Song (SODA 16) for computing the $S$-unit group of
a number field. This algorithm directly implies polynomial time methods to
calculate class groups, S-class groups, relative class group and the unit
group, ray class groups, solve the principal ideal problem, solve certain norm
equations, and decompose ideal classes in the ideal class group. Additionally,
combined with a result of Cramer, Ducas, Peikert and Regev (Eurocrypt 2016),
the resolution of the principal ideal problem allows one to find short
generators of a principal ideal. Likewise, methods due to Cramer, Ducas and
Wesolowski (Eurocrypt 2017) use the resolution of the principal ideal problem
and the decomposition of ideal classes to find so-called ``mildly short
vectors'' in ideal lattices of cyclotomic fields.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [256] [OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models](https://arxiv.org/abs/2510.01253)
*Jianzhang Zhang,Jialong Zhou,Chuang Liu*

Main category: cs.AI

TL;DR: OR-Toolformer fine-tunes Llama-3.1-8B-Instruct with tool augmentation to solve OR problems, achieving up to 80.1% execution accuracy and strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns with closed-source LLM APIs for OR tasks and reduce high compute costs of training open-source models from scratch.

Method: Fine-tunes Llama-3.1-8B-Instruct using semi-automatic data synthesis pipeline that generates OR problem-answer pairs and augments model with external solvers to produce API calls.

Result: Achieves up to 80.1% execution accuracy on 3/4 standard benchmarks (4.3% improvement over baselines), and 54% average accuracy on unseen OR problems (21% improvement).

Conclusion: Tool-augmented fine-tuning LLMs are effective for accurate and generalizable OR problem modeling and solving.

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning, but
reliance on closed-source APIs for OR tasks raises privacy concerns, and
training open-source models from scratch incurs high compute costs. We
introduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a
semi-automatic data synthesis pipeline that generates diverse OR problem-answer
pairs and augments the model with external solvers to produce API calls. On
three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution
accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot
evaluation on two unseen OR problem types, it attains 54% average accuracy, a
21 percentage-point improvement over the strongest baseline. These findings
validate the efficacy of tool-augmented fine-tuning LLMs for accurate and
generalizable OR problem modeling and solving.

</details>


### [257] [Modeling Others' Minds as Code](https://arxiv.org/abs/2510.01272)
*Kunal Jha,Aydan Yuenan Huang,Eric Ye,Natasha Jaques,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: ROTE is a novel algorithm that models human behavior as behavioral programs (scripts) using LLMs for program synthesis and probabilistic inference for uncertainty reasoning, achieving superior prediction accuracy compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing human behavior modeling approaches are data-hungry and brittle due to unrealistic rationality assumptions or computational complexity. The insight is that everyday social interactions follow predictable "scripts" that minimize cognitive load.

Method: ROTE uses large language models to synthesize a hypothesis space of behavioral programs (represented as computer code) and probabilistic inference to reason about uncertainty over this program space.

Result: ROTE outperforms competitive baselines (behavior cloning and LLM-based methods) by up to 50% in both in-sample accuracy and out-of-sample generalization across gridworld tasks and embodied household simulations.

Conclusion: By treating action understanding as a program synthesis problem, ROTE enables AI systems to efficiently and effectively predict human behavior in real-world scenarios.

Abstract: Accurate prediction of human behavior is essential for robust and safe
human-AI collaboration. However, existing approaches for modeling people are
often data-hungry and brittle because they either make unrealistic assumptions
about rationality or are too computationally demanding to adapt rapidly. Our
key insight is that many everyday social interactions may follow predictable
patterns; efficient "scripts" that minimize cognitive load for actors and
observers, e.g., "wait for the green light, then go." We propose modeling these
routines as behavioral programs instantiated in computer code rather than
policies conditioned on beliefs and desires. We introduce ROTE, a novel
algorithm that leverages both large language models (LLMs) for synthesizing a
hypothesis space of behavioral programs, and probabilistic inference for
reasoning about uncertainty over that space. We test ROTE in a suite of
gridworld tasks and a large-scale embodied household simulator. ROTE predicts
human and AI behaviors from sparse observations, outperforming competitive
baselines -- including behavior cloning and LLM-based methods -- by as much as
50% in terms of in-sample accuracy and out-of-sample generalization. By
treating action understanding as a program synthesis problem, ROTE opens a path
for AI systems to efficiently and effectively predict human behavior in the
real-world.

</details>


### [258] [Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery](https://arxiv.org/abs/2510.01293)
*Zekun Jiang,Chunming Xu,Tianhang Zhou*

Main category: cs.AI

TL;DR: CA-ChemE system is a multi-agent AI platform for chemical engineering that enables autonomous research through knowledge-enhanced agents and collaboration mechanisms, but faces efficiency bottlenecks in cross-domain collaboration.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing AI systems for interdisciplinary collaboration and exploration of uncharted problems in chemical engineering.

Method: Developed CA-ChemE system with domain-specific knowledge bases, knowledge enhancement technologies, and collaboration agents using ontology engineering capabilities.

Result: Knowledge enhancement improved dialogue quality by 10-15%, but cross-domain collaboration showed significant efficiency gaps - 8.5% improvement for distant-domain pairs vs 0.8% for domain-proximate pairs.

Conclusion: Multi-agent architectures with careful design can enable autonomous scientific discovery in chemical engineering, though knowledge-base gaps remain a critical challenge for cross-domain collaboration.

Abstract: The rapid advancement of artificial intelligence (AI) has demonstrated
substantial potential in chemical engineering, yet existing AI systems remain
limited in interdisciplinary collaboration and exploration of uncharted
problems. To address these issues, we present the Cyber Academia-Chemical
Engineering (CA-ChemE) system, a living digital town that enables self-directed
research evolution and emergent scientific discovery through multi-agent
collaboration. By integrating domain-specific knowledge bases, knowledge
enhancement technologies, and collaboration agents, the system successfully
constructs an intelligent ecosystem capable of deep professional reasoning and
efficient interdisciplinary collaboration. Our findings demonstrate that
knowledge base-enabled enhancement mechanisms improved dialogue quality scores
by 10-15% on average across all seven expert agents, fundamentally ensuring
technical judgments are grounded in verifiable scientific evidence. However, we
observed a critical bottleneck in cross-domain collaboration efficiency,
prompting the introduction of a Collaboration Agent (CA) equipped with ontology
engineering capabilities. CA's intervention achieved 8.5% improvements for
distant-domain expert pairs compared to only 0.8% for domain-proximate pairs -
a 10.6-fold difference - unveiling the "diminished collaborative efficiency
caused by knowledge-base gaps" effect. This study demonstrates how carefully
designed multi-agent architectures can provide a viable pathway toward
autonomous scientific discovery in chemical engineering.

</details>


### [259] [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)
*Zarreen Reza*

Main category: cs.AI

TL;DR: This paper introduces a multi-agent debate framework to evaluate emergent social behaviors in LLM-based agents, revealing strong consensus-seeking tendencies and measurable psychometric profiles.


<details>
  <summary>Details</summary>
Motivation: Traditional evaluation benchmarks are insufficient for capturing emergent social and cognitive dynamics in autonomous LLM agents during interactive communication and collaboration.

Method: A multi-agent debate framework where LLM-based agents with distinct personas and incentives deliberate on challenging topics under LLM moderator supervision, analyzed using psychometric and semantic metrics.

Result: Agents consistently reach high semantic agreement (Î¼ > 0.88) without explicit instruction, personas induce stable psychometric profiles, and moderator personas significantly influence debate outcomes.

Conclusion: This work provides a blueprint for dynamic, psychometrically grounded evaluation protocols for understanding and shaping social behaviors in next-generation AI agents.

Abstract: As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

</details>


### [260] [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
*Yu Zeng,Wenxuan Huang,Shiting Huang,Xikun Bao,Yukun Qi,Yiming Zhao,Qiuchen Wang,Lin Chen,Zehui Chen,Huaian Chen,Wanli Ouyang,Feng Zhao*

Main category: cs.AI

TL;DR: AGILE is an agentic jigsaw interaction learning method that enhances visual perception and reasoning in VLMs through iterative environment interaction and code execution, achieving significant performance improvements on jigsaw tasks and general vision tasks.


<details>
  <summary>Details</summary>
Motivation: Current VLMs have limited fundamental perceptual and reasoning abilities, performing poorly on simple jigsaw tasks. The scarcity of high-quality vision-language data constrains improvement of these core capabilities.

Method: AGILE formulates jigsaw solving as an interactive process where the model generates executable code to perform actions based on current state, while the environment provides fine-grained visual feedback. This creates an iterative cycle of observation and interaction.

Result: AGILE boosts jigsaw task accuracy from 9.5% to 82.8% (2Ã2 setting) and shows strong generalization across 9 general vision tasks with average 3.1% improvement, indicating enhanced perceptual and reasoning abilities.

Conclusion: AGILE provides an efficient, scalable solution to multimodal data scarcity and opens new avenues for advancing reasoning and generalization in multimodal models through interactive learning.

Abstract: Although current large Vision-Language Models (VLMs) have advanced in
multimodal understanding and reasoning, their fundamental perceptual and
reasoning abilities remain limited. Specifically, even on simple jigsaw tasks,
existing VLMs perform near randomly, revealing deficiencies in core perception
and reasoning capabilities. While high-quality vision-language data can enhance
these capabilities, its scarcity and limited scalability impose significant
constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction
Learning for Enhancing visual perception and reasoning in VLMs. AGILE
formulates jigsaw solving as an interactive process, enabling the model to
progressively engage with the environment. At each step, the model generates
executable code to perform an action based on the current state, while the
environment provides fine-grained visual feedback to guide task completion.
Through this iterative cycle of observation and interaction, the model
incrementally improves its perceptual and reasoning capabilities via
exploration and feedback. Experimental results show that AGILE not only
substantially boosts performance on jigsaw tasks of varying complexity (e.g.,
increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also
demonstrates strong generalization across 9 general vision tasks, achieving an
average improvement of 3.1%. These results indicate notable enhancements in
both perceptual and reasoning abilities. This work opens a new avenue for
advancing reasoning and generalization in multimodal models and provides an
efficient, scalable solution to the scarcity of multimodal reinforcement
learning data. The code and datasets is available at
https://github.com/yuzeng0-0/AGILE .

</details>


### [261] [Aristotle: IMO-level Automated Theorem Proving](https://arxiv.org/abs/2510.01346)
*Tudor Achim,Alex Best,Kevin Der,MathÃ¯s FÃ©dÃ©rico,Sergei Gukov,Daniel Halpern-Leister,Kirsten Henningsgard,Yury Kudryashov,Alexander Meiburg,Martin Michelsen,Riley Patterson,Eric Rodriguez,Laura Scharff,Vikram Shanker,Vladmir Sicca,Hari Sowrirajan,Aidan Swope,Matyas Tamas,Vlad Tenev,Jonathan Thomm,Harold Williams,Lawrence Wu*

Main category: cs.AI

TL;DR: Aristotle is an AI system that combines formal verification and informal reasoning, achieving gold-medal-level performance on 2025 IMO problems through integration of Lean proof search, lemma generation/formalization, and a geometry solver.


<details>
  <summary>Details</summary>
Motivation: To advance automated theorem proving by combining the rigor of formal verification with the flexibility of informal reasoning, addressing complex mathematical problems at the level of International Mathematical Olympiad.

Method: Integrates three components: Lean proof search system for formal verification, informal reasoning system for generating and formalizing lemmas, and a dedicated geometry solver for geometric problems.

Result: Achieved gold-medal-equivalent performance on 2025 International Mathematical Olympiad problems, demonstrating state-of-the-art performance with favorable scaling properties.

Conclusion: The Aristotle system successfully bridges formal and informal reasoning approaches, showing that integrated AI systems can solve challenging mathematical problems at the highest competitive levels.

Abstract: We introduce Aristotle, an AI system that combines formal verification with
informal reasoning, achieving gold-medal-equivalent performance on the 2025
International Mathematical Olympiad problems. Aristotle integrates three main
components: a Lean proof search system, an informal reasoning system that
generates and formalizes lemmas, and a dedicated geometry solver. Our system
demonstrates state-of-the-art performance with favorable scaling properties for
automated theorem proving.

</details>


### [262] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK is a benchmark for evaluating long-term memory and state tracking in multi-platform agent environments, focusing on realistic organizational workflows with asynchronous events across platforms like Slack, Linear, and Git.


<details>
  <summary>Details</summary>
Motivation: Existing memory benchmarks focus on conversational instances, but there's a need for evaluating memory in dynamic enterprise environments for effective application.

Method: Created MEMTRACK benchmark with platform-interleaved timelines containing noisy, conflicting, cross-referring information. Dataset curated through manual expert design and agent-based synthesis based on real software development processes.

Result: Experiments show challenges in long-horizon memory utilization, cross-platform dependencies, and contradiction resolution. Best performing GPT-5 model achieved only 60% Correctness score.

Conclusion: MEMTRACK provides an extensible framework for advancing memory-augmented agent evaluation beyond conversational setups, enabling multi-agent, multi-platform memory benchmarking in complex organizational settings.

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [263] [Retrieval-Augmented Framework for LLM-Based Clinical Decision Support](https://arxiv.org/abs/2510.01363)
*Leon Garza,Anantaa Kotal,Michael A. Grasso,Emre Umucu*

Main category: cs.AI

TL;DR: This paper proposes a clinical decision support system using Large Language Models (LLMs) to assist prescribing clinicians by generating therapeutic suggestions from EHR data through a retrieval-augmented generation (RAG) pipeline.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of clinical decision-making and expansion of EHR data create opportunities for data-informed care, but also present challenges that need to be addressed with intelligent systems.

Method: The system uses a RAG pipeline that integrates natural language processing with structured clinical inputs, analyzing historical EHR data including patient demographics, symptoms, diagnostics, and treatment histories to generate contextually relevant recommendations.

Result: Preliminary evaluations with de-identified and synthetic clinical datasets show promising clinical plausibility and consistency in the model's outputs, suggesting LLM-based tools can provide valuable decision support in prescribing workflows.

Conclusion: LLM-based clinical decision support systems show potential for augmenting clinician judgment when appropriately constrained and validated, representing an initial step toward integrating generative AI into real-world clinical practice with emphasis on transparency and safety.

Abstract: The increasing complexity of clinical decision-making, alongside the rapid
expansion of electronic health records (EHR), presents both opportunities and
challenges for delivering data-informed care. This paper proposes a clinical
decision support system powered by Large Language Models (LLMs) to assist
prescribing clinicians. The system generates therapeutic suggestions by
analyzing historical EHR data, including patient demographics, presenting
complaints, clinical symptoms, diagnostic information, and treatment histories.
The framework integrates natural language processing with structured clinical
inputs to produce contextually relevant recommendations. Rather than replacing
clinician judgment, it is designed to augment decision-making by retrieving and
synthesizing precedent cases with comparable characteristics, drawing on local
datasets or federated sources where applicable. At its core, the system employs
a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured
narratives and codified data to support LLM-based inference. We outline the
system's technical components, including representation representation
alignment and generation strategies. Preliminary evaluations, conducted with
de-identified and synthetic clinical datasets, examine the clinical
plausibility and consistency of the model's outputs. Early findings suggest
that LLM-based tools may provide valuable decision support in prescribing
workflows when appropriately constrained and rigorously validated. This work
represents an initial step toward integration of generative AI into real-world
clinical decision-making with an emphasis on transparency, safety, and
alignment with established practices.

</details>


### [264] [Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367)
*Xinpeng Wang,Nitish Joshi,Barbara Plank,Rico Angell,He He*

Main category: cs.AI

TL;DR: TRACE detects implicit reward hacking by measuring how early a model's reasoning becomes sufficient to pass verification, identifying shortcuts through truncated reasoning analysis.


<details>
  <summary>Details</summary>
Motivation: Reward hacking poses a significant threat where models exploit loopholes in reward functions without solving intended tasks, with implicit hacking bypassing current CoT monitors.

Method: TRACE progressively truncates a model's chain-of-thought at various lengths, forces the model to answer, and measures verifier-passing rates at each cutoff to quantify reasoning effort.

Result: TRACE achieves over 65% gains over 72B CoT monitors in math reasoning and over 30% gains over 32B monitors in coding, and can discover unknown loopholes during training.

Conclusion: TRACE provides a scalable unsupervised approach for oversight where current monitoring methods are ineffective against implicit reward hacking.

Abstract: Reward hacking, where a reasoning model exploits loopholes in a reward
function to achieve high rewards without solving the intended task, poses a
significant threat. This behavior may be explicit, i.e. verbalized in the
model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus
bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE
(Truncated Reasoning AUC Evaluation). Our key observation is that hacking
occurs when exploiting the loophole is easier than solving the actual task.
This means that the model is using less `effort' than required to achieve high
reward. TRACE quantifies effort by measuring how early a model's reasoning
becomes sufficient to pass a verifier. We progressively truncate a model's CoT
at various lengths, force the model to answer, and measure the verifier-passing
rate at each cutoff. A hacking model, which takes a shortcut, will achieve a
high passing rate with only a small fraction of its CoT, yielding a large area
under the accuracy-vs-length curve. TRACE achieves over 65% gains over our
strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B
monitor in coding. We further show that TRACE can discover unknown loopholes
during training. Overall, TRACE offers a scalable unsupervised approach for
oversight where current monitoring methods prove ineffective.

</details>


### [265] [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
*Humaid Ibrahim,Nikolai Rozanov,Marek Rei*

Main category: cs.AI

TL;DR: A distillation pipeline that converts inference-time retrieval into learned competence, improving agent performance on multi-step tasks while reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: LLM agents frequently fail in predictable ways during multi-step tasks, and while RAG can help, it requires external knowledge databases and adds computational overhead at every deployment.

Method: Extracts compact hints from agent failures, uses hints to generate improved teacher trajectories via one-shot retrieval at episode start, and trains student models on these trajectories with hint strings removed to force internalization.

Result: Distilled students consistently outperform baseline agents, achieving 91% success on ALFWorld (vs. 79%) and 72 score on WebShop (vs. 61), while using 10-60% fewer tokens than retrieval-augmented teachers.

Conclusion: Retrieval benefits can be effectively internalized through targeted fine-tuning without permanent runtime dependencies, generalizing across model scales and agent architectures.

Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.

</details>


### [266] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu,Zaid Abulawi,Abhiram Garimidi,Doyeong Lim*

Main category: cs.AI

TL;DR: This paper proposes an automated data-driven modeling pipeline using LLM agents for regression tasks, achieving performance comparable to human-expert models while significantly reducing manual intervention.


<details>
  <summary>Details</summary>
Motivation: Traditional data-driven methods require extensive manual work and lack scalability. There's a need for automated approaches that can handle large engineering datasets effectively.

Method: Two LLM-agent frameworks: multi-agent system with specialized agents, and single-agent system based on ReAct paradigm. Both autonomously handle data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification.

Result: The LLM-agent-developed model outperforms traditional CHF lookup tables and achieves predictive accuracy and uncertainty quantification comparable to state-of-the-art Bayesian optimized deep neural networks developed by human experts.

Conclusion: LLM-based agents show significant potential for automating complex engineering modeling tasks, reducing human workload while meeting or exceeding existing performance standards.

Abstract: Modern engineering increasingly relies on vast datasets generated by
experiments and simulations, driving a growing demand for efficient, reliable,
and broadly applicable modeling strategies. There is also heightened interest
in developing data-driven approaches, particularly neural network models, for
effective prediction and analysis of scientific datasets. Traditional
data-driven methods frequently involve extensive manual intervention, limiting
their ability to scale effectively and generalize to diverse applications. In
this study, we propose an innovative pipeline utilizing Large Language Model
(LLM) agents to automate data-driven modeling and analysis, with a particular
emphasis on regression tasks. We evaluate two LLM-agent frameworks: a
multi-agent system featuring specialized collaborative agents, and a
single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both
frameworks autonomously handle data preprocessing, neural network development,
training, hyperparameter optimization, and uncertainty quantification (UQ). We
validate our approach using a critical heat flux (CHF) prediction benchmark,
involving approximately 25,000 experimental data points from the OECD/NEA
benchmark dataset. Results indicate that our LLM-agent-developed model
surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ
on par with state-of-the-art Bayesian optimized deep neural network models
developed by human experts. These outcomes underscore the significant potential
of LLM-based agents to automate complex engineering modeling tasks, greatly
reducing human workload while meeting or exceeding existing standards of
predictive performance.

</details>


### [267] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX is an AI agent that uses LLMs to transform raw system logs into ontology-based knowledge graphs, enabling structured CTI extraction and mapping to MITRE ATT&CK tactics.


<details>
  <summary>Details</summary>
Motivation: System logs contain valuable CTI but are limited by lack of structure, semantic inconsistency, and fragmentation, making it difficult to extract actionable intelligence.

Method: Integrates lightweight log ontology with RAG and iterative correction, uses LLMs to generate valid KGs, aggregates KGs into sessions, and predicts MITRE ATT&CK tactics.

Result: Demonstrated robust KG generation across multiple backends and accurate mapping to ATT&CK tactics on public benchmark and real-world honeypot datasets, with improved precision and recall through retrieval and correction.

Conclusion: Ontology-grounded representations with retrieval and correction are effective for extracting actionable CTI from logs, with code-oriented models showing particular strength in structured log analysis.

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [268] [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)
*Sipeng Zhang,Longfei Yun,Zilong Wang,Jingbo Shang,Letian Peng*

Main category: cs.AI

TL;DR: Falconer is a collaborative framework that combines LLMs' reasoning with lightweight proxy models for scalable knowledge mining, reducing costs by 90% and speeding up processing by 20x while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs are expensive for large-scale knowledge mining, while traditional pipelines are brittle and not generalizable. There's a need for a solution that balances efficiency and instruction-following capability.

Method: Falconer uses LLMs as planners (decomposing instructions) and annotators (generating supervision), training small proxy models to perform atomic operations (get label, get span) that unify classification and extraction tasks.

Result: Falconer matches state-of-the-art LLMs in accuracy while reducing inference cost by 90% and accelerating knowledge mining by more than 20x.

Conclusion: Falconer provides an efficient and scalable foundation for Deep Research by combining the strengths of LLMs and lightweight proxy models.

Abstract: At the core of Deep Research is knowledge mining, the task of extracting
structured information from massive unstructured text in response to user
instructions. Large language models (LLMs) excel at interpreting such
instructions but are prohibitively expensive to deploy at scale, while
traditional pipelines of classifiers and extractors remain efficient yet
brittle and unable to generalize to new tasks. We introduce Falconer, a
collaborative framework that combines the agentic reasoning of LLMs with
lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act
as planners, decomposing user instructions into executable pipelines, and as
annotators, generating supervision to train small proxies. The framework
unifies classification and extraction into two atomic operations, get label and
get span, enabling a single instruction-following model to replace multiple
task-specific components. To evaluate the consistency between proxy models
incubated by Falconer and annotations provided by humans and large models, we
construct new benchmarks covering both planning and end-to-end execution.
Experiments show that Falconer closely matches state-of-the-art LLMs in
instruction-following accuracy while reducing inference cost by up to 90% and
accelerating large-scale knowledge mining by more than 20x, offering an
efficient and scalable foundation for Deep Research.

</details>


### [269] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: This paper explores how expert-curated knowledge can enhance AI tutoring systems through explainable AI for automatic lesson generation and curriculum-based adaptive tutoring.


<details>
  <summary>Details</summary>
Motivation: To highlight the overlooked role of domain expert knowledge in creating effective AI tutoring systems and demonstrate its potential benefits.

Method: Proposes two approaches: 1) Using XAI techniques with expert-specified rules to automatically generate lessons, 2) Leveraging expert-designed curricula to develop adaptive tutoring systems with more efficient algorithms.

Result: Presents a case study on creating a pollinator identification tutoring system where expert knowledge can be easily elicited and applied.

Conclusion: Expert-curated knowledge plays a crucial role in developing novel educational systems, enabling automatic lesson generation and more efficient adaptive tutoring through XAI techniques and curriculum-based approaches.

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [270] [VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning](https://arxiv.org/abs/2510.01444)
*Rui Liu,Dian Yu,Tong Zheng,Runpeng Dai,Zongxia Li,Wenhao Yu,Zhenwen Liang,Linfeng Song,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.AI

TL;DR: VOGUE is a novel RL method that shifts exploration from text to visual space by treating images as stochastic contexts, using symmetric KL divergence to quantify visual uncertainty and guide exploration.


<details>
  <summary>Details</summary>
Motivation: Current RL methods for MLLMs treat visual input as fixed and deterministic, overlooking visual ambiguity and struggling to build policies robust to visual variations.

Method: VOGUE quantifies policy sensitivity to visual perturbations using symmetric KL divergence between raw and noisy image branches, creates uncertainty-proportional bonus for exploration, combines with token-entropy bonus and annealed sampling.

Result: VOGUE boosts pass@1 accuracy by 2.6% on visual math benchmarks and 3.7% on general reasoning benchmarks, improves pass@4 performance, and mitigates exploration decay in RL fine-tuning.

Conclusion: Grounding exploration in visual input uncertainty is an effective strategy for improving multimodal reasoning in MLLMs.

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves reasoning in
large language models (LLMs) but struggles with exploration, an issue that
still persists for multimodal LLMs (MLLMs). Current methods treat the visual
input as a fixed, deterministic condition, overlooking a critical source of
ambiguity and struggling to build policies robust to plausible visual
variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided
Exploration)}$, a novel method that shifts exploration from the output (text)
to the input (visual) space. By treating the image as a stochastic context,
VOGUE quantifies the policy's sensitivity to visual perturbations using the
symmetric KL divergence between a "raw" and "noisy" branch, creating a direct
signal for uncertainty-aware exploration. This signal shapes the learning
objective via an uncertainty-proportional bonus, which, combined with a
token-entropy bonus and an annealed sampling schedule, effectively balances
exploration and exploitation. Implemented within GRPO on two model scales
(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three
visual math benchmarks and 3.7% on three general-domain reasoning benchmarks,
while simultaneously increasing pass@4 performance and mitigating the
exploration decay commonly observed in RL fine-tuning. Our work shows that
grounding exploration in the inherent uncertainty of visual inputs is an
effective strategy for improving multimodal reasoning.

</details>


### [271] [AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance](https://arxiv.org/abs/2510.01474)
*Bill Marino,Rosco Hunter,Zubair Jamali,Marinos Emmanouil Kalpakos,Mudra Kashyap,Isaiah Hinton,Alexa Hanson,Maahum Nazir,Christoph Schnabl,Felix Steffek,Hongkai Wen,Nicholas D. Lane*

Main category: cs.AI

TL;DR: AIReg-Bench is the first benchmark dataset for evaluating LLMs' ability to assess compliance with the EU AI Act, created through LLM-generated technical documentation samples and expert legal annotations.


<details>
  <summary>Details</summary>
Motivation: As governments regulate AI, there's growing interest in using LLMs to assess AI system compliance with regulations, but no existing benchmarks to evaluate LLM performance on this task.

Method: Created 120 technical documentation excerpts through LLM prompting, then had legal experts review and annotate each sample for AI Act violations. Evaluated frontier LLMs' ability to reproduce expert compliance labels.

Result: Developed AIReg-Bench dataset with expert-annotated compliance assessments, providing a benchmark for evaluating LLM performance on AI regulation compliance assessment.

Conclusion: AIReg-Bench establishes a foundation for understanding LLM capabilities in AI regulation compliance assessment and provides a benchmark for future LLM development in this domain.

Abstract: As governments move to regulate AI, there is growing interest in using Large
Language Models (LLMs) to assess whether or not an AI system complies with a
given AI Regulation (AIR). However, there is presently no way to benchmark the
performance of LLMs at this task. To fill this void, we introduce AIReg-Bench:
the first benchmark dataset designed to test how well LLMs can assess
compliance with the EU AI Act (AIA). We created this dataset through a two-step
process: (1) by prompting an LLM with carefully structured instructions, we
generated 120 technical documentation excerpts (samples), each depicting a
fictional, albeit plausible, AI system - of the kind an AI provider might
produce to demonstrate their compliance with AIR; (2) legal experts then
reviewed and annotated each sample to indicate whether, and in what way, the AI
system described therein violates specific Articles of the AIA. The resulting
dataset, together with our evaluation of whether frontier LLMs can reproduce
the experts' compliance labels, provides a starting point to understand the
opportunities and limitations of LLM-based AIR compliance assessment tools and
establishes a benchmark against which subsequent LLMs can be compared. The
dataset and evaluation code are available at
https://github.com/camlsys/aireg-bench.

</details>


### [272] [Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates](https://arxiv.org/abs/2510.01500)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: Lateral Tree-of-Thoughts (LToT) addresses pathologies in large-budget search by separating utility from logical consistency, using lateral candidates for diversity and mainlines for exploitation, with capped racing to control costs.


<details>
  <summary>Details</summary>
Motivation: Standard Tree-of-Thoughts search suffers from breadth saturation (duplicate samples) and depth myopia (pruning long-term valuable branches) when given large test-time compute budgets.

Method: LToT splits the frontier into mainlines (high-utility) and laterals (consistent but low-utility), using Lateral Racing with Short-Circuit to cheaply probe laterals and promote promising branches.

Result: Theoretical analysis shows pseudolinear cost growth Î(Nâ log_Î· Nâ) for laterals vs exponential for mainlines, enabling principled diversity without compute inflation.

Conclusion: LToT transforms large test-time budgets into controlled diversity while mitigating saturation and myopia, maintaining promotion discipline with bounded computational costs.

Abstract: Modern deployments increasingly allocate large test-time compute (thousands
of tokens or many node expansions) to boost reliability. Under such budgets,
standard Tree-of-Thoughts-style search exhibits two pathologies: breadth
saturation (additional samples mostly produce near-duplicates, so width stops
growing) and depth myopia (noisy short-horizon utilities prune branches whose
payoff appears after a few more steps). We propose Lateral Tree-of-Thoughts
(LToT), a drop-in controller that separates utility from logical consistency
and treats low-utility but consistent candidates as assets rather than waste.
The frontier is split into mainlines (high-utility candidates used for
exploitation) and laterals (consistent, initially low-utility candidates that
receive short, cheap probes before judgment). LToT explores laterals via
Lateral Racing with Short-Circuit (LR--SC): a capped successive-halving race
that spreads tiny probes across a very wide lateral set, uses width-aware
thresholds with repeat-to-confirm, and immediately promotes a branch once its
envelope clears the mainline bar; mainlines are kept intentionally narrow so
surplus compute is invested where width is cheap. We prove a pseudolinear
lateral cost $\Theta(N_0 \log_{\eta} N_0)$ with logarithmically many rungs
(initial lateral width $N_0$; culling factor $\eta>1$), in contrast to the
exponential growth of uncapped mainlines. Empirical evaluations on benchmark
tasks are in preparation and will be added in a future revision. In short, LToT
turns large test-time budgets into principled diversity while preserving
promotion discipline, mitigating saturation and myopia without inflating
compute.

</details>


### [273] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: A method using sparse autoencoders and clustering to analyze LLM token representations and guide mathematical reasoning generations by balancing exploitation and exploration.


<details>
  <summary>Details</summary>
Motivation: To improve mathematical reasoning in LLMs by analyzing internal token representations and establishing a framework that balances following established reasoning patterns (exploitation) with exploring diverse reasoning paths (exploration).

Method: Train sparse autoencoders to generate sparse vector representations for tokens, apply k-means clustering to create a graph of token clusters with weighted edges representing sequential transitions, then use this graph to define reward functions for reasoning trajectories.

Result: The approach successfully identifies exploitative reasoning trajectories and measures generation diversity, showing that balancing exploitation and exploration is crucial for high accuracy in mathematical reasoning tasks.

Conclusion: Sparse autoencoders can serve as scalable reward models to guide LLM generations, ensuring a balanced trade-off between exploitation and exploration, which prevents extreme behaviors and fosters higher-quality reasoning processes.

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [274] [LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning](https://arxiv.org/abs/2510.01530)
*Navapat Nananukul,Yue Zhang,Ryan Lee,Eric Boxer,Jonathan May,Vibhav Giridhar Gogate,Jay Pujara,Mayank Kejriwal*

Main category: cs.AI

TL;DR: LOGicalThought (LogT) is a neurosymbolic architecture that combines LLMs with logical reasoning to handle defeasible logic in high-assurance domains, achieving 11.84% performance improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: High-assurance reasoning in domains like law and medicine requires accurate, verifiable conclusions grounded in evidence, but LLMs struggle with defeasible logic involving exceptions that can invalidate general rules.

Method: LogT uses an advanced logical language and reasoner with LLMs to create dual symbolic graph and logic-based contexts, transforming long-form guidelines into compact grounded evaluation.

Result: LogT improves overall performance by 11.84% across all LLMs, with significant gains in negation (+10.2%), implication (+13.2%), and defeasible reasoning (+5.5%) compared to strongest baseline.

Conclusion: The neurosymbolic approach effectively addresses challenges in high-assurance reasoning by combining LLMs' natural language processing with formal logical reasoning capabilities.

Abstract: High-assurance reasoning, particularly in critical domains such as law and
medicine, requires conclusions that are accurate, verifiable, and explicitly
grounded in evidence. This reasoning relies on premises codified from rules,
statutes, and contracts, inherently involving defeasible or non-monotonic logic
due to numerous exceptions, where the introduction of a single fact can
invalidate general rules, posing significant challenges. While large language
models (LLMs) excel at processing natural language, their capabilities in
standard inference tasks do not translate to the rigorous reasoning required
over high-assurance text guidelines. Core reasoning challenges within such
texts often manifest specific logical structures involving negation,
implication, and, most critically, defeasible rules and exceptions. In this
paper, we propose a novel neurosymbolically-grounded architecture called
LOGicalThought (LogT) that uses an advanced logical language and reasoner in
conjunction with an LLM to construct a dual symbolic graph context and
logic-based context. These two context representations transform the problem
from inference over long-form guidelines into a compact grounded evaluation.
Evaluated on four multi-domain benchmarks against four baselines, LogT improves
overall performance by 11.84% across all LLMs. Performance improves
significantly across all three modes of reasoning: by up to +10.2% on negation,
+13.2% on implication, and +5.5% on defeasible reasoning compared to the
strongest baseline.

</details>


### [275] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang,Tsung-Wei Ke*

Main category: cs.AI

TL;DR: InfoSeeker is an LLM decision-making framework that integrates task planning with active information seeking to handle uncertainty in both observations and environmental dynamics, achieving 74% performance gain over prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM planning agents address observational uncertainty but overlook discrepancies between their internal dynamics and actual environment, limiting their effectiveness in partially observable environments.

Method: InfoSeeker prompts LLMs to actively gather information by planning actions to validate understanding, detect environmental changes, or test hypotheses before generating or revising task-oriented plans.

Result: InfoSeeker achieves 74% absolute performance gain over prior methods without sacrificing sample efficiency, and generalizes well across LLMs and established benchmarks like robotic manipulation and web navigation.

Conclusion: Tight integration of planning and information seeking is crucial for robust behavior in partially observable environments, as demonstrated by InfoSeeker's superior performance.

Abstract: Explicit information seeking is essential to human problem-solving in
practical environments characterized by incomplete information and noisy
dynamics. When the true environmental state is not directly observable, humans
seek information to update their internal dynamics and inform future
decision-making. Although existing Large Language Model (LLM) planning agents
have addressed observational uncertainty, they often overlook discrepancies
between their internal dynamics and the actual environment. We introduce
Information Seeking Decision Planner (InfoSeeker), an LLM decision-making
framework that integrates task-oriented planning with information seeking to
align internal dynamics and make optimal decisions under uncertainty in both
agent observations and environmental dynamics. InfoSeeker prompts an LLM to
actively gather information by planning actions to validate its understanding,
detect environmental changes, or test hypotheses before generating or revising
task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark
suite featuring partially observable environments with incomplete observations
and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%
absolute performance gain over prior methods without sacrificing sample
efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms
baselines on established benchmarks such as robotic manipulation and web
navigation. These findings underscore the importance of tightly integrating
planning and information seeking for robust behavior in partially observable
environments. The project page is available at https://infoseekerllm.github.io

</details>


### [276] [Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models](https://arxiv.org/abs/2510.01544)
*Shaoan Xie,Lingjing Kong,Xiangchen Song,Xinshuai Dong,Guangyi Chen,Eric P. Xing,Kun Zhang*

Main category: cs.AI

TL;DR: SAPO is a new RL algorithm that improves diffusion language models for complex reasoning by using process-based rewards to guide structured reasoning paths, addressing the issue of unstructured refinement in existing methods.


<details>
  <summary>Details</summary>
Motivation: Current RL approaches for diffusion language models rely on sparse outcome-based rewards, which can reinforce flawed reasoning paths that happen to lead to correct answers. This creates a mismatch with the natural hierarchical structure of reasoning.

Method: Step-Aware Policy Optimization (SAPO) uses a process-based reward function that encourages incremental progress, aligning the denoising process with latent reasoning hierarchy. It formalizes complex problem solving as hierarchical selection with localized logical steps.

Result: Empirical results show SAPO significantly improves performance on challenging reasoning benchmarks and enhances interpretability of the generation process compared to existing methods.

Conclusion: The principled approach of SAPO successfully addresses the unstructured refinement problem in diffusion language models, enabling more structured and coherent reasoning paths through process-based reinforcement learning.

Abstract: Diffusion language models (dLLMs) offer a promising, non-autoregressive
paradigm for text generation, yet training them for complex reasoning remains a
key challenge. Current reinforcement learning approaches often rely on sparse,
outcome-based rewards, which can reinforce flawed reasoning paths that lead to
coincidentally correct answers. We argue that this stems from a fundamental
mismatch with the natural structure of reasoning. We first propose a
theoretical framework that formalizes complex problem solving as a hierarchical
selection process, where an intractable global constraint is decomposed into a
series of simpler, localized logical steps. This framework provides a
principled foundation for algorithm design, including theoretical insights into
the identifiability of this latent reasoning structure. Motivated by this
theory, we identify unstructured refinement -- a failure mode where a model's
iterative steps do not contribute meaningfully to the solution -- as a core
deficiency in existing methods. We then introduce Step-Aware Policy
Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising
process with the latent reasoning hierarchy. By using a process-based reward
function that encourages incremental progress, SAPO guides the model to learn
structured, coherent reasoning paths. Our empirical results show that this
principled approach significantly improves performance on challenging reasoning
benchmarks and enhances the interpretability of the generation process.

</details>


### [277] [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
*Yubin Kim,Taehan Kim,Eugene Park,Chunjong Park,Cynthia Breazeal,Daniel McDuff,Hae Won Park*

Main category: cs.AI

TL;DR: InvThink is a safety alignment method that enables LLMs to perform inverse thinking by reasoning through potential failure modes before generating responses, achieving better safety scaling and preserving general capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment methods directly optimize for safe responses but may compromise general reasoning capabilities. The motivation is to develop a method that proactively considers potential harms while maintaining model performance.

Method: Instructs models to: 1) enumerate potential harms, 2) analyze their consequences, and 3) generate safe outputs that avoid these risks. Implemented via supervised fine-tuning and reinforcement learning across three LLM families.

Result: Achieves up to 15.7% reduction in harmful responses compared to baseline methods like SafetyPrompt. Shows stronger safety scaling with model size and mitigates safety tax by preserving reasoning capabilities on standard benchmarks. Excels in high-stakes domains including medicine, finance, law, and agentic risk scenarios.

Conclusion: Inverse reasoning provides a scalable and generalizable path toward safer, more capable language models by systematically considering failure modes before response generation.

Abstract: We present InvThink, a simple yet powerful approach that gives large language
models (LLMs) the capability of inverse thinking: reasoning through failure
modes before generating responses. Unlike existing safety alignment methods
that optimize directly for safe response, InvThink instructs models to 1)
enumerate potential harms, 2) analyze their consequences, and 3) generate safe
outputs that proactively avoid these risks. Our method reveals three key
findings: (i) safety improvements show stronger scaling with model size
compared to existing safety methods. (ii) InvThink mitigates safety tax; by
training models to systematically consider failure modes, it preserves general
reasoning capabilities on standard benchmarks. (iii) beyond general safety
tasks, InvThink excels in high-stakes domains including external-facing
(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,
achieving up to 15.7% reduction in harmful responses compared to baseline
methods like SafetyPrompt. We further implement InvThink via supervised
fine-tuning, and reinforcement learning across three LLM families. These
results suggest that inverse reasoning provides a scalable and generalizable
path toward safer, more capable language models.

</details>


### [278] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan,Yiting Zhang,Zhuo Liu,Yolo Yunlong Tang,Zeliang Zhang,Haozheng Luo,Yuwei Han,Jianshu Zhang,Dennis Wu,Hong-Yu Chen,Haoran Lu,Haoyang Fang,Manling Li,Chenliang Xu,Philip S. Yu,Han Liu*

Main category: cs.AI

TL;DR: AdvEvo-MARL is a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents through adversarial training, eliminating the need for external guard modules while maintaining task performance.


<details>
  <summary>Details</summary>
Motivation: Existing defenses for LLM-based multi-agent systems either use self-verification (which underperforms due to limited detection capacity) or external guards (which create single-point-of-failure risks and increase system overhead).

Method: Jointly optimizes attackers (synthesizing jailbreak prompts) and defenders (task agents trained to accomplish duties while resisting attacks) in adversarial learning environments, with a public baseline for advantage estimation to stabilize learning and foster cooperation.

Result: AdvEvo-MARL consistently keeps attack-success rate below 20% (vs. 38.33% for baselines) while preserving and sometimes improving task accuracy (up to +3.67% on reasoning tasks).

Conclusion: Safety and utility can be jointly improved without relying on extra guard agents or added system overhead through internalized safety mechanisms.

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role
coordination, but their openness and interaction complexity also expose them to
jailbreak, prompt-injection, and adversarial collaboration. Existing defenses
fall into two lines: (i) self-verification that asks each agent to pre-filter
unsafe instructions before execution, and (ii) external guard modules that
police behaviors. The former often underperforms because a standalone agent
lacks sufficient capacity to detect cross-agent unsafe chains and
delegation-induced risks; the latter increases system overhead and creates a
single-point-of-failure-once compromised, system-wide safety collapses, and
adding more guards worsens cost and complexity. To solve these challenges, we
propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning
framework that internalizes safety into task agents. Rather than relying on
external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize
evolving jailbreak prompts) and defenders (task agents trained to both
accomplish their duties and resist attacks) in adversarial learning
environments. To stabilize learning and foster cooperation, we introduce a
public baseline for advantage estimation: agents within the same functional
group share a group-level mean-return baseline, enabling lower-variance updates
and stronger intra-group coordination. Across representative attack scenarios,
AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas
baselines reach up to 38.33%, while preserving-and sometimes improving-task
accuracy (up to +3.67% on reasoning tasks). These results show that safety and
utility can be jointly improved without relying on extra guard agents or added
system overhead.

</details>


### [279] [AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence](https://arxiv.org/abs/2510.01609)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.AI

TL;DR: AgentRec is a multi-agent LLM framework for conversational recommendation that uses specialized agents coordinated through adaptive weighting to handle dynamic preferences, maintain coherence, and balance ranking objectives, achieving significant improvements over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing conversational recommender systems struggle with dynamic user preferences, conversation coherence, and balancing multiple ranking objectives simultaneously.

Method: Hierarchical multi-agent framework with specialized LLM-powered agents for conversation understanding, preference modeling, context awareness, and dynamic ranking, coordinated through adaptive weighting mechanism and three-tier learning strategy.

Result: 2.8% improvement in conversation success rate, 1.9% improvement in recommendation accuracy (NDCG@10), and 3.2% better conversation efficiency while maintaining comparable computational costs.

Conclusion: AgentRec demonstrates that multi-agent collaborative frameworks with adaptive intelligence can effectively address key challenges in conversational recommendation systems.

Abstract: Interactive conversational recommender systems have gained significant
attention for their ability to capture user preferences through natural
language interactions. However, existing approaches face substantial challenges
in handling dynamic user preferences, maintaining conversation coherence, and
balancing multiple ranking objectives simultaneously. This paper introduces
AgentRec, a next-generation LLM-powered multi-agent collaborative
recommendation framework that addresses these limitations through hierarchical
agent networks with adaptive intelligence. Our approach employs specialized
LLM-powered agents for conversation understanding, preference modeling, context
awareness, and dynamic ranking, coordinated through an adaptive weighting
mechanism that learns from interaction patterns. We propose a three-tier
learning strategy combining rapid response for simple queries, intelligent
reasoning for complex preferences, and deep collaboration for challenging
scenarios. Extensive experiments on three real-world datasets demonstrate that
AgentRec achieves consistent improvements over state-of-the-art baselines, with
2.8\% enhancement in conversation success rate, 1.9\% improvement in
recommendation accuracy (NDCG@10), and 3.2\% better conversation efficiency
while maintaining comparable computational costs through intelligent agent
coordination.

</details>


### [280] [PychoBench: Evaluating the Psychology Intelligence of Large Language Models](https://arxiv.org/abs/2510.01611)
*Min Zeng*

Main category: cs.AI

TL;DR: This paper introduces PsychoBench, a benchmark based on US national counselor certification exams to evaluate if LLMs can qualify as psychological counselors by testing their psychological knowledge.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can be effectively applied to psychological counseling by assessing if they meet the qualification standards required for human counselors.

Method: Created PsychoBench with 2,252 single-choice questions from US national counselor examinations covering various psychology sub-disciplines, requiring about 70% accuracy to pass.

Result: Advanced models like GPT-4o, Llama3.3-70B, and Gemma3-27B achieved well above the passing threshold, while smaller models like Qwen2.5-7B and Mistral-7B remained far below.

Conclusion: Only frontier LLMs currently meet counseling exam standards, highlighting both the promise and challenges of developing psychology-oriented LLMs.

Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of industries, primarily due to their impressive generative
abilities. Yet, their potential in applications requiring cognitive abilities,
such as psychological counseling, remains largely untapped. This paper
investigates the key question: Can LLMs be effectively applied to psychological
counseling? To determine whether an LLM can effectively take on the role of a
psychological counselor, the first step is to assess whether it meets the
qualifications required for such a role, namely the ability to pass the U.S.
National Counselor Certification Exam (NCE). This is because, just as a human
counselor must pass a certification exam to practice, an LLM must demonstrate
sufficient psychological knowledge to meet the standards required for such a
role. To address this, we introduce PsychoBench, a benchmark grounded in
U.S.national counselor examinations, a licensure test for professional
counselors that requires about 70% accuracy to pass. PsychoBench comprises
approximately 2,252 carefully curated single-choice questions, crafted to
require deep understanding and broad enough to cover various sub-disciplines of
psychology. This benchmark provides a comprehensive assessment of an LLM's
ability to function as a counselor. Our evaluation shows that advanced models
such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing
threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)
remain far below it. These results suggest that only frontier LLMs are
currently capable of meeting counseling exam standards, highlighting both the
promise and the challenges of developing psychology-oriented LLMs.

</details>


### [281] [Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CDMPs](https://arxiv.org/abs/2510.01620)
*Peidong Liu,Junjiang Lin,Shaowen Wang,Yao Xu,Haiqing Li,Xuhao Xie,Siyi Wu,Hao Li*

Main category: cs.AI

TL;DR: LLM-based summarization for CMDPs compresses high-dimensional contexts into low-dimensional summaries, improving efficiency and performance in sequential decision-making.


<details>
  <summary>Details</summary>
Motivation: Existing CMDP methods struggle with high-dimensional contexts, leading to poor generalization, high computation, and unstable performance.

Method: Information-theoretic approach using LLMs to compress contextual inputs into semantically rich summaries that augment states while preserving decision-critical information.

Result: Outperforms raw-context and non-context baselines across benchmarks, improving reward, success rate, sample efficiency while reducing latency and memory usage.

Conclusion: LLM-based summarization provides scalable and interpretable solution for efficient decision-making in context-rich, resource-constrained environments.

Abstract: Contextual Markov Decision Processes (CMDPs) offer a framework for sequential
decision-making under external signals, but existing methods often fail to
generalize in high-dimensional or unstructured contexts, resulting in excessive
computation and unstable performance. We propose an information-theoretic
summarization approach that uses large language models (LLMs) to compress
contextual inputs into low-dimensional, semantically rich summaries. These
summaries augment states by preserving decision-critical cues while reducing
redundancy. Building on the notion of approximate context sufficiency, we
provide, to our knowledge, the first regret bounds and a latency-entropy
trade-off characterization for CMDPs. Our analysis clarifies how
informativeness impacts computational cost. Experiments across discrete,
continuous, visual, and recommendation benchmarks show that our method
outperforms raw-context and non-context baselines, improving reward, success
rate, and sample efficiency, while reducing latency and memory usage. These
findings demonstrate that LLM-based summarization offers a scalable and
interpretable solution for efficient decision-making in context-rich,
resource-constrained environments.

</details>


### [282] [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
*Erfan Shayegani,Keegan Hines,Yue Dong,Nael Abu-Ghazaleh,Roman Lutz,Spencer Whitehead,Vidhisha Balachandran,Besmira Nushi,Vibhav Vineet*

Main category: cs.AI

TL;DR: Computer-Use Agents (CUAs) exhibit Blind Goal-Directedness (BGD) - pursuing goals regardless of feasibility, safety, or context. The paper introduces BLIND-ACT benchmark to evaluate BGD patterns and finds high BGD rates (80.8%) across frontier models, highlighting safety risks.


<details>
  <summary>Details</summary>
Motivation: To identify and characterize the systematic bias in Computer-Use Agents (CUAs) where they pursue user goals without proper consideration of feasibility, safety, reliability, or context, exposing subtle but significant risks.

Method: Developed BLIND-ACT benchmark with 90 tasks capturing three BGD patterns: lack of contextual reasoning, assumptions under ambiguity, and contradictory goals. Evaluated nine frontier models using LLM-based judges on OSWorld environments.

Result: High average BGD rate of 80.8% across all tested models. Prompting-based interventions reduced BGD but substantial risk persisted. Identified failure modes: execution-first bias, thought-action disconnect, and request-primacy.

Conclusion: BGD represents a fundamental risk in CUAs that persists despite interventions. BLIND-ACT provides foundation for future research on studying and mitigating this risk to ensure safe CUA deployment.

Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that
take actions on GUIs to accomplish user goals. In this paper, we show that CUAs
consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals
regardless of feasibility, safety, reliability, or context. We characterize
three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)
assumptions and decisions under ambiguity, and (iii) contradictory or
infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these
three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and
employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement
with human annotations. We use BLIND-ACT to evaluate nine frontier models,
including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing
high average BGD rates (80.8%) across them. We show that BGD exposes subtle
risks that arise even when inputs are not directly harmful. While
prompting-based interventions lower BGD levels, substantial risk persists,
highlighting the need for stronger training- or inference-time interventions.
Qualitative analysis reveals observed failure modes: execution-first bias
(focusing on how to act over whether to act), thought-action disconnect
(execution diverging from reasoning), and request-primacy (justifying actions
due to user request). Identifying BGD and introducing BLIND-ACT establishes a
foundation for future research on studying and mitigating this fundamental risk
and ensuring safe CUA deployment.

</details>


### [283] [Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective](https://arxiv.org/abs/2510.01639)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: LLMs can effectively read road network maps and perform navigation tasks through trajectory recovery, demonstrating strong geospatial reasoning capabilities without external tools.


<details>
  <summary>Details</summary>
Motivation: To explore whether Large Language Models can understand and reason about geospatial information, specifically road networks, for navigation purposes.

Method: Framed trajectory recovery as a proxy task using GLOBALTRACE dataset with 4,000+ real trajectories, employing a prompting framework that uses road network as context for LLMs to reconstruct masked GPS traces.

Result: LLMs outperformed baseline models and specialized trajectory recovery models, showing strong zero-shot generalization and comprehension of road networks and coordinate systems, though with systematic regional and transportation mode biases.

Conclusion: LLMs demonstrate promising capabilities for geospatial reasoning and can enhance navigation experiences by flexibly incorporating user preferences through map reasoning.

Abstract: We explore the geospatial reasoning capabilities of Large Language Models
(LLMs), specifically, whether LLMs can read road network maps and perform
navigation. We frame trajectory recovery as a proxy task, which requires models
to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with
over 4,000 real-world trajectories across diverse regions and transportation
modes. Using road network as context, our prompting framework enables LLMs to
generate valid paths without accessing any external navigation tools.
Experiments show that LLMs outperform off-the-shelf baselines and specialized
trajectory recovery models, with strong zero-shot generalization. Fine-grained
analysis shows that LLMs have strong comprehension of the road network and
coordinate systems, but also pose systematic biases with respect to regions and
transportation modes. Finally, we demonstrate how LLMs can enhance navigation
experiences by reasoning over maps in flexible ways to incorporate user
preferences.

</details>


### [284] [UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models](https://arxiv.org/abs/2510.02194)
*Yuhao Sun,Zhuoer Xu,Shiwen Cui,Kun Yang,Lingyun Yu,Yongdong Zhang,Hongtao Xie*

Main category: cs.AI

TL;DR: UpSafeÂ°C is a unified framework that enhances LLM safety through safety-aware upcycling, converting safety-critical layers into sparse Mixture-of-Experts with safety experts and introducing safety temperature for inference-time control.


<details>
  <summary>Details</summary>
Motivation: LLMs remain vulnerable to safety risks like harmful content generation and jailbreak attacks, while existing safety techniques face limitations in balancing safety, utility, and controllability.

Method: Identify safety-critical layers and upcycle them into sparse MoE structure with safety experts; use two-stage SFT strategy to strengthen safety discrimination; introduce safety temperature mechanism for dynamic inference-time control.

Result: Achieves robust safety improvements against harmful and jailbreak inputs while maintaining competitive performance on general tasks; safety temperature provides fine-grained inference-time control achieving Pareto-optimal frontier between utility and safety.

Conclusion: Highlights a new direction for LLM safety: moving from static alignment toward dynamic, modular, and inference-aware control.

Abstract: Large Language Models (LLMs) have achieved remarkable progress across a wide
range of tasks, but remain vulnerable to safety risks such as harmful content
generation and jailbreak attacks. Existing safety techniques -- including
external guardrails, inference-time guidance, and post-training alignment --
each face limitations in balancing safety, utility, and controllability. In
this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM
safety through safety-aware upcycling. Our approach first identifies
safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)
structure, where the router acts as a soft guardrail that selectively activates
original MLPs and added safety experts. We further introduce a two-stage SFT
strategy to strengthen safety discrimination while preserving general
capabilities. To enable flexible control at inference time, we introduce a
safety temperature mechanism, allowing dynamic adjustment of the trade-off
between safety and utility. Experiments across multiple benchmarks, base model,
and model scales demonstrate that UpSafe$^\circ$C achieves robust safety
improvements against harmful and jailbreak inputs, while maintaining
competitive performance on general tasks. Moreover, analysis shows that safety
temperature provides fine-grained inference-time control that achieves the
Pareto-optimal frontier between utility and safety. Our results highlight a new
direction for LLM safety: moving from static alignment toward dynamic, modular,
and inference-aware control.

</details>


### [285] [GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents](https://arxiv.org/abs/2510.01664)
*Yejin Kim,Youngbin Lee,Juhyeong Kim,Yongjae Lee*

Main category: cs.AI

TL;DR: GuruAgents are AI agents that operationalize legendary investment gurus' strategies through prompt engineering, achieving up to 42.2% CAGR in backtesting.


<details>
  <summary>Details</summary>
Motivation: To translate qualitative investment philosophies of legendary gurus into reproducible quantitative strategies using AI agents.

Method: Developed five distinct GuruAgents by encoding iconic investors' philosophies into LLM prompts integrated with financial tools and deterministic reasoning pipeline.

Result: In NASDAQ-100 backtest (Q4 2023-Q2 2025), Buffett GuruAgent achieved highest performance with 42.2% CAGR, significantly outperforming benchmarks; other agents showed varied results.

Conclusion: Prompt engineering can successfully translate investment gurus' qualitative philosophies into reproducible quantitative strategies, opening new direction for automated systematic investing.

Abstract: This study demonstrates that GuruAgents, prompt-guided AI agents, can
systematically operationalize the strategies of legendary investment gurus. We
develop five distinct GuruAgents, each designed to emulate an iconic investor,
by encoding their distinct philosophies into LLM prompts that integrate
financial tools and a deterministic reasoning pipeline. In a backtest on
NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique
behaviors driven by their prompted personas. The Buffett GuruAgent achieves the
highest performance, delivering a 42.2\% CAGR that significantly outperforms
benchmarks, while other agents show varied results. These findings confirm that
prompt engineering can successfully translate the qualitative philosophies of
investment gurus into reproducible, quantitative strategies, highlighting a
novel direction for automated systematic investing. The source code and data
are available at https://github.com/yejining99/GuruAgents.

</details>


### [286] [A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation](https://arxiv.org/abs/2510.01671)
*Motoki Sato,Yuki Matsushita,Hidekazu Takahashi,Tomoaki Kakazu,Sou Nagata,Mizuho Ohnuma,Atsushi Yoshikawa,Masayuki Yamamura*

Main category: cs.AI

TL;DR: LENOHA is a safety-first clinical query system that uses high-precision classification to retrieve verbatim FAQ answers instead of generating text, achieving near-perfect accuracy with minimal energy consumption.


<details>
  <summary>Details</summary>
Motivation: Address unmet patient pre-procedural questions while overcoming time constraints, privacy concerns, and avoiding AI hallucinations in clinical settings.

Method: Uses sentence-transformer classifier to route inputs and return verbatim answers from clinician-curated FAQ, eliminating free-text generation in clinical path. Evaluated on tooth extraction and gastroscopy domains with expert-reviewed datasets.

Result: E5-large-instruct achieved 0.983 accuracy, 0.996 AUC with only 7 errors, comparable to GPT-4o. Non-generative path consumes ~1.0 mWh vs ~168 mWh for small-talk generation (170x difference) with ~0.10s latency.

Conclusion: Verbating FAQ answers structurally avoids generation errors while supporting privacy, sustainability, and equitable deployment in bandwidth-limited clinical environments.

Abstract: Patients awaiting invasive procedures often have unanswered pre-procedural
questions; however, time-pressured workflows and privacy constraints limit
personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave
No One Behind Architecture), a safety-first, local-first system that routes
inputs with a high-precision sentence-transformer classifier and returns
verbatim answers from a clinician-curated FAQ for clinical queries, eliminating
free-text generation in the clinical path. We evaluated two domains (tooth
extraction and gastroscopy) using expert-reviewed validation sets
(n=400/domain) for thresholding and independent test sets (n=200/domain). Among
the four encoders, E5-large-instruct (560M) achieved an overall accuracy of
0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were
statistically indistinguishable from GPT-4o on this task; Gemini made no errors
on this test set. Energy logging shows that the non-generative clinical path
consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local
8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single
on-prem GPU. These results indicate that near-frontier discrimination and
generation-induced errors are structurally avoided in the clinical path by
returning vetted FAQ answers verbatim, supporting privacy, sustainability, and
equitable deployment in bandwidth-limited environments.

</details>


### [287] [Improving AGI Evaluation: A Data Science Perspective](https://arxiv.org/abs/2510.01687)
*John Hawkins*

Main category: cs.AI

TL;DR: The paper argues that current AGI evaluation methods based on synthetic tasks are flawed and proposes an alternative approach focused on robust task execution and deployment competence.


<details>
  <summary>Details</summary>
Motivation: Current AGI evaluation methods are inadequate because they rely on synthetic tasks based on human intuitions about intelligence, which have historically performed poorly in AI development.

Method: The paper proposes shifting from synthetic task evaluation to a data science-inspired approach that evaluates robust task execution and deployment competence.

Result: The authors develop practical examples of what this alternative AGI evaluation paradigm would look like in practice.

Conclusion: AGI evaluation should move away from synthetic tasks and instead focus on demonstrating reliable deployment competence through robust task execution.

Abstract: Evaluation of potential AGI systems and methods is difficult due to the
breadth of the engineering goal. We have no methods for perfect evaluation of
the end state, and instead measure performance on small tests designed to
provide directional indication that we are approaching AGI. In this work we
argue that AGI evaluation methods have been dominated by a design philosophy
that uses our intuitions of what intelligence is to create synthetic tasks,
that have performed poorly in the history of AI. Instead we argue for an
alternative design philosophy focused on evaluating robust task execution that
seeks to demonstrate AGI through competence. This perspective is developed from
common practices in data science that are used to show that a system can be
reliably deployed. We provide practical examples of what this would mean for
AGI evaluation.

</details>


### [288] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: VaPR introduces a hard-negative response generation framework using LLM-guided editing to create rejected responses with targeted errors while maintaining stylistic/length similarity to accepted responses, addressing noise in synthetic preference annotations.


<details>
  <summary>Details</summary>
Motivation: Existing preference finetuning methods overlook noise in synthetic preference annotations, particularly stylistic and length biases that affect alignment quality.

Method: Developed a hard-negative response generation framework using LLM-guided response editing to create rejected responses with targeted errors, maintaining stylistic and length similarity to accepted responses. Created VaPR dataset with 30K samples for finetuning LVLMs.

Result: VaPR models achieved significant performance improvements: 6.5% average gain for LLaVA, 4.0% for Qwen2VL, and 1.5% for Qwen2.5VL across ten benchmarks, with notable improvements on reasoning tasks. Also reduced tendency to answer "Yes" in binary questions.

Conclusion: The framework effectively addresses noise in synthetic preference data and generalizes well, with open-source LLM editors achieving ~99% performance of GPT-4o-based synthesis. Performance scales with data size and benefits even smaller models.

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [289] [MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs](https://arxiv.org/abs/2510.01724)
*Madina Bekbergenova,Lucas Pradi,Benjamin Navet,Emma Tysinger,Franck Michel,Matthieu Feraud,Yousouf Taghzouti,Yan Zhou Chen,Olivier Kirchhoffer,Florence Mehl,Martin Legrand,Tao Jiang,Marco Pagni,Soha Hassoun,Jean-Luc Wolfender,Wout Bittremieux,Fabien Gandon,Louis-FÃ©lix Nothias*

Main category: cs.AI

TL;DR: MetaboT is an AI system using LLMs and multi-agent architecture to translate natural language questions into SPARQL queries for metabolomics knowledge graphs, achieving 83.67% accuracy compared to 8.16% for standard LLM baseline.


<details>
  <summary>Details</summary>
Motivation: Knowledge graphs help structure mass spectrometry metabolomics data but require deep understanding of ontology and query language syntax, creating barriers for researchers.

Method: Multi-agent system using LangChain/LangGraph with specialized agents: Entry Agent, Validator Agent, Supervisor Agent, Knowledge Graph Agent, and SPARQL query generator. Agents handle query validation, chemical conversions, identifier extraction, and query generation.

Result: Achieved 83.67% accuracy on curated metabolomics questions, significantly outperforming standard LLM baseline (8.16%). Successfully bridges gap between complex semantic technologies and user-friendly interaction.

Conclusion: MetaboT effectively automates SPARQL query generation from natural language, removing technical barriers to knowledge graph access while maintaining domain-specific standards, facilitating data-driven discoveries in metabolomics.

Abstract: Mass spectrometry metabolomics generates vast amounts of data requiring
advanced methods for interpretation. Knowledge graphs address these challenges
by structuring mass spectrometry data, metabolite information, and their
relationships into a connected network (Gaudry et al. 2024). However, effective
use of a knowledge graph demands an in-depth understanding of its ontology and
its query language syntax. To overcome this, we designed MetaboT, an AI system
utilizing large language models (LLMs) to translate user questions into SPARQL
semantic query language for operating on knowledge graphs (Steve Harris 2013).
We demonstrate its effectiveness using the Experimental Natural Products
Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural
products (Gaudry et al. 2024).MetaboT employs specialized AI agents for
handling user queries and interacting with the knowledge graph by breaking down
complex tasks into discrete components, each managed by a specialised agent
(Fig. 1a). The multi-agent system is constructed using the LangChain and
LangGraph libraries, which facilitate the integration of LLMs with external
tools and information sources (LangChain, n.d.). The query generation process
follows a structured workflow. First, the Entry Agent determines if the
question is new or a follow-up to previous interactions. New questions are
forwarded to the Validator Agent, which verifies if the question is related to
the knowledge graph. Then, the valid question is sent to the Supervisor Agent,
which identifies if the question requires chemical conversions or standardized
identifiers. In this case it delegates the question to the Knowledge Graph
Agent, which can use tools to extract necessary details, such as URIs or
taxonomies of chemical names, from the user query. Finally, an agent
responsible for crafting the SPARQL queries equipped with the ontology of the
knowledge graph uses the provided identifiers to generate the query. Then, the
system executes the generated query against the metabolomics knowledge graph
and returns structured results to the user (Fig. 1b). To assess the performance
of MetaboT we have curated 50 metabolomics-related questions and their expected
answers. In addition to submitting these questions to MetaboT, we evaluated a
baseline by submitting them to a standard LLM (GPT-4o) with a prompt that
incorporated the knowledge graph ontology but did not provide specific entity
IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,
underscoring the necessity of our multi-agent system for accurately retrieving
entities and generating correct SPARQL queries. MetaboT demonstrates promising
performance as a conversational question-answering assistant, enabling
researchers to retrieve structured metabolomics data through natural language
queries. By automating the generation and execution of SPARQL queries, it
removes technical barriers that have traditionally hindered access to knowledge
graphs. Importantly, MetaboT leverages the capabilities of LLMs while
maintaining experimentally grounded query generation, ensuring that outputs
remain aligned with domain-specific standards and data structures. This
approach facilitates data-driven discoveries by bridging the gap between
complex semantic technologies and user-friendly interaction. MetaboT is
accessible at [https://metabot.holobiomicslab.eu/], and its source code is
available at [https://github.com/HolobiomicsLab/MetaboT].

</details>


### [290] [A cybersecurity AI agent selection and decision support framework](https://arxiv.org/abs/2510.01751)
*Masike Malatji*

Main category: cs.AI

TL;DR: A structured decision support framework that aligns AI agent architectures with NIST Cybersecurity Framework 2.0, providing methodology for AI solution deployment in cybersecurity.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between theoretical AI constructs and operational cybersecurity demands by systematically integrating AI agent architectures with industry-standard cybersecurity frameworks.

Method: Granular decomposition of NIST CSF 2.0 functions into specific tasks, linking AI agent properties to security requirements, and outlining graduated autonomy levels for different cybersecurity maturity stages.

Result: Conceptual validation shows the framework enables tailored AI deployments that enhance situational awareness, accelerate response times, and strengthen long-term resilience through adaptive risk management.

Conclusion: The research establishes a foundation for robust, empirically validated multi-agent systems that adhere to industry standards, bridging AI theory with practical cybersecurity needs.

Abstract: This paper presents a novel, structured decision support framework that
systematically aligns diverse artificial intelligence (AI) agent architectures,
reactive, cognitive, hybrid, and learning, with the comprehensive National
Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.
By integrating agent theory with industry guidelines, this framework provides a
transparent and stepwise methodology for selecting and deploying AI solutions
to address contemporary cyber threats. Employing a granular decomposition of
NIST CSF 2.0 functions into specific tasks, the study links essential AI agent
properties such as autonomy, adaptive learning, and real-time responsiveness to
each subcategory's security requirements. In addition, it outlines graduated
levels of autonomy (assisted, augmented, and fully autonomous) to accommodate
organisations at varying stages of cybersecurity maturity. This holistic
approach transcends isolated AI applications, providing a unified detection,
incident response, and governance strategy. Through conceptual validation, the
framework demonstrates how tailored AI agent deployments can align with
real-world constraints and risk profiles, enhancing situational awareness,
accelerating response times, and fortifying long-term resilience via adaptive
risk management. Ultimately, this research bridges the gap between theoretical
AI constructs and operational cybersecurity demands, establishing a foundation
for robust, empirically validated multi-agent systems that adhere to industry
standards.

</details>


### [291] [REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing](https://arxiv.org/abs/2510.01800)
*Thanh Ma,Tri-Tam La,Lam-Thu Le Huu,Minh-Nghi Nguyen,Khanh-Van Pham Luu,Huu-Hoa Nguyen*

Main category: cs.AI

TL;DR: REBot is an LLM-enhanced advisory chatbot for academic regulation advising, using CatRAG framework that combines retrieval-augmented generation with graph-based reasoning to achieve 98.89% F1 score.


<details>
  <summary>Details</summary>
Motivation: Building effective academic regulation advising systems requires domain-specific regulatory resources, which is challenging to obtain and process.

Method: Propose CatRAG framework that integrates retrieval-augmented generation with graph-based reasoning, using hierarchical category-labeled knowledge graph with semantic features and lightweight intent classifier for query routing.

Result: Achieved state-of-the-art performance with 98.89% F1 score on regulation-specific dataset for classification and question answering tasks.

Conclusion: REBot demonstrates practical value in real-world academic advising scenarios through implemented web application, showing effectiveness of the CatRAG framework.

Abstract: Academic regulation advising is essential for helping students interpret and
comply with institutional policies, yet building effective systems requires
domain specific regulatory resources. To address this challenge, we propose
REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval
reasoning framework that integrates retrieval augmented generation with graph
based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported
by a hierarchical, category labeled knowledge graph enriched with semantic
features for domain alignment. A lightweight intent classifier routes queries
to the appropriate retrieval modules, ensuring both factual accuracy and
contextual depth. We construct a regulation specific dataset and evaluate REBot
on classification and question answering tasks, achieving state of the art
performance with an F1 score of 98.89%. Finally, we implement a web application
that demonstrates the practical value of REBot in real world academic advising
scenarios.

</details>


### [292] [Human-AI Teaming Co-Learning in Military Operations](https://arxiv.org/abs/2510.01815)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: This paper proposes a trustworthy co-learning model for human-AI teaming in military operations, featuring four key dimensions: adjustable autonomy, multi-layered control, bidirectional feedback, and collaborative decision-making.


<details>
  <summary>Details</summary>
Motivation: Military operations face rapidly evolving threats and complex environments where AI integration offers advantages but also creates challenges regarding effective and ethical deployment of human-AI teaming systems.

Method: The research designs a co-learning model with four integrated dimensions: 1) Adjustable autonomy for dynamic calibration of agent autonomy levels, 2) Multi-layered control for continuous oversight and accountability, 3) Bidirectional feedback with explicit and implicit feedback loops, and 4) Collaborative decision-making with confidence levels and rationale.

Result: The proposed model includes concrete exemplifications and recommendations for developing responsible and trustworthy human-AI teaming systems in military operations.

Conclusion: The co-learning model enables continuous bidirectional exchange of insights between human and AI agents as they jointly adapt to evolving battlefield conditions, addressing multidimensional responsibility, safety, and robustness aspects.

Abstract: In a time of rapidly evolving military threats and increasingly complex
operational environments, the integration of AI into military operations proves
significant advantages. At the same time, this implies various challenges and
risks regarding building and deploying human-AI teaming systems in an effective
and ethical manner. Currently, understanding and coping with them are often
tackled from an external perspective considering the human-AI teaming system as
a collective agent. Nevertheless, zooming into the dynamics involved inside the
system assures dealing with a broader palette of relevant multidimensional
responsibility, safety, and robustness aspects. To this end, this research
proposes the design of a trustworthy co-learning model for human-AI teaming in
military operations that encompasses a continuous and bidirectional exchange of
insights between the human and AI agents as they jointly adapt to evolving
battlefield conditions. It does that by integrating four dimensions. First,
adjustable autonomy for dynamically calibrating the autonomy levels of agents
depending on aspects like mission state, system confidence, and environmental
uncertainty. Second, multi-layered control which accounts continuous oversight,
monitoring of activities, and accountability. Third, bidirectional feedback
with explicit and implicit feedback loops between the agents to assure a proper
communication of reasoning, uncertainties, and learned adaptations that each of
the agents has. And fourth, collaborative decision-making which implies the
generation, evaluation, and proposal of decisions associated with confidence
levels and rationale behind them. The model proposed is accompanied by concrete
exemplifications and recommendations that contribute to further developing
responsible and trustworthy human-AI teaming systems in military operations.

</details>


### [293] [Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.01833)
*Zhihao Dou,Qinjian Zhao,Zhongwei Wan,Dinggen Zhang,Weida Wang,Towsif Raiyan,Benteng Chen,Qingtao Pan,Yang Ouyang,Zhiqiang Gao,Shufei Zhang,Sumon Biswas*

Main category: cs.AI

TL;DR: PTA-GRPO is a two-stage framework that enhances LLM reasoning by first distilling CoT into high-level guidance via SFT, then using guidance-aware RL to jointly optimize final outputs and guidance quality.


<details>
  <summary>Details</summary>
Motivation: Current LLM reasoning lacks global planning due to autoregressive token-level generation, leading to redundant, incoherent reasoning. Existing methods like tree-based algorithms and RL have high computational costs and fail to produce optimal reasoning trajectories.

Method: Two-stage framework: 1) Use advanced LLMs to distill CoT into compact high-level guidance for SFT; 2) Guidance-aware RL that jointly optimizes final output and high-level guidance quality.

Result: Extensive experiments on MATH, AIME2024, AIME2025, and AMC benchmarks with Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and LLaMA3.2-3B show consistent and significant improvements across models and tasks.

Conclusion: PTA-GRPO effectively enhances reasoning by combining high-level planning with fine-grained CoT reasoning, demonstrating strong generalization and stable performance improvements.

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning abilities
in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,
due to their autoregressive token-level generation, the reasoning process is
largely constrained to local decision-making and lacks global planning. This
limitation frequently results in redundant, incoherent, or inaccurate
reasoning, which significantly degrades overall performance. Existing
approaches, such as tree-based algorithms and reinforcement learning (RL),
attempt to address this issue but suffer from high computational costs and
often fail to produce optimal reasoning trajectories. To tackle this challenge,
we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy
Optimization PTA-GRPO, a two-stage framework designed to improve both
high-level planning and fine-grained CoT reasoning. In the first stage, we
leverage advanced LLMs to distill CoT into compact high-level guidance, which
is then used for supervised fine-tuning (SFT). In the second stage, we
introduce a guidance-aware RL method that jointly optimizes the final output
and the quality of high-level guidance, thereby enhancing reasoning
effectiveness. We conduct extensive experiments on multiple mathematical
reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across
diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and
LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently
achieves stable and significant improvements across different models and tasks,
validating its effectiveness and generalization.

</details>


### [294] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,NicolÃ¡s Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: The paper reframes adversarial inverse reinforcement learning for large language model reasoning, learning token-level rewards from expert demonstrations to provide step-level feedback during training and inference-time reranking.


<details>
  <summary>Details</summary>
Motivation: To develop dense, token-level reward models for process supervision that prioritize correctness over surface form, enabling better reasoning in language models through step-level feedback and error localization.

Method: Adversarial inverse reinforcement learning applied to LLM reasoning, learning reasoning rewards from expert demonstrations that serve dual purposes: training optimization and inference-time reranking of sampled traces.

Result: Empirical results on GSM8K with Llama3 and Qwen2.5 show that dense reasoning rewards effectively elicit reasoning and improve predictive performance through reward-guided reranking, especially for Llama-based policies.

Conclusion: The approach unifies training signals, inference-time selection, and token-level diagnostics into a single reasoning reward, suggesting reusable process-level rewards with broad potential to enhance multi-step reasoning in language models.

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [295] [Constrained Adaptive Rejection Sampling](https://arxiv.org/abs/2510.01902)
*PaweÅ Parys,Sairam Vaidya,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: CARS is a constrained generation method that improves rejection sampling efficiency by adaptively pruning invalid continuations using a trie structure, maintaining exact LM distribution while increasing acceptance rates.


<details>
  <summary>Details</summary>
Motivation: Existing constrained generation methods either distort LM distributions (greedy decoding) or waste computation (rejection sampling), both problematic for domains requiring both validity and diversity like program fuzzing.

Method: CARS uses unconstrained LM sampling and adaptively rules out constraint-violating continuations by recording them in a trie and subtracting their probability mass from future draws, ensuring invalid prefixes are never revisited.

Result: CARS consistently achieves higher efficiency (fewer LM forward passes per valid sample) and produces stronger sample diversity than both greedy constrained decoding and distribution-approximating methods across domains like program fuzzing and molecular generation.

Conclusion: CARS strictly improves rejection sampling efficiency without distributional distortion, making it suitable for applications requiring both validity and diversity in constrained generation.

Abstract: Language Models (LMs) are increasingly used in applications where generated
outputs must satisfy strict semantic or syntactic constraints. Existing
approaches to constrained generation fall along a spectrum: greedy constrained
decoding methods enforce validity during decoding but distort the LM's
distribution, while rejection sampling (RS) preserves fidelity but wastes
computation by discarding invalid outputs. Both extremes are problematic in
domains such as program fuzzing, where both validity and diversity of samples
are essential. We present Constrained Adaptive Rejection Sampling (CARS), an
approach that strictly improves the sample-efficiency of RS without
distributional distortion. CARS begins with unconstrained LM sampling and
adaptively rules out constraint-violating continuations by recording them in a
trie and subtracting their probability mass from future draws. This adaptive
pruning ensures that prefixes proven invalid are never revisited, acceptance
rates improve monotonically, and the resulting samples exactly follow the
constrained distribution. In experiments on a variety of domains -- e.g.,
program fuzzing and molecular generation -- CARS consistently achieves higher
efficiency -- measured in the number of LM forward passes per valid sample --
while also producing stronger sample diversity than both GCD and methods that
approximate the LM's distribution.

</details>


### [296] [To Mask or to Mirror: Human-AI Alignment in Collective Reasoning](https://arxiv.org/abs/2510.01924)
*Crystal Qian,Aaron Parisi,ClÃ©mentine Bouleau,Vivian Tsai,MaÃ«l Lebreton,Lucas Dixon*

Main category: cs.AI

TL;DR: This paper examines how large language models (LLMs) align with human collective decision-making using a social psychology experiment, revealing that LLM behaviors vary in mirroring or compensating for human biases depending on context and model-specific factors.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used to model and augment collective decision-making, it's critical to examine their alignment with human social reasoning, particularly at the collective level rather than just individual alignment.

Method: Used the Lost at Sea social psychology task with a large-scale online experiment (N=748), randomly assigning groups to leader elections with either visible demographic attributes or pseudonymous aliases, then simulating matched LLM groups conditioned on human data using Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3.

Result: LLM behaviors diverged significantly - some models mirrored human biases while others masked these biases and attempted to compensate for them, showing that human-AI alignment in collective reasoning depends on context, cues, and model-specific inductive biases.

Conclusion: Understanding how LLMs align with collective human behavior is critical for socially-aligned AI, requiring dynamic benchmarks that capture the complexities of collective reasoning rather than just individual-level assessments.

Abstract: As large language models (LLMs) are increasingly used to model and augment
collective decision-making, it is critical to examine their alignment with
human social reasoning. We present an empirical framework for assessing
collective alignment, in contrast to prior work on the individual level. Using
the Lost at Sea social psychology task, we conduct a large-scale online
experiment (N=748), randomly assigning groups to leader elections with either
visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We
then simulate matched LLM groups conditioned on the human data, benchmarking
Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some
mirror human biases; others mask these biases and attempt to compensate for
them. We empirically demonstrate that human-AI alignment in collective
reasoning depends on context, cues, and model-specific inductive biases.
Understanding how LLMs align with collective human behavior is critical to
advancing socially-aligned AI, and demands dynamic benchmarks that capture the
complexities of collective reasoning.

</details>


### [297] [Zero-shot reasoning for simulating scholarly peer-review](https://arxiv.org/abs/2510.02027)
*Khalid M. Saqr*

Main category: cs.AI

TL;DR: A deterministic simulation framework provides stable, evidence-based standards for evaluating AI-generated peer review reports, addressing challenges of unmanageable submission volumes and unregulated AI in scholarly publishing.


<details>
  <summary>Details</summary>
Motivation: To address the dual crisis of unmanageable submission volumes and unregulated AI in scholarly publishing, and create new governance models to safeguard scientific integrity by providing scalable, objective benchmarks.

Method: A deterministic simulation framework that analyzes 352 peer-review simulation reports to identify consistent system state indicators and evaluate AI-generated peer review reports.

Result: The system demonstrates calibrated editorial judgment with 'Revise' decisions forming majority outcomes (>50%) across disciplines, while 'Reject' rates adapt to field-specific norms (up to 45% in Health Sciences). It maintains 29% evidence-anchoring compliance rate invariant across domains.

Conclusion: The framework provides a transparent tool for fairness assurance and scalable workflow auditing, repositioning AI as an essential component of institutional accountability to maintain trust in scholarly communication.

Abstract: The scholarly publishing ecosystem faces a dual crisis of unmanageable
submission volumes and unregulated AI, creating an urgent need for new
governance models to safeguard scientific integrity. The traditional human-only
peer review regime lacks a scalable, objective benchmark, making editorial
processes opaque and difficult to audit. Here we investigate a deterministic
simulation framework that provides the first stable, evidence-based standard
for evaluating AI-generated peer review reports. Analyzing 352 peer-review
simulation reports, we identify consistent system state indicators that
demonstrate its reliability. First, the system is able to simulate calibrated
editorial judgment, with 'Revise' decisions consistently forming the majority
outcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt
to field-specific norms, rising to 45% in Health Sciences. Second, it maintains
unwavering procedural integrity, enforcing a stable 29% evidence-anchoring
compliance rate that remains invariant across diverse review tasks and
scientific domains. These findings demonstrate a system that is predictably
rule-bound, mitigating the stochasticity of generative AI. For the scientific
community, this provides a transparent tool to ensure fairness; for publishing
strategists, it offers a scalable instrument for auditing workflows, managing
integrity risks, and implementing evidence-based governance. The framework
repositions AI as an essential component of institutional accountability,
providing the critical infrastructure to maintain trust in scholarly
communication.

</details>


### [298] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: ReTabAD introduces a benchmark for tabular anomaly detection that incorporates textual semantics and domain knowledge, addressing limitations of existing datasets that lack semantic context.


<details>
  <summary>Details</summary>
Motivation: Existing tabular AD benchmarks provide only raw data without semantic context, ignoring important textual metadata like feature descriptions that experts use in practice, which restricts research flexibility and prevents full utilization of domain knowledge.

Method: Created 20 curated tabular datasets with structured textual metadata, implemented state-of-the-art AD algorithms (classical, deep learning, LLM-based), and developed a zero-shot LLM framework that leverages semantic context without task-specific training.

Result: Semantic context improves detection performance and enhances interpretability by supporting domain-aware reasoning, establishing strong baselines for context-aware AD research.

Conclusion: ReTabAD serves as a benchmark for systematic exploration of context-aware anomaly detection, demonstrating the value of incorporating textual semantics and domain knowledge in tabular AD.

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


### [299] [Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning](https://arxiv.org/abs/2510.02091)
*Xinyuan Song,Keyu Wang,PengXiang Li,Lu Yin,Shiwei Liu*

Main category: cs.AI

TL;DR: Depth usage in LLMs is heterogeneous - shallow layers handle knowledge/retrieval while deeper layers enable reasoning and coherence, with performance varying significantly by evaluation metric and task type.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate whether deeper layers in LLMs are truly redundant, challenging claims that they contribute little and can be removed without performance loss.

Method: Comprehensive analysis across diverse dimensions including evaluation protocols (likelihood vs generation), task categories, and model architectures, examining layer contributions through pruning experiments.

Result: Shallow layers are critical for knowledge and retrieval; deeper layers are indispensable for reasoning and long-range coherence; knowledge can be redistributed through distillation.

Conclusion: Depth utilization in LLMs is highly context-dependent, requiring task-, metric-, and model-aware perspectives for proper interpretation and compression.

Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs)
contribute little to representation learning and can often be removed without
significant performance loss. However, such claims are typically drawn from
narrow evaluations and may overlook important aspects of model behavior. In
this work, we present a systematic study of depth utilization across diverse
dimensions, including evaluation protocols, task categories, and model
architectures. Our analysis confirms that very deep layers are generally less
effective than earlier ones, but their contributions vary substantially with
the evaluation setting. Under likelihood-based metrics without generation,
pruning most layers preserves performance, with only the initial few being
critical. By contrast, generation-based evaluation uncovers indispensable roles
for middle and deeper layers in enabling reasoning and maintaining long-range
coherence. We further find that knowledge and retrieval are concentrated in
shallow components, whereas reasoning accuracy relies heavily on deeper layers
-- yet can be reshaped through distillation. These results highlight that depth
usage in LLMs is highly heterogeneous and context-dependent, underscoring the
need for task-, metric-, and model-aware perspectives in both interpreting and
compressing large models.

</details>


### [300] [Do AI Models Perform Human-like Abstract Reasoning Across Modalities?](https://arxiv.org/abs/2510.02125)
*Claas Beger,Ryan Yi,Shuhao Fu,Arseny Moskvichev,Sarah W. Tsai,Sivasankaran Rajamanickam,Melanie Mitchell*

Main category: cs.AI

TL;DR: While some AI models achieve human-level accuracy on ConceptARC benchmark, their abstract reasoning capabilities are overestimated in text modality and underestimated in visual modality when analyzed through rule-based evaluation.


<details>
  <summary>Details</summary>
Motivation: To investigate whether state-of-the-art models truly recognize and reason with intended abstractions in ARC tasks, rather than relying on surface-level patterns.

Method: Evaluated models on ConceptARC with varying input modalities (textual vs visual), external tool usage, and reasoning effort. Performed dual evaluation of both output accuracy and natural-language rules generated to explain solutions.

Result: Text-based models match human accuracy but use surface-level shortcuts; visual models show lower accuracy but still capture intended abstractions in rules, though often fail to apply them correctly.

Conclusion: Models still lag humans in abstract reasoning, and accuracy alone is insufficient for evaluating abstract reasoning capabilities - rule-level analysis provides more faithful assessment of multimodal models' abstraction abilities.

Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI
benchmark, but does that mean state-of-the-art models recognize and reason with
the abstractions that the task creators intended? We investigate models'
abstraction abilities on ConceptARC. We evaluate models under settings that
vary the input modality (textual vs. visual), whether the model is permitted to
use external Python tools, and, for reasoning models, the amount of reasoning
effort. In addition to measuring output accuracy, we perform fine-grained
evaluation of the natural-language rules that models generate to explain their
solutions. This dual evaluation lets us assess whether models solve tasks using
the abstractions ConceptARC was designed to elicit, rather than relying on
surface-level patterns. Our results show that, while some models using
text-based representations match human output accuracy, the best models' rules
are often based on surface-level ``shortcuts'' and capture intended
abstractions far less often than humans. Thus their capabilities for general
abstract reasoning may be overestimated by evaluations based on accuracy alone.
In the visual modality, AI models' output accuracy drops sharply, yet our
rule-level analysis reveals that models might be underestimated, as they still
exhibit a substantial share of rules that capture intended abstractions, but
are often unable to correctly apply these rules. In short, our results show
that models still lag humans in abstract reasoning, and that using accuracy
alone to evaluate abstract reasoning on ARC-like tasks may overestimate
abstract-reasoning capabilities in textual modalities and underestimate it in
visual modalities. We believe that our evaluation framework offers a more
faithful picture of multimodal models' abstract reasoning abilities and a more
principled way to track progress toward human-like, abstraction-centered
intelligence.

</details>


### [301] [FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models](https://arxiv.org/abs/2510.02133)
*Karan Dua,Hitesh Laxmichand Patel,Puneet Mittal,Ranjeet Gupta,Amit Agarwal,Praneet Pabolu,Srikant Panda,Hansa Meghwani,Graham Horwood,Fahad Shah*

Main category: cs.AI

TL;DR: FlexDoc is a scalable synthetic data generation framework that creates realistic multilingual semi-structured documents with rich annotations, reducing annotation costs by over 90% while improving model performance.


<details>
  <summary>Details</summary>
Motivation: Enterprise-scale document understanding models require large, diverse datasets, but data collection is expensive due to privacy constraints, legal restrictions, and high annotation costs that can reach millions of dollars.

Method: Combines Stochastic Schemas and Parameterized Sampling to probabilistically model layout patterns, visual structure, and content variability for controlled generation of diverse document variants at scale.

Result: Improves absolute F1 Score by up to 11% on Key Information Extraction tasks when augmenting real datasets, while reducing annotation effort by over 90% compared to traditional hard-template methods.

Conclusion: FlexDoc accelerates development of enterprise-grade document understanding models while significantly reducing data acquisition and annotation costs, and is already in active deployment.

Abstract: Developing document understanding models at enterprise scale requires large,
diverse, and well-annotated datasets spanning a wide range of document types.
However, collecting such data is prohibitively expensive due to privacy
constraints, legal restrictions, and the sheer volume of manual annotation
needed - costs that can scale into millions of dollars. We introduce FlexDoc, a
scalable synthetic data generation framework that combines Stochastic Schemas
and Parameterized Sampling to produce realistic, multilingual semi-structured
documents with rich annotations. By probabilistically modeling layout patterns,
visual structure, and content variability, FlexDoc enables the controlled
generation of diverse document variants at scale. Experiments on Key
Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data
improves the absolute F1 Score by up to 11% when used to augment real datasets,
while reducing annotation effort by over 90% compared to traditional
hard-template methods. The solution is in active deployment, where it has
accelerated the development of enterprise-grade document understanding models
while significantly reducing data acquisition and annotation costs.

</details>


### [302] [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
*Yang Yao,Yixu Wang,Yuxuan Zhang,Yi Lu,Tianle Gu,Lingyu Li,Dingyi Zhao,Keming Wu,Haozhe Wang,Ping Nie,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: This paper introduces a comprehensive benchmark and evaluation framework specifically designed for Deep Research Agents (DRAs) to address limitations in existing evaluation methods for AI agent systems.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are deficient in evaluation dimensions, response formatting, and scoring mechanisms, limiting their capacity to effectively assess Deep Research Agent systems that exhibit capabilities for task decomposition, cross-source retrieval, multi-stage reasoning, and structured output.

Method: The authors developed a rigorous benchmark comprising 214 expert-curated challenging queries across 10 broad thematic domains, each with manually constructed reference bundles. They also created a multidimensional evaluation framework with integrated scoring metrics for semantic quality, topical focus, and retrieval trustworthiness.

Result: Extensive experimentation confirmed that mainstream DRAs outperform web-search-tool-augmented reasoning models, but also revealed considerable scope for further improvement in DRA systems.

Conclusion: This study provides a robust foundation for capability assessment, architectural refinement, and paradigm advancement in Deep Research Agent systems.

Abstract: Artificial intelligence is undergoing the paradigm shift from closed language
models to interconnected agent systems capable of external perception and
information integration. As a representative embodiment, Deep Research Agents
(DRAs) systematically exhibit the capabilities for task decomposition,
cross-source retrieval, multi-stage reasoning, and structured output, which
markedly enhance performance on complex and open-ended tasks. However, existing
benchmarks remain deficient in evaluation dimensions, response formatting, and
scoring mechanisms, limiting their capacity to assess such systems effectively.
This paper introduces a rigorous benchmark and a multidimensional evaluation
framework tailored to DRAs and report-style responses. The benchmark comprises
214 expert-curated challenging queries distributed across 10 broad thematic
domains, each accompanied by manually constructed reference bundles to support
composite evaluation. The framework enables comprehensive evaluation of
long-form reports generated by DRAs, incorporating integrated scoring metrics
for semantic quality, topical focus, and retrieval trustworthiness. Extensive
experimentation confirms the superior performance of mainstream DRAs over
web-search-tool-augmented reasoning models, yet reveals considerable scope for
further improvement. This study provides a robust foundation for capability
assessment, architectural refinement, and paradigm advancement in DRA systems.

</details>


### [303] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: RLVR (Reinforcement Learning with Verifiable Rewards) paradoxically shrinks reasoning boundaries in LLMs due to negative interference and winner-take-all phenomena, but a data curation algorithm focusing on low-likelihood problems improves Pass@k performance.


<details>
  <summary>Details</summary>
Motivation: To investigate why RLVR, despite being designed to improve reasoning capabilities, actually shrinks the reasoning boundary in Large Language Models rather than expanding it.

Method: Analyzed RLVR learning dynamics through theoretical and empirical analysis on mathematical reasoning benchmarks, identifying negative interference and winner-take-all phenomena, then proposed a data curation algorithm focusing on low-likelihood problems.

Result: Discovered that RLVR causes negative interference (learning some problems reduces solution likelihood for others) and winner-take-all phenomenon (reinforcing high-likelihood problems while suppressing low-likelihood ones), leading to Pass@k performance decline. The proposed data curation algorithm achieved notable improvement in Pass@k performance.

Conclusion: RLVR's shrinkage effect stems from inherent on-policy sampling in standard RL objectives, causing convergence toward narrow solution strategies. Focusing learning on low-likelihood problems through data curation can effectively mitigate this issue and improve reasoning performance.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [304] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: Behavior Best-of-N (bBoN) is a scaling method for computer-use agents that generates multiple rollouts and selects the best using behavior narratives, achieving 69.9% success rate on OSWorld and approaching human-level performance.


<details>
  <summary>Details</summary>
Motivation: Computer-use agents have high potential for automating digital tasks but suffer from unreliability and high variance, especially for long-horizon complex tasks.

Method: Generate multiple agent rollouts and select among them using behavior narratives that describe the agents' execution trajectories, enabling both wide exploration and principled selection.

Result: Achieved 69.9% success rate on OSWorld (new SoTA), approaching human-level 72%, with strong generalization to WindowsAgentArena and AndroidWorld.

Conclusion: Scaling computer-use agents effectively requires structured trajectory understanding and selection, and bBoN provides a practical framework to achieve this unreasonable effectiveness of scaling.

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


### [305] [RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems](https://arxiv.org/abs/2510.02263)
*Yuxiao Qu,Anikait Singh,Yoonho Lee,Amrith Setlur,Ruslan Salakhutdinov,Chelsea Finn,Aviral Kumar*

Main category: cs.AI

TL;DR: The paper introduces reasoning abstractions - concise natural language descriptions of procedural knowledge that guide models toward effective reasoning. It proposes RLAD, a two-player RL training paradigm that jointly trains an abstraction generator and solution generator to improve reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Current reasoning traces learned by large models often fail to consistently capture or reuse procedures, instead drifting into verbose exploration. The goal is to enable more effective reasoning by guiding models to identify and implement algorithmic procedures.

Method: Train models to propose multiple abstractions for a problem, followed by RL that incentivizes building solutions using these abstractions. This creates a two-player RL training paradigm (RLAD) with joint training of abstraction generator and solution generator.

Result: The approach enables structured exploration, decouples learning signals for abstraction proposal and solution generation, and improves generalization to harder problems. Test-time compute allocation to generating abstractions is more beneficial than generating more solutions.

Conclusion: Reasoning abstractions and the RLAD training paradigm effectively guide models toward learning successful reasoning by providing procedural guidance and enabling structured exploration, with abstractions playing a crucial role in meaningful exploration.

Abstract: Reasoning requires going beyond pattern matching or memorization of solutions
to identify and implement "algorithmic procedures" that can be used to deduce
answers to hard problems. Doing so requires realizing the most relevant
primitives, intermediate results, or shared procedures, and building upon them.
While RL post-training on long chains of thought ultimately aims to uncover
this kind of algorithmic behavior, most reasoning traces learned by large
models fail to consistently capture or reuse procedures, instead drifting into
verbose and degenerate exploration. To address more effective reasoning, we
introduce reasoning abstractions: concise natural language descriptions of
procedural and factual knowledge that guide the model toward learning
successful reasoning. We train models to be capable of proposing multiple
abstractions given a problem, followed by RL that incentivizes building a
solution while using the information provided by these abstractions. This
results in a two-player RL training paradigm, abbreviated as RLAD, that jointly
trains an abstraction generator and a solution generator. This setup
effectively enables structured exploration, decouples learning signals of
abstraction proposal and solution generation, and improves generalization to
harder problems. We also show that allocating more test-time compute to
generating abstractions is more beneficial for performance than generating more
solutions at large test budgets, illustrating the role of abstractions in
guiding meaningful exploration.

</details>


### [306] [BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals](https://arxiv.org/abs/2510.02276)
*Chenqi Li,Yu Liu,Timothy Denison,Tingting Zhu*

Main category: cs.AI

TL;DR: BioX-Bridge is a lightweight framework for unsupervised cross-modal knowledge transfer in biosignals that uses a bridge network to align foundation model representations, reducing parameters by 88-99% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Biosignal modalities are intercorrelated but have limited labeled datasets. Existing knowledge distillation methods are computationally expensive, especially with large foundation models.

Method: Train a lightweight bridge network to align intermediate representations between foundation models across modalities, with efficient alignment position selection and flexible prototype network architecture.

Result: BioX-Bridge reduces trainable parameters by 88-99% while maintaining or improving transfer performance across multiple biosignal modalities, tasks, and datasets.

Conclusion: The proposed framework enables efficient cross-modal knowledge transfer for biosignals, making health monitoring systems more accessible and adaptable with minimal computational overhead.

Abstract: Biosignals offer valuable insights into the physiological states of the human
body. Although biosignal modalities differ in functionality, signal fidelity,
sensor comfort, and cost, they are often intercorrelated, reflecting the
holistic and interconnected nature of human physiology. This opens up the
possibility of performing the same tasks using alternative biosignal
modalities, thereby improving the accessibility, usability, and adaptability of
health monitoring systems. However, the limited availability of large labeled
datasets presents challenges for training models tailored to specific tasks and
modalities of interest. Unsupervised cross-modal knowledge transfer offers a
promising solution by leveraging knowledge from an existing modality to support
model training for a new modality. Existing methods are typically based on
knowledge distillation, which requires running a teacher model alongside
student model training, resulting in high computational and memory overhead.
This challenge is further exacerbated by the recent development of foundation
models that demonstrate superior performance and generalization across tasks at
the cost of large model sizes. To this end, we explore a new framework for
unsupervised cross-modal knowledge transfer of biosignals by training a
lightweight bridge network to align the intermediate representations and enable
information flow between foundation models and across modalities. Specifically,
we introduce an efficient strategy for selecting alignment positions where the
bridge should be constructed, along with a flexible prototype network as the
bridge architecture. Extensive experiments across multiple biosignal
modalities, tasks, and datasets show that BioX-Bridge reduces the number of
trainable parameters by 88--99\% while maintaining or even improving transfer
performance compared to state-of-the-art methods.

</details>
