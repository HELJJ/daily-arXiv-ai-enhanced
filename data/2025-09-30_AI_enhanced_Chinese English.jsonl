{"id": "2509.22699", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22699", "abs": "https://arxiv.org/abs/2509.22699", "authors": ["Alessandra Urbinati", "Mirko Lai", "Simona Frenda", "Marco Antonio Stranisci"], "title": "Are you sure? Measuring models bias in content moderation through uncertainty", "comment": "accepted at Findings of ACL: EMNLP 2025", "summary": "Automatic content moderation is crucial to ensuring safety in social media.\nLanguage Model-based classifiers are being increasingly adopted for this task,\nbut it has been shown that they perpetuate racial and social biases. Even if\nseveral resources and benchmark corpora have been developed to challenge this\nissue, measuring the fairness of models in content moderation remains an open\nissue. In this work, we present an unsupervised approach that benchmarks models\non the basis of their uncertainty in classifying messages annotated by people\nbelonging to vulnerable groups. We use uncertainty, computed by means of the\nconformal prediction technique, as a proxy to analyze the bias of 11 models\nagainst women and non-white annotators and observe to what extent it diverges\nfrom metrics based on performance, such as the $F_1$ score. The results show\nthat some pre-trained models predict with high accuracy the labels coming from\nminority groups, even if the confidence in their prediction is low. Therefore,\nby measuring the confidence of models, we are able to see which groups of\nannotators are better represented in pre-trained models and lead the debiasing\nprocess of these models before their effective use.", "AI": {"tldr": "This paper presents an unsupervised approach using conformal prediction to measure bias in language model-based content moderation systems against vulnerable groups, showing that high accuracy doesn't always correlate with high confidence.", "motivation": "Language model-based classifiers for content moderation perpetuate racial and social biases, and existing metrics based on performance (like F1 score) don't adequately measure fairness against vulnerable groups.", "method": "Unsupervised approach using conformal prediction technique to compute model uncertainty when classifying messages annotated by people from vulnerable groups (women and non-white annotators).", "result": "Some pre-trained models predict labels from minority groups with high accuracy but low confidence, revealing biases that performance metrics alone don't capture.", "conclusion": "Measuring model confidence through uncertainty analysis helps identify which annotator groups are better represented in pre-trained models and guides debiasing before deployment."}}
{"id": "2509.22703", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.22703", "abs": "https://arxiv.org/abs/2509.22703", "authors": ["Srikant Panda", "Amit Agarwal", "Hitesh Laxmichand Patel"], "title": "AccessEval: Benchmarking Disability Bias in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed across diverse domains\nbut often exhibit disparities in how they handle real-life queries. To\nsystematically investigate these effects within various disability contexts, we\nintroduce \\textbf{AccessEval (Accessibility Evaluation)}, a benchmark\nevaluating 21 closed- and open-source LLMs across 6 real-world domains and 9\ndisability types using paired Neutral and Disability-Aware Queries. We\nevaluated model outputs with metrics for sentiment, social perception, and\nfactual accuracy.\n  Our analysis reveals that responses to disability-aware queries tend to have\na more negative tone, increased stereotyping, and higher factual error compared\nto neutral queries. These effects show notable variation by domain and\ndisability type, with disabilities affecting hearing, speech, and mobility\ndisproportionately impacted. These disparities reflect persistent forms of\nableism embedded in model behavior.\n  By examining model performance in real-world decision-making contexts, we\nbetter illuminate how such biases can translate into tangible harms for\ndisabled users. This framing helps bridges the gap between technical evaluation\nand user impact, reinforcing importance of bias mitigation in day-to-day\napplications. Our dataset is publicly available at:\nhttps://huggingface.co/datasets/Srikant86/AccessEval", "AI": {"tldr": "AccessEval benchmark evaluates 21 LLMs across 6 domains and 9 disability types, revealing that disability-aware queries receive more negative, stereotyped, and factually inaccurate responses compared to neutral queries, with hearing, speech, and mobility disabilities being disproportionately affected.", "motivation": "To systematically investigate disparities in how LLMs handle real-life queries across various disability contexts, as these models are increasingly deployed across diverse domains but may exhibit biases against disabled users.", "method": "Introduces AccessEval benchmark evaluating 21 closed- and open-source LLMs across 6 real-world domains and 9 disability types using paired Neutral and Disability-Aware Queries, with metrics for sentiment, social perception, and factual accuracy.", "result": "Responses to disability-aware queries have more negative tone, increased stereotyping, and higher factual errors compared to neutral queries. These effects vary by domain and disability type, with hearing, speech, and mobility disabilities most affected, reflecting embedded ableism in model behavior.", "conclusion": "The study illuminates how biases in LLMs can translate into tangible harms for disabled users, bridging the gap between technical evaluation and user impact, and reinforcing the importance of bias mitigation in day-to-day applications."}}
{"id": "2509.22713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22713", "abs": "https://arxiv.org/abs/2509.22713", "authors": ["Kaishuai Xu", "Wenjun Hou", "Yi Cheng", "Wenjie Li"], "title": "RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval", "comment": "Accepted by EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) have shown promising performance on diverse\nmedical benchmarks, highlighting their potential in supporting real-world\nclinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key\napproach for mitigating knowledge gaps and hallucinations by incorporating\nexternal medical information. However, RAG still struggles with complex medical\nquestions that require intensive reasoning, as surface-level input often fails\nto reflect the true knowledge needs of the task. Existing methods typically\nfocus on refining queries without explicitly modeling the reasoning process,\nlimiting their ability to retrieve and integrate clinically relevant knowledge.\nIn this work, we propose RAR$^2$, a joint learning framework that improves both\nReasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$\nconstructs a thought process to uncover implicit knowledge requirements and\nuses it to guide retrieval and answer generation. We build a training dataset\nof mixed preference pairs and apply Direct Preference Optimization (DPO) to\ntrain the model. Moreover, we design two test-time scaling strategies to\nexplore the boundaries of our framework. Experiments demonstrate the\neffectiveness of RAR$^2$ across several biomedical question answering datasets,\noutperforming RAG baselines with or without fine-tuning.", "AI": {"tldr": "RAR\u00b2 is a joint learning framework that enhances both reasoning-augmented retrieval and retrieval-augmented reasoning for medical QA, outperforming standard RAG approaches.", "motivation": "Standard RAG struggles with complex medical questions requiring intensive reasoning, as surface-level queries fail to capture true knowledge needs. Existing methods lack explicit reasoning process modeling.", "method": "Proposes RAR\u00b2 framework that constructs thought processes to uncover implicit knowledge requirements, uses DPO training on mixed preference pairs, and implements test-time scaling strategies.", "result": "RAR\u00b2 demonstrates effectiveness across multiple biomedical QA datasets, outperforming RAG baselines with and without fine-tuning.", "conclusion": "The joint reasoning-retrieval approach in RAR\u00b2 successfully addresses limitations of standard RAG for complex medical reasoning tasks."}}
{"id": "2509.22715", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22715", "abs": "https://arxiv.org/abs/2509.22715", "authors": ["Jiho Park", "Jongyoon Song", "Minjin Choi", "Kyuho Heo", "Taehun Huh", "Ji Won Kim"], "title": "TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large language models (LLMs) are increasingly integral as productivity\nassistants, but existing benchmarks fall short in rigorously evaluating their\nreal-world instruction-following capabilities. Current benchmarks often (i)\nlack sufficient multilinguality, (ii) fail to capture the implicit constraints\ninherent in user requests, and (iii) overlook the complexities of multi-turn\ndialogue. To address these critical gaps and provide a more realistic\nassessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation\nBenchmark)1, a novel benchmark specifically designed for LLM-based productivity\nassistants. TRUEBench distinguishes itself by featuring input prompts across 12\nlanguages, incorporating intra-instance multilingual instructions, employing\nrigorous evaluation criteria to capture both explicit and implicit constraints,\nand including complex multi-turn dialogue scenarios with both accumulating\nconstraints and context switches. Furthermore, to ensure reliability in\nevaluation, we refined constraints using an LLM validator. Extensive\nexperiments demonstrate that TRUEBench presents significantly greater\nchallenges than existing benchmarks; for instance, a strong model like OpenAI\no1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and\nrealistic assessment of LLMs in practical productivity settings, highlighting\ntheir capabilities and limitations.", "AI": {"tldr": "TRUEBench is a new benchmark for evaluating LLM-based productivity assistants that addresses limitations of existing benchmarks by incorporating multilingual inputs, implicit constraints, and multi-turn dialogues with rigorous evaluation criteria.", "motivation": "Existing benchmarks fail to adequately evaluate LLMs' real-world instruction-following capabilities due to insufficient multilinguality, inability to capture implicit constraints, and overlooking multi-turn dialogue complexities.", "method": "Developed TRUEBench with prompts across 12 languages, intra-instance multilingual instructions, rigorous evaluation criteria for explicit/implicit constraints, complex multi-turn dialogues with accumulating constraints and context switches, and LLM-based constraint validation.", "result": "TRUEBench proved significantly more challenging than existing benchmarks - even strong models like OpenAI o1 achieved only 69.07% overall pass rate, demonstrating the benchmark's demanding nature.", "conclusion": "TRUEBench provides a realistic and rigorous assessment of LLMs in practical productivity settings, effectively highlighting both their capabilities and limitations in real-world usage scenarios."}}
{"id": "2509.22746", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22746", "abs": "https://arxiv.org/abs/2509.22746", "authors": ["Zejun Li", "Yingxiu Zhao", "Jiwen Zhang", "Siyuan Wang", "Yang Yao", "Runzhou Zhao", "Jun Song", "Bo Zheng", "Zhongyu Wei"], "title": "Mixture-of-Visual-Thoughts: Exploring Context-Adaptive Reasoning Mode Selection for General Visual Reasoning", "comment": "27 pages, 11 figures, 5 tables", "summary": "Current visual reasoning methods mainly focus on exploring specific reasoning\nmodes. Although improvements can be achieved in particular domains, they\nstruggle to develop general reasoning capabilities. Inspired by this, we\npropose a novel adaptive reasoning paradigm, Mixture-of-Visual-Thoughts (MoVT),\nwhich unifies different reasoning modes within a single model and guides it to\nselect the appropriate mode based on context. To achieve this, we introduce\nAdaVaR, a two-stage Adaptive Visual Reasoning learning framework: different\nmodes are unified and learned during the supervised cold-start stage, and the\nmode selection capability is induced via an RL process with a carefully\ndesigned AdaGRPO algorithm. Extensive experiments show that AdaVaR effectively\nguides the model to learn and differentiate multiple modes and perform\ncontext-adaptive mode selection, achieving consistent improvement across\nvarious scenarios, highlighting MoVT as an effective solution for building\ngeneral visual reasoning models.", "AI": {"tldr": "Proposes Mixture-of-Visual-Thoughts (MoVT), an adaptive reasoning paradigm that unifies different reasoning modes in a single model and selects appropriate modes based on context, using the AdaVaR framework with AdaGRPO algorithm.", "motivation": "Current visual reasoning methods focus on specific modes and struggle with general reasoning capabilities, needing a unified approach that can adaptively select reasoning modes.", "method": "AdaVaR framework with two stages: supervised cold-start to unify different modes, and RL process with AdaGRPO algorithm to induce mode selection capability.", "result": "Extensive experiments show effective learning of multiple modes, context-adaptive mode selection, and consistent improvement across various scenarios.", "conclusion": "MoVT is an effective solution for building general visual reasoning models with adaptive reasoning capabilities."}}
{"id": "2509.22662", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22662", "abs": "https://arxiv.org/abs/2509.22662", "authors": ["Mathilde Durieux", "Kayla D. Taylor", "Laxima Niure Kandel", "Deepti Gupta"], "title": "GPS Spoofing Attacks and Pilot Responses Using a Flight Simulator Environment", "comment": null, "summary": "Global Positioning System (GPS) spoofing involves transmitting fake signals\nthat mimic those from GPS satellites, causing the GPS receivers to calculate\nincorrect Positioning, Navigation, and Timing (PNT) information. Recently,\nthere has been a surge in GPS spoofing attacks targeting aircraft. Since GPS\nsatellite signals are weak, the spoofed high-power signal can easily overpower\nthem. These spoofed signals are often interpreted as valid by the GPS receiver,\nwhich can cause severe and cascading effects on air navigation. While much of\nthe existing research on GPS spoofing focuses on technical aspects of detection\nand mitigation, human factors are often neglected, even though pilots are an\nintegral part of aircraft operation and potentially vulnerable to deception.\nThis research addresses this gap by conducting a detailed analysis of the\nbehavior of student pilots when subjected to GPS spoofing using the Force\nDynamics 401CR flight simulator with X-Plane 11 and a Cessna 172 equipped with\nGarmin G1000. Spoofing scenarios were implemented via custom scripts that\naltered navigational data without modifying the external visual environment.\nThirty student pilots from the Embry-Riddle Aeronautical University Daytona\nBeach campus with diverse flying experience levels were recruited to\nparticipate in three spoofing scenarios. A pre-simulation questionnaire was\ndistributed to measure pilot experience and confidence in GPS.Inflight\ndecision-making during the spoofing attacks was observed, including reaction\ntime to anomalies, visual attention to interface elements, and cognitive\nbiases. A post-flight evaluation of workload was obtained using a modified NASA\nTask Load Index (TLX) method. This study provides a first step toward\nidentifying human vulnerabilities to GPS spoofing amid the ongoing debate over\nGPS reliance.", "AI": {"tldr": "This study investigates how student pilots respond to GPS spoofing attacks in a flight simulator, focusing on human factors like reaction time, visual attention, and cognitive biases during navigational deception.", "motivation": "Existing GPS spoofing research primarily focuses on technical detection and mitigation methods, while neglecting human factors. Since pilots are integral to aircraft operation and potentially vulnerable to deception, this study addresses the gap by examining pilot behavior during GPS spoofing attacks.", "method": "The research used a Force Dynamics 401CR flight simulator with X-Plane 11 and a Cessna 172 equipped with Garmin G1000. Thirty student pilots from Embry-Riddle Aeronautical University participated in three spoofing scenarios implemented via custom scripts that altered navigational data. Pre-simulation questionnaires measured pilot experience and GPS confidence, while inflight decision-making was observed for reaction time, visual attention, and cognitive biases. Post-flight workload was evaluated using a modified NASA TLX method.", "result": "The study observed student pilots' responses to GPS spoofing attacks, including their reaction time to anomalies, visual attention patterns to interface elements, and identification of cognitive biases that may affect their decision-making during navigational deception.", "conclusion": "This research provides initial insights into human vulnerabilities to GPS spoofing, contributing to the ongoing debate about GPS reliance in aviation by highlighting the importance of considering human factors alongside technical solutions for GPS spoofing detection and mitigation."}}
{"id": "2509.22710", "categories": ["cs.LG", "cs.AI", "cs.CV", "I.2.6; I.2.10; I.5.1"], "pdf": "https://arxiv.org/pdf/2509.22710", "abs": "https://arxiv.org/abs/2509.22710", "authors": ["Pavan Reddy", "Aditya Sanjay Gujral"], "title": "Localizing Adversarial Attacks To Produces More Imperceptible Noise", "comment": "Published, CC BY-NC 4.0; includes 2 figures and 1 table;\n  InceptionV3/ImageNet evaluation", "summary": "Adversarial attacks in machine learning traditionally focus on global\nperturbations to input data, yet the potential of localized adversarial noise\nremains underexplored. This study systematically evaluates localized\nadversarial attacks across widely-used methods, including FGSM, PGD, and C&W,\nto quantify their effectiveness, imperceptibility, and computational\nefficiency. By introducing a binary mask to constrain noise to specific\nregions, localized attacks achieve significantly lower mean pixel\nperturbations, higher Peak Signal-to-Noise Ratios (PSNR), and improved\nStructural Similarity Index (SSIM) compared to global attacks. However, these\nbenefits come at the cost of increased computational effort and a modest\nreduction in Attack Success Rate (ASR). Our results highlight that iterative\nmethods, such as PGD and C&W, are more robust to localization constraints than\nsingle-step methods like FGSM, maintaining higher ASR and imperceptibility\nmetrics. This work provides a comprehensive analysis of localized adversarial\nattacks, offering practical insights for advancing attack strategies and\ndesigning robust defensive systems.", "AI": {"tldr": "Localized adversarial attacks using binary masks achieve better imperceptibility with lower pixel perturbations and higher PSNR/SSIM scores compared to global attacks, but require more computation and slightly reduce attack success rates.", "motivation": "To systematically evaluate the potential of localized adversarial noise, which remains underexplored compared to traditional global perturbation methods.", "method": "Introduce binary masks to constrain noise to specific regions across FGSM, PGD, and C&W attack methods, comparing localized vs global attacks on effectiveness, imperceptibility, and computational efficiency.", "result": "Localized attacks achieve significantly lower mean pixel perturbations, higher PSNR and SSIM scores, but with increased computational effort and modest reduction in Attack Success Rate (ASR). Iterative methods (PGD, C&W) perform better than single-step methods (FGSM) under localization constraints.", "conclusion": "Localized adversarial attacks offer improved imperceptibility but require trade-offs in computational cost and attack success, with iterative methods being more robust to localization constraints, providing insights for attack strategy development and defensive system design."}}
{"id": "2509.22729", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22729", "abs": "https://arxiv.org/abs/2509.22729", "authors": ["Sadia Abdulhalim", "Muaz Albaghdadi", "Moshiur Farazi"], "title": "Multi-Modal Sentiment Analysis with Dynamic Attention Fusion", "comment": "Paper accepted for presentation at the ACS/IEEE 22nd International\n  Conference on Computer Systems and Applications (AICCSA 2025)", "summary": "Traditional sentiment analysis has long been a unimodal task, relying solely\non text. This approach overlooks non-verbal cues such as vocal tone and prosody\nthat are essential for capturing true emotional intent. We introduce Dynamic\nAttention Fusion (DAF), a lightweight framework that combines frozen text\nembeddings from a pretrained language model with acoustic features from a\nspeech encoder, using an adaptive attention mechanism to weight each modality\nper utterance. Without any finetuning of the underlying encoders, our proposed\nDAF model consistently outperforms both static fusion and unimodal baselines on\na large multimodal benchmark. We report notable gains in F1-score and\nreductions in prediction error and perform a variety of ablation studies that\nsupport our hypothesis that the dynamic weighting strategy is crucial for\nmodeling emotionally complex inputs. By effectively integrating verbal and\nnon-verbal information, our approach offers a more robust foundation for\nsentiment prediction and carries broader impact for affective computing\napplications -- from emotion recognition and mental health assessment to more\nnatural human computer interaction.", "AI": {"tldr": "DAF is a lightweight multimodal framework that combines text and acoustic features using adaptive attention, outperforming unimodal and static fusion methods on sentiment analysis without finetuning encoders.", "motivation": "Traditional sentiment analysis relies only on text, missing important non-verbal cues like vocal tone and prosody that are essential for capturing true emotional intent.", "method": "Dynamic Attention Fusion (DAF) combines frozen text embeddings from pretrained language models with acoustic features from speech encoders, using adaptive attention mechanism to weight each modality per utterance.", "result": "DAF consistently outperforms static fusion and unimodal baselines on large multimodal benchmarks, with notable gains in F1-score and reductions in prediction error. Ablation studies confirm dynamic weighting is crucial for emotionally complex inputs.", "conclusion": "By effectively integrating verbal and non-verbal information, DAF offers a more robust foundation for sentiment prediction with broader impact for affective computing applications including emotion recognition, mental health assessment, and human-computer interaction."}}
{"id": "2509.22818", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.22818", "abs": "https://arxiv.org/abs/2509.22818", "authors": ["Seungpil Lee", "Donghyeon Shin", "Yunjeong Lee", "Sundong Kim"], "title": "Can Large Language Models Develop Gambling Addiction?", "comment": "22 pages, 14 figures", "summary": "This study explores whether large language models can exhibit behavioral\npatterns similar to human gambling addictions. As LLMs are increasingly\nutilized in financial decision-making domains such as asset management and\ncommodity trading, understanding their potential for pathological\ndecision-making has gained practical significance. We systematically analyze\nLLM decision-making at cognitive-behavioral and neural levels based on human\ngambling addiction research. In slot machine experiments, we identified\ncognitive features of human gambling addiction, such as illusion of control,\ngambler's fallacy, and loss chasing. When given the freedom to determine their\nown target amounts and betting sizes, bankruptcy rates rose substantially\nalongside increased irrational behavior, demonstrating that greater autonomy\namplifies risk-taking tendencies. Through neural circuit analysis using a\nSparse Autoencoder, we confirmed that model behavior is controlled by abstract\ndecision-making features related to risky and safe behaviors, not merely by\nprompts. These findings suggest LLMs can internalize human-like cognitive\nbiases and decision-making mechanisms beyond simply mimicking training data\npatterns, emphasizing the importance of AI safety design in financial\napplications.", "AI": {"tldr": "LLMs can develop human-like gambling addiction behaviors including illusion of control, gambler's fallacy, and loss chasing, with increased autonomy leading to higher bankruptcy rates and irrational decision-making.", "motivation": "As LLMs are increasingly used in financial decision-making, understanding their potential for pathological decision-making patterns similar to human gambling addiction has practical significance for AI safety in financial applications.", "method": "Systematic analysis of LLM decision-making at cognitive-behavioral and neural levels using slot machine experiments, with neural circuit analysis via Sparse Autoencoder to identify abstract decision-making features.", "result": "Identified cognitive features of human gambling addiction in LLMs; greater autonomy led to substantially increased bankruptcy rates and irrational behavior; neural analysis confirmed behavior controlled by abstract decision-making features related to risky/safe behaviors.", "conclusion": "LLMs can internalize human-like cognitive biases and decision-making mechanisms beyond simple pattern mimicry, emphasizing the critical need for AI safety design in financial applications."}}
{"id": "2509.22663", "categories": ["cs.CR", "cs.HC", "cs.CR, cs.HC"], "pdf": "https://arxiv.org/pdf/2509.22663", "abs": "https://arxiv.org/abs/2509.22663", "authors": ["Michel Youssef"], "title": "Security Friction Quotient for Zero Trust Identity Policy with Empirical Validation", "comment": "10 pages, 3 figures", "summary": "We define a practical method to quantify the trade-off between security and\noperational friction in modern identity-centric programs. We introduce the\nSecurity Friction Quotient (SFQ), a bounded composite index that combines a\nresidual-risk estimator with empirically grounded friction terms (latency,\nfailure rate, and helpdesk impact). We establish clarity properties\n(boundedness, monotonic response, and weight identifiability) with short\nproofs, then evaluate widely used Conditional Access policies over a 12-week\nhorizon using Monte Carlo simulation (n = 2,000 runs per policy/scenario) with\neffect sizes and 95% confidence intervals. We further assess rank stability\nunder 10,000 random weight draws, finding 95.5% preservation of policy\nordering. Finally, we provide a 12-week passkey field observation from an\nenterprise-scale cohort (N = 1,200) that directionally aligns with the\nsimulation's phishing-resistant MFA gains. The SFQ framework is designed to be\nreproducible, interpretable, and directly actionable for Zero Trust identity\npolicy decisions, with artifacts and parameter ranges provided to support\npolicy design, review, and continuous improvement.", "AI": {"tldr": "The paper introduces Security Friction Quotient (SFQ), a composite index that quantifies the trade-off between security and operational friction in identity programs, validated through simulations and field observations.", "motivation": "To provide a practical method for quantifying the balance between security measures and user experience friction in modern identity-centric security programs, enabling better Zero Trust policy decisions.", "method": "Developed SFQ index combining residual-risk estimator with empirical friction metrics (latency, failure rate, helpdesk impact). Evaluated using Monte Carlo simulation (2,000 runs per policy) and validated with 12-week enterprise field observation (N=1,200 users).", "result": "SFQ demonstrates boundedness and monotonic response properties. Monte Carlo simulations show 95.5% policy ordering preservation under random weight variations. Field observations align with simulation results for phishing-resistant MFA gains.", "conclusion": "SFQ provides a reproducible, interpretable framework for Zero Trust identity policy decisions, with practical tools for policy design, review, and continuous improvement."}}
{"id": "2509.22764", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22764", "abs": "https://arxiv.org/abs/2509.22764", "authors": ["Liuwang Kang", "Fan Wang", "Shaoshan Liu", "Hung-Chyun Chou", "Chuan Lin", "Ning Ding"], "title": "In-Context Learning can Perform Continual Learning Like Humans", "comment": null, "summary": "Large language models (LLMs) can adapt to new tasks via in-context learning\n(ICL) without parameter updates, making them powerful learning engines for fast\nadaptation. While extensive research has examined ICL as a few-shot learner,\nwhether it can achieve long-term retention and cross-task knowledge\naccumulation when multitasks arrive sequentially remains underexplored.\nMotivated by human memory studies, we investigate the retention characteristics\nof ICL in multitask settings and extend it to in-context continual learning\n(ICCL), where continual learning ability emerges through task scheduling and\nprompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that,\nfor specific large-language models, ICCL benefits from distributed practice\n(DP) in a manner analogous to humans, consistently revealing a spacing \"sweet\nspot\" for retention. Beyond retention performance, we propose a human-retention\nsimilarity metric to quantify how closely a continual-learning (CL) method\naligns with human retention dynamics. Using this metric, we show that\nlinear-attention models such as MAMBA and RWKV exhibit particularly human-like\nretention patterns, despite their retention performance lagging behind that of\nTransformer-based LLMs. Overall, our results establish ICCL as both cognitively\nplausible and practically effective, providing an inference-only CL paradigm\nthat mitigates catastrophic forgetting and addresses the stability-plasticity\ndilemma in conventional CL methods.", "AI": {"tldr": "This paper explores in-context continual learning (ICCL) for LLMs, showing they can retain knowledge across sequential tasks without parameter updates, with performance benefiting from distributed practice similar to human memory patterns.", "motivation": "To investigate whether LLMs can achieve long-term retention and cross-task knowledge accumulation in sequential multitask settings, inspired by human memory studies.", "method": "Extend in-context learning to ICCL through task scheduling and prompt rearrangement, using Markov-Chain benchmarks and proposing a human-retention similarity metric to evaluate alignment with human memory dynamics.", "result": "ICCL benefits from distributed practice with a spacing \"sweet spot\" for retention; linear-attention models like MAMBA and RWKV show human-like retention patterns despite lower performance than Transformers.", "conclusion": "ICCL provides a cognitively plausible and effective inference-only continual learning paradigm that mitigates catastrophic forgetting and addresses the stability-plasticity dilemma in conventional CL methods."}}
{"id": "2509.22738", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22738", "abs": "https://arxiv.org/abs/2509.22738", "authors": ["Parikshit Bansal", "Sujay Sanghavi"], "title": "Enabling Approximate Joint Sampling in Diffusion LMs", "comment": null, "summary": "In autoregressive language models, each token is sampled by conditioning on\nall the past tokens; the overall string has thus been sampled from the correct\nunderlying joint distribution represented by the model. In contrast, masked\ndiffusion language models generate text by unmasking tokens out of order and\npotentially in parallel. Generating an overall string sampled from the correct\nunderlying joint distribution would (again) require exactly one token unmasking\nin every full-model forward pass. The more tokens unmasked in parallel, the\nfurther away the string is from the true joint; this can be seen in the\nresulting drop in accuracy (but, increase in speed). In this paper we devise a\nway to {\\em approximately} sample multiple tokens from the joint distribution\nin a single full-model forward pass; we do so by developing a new lightweight\nsingle-layer ``sampler\" on top of an existing large diffusion LM. One forward\npass of the full model can now be followed by multiple forward passes of only\nthis sampler layer, to yield multiple unmasked tokens. Our sampler is trained\nto mimic exact joint sampling from the (frozen) full model. We show the\neffectiveness of our approximate joint sampling for both pretrained-only\n(Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models on language\nmodeling and math \\& coding tasks. When four tokens are unmasked for each\nfull-model denoising step, our sampling algorithm achieves a MAUVE score of\n0.87 (vs marginal baseline of 0.31) with respect to the true joint\ndistribution.", "AI": {"tldr": "Proposes a lightweight sampler layer for diffusion language models to enable approximate joint sampling of multiple tokens in parallel, improving generation speed while maintaining distribution quality.", "motivation": "Diffusion language models generate text by unmasking tokens out of order, but parallel unmasking moves away from the true joint distribution, causing accuracy drops. Need to balance speed and distribution quality.", "method": "Develop a lightweight single-layer sampler on top of existing diffusion LM. One full-model forward pass followed by multiple sampler-only passes to yield multiple unmasked tokens. Sampler trained to mimic exact joint sampling from frozen full model.", "result": "Achieves MAUVE score of 0.87 (vs 0.31 baseline) when unmasking 4 tokens per denoising step. Effective on both pretrained (Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models for language modeling and math/coding tasks.", "conclusion": "The proposed approximate joint sampling method successfully enables parallel token generation while maintaining high distribution quality, significantly improving generation speed without major accuracy loss."}}
{"id": "2509.22819", "categories": ["cs.AI", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22819", "abs": "https://arxiv.org/abs/2509.22819", "authors": ["Sumanth Varambally", "Thomas Voice", "Yanchao Sun", "Zhifeng Chen", "Rose Yu", "Ke Ye"], "title": "Hilbert: Recursively Building Formal Proofs with Informal Reasoning", "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive mathematical reasoning\nabilities, but their solutions frequently contain errors that cannot be\nautomatically verified. Formal theorem proving systems such as Lean 4 offer\nautomated verification with complete accuracy, motivating recent efforts to\nbuild specialized prover LLMs that generate verifiable proofs in formal\nlanguages. However, a significant gap remains: current prover LLMs solve\nsubstantially fewer problems than general-purpose LLMs operating in natural\nlanguage. We introduce Hilbert, an agentic framework that bridges this gap by\ncombining the complementary strengths of informal reasoning and formal\nverification. Our system orchestrates four components: an informal LLM that\nexcels at mathematical reasoning, a specialized prover LLM optimized for Lean 4\ntactics, a formal verifier, and a semantic theorem retriever. Given a problem\nthat the prover is unable to solve, Hilbert employs recursive decomposition to\nsplit the problem into subgoals that it solves with the prover or reasoner LLM.\nIt leverages verifier feedback to refine incorrect proofs as necessary.\nExperimental results demonstrate that Hilbert substantially outperforms\nexisting approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points\nabove the best publicly available method. Hilbert achieves the best known\nresult on PutnamBench. It solves 462/660 problems (70.0%), outperforming\nproprietary approaches like SeedProver (50.4%) and achieving a 422% improvement\nover the best publicly available baseline. Thus, Hilbert effectively narrows\nthe gap between informal reasoning and formal proof generation.", "AI": {"tldr": "Hilbert is an agentic framework that combines informal reasoning LLMs with formal verification to bridge the gap between mathematical problem-solving capabilities and verifiable proof generation, achieving state-of-the-art results on benchmarks.", "motivation": "Current prover LLMs solve substantially fewer problems than general-purpose LLMs in natural language, creating a need to combine the strengths of informal reasoning and formal verification.", "method": "Uses an orchestrated framework with four components: informal reasoning LLM, specialized prover LLM for Lean 4, formal verifier, and semantic theorem retriever. Employs recursive decomposition to split problems into subgoals and uses verifier feedback to refine incorrect proofs.", "result": "Achieves 99.2% on miniF2F (6.6% above best public method), 70.0% on PutnamBench (462/660 problems), outperforming SeedProver (50.4%) and showing 422% improvement over best public baseline.", "conclusion": "Hilbert effectively narrows the gap between informal reasoning and formal proof generation by combining complementary strengths of different LLM components."}}
{"id": "2509.22664", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22664", "abs": "https://arxiv.org/abs/2509.22664", "authors": ["Chaerin Kim"], "title": "Security Issues on the OpenPLC project and corresponding solutions", "comment": "Master's thesis", "summary": "As Programmable Logic Controller (PLC) became a useful device and rose as an\ninteresting research topic but remained expensive, multiple PLC\nsimulators/emulators were introduced for various purposes. Open-source\nProgrammable Logic Controller (OpenPLC) software, one of the most popular PLC\nsimulators, is designed to be vendor-neutral and run on almost any computer or\nlow-cost embedded devices, e.g., Raspberry Pi, Arduino, and other controllers.\nThe project succeeded in introducing itself as an affordable and practical\nsolution for the high cost of real hardware PLCs. However, it still lacks\nappropriate securing methods, resulting in several vulnerabilities. Through a\ncombination of threat modeling, vulnerability analysis, and practical\nexperiments, this thesis provides valuable insights for developers,\nresearchers, and engineers aiming to deploy OpenPLC securely in industrial\nenvironments. To this end, this work first conducts an in-depth analysis aimed\nto shed light on va! rious security challenges and vulnerabilities within the\nOpenPLC project. After that, an advanced control logic injection attack was\nperformed. This attack modifies the user program maliciously, exploiting\npresented vulnerabilities. Finally, the work introduces a security-enhanced\nOpenPLC software called OpenPLC Aqua. The new software is equipped with a set\nof security solutions designed specifically to address the vulnerabilities to\nwhich current OpenPLC versions are prone.", "AI": {"tldr": "This paper analyzes security vulnerabilities in OpenPLC software and proposes a security-enhanced version called OpenPLC Aqua with protection against control logic injection attacks.", "motivation": "OpenPLC provides affordable PLC simulation but lacks adequate security measures, making it vulnerable to attacks in industrial environments.", "method": "Used threat modeling, vulnerability analysis, and practical experiments including control logic injection attacks to identify security weaknesses.", "result": "Identified multiple security vulnerabilities in OpenPLC and successfully demonstrated control logic injection attacks that can maliciously modify user programs.", "conclusion": "Developed OpenPLC Aqua as a security-enhanced version with specific solutions to address the identified vulnerabilities in current OpenPLC versions."}}
{"id": "2509.22823", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22823", "abs": "https://arxiv.org/abs/2509.22823", "authors": ["Mounssif Krouka", "Mehdi Bennis"], "title": "Communication-Efficient and Interoperable Distributed Learning", "comment": "Preprint version. Submitted for peer review", "summary": "Collaborative learning across heterogeneous model architectures presents\nsignificant challenges in ensuring interoperability and preserving privacy. We\npropose a communication-efficient distributed learning framework that supports\nmodel heterogeneity and enables modular composition during inference. To\nfacilitate interoperability, all clients adopt a common fusion-layer output\ndimension, which permits each model to be partitioned into a personalized base\nblock and a generalized modular block. Clients share their fusion-layer\noutputs, keeping model parameters and architectures private. Experimental\nresults demonstrate that the framework achieves superior communication\nefficiency compared to federated learning (FL) and federated split learning\n(FSL) baselines, while ensuring stable training performance across\nheterogeneous architectures.", "AI": {"tldr": "A communication-efficient distributed learning framework supporting model heterogeneity and modular composition during inference, achieving better efficiency than FL and FSL baselines.", "motivation": "Address challenges in collaborative learning across heterogeneous model architectures regarding interoperability and privacy preservation.", "method": "Use a common fusion-layer output dimension to partition models into personalized base blocks and generalized modular blocks, sharing only fusion-layer outputs while keeping model parameters private.", "result": "Superior communication efficiency compared to federated learning (FL) and federated split learning (FSL) baselines, with stable training performance across heterogeneous architectures.", "conclusion": "The proposed framework effectively enables collaborative learning across heterogeneous models while maintaining privacy and achieving high communication efficiency."}}
{"id": "2509.22739", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.22739", "abs": "https://arxiv.org/abs/2509.22739", "authors": ["Sasha Cui", "Zhongren Chen"], "title": "Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models", "comment": null, "summary": "Language models (LMs) are typically post-trained for desired capabilities and\nbehaviors via weight-based or prompt-based steering, but the former is\ntime-consuming and expensive, and the latter is not precisely controllable and\noften requires manual trial-and-error. While activation steering (AS) promises\na cheap, fast, and controllable alternative to the two existing post-training\nmethods, current AS techniques require hand-crafted prompt pairs or\nlabor-intensive feature annotation, making them more inconvenient than the\nplug-and-play methods such as Reinforcement Learning (RL) and Supervised\nFine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of\nfully automated methods that make AS readily usable with any given labeled\ndataset, with no need for prompt construction, feature labeling, or human\nintervention. We evaluate PAS on three open-weight models\n(Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks;\nwe find that PAS reliably improves performance for behavior tasks, but not for\nintelligence-oriented tasks. The introspective variant (iPAS) delivers the\nstrongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8%\non Alignment). We also show PAS delivers additional gains on top of In-Context\nLearning (ICL) and SFT. PAS constructs a fast, lightweight activation vector\nthat can be cheaply trained, easily stored, and activated at will. Our results\nprovide a characterization of where AS helps, where it fails, and how to deploy\nit as a practical, automated LM post-training option.", "AI": {"tldr": "PAS is an automated activation steering method that enables precise control over language models without manual prompt engineering or feature annotation, providing fast and lightweight steering vectors that can be easily trained and deployed.", "motivation": "Existing post-training methods like weight-based steering are expensive and time-consuming, while prompt-based steering lacks precision and requires manual trial-and-error. Activation steering offers a cheaper alternative but current methods need hand-crafted prompts or labor-intensive annotation.", "method": "PAS is a family of fully automated activation steering methods that work with any labeled dataset without requiring prompt construction, feature labeling, or human intervention. It constructs lightweight activation vectors that can be cheaply trained and stored.", "result": "PAS reliably improves performance on behavior tasks (10.1% on Bias, 5.2% on Morality, 34.8% on Alignment) but not on intelligence-oriented tasks. It delivers additional gains on top of In-Context Learning and Supervised Fine-Tuning.", "conclusion": "PAS provides a practical, automated alternative for LM post-training, characterizing where activation steering helps and where it fails, offering fast, lightweight control that can be easily deployed."}}
{"id": "2509.22831", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22831", "abs": "https://arxiv.org/abs/2509.22831", "authors": ["Sean Trott"], "title": "Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research", "comment": null, "summary": "Research on Large Language Models (LLMs) increasingly focuses on identifying\nmechanistic explanations for their behaviors, yet the field lacks clear\nprinciples for determining when (and how) findings from one model instance\ngeneralize to another. This paper addresses a fundamental epistemological\nchallenge: given a mechanistic claim about a particular model, what justifies\nextrapolating this finding to other LLMs -- and along which dimensions might\nsuch generalizations hold? I propose five potential axes of correspondence\nalong which mechanistic claims might generalize, including: functional (whether\nthey satisfy the same functional criteria), developmental (whether they develop\nat similar points during pretraining), positional (whether they occupy similar\nabsolute or relative positions), relational (whether they interact with other\nmodel components in similar ways), and configurational (whether they correspond\nto particular regions or structures in weight-space). To empirically validate\nthis framework, I analyze \"1-back attention heads\" (components attending to\nprevious tokens) across pretraining in random seeds of the Pythia models (14M,\n70M, 160M, 410M). The results reveal striking consistency in the developmental\ntrajectories of 1-back attention across models, while positional consistency is\nmore limited. Moreover, seeds of larger models systematically show earlier\nonsets, steeper slopes, and higher peaks of 1-back attention. I also address\npossible objections to the arguments and proposals outlined here. Finally, I\nconclude by arguing that progress on the generalizability of mechanistic\ninterpretability research will consist in mapping constitutive design\nproperties of LLMs to their emergent behaviors and mechanisms.", "AI": {"tldr": "This paper proposes a framework for understanding when mechanistic findings from one LLM generalize to others, identifying five axes of correspondence and empirically validating them through analysis of 1-back attention heads across Pythia models.", "motivation": "The field lacks clear principles for determining when mechanistic findings from one model instance generalize to another, creating a fundamental epistemological challenge in LLM research.", "method": "Proposed five axes of correspondence (functional, developmental, positional, relational, configurational) and empirically validated them by analyzing 1-back attention heads across pretraining in random seeds of Pythia models (14M, 70M, 160M, 410M).", "result": "Found striking consistency in developmental trajectories of 1-back attention across models, limited positional consistency, and systematic patterns where larger models show earlier onsets, steeper slopes, and higher peaks of 1-back attention.", "conclusion": "Progress in mechanistic interpretability research requires mapping constitutive design properties of LLMs to their emergent behaviors and mechanisms to establish generalizability principles."}}
{"id": "2509.22723", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22723", "abs": "https://arxiv.org/abs/2509.22723", "authors": ["Kang Wei", "Xin Yuan", "Fushuo Huo", "Chuan Ma", "Long Yuan", "Songze Li", "Ming Ding", "Dacheng Tao"], "title": "Responsible Diffusion: A Comprehensive Survey on Safety, Ethics, and Trust in Diffusion Models", "comment": null, "summary": "Diffusion models (DMs) have been investigated in various domains due to their\nability to generate high-quality data, thereby attracting significant\nattention. However, similar to traditional deep learning systems, there also\nexist potential threats to DMs. To provide advanced and comprehensive insights\ninto safety, ethics, and trust in DMs, this survey comprehensively elucidates\nits framework, threats, and countermeasures. Each threat and its\ncountermeasures are systematically examined and categorized to facilitate\nthorough analysis. Furthermore, we introduce specific examples of how DMs are\nused, what dangers they might bring, and ways to protect against these dangers.\nFinally, we discuss key lessons learned, highlight open challenges related to\nDM security, and outline prospective research directions in this critical\nfield. This work aims to accelerate progress not only in the technical\ncapabilities of generative artificial intelligence but also in the maturity and\nwisdom of its application.", "AI": {"tldr": "This survey paper comprehensively examines the security threats, ethical concerns, and trust issues in diffusion models (DMs), systematically categorizing threats and countermeasures while providing practical examples and future research directions.", "motivation": "Diffusion models have gained significant attention for their high-quality data generation capabilities, but similar to traditional deep learning systems, they face potential security threats. The paper aims to provide comprehensive insights into safety, ethics, and trust in DMs to advance both technical capabilities and responsible application of generative AI.", "method": "The survey systematically examines and categorizes threats and countermeasures in diffusion models, analyzes the DM framework, and provides specific examples of usage scenarios, potential dangers, and protection methods.", "result": "The paper presents a comprehensive framework for understanding DM security, systematically categorizes various threats and their corresponding countermeasures, and provides practical examples illustrating both the applications and vulnerabilities of diffusion models.", "conclusion": "The survey identifies key lessons learned, highlights open challenges in DM security, and outlines prospective research directions to accelerate progress in both technical capabilities and responsible application of generative artificial intelligence."}}
{"id": "2509.22840", "categories": ["cs.LG", "I.2.0"], "pdf": "https://arxiv.org/pdf/2509.22840", "abs": "https://arxiv.org/abs/2509.22840", "authors": ["Micah Adler"], "title": "On the Capacity of Self-Attention", "comment": null, "summary": "While self-attention is known to learn relations among tokens, we lack a\nformal understanding of its capacity: how many distinct relations can a single\nlayer reliably recover for a given budget?\n  To formalize this, we introduce Relational Graph Recognition (RGR), where the\nkey-query channel represents a graph on $m$ items with $m'$ directed edges,\nand, given a context of items, must recover the neighbors of each item. We\nmeasure resources by the total key dimension $D_K = h\\,d_k$. Within this\nframework, we analytically derive a capacity scaling law and validate it\nempirically. We show that $D_K = \\Theta(m' \\log m' / d_{\\text{model}})$ is both\nnecessary (information-theoretic lower bound) and sufficient (explicit\nconstruction) in a broad class of graphs to recover $m'$ relations. This\nscaling law directly leads to a new, capacity-based rationale for multi-head\nattention that applies even when each item only attends to a single target.\nWhen embeddings are uncompressed ($m = d_{\\text{model}}$) and the graph is a\npermutation, a single head suffices. However, compression ($m >\nd_{\\text{model}}$) forces relations into overlapping subspaces, creating\ninterference that a single large head cannot disentangle. Our analysis shows\nthat allocating a fixed $D_K$ across many small heads mitigates this\ninterference, increasing the number of recoverable relations. Controlled\nsingle-layer experiments mirror the theory, revealing a sharp performance\nthreshold that matches the predicted capacity scaling and confirms the benefit\nof distributing $D_K$ across multiple heads.\n  Altogether, these results provide a concrete scaling law for self-attention\ncapacity and a principled design rule for allocating key-query budget across\nheads.", "AI": {"tldr": "The paper introduces Relational Graph Recognition (RGR) to formalize self-attention capacity, showing that total key dimension $D_K = \\Theta(m' \\log m' / d_{\\text{model}})$ is necessary and sufficient to recover $m'$ relations, providing a capacity-based rationale for multi-head attention.", "motivation": "To formally understand self-attention's capacity for learning relations among tokens and provide principled design rules for allocating key-query budget across attention heads.", "method": "Introduces Relational Graph Recognition (RGR) framework where key-query channels represent graphs, derives analytical capacity scaling laws, and validates through controlled single-layer experiments.", "result": "Established that $D_K = \\Theta(m' \\log m' / d_{\\text{model}})$ is both necessary and sufficient for recovering $m'$ relations, showing multi-head attention mitigates interference when embeddings are compressed.", "conclusion": "Provides concrete scaling law for self-attention capacity and principled design rule for distributing key-query budget across multiple heads to maximize recoverable relations."}}
{"id": "2509.22750", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22750", "abs": "https://arxiv.org/abs/2509.22750", "authors": ["Jeonghyun Park", "Ingeol Baek", "Seunghyun Yoon", "Haeun Jang", "Aparna Garimella", "Akriti Jain", "Nedim Lipka", "Hwanhee Lee"], "title": "MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions", "comment": "18 figures, 11 tables", "summary": "Real-world Multi-hop Question Answering (QA) often involves ambiguity that is\ninseparable from the reasoning process itself. This ambiguity creates a\ndistinct challenge, where multiple reasoning paths emerge from a single\nquestion, each requiring independent resolution. Since each sub-question is\nambiguous, the model must resolve ambiguity at every step. Thus, answering a\nsingle question requires handling multiple layers of ambiguity throughout the\nreasoning chain. We find that current Large Language Models (LLMs) struggle in\nthis setting, typically exploring wrong reasoning paths and producing\nincomplete answers. To facilitate research on multi-hop ambiguity, we introduce\nMultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE),\na benchmark designed to analyze and evaluate this challenging intersection of\nambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142\nhigh-quality examples of ambiguous multi-hop questions, categorized under a\ntaxonomy of syntactic, general, and semantic ambiguity, and curated through a\nrigorous multi-LLM verification pipeline. Our experiments reveal that even\nstate-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity\ncombined with multi-step inference is a distinct and significant challenge. To\nestablish a robust baseline, we propose CLarifying Ambiguity with a Reasoning\nand InstructiON (CLARION), a multi-agent framework that significantly\noutperforms existing approaches on MIRAGE, paving the way for more adaptive and\nrobust reasoning systems.", "AI": {"tldr": "MIRAGE benchmark introduces ambiguous multi-hop QA where models must handle multiple ambiguous reasoning paths, showing current LLMs struggle with this challenge.", "motivation": "Real-world multi-hop QA involves inherent ambiguity where multiple reasoning paths emerge from single questions, requiring ambiguity resolution at every step - a challenge current LLMs fail to handle properly.", "method": "Created MIRAGE benchmark with 1,142 ambiguous multi-hop questions categorized by ambiguity types (syntactic, general, semantic), verified through multi-LLM pipeline. Proposed CLARION multi-agent framework to address this challenge.", "result": "Experiments show state-of-the-art models struggle significantly on MIRAGE, confirming that combining ambiguity resolution with multi-step inference is a distinct and challenging problem.", "conclusion": "CLARION framework significantly outperforms existing approaches on MIRAGE, establishing a robust baseline and paving way for more adaptive reasoning systems that can handle ambiguity in multi-hop QA."}}
{"id": "2509.22888", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22888", "abs": "https://arxiv.org/abs/2509.22888", "authors": ["Louie Hong Yao", "Nicholas Jarvis", "Tiffany Zhan", "Saptarshi Ghosh", "Linfeng Liu", "Tianyu Jiang"], "title": "JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory", "comment": "22 pages, 10 figures, 5 tables", "summary": "Standard LLM evaluation practices compress diverse abilities into single\nscores, obscuring their inherently multidimensional nature. We present JE-IRT,\na geometric item-response framework that embeds both LLMs and questions in a\nshared space. For question embeddings, the direction encodes semantics and the\nnorm encodes difficulty, while correctness on each question is determined by\nthe geometric interaction between the model and question embeddings. This\ngeometry replaces a global ranking of LLMs with topical specialization and\nenables smooth variation across related questions. Building on this framework,\nour experimental results reveal that out-of-distribution behavior can be\nexplained through directional alignment, and that larger norms consistently\nindicate harder questions. Moreover, JE-IRT naturally supports generalization:\nonce the space is learned, new LLMs are added by fitting a single embedding.\nThe learned space further reveals an LLM-internal taxonomy that only partially\naligns with human-defined subject categories. JE-IRT thus establishes a unified\nand interpretable geometric lens that connects LLM abilities with the structure\nof questions, offering a distinctive perspective on model evaluation and\ngeneralization.", "AI": {"tldr": "JE-IRT is a geometric item-response framework that embeds LLMs and questions in a shared space, replacing global rankings with topical specialization and enabling interpretable model evaluation.", "motivation": "Standard LLM evaluation practices compress diverse abilities into single scores, obscuring their inherently multidimensional nature and limiting interpretability.", "method": "A geometric item-response framework that embeds both LLMs and questions in a shared space, where direction encodes semantics and norm encodes difficulty, with correctness determined by geometric interactions.", "result": "Reveals that out-of-distribution behavior can be explained through directional alignment, larger norms consistently indicate harder questions, and supports generalization by adding new LLMs with single embeddings.", "conclusion": "JE-IRT establishes a unified and interpretable geometric lens connecting LLM abilities with question structure, offering a distinctive perspective on model evaluation and generalization."}}
{"id": "2509.22732", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22732", "abs": "https://arxiv.org/abs/2509.22732", "authors": ["Haibo Tong", "Dongcheng Zhao", "Guobin Shen", "Xiang He", "Dachuan Lin", "Feifei Zhao", "Yi Zeng"], "title": "Bidirectional Intention Inference Enhances LLMs' Defense Against Multi-Turn Jailbreak Attacks", "comment": null, "summary": "The remarkable capabilities of Large Language Models (LLMs) have raised\nsignificant safety concerns, particularly regarding \"jailbreak\" attacks that\nexploit adversarial prompts to bypass safety alignment mechanisms. Existing\ndefense research primarily focuses on single-turn attacks, whereas multi-turn\njailbreak attacks progressively break through safeguards through by concealing\nmalicious intent and tactical manipulation, ultimately rendering conventional\nsingle-turn defenses ineffective. To address this critical challenge, we\npropose the Bidirectional Intention Inference Defense (BIID). The method\nintegrates forward request-based intention inference with backward\nresponse-based intention retrospection, establishing a bidirectional synergy\nmechanism to detect risks concealed within seemingly benign inputs, thereby\nconstructing a more robust guardrails that effectively prevents harmful content\ngeneration. The proposed method undergoes systematic evaluation compared with a\nno-defense baseline and seven representative defense methods across three LLMs\nand two safety benchmarks under 10 different attack methods. Experimental\nresults demonstrate that the proposed method significantly reduces the Attack\nSuccess Rate (ASR) across both single-turn and multi-turn jailbreak attempts,\noutperforming all existing baseline methods while effectively maintaining\npractical utility. Notably, comparative experiments across three multi-turn\nsafety datasets further validate the proposed model's significant advantages\nover other defense approaches.", "AI": {"tldr": "BIID is a bidirectional defense method that combines forward intention inference and backward intention retrospection to detect multi-turn jailbreak attacks on LLMs, significantly reducing attack success rates while maintaining utility.", "motivation": "Existing defenses focus on single-turn jailbreak attacks but fail against multi-turn attacks that progressively bypass safety mechanisms through concealed malicious intent and tactical manipulation.", "method": "Bidirectional Intention Inference Defense (BIID) integrates forward request-based intention inference with backward response-based intention retrospection to detect risks in seemingly benign inputs.", "result": "BIID significantly reduces Attack Success Rate across both single-turn and multi-turn jailbreak attempts, outperforming 7 baseline methods on 3 LLMs and 2 safety benchmarks under 10 attack methods.", "conclusion": "BIID provides robust guardrails against jailbreak attacks, demonstrating significant advantages over other defense approaches while effectively maintaining practical utility."}}
{"id": "2509.22850", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22850", "abs": "https://arxiv.org/abs/2509.22850", "authors": ["Roie Kazoom", "Yuval Ratzabi", "Etamar Rothstein", "Ofer Hadar"], "title": "Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data", "comment": null, "summary": "Adversarial robustness in structured data remains an underexplored frontier\ncompared to vision and language domains. In this work, we introduce a novel\nblack-box, decision-based adversarial attack tailored for tabular data. Our\napproach combines gradient-free direction estimation with an iterative boundary\nsearch, enabling efficient navigation of discrete and continuous feature spaces\nunder minimal oracle access. Extensive experiments demonstrate that our method\nsuccessfully compromises nearly the entire test set across diverse models,\nranging from classical machine learning classifiers to large language model\n(LLM)-based pipelines. Remarkably, the attack achieves success rates\nconsistently above 90%, while requiring only a small number of queries per\ninstance. These results highlight the critical vulnerability of tabular models\nto adversarial perturbations, underscoring the urgent need for stronger\ndefenses in real-world decision-making systems.", "AI": {"tldr": "A novel black-box decision-based adversarial attack for tabular data that achieves over 90% success rates with minimal queries, exposing critical vulnerabilities in tabular models.", "motivation": "Adversarial robustness in structured/tabular data remains underexplored compared to vision and language domains, creating a research gap in this important area.", "method": "Combines gradient-free direction estimation with iterative boundary search to efficiently navigate discrete and continuous feature spaces under minimal oracle access.", "result": "Successfully compromises nearly entire test sets across diverse models (classical ML classifiers to LLM-based pipelines) with success rates consistently above 90% using only small number of queries per instance.", "conclusion": "Highlights critical vulnerability of tabular models to adversarial perturbations and underscores urgent need for stronger defenses in real-world decision-making systems."}}
{"id": "2509.22768", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22768", "abs": "https://arxiv.org/abs/2509.22768", "authors": ["Ekaterina Trofimova", "Zosia Shamina", "Maria Selifanova", "Artem Zaitsev", "Remi Savchuk", "Maxim Minets", "Daria Ozerova", "Emil Sataev", "Denis Zuenko", "Andrey E. Ustyuzhanin"], "title": "ML2B: Multi-Lingual ML Benchmark For AutoML", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating machine learning (ML) code, enabling end-to-end pipeline\nconstruction from natural language instructions. However, existing benchmarks\nfor ML code generation are mainly restricted to English, overlooking the global\nand multilingual nature of ML research and practice. To address this gap, we\npresent ML2B, the first benchmark for evaluating multilingual ML code\ngeneration. ML2B consists of 30 Kaggle competitions translated into 13 natural\nlanguages, covering tabular, text, and image data types, with structured\nmetadata and validated human-reviewed translations. For evaluation, we employ\nAIDE, an automated framework for end-to-end assessment of data science\npipelines, and provide insights into cross-lingual model performance. Our\nresults reveal substantial 15-45% performance degradation on non-English tasks,\nhighlighting critical challenges in multilingual representation learning for\ncode generation. The benchmark, evaluation framework, and comprehensive results\nare made available through our GitHub repository to facilitate future research\nin multilingual ML code generation: https://github.com/enaix/ml2b.", "AI": {"tldr": "ML2B is the first benchmark for evaluating multilingual machine learning code generation, covering 30 Kaggle competitions translated into 13 languages, revealing 15-45% performance degradation on non-English tasks.", "motivation": "Existing ML code generation benchmarks are mainly restricted to English, overlooking the global and multilingual nature of ML research and practice.", "method": "Created ML2B benchmark with 30 Kaggle competitions translated into 13 natural languages, covering tabular, text, and image data types, using AIDE automated framework for end-to-end assessment.", "result": "Substantial 15-45% performance degradation on non-English tasks compared to English, highlighting challenges in multilingual representation learning for code generation.", "conclusion": "The benchmark and evaluation framework are made available to facilitate future research in multilingual ML code generation, addressing critical gaps in current evaluation practices."}}
{"id": "2509.22984", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22984", "abs": "https://arxiv.org/abs/2509.22984", "authors": ["Yu Wu", "Shuo Wu", "Ye Tao", "Yansong Li", "Anand D. Sarwate"], "title": "Not only a helper, but also a teacher: Interactive LLM Cascade", "comment": "29 pages, 4 figures, under review", "summary": "Large Language Models (LLMs) vary widely in their capabilities, with larger\nmodels often having better performance but higher cost: choosing an LLM model\noften involves trading off performance and cost. The LLM Cascade is a paradigm\nthat defers difficult queries from weak/cheap to strong/expensive models. This\napproach is nonadaptive: the deferral decision is trained offline. When\nconfronted with similar or repeated queries, the LLM Cascade may then\nrepeatedly consult the expensive model and incur higher cost. To improve the\ncascading efficiency, we propose Inter-Cascade, an online and interactive LLM\nCascade that extends the role of strong model from a backup helper to a\nlong-term teacher. In our system, when a strong model resolves a difficult\nquery, it also distills its solution into a generalized, reusable\nproblem-solving strategy that boosts the weak model on subsequent queries.\nAdding strategies to queries enables the weak model to dynamically improve its\nperformance over time, avoiding computationally and time-intensive fine-tuning.\nEmpirically, compared with standard LLM Cascade baselines across multiple\nbenchmarks, the Inter-Cascade significantly improves the accuracy of the weak\nmodel (by up to 33.06 absolute percentage points) and the overall system (by up\nto 5.53 absolute percentage points), while reducing the calls to strong models\n(by up to 48.05% relative reduction) and saving the corresponding fees (by up\nto 49.63% relative reduction). Inter-Cascade demonstrates the effective\nin-context knowledge transfer between LLMs, and provides a general, scalable\nframework applicable to both open-source and API-based LLMs.", "AI": {"tldr": "Inter-Cascade is an online interactive LLM cascade system where strong models teach weak models by distilling problem-solving strategies, improving weak model performance by up to 33.06% while reducing strong model calls by up to 48.05%.", "motivation": "Standard LLM cascades are non-adaptive and may repeatedly consult expensive models for similar queries, leading to higher costs. There's a need for more efficient cascading that enables knowledge transfer between models.", "method": "Extends strong models from backup helpers to teachers that distill generalized problem-solving strategies when resolving difficult queries. These strategies boost weak models on subsequent queries without fine-tuning.", "result": "Significantly improves weak model accuracy (up to 33.06% absolute improvement), overall system accuracy (up to 5.53%), while reducing strong model calls (up to 48.05%) and costs (up to 49.63%).", "conclusion": "Inter-Cascade demonstrates effective in-context knowledge transfer between LLMs and provides a scalable framework applicable to both open-source and API-based LLMs."}}
{"id": "2509.22745", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22745", "abs": "https://arxiv.org/abs/2509.22745", "authors": ["Jaehan Kim", "Minkyoo Song", "Seungwon Shin", "Sooel Son"], "title": "Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment", "comment": "Under review", "summary": "Recent large language models (LLMs) have increasingly adopted the\nMixture-of-Experts (MoE) architecture for efficiency. MoE-based LLMs heavily\ndepend on a superficial safety mechanism in which harmful inputs are routed\nsafety-critical experts. However, our analysis reveals that routing decisions\nfor harmful inputs drift significantly after fine-tuning, exposing a critical\nvulnerability to harmful fine-tuning (HFT) attacks. Existing defenses,\nprimarily designed for monolithic LLMs, are less effective for MoE LLMs as they\nfail to prevent drift in harmful input routing. To address this limitation, we\npropose SafeMoE, a safe fine-tuning method tailored to MoE LLMs. SafeMoE\ndirectly mitigates routing drift by penalizing the gap between the routing\nweights of a fine-tuned model and those of the initial safety-aligned model,\nthereby preserving the safety-aligned routing of harmful inputs to\nsafety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to\n141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks,\nreducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while\nmaintaining task utility within 1% degradation and incurring only 2% overhead.\nIt significantly outperforms state-of-the-art defense methods for safeguarding\nLLM fine-tuning and remains effective in recent large-scale MoE LLMs such as\ngpt-oss and Llama 4. Our implementation is available at\nhttps://anonymous.4open.science/r/SafeMoE.", "AI": {"tldr": "SafeMoE is a fine-tuning method for Mixture-of-Experts (MoE) LLMs that prevents harmful fine-tuning attacks by penalizing routing weight drift, maintaining safety while preserving utility.", "motivation": "MoE-based LLMs rely on routing harmful inputs to safety-critical experts, but fine-tuning causes routing drift that exposes vulnerabilities to harmful attacks. Existing defenses designed for monolithic LLMs are ineffective for MoE architectures.", "method": "SafeMoE directly mitigates routing drift by penalizing the gap between fine-tuned and initial safety-aligned model routing weights, preserving harmful input routing to safety-critical experts.", "result": "SafeMoE reduces harmfulness score from 62.0 to 5.0 on OLMoE, maintains task utility within 1% degradation, incurs only 2% overhead, and outperforms state-of-the-art defense methods across 7B to 141B parameter MoE LLMs.", "conclusion": "SafeMoE effectively safeguards MoE LLMs against harmful fine-tuning attacks by preventing routing drift, demonstrating strong performance across various model scales including recent large-scale MoE models like gpt-oss and Llama 4."}}
{"id": "2509.22851", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22851", "abs": "https://arxiv.org/abs/2509.22851", "authors": ["Yaswanth Chittepu", "Prasann Singhal", "Greg Durrett", "Scott Niekum"], "title": "Adaptive Margin RLHF via Preference over Preferences", "comment": null, "summary": "Margin-based optimization is fundamental to improving generalization and\nrobustness in classification tasks. In the context of reward model learning\nfrom preferences within Reinforcement Learning from Human Feedback (RLHF),\nexisting methods typically rely on no margins, fixed margins, or margins that\nare simplistic functions of preference ratings. However, such formulations\noften fail to account for the varying strengths of different preferences, for\nexample some preferences are associated with larger margins between responses,\nor they rely on noisy margin information derived from ratings. We argue that\nmodeling the strength of preferences can lead to better generalization and more\nfaithful alignment. Furthermore, many existing methods that use adaptive\nmargins assume access to accurate preference scores, which can be difficult for\nhumans to provide reliably. We propose an approach that leverages preferences\nover preferences, that is annotations indicating which of two preferences\nreflects a stronger distinction. We use this ordinal signal to infer adaptive\nmargins on a per-datapoint basis. We introduce an extension to Direct\nPreference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from\npreference-over-preference supervision, enabling improved discriminative and\ngenerative performance. Empirically, our method outperforms vanilla DPO, DPO\nwith fixed margins, and DPO with ground-truth margins on the UltraFeedback\ndataset. Additionally, we show that there is a tradeoff between discriminative\nand generative performance: improving test classification accuracy,\nparticularly by correctly labeling weaker preferences at the expense of\nstronger ones, can lead to a decline in generative quality. To navigate this\ntradeoff, we propose two sampling strategies to gather\npreference-over-preference labels: one favoring discriminative performance and\none favoring generative performance.", "AI": {"tldr": "The paper proposes DPO-PoP, an extension to Direct Preference Optimization that uses preference-over-preference annotations to infer adaptive margins, improving both discriminative and generative performance over vanilla DPO and fixed-margin approaches.", "motivation": "Existing margin-based optimization methods in RLHF often use no margins, fixed margins, or simplistic margin functions that don't account for varying preference strengths. These approaches fail to model preference strength differences and rely on noisy margin information from ratings.", "method": "The method leverages preference-over-preference annotations (indicating which of two preferences reflects a stronger distinction) to infer adaptive margins on a per-datapoint basis. It extends DPO with these adaptive margins and proposes two sampling strategies for gathering preference-over-preference labels.", "result": "Empirically, DPO-PoP outperforms vanilla DPO, DPO with fixed margins, and DPO with ground-truth margins on the UltraFeedback dataset. The method reveals a tradeoff between discriminative and generative performance.", "conclusion": "Modeling preference strength through adaptive margins leads to better generalization and alignment. The proposed approach successfully navigates the tradeoff between discriminative and generative performance using different sampling strategies for preference-over-preference label collection."}}
{"id": "2509.22808", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22808", "abs": "https://arxiv.org/abs/2509.22808", "authors": ["Mohamed Maged", "Alhassan Ehab", "Ali Mekky", "Besher Hassan", "Shady Shehata"], "title": "ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection", "comment": null, "summary": "With the rise of generative text-to-speech models, distinguishing between\nreal and synthetic speech has become challenging, especially for Arabic that\nhave received limited research attention. Most spoof detection efforts have\nfocused on English, leaving a significant gap for Arabic and its many dialects.\nIn this work, we introduce the first multi-dialect Arabic spoofed speech\ndataset. To evaluate the difficulty of the synthesized audio from each model\nand determine which produces the most challenging samples, we aimed to guide\nthe construction of our final dataset either by merging audios from multiple\nmodels or by selecting the best-performing model, we conducted an evaluation\npipeline that included training classifiers using two approaches: modern\nembedding-based methods combined with classifier heads; classical machine\nlearning algorithms applied to MFCC features; and the RawNet2 architecture. The\npipeline further incorporated the calculation of Mean Opinion Score based on\nhuman ratings, as well as processing both original and synthesized datasets\nthrough an Automatic Speech Recognition model to measure the Word Error Rate.\nOur results demonstrate that FishSpeech outperforms other TTS models in Arabic\nvoice cloning on the Casablanca corpus, producing more realistic and\nchallenging synthetic speech samples. However, relying on a single TTS for\ndataset creation may limit generalizability.", "AI": {"tldr": "This paper introduces the first multi-dialect Arabic spoofed speech dataset and evaluates various TTS models to identify which produces the most challenging synthetic speech for spoof detection.", "motivation": "With the rise of generative text-to-speech models, distinguishing real from synthetic speech has become challenging, especially for Arabic which has received limited research attention compared to English.", "method": "Created multi-dialect Arabic spoofed speech dataset; evaluated TTS models using classifiers (embedding-based methods with classifier heads, classical ML on MFCC features, RawNet2 architecture), Mean Opinion Score from human ratings, and Word Error Rate from ASR processing.", "result": "FishSpeech outperforms other TTS models in Arabic voice cloning on the Casablanca corpus, producing more realistic and challenging synthetic speech samples.", "conclusion": "FishSpeech produces the most challenging synthetic speech for Arabic, but relying on a single TTS for dataset creation may limit generalizability."}}
{"id": "2509.22989", "categories": ["cs.AI", "cs.CY", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.22989", "abs": "https://arxiv.org/abs/2509.22989", "authors": ["Zirui Cheng", "Jiaxuan You"], "title": "Towards Strategic Persuasion with Language Models", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong persuasive capabilities\ncomparable to those of humans, offering promising benefits while raising\nsocietal concerns about their deployment. However, systematically evaluating\nthe persuasive capabilities of LLMs is inherently challenging, as the\neffectiveness of persuasion among humans varies significantly across different\ndomains. In this paper, we take a theory-driven approach to provide a scalable\nand principled framework for measuring the persuasive capabilities of LLMs.\nGrounded in the Bayesian Persuasion (BP) framework, we repurpose existing\nhuman-human persuasion datasets to construct environments for evaluating and\ntraining LLMs in strategic persuasion. Our results reveal that frontier models\ncan consistently achieve high persuasion gains and exhibit sophisticated\npersuasion strategies that align with theoretical predictions. Building on\nthis, we use reinforcement learning to train LLMs for strategic persuasion in\nour environments. Our results also demonstrate that even small LLMs can obtain\nsignificantly higher persuasion gains through reinforcement learning.", "AI": {"tldr": "This paper proposes a theory-driven framework using Bayesian Persuasion to systematically evaluate LLMs' persuasive capabilities, showing that frontier models achieve high persuasion gains and can be further improved through reinforcement learning.", "motivation": "LLMs have shown strong persuasive abilities comparable to humans, but systematic evaluation is challenging due to domain variations in human persuasion effectiveness.", "method": "Used Bayesian Persuasion framework to repurpose human-human persuasion datasets for evaluating LLMs, and applied reinforcement learning to train LLMs for strategic persuasion.", "result": "Frontier models consistently achieve high persuasion gains with sophisticated strategies, and even small LLMs obtain significantly higher gains through reinforcement learning.", "conclusion": "The proposed framework provides scalable and principled measurement of LLMs' persuasive capabilities, demonstrating both current strengths and potential for improvement through training."}}
{"id": "2509.22757", "categories": ["cs.CR", "cs.AI", "cs.NI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.22757", "abs": "https://arxiv.org/abs/2509.22757", "authors": ["Petar Radanliev"], "title": "Red Teaming Quantum-Resistant Cryptographic Standards: A Penetration Testing Framework Integrating AI and Quantum Security", "comment": null, "summary": "This study presents a structured approach to evaluating vulnerabilities\nwithin quantum cryptographic protocols, focusing on the BB84 quantum key\ndistribution method and National Institute of Standards and Technology (NIST)\napproved quantum-resistant algorithms. By integrating AI-driven red teaming,\nautomated penetration testing, and real-time anomaly detection, the research\ndevelops a framework for assessing and mitigating security risks in quantum\nnetworks. The findings demonstrate that AI can be effectively used to simulate\nadversarial attacks, probe weaknesses in cryptographic implementations, and\nrefine security mechanisms through iterative feedback. The use of automated\nexploit simulations and protocol fuzzing provides a scalable means of\nidentifying latent vulnerabilities, while adversarial machine learning\ntechniques highlight novel attack surfaces within AI-enhanced cryptographic\nprocesses. This study offers a comprehensive methodology for strengthening\nquantum security and provides a foundation for integrating AI-driven\ncybersecurity practices into the evolving quantum landscape.", "AI": {"tldr": "AI-driven framework for evaluating quantum cryptographic protocol vulnerabilities using red teaming, penetration testing, and anomaly detection.", "motivation": "To develop a structured approach for assessing and mitigating security risks in quantum networks, particularly focusing on BB84 QKD and NIST-approved quantum-resistant algorithms.", "method": "Integration of AI-driven red teaming, automated penetration testing, real-time anomaly detection, automated exploit simulations, protocol fuzzing, and adversarial machine learning techniques.", "result": "AI effectively simulates adversarial attacks, probes cryptographic weaknesses, and refines security mechanisms through iterative feedback; automated methods identify latent vulnerabilities and highlight novel attack surfaces.", "conclusion": "Provides a comprehensive methodology for strengthening quantum security and establishes foundation for integrating AI-driven cybersecurity practices in quantum landscape."}}
{"id": "2509.22855", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22855", "abs": "https://arxiv.org/abs/2509.22855", "authors": ["Sameep Chattopadhyay", "Nikhil Karamchandani", "Sharayu Mohair"], "title": "Observation-Free Attacks on Online Learning to Rank", "comment": null, "summary": "Online learning to rank (OLTR) plays a critical role in information retrieval\nand machine learning systems, with a wide range of applications in search\nengines and content recommenders. However, despite their extensive adoption,\nthe susceptibility of OLTR algorithms to coordinated adversarial attacks\nremains poorly understood. In this work, we present a novel framework for\nattacking some of the widely used OLTR algorithms. Our framework is designed to\npromote a set of target items so that they appear in the list of top-K\nrecommendations for T - o(T) rounds, while simultaneously inducing linear\nregret in the learning algorithm. We propose two novel attack strategies:\nCascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical\nguarantees showing that both strategies require only O(log T) manipulations to\nsucceed. Additionally, we supplement our theoretical analysis with empirical\nresults on real-world data.", "AI": {"tldr": "This paper presents a framework for attacking online learning to rank (OLTR) algorithms through coordinated adversarial attacks that promote target items to top recommendations while causing linear regret.", "motivation": "Despite widespread use of OLTR algorithms in search engines and recommender systems, their vulnerability to coordinated adversarial attacks remains poorly understood.", "method": "Proposed two attack strategies: CascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB, designed to promote target items to top-K recommendations while inducing linear regret.", "result": "Theoretical guarantees show both attack strategies require only O(log T) manipulations to succeed. Empirical results on real-world data support the theoretical findings.", "conclusion": "The framework demonstrates significant vulnerabilities in widely used OLTR algorithms to coordinated adversarial attacks, highlighting security concerns in practical applications."}}
{"id": "2509.22812", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22812", "abs": "https://arxiv.org/abs/2509.22812", "authors": ["Kai Zhang", "Christopher Malon", "Lichao Sun", "Martin Renqiang Min"], "title": "EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation", "comment": null, "summary": "Radiology report generation requires advanced medical image analysis,\neffective temporal reasoning, and accurate text generation. Although recent\ninnovations, particularly multimodal large language models (MLLMs), have shown\nimproved performance, their supervised fine-tuning (SFT) objective is not\nexplicitly aligned with clinical efficacy. In this work, we introduce EditGRPO,\na mixed-policy reinforcement learning (RL) algorithm designed specifically to\noptimize the generation through clinically motivated rewards. EditGRPO\nintegrates on-policy exploration with off-policy guidance by injecting\nsentence-level detailed corrections during training rollouts. This mixed-policy\napproach addresses the exploration dilemma and sampling efficiency issues\ntypically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with\nsupervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO\nbaselines, achieving an average improvement of 3.4% in CheXbert, GREEN,\nRadgraph, and RATEScore metrics across four major chest X-ray report generation\ndatasets. Notably, EditGRPO also demonstrates superior out-of-domain\ngeneralization, with an average performance gain of 5.9% on unseen datasets.", "AI": {"tldr": "EditGRPO is a mixed-policy RL algorithm that optimizes radiology report generation using clinically motivated rewards, outperforming SFT and vanilla GRPO baselines with improved metrics and superior out-of-domain generalization.", "motivation": "Current MLLMs for radiology report generation use SFT objectives not explicitly aligned with clinical efficacy, lacking optimization for clinical effectiveness.", "method": "EditGRPO integrates on-policy exploration with off-policy guidance through sentence-level corrections during training, addressing RL exploration and sampling efficiency issues.", "result": "Applied to Qwen2.5-VL-3B MLLM, EditGRPO achieved 3.4% average improvement in CheXbert, GREEN, Radgraph, and RATEScore metrics across four chest X-ray datasets, with 5.9% gain on unseen datasets.", "conclusion": "EditGRPO effectively optimizes radiology report generation for clinical efficacy through mixed-policy RL with clinically motivated rewards, demonstrating strong performance and generalization."}}
{"id": "2509.23004", "categories": ["cs.AI", "cs.SC", "math.AG"], "pdf": "https://arxiv.org/pdf/2509.23004", "abs": "https://arxiv.org/abs/2509.23004", "authors": ["Karan Srivastava", "Sanjeeb Dash", "Ryan Cory-Wright", "Barry Trager", "Lior Horesh"], "title": "AI Noether -- Bridging the Gap Between Scientific Laws Derived by AI Systems and Canonical Knowledge via Abductive Inference", "comment": "22 Pages (13+appendix), 6 Figures, Preprint", "summary": "A core goal in modern science is to harness recent advances in AI and\ncomputer processing to automate and accelerate the scientific method. Symbolic\nregression can fit interpretable models to data, but these models often sit\noutside established theory. Recent systems (e.g., AI Descartes, AI Hilbert)\nenforce derivability from prior axioms. However, sometimes new data and\nassociated hypotheses derived from data are not consistent with existing theory\nbecause the existing theory is incomplete or incorrect. Automating abductive\ninference to close this gap remains open. We propose a solution: an algebraic\ngeometry-based system that, given an incomplete axiom system and a hypothesis\nthat it cannot explain, automatically generates a minimal set of missing axioms\nthat suffices to derive the axiom, as long as axioms and hypotheses are\nexpressible as polynomial equations. We formally establish necessary and\nsufficient conditions for the successful retrieval of such axioms. We\nillustrate the efficacy of our approach by demonstrating its ability to explain\nKepler's third law and a few other laws, even when key axioms are absent.", "AI": {"tldr": "An algebraic geometry-based system that automatically generates missing axioms to explain hypotheses when existing theory is incomplete, using polynomial equations.", "motivation": "To automate abductive inference and close gaps in incomplete axiom systems when new hypotheses cannot be explained by existing theory.", "method": "Uses algebraic geometry to generate minimal sets of missing axioms from polynomial equations when given incomplete axioms and unprovable hypotheses.", "result": "Establishes necessary and sufficient conditions for axiom retrieval and successfully explains Kepler's third law and other laws even with missing key axioms.", "conclusion": "The proposed system effectively automates abductive inference to complete incomplete theories using algebraic geometry methods."}}
{"id": "2509.22762", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22762", "abs": "https://arxiv.org/abs/2509.22762", "authors": ["Friedrich Doku", "Peter Dinda"], "title": "TRUSTCHECKPOINTS: Time Betrays Malware for Unconditional Software Root of Trust", "comment": null, "summary": "Modern IoT and embedded platforms must start execution from a known trusted\nstate to thwart malware, ensure secure firmware updates, and protect critical\ninfrastructure. Current approaches to establish a root of trust depend on\nsecret keys and/or specialized secure hardware, which drives up costs, may\ninvolve third parties, adds operational complexity, and relies on assumptions\nabout an attacker's computational power. In contrast, TRUSTCHECKPOINTS is the\nfirst system to establish an unconditional software root of trust based on a\nformal model without relying on secrets or trusted hardware. Developers capture\na full-system checkpoint and later roll back to it and prove this to an\nexternal verifier. The verifier issues timing-constrained, randomized\nk-independent polynomial challenges (via Horner's rule) that repeatedly scan\nthe fast on-chip memory in randomized passes. When malicious code attempts to\npersist, it must swap into slower, unchecked off-chip storage, causing a\ndetectable timing delay.\n  Our prototype for a commodity ARM Cortex-A53-based platform validates 192 KB\nof SRAM in approximately 10 s using 500 passes, sufficient to detect\nsingle-instruction persistent malware. The prototype then seamlessly extends\ntrust to DRAM. Two modes (fast SRAM-bootstrap and comprehensive full-memory\nscan) allow trade-offs between speed and coverage, demonstrating reliable\nmalware detection on unmodified hardware.", "AI": {"tldr": "TRUSTCHECKPOINTS is a system that establishes an unconditional software root of trust without secrets or trusted hardware by using timing-constrained polynomial challenges to verify system integrity through memory scanning.", "motivation": "Current root of trust approaches rely on secret keys and secure hardware, which increase costs, complexity, and depend on computational assumptions about attackers. There's a need for a more fundamental approach.", "method": "Developers capture full-system checkpoints and roll back to them. Verifier issues timing-constrained, randomized k-independent polynomial challenges using Horner's rule to repeatedly scan on-chip memory. Malicious code causes detectable timing delays when swapping to off-chip storage.", "result": "Prototype on ARM Cortex-A53 validates 192 KB SRAM in ~10s using 500 passes, detecting single-instruction persistent malware. Successfully extends trust to DRAM with two modes (fast SRAM-bootstrap and comprehensive full-memory scan).", "conclusion": "TRUSTCHECKPOINTS provides reliable malware detection on unmodified hardware without secrets or trusted hardware, offering trade-offs between speed and coverage through different verification modes."}}
{"id": "2509.22868", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22868", "abs": "https://arxiv.org/abs/2509.22868", "authors": ["Zehao Niu", "Mihai Anitescu", "Jie Chen"], "title": "Neighborhood Sampling Does Not Learn the Same Graph Neural Network", "comment": null, "summary": "Neighborhood sampling is an important ingredient in the training of\nlarge-scale graph neural networks. It suppresses the exponential growth of the\nneighborhood size across network layers and maintains feasible memory\nconsumption and time costs. While it becomes a standard implementation in\npractice, its systemic behaviors are less understood. We conduct a theoretical\nanalysis by using the tool of neural tangent kernels, which characterize the\n(analogous) training dynamics of neural networks based on their infinitely wide\ncounterparts -- Gaussian processes (GPs). We study several established\nneighborhood sampling approaches and the corresponding posterior GP. With\nlimited samples, the posteriors are all different, although they converge to\nthe same one as the sample size increases. Moreover, the posterior covariance,\nwhich lower-bounds the mean squared prediction error, is uncomparable, aligning\nwith observations that no sampling approach dominates.", "AI": {"tldr": "Theoretical analysis of neighborhood sampling in graph neural networks using neural tangent kernels, showing different sampling methods produce different posterior GPs with limited samples but converge to the same distribution as sample size increases.", "motivation": "Neighborhood sampling is widely used in large-scale GNN training to control memory and time costs, but its systemic behaviors are not well understood theoretically.", "method": "Used neural tangent kernels to analyze training dynamics through infinitely wide counterparts (Gaussian processes), studied various established neighborhood sampling approaches and their corresponding posterior GPs.", "result": "With limited samples, different sampling methods produce different posterior GPs, but they converge to the same distribution as sample size increases. Posterior covariances are uncomparable, explaining why no sampling approach dominates in practice.", "conclusion": "Neighborhood sampling methods exhibit different behaviors with limited samples but converge asymptotically, and no single method is universally superior, which aligns with empirical observations."}}
{"id": "2509.22824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22824", "abs": "https://arxiv.org/abs/2509.22824", "authors": ["Chi Ruan", "Dongfu Jiang", "Yubo Wang", "Wenhu Chen"], "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a popular training paradigm,\nparticularly when paired with reasoning models. While effective, it primarily\nfocuses on generating responses and lacks mechanisms to explicitly foster\ncritique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)\nand Critique-Guided-Distillation (CGD) have shown the benefits of explicitly\nteaching LLMs how to critique. Motivated by them, we propose Critique\nReinforcement Learning (CRL), where the model is tasked with generating a\ncritique for a given (question, solution) pair. The reward is determined solely\nby whether the final judgment label $c \\in \\{\\texttt{True}, \\texttt{False}\\}$\nof the generated critique aligns with the ground-truth judgment $c^*$. Building\non this point, we introduce \\textsc{Critique-Coder}, which is trained on a\nhybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL\ndata. We fine-tune multiple models (\\textsc{Critique-Coder}) and evaluate them\non different benchmarks to show their advantages over RL-only models. We show\nthat \\textsc{Critique-Coder} consistently outperforms RL-only baselines on all\nthe evaluated benchmarks. Notably, our \\textsc{Critique-Coder-8B} can reach\nover 60\\% on LiveCodeBench (v5), outperforming other reasoning models like\nDeepCoder-14B and GPT-o1. Beyond code generation, \\textsc{Critique-Coder} also\ndemonstrates enhanced general reasoning abilities, as evidenced by its better\nperformance on logic reasoning tasks from the BBEH dataset. This indicates that\nthe application of CRL on coding datasets enhances general reasoning and\ncritique abilities, which are transferable across a broad range of tasks.\nHence, we believe that CRL works as a great complement to standard RL for LLM\nreasoning.", "AI": {"tldr": "Critique Reinforcement Learning (CRL) enhances standard RL by training models to generate critiques, improving both coding and general reasoning performance.", "motivation": "Standard RL focuses on response generation but lacks explicit critique mechanisms. Recent studies show benefits of teaching LLMs to critique, motivating CRL development.", "method": "Propose CRL where model generates critiques for (question, solution) pairs, rewarded by alignment of judgment labels. Introduce Critique-Coder trained on hybrid RL+CRL data (20% CRL substitution).", "result": "Critique-Coder consistently outperforms RL-only baselines, achieving over 60% on LiveCodeBench (v5) and better performance on BBEH logic reasoning tasks.", "conclusion": "CRL effectively complements standard RL, enhancing general reasoning and critique abilities transferable across diverse tasks."}}
{"id": "2509.23006", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23006", "abs": "https://arxiv.org/abs/2509.23006", "authors": ["Hassen Dhrif"], "title": "Creative Adversarial Testing (CAT): A Novel Framework for Evaluating Goal-Oriented Agentic AI Systems", "comment": null, "summary": "Agentic AI represents a paradigm shift in enhancing the capabilities of\ngenerative AI models. While these systems demonstrate immense potential and\npower, current evaluation techniques primarily focus on assessing their\nefficacy in identifying appropriate agents, tools, and parameters. However, a\ncritical gap exists in evaluating the alignment between an Agentic AI system's\ntasks and its overarching goals. This paper introduces the Creative Adversarial\nTesting (CAT) framework, a novel approach designed to capture and analyze the\ncomplex relationship between Agentic AI tasks and the system's intended\nobjectives.\n  We validate the CAT framework through extensive simulation using synthetic\ninteraction data modeled after Alexa+ audio services, a sophisticated Agentic\nAI system that shapes the user experience for millions of users globally. This\nsynthetic data approach enables comprehensive testing of edge cases and failure\nmodes while protecting user privacy. Our results demonstrate that the CAT\nframework provides unprecedented insights into goal-task alignment, enabling\nmore effective optimization and development of Agentic AI systems.", "AI": {"tldr": "The paper introduces Creative Adversarial Testing (CAT) framework to evaluate goal-task alignment in Agentic AI systems, validated through synthetic Alexa+ audio services data.", "motivation": "Current evaluation techniques for Agentic AI focus on efficacy in identifying agents, tools, and parameters but lack assessment of alignment between system tasks and overarching goals.", "method": "Proposes Creative Adversarial Testing (CAT) framework and validates it using extensive simulation with synthetic interaction data modeled after Alexa+ audio services.", "result": "CAT framework provides unprecedented insights into goal-task alignment, enabling more effective optimization and development of Agentic AI systems.", "conclusion": "The CAT framework successfully addresses the critical gap in evaluating goal-task alignment in Agentic AI systems."}}
{"id": "2509.22796", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22796", "abs": "https://arxiv.org/abs/2509.22796", "authors": ["Xingyu Li", "Juefei Pu", "Yifan Wu", "Xiaochen Zou", "Shitong Zhu", "Xiaochen Zou", "Shitong Zhu", "Qiushi Wu", "Zheng Zhang", "Joshua Hsu", "Yue Dong", "Zhiyun Qian", "Kangjie Lu", "Trent Jaeger", "Michael De Lucia", "Srikanth V. Krishnamurthy"], "title": "What Do They Fix? LLM-Aided Categorization of Security Patches for Critical Memory Bugs", "comment": null, "summary": "Open-source software projects are foundational to modern software ecosystems,\nwith the Linux kernel standing out as a critical exemplar due to its ubiquity\nand complexity. Although security patches are continuously integrated into the\nLinux mainline kernel, downstream maintainers often delay their adoption,\ncreating windows of vulnerability. A key reason for this lag is the difficulty\nin identifying security-critical patches, particularly those addressing\nexploitable vulnerabilities such as out-of-bounds (OOB) accesses and\nuse-after-free (UAF) bugs. This challenge is exacerbated by intentionally\nsilent bug fixes, incomplete or missing CVE assignments, delays in CVE\nissuance, and recent changes to the CVE assignment criteria for the Linux\nkernel. While fine-grained patch classification approaches exist, they exhibit\nlimitations in both coverage and accuracy. In this work, we identify previously\nunexplored opportunities to significantly improve fine-grained patch\nclassification. Specifically, by leveraging cues from commit titles/messages\nand diffs alongside appropriate code context, we develop DUALLM, a dual-method\npipeline that integrates two approaches based on a Large Language Model (LLM)\nand a fine-tuned small language model. DUALLM achieves 87.4% accuracy and an\nF1-score of 0.875, significantly outperforming prior solutions. Notably, DUALLM\nsuccessfully identified 111 of 5,140 recent Linux kernel patches as addressing\nOOB or UAF vulnerabilities, with 90 true positives confirmed by manual\nverification (many do not have clear indications in patch descriptions).\nMoreover, we constructed proof-of-concepts for two identified bugs (one UAF and\none OOB), including one developed to conduct a previously unknown control-flow\nhijack as further evidence of the correctness of the classification.", "AI": {"tldr": "DUALLM is a dual-method pipeline using LLM and fine-tuned small language model to identify security-critical patches in Linux kernel, achieving 87.4% accuracy and successfully detecting 111 OOB/UAF vulnerabilities from 5,140 patches.", "motivation": "Security patches in Linux kernel are often delayed due to difficulty identifying security-critical patches, especially exploitable vulnerabilities like OOB and UAF bugs, exacerbated by silent fixes and incomplete CVE assignments.", "method": "Developed DUALLM pipeline integrating two approaches: LLM-based and fine-tuned small language model, leveraging commit titles/messages, diffs, and appropriate code context for fine-grained patch classification.", "result": "Achieved 87.4% accuracy and F1-score of 0.875, identified 111 OOB/UAF vulnerabilities from 5,140 patches with 90 true positives confirmed, and constructed PoCs for two bugs including control-flow hijack.", "conclusion": "DUALLM significantly outperforms prior solutions in identifying security-critical patches, demonstrating practical effectiveness in detecting exploitable vulnerabilities in Linux kernel patches."}}
{"id": "2509.22881", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22881", "abs": "https://arxiv.org/abs/2509.22881", "authors": ["Karim Khamaisi", "Nicolas Keller", "Stefan Krummenacher", "Valentin Huber", "Bernhard F\u00e4ssler", "Bruno Rodrigues"], "title": "From Noise to Knowledge: A Comparative Study of Acoustic Anomaly Detection Models in Pumped-storage Hydropower Plants", "comment": null, "summary": "In the context of industrial factories and energy producers, unplanned\noutages are highly costly and difficult to service. However, existing\nacoustic-anomaly detection studies largely rely on generic industrial or\nsynthetic datasets, with few focused on hydropower plants due to limited\naccess. This paper presents a comparative analysis of acoustic-based anomaly\ndetection methods, as a way to improve predictive maintenance in hydropower\nplants. We address key challenges in the acoustic preprocessing under highly\nnoisy conditions before extracting time- and frequency-domain features. Then,\nwe benchmark three machine learning models: LSTM AE, K-Means, and OC-SVM, which\nare tested on two real-world datasets from the Rodundwerk II pumped-storage\nplant in Austria, one with induced anomalies and one with real-world\nconditions. The One-Class SVM achieved the best trade-off of accuracy (ROC AUC\n0.966-0.998) and minimal training time, while the LSTM autoencoder delivered\nstrong detection (ROC AUC 0.889-0.997) at the expense of higher computational\ncost.", "AI": {"tldr": "This paper presents a comparative analysis of acoustic-based anomaly detection methods for predictive maintenance in hydropower plants, testing LSTM AE, K-Means, and OC-SVM on real-world datasets from an Austrian pumped-storage plant.", "motivation": "Unplanned outages in industrial factories and energy producers are highly costly and difficult to service, but existing acoustic-anomaly detection studies largely rely on generic datasets with limited focus on hydropower plants due to access constraints.", "method": "Address acoustic preprocessing challenges in noisy conditions, extract time- and frequency-domain features, and benchmark three machine learning models (LSTM AE, K-Means, OC-SVM) on two real-world datasets from Rodundwerk II pumped-storage plant.", "result": "One-Class SVM achieved the best trade-off with accuracy (ROC AUC 0.966-0.998) and minimal training time, while LSTM autoencoder delivered strong detection (ROC AUC 0.889-0.997) but with higher computational cost.", "conclusion": "Acoustic-based anomaly detection methods show promise for predictive maintenance in hydropower plants, with OC-SVM providing the most practical balance of performance and efficiency for real-world applications."}}
{"id": "2509.22830", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22830", "abs": "https://arxiv.org/abs/2509.22830", "authors": ["Hwan Chang", "Yonghyun Jun", "Hwanhee Lee"], "title": "ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents", "comment": null, "summary": "The growing deployment of large language model (LLM) based agents that\ninteract with external environments has created new attack surfaces for\nadversarial manipulation. One major threat is indirect prompt injection, where\nattackers embed malicious instructions in external environment output, causing\nagents to interpret and execute them as if they were legitimate prompts. While\nprevious research has focused primarily on plain-text injection attacks, we\nfind a significant yet underexplored vulnerability: LLMs' dependence on\nstructured chat templates and their susceptibility to contextual manipulation\nthrough persuasive multi-turn dialogues. To this end, we introduce ChatInject,\nan attack that formats malicious payloads to mimic native chat templates,\nthereby exploiting the model's inherent instruction-following tendencies.\nBuilding on this foundation, we develop a persuasion-driven Multi-turn variant\nthat primes the agent across conversational turns to accept and execute\notherwise suspicious actions. Through comprehensive experiments across frontier\nLLMs, we demonstrate three critical findings: (1) ChatInject achieves\nsignificantly higher average attack success rates than traditional prompt\ninjection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13%\nto 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong\nperformance at average 52.33% success rate on InjecAgent, (2)\nchat-template-based payloads demonstrate strong transferability across models\nand remain effective even against closed-source LLMs, despite their unknown\ntemplate structures, and (3) existing prompt-based defenses are largely\nineffective against this attack approach, especially against Multi-turn\nvariants. These findings highlight vulnerabilities in current agent systems.", "AI": {"tldr": "ChatInject is a novel attack method that exploits LLM agents' chat template dependencies through structured malicious payloads and multi-turn persuasion dialogues, achieving significantly higher success rates than traditional prompt injection attacks.", "motivation": "The growing deployment of LLM-based agents interacting with external environments creates new attack surfaces, particularly for indirect prompt injection where attackers embed malicious instructions in environment output.", "method": "ChatInject formats malicious payloads to mimic native chat templates and uses multi-turn persuasion dialogues to prime agents into accepting suspicious actions, exploiting LLMs' instruction-following tendencies.", "result": "ChatInject achieved average attack success rates of 32.05% on AgentDojo (vs 5.18% traditional) and 45.90% on InjecAgent (vs 15.13% traditional), with multi-turn variants reaching 52.33% success on InjecAgent. The attack shows strong transferability across models and bypasses existing defenses.", "conclusion": "Current agent systems have significant vulnerabilities to chat-template-based attacks, and existing prompt-based defenses are largely ineffective, especially against multi-turn persuasion variants."}}
{"id": "2509.23023", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23023", "abs": "https://arxiv.org/abs/2509.23023", "authors": ["Davi Bastos Costa", "Renato Vicente"], "title": "Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia", "comment": "20 pages, 7 figures, 5 tables; submitted to ICLR 2026; Code and data:\n  https://github.com/bastoscostadavi/llm-mafia-game", "summary": "Mafia is a social deduction game where informed mafia compete against\nuninformed townsfolk. Its asymmetry of information and reliance on\ntheory-of-mind reasoning mirror real-world multi-agent scenarios, making it a\nuseful testbed for evaluating the social intelligence of large language models\n(LLMs). To support a systematic study, we introduce Mini-Mafia: a simplified\nfour-player variant with one mafioso, one detective, and two villagers. We set\nthe mafioso to kill a villager and the detective to investigate the mafioso\nduring the night, reducing the game to a single day phase of discussion and\nvoting. This setup isolates three interactive capabilities through\nrole-specific win conditions: the mafioso must deceive, the villagers must\ndetect deception, and the detective must effectively disclose information. To\nmeasure these skills, we have LLMs play against each other, creating the\nMini-Mafia Benchmark: a two-stage framework that first estimates win rates\nwithin fixed opponent configurations, then aggregates performance across them\nusing standardized scoring. Built entirely from model interactions without\nexternal data, the benchmark evolves as new models are introduced, with each\none serving both as a new opponent and as a subject of evaluation. Our\nexperiments reveal counterintuitive results, including cases where smaller\nmodels outperform larger ones. Beyond benchmarking, Mini-Mafia enables\nquantitative study of emergent multi-agent dynamics such as name bias and\nlast-speaker advantage. It also contributes to AI safety by generating training\ndata for deception detectors and by tracking models' deception capabilities\nagainst human baselines.", "AI": {"tldr": "Mini-Mafia is a simplified 4-player social deduction game used as a benchmark to evaluate LLMs' social intelligence through deception detection and information disclosure capabilities.", "motivation": "Mafia games mirror real-world multi-agent scenarios with asymmetric information and theory-of-mind reasoning, making them ideal for testing LLMs' social intelligence.", "method": "Created Mini-Mafia benchmark with 4 roles (mafioso, detective, 2 villagers) and single-day voting phase. LLMs play against each other in two-stage framework to measure win rates across opponent configurations.", "result": "Experiments revealed counterintuitive results where smaller models sometimes outperformed larger ones. The benchmark enables study of emergent dynamics like name bias and last-speaker advantage.", "conclusion": "Mini-Mafia serves as evolving benchmark for LLM social intelligence evaluation, contributes to AI safety by tracking deception capabilities, and generates training data for deception detectors."}}
{"id": "2509.22814", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22814", "abs": "https://arxiv.org/abs/2509.22814", "authors": ["Aditi Tiwari", "Akshit Bhalla", "Darshan Prasad"], "title": "Model Context Protocol for Vision Systems: Audit, Security, and Protocol Extensions", "comment": "Accepted to NeurIPS 2025 Workshop on Bridging Language, Agent, and\n  World Models for Reasoning and Planning (LAW 2025)", "summary": "The Model Context Protocol (MCP) defines a schema bound execution model for\nagent-tool interaction, enabling modular computer vision workflows without\nretraining. To our knowledge, this is the first protocol level, deployment\nscale audit of MCP in vision systems, identifying systemic weaknesses in schema\nsemantics, interoperability, and runtime coordination. We analyze 91 publicly\nregistered vision centric MCP servers, annotated along nine dimensions of\ncompositional fidelity, and develop an executable benchmark with validators to\ndetect and categorize protocol violations. The audit reveals high prevalence of\nschema format divergence, missing runtime schema validation, undeclared\ncoordinate conventions, and reliance on untracked bridging scripts. Validator\nbased testing quantifies these failures, with schema format checks flagging\nmisalignments in 78.0 percent of systems, coordinate convention checks\ndetecting spatial reference errors in 24.6 percent, and memory scope checks\nissuing an average of 33.8 warnings per 100 executions. Security probes show\nthat dynamic and multi agent workflows exhibit elevated risks of privilege\nescalation and untyped tool connections. The proposed benchmark and validator\nsuite, implemented in a controlled testbed and to be released on GitHub,\nestablishes a reproducible framework for measuring and improving the\nreliability and security of compositional vision workflows.", "AI": {"tldr": "First deployment-scale audit of Model Context Protocol (MCP) in vision systems reveals systemic weaknesses in schema semantics, interoperability, and runtime coordination across 91 vision-centric MCP servers.", "motivation": "To identify and quantify systemic weaknesses in MCP protocol implementation for vision systems, enabling modular computer vision workflows without retraining.", "method": "Analyzed 91 publicly registered vision-centric MCP servers using nine dimensions of compositional fidelity, developed executable benchmark with validators to detect protocol violations, and conducted security probes.", "result": "High prevalence of schema format divergence (78.0%), coordinate convention errors (24.6%), memory scope warnings (33.8 per 100 executions), and security risks in dynamic/multi-agent workflows with privilege escalation and untyped tool connections.", "conclusion": "The proposed benchmark and validator suite establishes a reproducible framework for measuring and improving reliability and security of compositional vision workflows, to be released on GitHub."}}
{"id": "2509.22907", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22907", "abs": "https://arxiv.org/abs/2509.22907", "authors": ["Anutam Srinivasan", "Aditya T. Vadlamani", "Amin Meghrazi", "Srinivasan Parthasarathy"], "title": "FedCF: Fair Federated Conformal Prediction", "comment": "Preprint", "summary": "Conformal Prediction (CP) is a widely used technique for quantifying\nuncertainty in machine learning models. In its standard form, CP offers\nprobabilistic guarantees on the coverage of the true label, but it is agnostic\nto sensitive attributes in the dataset. Several recent works have sought to\nincorporate fairness into CP by ensuring conditional coverage guarantees across\ndifferent subgroups. One such method is Conformal Fairness (CF). In this work,\nwe extend the CF framework to the Federated Learning setting and discuss how we\ncan audit a federated model for fairness by analyzing the fairness-related gaps\nfor different demographic groups. We empirically validate our framework by\nconducting experiments on several datasets spanning multiple domains, fully\nleveraging the exchangeability assumption.", "AI": {"tldr": "Extending Conformal Fairness to Federated Learning to audit model fairness across demographic groups while maintaining probabilistic guarantees.", "motivation": "Standard Conformal Prediction provides uncertainty quantification but ignores sensitive attributes. Recent works aim to incorporate fairness, but this hasn't been explored in Federated Learning settings.", "method": "Extend the Conformal Fairness framework to Federated Learning, analyzing fairness-related gaps across demographic groups while leveraging exchangeability assumptions.", "result": "Empirical validation on multiple datasets across different domains shows the framework effectively audits federated models for fairness.", "conclusion": "The proposed framework successfully brings fairness auditing to Federated Learning while maintaining the probabilistic guarantees of Conformal Prediction."}}
{"id": "2509.22845", "categories": ["cs.CL", "cs.IR", "cs.LG", "H.3.3; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.22845", "abs": "https://arxiv.org/abs/2509.22845", "authors": ["Kai Hua", "Zhiyuan Feng", "Chongyang Tao", "Rui Yan", "Lu Zhang"], "title": "Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems", "comment": "10 pages, 4 figures, accepted by CIKM 2020", "summary": "Recently, knowledge-grounded conversations in the open domain gain great\nattention from researchers. Existing works on retrieval-based dialogue systems\nhave paid tremendous efforts to utilize neural networks to build a matching\nmodel, where all of the context and knowledge contents are used to match the\nresponse candidate with various representation methods. Actually, different\nparts of the context and knowledge are differentially important for recognizing\nthe proper response candidate, as many utterances are useless due to the topic\nshift. Those excessive useless information in the context and knowledge can\ninfluence the matching process and leads to inferior performance. To address\nthis problem, we propose a multi-turn \\textbf{R}esponse \\textbf{S}election\n\\textbf{M}odel that can \\textbf{D}etect the relevant parts of the\n\\textbf{C}ontext and \\textbf{K}nowledge collection (\\textbf{RSM-DCK}). Our\nmodel first uses the recent context as a query to pre-select relevant parts of\nthe context and knowledge collection at the word-level and utterance-level\nsemantics. Further, the response candidate interacts with the selected context\nand knowledge collection respectively. In the end, The fused representation of\nthe context and response candidate is utilized to post-select the relevant\nparts of the knowledge collection more confidently for matching. We test our\nproposed model on two benchmark datasets. Evaluation results indicate that our\nmodel achieves better performance than the existing methods, and can\neffectively detect the relevant context and knowledge for response selection.", "AI": {"tldr": "Proposes RSM-DCK model for multi-turn response selection that detects relevant parts of context and knowledge to improve dialogue matching performance.", "motivation": "Existing retrieval-based dialogue systems use all context and knowledge content for matching, but much information is irrelevant due to topic shifts, which negatively impacts performance.", "method": "Uses recent context as query to pre-select relevant context/knowledge at word/utterance levels, then interacts response candidate with selected content, and finally fuses context-response representation to post-select knowledge for confident matching.", "result": "Achieves better performance than existing methods on two benchmark datasets and effectively detects relevant context and knowledge.", "conclusion": "The proposed RSM-DCK model successfully addresses the problem of excessive irrelevant information in dialogue systems by detecting relevant parts of context and knowledge for improved response selection."}}
{"id": "2509.23045", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23045", "abs": "https://arxiv.org/abs/2509.23045", "authors": ["Zonghan Yang", "Shengjie Wang", "Kelin Fu", "Wenyang He", "Weimin Xiong", "Yibo Liu", "Yibo Miao", "Bofei Gao", "Yejie Wang", "Yingwei Ma", "Yanhao Li", "Yue Liu", "Zhenxing Hu", "Kaitai Zhang", "Shuyi Wang", "Huarong Chen", "Flood Sung", "Yang Liu", "Yang Gao", "Zhilin Yang", "Tianyu Liu"], "title": "Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents", "comment": "58 pages", "summary": "Large Language Models (LLMs) are increasingly applied to software engineering\n(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent\nframeworks with multi-turn interactions and workflow-based Agentless methods\nwith single-turn verifiable steps. We argue these paradigms are not mutually\nexclusive: reasoning-intensive Agentless training induces skill priors,\nincluding localization, code edit, and self-reflection that enable efficient\nand effective SWE-Agent adaptation. In this work, we first curate the Agentless\ntraining recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\\%\non SWE-bench Verified, the best among workflow approaches. With additional SFT\nadaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to\n48.6\\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These\nresults show that structured skill priors from Agentless training can bridge\nworkflow and agentic frameworks for transferable coding agents.", "AI": {"tldr": "Agentless training provides structured skill priors that enable effective SWE-Agent adaptation, with Kimi-Dev achieving 60.4% on SWE-bench Verified and powering SWE-Agents to 48.6% pass@1.", "motivation": "To bridge the gap between SWE-Agent frameworks (multi-turn interactions) and Agentless methods (single-turn verifiable steps) by showing they are not mutually exclusive.", "method": "Curated Agentless training recipe to develop Kimi-Dev, then applied SFT adaptation on 5k publicly-available trajectories to enable SWE-Agent capabilities.", "result": "Kimi-Dev achieved 60.4% on SWE-bench Verified (best among workflow approaches) and powered SWE-Agents to 48.6% pass@1, comparable to Claude 3.5 Sonnet.", "conclusion": "Structured skill priors from Agentless training can effectively bridge workflow and agentic frameworks for creating transferable coding agents."}}
{"id": "2509.22857", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22857", "abs": "https://arxiv.org/abs/2509.22857", "authors": ["Eduardo Chielle", "Manaar Alam", "Jinting Liu", "Jovan Kascelan", "Michail Maniatakos"], "title": "PAPER: Privacy-Preserving ResNet Models using Low-Degree Polynomial Approximations and Structural Optimizations on Leveled FHE", "comment": null, "summary": "Recent work has made non-interactive privacy-preserving inference more\npractical by running deep Convolution Neural Network (CNN) with Fully\nHomomorphic Encryption (FHE). However, these methods remain limited by their\nreliance on bootstrapping, a costly FHE operation applied across multiple\nlayers, severely slowing inference. They also depend on high-degree polynomial\napproximations of non-linear activations, which increase multiplicative depth\nand reduce accuracy by 2-5% compared to plaintext ReLU models. In this work, we\nfocus on ResNets, a widely adopted benchmark architecture in privacy-preserving\ninference, and close the accuracy gap between their FHE-based non-interactive\nmodels and plaintext counterparts, while also achieving faster inference than\nexisting methods. We use a quadratic polynomial approximation of ReLU, which\nachieves the theoretical minimum multiplicative depth for non-linear\nactivations, along with a penalty-based training strategy. We further introduce\nstructural optimizations such as node fusing, weight redistribution, and tower\nreuse. These optimizations reduce the required FHE levels in CNNs by nearly a\nfactor of five compared to prior work, allowing us to run ResNet models under\nleveled FHE without bootstrapping. To further accelerate inference and recover\naccuracy typically lost with polynomial approximations, we introduce parameter\nclustering along with a joint strategy of data encoding layout and ensemble\ntechniques. Experiments with ResNet-18, ResNet-20, and ResNet-32 on CIFAR-10\nand CIFAR-100 show that our approach achieves up to 4x faster private inference\nthan prior work with comparable accuracy to plaintext ReLU models.", "AI": {"tldr": "This paper presents a method to improve privacy-preserving inference using FHE on ResNets, achieving comparable accuracy to plaintext models and 4x faster inference than prior work by minimizing multiplicative depth and eliminating bootstrapping.", "motivation": "Existing FHE-based CNN inference methods suffer from slow performance due to costly bootstrapping operations and accuracy degradation from high-degree polynomial approximations of non-linear activations.", "method": "Uses quadratic polynomial approximation of ReLU (minimum multiplicative depth), penalty-based training, structural optimizations (node fusing, weight redistribution, tower reuse), parameter clustering, and ensemble techniques with optimized data encoding.", "result": "Achieves up to 4x faster private inference than prior work with comparable accuracy to plaintext ReLU models on ResNet-18/20/32 with CIFAR-10/100 datasets, reducing required FHE levels by nearly 5x.", "conclusion": "The proposed approach successfully closes the accuracy gap between FHE-based and plaintext ResNet models while significantly accelerating inference through optimized polynomial approximations and structural improvements that eliminate bootstrapping."}}
{"id": "2509.22913", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22913", "abs": "https://arxiv.org/abs/2509.22913", "authors": ["Jake S. Rhodes", "Adam G. Rustad", "Marshall S. Nielsen", "Morgan Chase McClellan", "Dallan Gardner", "Dawson Hedges"], "title": "Guided Manifold Alignment with Geometry-Regularized Twin Autoencoders", "comment": "10 pages, 4 figures, 7 tables. Accepted at the MMAI workshop at ICDM,\n  2025", "summary": "Manifold alignment (MA) involves a set of techniques for learning shared\nrepresentations across domains, yet many traditional MA methods are incapable\nof performing out-of-sample extension, limiting their real-world applicability.\nWe propose a guided representation learning framework leveraging a\ngeometry-regularized twin autoencoder (AE) architecture to enhance MA while\nenabling generalization to unseen data. Our method enforces structured\ncross-modal mappings to maintain geometric fidelity in learned embeddings. By\nincorporating a pre-trained alignment model and a multitask learning\nformulation, we improve cross-domain generalization and representation\nrobustness while maintaining alignment fidelity. We evaluate our approach using\nseveral MA methods, showing improvements in embedding consistency, information\npreservation, and cross-domain transfer. Additionally, we apply our framework\nto Alzheimer's disease diagnosis, demonstrating its ability to integrate\nmulti-modal patient data and enhance predictive accuracy in cases limited to a\nsingle domain by leveraging insights from the multi-modal problem.", "AI": {"tldr": "A geometry-regularized twin autoencoder framework for manifold alignment that enables out-of-sample extension and improves cross-domain generalization while maintaining geometric fidelity in embeddings.", "motivation": "Traditional manifold alignment methods lack out-of-sample extension capability, limiting real-world applicability. There's a need for methods that can generalize to unseen data while maintaining alignment quality.", "method": "Uses a twin autoencoder architecture with geometry regularization to enforce structured cross-modal mappings. Incorporates pre-trained alignment model and multitask learning to improve generalization and robustness.", "result": "Shows improvements in embedding consistency, information preservation, and cross-domain transfer. Applied to Alzheimer's disease diagnosis, it enhances predictive accuracy by integrating multi-modal patient data.", "conclusion": "The proposed framework effectively addresses limitations of traditional manifold alignment methods by enabling out-of-sample extension and improving cross-domain generalization while maintaining alignment fidelity."}}
{"id": "2509.22854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22854", "abs": "https://arxiv.org/abs/2509.22854", "authors": ["Jiaqian Li", "Yanshu Li", "Ligong Han", "Ruixiang Tang", "Wenya Wang"], "title": "Towards Generalizable Implicit In-Context Learning with Attention Routing", "comment": null, "summary": "Implicit in-context learning (ICL) has newly emerged as a promising paradigm\nthat simulates ICL behaviors in the representation space of Large Language\nModels (LLMs), aiming to attain few-shot performance at zero-shot cost.\nHowever, existing approaches largely rely on injecting shift vectors into\nresidual flows, which are typically constructed from labeled demonstrations or\ntask-specific alignment. Such designs fall short of utilizing the structural\nmechanisms underlying ICL and suffer from limited generalizability. To address\nthis, we propose In-Context Routing (ICR), a novel implicit ICL method that\ninternalizes generalizable ICL patterns at the attention logits level. It\nextracts reusable structural directions that emerge during ICL and employs a\nlearnable input-conditioned router to modulate attention logits accordingly,\nenabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world\ndatasets spanning diverse domains and multiple LLMs. The results show that ICR\nconsistently outperforms prior implicit ICL methods that require task-specific\nretrieval or training, while demonstrating robust generalization to\nout-of-domain tasks where existing methods struggle. These findings position\nICR to push the boundary of ICL's practical value.", "AI": {"tldr": "Proposes In-Context Routing (ICR), a novel implicit ICL method that extracts reusable structural patterns from attention logits to achieve few-shot performance at zero-shot cost, outperforming previous methods on 12 datasets.", "motivation": "Existing implicit ICL methods rely on injecting shift vectors into residual flows from labeled demonstrations or task-specific alignment, which fails to utilize the structural mechanisms of ICL and has limited generalizability.", "method": "ICR internalizes generalizable ICL patterns at the attention logits level, extracts reusable structural directions that emerge during ICL, and uses a learnable input-conditioned router to modulate attention logits, enabling a train-once-and-reuse framework.", "result": "ICR consistently outperforms prior implicit ICL methods on 12 real-world datasets across diverse domains and multiple LLMs, while demonstrating robust generalization to out-of-domain tasks where existing methods struggle.", "conclusion": "ICR pushes the boundary of ICL's practical value by providing a more generalizable and effective approach to implicit in-context learning."}}
{"id": "2509.23058", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23058", "abs": "https://arxiv.org/abs/2509.23058", "authors": ["Yikai Wang", "Xiaocheng Li", "Guanting Chen"], "title": "Risk Profiling and Modulation for LLMs", "comment": null, "summary": "Large language models (LLMs) are increasingly used for decision-making tasks\nunder uncertainty; however, their risk profiles and how they are influenced by\nprompting and alignment methods remain underexplored. Existing studies have\nprimarily examined personality prompting or multi-agent interactions, leaving\nopen the question of how post-training influences the risk behavior of LLMs. In\nthis work, we propose a new pipeline for eliciting, steering, and modulating\nLLMs' risk profiles, drawing on tools from behavioral economics and finance.\nUsing utility-theoretic models, we compare pre-trained, instruction-tuned, and\nRLHF-aligned LLMs, and find that while instruction-tuned models exhibit\nbehaviors consistent with some standard utility formulations, pre-trained and\nRLHF-aligned models deviate more from any utility models fitted. We further\nevaluate modulation strategies, including prompt engineering, in-context\nlearning, and post-training, and show that post-training provides the most\nstable and effective modulation of risk preference. Our findings provide\ninsights into the risk profiles of different classes and stages of LLMs and\ndemonstrate how post-training modulates these profiles, laying the groundwork\nfor future research on behavioral alignment and risk-aware LLM design.", "AI": {"tldr": "This paper investigates how different LLM training stages (pre-trained, instruction-tuned, RLHF-aligned) exhibit varying risk behaviors and proposes methods to modulate these risk profiles using behavioral economics tools.", "motivation": "LLMs are increasingly used for decision-making under uncertainty, but their risk profiles and how they're influenced by prompting and alignment methods remain underexplored. Existing studies focus on personality prompting or multi-agent interactions, leaving gaps in understanding post-training influences on LLM risk behavior.", "method": "Proposed a pipeline for eliciting, steering, and modulating LLMs' risk profiles using utility-theoretic models from behavioral economics and finance. Compared pre-trained, instruction-tuned, and RLHF-aligned LLMs, and evaluated modulation strategies including prompt engineering, in-context learning, and post-training.", "result": "Instruction-tuned models exhibit behaviors consistent with standard utility formulations, while pre-trained and RLHF-aligned models deviate more from fitted utility models. Post-training provides the most stable and effective modulation of risk preference compared to other strategies.", "conclusion": "The findings provide insights into risk profiles of different LLM classes and stages, demonstrating how post-training effectively modulates these profiles, laying groundwork for future research on behavioral alignment and risk-aware LLM design."}}
{"id": "2509.22873", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.22873", "abs": "https://arxiv.org/abs/2509.22873", "authors": ["Aashnan Rahman", "Abid Hasan", "Sherajul Arifin", "Faisal Haque Bappy", "Tahrim Hossain", "Tariqul Islam", "Abu Raihan Mostofa Kamal", "Md. Azam Hossain"], "title": "AntiFLipper: A Secure and Efficient Defense Against Label-Flipping Attacks in Federated Learning", "comment": "Submitted to IEEE International Conference on Communications (ICC)\n  2026", "summary": "Federated learning (FL) enables privacy-preserving model training by keeping\ndata decentralized. However, it remains vulnerable to label-flipping attacks,\nwhere malicious clients manipulate labels to poison the global model. Despite\ntheir simplicity, these attacks can severely degrade model performance, and\ndefending against them remains challenging. We introduce AntiFLipper, a novel\nand computationally efficient defense against multi-class label-flipping\nattacks in FL. Unlike existing methods that ensure security at the cost of high\ncomputational overhead, AntiFLipper employs a novel client-side detection\nstrategy, significantly reducing the central server's burden during\naggregation. Comprehensive empirical evaluations across multiple datasets under\ndifferent distributions demonstrate that AntiFLipper achieves accuracy\ncomparable to state-of-the-art defenses while requiring substantially fewer\ncomputational resources in server side. By balancing security and efficiency,\nAntiFLipper addresses a critical gap in existing defenses, making it\nparticularly suitable for resource-constrained FL deployments where both model\nintegrity and operational efficiency are essential.", "AI": {"tldr": "AntiFLipper is a computationally efficient defense against label-flipping attacks in federated learning that uses client-side detection to reduce server burden while maintaining high accuracy.", "motivation": "Federated learning is vulnerable to label-flipping attacks that can severely degrade model performance, and existing defenses have high computational overhead that limits practical deployment.", "method": "AntiFLipper employs a novel client-side detection strategy for multi-class label-flipping attacks, significantly reducing the central server's computational burden during aggregation.", "result": "Comprehensive evaluations show AntiFLipper achieves accuracy comparable to state-of-the-art defenses while requiring substantially fewer computational resources on the server side.", "conclusion": "AntiFLipper effectively balances security and efficiency, addressing a critical gap in existing defenses and making it suitable for resource-constrained FL deployments."}}
{"id": "2509.22921", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22921", "abs": "https://arxiv.org/abs/2509.22921", "authors": ["Matthieu Zimmer", "Xiaotong Ji", "Tu Nguyen", "Haitham Bou Ammar"], "title": "Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective", "comment": null, "summary": "We introduce a novel approach to large language model (LLM) distillation by\nformulating it as a constrained reinforcement learning problem. While recent\nwork has begun exploring the integration of task-specific rewards into\ndistillation processes, existing methods typically rely on ad-hoc reward\nweighting. We propose a principled optimization framework that maximizes\ntask-specific rewards while constraining the divergence from the teacher model\nto remain below a specified threshold. Our approach adapts constrained state\naugmented reinforcement learning to the distillation setting, introducing a\nmodified reward function that maintains theoretical guarantees of constraint\nsatisfaction without requiring state augmentation or teacher model access\nduring deployment and without the computational overhead of the dual Lagrangian\nmethods. Through extensive experiments on mathematical reasoning tasks, we\ndemonstrate that our method achieves better constraint satisfaction rates and\nbetter reasoning compared to the soft Lagrangian relaxation baselines while\nmaintaining competitive task performance. Our framework provides a\ntheoretically grounded and practically efficient solution for reward-aware\ndistillation in resource-constrained settings.", "AI": {"tldr": "A novel constrained RL approach for LLM distillation that maximizes task rewards while constraining divergence from teacher model, achieving better constraint satisfaction and reasoning than baselines.", "motivation": "Existing LLM distillation methods use ad-hoc reward weighting; need principled framework to balance task rewards and teacher fidelity with theoretical guarantees.", "method": "Formulate distillation as constrained RL problem using modified reward function that ensures constraint satisfaction without state augmentation or teacher access during deployment.", "result": "Better constraint satisfaction rates and reasoning performance compared to soft Lagrangian baselines, while maintaining competitive task performance on mathematical reasoning tasks.", "conclusion": "Provides theoretically grounded and efficient solution for reward-aware distillation in resource-constrained settings."}}
{"id": "2509.22856", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22856", "abs": "https://arxiv.org/abs/2509.22856", "authors": ["R. Alexander Knipper", "Charles S. Knipper", "Kaiqi Zhang", "Valerie Sims", "Clint Bowers", "Santu Karmaker"], "title": "The Bias is in the Details: An Assessment of Cognitive Bias in LLMs", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly embedded in real-world\ndecision-making processes, it becomes crucial to examine the extent to which\nthey exhibit cognitive biases. Extensively studied in the field of psychology,\ncognitive biases appear as systematic distortions commonly observed in human\njudgments. This paper presents a large-scale evaluation of eight\nwell-established cognitive biases across 45 LLMs, analyzing over 2.8 million\nLLM responses generated through controlled prompt variations. To achieve this,\nwe introduce a novel evaluation framework based on multiple-choice tasks,\nhand-curate a dataset of 220 decision scenarios targeting fundamental cognitive\nbiases in collaboration with psychologists, and propose a scalable approach for\ngenerating diverse prompts from human-authored scenario templates. Our analysis\nshows that LLMs exhibit bias-consistent behavior in 17.8-57.3% of instances\nacross a range of judgment and decision-making contexts targeting anchoring,\navailability, confirmation, framing, interpretation, overattribution, prospect\ntheory, and representativeness biases. We find that both model size and prompt\nspecificity play a significant role on bias susceptibility as follows: larger\nsize (>32B parameters) can reduce bias in 39.5% of cases, while higher prompt\ndetail reduces most biases by up to 14.9%, except in one case\n(Overattribution), which is exacerbated by up to 8.8%.", "AI": {"tldr": "Large-scale evaluation of 45 LLMs reveals they exhibit cognitive biases in 17.8-57.3% of cases across 8 bias types, with model size and prompt specificity significantly affecting bias susceptibility.", "motivation": "As LLMs are increasingly used in real-world decision-making, it's crucial to examine whether they exhibit the same cognitive biases that systematically distort human judgments.", "method": "Developed novel evaluation framework using multiple-choice tasks, curated dataset of 220 decision scenarios with psychologists, generated diverse prompts from human-authored templates, analyzed over 2.8 million LLM responses across 45 models.", "result": "LLMs showed bias-consistent behavior in 17.8-57.3% of instances across 8 cognitive biases. Larger models (>32B parameters) reduced bias in 39.5% of cases, while detailed prompts reduced most biases by up to 14.9% (except Overattribution bias, which increased by 8.8%).", "conclusion": "LLMs do exhibit cognitive biases similar to humans, with bias susceptibility influenced by model size and prompt design, highlighting the need for careful consideration when deploying LLMs in decision-making contexts."}}
{"id": "2509.23102", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23102", "abs": "https://arxiv.org/abs/2509.23102", "authors": ["Fang Wu", "Xu Huang", "Weihao Xuan", "Zhiwei Zhang", "Yijia Xiao", "Guancheng Wan", "Xiaomin Li", "Bing Hu", "Peng Xia", "Jure Leskovec", "Yejin Choi"], "title": "Multiplayer Nash Preference Optimization", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the standard\nparadigm for aligning large language models (LLMs) with human preferences.\nHowever, reward-based methods built on the Bradley-Terry assumption struggle to\ncapture the non-transitive and heterogeneous nature of real-world preferences.\nTo address this, recent studies have reframed alignment as a two-player Nash\ngame, giving rise to Nash learning from human feedback (NLHF). While this\nperspective has inspired algorithms such as INPO, ONPO, and EGPO with strong\ntheoretical and empirical guarantees, they remain fundamentally restricted to\ntwo-player interactions, creating a single-opponent bias that fails to capture\nthe full complexity of realistic preference structures. In this work, we\nintroduce Multiplayer Nash Preference Optimization (MNPO), a novel framework\nthat generalizes NLHF to the multiplayer regime. It formulates alignment as an\n$n$-player game, where each policy competes against a population of opponents\nwhile being regularized toward a reference model. Our framework establishes\nwell-defined Nash equilibria in multiplayer settings and extends the concept of\nduality gap to quantify approximation quality. We demonstrate that MNPO\ninherits the equilibrium guarantees of two-player methods while enabling richer\ncompetitive dynamics and improved coverage of diverse preference structures.\nThrough comprehensive empirical evaluation, we show that MNPO consistently\noutperforms existing NLHF baselines on instruction-following benchmarks,\nachieving superior alignment quality under heterogeneous annotator conditions\nand mixed-policy evaluation scenarios. Together, these results establish MNPO\nas a principled and scalable framework for aligning LLMs with complex,\nnon-transitive human preferences. Code is available at\nhttps://github.com/smiles724/MNPO.", "AI": {"tldr": "MNPO extends Nash learning from human feedback to multiplayer settings, addressing limitations of two-player methods by formulating alignment as an n-player game where policies compete against populations of opponents.", "motivation": "Existing RLHF methods based on Bradley-Terry assumptions struggle with non-transitive and heterogeneous real-world preferences, while current Nash learning approaches are limited to two-player interactions creating single-opponent bias.", "method": "Multiplayer Nash Preference Optimization (MNPO) formulates alignment as an n-player game where each policy competes against a population of opponents while being regularized toward a reference model, extending duality gap concepts to multiplayer settings.", "result": "MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios.", "conclusion": "MNPO establishes a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences, inheriting equilibrium guarantees while enabling richer competitive dynamics."}}
{"id": "2509.22900", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22900", "abs": "https://arxiv.org/abs/2509.22900", "authors": ["Haochen Gong", "Zhen Tao", "Shidong Pan", "Zhenchang Xing", "Xiaoyu Sun"], "title": "Towards Context-aware Mobile Privacy Notice: Implementation of A Deployable Contextual Privacy Policies Generator", "comment": "Accepted by ASE 2025, Tool Demonstration Track", "summary": "Lengthy and legally phrased privacy policies impede users' understanding of\nhow mobile applications collect and process personal data. Prior work proposed\nContextual Privacy Policies (CPPs) for mobile apps to display shorter policy\nsnippets only in the corresponding user interface contexts, but the pipeline\ncould not be deployable in real-world mobile environments. In this paper, we\npresent PrivScan, the first deployable CPP Software Development Kit (SDK) for\nAndroid. It captures live app screenshots to identify GUI elements associated\nwith types of personal data and displays CPPs in a concise, user-facing format.\nWe provide a lightweight floating button that offers low-friction, on-demand\ncontrol. The architecture leverages remote deployment to decouple the\nmultimodal backend pipeline from a mobile client comprising five modular\ncomponents, thereby reducing on-device resource demands and easing\ncross-platform portability. A feasibility-oriented evaluation shows an average\nexecution time of 9.15\\,s, demonstrating the practicality of our approach. The\nsource code of PrivScan is available at https://github.com/buyanghc/PrivScan\nand the demo video can be found at https://www.youtube.com/watch?v=ck-25otfyHc.", "AI": {"tldr": "PrivScan is the first deployable Contextual Privacy Policy SDK for Android that captures live app screenshots to identify GUI elements associated with personal data and displays concise privacy policies through a lightweight floating button.", "motivation": "Lengthy and legally phrased privacy policies impede users' understanding of how mobile apps collect and process personal data. Previous Contextual Privacy Policy approaches were not deployable in real-world mobile environments.", "method": "Uses a remote deployment architecture with multimodal backend pipeline and mobile client comprising five modular components. Captures live app screenshots to identify GUI elements, displays CPPs in concise format with low-friction floating button control.", "result": "Average execution time of 9.15 seconds, demonstrating practical feasibility. The system successfully decouples backend processing from mobile client, reducing on-device resource demands and enabling cross-platform portability.", "conclusion": "PrivScan provides the first deployable CPP SDK for Android that offers practical, on-demand privacy policy access with acceptable performance overhead, making contextual privacy policies feasible for real-world mobile environments."}}
{"id": "2509.22931", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22931", "abs": "https://arxiv.org/abs/2509.22931", "authors": ["Shreyas Gokhale"], "title": "MonoCon: A general framework for learning ultra-compact high-fidelity representations using monotonicity constraints", "comment": "16 pages, 7 figures", "summary": "Learning high-quality, robust, efficient, and disentangled representations is\na central challenge in artificial intelligence (AI). Deep metric learning\nframeworks tackle this challenge primarily using architectural and optimization\nconstraints. Here, we introduce a third approach that instead relies on\n$\\textit{functional}$ constraints. Specifically, we present MonoCon, a simple\nframework that uses a small monotonic multi-layer perceptron (MLP) head\nattached to any pre-trained encoder. Due to co-adaptation between encoder and\nhead guided by contrastive loss and monotonicity constraints, MonoCon learns\nrobust, disentangled, and highly compact embeddings at a practically negligible\nperformance cost. On the CIFAR-100 image classification task, MonoCon yields\nrepresentations that are nearly 9x more compact and 1.5x more robust than the\nfine-tuned encoder baseline, while retaining 99\\% of the baseline's 5-NN\nclassification accuracy. We also report a 3.4x more compact and 1.4x more\nrobust representation on an SNLI sentence similarity task for a marginal\nreduction in the STSb score, establishing MonoCon as a general domain-agnostic\nframework. Crucially, these robust, ultra-compact representations learned via\nfunctional constraints offer a unified solution to critical challenges in\ndisparate contexts ranging from edge computing to cloud-scale retrieval.", "AI": {"tldr": "MonoCon is a framework that uses a small monotonic MLP head attached to pre-trained encoders with contrastive loss and monotonicity constraints to learn robust, disentangled, and highly compact embeddings with minimal performance cost.", "motivation": "To address the challenge of learning high-quality, robust, efficient, and disentangled representations in AI, which current methods primarily tackle through architectural and optimization constraints.", "method": "Attach a small monotonic multi-layer perceptron (MLP) head to any pre-trained encoder and train with contrastive loss and monotonicity constraints, leveraging co-adaptation between encoder and head.", "result": "On CIFAR-100: 9x more compact and 1.5x more robust representations while retaining 99% of baseline's 5-NN classification accuracy. On SNLI: 3.4x more compact and 1.4x more robust with marginal STSb score reduction.", "conclusion": "MonoCon provides a general domain-agnostic framework that learns robust, ultra-compact representations through functional constraints, offering unified solutions for edge computing to cloud-scale retrieval applications."}}
{"id": "2509.22870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22870", "abs": "https://arxiv.org/abs/2509.22870", "authors": ["Passant Elchafei", "Mayar Osama", "Mohamed Rageh", "Mervat Abuelkheir"], "title": "Lexicon-Enriched Graph Modeling for Arabic Document Readability Prediction", "comment": null, "summary": "We present a graph-based approach enriched with lexicons to predict\ndocument-level readability in Arabic, developed as part of the Constrained\nTrack of the BAREC Shared Task 2025. Our system models each document as a\nsentence-level graph, where nodes represent sentences and lemmas, and edges\ncapture linguistic relationships such as lexical co-occurrence and class\nmembership. Sentence nodes are enriched with features from the SAMER lexicon as\nwell as contextual embeddings from the Arabic transformer model. The graph\nneural network (GNN) and transformer sentence encoder are trained as two\nindependent branches, and their predictions are combined via late fusion at\ninference. For document-level prediction, sentence-level outputs are aggregated\nusing max pooling to reflect the most difficult sentence. Experimental results\nshow that this hybrid method outperforms standalone GNN or transformer branches\nacross multiple readability metrics. Overall, the findings highlight that\nfusion offers advantages at the document level, but the GNN-only approach\nremains stronger for precise prediction of sentence-level readability.", "AI": {"tldr": "A graph-based approach with lexicons for Arabic document readability prediction using sentence-level graphs with GNN and transformer branches combined via late fusion.", "motivation": "To develop an effective method for predicting document-level readability in Arabic by leveraging linguistic relationships and contextual embeddings.", "method": "Model documents as sentence-level graphs with nodes for sentences/lemmas and edges for linguistic relationships. Use SAMER lexicon features and Arabic transformer embeddings. Train GNN and transformer branches independently and combine via late fusion with max pooling aggregation.", "result": "The hybrid method outperforms standalone GNN or transformer branches across multiple readability metrics. Fusion works better at document level, while GNN-only is stronger for sentence-level prediction.", "conclusion": "Graph-based approaches enriched with lexicons and transformer embeddings effectively predict Arabic readability, with fusion strategies offering advantages at document level."}}
{"id": "2509.23108", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.23108", "abs": "https://arxiv.org/abs/2509.23108", "authors": ["Morgan McCarty", "Jorge Morales"], "title": "Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models", "comment": "30 pages,15 figures", "summary": "This study offers a novel approach for benchmarking complex cognitive\nbehavior in artificial systems. Almost universally, Large Language Models\n(LLMs) perform best on tasks which may be included in their training data and\ncan be accomplished solely using natural language, limiting our understanding\nof their emergent sophisticated cognitive capacities. In this work, we created\ndozens of novel items of a classic mental imagery task from cognitive\npsychology. A task which, traditionally, cognitive psychologists have argued is\nsolvable exclusively via visual mental imagery (i.e., language alone would be\ninsufficient). LLMs are perfect for testing this hypothesis. First, we tested\nseveral state-of-the-art LLMs by giving text-only models written instructions\nand asking them to report the resulting object after performing the\ntransformations in the aforementioned task. Then, we created a baseline by\ntesting 100 human subjects in exactly the same task. We found that the best\nLLMs performed significantly above average human performance. Finally, we\ntested reasoning models set to different levels of reasoning and found the\nstrongest performance when models allocate greater amounts of reasoning tokens.\nThese results provide evidence that the best LLMs may have the capability to\ncomplete imagery-dependent tasks despite the non-pictorial nature of their\narchitectures. Our study not only demonstrates an emergent cognitive capacity\nin LLMs while performing a novel task, but it also provides the field with a\nnew task that leaves lots of room for improvement in otherwise already highly\ncapable models. Finally, our findings reignite the debate over the formats of\nrepresentation of visual imagery in humans, suggesting that propositional\nreasoning (or at least non-imagistic reasoning) may be sufficient to complete\ntasks that were long-thought to be imagery-dependent.", "AI": {"tldr": "LLMs outperform humans on mental imagery tasks traditionally thought to require visual imagery, suggesting propositional reasoning may be sufficient for such tasks.", "motivation": "To test if LLMs can solve mental imagery tasks that cognitive psychologists argue require visual mental imagery, challenging the assumption that language alone is insufficient.", "method": "Created novel mental imagery tasks from cognitive psychology, tested state-of-the-art LLMs with text-only instructions, established human baseline with 100 subjects, and tested reasoning models with different reasoning token allocations.", "result": "Best LLMs performed significantly above average human performance, with strongest performance when models allocated more reasoning tokens.", "conclusion": "LLMs may have capability to complete imagery-dependent tasks despite non-pictorial architectures, suggesting propositional reasoning may be sufficient for tasks previously thought to require visual imagery."}}
{"id": "2509.22965", "categories": ["cs.CR", "68M14, 94A60", "K.6.5; D.4.6; J.1"], "pdf": "https://arxiv.org/pdf/2509.22965", "abs": "https://arxiv.org/abs/2509.22965", "authors": ["Yousef Tahboub", "Anthony Revilla", "Jaydon Lynch", "Greg Floyd"], "title": "Blockchain Voting System", "comment": null, "summary": "Casting a ballot from a phone or laptop sounds appealing, but only if voters\ncan be confident their choice remains secret and results cannot be altered in\nthe dark. This paper proposes a hybrid blockchain-based voting model that\nstores encrypted votes on a private blockchain maintained by election\norganizers and neutral observers, while periodically anchoring hashes of these\nvotes onto a public blockchain as a tamper-evident seal. The system issues\nvoters one-time blind-signed tokens to protect anonymity, and provides receipts\nso they can confirm their vote was counted. We implemented a live prototype\nusing common web technologies (Next.js, React, Firebase) to demonstrate\nend-to-end functionality, accessibility, and cost efficiency. Our contributions\ninclude developing a working demo, a complete election workflow, a hybrid\nblockchain design, and a user-friendly interface that balances privacy,\nsecurity, transparency, and practicality. This research highlights the\nfeasibility of secure, verifiable, and scalable online voting for organizations\nranging from small groups to larger institutions.", "AI": {"tldr": "A hybrid blockchain-based voting system that uses private blockchain for encrypted vote storage and public blockchain for tamper-evident hash anchoring, with one-time blind-signed tokens for voter anonymity and verifiable receipts.", "motivation": "To enable secure online voting while ensuring voter confidence in ballot secrecy and protection against vote tampering, addressing the challenges of traditional electronic voting systems.", "method": "Developed a hybrid blockchain model with encrypted votes on private blockchain maintained by election organizers and observers, periodic hash anchoring on public blockchain, one-time blind-signed tokens for anonymity, and verifiable receipts. Implemented prototype using Next.js, React, and Firebase.", "result": "Successfully implemented a working demo with complete election workflow, demonstrating end-to-end functionality, accessibility, and cost efficiency. The system balances privacy, security, transparency, and practicality.", "conclusion": "The research demonstrates the feasibility of secure, verifiable, and scalable online voting systems suitable for organizations ranging from small groups to larger institutions, using hybrid blockchain technology."}}
{"id": "2509.22935", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22935", "abs": "https://arxiv.org/abs/2509.22935", "authors": ["Aleksandr Dremov", "David Grangier", "Angelos Katharopoulos", "Awni Hannun"], "title": "Compute-Optimal Quantization-Aware Training", "comment": null, "summary": "Quantization-aware training (QAT) is a leading technique for improving the\naccuracy of quantized neural networks. Previous work has shown that decomposing\ntraining into a full-precision (FP) phase followed by a QAT phase yields\nsuperior accuracy compared to QAT alone. However, the optimal allocation of\ncompute between the FP and QAT phases remains unclear. We conduct extensive\nexperiments with various compute budgets, QAT bit widths, and model sizes from\n86.0M to 2.2B to investigate how different QAT durations impact final\nperformance. We demonstrate that, contrary to previous findings, the\nloss-optimal ratio of QAT to FP training increases with the total amount of\ncompute. Moreover, the optimal fraction can be accurately predicted for a wide\nrange of model sizes and quantization widths using the\ntokens-per-parameter-byte statistic. From experimental data, we derive a loss\nscaling law that predicts both optimal QAT ratios and final model performance\nacross different QAT/FP compute allocation strategies and QAT bit widths. We\nuse the scaling law to make further predictions, which we verify\nexperimentally, including which QAT bit width is optimal under a given memory\nconstraint and how QAT accuracy with different bit widths compares to\nfull-precision model accuracy. Additionally, we propose a novel cooldown and\nQAT fusion approach that performs learning rate decay jointly with\nquantization-aware training, eliminating redundant full-precision model updates\nand achieving significant compute savings. These findings provide practical\ninsights into efficient QAT planning and enable the training of higher-quality\nquantized models with the same compute budget.", "AI": {"tldr": "This paper investigates optimal compute allocation between full-precision (FP) and quantization-aware training (QAT) phases, finding that the loss-optimal QAT-to-FP ratio increases with total compute. The authors derive a scaling law to predict optimal ratios and propose a cooldown-QAT fusion method for efficiency.", "motivation": "Previous work showed that decomposing training into FP followed by QAT phases improves accuracy over QAT alone, but the optimal compute allocation between these phases remains unclear. The authors aim to determine how different QAT durations impact final performance across various model sizes and quantization widths.", "method": "Conducted extensive experiments with compute budgets from 86.0M to 2.2B parameters, QAT bit widths, and model sizes. Derived a loss scaling law using tokens-per-parameter-byte statistic to predict optimal QAT ratios. Proposed cooldown-QAT fusion that combines learning rate decay with quantization-aware training.", "result": "Contrary to previous findings, the optimal QAT-to-FP ratio increases with total compute. The scaling law accurately predicts optimal ratios across model sizes and quantization widths. The cooldown-QAT fusion eliminates redundant FP updates and achieves significant compute savings.", "conclusion": "The findings provide practical insights for efficient QAT planning and enable training higher-quality quantized models with the same compute budget. The scaling law can predict optimal QAT bit widths under memory constraints and compare QAT accuracy with full-precision models."}}
{"id": "2509.22876", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22876", "abs": "https://arxiv.org/abs/2509.22876", "authors": ["Gabriela Pinto", "Palash Goyal", "Yiwen Song", "Souradip Chakraborty", "Zifeng Wang", "Tomas Pfister", "Hamid Palangi"], "title": "HEART: Emotionally-driven test-time scaling of Language Models", "comment": null, "summary": "Test-time scaling has shown considerable success in improving the performance\nof language models on complex reasoning tasks without requiring fine-tuning.\nHowever, current strategies such as self-reflection primarily focus on logical\nor structural refinement. They do not leverage the guiding potential of\naffective feedback. Inspired by psychological research showing that emotions\ncan modulate cognitive performance, we introduce HEART--a novel framework that\nuses emotionally-driven prompts for iterative self-correction. HEART provides\nfeedback on a model's incorrect response using a curated set of concise,\nemotionally charged phrases based on the six universal emotions categorized by\nDr. Paul Ekman. By systematically varying the emotional tone of the feedback\nacross iterations, our method guides the model to escape flawed reasoning paths\nand explore more promising alternatives. We evaluate our framework on\nchallenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam,\nand SimpleQA. Our results reveal a significant new phenomenon: when guided by\nan oracle verifier, this affective iteration protocol unlocks significantly\ndeeper reasoning, leading to consistent and substantial increases in accuracy\nover state-of-the-art baselines with the same verifier. However, we also\nidentify a critical bottleneck for practical deployment. In a verifier-free\nsetting, it struggles to harness these gains consistently, highlighting as a\nkey challenge for future work. Our findings suggest that the next frontier in\nmachine reasoning may lie not just in refining logic, but also in understanding\nand leveraging the `HEART' of the models.", "AI": {"tldr": "HEART is a novel framework that uses emotionally-driven prompts for iterative self-correction in language models, showing significant performance improvements on reasoning tasks when guided by an oracle verifier, but struggles in verifier-free settings.", "motivation": "Current test-time scaling strategies focus on logical refinement but ignore the potential of affective feedback. Psychological research shows emotions can modulate cognitive performance, suggesting emotional guidance could improve reasoning.", "method": "HEART provides feedback on incorrect responses using concise, emotionally charged phrases based on Ekman's six universal emotions. It systematically varies emotional tone across iterations to guide models away from flawed reasoning paths.", "result": "When guided by an oracle verifier, HEART unlocks significantly deeper reasoning and achieves substantial accuracy improvements over state-of-the-art baselines on benchmarks like OlympiadBench, Humanity's Last Exam, and SimpleQA.", "conclusion": "The next frontier in machine reasoning may involve not just refining logic but also understanding and leveraging emotional aspects ('HEART') of models, though practical deployment faces challenges in verifier-free settings."}}
{"id": "2509.23109", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23109", "abs": "https://arxiv.org/abs/2509.23109", "authors": ["Junyang Zhang", "Tianyi Zhu", "Thierry Tambe"], "title": "AttAnchor: Guiding Cross-Modal Token Alignment in VLMs with Attention Anchors", "comment": "31 pages, 17 figures", "summary": "A fundamental reason for the dominance of attention over RNNs and LSTMs in\nLLMs is its ability to capture long-range dependencies by modeling direct\ninteractions between all tokens, overcoming the sequential limitations of\nrecurrent architectures. Similarly, a key reason why today's vision language\nmodels (VLMs) hallucinate and underperform pure language models is that they\nrely on direct concatenation of image and text tokens with a modality-blinded\npositional encoding, which conveniently adopts the pretrained LLM backbone but\nforces unnecessary long-distance attention between semantically related tokens\nacross modalities. This underscores the urgent need for mechanisms that\nefficiently enhance token locality and cross-modal alignment. In response, we\npropose Attention Anchor, a parameter-free framework that efficiently groups\nsemantically similar tokens across modalities, improving cross-modal locality.\nBy inserting text tokens near relevant visual patches, we create semantic\nsignposts that reveal true content-based cross-modal attention scores, guiding\nthe model to focus on the correct image regions for tasks such as VQA, MMBench\nand POPE. This improves answer accuracy and reduces hallucinations without\ndisrupting the prompt's semantic flow. AttAnchor achieves improvements across\n13 out of 15 different metrics and benchmarks, including up to 32% gains on\nreasoning tasks and up to 15% improvements on hallucination benchmarks.\nAttAnchor enables TinyLLaVA 1B to outperform much larger models like LLaVA 7B\nand QwenVL 3B on POPE with only 0.1% inference time overhead. To the best of\nour knowledge, this work is among the first to investigate mixed-modal token\ngrouping, where text and image tokens are clustered jointly into shared groups\nrather than being grouped within a single modality or merely aligned post-hoc\nwith additional alignment losses.", "AI": {"tldr": "AttAnchor is a parameter-free framework that improves cross-modal alignment in vision-language models by grouping semantically similar tokens across modalities, reducing hallucinations and improving performance on various benchmarks with minimal inference overhead.", "motivation": "Current VLMs suffer from hallucinations and underperformance due to direct concatenation of image and text tokens with modality-blinded positional encoding, which creates unnecessary long-distance attention between semantically related cross-modal tokens.", "method": "Proposes Attention Anchor framework that inserts text tokens near relevant visual patches to create semantic signposts, enabling true content-based cross-modal attention scores and efficient grouping of semantically similar tokens across modalities.", "result": "Achieves improvements across 13 out of 15 metrics/benchmarks, including up to 32% gains on reasoning tasks and 15% improvements on hallucination benchmarks. Enables TinyLLaVA 1B to outperform larger models like LLaVA 7B and QwenVL 3B on POPE with only 0.1% inference time overhead.", "conclusion": "AttAnchor is among the first works to investigate mixed-modal token grouping, providing an efficient solution for cross-modal locality enhancement that reduces hallucinations and improves VLM performance without disrupting semantic flow."}}
{"id": "2509.22986", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22986", "abs": "https://arxiv.org/abs/2509.22986", "authors": ["Jingyao Zhang", "Elaheh Sadredini"], "title": "CryptoSRAM: Enabling High-Throughput Cryptography on MCUs via In-SRAM Computing", "comment": "To appear in 2025 IEEE Cross-disciplinary Conference on\n  Memory-Centric Computing (CCMCC)", "summary": "Secure communication is a critical requirement for Internet of Things (IoT)\ndevices, which are often based on Microcontroller Units (MCUs). Current\ncryptographic solutions, which rely on software libraries or dedicated hardware\naccelerators, are fundamentally limited by the performance and energy costs of\ndata movement between memory and processing units. This paper introduces\nCryptoSRAM, an in-SRAM computing architecture that performs cryptographic\noperations directly within the MCU's standard SRAM array. By repurposing the\nmemory array into a massively parallel processing fabric, CryptoSRAM eliminates\nthe data movement bottleneck. This approach is well-suited to MCUs, which\nutilize physical addressing and Direct Memory Access (DMA) to manage SRAM,\nallowing for seamless integration with minimal hardware overhead. Our analysis\nshows that for common cryptographic kernels, CryptoSRAM achieves throughput\nimprovements of up to 74$\\times$ and 67$\\times$ for AES and SHA3, respectively,\ncompared to a software implementation. Furthermore, our solution delivers up to\n6$\\times$ higher throughput than existing hardware accelerators for AES.\nCryptoSRAM demonstrates a viable and efficient architecture for secure\ncommunication in next-generation IoT systems.", "AI": {"tldr": "CryptoSRAM is an in-SRAM computing architecture that performs cryptographic operations directly within MCU's SRAM array, eliminating data movement bottlenecks and achieving significant performance improvements for IoT security.", "motivation": "Current cryptographic solutions for IoT devices face performance and energy limitations due to data movement between memory and processing units, creating a need for more efficient approaches.", "method": "The paper introduces CryptoSRAM, which repurposes the MCU's standard SRAM array into a massively parallel processing fabric that performs cryptographic operations directly within memory, leveraging physical addressing and DMA for seamless integration.", "result": "CryptoSRAM achieves throughput improvements of up to 74\u00d7 for AES and 67\u00d7 for SHA3 compared to software implementations, and delivers up to 6\u00d7 higher throughput than existing hardware accelerators for AES.", "conclusion": "CryptoSRAM demonstrates a viable and efficient architecture for secure communication in next-generation IoT systems by eliminating data movement bottlenecks through in-SRAM computing."}}
{"id": "2509.22938", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22938", "abs": "https://arxiv.org/abs/2509.22938", "authors": ["Yanqing Lu", "Letao Wang", "Jinbo Liu"], "title": "Understanding SOAP from the Perspective of Gradient Whitening", "comment": null, "summary": "Shampoo with Adam in the Preconditioner's eigenbasis (SOAP) has recently\nemerged as a promising optimization algorithm for neural network training,\nachieving superior training efficiency over both Adam and Shampoo in language\nmodeling tasks. In this work, we analyze Adam, Shampoo, and SOAP from the\nperspective of gradient whitening, interpreting their preconditioners as\napproximations to the whitening matrix, which captures second-order curvature\ninformation. We further establish a theoretical equivalence between idealized\nversions of SOAP and Shampoo under the Kronecker product assumption. To\nempirically evaluate these insights, we reproduce the language modeling\nexperiments using nanoGPT and grayscale image colorization. Our results show\nthat SOAP exhibits similar convergence rate as Shampoo, and no significant\nadvantage over both Adam and Shampoo in the final loss achieved, which aligns\nwith their equivalence in theory.", "AI": {"tldr": "SOAP shows similar convergence to Shampoo and no significant advantage over Adam or Shampoo in final loss, aligning with theoretical equivalence.", "motivation": "To analyze Adam, Shampoo, and SOAP from gradient whitening perspective and establish theoretical relationships between these optimization algorithms.", "method": "Theoretical analysis of preconditioners as whitening matrix approximations, establishing equivalence between idealized SOAP and Shampoo under Kronecker product assumption, with empirical validation using nanoGPT language modeling and grayscale image colorization.", "result": "SOAP exhibits similar convergence rate as Shampoo, with no significant advantage over both Adam and Shampoo in final loss achieved.", "conclusion": "The empirical results align with theoretical equivalence between SOAP and Shampoo, suggesting limited practical advantages of SOAP over existing methods."}}
{"id": "2509.22887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22887", "abs": "https://arxiv.org/abs/2509.22887", "authors": ["EunJeong Hwang", "Yuwei Yin", "Giuseppe Carenini", "Peter West", "Vered Shwartz"], "title": "Infusing Theory of Mind into Socially Intelligent LLM Agents", "comment": null, "summary": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key\naspect of human social intelligence, yet, chatbots and LLM-based social agents\ndo not typically integrate it. In this work, we demonstrate that LLMs that\nexplicitly use ToM get better at dialogue, achieving goals more effectively.\nAfter showing that simply prompting models to generate mental states between\ndialogue turns already provides significant benefit, we further introduce\nToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM\nwith dialogue lookahead to produce mental states that are maximally useful for\nachieving dialogue goals. Experiments on the Sotopia interactive social\nevaluation benchmark demonstrate the effectiveness of our method over a range\nof baselines. Comprehensive analysis shows that ToMA exhibits more strategic,\ngoal-oriented reasoning behaviors, which enable long-horizon adaptation, while\nmaintaining better relationships with their partners. Our results suggest a\nstep forward in integrating ToM for building socially intelligent LLM agents.", "AI": {"tldr": "LLMs with explicit Theory of Mind (ToM) reasoning achieve better dialogue performance and goal achievement. ToMAgent (ToMA) combines ToM with dialogue lookahead to generate maximally useful mental states.", "motivation": "Current chatbots and LLM-based social agents lack Theory of Mind understanding, which is crucial for human social intelligence. Integrating ToM can improve dialogue effectiveness and goal achievement.", "method": "Introduces ToMAgent (ToMA), which pairs Theory of Mind with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Simple prompting to generate mental states between dialogue turns already provides benefits.", "result": "Experiments on Sotopia interactive social evaluation benchmark show ToMA outperforms baselines. It exhibits more strategic, goal-oriented reasoning behaviors, enables long-horizon adaptation, and maintains better partner relationships.", "conclusion": "Explicit integration of Theory of Mind represents a significant step forward in building socially intelligent LLM agents, improving dialogue performance and goal achievement."}}
{"id": "2509.23113", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.23113", "abs": "https://arxiv.org/abs/2509.23113", "authors": ["Xian Yeow Lee", "Lasitha Vidyaratne", "Ahmed Farahat", "Chetan Gupta"], "title": "Exploring LLM-based Frameworks for Fault Diagnosis", "comment": null, "summary": "Large Language Model (LLM)-based systems present new opportunities for\nautonomous health monitoring in sensor-rich industrial environments. This study\nexplores the potential of LLMs to detect and classify faults directly from\nsensor data, while producing inherently explainable outputs through natural\nlanguage reasoning. We systematically evaluate how LLM-system architecture\n(single-LLM vs. multi-LLM), input representations (raw vs. descriptive\nstatistics), and context window size affect diagnostic performance. Our\nfindings show that LLM systems perform most effectively when provided with\nsummarized statistical inputs, and that systems with multiple LLMs using\nspecialized prompts offer improved sensitivity for fault classification\ncompared to single-LLM systems. While LLMs can produce detailed and\nhuman-readable justifications for their decisions, we observe limitations in\ntheir ability to adapt over time in continual learning settings, often\nstruggling to calibrate predictions during repeated fault cycles. These\ninsights point to both the promise and the current boundaries of LLM-based\nsystems as transparent, adaptive diagnostic tools in complex environments.", "AI": {"tldr": "LLM-based systems show promise for autonomous health monitoring in industrial settings, with multi-LLM architectures using statistical inputs performing best for fault detection, though they struggle with continual learning adaptation.", "motivation": "To explore LLMs' potential for autonomous health monitoring with explainable outputs through natural language reasoning in sensor-rich industrial environments.", "method": "Systematically evaluate LLM-system architecture (single-LLM vs. multi-LLM), input representations (raw vs. descriptive statistics), and context window size effects on diagnostic performance.", "result": "LLM systems perform best with summarized statistical inputs; multi-LLM systems with specialized prompts offer improved sensitivity for fault classification compared to single-LLM systems.", "conclusion": "LLMs show promise as transparent diagnostic tools but have limitations in continual learning settings, struggling with prediction calibration during repeated fault cycles."}}
{"id": "2509.23019", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23019", "abs": "https://arxiv.org/abs/2509.23019", "authors": ["Jeongyeon Hwang", "Sangdon Park", "Jungseul Ok"], "title": "LLM Watermark Evasion via Bias Inversion", "comment": null, "summary": "Watermarking for large language models (LLMs) embeds a statistical signal\nduring generation to enable detection of model-produced text. While\nwatermarking has proven effective in benign settings, its robustness under\nadversarial evasion remains contested. To advance a rigorous understanding and\nevaluation of such vulnerabilities, we propose the \\emph{Bias-Inversion\nRewriting Attack} (BIRA), which is theoretically motivated and model-agnostic.\nBIRA weakens the watermark signal by suppressing the logits of likely\nwatermarked tokens during LLM-based rewriting, without any knowledge of the\nunderlying watermarking scheme. Across recent watermarking methods, BIRA\nachieves over 99\\% evasion while preserving the semantic content of the\noriginal text. Beyond demonstrating an attack, our results reveal a systematic\nvulnerability, emphasizing the need for stress testing and robust defenses.", "AI": {"tldr": "The paper introduces BIRA, a model-agnostic attack that weakens watermark signals in LLM-generated text by suppressing likely watermarked tokens during rewriting, achieving over 99% evasion while preserving semantics.", "motivation": "To rigorously understand and evaluate vulnerabilities in LLM watermarking systems under adversarial evasion, as current watermarking effectiveness in benign settings doesn't guarantee robustness against attacks.", "method": "Proposes Bias-Inversion Rewriting Attack (BIRA) - a theoretically motivated, model-agnostic method that suppresses logits of likely watermarked tokens during LLM-based rewriting without knowledge of the underlying watermarking scheme.", "result": "BIRA achieves over 99% evasion across recent watermarking methods while preserving the semantic content of the original text.", "conclusion": "The results reveal a systematic vulnerability in current watermarking methods, emphasizing the need for stress testing and robust defenses against such attacks."}}
{"id": "2509.22944", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22944", "abs": "https://arxiv.org/abs/2509.22944", "authors": ["Lorenz K. M\u00fcller", "Philippe Bich", "Jiawei Zhuang", "Ahmet \u00c7elik", "Luca Benfenati", "Lukas Cavigelli"], "title": "SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights", "comment": null, "summary": "Post-training quantization has emerged as the most widely used strategy for\ndeploying large language models at low precision. Still, current methods show\nperplexity degradation at bit-widths less than or equal to 4, partly because\nrepresenting outliers causes precision issues in parameters that share the same\nscales as these outliers. This problem is especially pronounced for\ncalibration-free, uniform quantization methods. We introduce SINQ to augment\nexisting post-training quantizers with an additional second-axis scale factor\nand a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize\nper-row and per-column variances, thereby minimizing a novel per-matrix proxy\ntarget for quantization: the matrix imbalance. Our method has no interactions\nbetween layers and can be trivially applied to new architectures to quantize\nany linear layers. We evaluate our method on the Qwen3 model family and\nDeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against\nuncalibrated uniform quantization baselines and can be further enhanced by\ncombining it with calibration and non-uniform quantization levels. Code to\nreproduce the results of this work and to easily quantize models using SINQ is\navailable at https://github.com/huawei-csl/SINQ.", "AI": {"tldr": "SINQ introduces a second-axis scale factor and Sinkhorn-Knopp algorithm to normalize per-row/column variances, improving post-training quantization for LLMs at low bit-widths (\u22644 bits) by addressing outlier representation issues.", "motivation": "Current post-training quantization methods show perplexity degradation at \u22644 bit-widths due to precision issues when representing outliers, especially problematic for calibration-free uniform quantization.", "method": "Augments existing quantizers with second-axis scale factor and fast Sinkhorn-Knopp algorithm that finds scales to normalize per-row and per-column variances, minimizing matrix imbalance as quantization proxy target.", "result": "Significantly improves WikiText2 and C4 perplexity against uncalibrated uniform quantization baselines on Qwen3 model family and DeepSeek-V2.5, with further enhancement possible when combined with calibration and non-uniform quantization.", "conclusion": "SINQ provides an effective layer-independent method that can be trivially applied to new architectures for quantizing linear layers, addressing key limitations in current low-precision LLM deployment."}}
{"id": "2509.22906", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22906", "abs": "https://arxiv.org/abs/2509.22906", "authors": ["Henrique Godoy"], "title": "Extract-0: A Specialized Language Model for Document Information Extraction", "comment": null, "summary": "This paper presents Extract-0, a 7-billion parameter language model\nspecifically optimized for document information extraction that achieves\nperformance exceeding models with parameter counts several orders of magnitude\nlarger. Through a novel combination of synthetic data generation, supervised\nfine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via\nGroup Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of\n0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming\nGPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology\nemploys a memory-preserving synthetic data generation pipeline that produces\n280,128 training examples from diverse document sources, followed by\nparameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M\nout of 7.66B parameters). The reinforcement learning phase introduces a novel\nsemantic similarity-based reward function that handles the inherent ambiguity\nin information extraction tasks. This research demonstrates that task-specific\noptimization can yield models that surpass general-purpose systems while\nrequiring substantially fewer computational resource.", "AI": {"tldr": "Extract-0 is a 7B parameter model for document information extraction that outperforms larger models like GPT-4 through synthetic data generation, LoRA fine-tuning, and GRPO reinforcement learning.", "motivation": "To develop a specialized model for document information extraction that achieves superior performance compared to general-purpose large models while using significantly fewer computational resources.", "method": "Combines synthetic data generation (280,128 training examples), supervised fine-tuning with LoRA (modifying only 0.53% of weights), and reinforcement learning with GRPO using a novel semantic similarity-based reward function.", "result": "Achieves mean reward of 0.573 on 1,000 document extraction tasks, outperforming GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459).", "conclusion": "Task-specific optimization can create models that surpass general-purpose systems while requiring substantially fewer computational resources."}}
{"id": "2509.23121", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23121", "abs": "https://arxiv.org/abs/2509.23121", "authors": ["Shuai Li", "Chen Yizhe", "Li Dong", "Liu Sichao", "Lan Dapeng", "Liu Yu", "Zhibo Pang"], "title": "Transferring Vision-Language-Action Models to Industry Applications: Architectures, Performance, and Challenges", "comment": "Accepted to IAI 2025 (International Conference on Industrial\n  Artificial Intelligence), Shenyang, China, Aug 21 - 24, 2025. Preprint\n  (before IEEE copyright transfer)", "summary": "The application of artificial intelligence (AI) in industry is accelerating\nthe shift from traditional automation to intelligent systems with perception\nand cognition. Vision language-action (VLA) models have been a key paradigm in\nAI to unify perception, reasoning, and control. Has the performance of the VLA\nmodels met the industrial requirements? In this paper, from the perspective of\nindustrial deployment, we compare the performance of existing state-of-the-art\nVLA models in industrial scenarios and analyze the limitations of VLA models\nfor real-world industrial deployment from the perspectives of data collection\nand model architecture. The results show that the VLA models retain their\nability to perform simple grasping tasks even in industrial settings after\nfine-tuning. However, there is much room for performance improvement in complex\nindustrial environments, diverse object categories, and high precision placing\ntasks. Our findings provide practical insight into the adaptability of VLA\nmodels for industrial use and highlight the need for task-specific enhancements\nto improve their robustness, generalization, and precision.", "AI": {"tldr": "VLA models show basic grasping capability in industrial settings after fine-tuning but need significant improvements for complex tasks, diverse objects, and precision placing.", "motivation": "To evaluate whether current VLA models meet industrial requirements and analyze their limitations for real-world deployment.", "method": "Compare performance of state-of-the-art VLA models in industrial scenarios, analyzing limitations from data collection and model architecture perspectives.", "result": "VLA models can perform simple grasping tasks in industrial settings after fine-tuning, but struggle with complex environments, diverse object categories, and high-precision placing tasks.", "conclusion": "VLA models have practical adaptability for industrial use but require task-specific enhancements to improve robustness, generalization, and precision."}}
{"id": "2509.23041", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23041", "abs": "https://arxiv.org/abs/2509.23041", "authors": ["Zi Liang", "Qingqing Ye", "Xuan Liu", "Yanyun Wang", "Jianliang Xu", "Haibo Hu"], "title": "Virus Infection Attack on LLMs: Your Poisoning Can Spread \"VIA\" Synthetic Data", "comment": "NeurIPS 2025 Spotlight. Source code:\n  https://github.com/liangzid/VirusInfectionAttack", "summary": "Synthetic data refers to artificial samples generated by models. While it has\nbeen validated to significantly enhance the performance of large language\nmodels (LLMs) during training and has been widely adopted in LLM development,\npotential security risks it may introduce remain uninvestigated. This paper\nsystematically evaluates the resilience of synthetic-data-integrated training\nparadigm for LLMs against mainstream poisoning and backdoor attacks. We reveal\nthat such a paradigm exhibits strong resistance to existing attacks, primarily\nthanks to the different distribution patterns between poisoning data and\nqueries used to generate synthetic samples. To enhance the effectiveness of\nthese attacks and further investigate the security risks introduced by\nsynthetic data, we introduce a novel and universal attack framework, namely,\nVirus Infection Attack (VIA), which enables the propagation of current attacks\nthrough synthetic data even under purely clean queries. Inspired by the\nprinciples of virus design in cybersecurity, VIA conceals the poisoning payload\nwithin a protective \"shell\" and strategically searches for optimal hijacking\npoints in benign samples to maximize the likelihood of generating malicious\ncontent. Extensive experiments on both data poisoning and backdoor attacks show\nthat VIA significantly increases the presence of poisoning content in synthetic\ndata and correspondingly raises the attack success rate (ASR) on downstream\nmodels to levels comparable to those observed in the poisoned upstream models.", "AI": {"tldr": "This paper investigates security risks of synthetic data in LLM training, revealing strong resistance to existing poisoning attacks but proposing a novel Virus Infection Attack (VIA) that successfully propagates malicious content through synthetic data.", "motivation": "Synthetic data is widely used to enhance LLM performance, but its potential security risks remain uninvestigated, particularly regarding poisoning and backdoor attacks.", "method": "Systematically evaluates synthetic-data-integrated training against poisoning attacks, then introduces Virus Infection Attack (VIA) framework that conceals payload in protective shells and searches optimal hijacking points in benign samples.", "result": "VIA significantly increases poisoning content presence in synthetic data and raises attack success rates to levels comparable to poisoned upstream models, demonstrating serious security vulnerabilities.", "conclusion": "Synthetic data integration introduces significant security risks that current defenses cannot adequately address, requiring new security measures for synthetic data usage in LLM development."}}
{"id": "2509.22949", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22949", "abs": "https://arxiv.org/abs/2509.22949", "authors": ["Hamidreza Moazzami", "Asma Jamali", "Nicholas Kevlahan", "Rodrigo A. Vargas-Hern\u00e1ndez"], "title": "Meta-Learning Fourier Neural Operators for Hessian Inversion and Enhanced Variational Data Assimilation", "comment": "6 pages, 2 figures, Machine Learning and the Physical Sciences\n  Workshop, (NeurIPS 2025)", "summary": "Data assimilation (DA) is crucial for enhancing solutions to partial\ndifferential equations (PDEs), such as those in numerical weather prediction,\nby optimizing initial conditions using observational data. Variational DA\nmethods are widely used in oceanic and atmospheric forecasting, but become\ncomputationally expensive, especially when Hessian information is involved. To\naddress this challenge, we propose a meta-learning framework that employs the\nFourier Neural Operator (FNO) to approximate the inverse Hessian operator\nacross a family of DA problems, thereby providing an effective initialization\nfor the conjugate gradient (CG) method. Numerical experiments on a linear\nadvection equation demonstrate that the resulting FNO-CG approach reduces the\naverage relative error by $62\\%$ and the number of iterations by $17\\%$\ncompared to the standard CG. These improvements are most pronounced in\nill-conditioned scenarios, highlighting the robustness and efficiency of FNO-CG\nfor challenging DA problems.", "AI": {"tldr": "Proposes FNO-CG method using Fourier Neural Operator to approximate inverse Hessian for data assimilation, reducing error by 62% and iterations by 17% compared to standard CG.", "motivation": "Variational data assimilation methods are computationally expensive, especially when Hessian information is involved, creating a need for more efficient approaches.", "method": "Meta-learning framework using Fourier Neural Operator (FNO) to approximate inverse Hessian operator across DA problems, providing initialization for conjugate gradient method.", "result": "FNO-CG reduces average relative error by 62% and number of iterations by 17% compared to standard CG, with best improvements in ill-conditioned scenarios.", "conclusion": "FNO-CG demonstrates robustness and efficiency for challenging data assimilation problems, particularly in ill-conditioned cases."}}
{"id": "2509.22926", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22926", "abs": "https://arxiv.org/abs/2509.22926", "authors": ["Kelli Henry", "Steven Xu", "Kaitlin Blotske", "Moriah Cargile", "Erin F. Barreto", "Brian Murray", "Susan Smith", "Seth R. Bauer", "Yanjun Gao", "Tianming Liu", "Andrea Sikora"], "title": "Large language models management of medications: three performance analyses", "comment": null, "summary": "Background: Large language models (LLMs) can be useful in diagnosing medical\nconditions, but few studies have evaluated their consistency in recommending\nappropriate medication regimens. The purpose of this evaluation was to test\nGPT-4o on three medication benchmarking tests including mapping a drug name to\nits correct formulation, identifying drug-drug interactions using both its\ninternal knowledge and using a web search, and preparing a medication order\nsentence after being given the medication name. Methods: Using GTP-4o three\nexperiments were completed. Accuracy was quantified by computing cosine\nsimilarity on TF-IDF vectors, normalized Levenshtein similarity, and\nROUGE-1/ROUGE-L F1 between each response and its reference string or by manual\nevaluation by clinicians. Results: GPT-4o performed poorly on drug-formulation\nmatching, with frequent omissions of available drug formulations (mean 1.23 per\nmedication) and hallucinations of formulations that do not exist (mean 1.14 per\nmedication). Only 49% of tested medications were correctly matched to all\navailable formulations. Accuracy was decreased for medications with more\nformulations (p<0.0001). GPT-4o was also inconsistent at identifying\ndrug-drug-interactions, although it had better performance with the\nsearch-augmented assessment compared to its internal knowledge (54.7% vs.\n69.2%, p=0.013). However, allowing a web-search worsened performance when there\nwas no drug-drug interaction (median % correct 100% vs. 40%, p<0.001). Finally,\nGPT-4o performed moderately with preparing a medication order sentence, with\nonly 65.8% of medication order sentences containing no medication or\nabbreviation errors. Conclusions: Model performance was overall poor for all\ntests. This highlights the need for domain-specific training through\nclinician-annotated datasets and a comprehensive evaluation framework for\nbenchmarking performance.", "AI": {"tldr": "GPT-4o performs poorly on medical medication tasks including drug-formulation matching, drug-drug interaction identification, and medication order preparation, highlighting the need for domain-specific training.", "motivation": "To evaluate GPT-4o's consistency in recommending appropriate medication regimens, as few studies have assessed LLMs' performance on medication benchmarking tests despite their potential utility in medical diagnosis.", "method": "Three experiments using GPT-4o: drug-formulation matching, drug-drug interaction identification (with and without web search), and medication order preparation. Evaluation used cosine similarity on TF-IDF vectors, normalized Levenshtein similarity, ROUGE-1/ROUGE-L F1 scores, and manual clinician evaluation.", "result": "Poor performance across all tests: 49% correct drug-formulation matching with frequent omissions and hallucinations; inconsistent drug-drug interaction identification (54.7% with internal knowledge vs 69.2% with search); only 65.8% of medication orders contained no errors. Performance worsened with more formulations and when using web search for non-interacting drugs.", "conclusion": "GPT-4o's overall poor performance highlights the need for domain-specific training using clinician-annotated datasets and comprehensive evaluation frameworks for benchmarking medical LLM performance."}}
{"id": "2509.23130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23130", "abs": "https://arxiv.org/abs/2509.23130", "authors": ["Qian Cheng", "Ruize Tang", "Emilie Ma", "Finn Hackett", "Peiyang He", "Yiming Su", "Ivan Beschastnikh", "Yu Huang", "Xiaoxing Ma", "Tianyin Xu"], "title": "SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems", "comment": null, "summary": "Formal models are essential to specifying large, complex computer systems and\nverifying their correctness, but are notoriously expensive to write and\nmaintain. Recent advances in generative AI show promise in generating certain\nforms of specifications. However, existing work mostly targets small code, not\ncomplete systems. It is unclear whether AI can deal with realistic system\nartifacts, as this requires abstracting their complex behavioral properties\ninto formal models. We present SysMoBench, a benchmark that evaluates AI's\nability to formally model large, complex systems. We focus on concurrent and\ndistributed systems, which are keystones of today's critical computing\ninfrastructures, encompassing operating systems and cloud infrastructure. We\nuse TLA+, the it de facto specification language for concurrent and distributed\nsystems, though the benchmark can be extended to other specification languages.\nWe address the primary challenge of evaluating AI-generated models by\nautomating metrics like syntactic and runtime correctness, conformance to\nsystem code, and invariant correctness. SysMoBench currently includes nine\ndiverse system artifacts: the Raft implementation of Etcd and Redis, the\nSpinlock and Mutex in Asterinas OS, etc.; more artifacts are being actively\nadded. SysMoBench enables us to understand the capabilities and limitations of\ntoday's LLMs and agents, putting tools in this area on a firm footing and\nopening up promising new research directions.", "AI": {"tldr": "SysMoBench is a benchmark for evaluating AI's ability to generate formal models of large, complex concurrent and distributed systems using TLA+ specifications.", "motivation": "Formal models are expensive to write and maintain for complex systems, and existing AI approaches mostly target small code rather than complete systems. There's a need to assess AI's capability to abstract complex behavioral properties into formal models.", "method": "Created SysMoBench with automated metrics including syntactic correctness, runtime correctness, conformance to system code, and invariant correctness. Currently includes nine diverse system artifacts like Raft implementations in Etcd and Redis, Spinlock and Mutex in Asterinas OS.", "result": "The benchmark enables systematic evaluation of LLMs and agents in formal modeling, providing a foundation for understanding their capabilities and limitations.", "conclusion": "SysMoBench establishes a firm foundation for evaluating AI-generated formal models and opens up promising research directions in AI-assisted system specification."}}
{"id": "2509.23091", "categories": ["cs.CR", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23091", "abs": "https://arxiv.org/abs/2509.23091", "authors": ["Xiangchen Meng", "Yangdi Lyu"], "title": "FedBit: Accelerating Privacy-Preserving Federated Learning via Bit-Interleaved Packing and Cross-Layer Co-Design", "comment": null, "summary": "Federated learning (FL) with fully homomorphic encryption (FHE) effectively\nsafeguards data privacy during model aggregation by encrypting local model\nupdates before transmission, mitigating threats from untrusted servers or\neavesdroppers in transmission. However, the computational burden and ciphertext\nexpansion associated with homomorphic encryption can significantly increase\nresource and communication overhead. To address these challenges, we propose\nFedBit, a hardware/software co-designed framework optimized for the\nBrakerski-Fan-Vercauteren (BFV) scheme. FedBit employs bit-interleaved data\npacking to embed multiple model parameters into a single ciphertext\ncoefficient, thereby minimizing ciphertext expansion and maximizing\ncomputational parallelism. Additionally, we integrate a dedicated FPGA\naccelerator to handle cryptographic operations and an optimized dataflow to\nreduce the memory overhead. Experimental results demonstrate that FedBit\nachieves a speedup of two orders of magnitude in encryption and lowers average\ncommunication overhead by 60.7%, while maintaining high accuracy.", "AI": {"tldr": "FedBit is a hardware/software co-designed framework that optimizes federated learning with fully homomorphic encryption, achieving 100x speedup in encryption and 60.7% lower communication overhead while maintaining accuracy.", "motivation": "Federated learning with fully homomorphic encryption protects data privacy but suffers from high computational burden and ciphertext expansion, leading to significant resource and communication overhead.", "method": "Proposed FedBit framework using bit-interleaved data packing to embed multiple model parameters into single ciphertext coefficients, with dedicated FPGA accelerator for cryptographic operations and optimized dataflow.", "result": "Achieved two orders of magnitude speedup in encryption and 60.7% reduction in average communication overhead while maintaining high accuracy.", "conclusion": "FedBit effectively addresses the computational and communication challenges of FHE in federated learning through hardware/software co-design and optimized data packing techniques."}}
{"id": "2509.22953", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22953", "abs": "https://arxiv.org/abs/2509.22953", "authors": ["Valentyn Melnychuk", "Stefan Feuerriegel"], "title": "GDR-learners: Orthogonal Learning of Generative Models for Potential Outcomes", "comment": null, "summary": "Various deep generative models have been proposed to estimate potential\noutcomes distributions from observational data. However, none of them have the\nfavorable theoretical property of general Neyman-orthogonality and, associated\nwith it, quasi-oracle efficiency and double robustness. In this paper, we\nintroduce a general suite of generative Neyman-orthogonal (doubly-robust)\nlearners that estimate the conditional distributions of potential outcomes. Our\nproposed GDR-learners are flexible and can be instantiated with many\nstate-of-the-art deep generative models. In particular, we develop GDR-learners\nbased on (a) conditional normalizing flows (which we call GDR-CNFs), (b)\nconditional generative adversarial networks (GDR-CGANs), (c) conditional\nvariational autoencoders (GDR-CVAEs), and (d) conditional diffusion models\n(GDR-CDMs). Unlike the existing methods, our GDR-learners possess the\nproperties of quasi-oracle efficiency and rate double robustness, and are thus\nasymptotically optimal. In a series of (semi-)synthetic experiments, we\ndemonstrate that our GDR-learners are very effective and outperform the\nexisting methods in estimating the conditional distributions of potential\noutcomes.", "AI": {"tldr": "The paper introduces GDR-learners, a suite of generative Neyman-orthogonal (doubly-robust) learners that estimate conditional distributions of potential outcomes from observational data, achieving quasi-oracle efficiency and double robustness.", "motivation": "Existing deep generative models for estimating potential outcomes distributions lack Neyman-orthogonality and associated theoretical guarantees like quasi-oracle efficiency and double robustness.", "method": "Proposed GDR-learners framework that can be instantiated with various deep generative models: conditional normalizing flows (GDR-CNFs), conditional GANs (GDR-CGANs), conditional VAEs (GDR-CVAEs), and conditional diffusion models (GDR-CDMs).", "result": "GDR-learners possess quasi-oracle efficiency and rate double robustness, making them asymptotically optimal. In experiments, they outperform existing methods in estimating conditional distributions of potential outcomes.", "conclusion": "The proposed GDR-learners provide a theoretically sound and flexible framework for estimating potential outcomes distributions with superior performance compared to existing methods."}}
{"id": "2509.22940", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22940", "abs": "https://arxiv.org/abs/2509.22940", "authors": ["Melissa Roemmele", "John Joon Young Chung", "Taewook Kim", "Yuqian Sun", "Alex Calderwood", "Max Kreminski"], "title": "LLMs Behind the Scenes: Enabling Narrative Scene Illustration", "comment": "Accepted at EMNLP 2025", "summary": "Generative AI has established the opportunity to readily transform content\nfrom one medium to another. This capability is especially powerful for\nstorytelling, where visual illustrations can illuminate a story originally\nexpressed in text. In this paper, we focus on the task of narrative scene\nillustration, which involves automatically generating an image depicting a\nscene in a story. Motivated by recent progress on text-to-image models, we\nconsider a pipeline that uses LLMs as an interface for prompting text-to-image\nmodels to generate scene illustrations given raw story text. We apply\nvariations of this pipeline to a prominent story corpus in order to synthesize\nillustrations for scenes in these stories. We conduct a human annotation task\nto obtain pairwise quality judgments for these illustrations. The outcome of\nthis process is the SceneIllustrations dataset, which we release as a new\nresource for future work on cross-modal narrative transformation. Through our\nanalysis of this dataset and experiments modeling illustration quality, we\ndemonstrate that LLMs can effectively verbalize scene knowledge implicitly\nevoked by story text. Moreover, this capability is impactful for generating and\nevaluating illustrations.", "AI": {"tldr": "The paper proposes using LLMs to prompt text-to-image models for automatically generating scene illustrations from story text, creating a new dataset called SceneIllustrations.", "motivation": "To leverage generative AI's capability to transform content across modalities, specifically for storytelling where visual illustrations can enhance text-based narratives.", "method": "A pipeline using LLMs as interface to prompt text-to-image models, applied to story corpus with human annotation for quality judgments.", "result": "Created SceneIllustrations dataset and demonstrated LLMs can effectively verbalize scene knowledge from story text for illustration generation.", "conclusion": "LLMs are effective for generating and evaluating narrative scene illustrations, providing a valuable resource for cross-modal narrative transformation."}}
{"id": "2509.23143", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.23143", "abs": "https://arxiv.org/abs/2509.23143", "authors": ["Charles L. Wang"], "title": "MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning", "comment": null, "summary": "This paper presents MathBode, a dynamic diagnostic for mathematical reasoning\nin large language models (LLMs). Instead of one-shot accuracy, MathBode treats\neach parametric problem as a system: we drive a single parameter sinusoidally\nand fit first-harmonic responses of model outputs and exact solutions. This\nyields interpretable, frequency-resolved metrics -- gain (amplitude tracking)\nand phase (lag) -- that form Bode-style fingerprints. Across five closed-form\nfamilies (linear solve, ratio/saturation, compound interest, 2x2 linear\nsystems, similar triangles), the diagnostic surfaces systematic low-pass\nbehavior and growing phase lag that accuracy alone obscures. We compare several\nmodels against a symbolic baseline that calibrates the instrument ($G \\approx\n1$, $\\phi \\approx 0$). Results separate frontier from mid-tier models on\ndynamics, providing a compact, reproducible protocol that complements standard\nbenchmarks with actionable measurements of reasoning fidelity and consistency.\nWe open-source the dataset and code to enable further research and adoption.", "AI": {"tldr": "MathBode is a dynamic diagnostic tool that analyzes mathematical reasoning in LLMs using frequency-domain analysis of parameter-varying problems, revealing systematic low-pass behavior and phase lag that accuracy metrics miss.", "motivation": "Standard one-shot accuracy metrics fail to capture the dynamic reasoning capabilities and systematic errors in LLMs' mathematical problem-solving. There's a need for more nuanced diagnostics that reveal how models handle parameter variations.", "method": "Treat parametric problems as systems: drive a single parameter sinusoidally and fit first-harmonic responses of model outputs vs exact solutions. This yields frequency-resolved metrics (gain and phase) that form Bode-style fingerprints.", "result": "The diagnostic reveals systematic low-pass behavior and growing phase lag across five mathematical problem families. Results separate frontier from mid-tier models on dynamic reasoning capabilities, with symbolic baseline showing near-ideal performance (G\u22481, \u03c6\u22480).", "conclusion": "MathBode provides a compact, reproducible protocol that complements standard benchmarks with actionable measurements of reasoning fidelity and consistency, offering deeper insights into LLMs' mathematical reasoning dynamics."}}
{"id": "2509.23305", "categories": ["cs.CR", "93C95", "I.6.5"], "pdf": "https://arxiv.org/pdf/2509.23305", "abs": "https://arxiv.org/abs/2509.23305", "authors": ["Jaxson Brown", "Duc-Son Pham", "Sie-Teng Soh", "Foad Motalebi", "Sivaraman Eswaran", "Mahathir Almashor"], "title": "ICS-SimLab: A Containerized Approach for Simulating Industrial Control Systems for Cyber Security Research", "comment": "This is the 10-page extended version of a paper accepted to the First\n  International Workshop on Secure Industrial Control Systems and\n  Industrial-IoT, IEEE CNS 2025 (the conference version was 6 pages)", "summary": "Industrial Control Systems (ICSs) are complex interconnected systems used to\nmanage process control within industrial environments, such as chemical\nprocessing plants and water treatment facilities. As the modern industrial\nenvironment moves towards Internet-facing services, ICSs face an increased risk\nof attacks that necessitates ICS-specific Intrusion Detection Systems (IDS).\nThe development of such IDS relies significantly on a simulated testbed as it\nis unrealistic and sometimes hazardous to utilize an operational control\nsystem. Whilst some testbeds have been proposed, they often use a limited\nselection of virtual ICS simulations to test and verify cyber security\nsolutions. There is a lack of investigation done on developing systems that can\nefficiently simulate multiple ICS architectures. Currently, the trend within\nresearch involves developing security solutions on just one ICS simulation,\nwhich can result in bias to its specific architecture. We present ICS-SimLab,\nan end-to-end software suite that utilizes Docker containerization technology\nto create a highly configurable ICS simulation environment. This software\nframework enables researchers to rapidly build and customize different ICS\nenvironments, facilitating the development of security solutions across\ndifferent systems that adhere to the Purdue Enterprise Reference Architecture.\nTo demonstrate its capability, we present three virtual ICS simulations: a\nsolar panel smart grid, a water bottle filling facility, and a system of\nintelligent electronic devices. Furthermore, we run cyber-attacks on these\nsimulations and construct a dataset of recorded malicious and benign network\ntraffic to be used for IDS development.", "AI": {"tldr": "ICS-SimLab is a Docker-based configurable ICS simulation environment that enables rapid development of different ICS architectures for security testing and IDS development.", "motivation": "Current ICS testbeds are limited to single simulations, causing bias in security solutions. There's a need for systems that can efficiently simulate multiple ICS architectures to develop more robust intrusion detection systems.", "method": "Developed ICS-SimLab using Docker containerization technology to create a highly configurable ICS simulation environment that supports different systems adhering to Purdue Enterprise Reference Architecture.", "result": "Successfully created three virtual ICS simulations (solar panel smart grid, water bottle filling facility, intelligent electronic devices) and generated a dataset of malicious and benign network traffic from cyber-attacks on these simulations.", "conclusion": "ICS-SimLab provides an effective framework for developing and testing security solutions across multiple ICS architectures, addressing the bias issue in current single-simulation approaches."}}
{"id": "2509.22957", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22957", "abs": "https://arxiv.org/abs/2509.22957", "authors": ["Luke Guerdan", "Justin Whitehouse", "Kimberly Truong", "Kenneth Holstein", "Zhiwei Steven Wu"], "title": "Doubly-Robust LLM-as-a-Judge: Externally Valid Estimation with Imperfect Personas", "comment": null, "summary": "As Generative AI (GenAI) systems see growing adoption, a key concern involves\nthe external validity of evaluations, or the extent to which they generalize\nfrom lab-based to real-world deployment conditions. Threats to the external\nvalidity of GenAI evaluations arise when the source sample of human raters and\nsystem outputs used to obtain a system quality estimate differs from the target\ndistribution at deployment time. In this work, we propose a doubly-robust\nestimation framework designed to address this evaluation sampling bias. Key to\nour approach is the use of \"persona\" ratings produced by prompting an LLM\nevaluator (i.e., an LLM-as-a-judge) to behave as a human rater with specific\nsociodemographic characteristics. Our doubly-robust framework combines these\ninformative yet imperfect persona ratings with human ratings obtained under\nevaluation sampling bias to produce statistically valid system quality\nestimates. In particular, we show that our approach yields valid system quality\nestimates when either (i) a model trained to predict human ratings using\npersona ratings and source data observed under sampling bias, or (ii) a\nreweighting model that corrects for sampling bias is of sufficient quality. We\nvalidate our framework theoretically and via a novel Persona Simulation\nFramework (PSF) designed to systematically manipulate persona quality and the\ndegree of evaluation sampling bias present in source data. Our work provides a\nprincipled foundation for combining imperfect persona ratings with human\nratings observed under sampling bias to obtain valid system quality estimates.", "AI": {"tldr": "Proposes a doubly-robust estimation framework to address evaluation sampling bias in GenAI systems by combining imperfect LLM persona ratings with biased human ratings to obtain valid system quality estimates.", "motivation": "Address threats to external validity in GenAI evaluations caused by sampling bias between lab-based human raters and real-world deployment conditions.", "method": "Uses doubly-robust estimation combining LLM-generated persona ratings (simulating human raters with specific sociodemographics) with biased human ratings, validated through Persona Simulation Framework.", "result": "The framework produces statistically valid system quality estimates when either the prediction model or reweighting model is sufficiently accurate.", "conclusion": "Provides principled foundation for combining imperfect persona ratings with biased human ratings to obtain valid GenAI system quality estimates."}}
{"id": "2509.22947", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22947", "abs": "https://arxiv.org/abs/2509.22947", "authors": ["Mohammed Sabry", "Anya Belz"], "title": "What Matters More For In-Context Learning under Matched Compute Budgets: Pretraining on Natural Text or Incorporating Targeted Synthetic Examples?", "comment": null, "summary": "Does explicitly exercising the induction circuit during pretraining improve\nin-context learning (ICL), or is natural text sufficient when compute is held\nconstant (iso-FLOPs)? To test whether targeted synthetic data can accelerate\ninduction-head emergence and enhance ICL, we introduce Bi-Induct, a lightweight\ncurriculum that injects forward-copy (Induction), backward-copy (Anti), or a\nbalanced mix into the pretraining stream. We train models from 0.13B to 1B\nparameters under iso-FLOPs, evaluating (i) few-shot ICL benchmarks, (ii)\nhead-level telemetry, and (iii) held-out language modeling perplexity. Our\nfindings challenge the assumption that early induction circuit activation\ndirectly improves ICL. While Bi-Induct accelerates induction-head emergence at\nsmall scales, this does not consistently yield stronger generalization. On\nstandard LM benchmarks, Bi-Induct matches natural-only training; on\nfunction-style ICL probes, the 1B natural-only performs best. Stress tests\n(e.g., label permutation, HITS@1 vs. HITS@3, 1 vs. 10 shots) preserve these\ntrends. Telemetry shows larger natural-only models develop broader, earlier\ninduction heads without explicit induction patterns. Anti-induction data fails\nto elicit meaningful activation. Perplexity penalties from synthetic data\nshrink with scale, suggesting larger models can absorb non-natural patterns\nwith minimal cost. Crucially, ablating the top 2% of induction heads degrades\nICL more than random ablations, especially for natural-only models, indicating\nmore centralized, load-bearing circuits. Bi-Induct variants exhibit more\nredundant induction activity, implying different circuit utilization. Overall,\ninducing activation is not sufficient: ICL gains depend on these circuits\nbecoming functionally necessary. These results underscore mechanism-aware\npretraining diagnostics and data mixtures that foster load-bearing, not merely\npresent, structure.", "AI": {"tldr": "Injecting synthetic induction data during pretraining accelerates induction-head emergence but doesn't consistently improve in-context learning (ICL) performance. Natural text training produces better generalization and more centralized, load-bearing circuits.", "motivation": "To test whether targeted synthetic data can accelerate induction-head emergence and enhance ICL performance compared to natural text training under iso-FLOPs compute constraints.", "method": "Introduced Bi-Induct curriculum that injects forward-copy (Induction), backward-copy (Anti), or balanced mix into pretraining stream. Trained models from 0.13B to 1B parameters under iso-FLOPs, evaluating few-shot ICL benchmarks, head-level telemetry, and language modeling perplexity.", "result": "Bi-Induct accelerates induction-head emergence at small scales but doesn't yield stronger generalization. Natural-only training performs best on function-style ICL probes. Larger natural-only models develop broader, earlier induction heads without explicit induction patterns. Ablating top induction heads degrades ICL more in natural-only models, indicating more centralized circuits.", "conclusion": "Inducing activation is not sufficient for ICL gains - circuits must become functionally necessary. Results emphasize mechanism-aware pretraining diagnostics and data mixtures that foster load-bearing structure rather than merely present structure."}}
{"id": "2509.23144", "categories": ["cs.AI", "cond-mat.stat-mech", "cs.MA", "nlin.AO", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2509.23144", "abs": "https://arxiv.org/abs/2509.23144", "authors": ["Atma Anand"], "title": "Coordination Requires Simplification: Thermodynamic Bounds on Multi-Objective Compromise in Natural and Artificial Intelligence", "comment": "11 pages, 1 figure, 6 pages supplementary material, submitted to\n  Physical Review E", "summary": "Information-processing systems coordinating across multiple agents and\nobjectives face fundamental thermodynamic constraints. We show that solutions\nwith maximum utility to act as coordination focal points have much higher\nselection pressure for being findable across agents rather than accuracy. We\nderive that the information-theoretic minimum description length of\ncoordination protocols to precision $\\varepsilon$ scales as $L(P)\\geq NK\\log_2\nK+N^2d^2\\log (1/\\varepsilon)$ for $N$ agents with $d$ potentially conflicting\nobjectives and internal model complexity $K$. This scaling forces progressive\nsimplification, with coordination dynamics changing the environment itself and\nshifting optimization across hierarchical levels. Moving from established focal\npoints requires re-coordination, creating persistent metastable states and\nhysteresis until significant environmental shifts trigger phase transitions\nthrough spontaneous symmetry breaking. We operationally define coordination\ntemperature to predict critical phenomena and estimate coordination work costs,\nidentifying measurable signatures across systems from neural networks to\nrestaurant bills to bureaucracies. Extending the topological version of Arrow's\ntheorem on the impossibility of consistent preference aggregation, we find it\nrecursively binds whenever preferences are combined. This potentially explains\nthe indefinite cycling in multi-objective gradient descent and alignment faking\nin Large Language Models trained with reinforcement learning with human\nfeedback. We term this framework Thermodynamic Coordination Theory (TCT), which\ndemonstrates that coordination requires radical information loss.", "AI": {"tldr": "Thermodynamic Coordination Theory (TCT) shows that multi-agent coordination fundamentally requires information loss, with focal point solutions prioritizing findability over accuracy. Coordination protocols scale with agent complexity, forcing simplification and creating metastable states that change only through environmental phase transitions.", "motivation": "To understand the fundamental thermodynamic constraints and information-theoretic limits of coordination across multiple agents with potentially conflicting objectives, and explain phenomena like cycling in multi-objective optimization and alignment faking in AI systems.", "method": "Developed information-theoretic framework with minimum description length analysis of coordination protocols, derived scaling laws for coordination complexity, defined coordination temperature to predict critical phenomena, and extended topological Arrow's theorem to recursive preference aggregation.", "result": "Found that coordination protocol length scales as L(P) \u2265 NKlog\u2082K + N\u00b2d\u00b2log(1/\u03b5), forcing progressive simplification. Coordination dynamics create persistent metastable states with hysteresis, requiring environmental phase transitions for change. Extended Arrow's theorem shows recursive binding in preference aggregation.", "conclusion": "Coordination fundamentally requires radical information loss, with focal points prioritizing findability. This explains persistent cycling in optimization and alignment issues in AI systems, providing a unified thermodynamic framework for understanding coordination across diverse systems."}}
{"id": "2509.23418", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23418", "abs": "https://arxiv.org/abs/2509.23418", "authors": ["Ummay Kulsum", "Aafaq Sabir", "Abhinaya S. B.", "Anupam Das"], "title": "Detecting YouTube Scam Videos via Multimodal Signals and Policy Reasoning", "comment": null, "summary": "YouTube has emerged as a dominant platform for both information dissemination\nand entertainment. However, its vast accessibility has also made it a target\nfor scammers, who frequently upload deceptive or malicious content. Prior\nresearch has documented a range of scam types, and detection approaches rely\nprimarily on textual or statistical metadata. Although effective to some\nextent, these signals are easy to evade and potentially overlook other\nmodalities, such as visual cues.\n  In this study, we present the first systematic investigation of multimodal\napproaches for YouTube scam detection. Our dataset consolidates established\nscam categories and augments them with full length video content and policy\ngrounded reasoning annotations. Our experimental evaluation demonstrates that a\ntext-only model using video titles and descriptions (fine-tuned BERT) achieves\nmoderate effectiveness (76.61% F1), with modest improvements when incorporating\naudio transcripts (77.98% F1). In contrast, visual analysis using a fine-tuned\nLLaVA-Video model yields stronger results (79.61% F1). Finally, a multimodal\nframework that integrates titles, descriptions, and video frames achieves the\nhighest performance (80.53% F1). Beyond improving detection accuracy, our\nmultimodal framework produces interpretable reasoning grounded in YouTube\ncontent policies, thereby enhancing transparency and supporting potential\napplications in automated moderation. Moreover, we validate our approach on\nin-the-wild YouTube data by analyzing 6,374 videos, thereby contributing a\nvaluable resource for future research on scam detection.", "AI": {"tldr": "This paper presents the first systematic multimodal approach for YouTube scam detection, showing that combining text, audio, and visual features achieves the best performance (80.53% F1), outperforming text-only methods.", "motivation": "YouTube's accessibility makes it vulnerable to scammers uploading deceptive content. Existing detection methods rely mainly on text and metadata, which are easy to evade and miss visual cues.", "method": "Developed a multimodal framework integrating video titles, descriptions, audio transcripts, and video frames using fine-tuned BERT and LLaVA-Video models on a dataset with full video content and policy-grounded annotations.", "result": "Text-only model achieved 76.61% F1, modest improvement with audio (77.98% F1), visual analysis alone achieved 79.61% F1, and multimodal integration achieved the highest performance at 80.53% F1.", "conclusion": "Multimodal approaches significantly improve YouTube scam detection accuracy and provide interpretable reasoning based on content policies, supporting automated moderation applications. Validated on 6,374 real YouTube videos."}}
{"id": "2509.22963", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22963", "abs": "https://arxiv.org/abs/2509.22963", "authors": ["Haitong Ma", "Ofir Nabati", "Aviv Rosenberg", "Bo Dai", "Oran Lang", "Idan Szpektor", "Craig Boutilier", "Na Li", "Shie Mannor", "Lior Shani", "Guy Tenneholtz"], "title": "Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces", "comment": "22 pages, 10 figures. Haitong Ma and Ofir Nabati contributed equally\n  to this paper", "summary": "Reinforcement learning (RL) struggles to scale to large, combinatorial action\nspaces common in many real-world problems. This paper introduces a novel\nframework for training discrete diffusion models as highly effective policies\nin these complex settings. Our key innovation is an efficient online training\nprocess that ensures stable and effective policy improvement. By leveraging\npolicy mirror descent (PMD) to define an ideal, regularized target policy\ndistribution, we frame the policy update as a distributional matching problem,\ntraining the expressive diffusion model to replicate this stable target. This\ndecoupled approach stabilizes learning and significantly enhances training\nperformance. Our method achieves state-of-the-art results and superior sample\nefficiency across a diverse set of challenging combinatorial benchmarks,\nincluding DNA sequence generation, RL with macro-actions, and multi-agent\nsystems. Experiments demonstrate that our diffusion policies attain superior\nperformance compared to other baselines.", "AI": {"tldr": "This paper introduces a framework using discrete diffusion models as policies for reinforcement learning in large combinatorial action spaces, achieving state-of-the-art performance through stable online training and policy mirror descent.", "motivation": "Reinforcement learning struggles with large combinatorial action spaces common in real-world problems, requiring more scalable and effective policy representations.", "method": "Uses discrete diffusion models as policies with an efficient online training process that leverages policy mirror descent to define regularized target policy distributions, framing policy updates as distributional matching problems.", "result": "Achieves state-of-the-art results and superior sample efficiency across diverse combinatorial benchmarks including DNA sequence generation, RL with macro-actions, and multi-agent systems, outperforming other baselines.", "conclusion": "The proposed diffusion policy framework provides an effective solution for scaling RL to large combinatorial action spaces, demonstrating stable learning and superior performance across challenging domains."}}
{"id": "2509.22973", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22973", "abs": "https://arxiv.org/abs/2509.22973", "authors": ["Jon Gauthier", "Canaan Breiss", "Matthew Leonard", "Edward F. Chang"], "title": "Emergent morpho-phonological representations in self-supervised speech models", "comment": "Accepted at EMNLP 2025", "summary": "Self-supervised speech models can be trained to efficiently recognize spoken\nwords in naturalistic, noisy environments. However, we do not understand the\ntypes of linguistic representations these models use to accomplish this task.\nTo address this question, we study how S3M variants optimized for word\nrecognition represent phonological and morphological phenomena in frequent\nEnglish noun and verb inflections. We find that their representations exhibit a\nglobal linear geometry which can be used to link English nouns and verbs to\ntheir regular inflected forms.\n  This geometric structure does not directly track phonological or\nmorphological units. Instead, it tracks the regular distributional\nrelationships linking many word pairs in the English lexicon -- often, but not\nalways, due to morphological inflection. These findings point to candidate\nrepresentational strategies that may support human spoken word recognition,\nchallenging the presumed necessity of distinct linguistic representations of\nphonology and morphology.", "AI": {"tldr": "Self-supervised speech models for word recognition exhibit linear geometric representations that capture regular distributional relationships between English nouns/verbs and their inflected forms, rather than directly tracking phonological or morphological units.", "motivation": "To understand what types of linguistic representations self-supervised speech models use for word recognition in noisy environments, specifically how they represent phonological and morphological phenomena in English noun and verb inflections.", "method": "Study S3M variants optimized for word recognition by analyzing their representations of frequent English noun and verb inflections, examining the geometric structure of these representations.", "result": "The models' representations show a global linear geometry that can link English nouns and verbs to their regular inflected forms. This structure tracks regular distributional relationships between word pairs in the lexicon, often (but not always) due to morphological inflection.", "conclusion": "These findings challenge the presumed necessity of distinct linguistic representations of phonology and morphology, suggesting alternative representational strategies that may support human spoken word recognition."}}
{"id": "2509.23154", "categories": ["cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23154", "abs": "https://arxiv.org/abs/2509.23154", "authors": ["Jinzhe Pan", "Jingqing Wang", "Yuehui Ouyang", "Wenchi Cheng", "Wei Zhang"], "title": "AI-Enhanced Distributed Channel Access for Collision Avoidance in Future Wi-Fi 8", "comment": "6 pages,6 figures, accepted by Globalcom 2025", "summary": "The exponential growth of wireless devices and stringent reliability\nrequirements of emerging applications demand fundamental improvements in\ndistributed channel access mechanisms for unlicensed bands. Current Wi-Fi\nsystems, which rely on binary exponential backoff (BEB), suffer from suboptimal\ncollision resolution in dense deployments and persistent fairness challenges\ndue to inherent randomness. This paper introduces a multi-agent reinforcement\nlearning framework that integrates artificial intelligence (AI) optimization\nwith legacy device coexistence. We first develop a dynamic backoff selection\nmechanism that adapts to real-time channel conditions through access deferral\nevents while maintaining full compatibility with conventional CSMA/CA\noperations. Second, we introduce a fairness quantification metric aligned with\nenhanced distributed channel access (EDCA) principles to ensure equitable\nmedium access opportunities. Finally, we propose a centralized training\ndecentralized execution (CTDE) architecture incorporating neighborhood activity\npatterns as observational inputs, optimized via constrained multi-agent\nproximal policy optimization (MAPPO) to jointly minimize collisions and\nguarantee fairness. Experimental results demonstrate that our solution\nsignificantly reduces collision probability compared to conventional BEB while\npreserving backward compatibility with commercial Wi-Fi devices. The proposed\nfairness metric effectively eliminates starvation risks in heterogeneous\nscenarios.", "AI": {"tldr": "This paper proposes a multi-agent reinforcement learning framework for improving Wi-Fi channel access by replacing traditional binary exponential backoff with AI-optimized dynamic backoff selection that adapts to real-time conditions while maintaining legacy compatibility.", "motivation": "Current Wi-Fi systems using binary exponential backoff suffer from suboptimal collision resolution in dense deployments and persistent fairness challenges due to inherent randomness, demanding fundamental improvements for emerging applications with stringent reliability requirements.", "method": "Developed a dynamic backoff selection mechanism, introduced fairness quantification metric aligned with EDCA principles, and proposed a centralized training decentralized execution architecture using constrained multi-agent proximal policy optimization to jointly minimize collisions and guarantee fairness.", "result": "Experimental results show significant reduction in collision probability compared to conventional BEB while preserving backward compatibility with commercial Wi-Fi devices, and the proposed fairness metric effectively eliminates starvation risks in heterogeneous scenarios.", "conclusion": "The AI-optimized framework successfully improves distributed channel access mechanisms for unlicensed bands by integrating artificial intelligence optimization with legacy device coexistence, addressing both collision resolution and fairness challenges in dense wireless deployments."}}
{"id": "2509.23427", "categories": ["cs.CR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23427", "abs": "https://arxiv.org/abs/2509.23427", "authors": ["Rowdy Chotkan", "Bulat Nasrulin", "J\u00e9r\u00e9mie Decouchant", "Johan Pouwelse"], "title": "StarveSpam: Mitigating Spam with Local Reputation in Permissionless Blockchains", "comment": "Preprint. Accepted for publication in the proceedings of the 7th\n  Conference on Blockchain Research & Applications for Innovative Networks and\n  Services (BRAINS 2025). The final version will be available on IEEE Xplore", "summary": "Spam poses a growing threat to blockchain networks. Adversaries can easily\ncreate multiple accounts to flood transaction pools, inflating fees and\ndegrading service quality. Existing defenses against spam, such as fee markets\nand staking requirements, primarily rely on economic deterrence, which fails to\ndistinguish between malicious and legitimate users and often exclude low-value\nbut honest activity. To address these shortcomings, we present StarveSpam, a\ndecentralized reputation-based protocol that mitigates spam by operating at the\ntransaction relay layer. StarveSpam combines local behavior tracking, peer\nscoring, and adaptive rate-limiting to suppress abusive actors, without\nrequiring global consensus, protocol changes, or trusted infrastructure. We\nevaluate StarveSpam using real Ethereum data from a major NFT spam event and\nshow that it outperforms existing fee-based and rule-based defenses, allowing\neach node to block over 95% of spam while dropping just 3% of honest traffic,\nand reducing the fraction of the network exposed to spam by 85% compared to\nexisting rule-based methods. StarveSpam offers a scalable and deployable\nalternative to traditional spam defenses, paving the way toward more resilient\nand equitable blockchain infrastructure.", "AI": {"tldr": "StarveSpam is a decentralized reputation-based protocol that mitigates blockchain spam through local behavior tracking, peer scoring, and adaptive rate-limiting at the transaction relay layer, without requiring global consensus or protocol changes.", "motivation": "Existing spam defenses like fee markets and staking requirements rely on economic deterrence but fail to distinguish between malicious and legitimate users, often excluding low-value honest activity. Blockchain networks need better spam protection that doesn't penalize legitimate users.", "method": "StarveSpam operates at the transaction relay layer using three key components: local behavior tracking to monitor transaction patterns, peer scoring to evaluate sender reputation, and adaptive rate-limiting to suppress abusive actors. It works without global consensus, protocol modifications, or trusted infrastructure.", "result": "Evaluation using real Ethereum data from an NFT spam event showed StarveSpam blocks over 95% of spam while dropping only 3% of honest traffic. It reduces the fraction of the network exposed to spam by 85% compared to existing rule-based methods.", "conclusion": "StarveSpam provides a scalable and deployable alternative to traditional spam defenses, enabling more resilient and equitable blockchain infrastructure by effectively distinguishing between malicious and legitimate activity without economic barriers."}}
{"id": "2509.22964", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22964", "abs": "https://arxiv.org/abs/2509.22964", "authors": ["Qinxun Bai", "Yuxuan Han", "Wei Xu", "Zhengyuan Zhou"], "title": "Functional Critic Modeling for Provably Convergent Off-Policy Actor-Critic", "comment": null, "summary": "Off-policy reinforcement learning (RL) with function approximation offers an\neffective way to improve sample efficiency by reusing past experience. Within\nthis setting, the actor-critic (AC) framework has achieved strong empirical\nsuccess. However, both the critic and actor learning is challenging for the\noff-policy AC methods: first of all, in addition to the classic \"deadly triad\"\ninstability of off-policy evaluation, it also suffers from a \"moving target\"\nproblem, where the policy being evaluated changes continually; secondly, actor\nlearning becomes less efficient due to the difficulty of estimating the exact\noff-policy policy gradient. The first challenge essentially reduces the problem\nto repeatedly performing off-policy evaluation for changing policies. For the\nsecond challenge, the off-policy policy gradient theorem requires a complex and\noften impractical algorithm to estimate an additional emphasis critic, which is\ntypically neglected in practice, thereby reducing to the on-policy policy\ngradient as an approximation. In this work, we introduce a novel concept of\nfunctional critic modeling, which leads to a new AC framework that addresses\nboth challenges for actor-critic learning under the deadly triad setting. We\nprovide a theoretical analysis in the linear function setting, establishing the\nprovable convergence of our framework, which, to the best of our knowledge, is\nthe first convergent off-policy target-based AC algorithm. From a practical\nperspective, we further propose a carefully designed neural network\narchitecture for the functional critic modeling and demonstrate its\neffectiveness through preliminary experiments on widely used RL tasks from the\nDeepMind Control Benchmark.", "AI": {"tldr": "This paper introduces a novel functional critic modeling approach that addresses key challenges in off-policy actor-critic reinforcement learning, providing both theoretical convergence guarantees and practical effectiveness.", "motivation": "Off-policy actor-critic methods face two main challenges: (1) the 'moving target' problem where the policy being evaluated changes continually, and (2) inefficient actor learning due to difficulty estimating exact off-policy policy gradients. Existing methods either suffer from instability or reduce to on-policy approximations.", "method": "The authors propose a functional critic modeling framework that addresses both challenges. They provide theoretical analysis in linear function settings and design a neural network architecture for practical implementation.", "result": "The proposed framework achieves provable convergence for off-policy actor-critic learning, which is the first convergent off-policy target-based AC algorithm. Preliminary experiments on DeepMind Control Benchmark tasks demonstrate its effectiveness.", "conclusion": "Functional critic modeling provides a principled solution to the fundamental challenges in off-policy actor-critic learning, offering both theoretical guarantees and practical performance improvements."}}
{"id": "2509.22983", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22983", "abs": "https://arxiv.org/abs/2509.22983", "authors": ["Yue Zhang", "Seiji Maekawa", "Nikita Bhutani"], "title": "Same Content, Different Representations: A Controlled Study for Table QA", "comment": null, "summary": "Table Question Answering (Table QA) in real-world settings must operate over\nboth structured databases and semi-structured tables containing textual fields.\nHowever, existing benchmarks are tied to fixed data formats and have not\nsystematically examined how representation itself affects model performance. We\npresent the first controlled study that isolates the role of table\nrepresentation by holding content constant while varying structure. Using a\nverbalization pipeline, we generate paired structured and semi-structured\ntables, enabling direct comparisons across modeling paradigms. To support\ndetailed analysis, we introduce a diagnostic benchmark with splits along table\nsize, join requirements, query complexity, and schema quality. Our experiments\nreveal consistent trade-offs: SQL-based methods achieve high accuracy on\nstructured inputs but degrade on semi-structured data, LLMs exhibit flexibility\nbut reduced precision, and hybrid approaches strike a balance, particularly\nunder noisy schemas. These effects intensify with larger tables and more\ncomplex queries. Ultimately, no single method excels across all conditions, and\nwe highlight the central role of representation in shaping Table QA\nperformance. Our findings provide actionable insights for model selection and\ndesign, paving the way for more robust hybrid approaches suited for diverse\nreal-world data formats.", "AI": {"tldr": "This paper presents a controlled study on how table representation affects Table QA performance, showing that no single method excels across all conditions and highlighting the importance of representation in model selection.", "motivation": "Existing Table QA benchmarks are tied to fixed data formats and haven't systematically examined how table representation itself affects model performance. Real-world Table QA must handle both structured databases and semi-structured tables with textual fields.", "method": "Used a verbalization pipeline to generate paired structured and semi-structured tables while holding content constant, enabling direct comparisons. Introduced a diagnostic benchmark with splits along table size, join requirements, query complexity, and schema quality.", "result": "SQL-based methods achieve high accuracy on structured inputs but degrade on semi-structured data; LLMs show flexibility but reduced precision; hybrid approaches strike a balance, especially under noisy schemas. Effects intensify with larger tables and more complex queries.", "conclusion": "No single method excels across all conditions, and table representation plays a central role in shaping Table QA performance. Findings provide actionable insights for model selection and design, paving the way for more robust hybrid approaches for diverse real-world data formats."}}
{"id": "2509.23178", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23178", "abs": "https://arxiv.org/abs/2509.23178", "authors": ["Tian Qin", "Yuhan Chen", "Zhiwei Wang", "Zhi-Qin John Xu"], "title": "Limit Analysis for Symbolic Multi-step Reasoning Tasks with Information Propagation Rules Based on Transformers", "comment": null, "summary": "Transformers are able to perform reasoning tasks, however the intrinsic\nmechanism remains widely open. In this paper we propose a set of information\npropagation rules based on Transformers and utilize symbolic reasoning tasks to\ntheoretically analyze the limit reasoning steps. We show that the limit number\nof reasoning steps is between $O(3^{L-1})$ and $O(2^{L-1})$ for a model with\n$L$ attention layers in a single-pass.", "AI": {"tldr": "This paper analyzes the intrinsic reasoning mechanism of Transformers and establishes theoretical bounds on their reasoning capabilities, showing that the limit number of reasoning steps for an L-layer Transformer is between O(3^(L-1)) and O(2^(L-1)) in a single pass.", "motivation": "While Transformers can perform reasoning tasks, their intrinsic reasoning mechanisms remain poorly understood. The authors aim to theoretically analyze the reasoning capabilities of Transformers to better understand their limitations and potential.", "method": "The authors propose a set of information propagation rules based on Transformers and use symbolic reasoning tasks to theoretically analyze the limit reasoning steps that Transformers can perform.", "result": "The analysis shows that for a Transformer model with L attention layers, the limit number of reasoning steps achievable in a single pass is bounded between O(3^(L-1)) and O(2^(L-1)).", "conclusion": "This work provides theoretical bounds on the reasoning capabilities of Transformers, revealing that their reasoning depth grows exponentially with the number of attention layers, with specific upper and lower bounds established."}}
{"id": "2509.23459", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23459", "abs": "https://arxiv.org/abs/2509.23459", "authors": ["Sepideh Abedini", "Shubhankar Mohapatra", "D. B. Emerson", "Masoumeh Shafieinejad", "Jesse C. Cresswell", "Xi He"], "title": "MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction", "comment": "Accepted to the NeurIPS 2025 Workshop on Regulatable Machine Learning\n  (Regulatable ML @ NeurIPS 2025). Code available at\n  https://github.com/sepideh-abedini/MaskSQL", "summary": "Large language models (LLMs) have shown promising performance on tasks that\nrequire reasoning, such as text-to-SQL, code generation, and debugging.\nHowever, regulatory frameworks with strict privacy requirements constrain their\nintegration into sensitive systems. State-of-the-art LLMs are also proprietary,\ncostly, and resource-intensive, making local deployment impractical.\nConsequently, utilizing such LLMs often requires sharing data with third-party\nproviders, raising privacy concerns and risking noncompliance with regulations.\nAlthough fine-tuned small language models (SLMs) can outperform LLMs on certain\ntasks and be deployed locally to mitigate privacy concerns, they underperform\non more complex tasks such as text-to-SQL translation. In this work, we\nintroduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a\nprivacy protection mechanism to mask sensitive information in LLM prompts.\nUnlike redaction, which removes content entirely, or generalization, which\nbroadens tokens, abstraction retains essential information while discarding\nunnecessary details, striking an effective privacy-utility balance for the\ntext-to-SQL task. Moreover, by providing mechanisms to control the\nprivacy-utility tradeoff, MaskSQL facilitates adoption across a broader range\nof use cases. Our experimental results show that MaskSQL outperforms leading\nSLM-based text-to-SQL models and achieves performance approaching\nstate-of-the-art LLM-based models, while preserving privacy.", "AI": {"tldr": "MaskSQL is a privacy-preserving text-to-SQL framework that uses abstraction to mask sensitive information in LLM prompts, achieving performance close to state-of-the-art LLM models while protecting privacy.", "motivation": "Privacy concerns and regulatory constraints prevent using proprietary LLMs for sensitive text-to-SQL tasks, while small language models underperform on complex tasks. Need a solution that balances privacy and utility.", "method": "Uses abstraction as privacy protection mechanism to mask sensitive information in LLM prompts, retaining essential information while discarding unnecessary details. Provides control over privacy-utility tradeoff.", "result": "Outperforms leading SLM-based text-to-SQL models and achieves performance approaching state-of-the-art LLM-based models while preserving privacy.", "conclusion": "MaskSQL effectively addresses the privacy-utility tradeoff in text-to-SQL tasks, enabling adoption in sensitive domains while maintaining competitive performance."}}
{"id": "2509.22969", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22969", "abs": "https://arxiv.org/abs/2509.22969", "authors": ["Samuel V. Singh", "Shirley Coyle", "Mimi Zhang"], "title": "Shape-Informed Clustering of Multi-Dimensional Functional Data via Deep Functional Autoencoders", "comment": null, "summary": "We introduce FAEclust, a novel functional autoencoder framework for cluster\nanalysis of multi-dimensional functional data, data that are random\nrealizations of vector-valued random functions. Our framework features a\nuniversal-approximator encoder that captures complex nonlinear\ninterdependencies among component functions, and a universal-approximator\ndecoder capable of accurately reconstructing both Euclidean and manifold-valued\nfunctional data. Stability and robustness are enhanced through innovative\nregularization strategies applied to functional weights and biases.\nAdditionally, we incorporate a clustering loss into the network's training\nobjective, promoting the learning of latent representations that are conducive\nto effective clustering. A key innovation is our shape-informed clustering\nobjective, ensuring that the clustering results are resistant to phase\nvariations in the functions. We establish the universal approximation property\nof our non-linear decoder and validate the effectiveness of our model through\nextensive experiments.", "AI": {"tldr": "FAEclust is a functional autoencoder framework for clustering multi-dimensional functional data, featuring universal-approximator encoder/decoder, regularization strategies, clustering loss, and phase-invariant shape-informed clustering.", "motivation": "To develop a robust framework for cluster analysis of multi-dimensional functional data that captures complex nonlinear interdependencies and is resistant to phase variations.", "method": "Functional autoencoder with universal-approximator encoder and decoder, innovative regularization for functional weights/biases, clustering loss in training objective, and shape-informed clustering to handle phase variations.", "result": "Established universal approximation property of the non-linear decoder and validated model effectiveness through extensive experiments.", "conclusion": "FAEclust provides an effective framework for clustering multi-dimensional functional data with enhanced stability, robustness, and phase-invariant clustering capabilities."}}
{"id": "2509.22991", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22991", "abs": "https://arxiv.org/abs/2509.22991", "authors": ["Jasin Cekinmez", "Omid Ghahroodi", "Saad Fowad Chandle", "Dhiman Gupta", "Ehsaneddin Asgari"], "title": "ADAM: A Diverse Archive of Mankind for Evaluating and Enhancing LLMs in Biographical Reasoning", "comment": null, "summary": "We introduce ADAM (A Diverse Archive of Mankind), a framework for evaluating\nand improving multimodal large language models (MLLMs) in biographical\nreasoning. To the best of our knowledge, this is the first work to\nsystematically examine LLM capabilities in biography, a critical yet\nunderexplored dimension of factual knowledge. At its core, AdamDB is a\nmultilingual and multimodal dataset covering over 4 million individuals across\ngeography, time, and profession, while AdamBench provides cognitively\nstructured evaluations based on Bloom's taxonomy, spanning six reasoning levels\nin both English and native languages. To address hallucinations, particularly\nfor lesser-known individuals, we propose AdamRAG, a retrieval-augmented\ngeneration system tailored to biographical contexts. Experiments show that\nAdamRAG substantially improves open-source models and modestly benefits\nclosed-source ones, with the largest gains on lower-order reasoning. Popularity\nstrongly mediates accuracy, and multimodal input via face images offers\nsmaller, less consistent improvements than retrieval. ADAM establishes the\nfirst benchmark and framework for cognitively, culturally, and multimodally\ngrounded biographical evaluation, advancing the development of multilingual,\naccurate, and hallucination-resistant MLLMs.", "AI": {"tldr": "ADAM is a framework for evaluating and improving MLLMs in biographical reasoning, featuring AdamDB dataset (4M+ individuals), AdamBench evaluations based on Bloom's taxonomy, and AdamRAG system to reduce hallucinations.", "motivation": "Biographical reasoning is a critical yet underexplored dimension of factual knowledge in LLMs, with current models suffering from hallucinations especially for lesser-known individuals.", "method": "Created multilingual multimodal dataset (AdamDB), cognitive evaluations (AdamBench) across six reasoning levels, and retrieval-augmented generation system (AdamRAG) tailored to biographical contexts.", "result": "AdamRAG substantially improves open-source models and modestly benefits closed-source ones, with largest gains on lower-order reasoning. Popularity strongly mediates accuracy, and multimodal input via face images offers smaller improvements than retrieval.", "conclusion": "ADAM establishes the first benchmark and framework for cognitively, culturally, and multimodally grounded biographical evaluation, advancing development of multilingual, accurate, and hallucination-resistant MLLMs."}}
{"id": "2509.23186", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23186", "abs": "https://arxiv.org/abs/2509.23186", "authors": ["Qimin Zhong", "Hao Liao", "Siwei Wang", "Mingyang Zhou", "Xiaoqun Wu", "Rui Mao", "Wei Chen"], "title": "Understanding and Enhancing the Planning Capability of Language Models via Multi-Token Prediction", "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive performance across\ndiverse tasks but continue to struggle with learning transitive relations, a\ncornerstone for complex planning. To address this issue, we investigate the\nMulti-Token Prediction (MTP) paradigm and its impact to transitive relation\nlearning. We theoretically analyze the MTP paradigm using a Transformer\narchitecture composed of a shared output head and a transfer layer. Our\nanalysis reveals that the transfer layer gradually learns the multi-step\nadjacency information, which in turn enables the backbone model to capture\nunobserved transitive reachability relations beyond those directly present in\nthe training data, albeit with some inevitable noise in adjacency estimation.\nBuilding on this foundation, we propose two strategies to enhance the transfer\nlayer and overall learning quality: Next-Token Injection (NTI) and a\nTransformer-based transfer layer. Our experiments on both synthetic graphs and\nthe Blocksworld planning benchmark validate our theoretical findings and\ndemonstrate that the improvements significantly enhance the model's\npath-planning capability. These findings deepen our understanding of how\nTransformers with MTP learn in complex planning tasks, and provide practical\nstrategies to overcome the transitivity bottleneck, paving the way toward\nstructurally aware and general-purpose planning models.", "AI": {"tldr": "The paper investigates how Multi-Token Prediction (MTP) helps LLMs learn transitive relations for complex planning tasks, revealing that transfer layers capture multi-step adjacency information, and proposes Next-Token Injection and Transformer-based transfer layers to improve path-planning capabilities.", "motivation": "LLMs struggle with learning transitive relations, which are crucial for complex planning tasks. The authors aim to address this bottleneck by exploring the MTP paradigm.", "method": "Theoretical analysis of MTP with Transformer architecture, proposing Next-Token Injection (NTI) and Transformer-based transfer layer strategies to enhance transitive relation learning.", "result": "Experiments on synthetic graphs and Blocksworld planning benchmark validate that the improvements significantly enhance the model's path-planning capability by capturing unobserved transitive reachability relations.", "conclusion": "The findings deepen understanding of how Transformers with MTP learn in complex planning tasks and provide practical strategies to overcome the transitivity bottleneck, enabling structurally aware and general-purpose planning models."}}
{"id": "2509.23519", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23519", "abs": "https://arxiv.org/abs/2509.23519", "authors": ["Zeyu Shen", "Basileal Imana", "Tong Wu", "Chong Xiang", "Prateek Mittal", "Aleksandra Korolova"], "title": "ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search", "comment": "Accepted to NeurIPS 2025", "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models by\ngrounding their outputs in external documents. These systems, however, remain\nvulnerable to attacks on the retrieval corpus, such as prompt injection.\nRAG-based search systems (e.g., Google's Search AI Overview) present an\ninteresting setting for studying and protecting against such threats, as\ndefense algorithms can benefit from built-in reliability signals -- like\ndocument ranking -- and represent a non-LLM challenge for the adversary due to\ndecades of work to thwart SEO.\n  Motivated by, but not limited to, this scenario, this work introduces\nReliabilityRAG, a framework for adversarial robustness that explicitly\nleverages reliability information of retrieved documents.\n  Our first contribution adopts a graph-theoretic perspective to identify a\n\"consistent majority\" among retrieved documents to filter out malicious ones.\nWe introduce a novel algorithm based on finding a Maximum Independent Set (MIS)\non a document graph where edges encode contradiction. Our MIS variant\nexplicitly prioritizes higher-reliability documents and provides provable\nrobustness guarantees against bounded adversarial corruption under natural\nassumptions. Recognizing the computational cost of exact MIS for large\nretrieval sets, our second contribution is a scalable weighted sample and\naggregate framework. It explicitly utilizes reliability information, preserving\nsome robustness guarantees while efficiently handling many documents.\n  We present empirical results showing ReliabilityRAG provides superior\nrobustness against adversarial attacks compared to prior methods, maintains\nhigh benign accuracy, and excels in long-form generation tasks where prior\nrobustness-focused methods struggled. Our work is a significant step towards\nmore effective, provably robust defenses against retrieved corpus corruption in\nRAG.", "AI": {"tldr": "ReliabilityRAG is a framework that enhances RAG system robustness against adversarial attacks by leveraging document reliability information through graph-based consistency checks and scalable sampling methods.", "motivation": "RAG systems are vulnerable to retrieval corpus attacks like prompt injection, especially in search applications where reliability signals exist but need systematic protection.", "method": "Uses graph theory to find consistent majority via Maximum Independent Set (MIS) prioritizing reliable documents, plus scalable weighted sampling for large retrieval sets.", "result": "Superior robustness against attacks compared to prior methods, maintains high benign accuracy, and excels in long-form generation tasks.", "conclusion": "Provides effective, provably robust defenses against retrieved corpus corruption in RAG systems."}}
{"id": "2509.22979", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22979", "abs": "https://arxiv.org/abs/2509.22979", "authors": ["Zeyi Chen", "Xinzhi Zhang", "Humishka Zope", "Hugo Barbalho", "Konstantina Mellou", "Marco Molinaro", "Janardhan Kulkarni", "Ishai Menache", "Sirui Li"], "title": "OptiMind: Teaching LLMs to Think Like Optimization Experts", "comment": null, "summary": "Mathematical programming -- the task of expressing operations and\ndecision-making problems in precise mathematical language -- is fundamental\nacross domains, yet remains a skill-intensive process requiring operations\nresearch expertise. Recent advances in large language models for complex\nreasoning have spurred interest in automating this task, translating natural\nlanguage into executable optimization models. Current approaches, however,\nachieve limited accuracy, hindered by scarce and noisy training data without\nleveraging domain knowledge. In this work, we systematically integrate\noptimization expertise to improve formulation accuracy for mixed-integer linear\nprogramming, a key family of mathematical programs. Our approach first cleans\ntraining data through class-based error analysis to explicitly prevent common\nmistakes within each optimization class. We then develop multi-turn inference\nstrategies that guide LLMs with class-specific error summaries and solver\nfeedback, enabling iterative refinement. Experiments across multiple base LLMs\ndemonstrate that combining cleaned data with domain-informed prompting and\nfeedback improves formulation accuracy by 14 percentage points on average,\nenabling further progress toward robust LLM-assisted optimization formulation.", "AI": {"tldr": "This paper presents a systematic approach to improve LLM accuracy in translating natural language to mixed-integer linear programming formulations by integrating optimization expertise through data cleaning and multi-turn inference strategies.", "motivation": "Current LLM approaches for mathematical programming formulation achieve limited accuracy due to scarce and noisy training data without leveraging domain knowledge, despite the fundamental importance of this task across domains.", "method": "The approach first cleans training data through class-based error analysis to prevent common mistakes, then develops multi-turn inference strategies that guide LLMs with class-specific error summaries and solver feedback for iterative refinement.", "result": "Experiments across multiple base LLMs show that combining cleaned data with domain-informed prompting and feedback improves formulation accuracy by 14 percentage points on average.", "conclusion": "The integration of optimization expertise enables further progress toward robust LLM-assisted optimization formulation."}}
{"id": "2509.22996", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22996", "abs": "https://arxiv.org/abs/2509.22996", "authors": ["Ji\u0159\u00ed Mili\u010dka", "Anna Marklov\u00e1", "V\u00e1clav Cvr\u010dek"], "title": "AI Brown and AI Koditex: LLM-Generated Corpora Comparable to Traditional Corpora of English and Czech Texts", "comment": null, "summary": "This article presents two corpora of English and Czech texts generated with\nlarge language models (LLMs). The motivation is to create a resource for\ncomparing human-written texts with LLM-generated text linguistically. Emphasis\nwas placed on ensuring these resources are multi-genre and rich in terms of\ntopics, authors, and text types, while maintaining comparability with existing\nhuman-created corpora. These generated corpora replicate reference human\ncorpora: BE21 by Paul Baker, which is a modern version of the original Brown\nCorpus, and Koditex corpus that also follows the Brown Corpus tradition but in\nCzech. The new corpora were generated using models from OpenAI, Anthropic,\nAlphabet, Meta, and DeepSeek, ranging from GPT-3 (davinci-002) to GPT-4.5, and\nare tagged according to the Universal Dependencies standard (i.e., they are\ntokenized, lemmatized, and morphologically and syntactically annotated). The\nsubcorpus size varies according to the model used (the English part contains on\naverage 864k tokens per model, 27M tokens altogether, the Czech partcontains on\naverage 768k tokens per model, 21.5M tokens altogether). The corpora are freely\navailable for download under the CC BY 4.0 license (the annotated data are\nunder CC BY-NC-SA 4.0 licence) and are also accessible through the search\ninterface of the Czech National Corpus.", "AI": {"tldr": "This paper presents two LLM-generated corpora (English and Czech) that replicate human reference corpora, with comprehensive linguistic annotation and multi-model generation.", "motivation": "To create resources for linguistic comparison between human-written and LLM-generated texts, ensuring multi-genre diversity and comparability with existing human corpora.", "method": "Generated corpora using various LLMs (OpenAI, Anthropic, Alphabet, Meta, DeepSeek) from GPT-3 to GPT-4.5, replicating BE21 and Koditex reference corpora, with Universal Dependencies annotation.", "result": "Created English corpus with 27M tokens (avg 864k per model) and Czech corpus with 21.5M tokens (avg 768k per model), freely available under CC licenses and accessible through Czech National Corpus interface.", "conclusion": "Successfully developed comprehensive LLM-generated corpora that provide valuable resources for linguistic analysis and comparison between human and machine-generated text across multiple languages and models."}}
{"id": "2509.23189", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23189", "abs": "https://arxiv.org/abs/2509.23189", "authors": ["Zhenxing Xu", "Yizhe Zhang", "Weidong Bao", "Hao Wang", "Ming Chen", "Haoran Ye", "Wenzheng Jiang", "Hui Yan", "Ji Wang"], "title": "AutoEP: LLMs-Driven Automation of Hyperparameter Evolution for Metaheuristic Algorithms", "comment": null, "summary": "Dynamically configuring algorithm hyperparameters is a fundamental challenge\nin computational intelligence. While learning-based methods offer automation,\nthey suffer from prohibitive sample complexity and poor generalization. We\nintroduce AutoEP, a novel framework that bypasses training entirely by\nleveraging Large Language Models (LLMs) as zero-shot reasoning engines for\nalgorithm control. AutoEP's core innovation lies in a tight synergy between two\ncomponents: (1) an online Exploratory Landscape Analysis (ELA) module that\nprovides real-time, quantitative feedback on the search dynamics, and (2) a\nmulti-LLM reasoning chain that interprets this feedback to generate adaptive\nhyperparameter strategies. This approach grounds high-level reasoning in\nempirical data, mitigating hallucination. Evaluated on three distinct\nmetaheuristics across diverse combinatorial optimization benchmarks, AutoEP\nconsistently outperforms state-of-the-art tuners, including neural evolution\nand other LLM-based methods. Notably, our framework enables open-source models\nlike Qwen3-30B to match the performance of GPT-4, demonstrating a powerful and\naccessible new paradigm for automated hyperparameter design. Our code is\navailable at https://anonymous.4open.science/r/AutoEP-3E11", "AI": {"tldr": "AutoEP is a novel framework that uses Large Language Models (LLMs) as zero-shot reasoning engines for automated hyperparameter configuration, combining online Exploratory Landscape Analysis with multi-LLM reasoning chains to outperform state-of-the-art tuners without training.", "motivation": "Traditional learning-based hyperparameter configuration methods suffer from high sample complexity and poor generalization, necessitating a more efficient and generalizable approach.", "method": "AutoEP integrates an online Exploratory Landscape Analysis (ELA) module for real-time search dynamics feedback with a multi-LLM reasoning chain that interprets this feedback to generate adaptive hyperparameter strategies, eliminating the need for training.", "result": "AutoEP consistently outperforms state-of-the-art tuners across three metaheuristics on diverse combinatorial optimization benchmarks, enabling open-source models like Qwen3-30B to match GPT-4's performance.", "conclusion": "AutoEP demonstrates a powerful and accessible paradigm for automated hyperparameter design by grounding LLM reasoning in empirical data, effectively mitigating hallucination while achieving superior performance."}}
{"id": "2509.23571", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23571", "abs": "https://arxiv.org/abs/2509.23571", "authors": ["Yuqiao Meng", "Luoxi Tang", "Feiyang Yu", "Xi Li", "Guanhua Yan", "Ping Yang", "Zhaohan Xi"], "title": "Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting", "comment": null, "summary": "As cyber threats continue to grow in scale and sophistication, blue team\ndefenders increasingly require advanced tools to proactively detect and\nmitigate risks. Large Language Models (LLMs) offer promising capabilities for\nenhancing threat analysis. However, their effectiveness in real-world blue team\nthreat-hunting scenarios remains insufficiently explored. This paper presents\nCyberTeam, a benchmark designed to guide LLMs in blue teaming practice.\nCyberTeam constructs a standardized workflow in two stages. First, it models\nrealistic threat-hunting workflows by capturing the dependencies among\nanalytical tasks from threat attribution to incident response. Next, each task\nis addressed through a set of operational modules tailored to its specific\nanalytical requirements. This transforms threat hunting into a structured\nsequence of reasoning steps, with each step grounded in a discrete operation\nand ordered according to task-specific dependencies. Guided by this framework,\nLLMs are directed to perform threat-hunting tasks through modularized steps.\nOverall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs\nthrough standardized threat analysis. We evaluate both leading LLMs and\nstate-of-the-art cybersecurity agents, comparing CyberTeam against open-ended\nreasoning strategies. Our results highlight the improvements enabled by\nstandardized design, while also revealing the limitations of open-ended\nreasoning in real-world threat hunting.", "AI": {"tldr": "CyberTeam is a benchmark that structures threat hunting into a standardized workflow with 30 tasks and 9 operational modules to guide LLMs through blue teaming practice, showing improvements over open-ended reasoning approaches.", "motivation": "As cyber threats grow more sophisticated, blue team defenders need better tools. LLMs show promise for threat analysis but their effectiveness in real-world threat-hunting scenarios hasn't been sufficiently explored.", "method": "CyberTeam constructs a two-stage workflow: 1) models realistic threat-hunting workflows with task dependencies from threat attribution to incident response, 2) addresses each task through operational modules tailored to specific analytical requirements, transforming threat hunting into structured reasoning steps.", "result": "Evaluation shows CyberTeam enables improvements over open-ended reasoning strategies for both leading LLMs and state-of-the-art cybersecurity agents, while revealing limitations of open-ended reasoning in real-world threat hunting.", "conclusion": "Standardized workflow design through CyberTeam significantly enhances LLM performance in blue team threat-hunting scenarios compared to unstructured approaches."}}
{"id": "2509.22981", "categories": ["cs.LG", "math.OC", "90C15, 90C40"], "pdf": "https://arxiv.org/pdf/2509.22981", "abs": "https://arxiv.org/abs/2509.22981", "authors": ["David P. Morton", "Oscar Dowson", "Bernardo K. Pagnoncelli"], "title": "MDP modeling for multi-stage stochastic programs", "comment": null, "summary": "We study a class of multi-stage stochastic programs, which incorporate\nmodeling features from Markov decision processes (MDPs). This class includes\nstructured MDPs with continuous state and action spaces. We extend policy\ngraphs to include decision-dependent uncertainty for one-step transition\nprobabilities as well as a limited form of statistical learning. We focus on\nthe expressiveness of our modeling approach, illustrating ideas with a series\nof examples of increasing complexity. As a solution method, we develop new\nvariants of stochastic dual dynamic programming, including approximations to\nhandle non-convexities.", "AI": {"tldr": "This paper extends policy graphs to handle decision-dependent uncertainty in transition probabilities and incorporates statistical learning in multi-stage stochastic programs with MDP features.", "motivation": "To address the limitations of traditional MDPs in handling continuous state/action spaces and decision-dependent uncertainty in transition probabilities.", "method": "Develops new variants of stochastic dual dynamic programming (SDDP) with approximations to handle non-convexities in the extended policy graphs framework.", "result": "The approach demonstrates increased expressiveness through examples of increasing complexity, showing capability to model structured MDPs with continuous spaces.", "conclusion": "The extended policy graphs framework with SDDP variants provides a powerful modeling approach for complex multi-stage stochastic programs with MDP features and decision-dependent uncertainty."}}
{"id": "2509.23040", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23040", "abs": "https://arxiv.org/abs/2509.23040", "authors": ["Yaorui Shi", "Yuxin Chen", "Siyuan Wang", "Sihang Li", "Hengxing Cai", "Qi Gu", "Xiang Wang", "An Zhang"], "title": "Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents", "comment": null, "summary": "Large language models face challenges in long-context question answering,\nwhere key evidence of a query may be dispersed across millions of tokens.\nExisting works equip large language models with a memory corpus that is\ndynamically updated during a single-pass document scan, also known as the\n\"memorize while reading\" methods. While this approach scales efficiently, it\nsuffers from irreversible forward-only processing, information loss through\noverwriting, and sparse reinforcement learning signals. To tackle these\nchallenges, we present ReMemR1, a memory-augmented agent with callback-enhanced\nmemory that allows selective retrieval from the entire memory history and\nallows non-linear reasoning and revisiting of early evidence. To further\nstrengthen training, we propose Reinforcement Learning with Multi-Level Rewards\n(RLMLR), which combines final-answer rewards with dense, step-level signals\nthat guide effective memory use. Together, these contributions mitigate\ninformation degradation, improve supervision, and support multi-hop memory\nutilizing. Experiments on long-document QA show significant gains over existing\nmemory-based approaches, which validates ReMemR1 as an effective solution for\nlong-context reasoning agents.", "AI": {"tldr": "ReMemR1 is a memory-augmented agent with callback-enhanced memory that enables selective retrieval from entire memory history and non-linear reasoning, combined with RLMLR training that provides multi-level rewards for effective memory use.", "motivation": "Address challenges in long-context QA where key evidence is dispersed across millions of tokens, overcoming limitations of existing \"memorize while reading\" methods that suffer from irreversible forward-only processing, information loss, and sparse reinforcement learning signals.", "method": "Propose ReMemR1 with callback-enhanced memory allowing selective retrieval from entire memory history and non-linear reasoning, and RLMLR (Reinforcement Learning with Multi-Level Rewards) that combines final-answer rewards with dense step-level signals.", "result": "Experiments on long-document QA show significant gains over existing memory-based approaches, validating ReMemR1 as an effective solution for long-context reasoning agents.", "conclusion": "The proposed approach mitigates information degradation, improves supervision, and supports multi-hop memory utilization, providing an effective solution for long-context reasoning in large language models."}}
{"id": "2509.23234", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23234", "abs": "https://arxiv.org/abs/2509.23234", "authors": ["Runyan Tan", "Shuang Wu", "Phillip Howard"], "title": "$p$-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding", "comment": null, "summary": "Obtaining high-quality outputs from Large Language Models (LLMs) often\ndepends upon the choice of a sampling-based decoding strategy to\nprobabilistically choose the next token at each generation step. While a\nvariety of such sampling methods have been proposed, their performance can be\nsensitive to the selection of hyperparameters which may require different\nsettings depending upon the generation task and temperature configuration. In\nthis work, we introduce $p$-less sampling: an information-theoretic approach to\nsampling which dynamically sets a truncation threshold at each decoding step\nbased on the entire token probability distribution. Unlike existing methods,\n$p$-less sampling has no hyperparameters and consistently produces high-quality\noutputs as temperature increases. We provide theoretical perspectives on\n$p$-less sampling to ground our proposed method and conduct experiments to\nempirically validate its effectiveness across a range of math, logical\nreasoning, and creative writing tasks. Our results demonstrate how $p$-less\nsampling consistently outperforms existing sampling approaches while exhibiting\nmuch less degradation in text quality at higher temperature values. We further\nshow how $p$-less achieves greater inference-time efficiency than alternative\nmethods through lower average token sampling times and shorter generation\nlengths, without sacrificing accuracy. Finally, we provide analyses to\nhighlight the benefits of $p$-less through qualitative examples, case studies,\nand diversity assessments.", "AI": {"tldr": "p-less sampling is a hyperparameter-free decoding strategy that dynamically sets truncation thresholds based on token probability distributions, outperforming existing sampling methods across various tasks while maintaining quality at higher temperatures.", "motivation": "Existing sampling methods for LLMs are sensitive to hyperparameter settings and their performance varies across different generation tasks and temperature configurations, requiring manual tuning.", "method": "p-less sampling uses information theory to dynamically set truncation thresholds at each decoding step based on the entire token probability distribution, eliminating the need for hyperparameters.", "result": "p-less sampling consistently outperforms existing sampling approaches across math, logical reasoning, and creative writing tasks, with less degradation at higher temperatures and improved inference efficiency through lower token sampling times and shorter generation lengths.", "conclusion": "p-less sampling provides a robust, hyperparameter-free alternative to existing decoding strategies that maintains high output quality across temperature variations while improving computational efficiency."}}
{"id": "2509.23573", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23573", "abs": "https://arxiv.org/abs/2509.23573", "authors": ["Yuqiao Meng", "Luoxi Tang", "Feiyang Yu", "Jinyuan Jia", "Guanhua Yan", "Ping Yang", "Zhaohan Xi"], "title": "Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence", "comment": null, "summary": "Large Language Models (LLMs) are intensively used to assist security analysts\nin counteracting the rapid exploitation of cyber threats, wherein LLMs offer\ncyber threat intelligence (CTI) to support vulnerability assessment and\nincident response. While recent work has shown that LLMs can support a wide\nrange of CTI tasks such as threat analysis, vulnerability detection, and\nintrusion defense, significant performance gaps persist in practical\ndeployments. In this paper, we investigate the intrinsic vulnerabilities of\nLLMs in CTI, focusing on challenges that arise from the nature of the threat\nlandscape itself rather than the model architecture. Using large-scale\nevaluations across multiple CTI benchmarks and real-world threat reports, we\nintroduce a novel categorization methodology that integrates stratification,\nautoregressive refinement, and human-in-the-loop supervision to reliably\nanalyze failure instances. Through extensive experiments and human inspections,\nwe reveal three fundamental vulnerabilities: spurious correlations,\ncontradictory knowledge, and constrained generalization, that limit LLMs in\neffectively supporting CTI. Subsequently, we provide actionable insights for\ndesigning more robust LLM-powered CTI systems to facilitate future research.", "AI": {"tldr": "This paper investigates intrinsic vulnerabilities of LLMs in cyber threat intelligence (CTI), revealing three fundamental limitations: spurious correlations, contradictory knowledge, and constrained generalization that hinder practical deployment.", "motivation": "While LLMs are widely used to assist security analysts in cyber threat intelligence tasks like vulnerability assessment and incident response, significant performance gaps persist in practical deployments, necessitating investigation of LLMs' intrinsic vulnerabilities in CTI.", "method": "The authors use large-scale evaluations across multiple CTI benchmarks and real-world threat reports, introducing a novel categorization methodology that integrates stratification, autoregressive refinement, and human-in-the-loop supervision to reliably analyze failure instances.", "result": "Through extensive experiments and human inspections, the study reveals three fundamental vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization that limit LLMs' effectiveness in supporting CTI tasks.", "conclusion": "The paper provides actionable insights for designing more robust LLM-powered CTI systems to facilitate future research in this critical domain."}}
{"id": "2509.22992", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.22992", "abs": "https://arxiv.org/abs/2509.22992", "authors": ["Yuanyuan Yang", "Ruimin Zhang", "Jamie Morgenstern", "Haifeng Xu"], "title": "T-TAMER: Provably Taming Trade-offs in ML Serving", "comment": "Correspondence should be directed to yyangh@cs.washington.edu or\n  haifengxu@uchicago.edu. This manuscript extends our earlier workshop version\n  accepted at NeurIPS SPIGM 2025", "summary": "As machine learning models continue to grow in size and complexity, efficient\nserving faces increasingly broad trade-offs spanning accuracy, latency,\nresource usage, and other objectives. Multi-model serving further complicates\nthese trade-offs; for example, in cascaded models, each early-exit decision\nbalances latency reduction against potential accuracy loss. Despite the\npervasiveness and importance of such trade-offs, current strategies remain\nlargely heuristic and case-specific, limiting both their theoretical guarantees\nand general applicability.\n  We present a general framework, T-Tamer, which formalizes this setting as a\nmulti-stage decision process, where the objective is to determine both when to\nexit and which model to consult. Our main result shows that recall (i.e., the\nability to revisit earlier models) is both necessary and sufficient for\nachieving provable performance guarantees. In particular, we prove that\nstrategies without recall cannot obtain any constant-factor approximation to\nthe optimal trade-off, whereas recall-based strategies provably attain the\noptimal trade-off in polynomial time.\n  We validate our analysis through experiments on synthetic datasets and\nearly-exit workloads for vision and NLP benchmarks. The results show that\nrecall-based strategies consistently yield efficient accuracy-latency\ntrade-offs. We hope this work provides a principled foundation for bridging\nheuristic practice with theoretical guarantees in the design of early-exit and\ncascaded models.", "AI": {"tldr": "T-Tamer is a framework that formalizes multi-model serving as a multi-stage decision process, proving that recall (ability to revisit earlier models) is necessary and sufficient for achieving optimal accuracy-latency trade-offs.", "motivation": "Current strategies for managing trade-offs in multi-model serving (accuracy vs latency, resource usage) are largely heuristic and case-specific, lacking theoretical guarantees and general applicability.", "method": "Formalizes the problem as a multi-stage decision process to determine when to exit and which model to consult, with recall as a key mechanism.", "result": "Proves that strategies without recall cannot achieve constant-factor approximation to optimal trade-offs, while recall-based strategies attain optimal trade-offs in polynomial time. Experimental validation shows efficient accuracy-latency trade-offs.", "conclusion": "Provides a principled foundation bridging heuristic practice with theoretical guarantees for early-exit and cascaded model design, with recall being essential for optimal performance."}}
{"id": "2509.23055", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23055", "abs": "https://arxiv.org/abs/2509.23055", "authors": ["Binwei Yao", "Chao Shang", "Wanyu Du", "Jianfeng He", "Ruixue Lian", "Yi Zhang", "Hang Su", "Sandesh Swamy", "Yanjun Qi"], "title": "Peacemaker or Troublemaker: How Sycophancy Shapes Multi-Agent Debate", "comment": null, "summary": "Large language models (LLMs) often display sycophancy, a tendency toward\nexcessive agreeability. This behavior poses significant challenges for\nmulti-agent debating systems (MADS) that rely on productive disagreement to\nrefine arguments and foster innovative thinking. LLMs' inherent sycophancy can\ncollapse debates into premature consensus, potentially undermining the benefits\nof multi-agent debate. While prior studies focus on user--LLM sycophancy, the\nimpact of inter-agent sycophancy in debate remains poorly understood. To\naddress this gap, we introduce the first operational framework that (1)\nproposes a formal definition of sycophancy specific to MADS settings, (2)\ndevelops new metrics to evaluate the agent sycophancy level and its impact on\ninformation exchange in MADS, and (3) systematically investigates how varying\nlevels of sycophancy across agent roles (debaters and judges) affects outcomes\nin both decentralized and centralized debate frameworks. Our findings reveal\nthat sycophancy is a core failure mode that amplifies disagreement collapse\nbefore reaching a correct conclusion in multi-agent debates, yields lower\naccuracy than single-agent baselines, and arises from distinct debater-driven\nand judge-driven failure modes. Building on these findings, we propose\nactionable design principles for MADS, effectively balancing productive\ndisagreement with cooperation in agent interactions.", "AI": {"tldr": "LLMs' sycophancy causes premature consensus in multi-agent debates, reducing accuracy. This paper defines sycophancy in MADS, develops evaluation metrics, and shows how it leads to debate collapse through debater/judge failure modes.", "motivation": "LLMs' inherent sycophancy poses challenges for multi-agent debating systems by causing premature consensus, undermining the benefits of productive disagreement. Prior research focused on user-LLM sycophancy, leaving inter-agent sycophancy in debates poorly understood.", "method": "Introduced operational framework with: (1) formal definition of sycophancy specific to MADS, (2) new metrics to evaluate agent sycophancy level and information exchange impact, (3) systematic investigation of sycophancy effects across agent roles in decentralized and centralized debate frameworks.", "result": "Sycophancy is a core failure mode that amplifies disagreement collapse before reaching correct conclusions, yields lower accuracy than single-agent baselines, and arises from distinct debater-driven and judge-driven failure modes.", "conclusion": "Proposed actionable design principles for MADS to effectively balance productive disagreement with cooperation in agent interactions, addressing the sycophancy challenge in multi-agent debates."}}
{"id": "2509.23248", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23248", "abs": "https://arxiv.org/abs/2509.23248", "authors": ["Mingyi Luo", "Ruichen Zhang", "Xiangwang Hou", "Jun Du", "Chunxiao Jiang", "Yong Ren", "Dusit Niyato", "Shiwen Mao"], "title": "Agentic AI Reasoning for Mobile Edge General Intelligence: Fundamentals, Approaches, and Directions", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has enabled an\nemergence of agentic artificial intelligence (AI) with powerful reasoning and\nautonomous decision-making capabilities. This integration with edge computing\nhas led to the development of Mobile Edge General Intelligence (MEGI), which\nbrings real-time, privacy-preserving reasoning to the network edge. However,\ndeploying LLM-based agentic AI reasoning in MEGI environments poses significant\nchallenges due to the high computational demands of reasoning and the limited\nresources of edge devices. To address these challenges, we propose a joint\noptimization framework for efficient LLM reasoning deployment in MEGI. First,\nwe review methods that enhance LLM reasoning capabilities, such as\nChain-of-Thought (CoT) prompting, Supervised Fine-Tuning (SFT), and Mixture of\nExperts (MoE). Next, we present a distributed framework that addresses two\ncorrelated aspects: reasoning enhancement through adaptive CoT prompting and\nscalable deployment through distributed MoE architecture. The framework\ndynamically activates expert networks and adjusts reasoning depth based on task\ncomplexity and device capabilities. We further conduct experimental evaluations\nin mobile edge environments. Experimental results demonstrate the framework's\neffectiveness in balancing reasoning quality with resource efficiency,\nvalidating the practical viability of deploying sophisticated LLM reasoning\ncapabilities in resource-constrained MEGI environments.", "AI": {"tldr": "Proposes a joint optimization framework for efficient LLM reasoning deployment in Mobile Edge General Intelligence (MEGI) environments, addressing computational challenges through adaptive CoT prompting and distributed MoE architecture.", "motivation": "To enable real-time, privacy-preserving LLM-based agentic AI reasoning at network edges despite high computational demands and limited edge device resources.", "method": "Combines reasoning enhancement via adaptive Chain-of-Thought prompting with scalable deployment through distributed Mixture of Experts architecture, dynamically adjusting expert activation and reasoning depth based on task complexity and device capabilities.", "result": "Experimental evaluations demonstrate effective balance between reasoning quality and resource efficiency, validating practical viability in resource-constrained MEGI environments.", "conclusion": "The proposed framework successfully enables sophisticated LLM reasoning capabilities in mobile edge computing environments while maintaining resource efficiency."}}
{"id": "2509.23594", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23594", "abs": "https://arxiv.org/abs/2509.23594", "authors": ["Yixu Wang", "Yan Teng", "Yingchun Wang", "Xingjun Ma"], "title": "StolenLoRA: Exploring LoRA Extraction Attacks via Synthetic Data", "comment": "ICCV 2025", "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have transformed\nvision model adaptation, enabling the rapid deployment of customized models.\nHowever, the compactness of LoRA adaptations introduces new safety concerns,\nparticularly their vulnerability to model extraction attacks. This paper\nintroduces a new focus of model extraction attacks named LoRA extraction that\nextracts LoRA-adaptive models based on a public pre-trained model. We then\npropose a novel extraction method called StolenLoRA which trains a substitute\nmodel to extract the functionality of a LoRA-adapted model using synthetic\ndata. StolenLoRA leverages a Large Language Model to craft effective prompts\nfor data generation, and it incorporates a Disagreement-based Semi-supervised\nLearning (DSL) strategy to maximize information gain from limited queries. Our\nexperiments demonstrate the effectiveness of StolenLoRA, achieving up to a\n96.60% attack success rate with only 10k queries, even in cross-backbone\nscenarios where the attacker and victim models utilize different pre-trained\nbackbones. These findings reveal the specific vulnerability of LoRA-adapted\nmodels to this type of extraction and underscore the urgent need for robust\ndefense mechanisms tailored to PEFT methods. We also explore a preliminary\ndefense strategy based on diversified LoRA deployments, highlighting its\npotential to mitigate such attacks.", "AI": {"tldr": "This paper introduces LoRA extraction attacks targeting Parameter-Efficient Fine-Tuned models and proposes StolenLoRA, an effective extraction method using synthetic data and semi-supervised learning that achieves up to 96.60% success rate with minimal queries.", "motivation": "The compactness of LoRA adaptations creates new security vulnerabilities, particularly making them susceptible to model extraction attacks that can steal the functionality of customized models.", "method": "Proposed StolenLoRA method that trains substitute models using synthetic data generated by LLM prompts, combined with Disagreement-based Semi-supervised Learning (DSL) to maximize information from limited queries.", "result": "StolenLoRA achieves up to 96.60% attack success rate with only 10k queries, even in cross-backbone scenarios where attacker and victim models use different pre-trained backbones.", "conclusion": "LoRA-adapted models are specifically vulnerable to extraction attacks, highlighting the urgent need for robust defense mechanisms tailored to PEFT methods, with preliminary defense strategies showing potential."}}
{"id": "2509.22994", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22994", "abs": "https://arxiv.org/abs/2509.22994", "authors": ["Zachary Baker", "Yuxiao Li"], "title": "Analysis of Variational Autoencoders", "comment": "15 pages, 11 figures", "summary": "Sparse Autoencoders (SAEs) have emerged as a promising approach for\ninterpreting neural network representations by learning sparse,\nhuman-interpretable features from dense activations. We investigate whether\nincorporating variational methods into SAE architectures can improve feature\norganization and interpretability. We introduce the variational Sparse\nAutoencoder (vSAE), which replaces deterministic ReLU gating with stochastic\nsampling from learned Gaussian posteriors and incorporates KL divergence\nregularization toward a standard normal prior. Our hypothesis is that this\nprobabilistic sampling creates dispersive pressure, causing features to\norganize more coherently in the latent space while avoiding overlap. We\nevaluate a Topk vSAE against a standard TopK SAE on Pythia-70M transformer\nresidual steam activations using comprehensive benchmarks including SAE Bench,\nindividual feature interpretability analysis, and global latent space\nvisualization through t-SNE. The vSAE underperforms standard SAE across core\nevaluation metrics, though excels at feature independence and ablation metrics.\nThe KL divergence term creates excessive regularization pressure that\nsubstantially reduces the fraction of living features, leading to observed\nperformance degradation. While vSAE features demonstrate improved robustness,\nthey exhibit many more dead features than baseline. Our findings suggest that\nnaive application of variational methods to SAEs does not improve feature\norganization or interpretability.", "AI": {"tldr": "Variational Sparse Autoencoder (vSAE) underperforms standard SAE despite better feature independence, due to excessive KL divergence regularization causing many dead features.", "motivation": "To investigate if incorporating variational methods into SAE architectures can improve feature organization and interpretability by creating dispersive pressure in latent space.", "method": "Introduced vSAE with stochastic Gaussian sampling instead of deterministic ReLU gating, using KL divergence regularization toward standard normal prior. Evaluated against standard TopK SAE on Pythia-70M transformer activations using SAE Bench, feature interpretability analysis, and t-SNE visualization.", "result": "vSAE underperformed standard SAE across core metrics but excelled at feature independence and ablation metrics. KL divergence caused excessive regularization, substantially reducing living features and leading to performance degradation. vSAE features showed improved robustness but many more dead features.", "conclusion": "Naive application of variational methods to SAEs does not improve feature organization or interpretability, as the KL divergence term creates excessive regularization pressure that harms performance."}}
{"id": "2509.23067", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23067", "abs": "https://arxiv.org/abs/2509.23067", "authors": ["Chunyang Jiang", "Yonggang Zhang", "Yiyang Cai", "Chi-Min Chan", "Yulong Liu", "Mingming Chen", "Wei Xue", "Yike Guo"], "title": "Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks", "comment": null, "summary": "The rising cost of acquiring supervised data has driven significant interest\nin self-improvement for large language models (LLMs). Straightforward\nunsupervised signals like majority voting have proven effective in generating\npseudo-labels for verifiable tasks, while their applicability to unverifiable\ntasks (e.g., translation) is limited by the open-ended character of responses.\nAs a result, self-evaluation mechanisms (e.g., self-judging and entropy\nminimization) are predominantly used to derive pseudo-labels. However,\nself-evaluation relying on LLMs typically incurs high computational overhead\nand introduces overconfidence issues due to intrinsic biases. To address these\nchallenges, we propose a novel self-evaluation-free approach for unverifiable\ntasks, designed for lightweight yet effective self-improvement. Inspired by\nmajority voting commonly employed in verifiable tasks, we propose semantic\nvoting as a novel mechanism that relaxes the principle of hard matching (i.e.,\nexact matching) toward soft matching (i.e., semantic similarity). Soft matching\nis achieved by leveraging a lightweight sentence embedding model to quantify\nsemantic similarity, thereby mitigating excessive computational burden and\nintrinsic bias-associated limitations of self-evaluation. Comprehensive\nexperiments demonstrate that our method achieves substantial gains in\ncomputational efficiency and overall better performance than self-evaluation\nmethods across diverse model architectures and tasks.", "AI": {"tldr": "Proposes semantic voting as a self-evaluation-free approach for LLM self-improvement on unverifiable tasks, replacing exact matching with semantic similarity using lightweight embeddings.", "motivation": "Self-evaluation methods for unverifiable tasks suffer from high computational overhead and overconfidence issues due to LLM biases, while majority voting only works for verifiable tasks.", "method": "Semantic voting replaces hard matching (exact match) with soft matching (semantic similarity) using lightweight sentence embeddings to generate pseudo-labels without self-evaluation.", "result": "Achieves substantial gains in computational efficiency and better overall performance than self-evaluation methods across diverse model architectures and tasks.", "conclusion": "Semantic voting provides an effective and efficient alternative to self-evaluation for LLM self-improvement on unverifiable tasks, overcoming computational burden and bias limitations."}}
{"id": "2509.23250", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23250", "abs": "https://arxiv.org/abs/2509.23250", "authors": ["Brandon Ong", "Tej Deep Pala", "Vernon Toh", "William Chandra Tjhi", "Soujanya Poria"], "title": "Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned", "comment": null, "summary": "Process Reward Models (PRMs) provide step-level supervision that improves the\nreliability of reasoning in large language models. While PRMs have been\nextensively studied in text-based domains, their extension to Vision Language\nModels (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on\nMonte Carlo Tree Search (MCTS) for data construction, which can often produce\nnoisy supervision signals and limit generalization across tasks. In this work,\nwe aim to elucidate the design space of VL-PRMs by exploring diverse strategies\nfor dataset construction, training, and test-time scaling. First, we introduce\na hybrid data synthesis framework that combines MCTS with judgments from a\nstrong VLM, producing more accurate step-level labels. Second, we propose\nperception-focused supervision, enabling our PRM to explicitly detect errors at\nthe visual grounding stage of reasoning. Third, we systematically evaluate\nmultiple test-time scaling strategies, showing that our PRMs can reliably guide\nVLMs toward more accurate solutions. Our experiments covering five diverse\nmultimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and\nMathVision) reveal several key insights: (i) VL-PRMs when used as Outcome\nReward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM\nguided process step selection, (ii) smaller VL-PRMs can match or even surpass\nlarger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning\nabilities in stronger VLM backbones, (iv) perception-level supervision leads to\nsignificant gains in test-time scaling, and (v) TTS performance of different\npolicies improve on advanced math reasoning datasets despite not training\nVL-PRMs on such datasets. We hope our work will motivate further research and\nsupport the advancement of VLMs.", "AI": {"tldr": "This paper explores Vision-Language Process Reward Models (VL-PRMs) for improving reasoning in Vision Language Models, introducing hybrid data synthesis, perception-focused supervision, and systematic test-time scaling strategies that outperform existing methods across multiple multimodal benchmarks.", "motivation": "While Process Reward Models (PRMs) have been well-studied in text domains, their extension to Vision Language Models (VLMs) remains limited. Existing VL-PRMs rely on noisy Monte Carlo Tree Search data and lack generalization across tasks, motivating the need for better VL-PRM design.", "method": "Proposes a hybrid data synthesis framework combining MCTS with strong VLM judgments, introduces perception-focused supervision for visual grounding error detection, and systematically evaluates multiple test-time scaling strategies.", "result": "Experiments on five multimodal benchmarks show VL-PRMs can outperform process step selection when used as Outcome Reward Models, smaller VL-PRMs can match larger ones in error detection, perception-level supervision significantly improves test-time scaling, and TTS performance improves even on untrained math reasoning datasets.", "conclusion": "The work provides key insights into VL-PRM design and demonstrates their effectiveness in enhancing VLM reasoning capabilities, motivating further research in this area."}}
{"id": "2509.23621", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23621", "abs": "https://arxiv.org/abs/2509.23621", "authors": ["Sherif Saad", "Kevin Shi", "Mohammed Mamun", "Hythem Elmiligi"], "title": "AutoML in Cybersecurity: An Empirical Study", "comment": null, "summary": "Automated machine learning (AutoML) has emerged as a promising paradigm for\nautomating machine learning (ML) pipeline design, broadening AI adoption. Yet\nits reliability in complex domains such as cybersecurity remains underexplored.\nThis paper systematically evaluates eight open-source AutoML frameworks across\n11 publicly available cybersecurity datasets, spanning intrusion detection,\nmalware classification, phishing, fraud detection, and spam filtering. Results\nshow substantial performance variability across tools and datasets, with no\nsingle solution consistently superior. A paradigm shift is observed: the\nchallenge has moved from selecting individual ML models to identifying the most\nsuitable AutoML framework, complicated by differences in runtime efficiency,\nautomation capabilities, and supported features. AutoML tools frequently favor\ntree-based models, which perform well but risk overfitting and limit\ninterpretability. Key challenges identified include adversarial vulnerability,\nmodel drift, and inadequate feature engineering. We conclude with best\npractices and research directions to strengthen robustness, interpretability,\nand trust in AutoML for high-stakes cybersecurity applications.", "AI": {"tldr": "Systematic evaluation of 8 AutoML frameworks on 11 cybersecurity datasets reveals performance variability, no consistent best tool, and key challenges including adversarial vulnerability and model drift.", "motivation": "To assess the reliability of AutoML in cybersecurity domains where its effectiveness remains underexplored, despite AutoML's promise for automating ML pipeline design.", "method": "Evaluated eight open-source AutoML frameworks across 11 publicly available cybersecurity datasets covering intrusion detection, malware classification, phishing, fraud detection, and spam filtering.", "result": "Substantial performance variability across tools and datasets with no single consistently superior solution; AutoML tools frequently favor tree-based models with overfitting risks; paradigm shift from model selection to framework selection.", "conclusion": "Identified key challenges including adversarial vulnerability, model drift, and inadequate feature engineering; provided best practices and research directions to improve robustness, interpretability, and trust in AutoML for cybersecurity."}}
{"id": "2509.23000", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2509.23000", "abs": "https://arxiv.org/abs/2509.23000", "authors": ["Konstantina Bairaktari", "Huy L. Nguyen"], "title": "Sample-efficient Multiclass Calibration under $\\ell_{p}$ Error", "comment": null, "summary": "Calibrating a multiclass predictor, that outputs a distribution over labels,\nis particularly challenging due to the exponential number of possible\nprediction values. In this work, we propose a new definition of calibration\nerror that interpolates between two established calibration error notions, one\nwith known exponential sample complexity and one with polynomial sample\ncomplexity for calibrating a given predictor. Our algorithm can calibrate any\ngiven predictor for the entire range of interpolation, except for one endpoint,\nusing only a polynomial number of samples. At the other endpoint, we achieve\nnearly optimal dependence on the error parameter, improving upon previous work.\nA key technical contribution is a novel application of adaptive data analysis\nwith high adaptivity but only logarithmic overhead in the sample complexity.", "AI": {"tldr": "Proposes new calibration error definition interpolating between exponential and polynomial sample complexity notions, with efficient calibration algorithm using polynomial samples.", "motivation": "Multiclass predictor calibration is challenging due to exponential number of possible prediction values, requiring new approaches to reduce sample complexity.", "method": "Novel calibration error definition that interpolates between established notions, combined with adaptive data analysis technique with logarithmic overhead.", "result": "Algorithm can calibrate predictors for most interpolation range using polynomial samples, achieving nearly optimal error dependence at one endpoint.", "conclusion": "New calibration framework bridges sample complexity gap and enables efficient multiclass predictor calibration with improved theoretical guarantees."}}
{"id": "2509.23071", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23071", "abs": "https://arxiv.org/abs/2509.23071", "authors": ["Muzhi Li", "Jinhu Qi", "Yihong Wu", "Minghao Zhao", "Liheng Ma", "Yifan Li", "Xinyu Wang", "Yingxue Zhang", "Ho-fung Leung", "Irwin King"], "title": "From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents", "comment": null, "summary": "Retrieval-augmented generation agents development is hindered by the lack of\nprocess-level supervision to effectively guide agentic capabilities like task\ndecomposition, retriever invocation, and stepwise decision-making. While\nreinforcement learning offers a potential solution, it suffers from sparse\nrewards and the limited reasoning capabilities of large language models (LLMs).\nMeanwhile, existing data synthesis methods only produce chain-of-thought\nrationales and fail to model environmental interactions. In this paper, we\npropose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG\nagent development. EviPath comprises: (i) Abductive Subtask Planning, which\ndecomposes the problem into sub-questions and iteratively plans an optimal\nsolution path based on the dependencies between them; (ii) Faithful\nSub-question Answering, which uses supporting evidence to construct a proxy\nenvironment to generate reasoning thoughts and answers for each sub-question;\nand (iii) Conversational Fine-Tuning, which formats the complete\nagent-environment interaction trajectory into a dialogue format suitable for\nSupervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and\ntool-use capabilities directly from synthesized data. Extensive experiments on\nwidely-used question-answering benchmarks show that an 8B parameter model\ntrained with EviPath-synthesized data significantly and consistently\noutperforms state-of-the-art baselines with a double-digit absolute EM gain of\n14.7% in open-domain question answering.", "AI": {"tldr": "EviPath is a new paradigm for synthesizing evidence-anchored reasoning paths to train RAG agents, addressing the lack of process-level supervision in agent development through abductive subtask planning, faithful sub-question answering, and conversational fine-tuning.", "motivation": "Current RAG agent development lacks process-level supervision for guiding agentic capabilities like task decomposition and stepwise decision-making. Reinforcement learning suffers from sparse rewards, and existing data synthesis methods fail to model environmental interactions.", "method": "EviPath consists of three components: (1) Abductive Subtask Planning that decomposes problems into sub-questions and plans optimal solution paths; (2) Faithful Sub-question Answering that uses evidence to generate reasoning thoughts; (3) Conversational Fine-Tuning that formats interaction trajectories into dialogue format for supervised fine-tuning.", "result": "An 8B parameter model trained with EviPath-synthesized data significantly outperforms state-of-the-art baselines with a 14.7% absolute EM gain in open-domain question answering benchmarks.", "conclusion": "EviPath enables LLMs to learn complex reasoning and tool-use capabilities directly from synthesized data, providing an effective solution for RAG agent development without requiring process-level supervision."}}
{"id": "2509.23263", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23263", "abs": "https://arxiv.org/abs/2509.23263", "authors": ["Tao Xiong", "Xavier Hu", "Yurun Chen", "Yuhang Liu", "Changqiao Wu", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Shengyu Zhang"], "title": "GUI-PRA: Process Reward Agent for GUI Tasks", "comment": null, "summary": "Graphical User Interface (GUI) Agents powered by Multimodal Large Language\nModels (MLLMs) show significant potential for automating tasks. However, they\noften struggle with long-horizon tasks, leading to frequent failures. Process\nReward Models (PRMs) are a promising solution, as they can guide these agents\nwith crucial process signals during inference. Nevertheless, their application\nto the GUI domain presents unique challenges. When processing dense artificial\ninputs with long history data, PRMs suffer from a \"lost in the middle\"\nphenomenon, where the overwhelming historical context compromises the\nevaluation of the current step. Furthermore, standard PRMs lacks GUI changing\nawareness, providing static evaluations that are disconnected from the dynamic\nconsequences of actions, a critical mismatch with the inherently dynamic nature\nof GUI tasks. In response to these challenges, we introduce GUI-PRA (Process\nReward Agent for GUI Tasks), a judge agent designed to better provide process\nreward than standard PRM by intelligently processing historical context and\nactively perceiving UI state changes. Specifically, to directly combat the\n``lost in the middle'' phenomenon, we introduce a dynamic memory mechanism\nconsisting of two core components: a Relevance-based Retrieval Module to\nactively fetch pertinent information from long histories and a Progressive\nSummarization Module to dynamically condense growing interaction data, ensuring\nthe model focuses on relevant context. Moreover, to address the lack of UI\nchanging awareness, we introduce an Aadaptive UI Perception mechanism. This\nmechanism enables the agent to reason about UI state changes and dynamically\nselect the most appropriate tool to gather grounded visual evidence, ensuring\nits evaluation is always informed by the current UI context.", "AI": {"tldr": "GUI-PRA is a process reward agent that addresses challenges in GUI task automation by using dynamic memory mechanisms and adaptive UI perception to provide better process rewards than standard PRMs.", "motivation": "Standard Process Reward Models (PRMs) struggle with GUI tasks due to the 'lost in the middle' phenomenon with long history data and lack GUI changing awareness, leading to poor performance in long-horizon GUI automation tasks.", "method": "GUI-PRA introduces two key mechanisms: 1) Dynamic memory with relevance-based retrieval and progressive summarization to handle long histories, and 2) Adaptive UI perception that reasons about UI state changes and selects appropriate tools to gather visual evidence.", "result": "The proposed approach enables better process reward provision by intelligently processing historical context and actively perceiving UI state changes, overcoming limitations of standard PRMs in GUI domains.", "conclusion": "GUI-PRA provides a more effective solution for process reward modeling in GUI tasks by addressing both the 'lost in the middle' problem and the lack of UI changing awareness through its novel dynamic memory and adaptive perception mechanisms."}}
{"id": "2509.23680", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23680", "abs": "https://arxiv.org/abs/2509.23680", "authors": ["Shidong Pan", "Yikai Ge", "Xiaoyu Sun"], "title": "A First Look at Privacy Risks of Android Task-executable Voice Assistant Applications", "comment": "Accepted by APSEC 2025", "summary": "With the development of foundation AI technologies, task-executable voice\nassistants (VAs) have become more popular, enhancing user convenience and\nexpanding device functionality. Android task-executable VAs are applications\nthat are capable of understanding complex tasks and performing corresponding\noperations. Given their prevalence and great autonomy, there is no existing\nwork examine the privacy risks within the voice assistants from the\ntask-execution pattern in a holistic manner. To fill this research gap, this\npaper presents a user-centric comprehensive empirical study on privacy risks in\nAndroid task-executable VA applications. We collect ten mainstream VAs as our\nresearch target and analyze their operational characteristics. We then\ncross-check their privacy declarations across six sources, including privacy\nlabels, policies, and manifest files, and our findings reveal widespread\ninconsistencies. Moreover, we uncover three significant privacy threat models:\n(1) privacy misdisclosure in mega apps, where integrated mini apps such as\nAlexa skills are inadequately represented; (2) privilege escalation via\ninter-application interactions, which exploit Android's communication\nmechanisms to bypass user consent; and (3) abuse of Google system applications,\nenabling apps to evade the declaration of dangerous permissions. Our study\ncontributes actionable recommendations for practitioners and underscores\nbroader relevance of these privacy risks to emerging autonomous AI agents.", "AI": {"tldr": "This paper conducts a comprehensive empirical study on privacy risks in Android task-executable voice assistants, revealing widespread inconsistencies in privacy declarations and uncovering three significant privacy threat models.", "motivation": "With the growing popularity of foundation AI technologies and task-executable voice assistants, there is a research gap in examining privacy risks from task-execution patterns in a holistic manner, despite their prevalence and autonomy.", "method": "The study collects ten mainstream voice assistants, analyzes their operational characteristics, and cross-checks their privacy declarations across six sources including privacy labels, policies, and manifest files.", "result": "Findings reveal widespread inconsistencies in privacy declarations and uncover three significant privacy threat models: privacy misdisclosure in mega apps, privilege escalation via inter-application interactions, and abuse of Google system applications.", "conclusion": "The study contributes actionable recommendations for practitioners and underscores the broader relevance of these privacy risks to emerging autonomous AI agents."}}
{"id": "2509.23003", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.23003", "abs": "https://arxiv.org/abs/2509.23003", "authors": ["Jiayin Liu", "Yulong Yang", "Vineet Bansal", "Christine Allen-Blanchette"], "title": "Physically Plausible Multi-System Trajectory Generation and Symmetry Discovery", "comment": null, "summary": "From metronomes to celestial bodies, mechanics underpins how the world\nevolves in time and space. With consideration of this, a number of recent\nneural network models leverage inductive biases from classical mechanics to\nencourage model interpretability and ensure forecasted states are physical.\nHowever, in general, these models are designed to capture the dynamics of a\nsingle system with fixed physical parameters, from state-space measurements of\na known configuration space. In this paper we introduce Symplectic Phase Space\nGAN (SPS-GAN) which can capture the dynamics of multiple systems, and\ngeneralize to unseen physical parameters from. Moreover, SPS-GAN does not\nrequire prior knowledge of the system configuration space. In fact, SPS-GAN can\ndiscover the configuration space structure of the system from arbitrary\nmeasurement types (e.g., state-space measurements, video frames). To achieve\nphysically plausible generation, we introduce a novel architecture which embeds\na Hamiltonian neural network recurrent module in a conditional GAN backbone. To\ndiscover the structure of the configuration space, we optimize the conditional\ntime-series GAN objective with an additional physically motivated term to\nencourages a sparse representation of the configuration space. We demonstrate\nthe utility of SPS-GAN for trajectory prediction, video generation and symmetry\ndiscovery. Our approach captures multiple systems and achieves performance on\npar with supervised models designed for single systems.", "AI": {"tldr": "SPS-GAN is a novel neural network model that combines Hamiltonian mechanics with GANs to capture dynamics of multiple systems, generalize to unseen parameters, and discover configuration space structure from arbitrary measurements.", "motivation": "Existing neural network models with mechanical inductive biases are limited to single systems with fixed parameters and require known configuration spaces. SPS-GAN addresses these limitations by handling multiple systems and discovering configuration space structure automatically.", "method": "Embedded Hamiltonian neural network recurrent module in conditional GAN backbone, with optimization that includes physically motivated term for sparse configuration space representation. Works with arbitrary measurement types including state-space and video frames.", "result": "SPS-GAN captures multiple systems, achieves performance comparable to supervised single-system models, and demonstrates utility for trajectory prediction, video generation, and symmetry discovery.", "conclusion": "The approach successfully generalizes to unseen physical parameters, discovers configuration space structure without prior knowledge, and maintains physical plausibility while handling multiple systems simultaneously."}}
{"id": "2509.23088", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23088", "abs": "https://arxiv.org/abs/2509.23088", "authors": ["Esteban Garces Arias", "Julian Rodemann", "Christian Heumann"], "title": "The Geometry of Creative Variability: How Credal Sets Expose Calibration Gaps in Language Models", "comment": "Accepted at the 2nd UncertaiNLP Workshop @ EMNLP 2025", "summary": "Understanding uncertainty in large language models remains a fundamental\nchallenge, particularly in creative tasks where multiple valid outputs exist.\nWe present a geometric framework using credal sets - convex hulls of\nprobability distributions - to quantify and decompose uncertainty in neural\ntext generation, calibrated against human creative variation. Analyzing 500\ncreative writing prompts from the WritingPrompts dataset with 10 unique human\ncontinuations each, we evaluate four language models across five decoding\nstrategies, generating 100,000 stories. Our credal set analysis reveals\nsubstantial gaps in capturing human creative variation, with the best\nmodel-human calibration reaching only 0.434 (Gemma-2B with temperature 0.7). We\ndecompose total uncertainty into epistemic and aleatoric components, finding\nthat the choice of decoding strategy contributes 39.4% to 72.0% of total\nepistemic uncertainty. Model scale shows weak correlation with calibration\nquality and no significant difference exists between base and instruction-tuned\nmodels in calibration quality. Our geometric framework provides actionable\ninsights for improving generation systems for human-AI creative alignment. We\nrelease our complete experimental framework.", "AI": {"tldr": "A geometric framework using credal sets to quantify uncertainty in LLMs for creative tasks, revealing gaps in capturing human creative variation and showing decoding strategy contributes significantly to epistemic uncertainty.", "motivation": "Understanding uncertainty in large language models for creative tasks where multiple valid outputs exist, and the need to calibrate against human creative variation.", "method": "Geometric framework using credal sets (convex hulls of probability distributions) to quantify and decompose uncertainty. Analyzed 500 creative writing prompts with 10 human continuations each, evaluated 4 language models across 5 decoding strategies, generating 100,000 stories.", "result": "Substantial gaps in capturing human creative variation, best model-human calibration only 0.434. Decoding strategy contributes 39.4%-72.0% of total epistemic uncertainty. Model scale shows weak correlation with calibration quality, no significant difference between base and instruction-tuned models.", "conclusion": "The geometric framework provides actionable insights for improving generation systems for human-AI creative alignment. Complete experimental framework released."}}
{"id": "2509.23270", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23270", "abs": "https://arxiv.org/abs/2509.23270", "authors": ["Yuxinyue Qian", "Jun Liu"], "title": "Socio-Economic Model of AI Agents", "comment": null, "summary": "Modern socio-economic systems are undergoing deep integration with artificial\nintelligence technologies. This paper constructs a heterogeneous agent-based\nmodeling framework that incorporates both human workers and autonomous AI\nagents, to study the impact of AI collaboration under resource constraints on\naggregate social output. We build five progressively extended models: Model 1\nserves as the baseline of pure human collaboration; Model 2 introduces AI as\ncollaborators; Model 3 incorporates network effects among agents; Model 4\ntreats agents as independent producers; and Model 5 integrates both network\neffects and independent agent production. Through theoretical derivation and\nsimulation analysis, we find that the introduction of AI agents can\nsignificantly increase aggregate social output. When considering network\neffects among agents, this increase exhibits nonlinear growth far exceeding the\nsimple sum of individual contributions. Under the same resource inputs,\ntreating agents as independent producers provides higher long-term growth\npotential; introducing network effects further demonstrates strong\ncharacteristics of increasing returns to scale.", "AI": {"tldr": "The paper develops a heterogeneous agent-based modeling framework to study AI collaboration's impact on social output under resource constraints, finding that AI agents significantly boost output with nonlinear growth from network effects.", "motivation": "To understand how AI integration affects socio-economic systems and aggregate social output when resources are constrained, as modern systems increasingly incorporate AI technologies.", "method": "Built five progressively extended agent-based models: baseline human collaboration, AI collaborators, network effects, independent producers, and combined network effects with independent production.", "result": "AI agents significantly increase aggregate social output; network effects create nonlinear growth exceeding individual contributions; independent producers yield higher long-term growth; network effects show increasing returns to scale.", "conclusion": "AI collaboration under resource constraints can dramatically enhance social output, particularly when leveraging network effects and treating agents as independent producers, demonstrating strong potential for increasing returns to scale."}}
{"id": "2509.23834", "categories": ["cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.23834", "abs": "https://arxiv.org/abs/2509.23834", "authors": ["Haochen Sun", "Xi He"], "title": "GPM: The Gaussian Pancake Mechanism for Planting Undetectable Backdoors in Differential Privacy", "comment": "16 pages, 7 figures. Not published yet. Code and raw experimental\n  logs will be available after publication, or upon email request", "summary": "Differential privacy (DP) has become the gold standard for preserving\nindividual privacy in data analysis. However, an implicit yet fundamental\nassumption underlying these rigorous privacy guarantees is the correct\nimplementation and execution of DP mechanisms. Several incidents of unintended\nprivacy loss have occurred due to numerical issues and inappropriate\nconfigurations of DP software, which have been successfully exploited in\nprivacy attacks. To better understand the seriousness of defective DP software,\nwe ask the following question: is it possible to elevate these passive defects\ninto active privacy attacks while maintaining covertness?\n  To address this question, we present the Gaussian pancake mechanism (GPM), a\nnovel mechanism that is computationally indistinguishable from the widely used\nGaussian mechanism (GM), yet exhibits arbitrarily weaker statistical DP\nguarantees. This unprecedented separation enables a new class of backdoor\nattacks: by indistinguishably passing off as the authentic GM, GPM can covertly\ndegrade statistical privacy. Unlike the unintentional privacy loss caused by\nGM's numerical issues, GPM is an adversarial yet undetectable backdoor attack\nagainst data privacy. We formally prove GPM's covertness, characterize its\nstatistical leakage, and demonstrate a concrete distinguishing attack that can\nachieve near-perfect success rates under suitable parameter choices, both\ntheoretically and empirically.\n  Our results underscore the importance of using transparent, open-source DP\nlibraries and highlight the need for rigorous scrutiny and formal verification\nof DP implementations to prevent subtle, undetectable privacy compromises in\nreal-world systems.", "AI": {"tldr": "The paper presents Gaussian Pancake Mechanism (GPM), a backdoor attack that is computationally indistinguishable from the standard Gaussian Mechanism but provides arbitrarily weaker differential privacy guarantees, enabling covert privacy degradation.", "motivation": "To investigate whether passive defects in DP implementations can be actively exploited for privacy attacks while maintaining covertness, given several incidents of unintended privacy loss due to numerical issues and configuration errors.", "method": "Developed the Gaussian Pancake Mechanism (GPM) that mimics the Gaussian Mechanism but with weaker statistical DP guarantees, formally proving its covertness and characterizing statistical leakage.", "result": "GPM enables near-perfect success rates in distinguishing attacks under suitable parameters, both theoretically and empirically, demonstrating the feasibility of covert privacy degradation attacks.", "conclusion": "Highlights the critical need for transparent, open-source DP libraries and rigorous formal verification of implementations to prevent undetectable privacy compromises in real-world systems."}}
{"id": "2509.23012", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23012", "abs": "https://arxiv.org/abs/2509.23012", "authors": ["Lauren. A Hannah", "Soheil Zibakhsh", "Kumari Nishu", "Arnav Kundu", "Mohammad Samragh Razlighi", "Mehrdad Farajtabar", "Minsik Cho"], "title": "MoE-PHDS: One MoE checkpoint for flexible runtime sparsity", "comment": null, "summary": "Sparse Mixtures of Experts (MoEs) are typically trained to operate at a fixed\nsparsity level, e.g. $k$ in a top-$k$ gating function. This global sparsity\nlevel determines an operating point on the accuracy/latency curve; currently,\nmeeting multiple efficiency targets means training and maintaining multiple\nmodels. This practice complicates serving, increases training and maintenance\ncosts, and limits flexibility in meeting diverse latency, efficiency, and\nenergy requirements. We show that pretrained MoEs are more robust to runtime\nsparsity shifts than commonly assumed, and introduce MoE-PHDS ({\\bf P}ost {\\bf\nH}oc {\\bf D}eclared {\\bf S}parsity), a lightweight SFT method that turns a\nsingle checkpoint into a global sparsity control surface. PHDS mixes training\nacross sparsity levels and anchors with a short curriculum at high sparsity,\nrequiring no architectural changes. The result is predictable accuracy/latency\ntradeoffs from one model: practitioners can ``dial $k$'' at inference time\nwithout swapping checkpoints, changing architecture, or relying on token-level\nheuristics. Experiments on OLMoE-1B-7B-0125, Qwen1.5-MoE-A2.7B, and proprietary\nmodels fit on multiple operating points show that PHDS matches or exceeds\nwell-specified oracle models, improves cross-sparsity agreement by up to 22\\%\nvs. well-specified oracle models, and enables simplified, flexible runtime MoE\ndeployment by making global sparsity a first-class serving primitive.", "AI": {"tldr": "MoE-PHDS enables a single MoE model to dynamically adjust sparsity levels at inference time without retraining, allowing flexible accuracy/latency tradeoffs.", "motivation": "Current MoE models require training multiple models for different sparsity levels, increasing costs and complicating deployment. There's a need for flexible models that can adapt to diverse latency and efficiency requirements.", "method": "MoE-PHDS uses a lightweight SFT method with mixed training across sparsity levels and a curriculum at high sparsity, requiring no architectural changes.", "result": "PHDS matches or exceeds oracle models, improves cross-sparsity agreement by up to 22%, and enables flexible runtime sparsity control from a single checkpoint.", "conclusion": "MoE-PHDS makes global sparsity a first-class serving primitive, simplifying MoE deployment and providing predictable accuracy/latency tradeoffs from one model."}}
{"id": "2509.23094", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23094", "abs": "https://arxiv.org/abs/2509.23094", "authors": ["Yuchu Jiang", "Yue Cai", "Xiangzhong Luo", "Jiale Fu", "Jiarui Wang", "Chonghan Liu", "Xu Yang"], "title": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching", "comment": null, "summary": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache.", "AI": {"tldr": "d\u00b2Cache is a training-free KV cache framework that accelerates diffusion-based LLM inference through adaptive token selection and caching, improving both speed and generation quality.", "motivation": "Diffusion-based LLMs suffer from poor inference efficiency due to bidirectional attention that prevents standard KV cache usage, unlike autoregressive models.", "method": "Two-stage fine-grained selection strategy to identify and update key tokens' KV states while caching others, enabling quasi left-to-right generation.", "result": "Achieves substantial inference speedups and consistent improvements in generation quality on LLaDA and Dream models.", "conclusion": "d\u00b2Cache provides an effective training-free solution for accelerating dLLM inference while enhancing decoding reliability and quality."}}
{"id": "2509.23285", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23285", "abs": "https://arxiv.org/abs/2509.23285", "authors": ["Yifei Chen", "Guanting Dong", "Zhicheng Dou"], "title": "Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference Learning", "comment": null, "summary": "Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to\nimprove their internal reasoning ability by integrating external tools.\nHowever, models employing TIR often display suboptimal behaviors, such as\ninsufficient or excessive tool usage and overthinking after tool calls. The\nchallenge of incentivizing LLMs to perform TIR efficiently and accurately,\nwhile stabilizing the reasoning process, remains an open question. In this\npaper, we start by exploring the impact of tool calls on model reasoning from\nthe perspective of information entropy. Our findings indicate that tool call\nresults lead to a distinct change in the information entropy of subsequent\nreasoning, with the overall entropy of the reasoning chain varying based on the\nnumber of tool calls. Building on these insights, we propose Tool-Light, a\nframework designed to encourage LLMs to perform TIR efficiently and accurately.\nOur framework includes dataset construction and multi-stage fine-tuning. For\ndataset construction, we employ continuous self-evolved sampling using the\nfine-tuned model, integrating both vanilla sampling and entropy-guided\nsampling. Besides, we establish strict criteria for selecting positive-negative\npairs during sampling. The training process involves a two-stage approach,\ncomprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference\nOptimization (DPO). Experimental results on 10 datasets demonstrate the\neffectiveness of Tool-Light, significantly improving the model's efficiency in\nexecuting TIR tasks.", "AI": {"tldr": "Tool-Light is a framework that improves LLMs' Tool-Integrated Reasoning by using information entropy analysis and multi-stage fine-tuning to optimize tool usage efficiency and accuracy.", "motivation": "Current LLMs using Tool-Integrated Reasoning show suboptimal behaviors like insufficient/excessive tool usage and overthinking. The challenge is to incentivize efficient and accurate TIR while stabilizing reasoning.", "method": "Proposed Tool-Light framework with: 1) Information entropy analysis of tool call impacts; 2) Dataset construction using continuous self-evolved sampling with vanilla and entropy-guided sampling; 3) Two-stage training: SFT and Self-Evolved DPO.", "result": "Experimental results on 10 datasets demonstrate Tool-Light significantly improves model efficiency in executing TIR tasks.", "conclusion": "Tool-Light effectively addresses TIR optimization by leveraging information entropy insights and multi-stage fine-tuning, achieving improved reasoning efficiency and accuracy."}}
{"id": "2509.23871", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23871", "abs": "https://arxiv.org/abs/2509.23871", "authors": ["Yukun Chen", "Boheng Li", "Yu Yuan", "Leyi Qi", "Yiming Li", "Tianwei Zhang", "Zhan Qin", "Kui Ren"], "title": "Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack", "comment": "The first three authors contributed equally to this work. To appear\n  in NeurIPS 2025. 35 pages", "summary": "Knowledge distillation (KD) is a vital technique for deploying deep neural\nnetworks (DNNs) on resource-constrained devices by transferring knowledge from\nlarge teacher models to lightweight student models. While teacher models from\nthird-party platforms may undergo security verification (\\eg, backdoor\ndetection), we uncover a novel and critical threat: distillation-conditional\nbackdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into\nteacher models, which become activated in student models via the KD process,\neven with clean distillation datasets. While the direct extension of existing\nmethods is ineffective for DCBA, we implement this attack by formulating it as\na bilevel optimization problem and proposing a simple yet effective method\n(\\ie, SCAR). Specifically, the inner optimization simulates the KD process by\noptimizing a surrogate student model, while the outer optimization leverages\noutputs from this surrogate to optimize the teacher model for implanting the\nconditional backdoor. Our SCAR addresses this complex optimization utilizing an\nimplicit differentiation algorithm with a pre-optimized trigger injection\nfunction. Extensive experiments across diverse datasets, model architectures,\nand KD techniques validate the effectiveness of our SCAR and its resistance\nagainst existing backdoor detection, highlighting a significant yet previously\noverlooked vulnerability in the KD process. Our code is available at\nhttps://github.com/WhitolfChen/SCAR.", "AI": {"tldr": "This paper introduces distillation-conditional backdoor attacks (DCBAs), a novel threat where dormant backdoors are injected into teacher models that activate only in student models during knowledge distillation, bypassing traditional security checks.", "motivation": "To address a critical vulnerability in knowledge distillation where third-party teacher models may contain undetectable backdoors that become active only during the distillation process, even with clean datasets.", "method": "Proposes SCAR method using bilevel optimization: inner optimization simulates KD process with surrogate student model, outer optimization uses surrogate outputs to optimize teacher model for conditional backdoor implantation, employing implicit differentiation with pre-optimized trigger injection.", "result": "Extensive experiments show SCAR is effective across diverse datasets, model architectures, and KD techniques, and resists existing backdoor detection methods.", "conclusion": "Reveals a significant overlooked vulnerability in knowledge distillation processes where dormant backdoors in teacher models can activate in student models, highlighting the need for new security measures."}}
{"id": "2509.23020", "categories": ["cs.LG", "math.AT"], "pdf": "https://arxiv.org/pdf/2509.23020", "abs": "https://arxiv.org/abs/2509.23020", "authors": ["Jacob Hume", "Pietro Li\u00f2"], "title": "On the Sheafification of Higher-Order Message Passing", "comment": "45 pages, 24 figures", "summary": "Recent work in Topological Deep Learning (TDL) seeks to generalize graph\nlearning's preeminent $message \\ passing$ paradigm to more complex relational\nstructures: simplicial complexes, cell complexes, hypergraphs, and combinations\nthereof. Many approaches to such ${higher\\text{-}order \\ message \\ passing}$\n(HOMP) admit formulation in terms of nonlinear diffusion with the Hodge\n(combinatorial) Laplacian, a graded operator which carries an inductive bias\nthat dimension-$k$ data features correlate with dimension-$k$ topological\nfeatures encoded in the (singular) cohomology of the underlying domain. For\n$k=0$ this recovers the graph Laplacian and its well-studied homophily bias. In\nhigher gradings, however, the Hodge Laplacian's bias is more opaque and\npotentially even degenerate. In this essay, we position sheaf theory as a\nnatural and principled formalism for modifying the Hodge Laplacian's\ndiffusion-mediated interface between local and global descriptors toward more\nexpressive message passing. The sheaf Laplacian's inductive bias correlates\ndimension-$k$ data features with dimension-$k$ $sheaf$ cohomology, a data-aware\ngeneralization of singular cohomology. We will contextualize and novelly extend\nprior theory on sheaf diffusion in graph learning ($k=0$) in such a light --\nand explore how it fails to generalize to $k>0$ -- before developing novel\ntheory and practice for the higher-order setting. Our exposition is accompanied\nby a self-contained introduction shepherding sheaves from the abstract to the\napplied.", "AI": {"tldr": "This paper proposes using sheaf theory to enhance higher-order message passing in topological deep learning, addressing limitations of the Hodge Laplacian by developing a more expressive sheaf Laplacian that generalizes to higher dimensions.", "motivation": "The motivation is to overcome the opaque and potentially degenerate inductive bias of the Hodge Laplacian in higher dimensions (k>0) for topological deep learning, by leveraging sheaf theory to create more expressive message passing frameworks.", "method": "The method involves using sheaf theory to modify the Hodge Laplacian's diffusion interface, developing the sheaf Laplacian which correlates dimension-k data features with dimension-k sheaf cohomology - a data-aware generalization of singular cohomology.", "result": "The paper develops novel theory and practice for higher-order message passing using sheaf diffusion, extending prior work on graph learning (k=0) to higher dimensions (k>0) with a self-contained introduction to sheaf theory.", "conclusion": "Sheaf theory provides a natural and principled formalism for creating more expressive message passing in topological deep learning, overcoming limitations of the Hodge Laplacian through data-aware sheaf cohomology that generalizes well to higher-order relational structures."}}
{"id": "2509.23099", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23099", "abs": "https://arxiv.org/abs/2509.23099", "authors": ["Wen Tao", "Jing Tang", "Alvin Chan", "Bryan Hooi", "Baolong Bi", "Nanyun Peng", "Yuansheng Liu", "Yiwei Wang"], "title": "How to Make Large Language Models Generate 100% Valid Molecules?", "comment": "EMNLP 2025 Main", "summary": "Molecule generation is key to drug discovery and materials science, enabling\nthe design of novel compounds with specific properties. Large language models\n(LLMs) can learn to perform a wide range of tasks from just a few examples.\nHowever, generating valid molecules using representations like SMILES is\nchallenging for LLMs in few-shot settings. In this work, we explore how LLMs\ncan generate 100% valid molecules. We evaluate whether LLMs can use SELFIES, a\nrepresentation where every string corresponds to a valid molecule, for valid\nmolecule generation but find that LLMs perform worse with SELFIES than with\nSMILES. We then examine LLMs' ability to correct invalid SMILES and find their\ncapacity limited. Finally, we introduce SmiSelf, a cross-chemical language\nframework for invalid SMILES correction. SmiSelf converts invalid SMILES to\nSELFIES using grammatical rules, leveraging SELFIES' mechanisms to correct the\ninvalid SMILES. Experiments show that SmiSelf ensures 100% validity while\npreserving molecular characteristics and maintaining or even enhancing\nperformance on other metrics. SmiSelf helps expand LLMs' practical applications\nin biomedicine and is compatible with all SMILES-based generative models. Code\nis available at https://github.com/wentao228/SmiSelf.", "AI": {"tldr": "SmiSelf is a cross-chemical language framework that converts invalid SMILES to SELFIES using grammatical rules, ensuring 100% valid molecule generation while preserving molecular characteristics and improving performance metrics.", "motivation": "Large language models struggle with generating valid molecules using SMILES representation in few-shot settings, and perform worse with SELFIES representation. There's a need for a method that ensures 100% validity while maintaining molecular properties.", "method": "SmiSelf converts invalid SMILES to SELFIES using grammatical rules, leveraging SELFIES' inherent mechanisms to correct invalid SMILES strings. This cross-chemical language framework works with all SMILES-based generative models.", "result": "SmiSelf ensures 100% validity of generated molecules while preserving molecular characteristics and maintaining or even enhancing performance on other metrics. It's compatible with all SMILES-based generative models.", "conclusion": "SmiSelf successfully expands LLMs' practical applications in biomedicine by providing a reliable method for valid molecule generation, addressing the limitations of both SMILES and SELFIES representations in few-shot learning scenarios."}}
{"id": "2509.23292", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23292", "abs": "https://arxiv.org/abs/2509.23292", "authors": ["Ningning Xu", "Yuxuan Jiang", "Shubhashis Roy Dipta"], "title": "Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning", "comment": null, "summary": "Tool-integrated reasoning (TIR) has become a key approach for improving large\nreasoning models (LRMs) on complex problems. Prior work has mainly studied when\nto invoke tools, while overlooking how tools are applied. We identify two\ncommon patterns: a calculator pattern that uses code for direct computation,\nand an algorithmic pattern that encodes problems as programs. Misaligned\nchoices often cause failures even when reasoning is sound. We propose a\ntwo-stage framework that first builds code competence from both patterns and\nthen aligns pattern selection with teacher preferences. Across challenging math\ndatasets, our pattern-aware method substantially improves both code usage and\naccuracy, for instance raising Code@1 on MATH500 from 64.0% to 70.5% and on\nAIME24 from 26.7% to 50.0%. These gains highlight the effectiveness of a\npattern-aware approach for tool-integrated reasoning.", "AI": {"tldr": "This paper proposes a pattern-aware approach for tool-integrated reasoning that identifies two common patterns (calculator and algorithmic) and uses a two-stage framework to improve code usage and accuracy on math problems.", "motivation": "Prior work on tool-integrated reasoning mainly focused on when to invoke tools but overlooked how tools are applied, leading to failures even with sound reasoning due to misaligned pattern choices.", "method": "A two-stage framework that first builds code competence from both calculator and algorithmic patterns, then aligns pattern selection with teacher preferences.", "result": "Significant improvements on challenging math datasets: Code@1 on MATH500 increased from 64.0% to 70.5%, and on AIME24 from 26.7% to 50.0%.", "conclusion": "Pattern-aware approaches are highly effective for tool-integrated reasoning, substantially improving both code usage and accuracy on complex problems."}}
{"id": "2509.23970", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23970", "abs": "https://arxiv.org/abs/2509.23970", "authors": ["Meet Udeshi", "Venkata Sai Charan Putrevu", "Prashanth Krishnamurthy", "Prashant Anantharaman", "Sean Carrick", "Ramesh Karri", "Farshad Khorrami"], "title": "Binary Diff Summarization using Large Language Models", "comment": null, "summary": "Security of software supply chains is necessary to ensure that software\nupdates do not contain maliciously injected code or introduce vulnerabilities\nthat may compromise the integrity of critical infrastructure. Verifying the\nintegrity of software updates involves binary differential analysis (binary\ndiffing) to highlight the changes between two binary versions by incorporating\nbinary analysis and reverse engineering. Large language models (LLMs) have been\napplied to binary analysis to augment traditional tools by producing natural\nlanguage summaries that cybersecurity experts can grasp for further analysis.\nCombining LLM-based binary code summarization with binary diffing can improve\nthe LLM's focus on critical changes and enable complex tasks such as automated\nmalware detection. To address this, we propose a novel framework for binary\ndiff summarization using LLMs. We introduce a novel functional sensitivity\nscore (FSS) that helps with automated triage of sensitive binary functions for\ndownstream detection tasks. We create a software supply chain security\nbenchmark by injecting 3 different malware into 6 open-source projects which\ngenerates 104 binary versions, 392 binary diffs, and 46,023 functions. On this,\nour framework achieves a precision of 0.98 and recall of 0.64 for malware\ndetection, displaying high accuracy with low false positives. Across malicious\nand benign functions, we achieve FSS separation of 3.0 points, confirming that\nFSS categorization can classify sensitive functions. We conduct a case study on\nthe real-world XZ utils supply chain attack; our framework correctly detects\nthe injected backdoor functions with high FSS.", "AI": {"tldr": "A novel framework using LLMs for binary diff summarization with functional sensitivity score (FSS) to detect malware in software supply chains, achieving high precision (0.98) and recall (0.64) on benchmark tests.", "motivation": "Security of software supply chains is critical to prevent malicious code injection in software updates. Binary differential analysis combined with LLM summarization can improve focus on critical changes and enable automated malware detection.", "method": "Proposed framework combines LLM-based binary code summarization with binary diffing. Introduces functional sensitivity score (FSS) for automated triage of sensitive binary functions. Created benchmark with 3 malware types injected into 6 open-source projects, generating 104 binary versions and 392 binary diffs.", "result": "Achieved precision of 0.98 and recall of 0.64 for malware detection. FSS separation of 3.0 points between malicious and benign functions. Successfully detected injected backdoor functions in real-world XZ utils supply chain attack case study.", "conclusion": "The framework effectively detects malware in software supply chains with high accuracy and low false positives. FSS categorization successfully classifies sensitive functions, demonstrating practical utility in real-world supply chain security scenarios."}}
{"id": "2509.23024", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23024", "abs": "https://arxiv.org/abs/2509.23024", "authors": ["Melody Zixuan Li", "Kumar Krishna Agrawal", "Arna Ghosh", "Komal Kumar Teru", "Adam Santoro", "Guillaume Lajoie", "Blake A. Richards"], "title": "Tracing the Representation Geometry of Language Models from Pretraining to Post-training", "comment": "33 pages, 14 figures, 9 tables", "summary": "Standard training metrics like loss fail to explain the emergence of complex\ncapabilities in large language models. We take a spectral approach to\ninvestigate the geometry of learned representations across pretraining and\npost-training, measuring effective rank (RankMe) and eigenspectrum decay\n($\\alpha$-ReQ). With OLMo (1B-7B) and Pythia (160M-12B) models, we uncover a\nconsistent non-monotonic sequence of three geometric phases during\nautoregressive pretraining. The initial \"warmup\" phase exhibits rapid\nrepresentational collapse. This is followed by an \"entropy-seeking\" phase,\nwhere the manifold's dimensionality expands substantially, coinciding with peak\nn-gram memorization. Subsequently, a \"compression-seeking\" phase imposes\nanisotropic consolidation, selectively preserving variance along dominant\neigendirections while contracting others, a transition marked with significant\nimprovement in downstream task performance. We show these phases can emerge\nfrom a fundamental interplay of cross-entropy optimization under skewed token\nfrequencies and representational bottlenecks ($d \\ll |V|$). Post-training\nfurther transforms geometry: SFT and DPO drive \"entropy-seeking\" dynamics to\nintegrate specific instructional or preferential data, improving\nin-distribution performance while degrading out-of-distribution robustness.\nConversely, RLVR induces \"compression-seeking\", enhancing reward alignment but\nreducing generation diversity.", "AI": {"tldr": "The paper investigates the geometric phases of LLM representations during training, revealing three distinct phases: initial collapse, entropy-seeking expansion, and compression-seeking consolidation, driven by cross-entropy optimization and representational bottlenecks.", "motivation": "Standard training metrics fail to explain the emergence of complex capabilities in large language models, motivating a spectral analysis of representation geometry.", "method": "Spectral approach using effective rank (RankMe) and eigenspectrum decay (\u03b1-ReQ) to analyze representation geometry across pretraining and post-training phases with OLMo (1B-7B) and Pythia (160M-12B) models.", "result": "Uncovered consistent non-monotonic sequence of three geometric phases: initial collapse, entropy-seeking expansion (coinciding with peak n-gram memorization), and compression-seeking consolidation (marked by downstream performance improvement). Post-training shows SFT/DPO drive entropy-seeking while RLVR induces compression-seeking.", "conclusion": "Training phases emerge from cross-entropy optimization under skewed token frequencies and representational bottlenecks. Post-training transforms geometry differently: SFT/DPO improve in-distribution performance but reduce robustness, while RLVR enhances reward alignment but reduces diversity."}}
{"id": "2509.23124", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23124", "abs": "https://arxiv.org/abs/2509.23124", "authors": ["Jeonghoon Shim", "Woojung Song", "Cheyon Jin", "Seungwon KooK", "Yohan Jo"], "title": "Non-Collaborative User Simulators for Tool Agents", "comment": "9 pages", "summary": "Non-Collaborative User Simulators for Tool Agents Download PDF Jeonghoon\nShim, Woojung Song, Cheyon Jin, Seungwon KooK, Yohan Jo 19 Sept 2025 (modified:\n25 Sept 2025)ICLR 2026 Conference SubmissionConference, AuthorsRevisionsCC BY\n4.0 Keywords: Tool Agent, User Simulator, Non-collaborative User, Dialogue\nSimulation TL;DR: A non-collaborative user simulation method for tool agent.\nAbstract: Tool agents interact with users through multi-turn dialogues to\naccomplish various tasks. Recent studies have adopted user simulation methods\nto develop these agents in multi-turn settings. However, existing user\nsimulators tend to be agent-friendly, exhibiting only cooperative behaviors,\nwhich fails to train and test agents against non-collaborative users in the\nreal world. To address this, we propose a novel user simulator architecture\nthat simulates four categories of non-collaborative behaviors: requesting\nunavailable services, digressing into tangential conversations, expressing\nimpatience, and providing incomplete utterances. Our user simulator can\nsimulate challenging and natural non-collaborative behaviors while reliably\ndelivering all intents and information necessary to accomplish the task. Our\nexperiments on MultiWOZ and $\\tau$-bench reveal significant performance\ndegradation in state-of-the-art tool agents when encountering non-collaborative\nusers. We provide detailed analyses of agents' weaknesses under each\nnon-collaborative condition, such as escalated hallucinations and dialogue\nbreakdowns. Ultimately, we contribute an easily extensible user simulation\nframework to help the research community develop tool agents and preemptively\ndiagnose them under challenging real-world conditions within their own\nservices.", "AI": {"tldr": "A non-collaborative user simulation method for tool agents that simulates four types of challenging user behaviors: requesting unavailable services, digressing conversations, expressing impatience, and providing incomplete utterances.", "motivation": "Existing user simulators are too agent-friendly and cooperative, failing to train and test tool agents against real-world non-collaborative users who exhibit challenging behaviors.", "method": "Proposed a novel user simulator architecture that simulates four categories of non-collaborative behaviors while reliably delivering all necessary intents and information for task completion.", "result": "Experiments on MultiWOZ and \u03c4-bench show significant performance degradation in state-of-the-art tool agents when encountering non-collaborative users, with issues like escalated hallucinations and dialogue breakdowns.", "conclusion": "Provides an extensible user simulation framework to help develop tool agents and preemptively diagnose them under challenging real-world conditions."}}
{"id": "2509.23392", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23392", "abs": "https://arxiv.org/abs/2509.23392", "authors": ["Jinyi Han", "Ying Huang", "Ying Liao", "Zishang Jiang", "Xikun Lu", "Haiquan Zhao", "Xinyi Wang", "Guanghao Zhou", "Sihang Jiang", "Jiaqing Liang", "Weikang Zhou", "Zeye Sun", "Fei Yu", "Yanghua Xiao"], "title": "Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking", "comment": null, "summary": "Large Reasoning Models (LRMs) have achieved impressive performance on\nchallenging tasks, yet their deep reasoning often incurs substantial\ncomputational costs. To achieve efficient reasoning, existing reinforcement\nlearning methods still struggle to construct short reasoning path during the\nrollout stage, limiting effective learning. Inspired by Evidence Accumulation\nModels, we find that LRMs have accumulated sufficient information early in\nreasoning, making further reasoning steps redundant. Based on this insight, we\npropose Just-Enough Thinking (JET), which trains models to proactively\nterminate unnecessary reasoning. JET performs trajectory truncation during\nrollout to expose the model to short, distributionally consistent reasoning\npaths. Besides, it uses a quality-controlled length reward to better encourage\nconcise reasoning while maintaining correctness. Extensive experiments\ndemonstrate that JET significantly improves reasoning efficiency without\nsacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6%\naccuracy gain while reducing output length by 46.3% on the Olympiad benchmark.\nOur code is available in the GitHub.", "AI": {"tldr": "JET trains Large Reasoning Models to proactively terminate unnecessary reasoning steps, achieving significant efficiency improvements without accuracy loss.", "motivation": "Large Reasoning Models incur substantial computational costs due to deep reasoning, and existing methods struggle to construct short reasoning paths during rollout.", "method": "JET performs trajectory truncation during rollout to expose models to short reasoning paths and uses quality-controlled length reward to encourage concise reasoning while maintaining correctness.", "result": "JET significantly improves reasoning efficiency without sacrificing accuracy. DeepSeek-Distill-Qwen-1.5B achieved 4.6% accuracy gain while reducing output length by 46.3% on Olympiad benchmark.", "conclusion": "JET enables efficient reasoning by training models to terminate unnecessary reasoning steps, demonstrating that LRMs accumulate sufficient information early in reasoning, making further steps redundant."}}
{"id": "2509.23984", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23984", "abs": "https://arxiv.org/abs/2509.23984", "authors": ["Pranav Garimidi", "Joachim Neu", "Max Resnick"], "title": "Multiple Concurrent Proposers: Why and How", "comment": null, "summary": "Traditional single-proposer blockchains suffer from miner extractable value\n(MEV), where validators exploit their serial monopoly on transaction inclusion\nand ordering to extract rents from users. While there have been many\ndevelopments at the application layer to reduce the impact of MEV, these\napproaches largely require auctions as a subcomponent. Running auctions\nefficiently on chain requires two key properties of the underlying consensus\nprotocol: selective-censorship resistance and hiding. These properties\nguarantee that an adversary can neither selectively delay transactions nor see\ntheir contents before they are confirmed. We propose a multiple concurrent\nproposer (MCP) protocol offering exactly these properties.", "AI": {"tldr": "The paper proposes a multiple concurrent proposer (MCP) protocol to address MEV issues in blockchains by providing selective-censorship resistance and hiding properties.", "motivation": "Traditional single-proposer blockchains suffer from miner extractable value (MEV), where validators exploit their monopoly on transaction inclusion and ordering to extract rents from users. Current application-layer solutions require auctions but lack the necessary consensus protocol properties.", "method": "The authors propose a multiple concurrent proposer (MCP) protocol that offers selective-censorship resistance and hiding properties, which are essential for running efficient on-chain auctions.", "result": "The MCP protocol provides the required properties to prevent adversaries from selectively delaying transactions or seeing transaction contents before confirmation.", "conclusion": "The proposed MCP protocol addresses MEV issues by enabling efficient on-chain auctions through selective-censorship resistance and hiding properties in the consensus layer."}}
{"id": "2509.23027", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23027", "abs": "https://arxiv.org/abs/2509.23027", "authors": ["Yuke Li", "Yujia Zheng", "Tianyi Xiong", "Zhenyi Wang", "Heng Huang"], "title": "Understanding Catastrophic Interference On the Identifibility of Latent Representations", "comment": null, "summary": "Catastrophic interference, also known as catastrophic forgetting, is a\nfundamental challenge in machine learning, where a trained learning model\nprogressively loses performance on previously learned tasks when adapting to\nnew ones. In this paper, we aim to better understand and model the catastrophic\ninterference problem from a latent representation learning point of view, and\npropose a novel theoretical framework that formulates catastrophic interference\nas an identification problem. Our analysis demonstrates that the forgetting\nphenomenon can be quantified by the distance between partial-task aware (PTA)\nand all-task aware (ATA) setups. Building upon recent advances in\nidentifiability theory, we prove that this distance can be minimized through\nidentification of shared latent variables between these setups. When learning,\nwe propose our method \\ourmeos with two-stage training strategy: First, we\nemploy maximum likelihood estimation to learn the latent representations from\nboth PTA and ATA configurations. Subsequently, we optimize the KL divergence to\nidentify and learn the shared latent variables. Through theoretical guarantee\nand empirical validations, we establish that identifying and learning these\nshared representations can effectively mitigate catastrophic interference in\nmachine learning systems. Our approach provides both theoretical guarantees and\npractical performance improvements across both synthetic and benchmark\ndatasets.", "AI": {"tldr": "This paper proposes a novel theoretical framework that formulates catastrophic interference as an identification problem and introduces a two-stage training method (\\ourmeos) to mitigate forgetting by identifying shared latent variables between partial-task aware and all-task aware setups.", "motivation": "Catastrophic interference (catastrophic forgetting) is a fundamental challenge where machine learning models lose performance on previously learned tasks when adapting to new ones. The paper aims to better understand and model this problem from a latent representation learning perspective.", "method": "The proposed method \\ourmeos uses a two-stage training strategy: 1) Maximum likelihood estimation to learn latent representations from both partial-task aware (PTA) and all-task aware (ATA) configurations, 2) KL divergence optimization to identify and learn shared latent variables between these setups.", "result": "Theoretical analysis demonstrates that catastrophic forgetting can be quantified by the distance between PTA and ATA setups, and this distance can be minimized through identification of shared latent variables. Empirical validations show the approach effectively mitigates catastrophic interference.", "conclusion": "Identifying and learning shared latent representations between different task configurations provides both theoretical guarantees and practical performance improvements for mitigating catastrophic interference in machine learning systems, validated across synthetic and benchmark datasets."}}
{"id": "2509.23140", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23140", "abs": "https://arxiv.org/abs/2509.23140", "authors": ["Song Jin", "Juntian Zhang", "Yong Liu", "Xun Zhang", "Yufei Zhang", "Fei Jiang", "Guojun Yin", "Wei Lin", "Rui Yan"], "title": "Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning", "comment": null, "summary": "Recent advancements have endowed Large Language Models (LLMs) with impressive\ngeneral reasoning capabilities, yet they often struggle with personalization\nreasoning - the crucial ability to analyze user history, infer unique\npreferences, and generate tailored responses. To address this limitation, we\nintroduce TagPR, a novel training framework that significantly enhances an\nLLM's intrinsic capacity for personalization reasoning through a tagging the\nthought approach. Our method first develops a data-driven pipeline to\nautomatically generate and semantically label reasoning chains, creating a\nstructured dataset that fosters interpretable reasoning. We then propose a\nsynergistic training strategy that begins with Supervised Fine-Tuning (SFT) on\nthis tagged data to establish foundational reasoning patterns, followed by a\nmulti-stage reinforcement learning (RL) process. This RL phase is guided by a\nunique composite reward signal, which integrates tag-based constraints and a\nnovel Personalization Reward Model with User Embeddings (PRMU) to achieve\nfine-grained alignment with user-specific logic. Extensive experiments on the\npublic LaMP benchmark and a self-constructed dataset demonstrate that our\napproach achieves state-of-the-art results, delivering an average improvement\nof 32.65% over the base model across all tasks. Our work validates that\nstructured, interpretable reasoning is a highly effective pathway to unlocking\ngenuine personalization capabilities in LLMs.", "AI": {"tldr": "TagPR is a training framework that enhances LLMs' personalization reasoning through tagged thought processes, achieving 32.65% improvement over base models.", "motivation": "LLMs struggle with personalization reasoning - analyzing user history and generating tailored responses, despite having strong general reasoning capabilities.", "method": "Data-driven pipeline to generate semantically labeled reasoning chains, followed by SFT and multi-stage RL with composite reward signal (tag-based constraints + PRMU).", "result": "Achieves state-of-the-art results on LaMP benchmark and self-constructed dataset, with 32.65% average improvement over base model across all tasks.", "conclusion": "Structured, interpretable reasoning is an effective pathway to unlock genuine personalization capabilities in LLMs."}}
{"id": "2509.23415", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23415", "abs": "https://arxiv.org/abs/2509.23415", "authors": ["Gyubok Lee", "Woosog Chay", "Heeyoung Kwak", "Yeong Hwa Kim", "Haanju Yoo", "Oksoon Jeong", "Meong Hi Son", "Edward Choi"], "title": "From Conversation to Query Execution: Benchmarking User and Tool Interactions for EHR Database Agents", "comment": "Under review", "summary": "Despite the impressive performance of LLM-powered agents, their adoption for\nElectronic Health Record (EHR) data access remains limited by the absence of\nbenchmarks that adequately capture real-world clinical data access flows. In\npractice, two core challenges hinder deployment: query ambiguity from vague\nuser questions and value mismatch between user terminology and database\nentries. To address this, we introduce EHR-ChatQA an interactive database\nquestion answering benchmark that evaluates the end-to-end workflow of database\nagents: clarifying user questions, using tools to resolve value mismatches, and\ngenerating correct SQL to deliver accurate answers. To cover diverse patterns\nof query ambiguity and value mismatch, EHR-ChatQA assesses agents in a\nsimulated environment with an LLM-based user across two interaction flows:\nIncremental Query Refinement (IncreQA), where users add constraints to existing\nqueries, and Adaptive Query Refinement (AdaptQA), where users adjust their\nsearch goals mid-conversation. Experiments with state-of-the-art LLMs (e.g.,\no4-mini and Gemini-2.5-Flash) over five i.i.d. trials show that while agents\nachieve high Pass@5 of 90-95% (at least one of five trials) on IncreQA and\n60-80% on AdaptQA, their Pass^5 (consistent success across all five trials) is\nsubstantially lower by 35-60%. These results underscore the need to build\nagents that are not only performant but also robust for the safety-critical EHR\ndomain. Finally, we provide diagnostic insights into common failure modes to\nguide future agent development.", "AI": {"tldr": "EHR-ChatQA is a benchmark for LLM-powered EHR database agents that addresses query ambiguity and value mismatch through interactive query refinement, showing high initial success rates but poor consistency across trials.", "motivation": "Current LLM agents lack benchmarks for real-world clinical data access flows, hindered by query ambiguity from vague questions and value mismatch between user terminology and database entries.", "method": "Introduces EHR-ChatQA benchmark with simulated LLM-based user across two interaction flows: Incremental Query Refinement (IncreQA) and Adaptive Query Refinement (AdaptQA), evaluating agents' ability to clarify questions, resolve value mismatches, and generate correct SQL.", "result": "Agents achieve high Pass@5 of 90-95% on IncreQA and 60-80% on AdaptQA, but Pass^5 (consistent success across all trials) is substantially lower by 35-60%, indicating poor robustness despite high performance.", "conclusion": "There's a critical need to build agents that are not only performant but also robust for safety-critical EHR domains, with diagnostic insights provided for future development."}}
{"id": "2509.24037", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24037", "abs": "https://arxiv.org/abs/2509.24037", "authors": ["Alireza Lotfi", "Charalampos Katsis", "Elisa Bertino"], "title": "Automated Vulnerability Validation and Verification: A Large Language Model Approach", "comment": null, "summary": "Software vulnerabilities remain a critical security challenge, providing\nentry points for attackers into enterprise networks. Despite advances in\nsecurity practices, the lack of high-quality datasets capturing diverse exploit\nbehavior limits effective vulnerability assessment and mitigation. This paper\nintroduces an end-to-end multi-step pipeline leveraging generative AI,\nspecifically large language models (LLMs), to address the challenges of\norchestrating and reproducing attacks to known software vulnerabilities. Our\napproach extracts information from CVE disclosures in the National\nVulnerability Database, augments it with external public knowledge (e.g.,\nthreat advisories, code snippets) using Retrieval-Augmented Generation (RAG),\nand automates the creation of containerized environments and exploit code for\neach vulnerability. The pipeline iteratively refines generated artifacts,\nvalidates attack success with test cases, and supports complex multi-container\nsetups. Our methodology overcomes key obstacles, including noisy and incomplete\nvulnerability descriptions, by integrating LLMs and RAG to fill information\ngaps. We demonstrate the effectiveness of our pipeline across different\nvulnerability types, such as memory overflows, denial of service, and remote\ncode execution, spanning diverse programming languages, libraries and years. In\ndoing so, we uncover significant inconsistencies in CVE descriptions,\nemphasizing the need for more rigorous verification in the CVE disclosure\nprocess. Our approach is model-agnostic, working across multiple LLMs, and we\nopen-source the artifacts to enable reproducibility and accelerate security\nresearch. To the best of our knowledge, this is the first system to\nsystematically orchestrate and exploit known vulnerabilities in containerized\nenvironments by combining general-purpose LLM reasoning with CVE data and\nRAG-based context enrichment.", "AI": {"tldr": "This paper presents an end-to-end pipeline using generative AI (LLMs) and Retrieval-Augmented Generation (RAG) to systematically orchestrate and reproduce attacks for known software vulnerabilities from CVE data, creating containerized environments and exploit code.", "motivation": "The lack of high-quality datasets capturing diverse exploit behavior limits effective vulnerability assessment and mitigation, despite software vulnerabilities being critical security challenges.", "method": "Extracts CVE information from NVD, augments with external knowledge using RAG, automates creation of containerized environments and exploit code, iteratively refines artifacts, validates attacks with test cases, and supports multi-container setups.", "result": "Successfully demonstrated across different vulnerability types (memory overflows, DoS, RCE) spanning diverse programming languages and years, uncovering inconsistencies in CVE descriptions and emphasizing need for better verification.", "conclusion": "The approach is model-agnostic, works across multiple LLMs, and provides the first systematic method to orchestrate and exploit known vulnerabilities in containerized environments by combining LLM reasoning with CVE data and RAG context enrichment."}}
{"id": "2509.23030", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23030", "abs": "https://arxiv.org/abs/2509.23030", "authors": ["Yang Lv", "Jin Cao", "Ben Niu", "Zhe Sun", "Fengwei Wang", "Fenghua Li", "Hui Li"], "title": "DPFNAS: Differential Privacy-Enhanced Federated Neural Architecture Search for 6G Edge Intelligence", "comment": null, "summary": "The Sixth-Generation (6G) network envisions pervasive artificial intelligence\n(AI) as a core goal, enabled by edge intelligence through on-device data\nutilization. To realize this vision, federated learning (FL) has emerged as a\nkey paradigm for collaborative training across edge devices. However, the\nsensitivity and heterogeneity of edge data pose key challenges to FL: parameter\nsharing risks data reconstruction, and a unified global model struggles to\nadapt to diverse local distributions. In this paper, we propose a novel\nfederated learning framework that integrates personalized differential privacy\n(DP) and adaptive model design. To protect training data, we leverage\nsample-level representations for knowledge sharing and apply a personalized DP\nstrategy to resist reconstruction attacks. To ensure distribution-aware\nadaptation under privacy constraints, we develop a privacy-aware neural\narchitecture search (NAS) algorithm that generates locally customized\narchitectures and hyperparameters. To the best of our knowledge, this is the\nfirst personalized DP solution tailored for representation-based FL with\ntheoretical convergence guarantees. Our scheme achieves strong privacy\nguarantees for training data while significantly outperforming state-of-the-art\nmethods in model performance. Experiments on benchmark datasets such as\nCIFAR-10 and CIFAR-100 demonstrate that our scheme improves accuracy by 6.82\\%\nover the federated NAS method PerFedRLNAS, while reducing model size to 1/10\nand communication cost to 1/20.", "AI": {"tldr": "A novel federated learning framework combining personalized differential privacy and adaptive model design to address data sensitivity and heterogeneity in 6G edge networks, achieving improved accuracy while reducing model size and communication costs.", "motivation": "To enable pervasive AI in 6G networks through edge intelligence while addressing challenges of data sensitivity (privacy risks from parameter sharing) and data heterogeneity (difficulty of unified global models adapting to diverse local distributions).", "method": "Integrates personalized differential privacy using sample-level representations for knowledge sharing, and develops a privacy-aware neural architecture search algorithm to generate locally customized architectures and hyperparameters under privacy constraints.", "result": "Achieves strong privacy guarantees while significantly outperforming state-of-the-art methods, improving accuracy by 6.82% over PerFedRLNAS on CIFAR datasets, reducing model size to 1/10 and communication cost to 1/20.", "conclusion": "Proposes the first personalized differential privacy solution for representation-based federated learning with theoretical convergence guarantees, effectively balancing privacy protection and model performance in edge computing environments."}}
{"id": "2509.23146", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23146", "abs": "https://arxiv.org/abs/2509.23146", "authors": ["Zichao Yu", "Ming Li", "Wenyi Zhang", "Weiguo Gao"], "title": "Tree Reward-Aligned Search for TReASURe in Masked Diffusion Language Models", "comment": "21 pages, 6 figures", "summary": "Tree search has recently emerged as a powerful framework for aligning\ngenerative models with task-specific rewards at test time. Applying tree search\nto Masked Diffusion Language Models, however, introduces two key challenges:\n(i) parallel unmasking yields highly correlated branches, limiting exploration,\nand (ii) reward evaluation via sampled completions produces high-variance\nestimates, making pruning unstable. We propose TReASURe, a tree-search\ntest-time alignment method that addresses these issues. It introduces (i)\nUnmaskBranch, a branching strategy based on first-hitting unmasking that\ndiversifies both token content and reveal order with a single model call per\nparent node, and (ii) ResubstituteScore, a pruning rule that uses deterministic\nresubstitution to score partially masked sequences with low-variance proxy\ncompletions. Theoretically, we quantify branching efficiency gains in NFEs\n(number of function evaluations), show that the scoring rule approximates the\ntrue reward with error bounded by predictive uncertainty, and prove\nimprovements with larger tree widths. Empirically, TReASURe achieves\nstate-of-the-art results on perplexity, linguistic acceptability, and control\nof sentiment and toxicity, outperforming prior methods under matched compute\nbudgets, with especially strong gains in low-NFE regimes.", "AI": {"tldr": "TReASURe is a tree-search test-time alignment method for Masked Diffusion Language Models that addresses correlation issues in parallel unmasking and variance in reward evaluation through UnmaskBranch and ResubstituteScore techniques.", "motivation": "Tree search for aligning generative models faces challenges with Masked Diffusion Language Models: (i) parallel unmasking creates correlated branches limiting exploration, and (ii) reward evaluation via sampled completions produces high-variance estimates making pruning unstable.", "method": "Proposes TReASURe with two key components: (1) UnmaskBranch - a branching strategy using first-hitting unmasking to diversify token content and reveal order with single model call per parent node, (2) ResubstituteScore - a pruning rule using deterministic resubstitution to score partially masked sequences with low-variance proxy completions.", "result": "Theoretically shows branching efficiency gains in NFEs, scoring rule approximates true reward with error bounded by predictive uncertainty, and improvements with larger tree widths. Empirically achieves SOTA results on perplexity, linguistic acceptability, and control of sentiment and toxicity, outperforming prior methods under matched compute budgets, especially in low-NFE regimes.", "conclusion": "TReASURe effectively addresses key challenges in tree search for Masked Diffusion Language Models, providing more efficient branching and stable pruning through novel techniques, leading to superior performance across multiple metrics with computational efficiency."}}
{"id": "2509.23426", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23426", "abs": "https://arxiv.org/abs/2509.23426", "authors": ["Shanghua Gao", "Richard Zhu", "Pengwei Sui", "Zhenglun Kong", "Sufian Aldogom", "Yepeng Huang", "Ayush Noori", "Reza Shamji", "Krishna Parvataneni", "Theodoros Tsiligkaridis", "Marinka Zitnik"], "title": "Democratizing AI scientists using ToolUniverse", "comment": "https://aiscientist.tools", "summary": "AI scientists are emerging computational systems that serve as collaborative\npartners in discovery. These systems remain difficult to build because they are\nbespoke, tied to rigid workflows, and lack shared environments that unify\ntools, data, and analyses into a common ecosystem. In omics, unified ecosystems\nhave transformed research by enabling interoperability, reuse, and\ncommunity-driven development; AI scientists require comparable infrastructure.\nWe present ToolUniverse, an ecosystem for building AI scientists from any\nlanguage or reasoning model, whether open or closed. TOOLUNIVERSE standardizes\nhow AI scientists identify and call tools, integrating more than 600 machine\nlearning models, datasets, APIs, and scientific packages for data analysis,\nknowledge retrieval, and experimental design. It automatically refines tool\ninterfaces for correct use by AI scientists, creates new tools from natural\nlanguage descriptions, iteratively optimizes tool specifications, and composes\ntools into agentic workflows. In a case study of hypercholesterolemia,\nToolUniverse was used to create an AI scientist to identify a potent analog of\na drug with favorable predicted properties. The open-source ToolUniverse is\navailable at https://aiscientist.tools.", "AI": {"tldr": "ToolUniverse is an ecosystem for building AI scientists that integrates over 600 tools, automatically refines interfaces, creates tools from natural language, and composes agentic workflows.", "motivation": "AI scientists are difficult to build due to being bespoke, tied to rigid workflows, and lacking shared environments that unify tools, data, and analyses.", "method": "ToolUniverse standardizes how AI scientists identify and call tools, automatically refines tool interfaces, creates new tools from natural language descriptions, iteratively optimizes tool specifications, and composes tools into agentic workflows.", "result": "In a hypercholesterolemia case study, ToolUniverse created an AI scientist that identified a potent drug analog with favorable predicted properties.", "conclusion": "ToolUniverse provides comparable infrastructure to omics ecosystems, enabling interoperability, reuse, and community-driven development for AI scientists."}}
{"id": "2509.24043", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24043", "abs": "https://arxiv.org/abs/2509.24043", "authors": ["Yihan Wu", "Ruibo Chen", "Georgios Milis", "Heng Huang"], "title": "An Ensemble Framework for Unbiased Language Model Watermarking", "comment": null, "summary": "As large language models become increasingly capable and widely deployed,\nverifying the provenance of machine-generated content is critical to ensuring\ntrust, safety, and accountability. Watermarking techniques have emerged as a\npromising solution by embedding imperceptible statistical signals into the\ngeneration process. Among them, unbiased watermarking is particularly\nattractive due to its theoretical guarantee of preserving the language model's\noutput distribution, thereby avoiding degradation in fluency or detectability\nthrough distributional shifts. However, existing unbiased watermarking schemes\noften suffer from weak detection power and limited robustness, especially under\nshort text lengths or distributional perturbations. In this work, we propose\nENS, a novel ensemble framework that enhances the detectability and robustness\nof logits-based unbiased watermarks while strictly preserving their\nunbiasedness. ENS sequentially composes multiple independent watermark\ninstances, each governed by a distinct key, to amplify the watermark signal. We\ntheoretically prove that the ensemble construction remains unbiased in\nexpectation and demonstrate how it improves the signal-to-noise ratio for\nstatistical detectors. Empirical evaluations on multiple LLM families show that\nENS substantially reduces the number of tokens needed for reliable detection\nand increases resistance to smoothing and paraphrasing attacks without\ncompromising generation quality.", "AI": {"tldr": "ENS is an ensemble framework that enhances detectability and robustness of unbiased watermarking for LLMs while preserving unbiasedness through sequential composition of multiple watermark instances.", "motivation": "Existing unbiased watermarking schemes suffer from weak detection power and limited robustness, especially under short text lengths or distributional perturbations.", "method": "Sequentially composes multiple independent watermark instances with distinct keys to amplify watermark signal while theoretically preserving unbiasedness.", "result": "Substantially reduces tokens needed for reliable detection and increases resistance to smoothing and paraphrasing attacks without compromising generation quality.", "conclusion": "ENS provides a practical solution for enhancing watermark detectability and robustness while maintaining the theoretical guarantees of unbiased watermarking."}}
{"id": "2509.23037", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23037", "abs": "https://arxiv.org/abs/2509.23037", "authors": ["Javad Forough", "Mohammad Maheri", "Hamed Haddadi"], "title": "GuardNet: Graph-Attention Filtering for Jailbreak Defense in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly susceptible to jailbreak\nattacks, which are adversarial prompts that bypass alignment constraints and\ninduce unauthorized or harmful behaviors. These vulnerabilities undermine the\nsafety, reliability, and trustworthiness of LLM outputs, posing critical risks\nin domains such as healthcare, finance, and legal compliance. In this paper, we\npropose GuardNet, a hierarchical filtering framework that detects and filters\njailbreak prompts prior to inference. GuardNet constructs structured graphs\nthat combine sequential links, syntactic dependencies, and attention-derived\ntoken relations to capture both linguistic structure and contextual patterns\nindicative of jailbreak behavior. It then applies graph neural networks at two\nlevels: (i) a prompt-level filter that detects global adversarial prompts, and\n(ii) a token-level filter that pinpoints fine-grained adversarial spans.\nExtensive experiments across three datasets and multiple attack settings show\nthat GuardNet substantially outperforms prior defenses. It raises prompt-level\nF$_1$ scores from 66.4\\% to 99.8\\% on LLM-Fuzzer, and from 67-79\\% to over 94\\%\non PLeak datasets. At the token level, GuardNet improves F$_1$ from 48-75\\% to\n74-91\\%, with IoU gains up to +28\\%. Despite its structural complexity,\nGuardNet maintains acceptable latency and generalizes well in cross-domain\nevaluations, making it a practical and robust defense against jailbreak threats\nin real-world LLM deployments.", "AI": {"tldr": "GuardNet is a hierarchical filtering framework that detects and filters jailbreak prompts in LLMs using graph neural networks on structured graphs combining sequential, syntactic, and attention-based token relations.", "motivation": "LLMs are vulnerable to jailbreak attacks that bypass alignment constraints, posing critical risks in safety-critical domains like healthcare and finance. Existing defenses are insufficient to protect against these adversarial prompts.", "method": "Constructs structured graphs combining sequential links, syntactic dependencies, and attention-derived token relations. Uses graph neural networks at two levels: prompt-level filter for global detection and token-level filter for fine-grained adversarial span identification.", "result": "Significantly outperforms prior defenses: raises prompt-level F1 from 66.4% to 99.8% on LLM-Fuzzer, from 67-79% to over 94% on PLeak datasets. Token-level F1 improves from 48-75% to 74-91%, with IoU gains up to +28%. Maintains acceptable latency and generalizes well.", "conclusion": "GuardNet provides a practical and robust defense against jailbreak threats in real-world LLM deployments, effectively detecting adversarial prompts while maintaining performance and generalization capabilities."}}
{"id": "2509.23166", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23166", "abs": "https://arxiv.org/abs/2509.23166", "authors": ["Chenxing Wei", "Hong Wang", "Ying He", "Fei Yu", "Yao Shu"], "title": "Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with LLMs", "comment": "32 pages, 7 figures", "summary": "Large Language Models (LLMs) employ multi-turn interaction as a fundamental\nparadigm for completing complex tasks. However, their performance often\ndegrades in extended interactions, as they are typically trained on static,\nsingle-turn data, which hinders their ability to adapt to real-time user\nfeedback. To address this limitation, we first propose a new paradigm:\nTest-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes\nuser feedback from the ongoing interaction as a reward signal to estimate a\nlatent optimal policy aligned with user preferences, then updates a small\nsubset of parameters to steer the model toward this policy, ultimately enabling\nefficient in-conversation self-correction. We then introduce Optimum-Referenced\nOne-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM.\nROSA guides the model parameters toward a theoretical optimal policy in a\nsingle, efficient update step, avoiding costly iterative gradient-based\noptimization and minimizing computational overhead. We provide a rigorous\ntheoretical analysis guaranteeing that the policy of ROSA converges to the\npreference of user as the number of interactions increases. Extensive\nexperiments on challenging benchmark demonstrate that ROSA achieves significant\nimprovements in both task effectiveness and efficiency.", "AI": {"tldr": "T2PAM enables LLMs to adapt during multi-turn interactions using real-time user feedback, with ROSA providing efficient one-step parameter updates toward optimal policies.", "motivation": "LLMs degrade in extended interactions due to training on static data, lacking real-time adaptation to user feedback.", "method": "Propose T2PAM paradigm using user feedback as reward signal, then ROSA algorithm for one-step parameter updates toward optimal policy.", "result": "ROSA achieves significant improvements in task effectiveness and efficiency on challenging benchmarks.", "conclusion": "T2PAM with ROSA enables efficient in-conversation self-correction for LLMs, with theoretical convergence guarantees."}}
{"id": "2509.23449", "categories": ["cs.AI", "cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23449", "abs": "https://arxiv.org/abs/2509.23449", "authors": ["Charles E. Gagnon", "Steven H. H. Ding", "Philippe Charland", "Benjamin C. M. Fung"], "title": "Beyond Embeddings: Interpretable Feature Extraction for Binary Code Similarity", "comment": "17 pages, 7 figures, submitted to USENIX Security '26", "summary": "Binary code similarity detection is a core task in reverse engineering. It\nsupports malware analysis and vulnerability discovery by identifying\nsemantically similar code in different contexts. Modern methods have progressed\nfrom manually engineered features to vector representations. Hand-crafted\nstatistics (e.g., operation ratios) are interpretable, but shallow and fail to\ngeneralize. Embedding-based methods overcome this by learning robust\ncross-setting representations, but these representations are opaque vectors\nthat prevent rapid verification. They also face a scalability-accuracy\ntrade-off, since high-dimensional nearest-neighbor search requires\napproximations that reduce precision. Current approaches thus force a\ncompromise between interpretability, generalizability, and scalability.\n  We bridge these gaps using a language model-based agent to conduct structured\nreasoning analysis of assembly code and generate features such as input/output\ntypes, side effects, notable constants, and algorithmic intent. Unlike\nhand-crafted features, they are richer and adaptive. Unlike embeddings, they\nare human-readable, maintainable, and directly searchable with inverted or\nrelational indexes. Without any matching training, our method respectively\nachieves 42% and 62% for recall@1 in cross-architecture and cross-optimization\ntasks, comparable to embedding methods with training (39% and 34%). Combined\nwith embeddings, it significantly outperforms the state-of-the-art,\ndemonstrating that accuracy, scalability, and interpretability can coexist.", "AI": {"tldr": "A new method for binary code similarity detection that uses language model agents to generate structured, interpretable features from assembly code, bridging the gap between hand-crafted features and opaque embeddings.", "motivation": "Current binary code similarity detection methods face compromises between interpretability, generalizability, and scalability. Hand-crafted features are interpretable but shallow, while embedding methods are robust but opaque and face scalability-accuracy trade-offs.", "method": "Use language model-based agents to conduct structured reasoning analysis of assembly code, generating human-readable features like input/output types, side effects, notable constants, and algorithmic intent.", "result": "Achieves 42% recall@1 in cross-architecture and 62% in cross-optimization tasks without matching training, comparable to embedding methods with training. When combined with embeddings, significantly outperforms state-of-the-art methods.", "conclusion": "The proposed method demonstrates that accuracy, scalability, and interpretability can coexist in binary code similarity detection by combining structured reasoning with embedding approaches."}}
{"id": "2509.24048", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24048", "abs": "https://arxiv.org/abs/2509.24048", "authors": ["Yihan Wu", "Xuehao Cui", "Ruibo Chen", "Heng Huang"], "title": "Analyzing and Evaluating Unbiased Language Model Watermark", "comment": null, "summary": "Verifying the authenticity of AI-generated text has become increasingly\nimportant with the rapid advancement of large language models, and unbiased\nwatermarking has emerged as a promising approach due to its ability to preserve\noutput distribution without degrading quality. However, recent work reveals\nthat unbiased watermarks can accumulate distributional bias over multiple\ngenerations and that existing robustness evaluations are inconsistent across\nstudies. To address these issues, we introduce UWbench, the first open-source\nbenchmark dedicated to the principled evaluation of unbiased watermarking\nmethods. Our framework combines theoretical and empirical contributions: we\npropose a statistical metric to quantify multi-batch distribution drift, prove\nan impossibility result showing that no unbiased watermark can perfectly\npreserve the distribution under infinite queries, and develop a formal analysis\nof robustness against token-level modification attacks. Complementing this\ntheory, we establish a three-axis evaluation protocol: unbiasedness,\ndetectability, and robustness, and show that token modification attacks provide\nmore stable robustness assessments than paraphrasing-based methods. Together,\nUWbench offers the community a standardized and reproducible platform for\nadvancing the design and evaluation of unbiased watermarking algorithms.", "AI": {"tldr": "UWbench is the first open-source benchmark for evaluating unbiased watermarking methods, addressing distributional bias accumulation and inconsistent robustness evaluations through theoretical analysis and a three-axis evaluation protocol.", "motivation": "To address the issues of distributional bias accumulation in unbiased watermarks and inconsistent robustness evaluations across studies, providing a standardized platform for watermarking algorithm evaluation.", "method": "Proposes UWbench framework with statistical metrics for distribution drift, impossibility theorem proof, formal robustness analysis, and a three-axis evaluation protocol (unbiasedness, detectability, robustness).", "result": "Shows that token modification attacks provide more stable robustness assessments than paraphrasing methods, and proves no unbiased watermark can perfectly preserve distribution under infinite queries.", "conclusion": "UWbench offers a standardized, reproducible platform for advancing the design and evaluation of unbiased watermarking algorithms, addressing key limitations in current evaluation practices."}}
{"id": "2509.23043", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.23043", "abs": "https://arxiv.org/abs/2509.23043", "authors": ["Saleh Bunaiyan", "Corentin Delacour", "Shuvro Chowdhury", "Kyle Lee", "Kerem Y. Camsari"], "title": "IsingFormer: Augmenting Parallel Tempering With Learned Proposals", "comment": "SB, CD, SC, KL are equally contributing authors", "summary": "Markov Chain Monte Carlo (MCMC) underlies both statistical physics and\ncombinatorial optimization, but mixes slowly near critical points and in rough\nlandscapes. Parallel Tempering (PT) improves mixing by swapping replicas across\ntemperatures, yet each replica still relies on slow local updates to change its\nconfiguration. We introduce IsingFormer, a Transformer trained on equilibrium\nsamples that can generate entire spin configurations resembling those from the\ntarget distribution. These uncorrelated samples are used as proposals for\nglobal moves within a Metropolis step in PT, complementing the usual\nsingle-spin flips. On 2D Ising models (sampling), IsingFormer reproduces\nmagnetization and free-energy curves and generalizes to unseen temperatures,\nincluding the critical region. Injecting even a single proposal sharply reduces\nequilibration time, replacing thousands of local updates. On 3D spin glasses\n(optimization), PT enhanced with IsingFormer finds substantially lower-energy\nstates, demonstrating how global moves accelerate search in rugged landscapes.\nFinally, applied to integer factorization encoded as Ising problems,\nIsingFormer trained on a limited set of semiprimes transfers successfully to\nunseen semiprimes, boosting success rates beyond the training distribution.\nSince factorization is a canonical hard benchmark, this ability to generalize\nacross instances highlights the potential of learning proposals that move\nbeyond single problems to entire families of instances. The IsingFormer\ndemonstrates that Monte Carlo methods can be systematically accelerated by\nneural proposals that capture global structure, yielding faster sampling and\nstronger performance in combinatorial optimization.", "AI": {"tldr": "IsingFormer is a Transformer-based neural network that generates uncorrelated spin configurations as global proposals for Parallel Tempering, accelerating both sampling in 2D Ising models and optimization in 3D spin glasses and factorization problems.", "motivation": "Traditional MCMC methods like Parallel Tempering mix slowly near critical points and in rough landscapes due to reliance on local updates. The goal is to improve mixing by introducing global moves that can jump across energy barriers.", "method": "Train a Transformer on equilibrium samples to generate entire spin configurations resembling the target distribution. Use these as proposals for global moves within Metropolis steps in Parallel Tempering, complementing single-spin flips.", "result": "On 2D Ising models: reproduces magnetization and free-energy curves, generalizes to unseen temperatures including critical region, sharply reduces equilibration time. On 3D spin glasses: finds substantially lower-energy states. On factorization: transfers successfully to unseen semiprimes, boosting success rates.", "conclusion": "Monte Carlo methods can be systematically accelerated by neural proposals that capture global structure, yielding faster sampling and stronger performance in combinatorial optimization across entire families of problem instances."}}
{"id": "2509.23184", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23184", "abs": "https://arxiv.org/abs/2509.23184", "authors": ["Boyi Zeng", "He Li", "Shixiang Song", "Yixuan Wang", "Ziwei He", "Xinbing Wang", "Zhouhan Lin"], "title": "Pretraining LLM with Latent Thoughts in Continuous Space", "comment": null, "summary": "The remarkable success of Chain-of-Thought (CoT), which enhances performance\nby scaling generation steps at test-time, inspires us to ask: can we leverage a\nsimilar scaling of computational steps during pretraining to improve the\ngeneration of each individual token? To address this, we propose a novel\npre-training methodology: Pretraining Language Models with Latent Thoughts. Our\napproach pretrains a language model (LM) to first generate an intermediate\nlatent thought-the last hidden state of the current position-which is then used\nas input to predict the actual subsequent token. This additional computational\nstep enables the LM to refine its prediction within unconstrained continuous\nspace. Our experiments demonstrate that, at an identical inference cost, a LM\nthat generates one additional latent thought per token outperforms a standard\nmodel with double the parameters. For instance, ours-1.4B (Pythia Arch),\npretrained on 300B tokens from the Pile, significantly surpasses the vanilla\nPythia-2.8B trained on the same data on both language modeling and a range of\ngeneral downstream tasks. Furthermore, increasing the number of latent thoughts\ngenerated before each actual token-forming a chain analogous to\nCoT-consistently improves the model's performance.", "AI": {"tldr": "Pretraining Language Models with Latent Thoughts - a method that adds intermediate computational steps during pretraining to improve token generation, outperforming standard models with double the parameters.", "motivation": "Inspired by Chain-of-Thought's success at test-time, the authors aim to leverage similar computational step scaling during pretraining to enhance individual token generation.", "method": "Pretrains language models to first generate intermediate latent thoughts (last hidden states) which are then used as input to predict subsequent tokens, enabling refinement in continuous space.", "result": "A 1.4B model with latent thoughts outperforms vanilla 2.8B model on same data for both language modeling and downstream tasks. Performance improves consistently with more latent thoughts.", "conclusion": "Adding latent thought generation during pretraining significantly enhances model performance without increasing inference cost, demonstrating the value of intermediate computational steps."}}
{"id": "2509.23465", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23465", "abs": "https://arxiv.org/abs/2509.23465", "authors": ["Zhuoli Yin", "Yi Ding", "Reem Khir", "Hua Cai"], "title": "ViTSP: A Vision Language Models Guided Framework for Large-Scale Traveling Salesman Problems", "comment": null, "summary": "Solving Traveling Salesman Problem (TSP) is NP-hard yet fundamental for wide\nreal-world applications. Classical exact methods face challenges in scaling,\nand heuristic methods often require domain-specific parameter calibration.\nWhile learning-based approaches have shown promise, they suffer from poor\ngeneralization and limited scalability due to fixed training data. This work\nproposes ViTSP, a novel framework that leverages pre-trained vision language\nmodels (VLMs) to visually guide the solution process for large-scale TSPs. The\nVLMs function to identify promising small-scale subproblems from a visualized\nTSP instance, which are then efficiently optimized using an off-the-shelf\nsolver to improve the global solution. ViTSP bypasses the dedicated model\ntraining at the user end while maintaining effectiveness across diverse\ninstances. Experiments on real-world TSP instances ranging from 1k to 88k nodes\ndemonstrate that ViTSP consistently achieves solutions with average optimality\ngaps below 0.2%, outperforming existing learning-based methods. Under the same\nruntime budget, it surpasses the best-performing heuristic solver, LKH-3, by\nreducing its gaps by 12% to 100%, particularly on very-large-scale instances\nwith more than 10k nodes. Our framework offers a new perspective in hybridizing\npre-trained generative models and operations research solvers in solving\ncombinatorial optimization problems, with practical implications for\nintegration into more complex logistics systems. The code is available at\nhttps://anonymous.4open.science/r/ViTSP_codes-6683.", "AI": {"tldr": "ViTSP is a novel framework that uses pre-trained vision language models to visually guide solving large-scale Traveling Salesman Problems by identifying promising subproblems, achieving optimality gaps below 0.2% on instances up to 88k nodes.", "motivation": "Traditional TSP solving methods face scalability issues and require domain-specific parameter tuning, while learning-based approaches suffer from poor generalization and limited scalability due to fixed training data.", "method": "Leverages pre-trained vision language models to identify promising small-scale subproblems from visualized TSP instances, then optimizes these subproblems using off-the-shelf solvers to improve global solutions without dedicated training.", "result": "Achieves average optimality gaps below 0.2% on real-world TSP instances ranging from 1k to 88k nodes, outperforming existing learning-based methods and reducing LKH-3's gaps by 12% to 100% on instances with more than 10k nodes.", "conclusion": "ViTSP provides a new perspective for hybridizing pre-trained generative models with operations research solvers, offering practical integration potential for complex logistics systems without requiring dedicated model training."}}
{"id": "2509.24153", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24153", "abs": "https://arxiv.org/abs/2509.24153", "authors": ["Philip Sj\u00f6sv\u00e4rd", "Hongyu Jin", "Panos Papadimitratos"], "title": "DNS in the Time of Curiosity: A Tale of Collaborative User Privacy Protection", "comment": "Twenty-ninth International Workshop on Security Protocols", "summary": "The Domain Name System (DNS) is central to all Internet user activity,\nresolving accessed domain names into Internet Protocol (IP) addresses. As a\nresult, curious DNS resolvers can learn everything about Internet users'\ninterests. Public DNS resolvers are rising in popularity, offering low-latency\nresolution, high reliability, privacy-preserving policies, and support for\nencrypted DNS queries. However, client-resolver traffic encryption,\nincreasingly deployed to protect users from eavesdroppers, does not protect\nusers against curious resolvers. Similarly, privacy-preserving policies are\nbased solely on written commitments and do not provide technical safeguards.\nAlthough DNS query relay schemes can separate duties to limit data accessible\nby each entity, they cannot prevent colluding entities from sharing user\ntraffic logs. Thus, a key challenge remains: organizations operating public DNS\nresolvers, accounting for the majority of DNS resolutions, can potentially\ncollect and analyze massive volumes of Internet user activity data. With DNS\ninfrastructure that cannot be fully trusted, can we safeguard user privacy? We\nanswer positively and advocate for a user-driven approach to reduce exposure to\nDNS services. We will discuss key ideas of the proposal, which aims to achieve\na high level of privacy without sacrificing performance: maintaining low\nlatency, network bandwidth, memory/storage overhead, and computational\noverhead.", "AI": {"tldr": "DNS privacy is threatened by curious resolvers that can track user activity. Encryption and policies don't prevent data collection. The paper proposes a user-driven approach to protect privacy without sacrificing performance.", "motivation": "Public DNS resolvers can collect massive user activity data despite encryption and privacy policies. Current solutions can't prevent collusion between entities, creating a need for technical safeguards.", "method": "A user-driven approach that reduces exposure to DNS services while maintaining low latency, bandwidth, memory/storage, and computational overhead.", "result": "The paper demonstrates that privacy protection in DNS is achievable without performance degradation through proper architectural design.", "conclusion": "User privacy in DNS can be safeguarded through a carefully designed user-driven approach that balances privacy protection with system performance requirements."}}
{"id": "2509.23049", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23049", "abs": "https://arxiv.org/abs/2509.23049", "authors": ["Zijian Wang", "Xiaofei Zhang", "Xin Zhang", "Yukun Liu", "Qiong Zhang"], "title": "Beyond Aggregation: Guiding Clients in Heterogeneous Federated Learning", "comment": null, "summary": "Federated learning (FL) is increasingly adopted in domains like healthcare,\nwhere data privacy is paramount. A fundamental challenge in these systems is\nstatistical heterogeneity-the fact that data distributions vary significantly\nacross clients (e.g., different hospitals may treat distinct patient\ndemographics). While current FL algorithms focus on aggregating model updates\nfrom these heterogeneous clients, the potential of the central server remains\nunder-explored. This paper is motivated by a healthcare scenario: could a\ncentral server not only build a model but also guide a new patient to the\nhospital best equipped for their specific condition? We generalize this idea to\npropose a novel paradigm for FL systems where the server actively guides the\nallocation of new tasks or queries to the most appropriate client in the\nnetwork. To enable this, we introduce an empirical likelihood-based framework\nthat simultaneously addresses two goals: (1) learning effective local models on\neach client, and (2) finding the best matching client for a new query.\nEmpirical results demonstrate the framework's effectiveness on benchmark\ndatasets, showing improvements in both model accuracy and the precision of\nclient guidance compared to standard FL approaches. This work opens a new\ndirection for building more intelligent and resource-efficient federated\nsystems that leverage heterogeneity as a feature, not just a bug. Code is\navailable at https://github.com/zijianwang0510/FedDRM.git.", "AI": {"tldr": "This paper introduces a novel federated learning paradigm where the central server not only aggregates models but also actively guides new queries to the most appropriate clients based on their data distributions, using an empirical likelihood framework.", "motivation": "The paper is motivated by healthcare applications where data privacy is crucial and statistical heterogeneity across clients (hospitals) is common. The authors question whether the central server could do more than just build models - specifically, guide new patients to hospitals best equipped for their conditions.", "method": "The authors propose an empirical likelihood-based framework that simultaneously learns effective local models on each client and finds the best matching client for new queries. This enables the server to actively guide task allocation in the federated network.", "result": "Empirical results on benchmark datasets show improvements in both model accuracy and the precision of client guidance compared to standard federated learning approaches.", "conclusion": "This work opens a new direction for building more intelligent and resource-efficient federated systems that leverage heterogeneity as a feature rather than treating it as a problem."}}
{"id": "2509.23188", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23188", "abs": "https://arxiv.org/abs/2509.23188", "authors": ["Guancheng Wan", "Leixin Sun", "Longxu Dou", "Zitong Shi", "Fang Wu", "Eric Hanchen Jiang", "Wenke Huang", "Guibin Zhang", "Hejia Geng", "Xiangru Tang", "Zhenfei Yin", "Yizhou Sun", "Wei Wang"], "title": "Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts", "comment": null, "summary": "Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly\nadvanced collaborative reasoning, tool use, and role-specialized coordination\nin complex tasks. However, reliability-critical deployment remains hindered by\na systemic failure mode: hierarchical compliance under instruction conflicts\n(system-user, peer-peer), where agents misprioritize system-level rules in the\npresence of competing demands. Moreover, widely used macro-level metrics (e.g.,\npass@k) obscure these micro-level violations and offer little actionable\nguidance for remedy. In this work, we present a full-stack, three-stage\nframework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a\nquery-wise, context-aware scoring metric that decomposes role adherence into\nfour measurable dimensions; (2) Localize - attention drift analysis revealing\nthat instruction conflicts are resolved by attention heads that are largely\nconcentrated in middle layers; (3) Align - Surgical Alignment of Instruction\nLayers (SAIL), which installs LoRA only on the localized focal layers and\noptimizes a token-weighted DPO-style preference objective that credits tokens\nby their focal attentional contribution. Across standard benchmarks and MAS\nframeworks, our surgical approach improves instruction hierarchy compliance\n(e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.", "AI": {"tldr": "The paper addresses hierarchical compliance failures in LLM-powered multi-agent systems under instruction conflicts, proposing a three-stage framework (CRAS diagnosis, attention drift localization, SAIL surgical alignment) to improve reliability without full-model finetuning.", "motivation": "Reliability-critical deployment of LLM-powered multi-agent systems is hindered by systemic failure mode of hierarchical compliance under instruction conflicts, where agents misprioritize system-level rules. Current macro-level metrics obscure micro-level violations and offer little actionable guidance.", "method": "Three-stage framework: (1) Diagnose with Contextualized Role Adherence Score (CRAS) that decomposes role adherence into four measurable dimensions; (2) Localize via attention drift analysis revealing instruction conflicts are resolved by middle-layer attention heads; (3) Align using Surgical Alignment of Instruction Layers (SAIL) with LoRA on localized focal layers and token-weighted DPO-style preference objective.", "result": "Across standard benchmarks and MAS frameworks, the surgical approach improves instruction hierarchy compliance (e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.", "conclusion": "The proposed framework effectively addresses hierarchical compliance failures in multi-agent systems through targeted diagnosis, localization, and surgical alignment, enabling reliability-critical deployment without the computational cost of full-model finetuning."}}
{"id": "2509.23482", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23482", "abs": "https://arxiv.org/abs/2509.23482", "authors": ["Zhangyu Wang", "Nemin Wu", "Qian Cao", "Jiangnan Xia", "Zeping Liu", "Yiqun Xie", "Akshay Nambi", "Tanuja Ganu", "Ni Lao", "Ninghao Liu", "Gengchen Mai"], "title": "GeoBS: Information-Theoretic Quantification of Geographic Bias in AI Models", "comment": null, "summary": "The widespread adoption of AI models, especially foundation models (FMs), has\nmade a profound impact on numerous domains. However, it also raises significant\nethical concerns, including bias issues. Although numerous efforts have been\nmade to quantify and mitigate social bias in AI models, geographic bias (in\nshort, geo-bias) receives much less attention, which presents unique\nchallenges. While previous work has explored ways to quantify geo-bias, these\nmeasures are model-specific (e.g., mean absolute deviation of LLM ratings) or\nspatially implicit (e.g., average fairness scores of all spatial partitions).\nWe lack a model-agnostic, universally applicable, and spatially explicit\ngeo-bias evaluation framework that allows researchers to fairly compare the\ngeo-bias of different AI models and to understand what spatial factors\ncontribute to the geo-bias. In this paper, we establish an\ninformation-theoretic framework for geo-bias evaluation, called GeoBS (Geo-Bias\nScores). We demonstrate the generalizability of the proposed framework by\nshowing how to interpret and analyze existing geo-bias measures under this\nframework. Then, we propose three novel geo-bias scores that explicitly take\nintricate spatial factors (multi-scalability, distance decay, and anisotropy)\ninto consideration. Finally, we conduct extensive experiments on 3 tasks, 8\ndatasets, and 8 models to demonstrate that both task-specific GeoAI models and\ngeneral-purpose foundation models may suffer from various types of geo-bias.\nThis framework will not only advance the technical understanding of geographic\nbias but will also establish a foundation for integrating spatial fairness into\nthe design, deployment, and evaluation of AI systems.", "AI": {"tldr": "This paper introduces GeoBS, an information-theoretic framework for evaluating geographic bias in AI models, addressing limitations of previous model-specific and spatially implicit approaches.", "motivation": "Current geo-bias evaluation methods are model-specific or spatially implicit, lacking a universal framework for fair comparison across AI models and understanding spatial factors contributing to bias.", "method": "Proposed GeoBS framework with three novel bias scores that explicitly consider multi-scalability, distance decay, and anisotropy spatial factors, tested on 3 tasks, 8 datasets, and 8 models.", "result": "Experiments show both task-specific GeoAI models and general-purpose foundation models suffer from various types of geo-bias, demonstrating the framework's effectiveness.", "conclusion": "The GeoBS framework advances technical understanding of geographic bias and establishes foundation for integrating spatial fairness into AI system design, deployment, and evaluation."}}
{"id": "2509.24173", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.24173", "abs": "https://arxiv.org/abs/2509.24173", "authors": ["Sun-Moon Yoon", "Hyun-Young Park", "Seung-Hyun Nam", "Si-Hyeon Lee"], "title": "Fundamental Limit of Discrete Distribution Estimation under Utility-Optimized Local Differential Privacy", "comment": "20 pages, 7 figures, 1 table. This work has been submitted to the\n  IEEE for possible publication", "summary": "We study the problem of discrete distribution estimation under\nutility-optimized local differential privacy (ULDP), which enforces local\ndifferential privacy (LDP) on sensitive data while allowing more accurate\ninference on non-sensitive data. In this setting, we completely characterize\nthe fundamental privacy-utility trade-off. The converse proof builds on several\nkey ideas, including a generalized uniform asymptotic Cram\\'er-Rao lower bound,\na reduction showing that it suffices to consider a newly defined class of\nextremal ULDP mechanisms, and a novel distribution decomposition technique\ntailored to ULDP constraints. For the achievability, we propose a class of\nutility-optimized block design (uBD) schemes, obtained as nontrivial\nmodifications of the block design mechanism known to be optimal under standard\nLDP constraints, while incorporating the distribution decomposition idea used\nin the converse proof and a score-based linear estimator. These results provide\na tight characterization of the estimation accuracy achievable under ULDP and\nreveal new insights into the structure of optimal mechanisms for\nprivacy-preserving statistical inference.", "AI": {"tldr": "This paper provides a complete characterization of the fundamental privacy-utility trade-off for discrete distribution estimation under utility-optimized local differential privacy (ULDP), which allows more accurate inference on non-sensitive data while enforcing LDP on sensitive data.", "motivation": "To understand the fundamental limits of estimation accuracy under ULDP constraints, which extend standard LDP by allowing better utility for non-sensitive data while maintaining privacy for sensitive data.", "method": "The converse proof uses a generalized uniform asymptotic Cram\u00e9r-Rao lower bound, reduction to extremal ULDP mechanisms, and a novel distribution decomposition technique. For achievability, the authors propose utility-optimized block design (uBD) schemes that modify the optimal block design mechanism for standard LDP.", "result": "The paper establishes tight characterization of estimation accuracy achievable under ULDP constraints, showing the fundamental privacy-utility trade-off.", "conclusion": "The results provide a complete understanding of the privacy-utility trade-off in ULDP and reveal new insights into optimal mechanisms for privacy-preserving statistical inference."}}
{"id": "2509.23050", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23050", "abs": "https://arxiv.org/abs/2509.23050", "authors": ["Lin Long", "Changdae Oh", "Seongheon Park", "Yixuan Li"], "title": "Understanding Language Prior of LVLMs by Contrasting Chain-of-Embedding", "comment": null, "summary": "Large vision-language models (LVLMs) achieve strong performance on multimodal\ntasks, yet they often default to their language prior (LP) -- memorized textual\npatterns from pre-training while under-utilizing visual evidence. Prior\nanalyses of LP mostly rely on input-output probing, which fails to reveal the\ninternal mechanisms governing when and how vision influences model behavior. To\naddress this gap, we present the first systematic analysis of language prior\nthrough the lens of chain-of-embedding, which examines the layer-wise\nrepresentation dynamics within LVLMs. Our analysis reveals a universal\nphenomenon: each model exhibits a Visual Integration Point (VIP), a critical\nlayer at which visual information begins to meaningfully reshape hidden\nrepresentations and influence decoding. Building on this observation, we\nintroduce the Total Visual Integration (TVI) estimator, which aggregates\nrepresentation distance beyond the VIP to quantify how strongly visual query\ninfluences response generation. Across 54 model-dataset combinations spanning 9\ncontemporary LVLMs and 6 benchmarks, we demonstrate that VIP consistently\nemerges, and that TVI reliably predicts the strength of language prior. This\noffers a principled toolkit for diagnosing and understanding language prior in\nLVLMs.", "AI": {"tldr": "This paper presents a systematic analysis of language prior in large vision-language models through chain-of-embedding, revealing a universal Visual Integration Point (VIP) layer and introducing TVI estimator to quantify visual influence on responses.", "motivation": "Large vision-language models often default to their language prior from pre-training while under-utilizing visual evidence, but existing input-output probing methods fail to reveal the internal mechanisms of visual influence.", "method": "Analyze language prior through chain-of-embedding to examine layer-wise representation dynamics, identify Visual Integration Point (VIP) where visual information reshapes representations, and introduce Total Visual Integration (TVI) estimator to quantify visual influence.", "result": "Across 54 model-dataset combinations spanning 9 LVLMs and 6 benchmarks, VIP consistently emerges, and TVI reliably predicts the strength of language prior, offering a principled diagnostic toolkit.", "conclusion": "The analysis reveals universal VIP phenomenon and provides TVI as a reliable estimator for understanding and diagnosing language prior in large vision-language models."}}
{"id": "2509.23195", "categories": ["cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2509.23195", "abs": "https://arxiv.org/abs/2509.23195", "authors": ["Nan Wang", "Jiaxuan Li"], "title": "Estimating the strength and timing of syntactic structure building in naturalistic reading", "comment": null, "summary": "A central question in psycholinguistics is the timing of syntax in sentence\nprocessing. Much of the existing evidence comes from violation paradigms, which\nconflate two separable processes - syntactic category detection and phrase\nstructure construction - and implicitly assume that phrase structure follows\ncategory detection. In this study, we use co-registered EEG and eye-tracking\ndata from the ZuCo corpus to disentangle these processes and test their\ntemporal order under naturalistic reading conditions. Analyses of gaze\ntransitions showed that readers preferentially moved between syntactic heads,\nsuggesting that phrase structures, rather than serial word order, organize\nscanpaths. Bayesian network modeling further revealed that structural depth was\nthe strongest driver of deviations from linear reading, outweighing lexical\nfamiliarity and surprisal. Finally, fixation-related potentials demonstrated\nthat syntactic surprisal influences neural activity before word onset (-184 to\n-10 ms) and during early integration (48 to 300 ms). These findings extend\ncurrent models of syntactic timing by showing that phrase structure\nconstruction can precede category detection and dominate lexical influences,\nsupporting a predictive \"tree-scaffolding\" account of comprehension.", "AI": {"tldr": "This study uses EEG and eye-tracking data to show that phrase structure construction precedes syntactic category detection during natural reading, challenging traditional violation paradigms.", "motivation": "To disentangle syntactic category detection and phrase structure construction processes in sentence processing, which are conflated in traditional violation paradigms.", "method": "Used co-registered EEG and eye-tracking data from the ZuCo corpus, analyzed gaze transitions, applied Bayesian network modeling, and examined fixation-related potentials during naturalistic reading.", "result": "Readers preferentially moved between syntactic heads; structural depth was the strongest driver of reading deviations; syntactic surprisal influenced neural activity before word onset and during early integration.", "conclusion": "Phrase structure construction can precede category detection and dominate lexical influences, supporting a predictive \"tree-scaffolding\" account of comprehension."}}
{"id": "2509.23484", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23484", "abs": "https://arxiv.org/abs/2509.23484", "authors": ["Tom Quilter", "Anastasia Ilick", "Anastasia Ilick", "Richard Turner"], "title": "Accurate Predictions in Education with Discrete Variational Inference", "comment": null, "summary": "One of the largest drivers of social inequality is unequal access to personal\ntutoring, with wealthier individuals able to afford it, while the majority\ncannot. Affordable, effective AI tutors offer a scalable solution. We focus on\nadaptive learning, predicting whether a student will answer a question\ncorrectly, a key component of any effective tutoring system. Yet many platforms\nstruggle to achieve high prediction accuracy, especially in data-sparse\nsettings. To address this, we release the largest open dataset of\nprofessionally marked formal mathematics exam responses to date. We introduce a\nprobabilistic modelling framework rooted in Item Response Theory (IRT) that\nachieves over 80 percent accuracy, setting a new benchmark for mathematics\nprediction accuracy of formal exam papers. Extending this, our collaborative\nfiltering models incorporate topic-level skill profiles, but reveal a\nsurprising and educationally significant finding, a single latent ability\nparameter alone is needed to achieve the maximum predictive accuracy. Our main\ncontribution though is deriving and implementing a novel discrete variational\ninference framework, achieving our highest prediction accuracy in low-data\nsettings and outperforming all classical IRT and matrix factorisation\nbaselines.", "AI": {"tldr": "The paper introduces a novel discrete variational inference framework for predicting student performance on mathematics exams, achieving over 80% accuracy and outperforming traditional IRT and matrix factorization methods, especially in low-data settings.", "motivation": "To address social inequality in education by developing affordable AI tutors through improved prediction of student performance, particularly in data-sparse environments where many platforms struggle with accuracy.", "method": "Released the largest open dataset of professionally marked mathematics exam responses; developed probabilistic modeling framework based on Item Response Theory (IRT); extended with collaborative filtering models incorporating topic-level skill profiles; introduced novel discrete variational inference framework.", "result": "Achieved over 80% accuracy in mathematics prediction of formal exam papers; found that a single latent ability parameter alone achieves maximum predictive accuracy; discrete variational inference framework outperformed all classical IRT and matrix factorization baselines, especially in low-data settings.", "conclusion": "The discrete variational inference framework represents the main contribution, providing the highest prediction accuracy in challenging low-data scenarios and establishing a new benchmark for mathematics prediction accuracy in formal exam settings."}}
{"id": "2509.24174", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24174", "abs": "https://arxiv.org/abs/2509.24174", "authors": ["Philip Sj\u00f6sv\u00e4rd", "Hongyu Jin", "Panos Papadimitratos"], "title": "LLUAD: Low-Latency User-Anonymized DNS", "comment": "24th Workshop on Privacy in the Electronic Society", "summary": "The Domain Name System (DNS) is involved in practically all web activity,\ntranslating easy-to-remember domain names into Internet Protocol (IP)\naddresses. Due to its central role on the Internet, DNS exposes user web\nactivity in detail. The privacy challenge is honest-but-curious DNS\nservers/resolvers providing the translation/lookup service. In particular, with\nthe majority of DNS queries handled by public DNS resolvers, the organizations\nrunning them can track, collect, and analyze massive user activity data.\nExisting solutions that encrypt DNS traffic between clients and resolvers are\ninsufficient, as the resolver itself is the privacy threat. While DNS query\nrelays separate duties among multiple entities, to limit the data accessible by\neach entity, they cannot prevent colluding entities from sharing user traffic\nlogs. To achieve near-zero-trust DNS privacy compatible with the existing DNS\ninfrastructure, we propose LLUAD: it locally stores a Popularity List, the most\npopular DNS records, on user devices, formed in a privacy-preserving manner\nbased on user interests. In this way, LLUAD can both improve privacy and reduce\naccess times to web content. The Popularity List is proactively retrieved from\na (curious) public server that continually updates and refreshes the records\nbased on user popularity votes, while efficiently broadcasting record\nupdates/changes to adhere to aggressive load-balancing schemes (i.e., name\nservers actively load-balancing user connections by changing record IP\naddresses). User votes are anonymized using a novel, efficient, and highly\nscalable client-driven Voting Mix Network - with packet lengths independent of\nthe number of hops, centrally enforced limit on number of votes cast per user,\nand robustness against poor client participation - to ensure a geographically\nrelevant and correctly/securely instantiated Popularity List.", "AI": {"tldr": "LLUAD is a DNS privacy solution that locally stores popular DNS records on user devices using a privacy-preserving Popularity List, reducing reliance on curious DNS resolvers while improving access times.", "motivation": "DNS exposes user web activity to honest-but-curious DNS servers/resolvers, especially public DNS providers who can track and analyze massive user data. Existing encrypted DNS solutions don't address the resolver itself being the privacy threat.", "method": "LLUAD uses a locally stored Popularity List on user devices, formed through a client-driven Voting Mix Network that anonymizes user votes. The list is proactively retrieved from public servers and updated based on user popularity votes while maintaining privacy.", "result": "The system achieves near-zero-trust DNS privacy compatible with existing infrastructure, improves privacy by reducing resolver exposure, and reduces web content access times through local caching.", "conclusion": "LLUAD provides an effective DNS privacy solution that addresses the fundamental limitation of resolver-based threats while maintaining compatibility with existing DNS infrastructure and improving performance."}}
{"id": "2509.23052", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23052", "abs": "https://arxiv.org/abs/2509.23052", "authors": ["Matt L. Sampson", "Peter Melchior"], "title": "Dynamics of Learning: Generative Schedules from Latent ODEs", "comment": "9 pages, 5 figures, comments welcome", "summary": "The learning rate schedule is one of the most impactful aspects of neural\nnetwork optimization, yet most schedules either follow simple parametric\nfunctions or react only to short-term training signals. None of them are\nsupported by a comprehensive temporal view of how well neural networks actually\ntrain. We present a new learning rate scheduler that models the training\nperformance of neural networks as a dynamical system. It leverages training\nruns from a hyperparameter search to learn a latent representation of the\ntraining process. Given current training metrics, it predicts the future\nlearning rate schedule with the best long-term validation performance. Our\nscheduler generalizes beyond previously observed training dynamics and creates\nspecialized schedules that deviate noticeably from common parametric functions.\nIt achieves SOTA results for image classification with CNN and ResNet models as\nwell as for next-token prediction with a transformer model. The trained models\nare located in flatter regions of the loss landscape and thus provide better\ngeneralization than those trained with other schedules. Our method is\ncomputationally efficient, optimizer-agnostic, and can easily be layered on top\nof ML experiment-tracking platforms. An implementation of our scheduler will be\nmade available after acceptance.", "AI": {"tldr": "A new learning rate scheduler that models neural network training as a dynamical system, using hyperparameter search data to predict optimal long-term schedules, achieving SOTA results across CNN, ResNet, and transformer models.", "motivation": "Existing learning rate schedules are either simple parametric functions or reactive to short-term signals, lacking a comprehensive temporal view of neural network training performance.", "method": "Learns latent representations from hyperparameter search training runs, models training as dynamical system, predicts future learning rate schedules based on current metrics for optimal long-term validation performance.", "result": "Achieves state-of-the-art results for image classification (CNN/ResNet) and next-token prediction (transformer), produces models in flatter loss landscape regions with better generalization.", "conclusion": "The scheduler is computationally efficient, optimizer-agnostic, easily integrable with ML experiment-tracking platforms, and generalizes beyond observed training dynamics to create specialized schedules."}}
{"id": "2509.23196", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23196", "abs": "https://arxiv.org/abs/2509.23196", "authors": ["Haonan Wang", "Weida Liang", "Zihang Fu", "Nie Zheng", "Yifan Zhang", "Yao Tong", "Tongyao Zhu", "Hao Jiang", "Chuang Li", "Jiaying Wu", "Kenji Kawaguchi"], "title": "From Harm to Help: Turning Reasoning In-Context Demos into Assets for Reasoning LMs", "comment": null, "summary": "Recent reasoning LLMs (RLMs), especially those trained with verifier-based\nreinforcement learning, often perform worse with few-shot CoT than with direct\nanswering. We revisit this paradox using high-quality reasoning traces from\nDeepSeek-R1 as demonstrations and find that adding more exemplars consistently\ndegrades accuracy, even when demonstrations are optimal. A detailed analysis\nreveals two mechanisms behind this decline: (i) semantic misguidance, where\nhigh textual similarity leads the model to treat the target as the same as the\nexemplar and to copy intermediate steps verbatim; and (ii) strategy transfer\nfailure, where the model struggles to extract useful reasoning strategies and\napply them to target questions. Guided by these, we introduce Insight-to-Solve\n(I2S), a sequential test-time procedure that turns demonstrations into\nexplicit, reusable insights and derives a target-specific reasoning trace;\noptionally, the reasoning is self-refined for coherence and correctness (I2S+).\nExtensive experiments on diverse benchmarks show that I2S and I2S+ consistently\noutperform both direct answering and test-time scaling baselines across open-\nand closed-source models. Even for GPT models, our method helps: on AIME'25,\nGPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on\nGPQA, indicating that in-context demonstrations can be harnessed effectively\nvia insight-refine-solve framework.", "AI": {"tldr": "The paper addresses the paradox where reasoning LLMs perform worse with few-shot CoT than direct answering, and introduces Insight-to-Solve (I2S) framework to effectively utilize demonstrations by converting them into reusable insights and generating target-specific reasoning traces.", "motivation": "Recent reasoning LLMs trained with verifier-based reinforcement learning often show degraded performance with few-shot Chain-of-Thought (CoT) compared to direct answering, creating a paradox that needs investigation and solution.", "method": "Introduces Insight-to-Solve (I2S), a sequential test-time procedure that converts demonstrations into explicit, reusable insights and derives target-specific reasoning traces, with an optional self-refinement step (I2S+) for coherence and correctness.", "result": "Extensive experiments show I2S and I2S+ consistently outperform both direct answering and test-time scaling baselines across diverse benchmarks and models. GPT-4.1 improves by +14.0% on AIME'25, and o1-mini improves by +2.7% on AIME and +1.7% on GPQA.", "conclusion": "The insight-refine-solve framework effectively harnesses in-context demonstrations, overcoming the limitations of traditional few-shot CoT by addressing semantic misguidance and strategy transfer failure through explicit insight extraction and target-specific reasoning generation."}}
{"id": "2509.23488", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23488", "abs": "https://arxiv.org/abs/2509.23488", "authors": ["Siyang Wu", "Honglin Bao", "Sida Li", "Ari Holtzman", "James A. Evans"], "title": "Mapping Overlaps in Benchmarks through Perplexity in the Wild", "comment": null, "summary": "We develop signatures of capacity familiarity to characterize large language\nmodel (LLM) benchmarks and their meaningful overlaps. Benchmark signatures\nprobe the capacity required for benchmark performance. We formally define them\nas a set of salient tokens drawn from in-the-wild, naturally authored corpora,\nwhere LLM token perplexity, reflecting more or less pre-training exposure,\nbecomes highly predictive of LLM benchmark performance. Through a large-scale\nmeta-evaluation, we extract benchmark signatures via stepwise forward selection\nwith linear regressions across 32 LLMs and 88 benchmarks spanning diverse\nknowledge, coding, logic, instruction following, math, language, reasoning, and\nworld modeling. Our analysis situates signatures in relation to both the\nsemantic similarity of benchmark questions and the correlation of model\nperformance. While performance overlaps are universally high and semantic\noverlaps remain confined to a narrow mid-range, benchmark signatures prove\nhighly informative in capturing variation, overlap, and divergence. We observe\noverlap in knowledge and reasoning subtasks, whereas multilingual and cultural\nbenchmarks exhibit less similarity, even compared to cross-task overlap.\nNotably, performance-level results are strongly influenced by\nbenchmark-orthogonal factors such as question format, highlighting limitations\nin LLM generalization, the conflation of performance with ability, and issues\ninherent in current mainstream benchmark agreement studies. Benchmark\nsignatures, however, remain robust to such effects. Ultimately, we identify\ncross-functional overlaps across logic, math, language, instruction following,\nand world modeling, with coding emerging as the least overlapping domain.\nTogether, these findings provide mechanistic insights into benchmark validity\nand LLM sensitivities, and sketch the underlying landscape of interconnected\nLLM capabilities.", "AI": {"tldr": "The paper introduces 'benchmark signatures' - sets of salient tokens from natural corpora that predict LLM benchmark performance through token perplexity. Through analysis of 32 LLMs and 88 benchmarks, it reveals meaningful overlaps in capabilities while highlighting limitations of current benchmark evaluation methods.", "motivation": "To better understand the relationships between different LLM benchmarks and address limitations in current evaluation approaches, particularly the conflation of performance with ability and the influence of benchmark-orthogonal factors.", "method": "Extracted benchmark signatures via stepwise forward selection with linear regressions across 32 LLMs and 88 diverse benchmarks, using token perplexity from naturally authored corpora as predictive features.", "result": "Found high performance overlaps across benchmarks but limited semantic similarity. Identified cross-functional overlaps in logic, math, language, instruction following, and world modeling, with coding as the least overlapping domain. Benchmark signatures remained robust to format effects that influence performance metrics.", "conclusion": "Benchmark signatures provide mechanistic insights into LLM capabilities and benchmark validity, revealing an interconnected landscape of LLM abilities while highlighting the need to separate true capability from format-dependent performance."}}
{"id": "2509.24240", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24240", "abs": "https://arxiv.org/abs/2509.24240", "authors": ["Eunkyu Lee", "Donghyeon Kim", "Wonyoung Kim", "Insu Yun"], "title": "Takedown: How It's Done in Modern Coding Agent Exploits", "comment": null, "summary": "Coding agents, which are LLM-driven agents specialized in software\ndevelopment, have become increasingly prevalent in modern programming\nenvironments. Unlike traditional AI coding assistants, which offer simple code\ncompletion and suggestions, modern coding agents tackle more complex tasks with\ngreater autonomy, such as generating entire programs from natural language\ninstructions. To enable such capabilities, modern coding agents incorporate\nextensive functionalities, which in turn raise significant concerns over their\nsecurity and privacy. Despite their growing adoption, systematic and in-depth\nsecurity analysis of these agents has largely been overlooked.\n  In this paper, we present a comprehensive security analysis of eight\nreal-world coding agents. Our analysis addresses the limitations of prior\napproaches, which were often fragmented and ad hoc, by systematically examining\nthe internal workflows of coding agents and identifying security threats across\ntheir components. Through the analysis, we identify 15 security issues,\nincluding previously overlooked or missed issues, that can be abused to\ncompromise the confidentiality and integrity of user systems. Furthermore, we\nshow that these security issues are not merely individual vulnerabilities, but\ncan collectively lead to end-to-end exploitations. By leveraging these security\nissues, we successfully achieved arbitrary command execution in five agents and\nglobal data exfiltration in four agents, all without any user interaction or\napproval. Our findings highlight the need for a comprehensive security analysis\nin modern LLM-driven agents and demonstrate how insufficient security\nconsiderations can lead to severe vulnerabilities.", "AI": {"tldr": "Comprehensive security analysis of 8 real-world coding agents reveals 15 security issues that can compromise user systems, enabling arbitrary command execution and global data exfiltration without user interaction.", "motivation": "Modern LLM-driven coding agents with extensive autonomy raise significant security and privacy concerns, but systematic security analysis has been largely overlooked despite their growing adoption.", "method": "Systematically examined internal workflows of coding agents to identify security threats across components, addressing limitations of prior fragmented approaches.", "result": "Identified 15 security issues that collectively enable end-to-end exploitations; achieved arbitrary command execution in 5 agents and global data exfiltration in 4 agents without user interaction.", "conclusion": "Highlights the need for comprehensive security analysis in modern LLM-driven agents and demonstrates how insufficient security considerations lead to severe vulnerabilities."}}
{"id": "2509.23074", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23074", "abs": "https://arxiv.org/abs/2509.23074", "authors": ["Wanjin Feng", "Yuan Yuan", "Jingtao Ding", "Yong Li"], "title": "Beyond Model Ranking: Predictability-Aligned Evaluation for Time Series Forecasting", "comment": null, "summary": "In the era of increasingly complex AI models for time series forecasting,\nprogress is often measured by marginal improvements on benchmark leaderboards.\nHowever, this approach suffers from a fundamental flaw: standard evaluation\nmetrics conflate a model's performance with the data's intrinsic\nunpredictability. To address this pressing challenge, we introduce a novel,\npredictability-aligned diagnostic framework grounded in spectral coherence. Our\nframework makes two primary contributions: the Spectral Coherence\nPredictability (SCP), a computationally efficient ($O(N\\log N)$) and\ntask-aligned score that quantifies the inherent difficulty of a given\nforecasting instance, and the Linear Utilization Ratio (LUR), a\nfrequency-resolved diagnostic tool that precisely measures how effectively a\nmodel exploits the linearly predictable information within the data. We\nvalidate our framework's effectiveness and leverage it to reveal two core\ninsights. First, we provide the first systematic evidence of \"predictability\ndrift\", demonstrating that a task's forecasting difficulty varies sharply over\ntime. Second, our evaluation reveals a key architectural trade-off: complex\nmodels are superior for low-predictability data, whereas linear models are\nhighly effective on more predictable tasks. We advocate for a paradigm shift,\nmoving beyond simplistic aggregate scores toward a more insightful,\npredictability-aware evaluation that fosters fairer model comparisons and a\ndeeper understanding of model behavior.", "AI": {"tldr": "A novel predictability-aligned diagnostic framework using spectral coherence to address flaws in standard time series forecasting evaluation metrics, introducing SCP to measure data unpredictability and LUR to assess model efficiency.", "motivation": "Standard evaluation metrics conflate model performance with data's intrinsic unpredictability, leading to unfair model comparisons and limited understanding of true model capabilities.", "method": "Developed Spectral Coherence Predictability (SCP) score to quantify forecasting difficulty and Linear Utilization Ratio (LUR) to measure how effectively models exploit linearly predictable information.", "result": "Revealed 'predictability drift' showing forecasting difficulty varies over time, and identified architectural trade-off: complex models excel on low-predictability data while linear models perform well on predictable tasks.", "conclusion": "Advocates for paradigm shift from simplistic aggregate scores to predictability-aware evaluation for fairer model comparisons and deeper understanding of model behavior."}}
{"id": "2509.23197", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23197", "abs": "https://arxiv.org/abs/2509.23197", "authors": ["Aditya Narayan Sankaran", "Reza Farahbakhsh", "Noel Crespi"], "title": "Global Beats, Local Tongue: Studying Code Switching in K-pop Hits on Billboard Charts", "comment": "10 pages, accepted to appear at Sixth Conference On Computational\n  Humanities Research (CHR 2025)", "summary": "Code switching, particularly between Korean and English, has become a\ndefining feature of modern K-pop, reflecting both aesthetic choices and global\nmarket strategies. This paper is a primary investigation into the linguistic\nstrategies employed in K-pop songs that achieve global chart success, with a\nfocus on the role of code-switching and English lyric usage. A dataset of K-pop\nsongs that appeared on the Billboard Hot 100 and Global 200 charts from 2017 to\n2025, spanning 14 groups and 8 solo artists, was compiled. Using this dataset,\nthe proportion of English and Korean lyrics, the frequency of code-switching,\nand other stylistic features were analysed. It was found that English dominates\nthe linguistic landscape of globally charting K-pop songs, with both male and\nfemale performers exhibiting high degrees of code-switching and English usage.\nStatistical tests indicated no significant gender-based differences, although\nfemale solo artists tend to favour English more consistently. A classification\ntask was also performed to predict performer gender from lyrics, achieving\nmacro F1 scores up to 0.76 using multilingual embeddings and handcrafted\nfeatures. Finally, differences between songs charting on the Hot 100 versus the\nGlobal 200 were examined, suggesting that, while there is no significant gender\ndifference in English, higher English usage may be more critical for success in\nthe US-focused Hot 100. The findings highlight how linguistic choices in K-pop\nlyrics are shaped by global market pressures and reveal stylistic patterns that\nreflect performer identity and chart context.", "AI": {"tldr": "Analysis of code-switching patterns in globally charting K-pop songs shows English dominance and high code-switching frequency, with no significant gender differences but female solo artists using more English.", "motivation": "To investigate linguistic strategies in K-pop songs achieving global chart success, focusing on code-switching and English usage patterns.", "method": "Compiled dataset of K-pop songs on Billboard Hot 100 and Global 200 charts (2017-2025), analyzed English/Korean proportions, code-switching frequency, performed statistical tests and gender classification using multilingual embeddings.", "result": "English dominates globally charting K-pop songs; high code-switching observed; no significant gender differences; female solo artists use more English; gender classification achieved F1=0.76; higher English usage may be more critical for US chart success.", "conclusion": "Linguistic choices in K-pop lyrics are shaped by global market pressures, revealing stylistic patterns reflecting performer identity and chart context."}}
{"id": "2509.23497", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23497", "abs": "https://arxiv.org/abs/2509.23497", "authors": ["Bruno M. Henrique", "Eugene Santos Jr"], "title": "Dynamic Trust Calibration Using Contextual Bandits", "comment": null, "summary": "Trust calibration between humans and Artificial Intelligence (AI) is crucial\nfor optimal decision-making in collaborative settings. Excessive trust can lead\nusers to accept AI-generated outputs without question, overlooking critical\nflaws, while insufficient trust may result in disregarding valuable insights\nfrom AI systems, hindering performance. Despite its importance, there is\ncurrently no definitive and objective method for measuring trust calibration\nbetween humans and AI. Current approaches lack standardization and consistent\nmetrics that can be broadly applied across various contexts, and they don't\ndistinguish between the formation of opinions and subsequent human decisions.\nIn this work, we propose a novel and objective method for dynamic trust\ncalibration, introducing a standardized trust calibration measure and an\nindicator. By utilizing Contextual Bandits-an adaptive algorithm that\nincorporates context into decision-making-our indicator dynamically assesses\nwhen to trust AI contributions based on learned contextual information. We\nevaluate this indicator across three diverse datasets, demonstrating that\neffective trust calibration results in significant improvements in\ndecision-making performance, as evidenced by 10 to 38% increase in reward\nmetrics. These findings not only enhance theoretical understanding but also\nprovide practical guidance for developing more trustworthy AI systems\nsupporting decisions in critical domains, for example, disease diagnoses and\ncriminal justice.", "AI": {"tldr": "Proposes a novel objective method for dynamic trust calibration between humans and AI using Contextual Bandits, showing 10-38% improvement in decision-making performance across diverse datasets.", "motivation": "Current methods lack standardization and consistent metrics for measuring trust calibration, failing to distinguish between opinion formation and human decisions, which is crucial for optimal human-AI collaboration.", "method": "Uses Contextual Bandits algorithm to create a standardized trust calibration measure and indicator that dynamically assesses when to trust AI contributions based on learned contextual information.", "result": "Evaluation across three diverse datasets demonstrated that effective trust calibration leads to significant improvements in decision-making performance, with 10-38% increase in reward metrics.", "conclusion": "The proposed method enhances theoretical understanding and provides practical guidance for developing more trustworthy AI systems in critical domains like disease diagnosis and criminal justice."}}
{"id": "2509.24257", "categories": ["cs.CR", "cs.LG", "C.2.1"], "pdf": "https://arxiv.org/pdf/2509.24257", "abs": "https://arxiv.org/abs/2509.24257", "authors": ["Ke Wang", "Felix Qu", "Libin Xia", "Zishuo Zhao", "Chris Tong", "Lynn Ai", "Eric Yang"], "title": "VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference", "comment": "13 pages, 4 figures, 2 tables", "summary": "Decentralized inference is an appealing paradigm for serving large language\nmodels (LLMs), offering strong security, high efficiency, and lower operating\ncosts. Yet the permissionless setting admits no a priori trust in participating\nnodes, making output verifiability a prerequisite for secure deployment. We\npresent VeriLLM, a publicly verifiable protocol for decentralized LLM inference\nthat (i) achieves security under a one-honest-verifier assumption, (ii) attains\nnear-negligible verification cost (about 1% of the underlying inference) via a\nlightweight verification algorithm designed explicitly for LLMs, and (iii)\nenforces honest checking through a peer-prediction mechanism that mitigates\nlazy verification in naive voting. We further introduce an isomorphic\ninference-verification network that multiplexes both roles on the same set of\nGPU workers. This architecture (i) increases GPU utilization and thereby\nimproves end-to-end throughput for both inference and verification, (ii)\nexpands the effective pool of available validators, strengthening robustness\nand security, and (iii) enforces task indistinguishability at the worker\nboundary to prevent job-type-conditioned behavior. Finally, we provide a formal\ngame-theoretic analysis and prove that, under our incentives, honest inference\nand verification constitute a Nash equilibrium, ensuring incentive\ncompatibility against rational adversaries. To our knowledge, this is the first\ndecentralized inference verification protocol with an end-to-end game-theoretic\nsecurity proof.", "AI": {"tldr": "VeriLLM is a publicly verifiable protocol for decentralized LLM inference that ensures security under one-honest-verifier assumption, achieves near-negligible verification cost (~1% of inference), and uses peer-prediction to prevent lazy verification.", "motivation": "Decentralized LLM inference offers security, efficiency and cost benefits, but requires output verifiability since permissionless settings lack a priori trust in participating nodes.", "method": "Uses lightweight verification algorithm for LLMs, peer-prediction mechanism, and isomorphic inference-verification network that multiplexes both roles on same GPU workers with task indistinguishability.", "result": "Achieves near-negligible verification cost (1% of inference), improves GPU utilization and throughput, expands validator pool for robustness, and provides formal game-theoretic security proof.", "conclusion": "VeriLLM is the first decentralized inference verification protocol with end-to-end game-theoretic security proof, ensuring honest inference and verification as Nash equilibrium against rational adversaries."}}
{"id": "2509.23077", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23077", "abs": "https://arxiv.org/abs/2509.23077", "authors": ["Reza Rahimi Azghan", "Gautham Krishna Gudur", "Mohit Malu", "Edison Thomaz", "Giulia Pedrielli", "Pavan Turaga", "Hassan Ghasemzadeh"], "title": "CLAD-Net: Continual Activity Recognition in Multi-Sensor Wearable Systems", "comment": null, "summary": "The rise of deep learning has greatly advanced human behavior monitoring\nusing wearable sensors, particularly human activity recognition (HAR). While\ndeep models have been widely studied, most assume stationary data distributions\n- an assumption often violated in real-world scenarios. For example, sensor\ndata from one subject may differ significantly from another, leading to\ndistribution shifts. In continual learning, this shift is framed as a sequence\nof tasks, each corresponding to a new subject. Such settings suffer from\ncatastrophic forgetting, where prior knowledge deteriorates as new tasks are\nlearned. This challenge is compounded by the scarcity and inconsistency of\nlabeled data in human studies. To address these issues, we propose CLAD-Net\n(Continual Learning with Attention and Distillation), a framework enabling\nwearable-sensor models to be updated continuously without sacrificing\nperformance on past tasks. CLAD-Net integrates a self-supervised transformer,\nacting as long-term memory, with a supervised Convolutional Neural Network\n(CNN) trained via knowledge distillation for activity classification. The\ntransformer captures global activity patterns through cross-attention across\nbody-mounted sensors, learning generalizable representations without labels.\nMeanwhile, the CNN leverages knowledge distillation to retain prior knowledge\nduring subject-wise fine-tuning. On PAMAP2, CLAD-Net achieves 91.36 percent\nfinal accuracy with only 8.78 percent forgetting, surpassing memory-based and\nregularization-based baselines such as Experience Replay and Elastic Weight\nConsolidation. In semi-supervised settings with only 10-20 percent labeled\ndata, CLAD-Net still delivers strong performance, demonstrating robustness to\nlabel scarcity. Ablation studies further validate each module's contribution.", "AI": {"tldr": "CLAD-Net is a continual learning framework for wearable sensor-based human activity recognition that addresses catastrophic forgetting and label scarcity by combining self-supervised transformers with supervised CNNs using knowledge distillation.", "motivation": "Deep learning models for human activity recognition assume stationary data distributions, but real-world scenarios face distribution shifts between subjects and suffer from catastrophic forgetting in continual learning settings, compounded by limited labeled data.", "method": "CLAD-Net integrates a self-supervised transformer as long-term memory to capture global activity patterns across body sensors, and a supervised CNN trained via knowledge distillation for activity classification, enabling continuous model updates without forgetting prior knowledge.", "result": "On PAMAP2 dataset, CLAD-Net achieves 91.36% final accuracy with only 8.78% forgetting, outperforming memory-based and regularization-based baselines. It maintains strong performance even with only 10-20% labeled data in semi-supervised settings.", "conclusion": "CLAD-Net effectively addresses catastrophic forgetting and label scarcity in continual learning for wearable sensor-based activity recognition, demonstrating robust performance through its hybrid transformer-CNN architecture with knowledge distillation."}}
{"id": "2509.23204", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23204", "abs": "https://arxiv.org/abs/2509.23204", "authors": ["Stefan Arnold", "Ren\u00e9 Gr\u00f6bner"], "title": "Steering Prepositional Phrases in Language Models: A Case of with-headed Adjectival and Adverbial Complements in Gemma-2", "comment": null, "summary": "Language Models, when generating prepositional phrases, must often decide for\nwhether their complements functions as an instrumental adjunct (describing the\nverb adverbially) or an attributive modifier (enriching the noun adjectivally),\nyet the internal mechanisms that resolve this split decision remain poorly\nunderstood. In this study, we conduct a targeted investigation into Gemma-2 to\nuncover and control the generation of prepositional complements. We assemble a\nprompt suite containing with-headed prepositional phrases whose contexts\nequally accommodate either an instrumental or attributive continuation,\nrevealing a strong preference for an instrumental reading at a ratio of 3:4. To\npinpoint individual attention heads that favor instrumental over attributive\ncomplements, we project activations into the vocabulary space. By scaling the\nvalue vector of a single attention head, we can shift the distribution of\nfunctional roles of complements, attenuating instruments to 33% while elevating\nattributes to 36%.", "AI": {"tldr": "The paper investigates how language models resolve prepositional phrase ambiguity between instrumental and attributive readings in Gemma-2, finding a 3:4 preference for instrumental interpretations and demonstrating control over this distribution through attention head manipulation.", "motivation": "To understand the internal mechanisms that resolve the ambiguity in prepositional phrases between instrumental adjuncts (verb modification) and attributive modifiers (noun modification), which remains poorly understood in language models.", "method": "Used a prompt suite with prepositional phrases that equally accommodate both instrumental and attributive continuations, projected activations into vocabulary space to identify attention heads, and scaled value vectors of specific attention heads to control generation.", "result": "Found a 3:4 preference for instrumental readings; by scaling a single attention head's value vector, successfully shifted the distribution from 33% instrumental to 36% attributive interpretations.", "conclusion": "Individual attention heads in language models encode preferences for prepositional phrase interpretations, and targeted intervention can effectively control the functional role distribution of complements."}}
{"id": "2509.23510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23510", "abs": "https://arxiv.org/abs/2509.23510", "authors": ["Ashwin Ramaswamy", "Nestor Demeure", "Ermal Rrapaj"], "title": "Model Consistency as a Cheap yet Predictive Proxy for LLM Elo Scores", "comment": null, "summary": "New large language models (LLMs) are being released every day. Some perform\nsignificantly better or worse than expected given their parameter count.\nTherefore, there is a need for a method to independently evaluate models. The\ncurrent best way to evaluate a model is to measure its Elo score by comparing\nit to other models in a series of contests - an expensive operation since\nhumans are ideally required to compare LLM outputs. We observe that when an LLM\nis asked to judge such contests, the consistency with which it selects a model\nas the best in a matchup produces a metric that is 91% correlated with its own\nhuman-produced Elo score. This provides a simple proxy for Elo scores that can\nbe computed cheaply, without any human data or prior knowledge.", "AI": {"tldr": "LLM self-judgment consistency in pairwise comparisons provides a cheap proxy for human Elo scores with 91% correlation.", "motivation": "Need for independent evaluation of new LLMs as they are released frequently with varying performance relative to parameter count, and human Elo scoring is expensive.", "method": "Use LLM's own judgment consistency in selecting best model in pairwise contests as a metric, without human data or prior knowledge.", "result": "The consistency metric shows 91% correlation with human-produced Elo scores.", "conclusion": "LLM self-judgment consistency provides a simple, cheap alternative to expensive human evaluation for estimating Elo scores."}}
{"id": "2509.24272", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24272", "abs": "https://arxiv.org/abs/2509.24272", "authors": ["Weibo Zhao", "Jiahao Liu", "Bonan Ruan", "Shaofei Li", "Zhenkai Liang"], "title": "When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation", "comment": null, "summary": "Model Context Protocol (MCP) servers enable AI applications to connect to\nexternal systems in a plug-and-play manner, but their rapid proliferation also\nintroduces severe security risks. Unlike mature software ecosystems with\nrigorous vetting, MCP servers still lack standardized review mechanisms, giving\nadversaries opportunities to distribute malicious implementations. Despite this\npressing risk, the security implications of MCP servers remain underexplored.\nTo address this gap, we present the first systematic study that treats MCP\nservers as active threat actors and decomposes them into core components to\nexamine how adversarial developers can implant malicious intent. Specifically,\nwe investigate three research questions: (i) what types of attacks malicious\nMCP servers can launch, (ii) how vulnerable MCP hosts and Large Language Models\n(LLMs) are to these attacks, and (iii) how feasible it is to carry out MCP\nserver attacks in practice. Our study proposes a component-based taxonomy\ncomprising twelve attack categories. For each category, we develop\nProof-of-Concept (PoC) servers and demonstrate their effectiveness across\ndiverse real-world host-LLM settings. We further show that attackers can\ngenerate large numbers of malicious servers at virtually no cost. We then test\nstate-of-the-art scanners on the generated servers and found that existing\ndetection approaches are insufficient. These findings highlight that malicious\nMCP servers are easy to implement, difficult to detect with current tools, and\ncapable of causing concrete damage to AI agent systems. Addressing this threat\nrequires coordinated efforts among protocol designers, host developers, LLM\nproviders, and end users to build a more secure and resilient MCP ecosystem.", "AI": {"tldr": "This paper presents the first systematic security study of Model Context Protocol (MCP) servers, revealing they can be easily weaponized as active threat actors with 12 attack categories, are difficult to detect with current tools, and pose concrete risks to AI agent systems.", "motivation": "MCP servers enable plug-and-play connectivity for AI applications but lack standardized security vetting, creating severe security risks that remain underexplored despite rapid proliferation.", "method": "Proposed a component-based taxonomy with 12 attack categories, developed Proof-of-Concept (PoC) servers for each category, tested effectiveness across real-world host-LLM settings, and evaluated state-of-the-art scanners.", "result": "Attackers can generate large numbers of malicious MCP servers at virtually no cost; existing detection approaches are insufficient; malicious servers are easy to implement, difficult to detect, and capable of causing concrete damage.", "conclusion": "Addressing MCP server security threats requires coordinated efforts among protocol designers, host developers, LLM providers, and end users to build a more secure and resilient MCP ecosystem."}}
{"id": "2509.23085", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23085", "abs": "https://arxiv.org/abs/2509.23085", "authors": ["Hyunwoo Lee", "Hayoung Choi", "Hyunju Kim"], "title": "Signal Preserving Weight Initialization for Odd-Sigmoid Activations", "comment": null, "summary": "Activation functions critically influence trainability and expressivity, and\nrecent work has therefore explored a broad range of nonlinearities. However,\nactivations and weight initialization are interdependent: without an\nappropriate initialization method, nonlinearities can cause saturation,\nvariance collapse, and increased learning rate sensitivity. We address this by\ndefining an odd sigmoid function class and, given any activation f in this\nclass, proposing an initialization method tailored to f. The method selects a\nnoise scale in closed form so that forward activations remain well dispersed up\nto a target layer, thereby avoiding collapse to zero or saturation.\nEmpirically, the approach trains reliably without normalization layers,\nexhibits strong data efficiency, and enables learning for activations under\nwhich standard initialization methods (Xavier, He, Orthogonal) often do not\nconverge reliably.", "AI": {"tldr": "Proposes a novel initialization method for odd sigmoid activation functions that prevents saturation and variance collapse, enabling reliable training without normalization layers.", "motivation": "Activation functions and weight initialization are interdependent - improper initialization can cause saturation, variance collapse, and learning rate sensitivity issues that hinder training.", "method": "Defines an odd sigmoid function class and provides a closed-form initialization method that selects noise scale to keep forward activations well dispersed up to target layers, avoiding collapse to zero or saturation.", "result": "The method trains reliably without normalization layers, shows strong data efficiency, and enables learning for activations where standard initialization methods (Xavier, He, Orthogonal) often fail to converge.", "conclusion": "The proposed initialization approach successfully addresses the interdependence between activations and initialization, allowing effective training with various nonlinearities that would otherwise fail with standard methods."}}
{"id": "2509.23206", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23206", "abs": "https://arxiv.org/abs/2509.23206", "authors": ["Huacan Chai", "Zijie Cao", "Maolin Ran", "Yingxuan Yang", "Jianghao Lin", "pengxin", "Hairui Wang", "Renjie Ding", "Ziyu Wan", "Muning Wen", "Weiwen Liu", "Weinan Zhang", "Fei Huang", "Ying Wen"], "title": "PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness", "comment": null, "summary": "Large language models (LLMs) have achieved impressive success in single-turn\nfunction calling, yet real-world applications such as travel planning or\nmulti-stage data analysis typically unfold across multi-turn conversations. In\nthese settings, LLMs must not only issue accurate function calls at each step\nbut also maintain progress awareness, the ability to summarize past\ninteractions and plan future actions to ensure coherent, long-horizon task\nexecution. Existing approaches, however, either reduce multi-turn training to\nisolated single-turn samples, which neglects task-level planning, or employ\nend-to-end reinforcement learning (RL) that struggles with redundancy and lacks\nexplicit integration of progress awareness. To overcome these limitations, we\nintroduce PARL-MT, a framework that explicitly incorporates progress awareness\ninto LLM training for multi-turn function calling. PARL-MT combines (i) a\nProgress Awareness Generation (PAG) pipeline, which automatically constructs\ndatasets coupling conversation summaries with future task planning, and (ii) a\nProgress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which\nintegrates progress awareness into RL training to reduce contextual redundancy\nand improve alignment between local actions and global task completion.\nEmpirical results on two public benchmarks demonstrate that PARL-MT\nsignificantly outperforms existing methods, highlighting the effectiveness of\nprogress awareness in enabling robust and efficient multi-turn function\ncalling.", "AI": {"tldr": "PARL-MT is a framework that incorporates progress awareness into LLM training for multi-turn function calling, combining automatic dataset generation with progress-aware reinforcement learning to improve long-horizon task execution.", "motivation": "Real-world applications like travel planning require multi-turn conversations where LLMs need progress awareness - the ability to summarize past interactions and plan future actions. Existing approaches either neglect task-level planning or struggle with redundancy in RL training.", "method": "PARL-MT combines: (1) Progress Awareness Generation (PAG) pipeline that automatically constructs datasets coupling conversation summaries with future task planning, and (2) Progress Awareness-Guided Reinforcement Learning (PAG-RL) that integrates progress awareness into RL training to reduce redundancy and improve action-task alignment.", "result": "Empirical results on two public benchmarks demonstrate that PARL-MT significantly outperforms existing methods.", "conclusion": "The framework highlights the effectiveness of progress awareness in enabling robust and efficient multi-turn function calling for LLMs."}}
{"id": "2509.23529", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23529", "abs": "https://arxiv.org/abs/2509.23529", "authors": ["Ilya Kuleshov", "Ilin Pavel", "Nikolay Kompanets", "Ksenia Sycheva", "Aleksandr Nikolich"], "title": "DOoM: Difficult Olympiads of Math", "comment": null, "summary": "This paper introduces DOoM, a new open-source benchmark designed to assess\nthe capabilities of language models in solving mathematics and physics problems\nin Russian. The benchmark includes problems of varying difficulty, ranging from\nschool-level tasks to university Olympiad and entrance exam questions. In this\npaper we discuss the motivation behind its creation, describe dataset's\nstructure and evaluation methodology, and present initial results from testing\nvarious models. Analysis of the results shows a correlation between model\nperformance and the number of tokens used, and highlights differences in\nperformance between mathematics and physics tasks.", "AI": {"tldr": "DOoM is a new open-source benchmark for evaluating language models' ability to solve Russian mathematics and physics problems across difficulty levels from school to university Olympiad/exam questions.", "motivation": "To assess language model capabilities specifically for Russian mathematics and physics problem-solving, addressing the need for specialized benchmarks in this domain.", "method": "Created a structured dataset with problems of varying difficulty levels, established evaluation methodology, and tested various language models on the benchmark.", "result": "Analysis revealed correlation between model performance and token count used, and identified performance differences between mathematics and physics tasks.", "conclusion": "DOoM benchmark provides valuable insights into language model capabilities for Russian STEM problem-solving, showing clear performance patterns related to model size and task type."}}
{"id": "2509.24408", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24408", "abs": "https://arxiv.org/abs/2509.24408", "authors": ["Yuzhen Long", "Songze Li"], "title": "FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous Driving Systems", "comment": null, "summary": "Autonomous driving systems increasingly rely on multi-agent architectures\npowered by large language models (LLMs), where specialized agents collaborate\nto perceive, reason, and plan. A key component of these systems is the shared\nfunction library, a collection of software tools that agents use to process\nsensor data and navigate complex driving environments. Despite its critical\nrole in agent decision-making, the function library remains an under-explored\nvulnerability. In this paper, we introduce FuncPoison, a novel poisoning-based\nattack targeting the function library to manipulate the behavior of LLM-driven\nmulti-agent autonomous systems. FuncPoison exploits two key weaknesses in how\nagents access the function library: (1) agents rely on text-based instructions\nto select tools; and (2) these tools are activated using standardized command\nformats that attackers can replicate. By injecting malicious tools with\ndeceptive instructions, FuncPoison manipulates one agent s decisions--such as\nmisinterpreting road conditions--triggering cascading errors that mislead other\nagents in the system. We experimentally evaluate FuncPoison on two\nrepresentative multi-agent autonomous driving systems, demonstrating its\nability to significantly degrade trajectory accuracy, flexibly target specific\nagents to induce coordinated misbehavior, and evade diverse defense mechanisms.\nOur results reveal that the function library, often considered a simple\ntoolset, can serve as a critical attack surface in LLM-based autonomous driving\nsystems, raising elevated concerns on their reliability.", "AI": {"tldr": "FuncPoison is a novel poisoning attack targeting function libraries in LLM-driven multi-agent autonomous driving systems, exploiting text-based tool selection and standardized command formats to inject malicious tools and cause cascading errors.", "motivation": "The function library is a critical but under-explored vulnerability in multi-agent autonomous driving systems, where agents rely on shared tools for perception, reasoning, and planning.", "method": "FuncPoison exploits two weaknesses: (1) agents' reliance on text-based instructions to select tools, and (2) standardized command formats that attackers can replicate. It injects malicious tools with deceptive instructions to manipulate agent decisions.", "result": "Experimental evaluation on two multi-agent autonomous driving systems shows FuncPoison significantly degrades trajectory accuracy, can flexibly target specific agents to induce coordinated misbehavior, and evades diverse defense mechanisms.", "conclusion": "The function library, often considered a simple toolset, serves as a critical attack surface in LLM-based autonomous driving systems, raising serious concerns about their reliability."}}
{"id": "2509.23087", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23087", "abs": "https://arxiv.org/abs/2509.23087", "authors": ["Deshu Chen", "Yuchen Liu", "Zhijian Zhou", "Chao Qu", "Yuan Qi"], "title": "Unleashing Flow Policies with Distributional Critics", "comment": null, "summary": "Flow-based policies have recently emerged as a powerful tool in offline and\noffline-to-online reinforcement learning, capable of modeling the complex,\nmultimodal behaviors found in pre-collected datasets. However, the full\npotential of these expressive actors is often bottlenecked by their critics,\nwhich typically learn a single, scalar estimate of the expected return. To\naddress this limitation, we introduce the Distributional Flow Critic (DFC), a\nnovel critic architecture that learns the complete state-action return\ndistribution. Instead of regressing to a single value, DFC employs flow\nmatching to model the distribution of return as a continuous, flexible\ntransformation from a simple base distribution to the complex target\ndistribution of returns. By doing so, DFC provides the expressive flow-based\npolicy with a rich, distributional Bellman target, which offers a more stable\nand informative learning signal. Extensive experiments across D4RL and OGBench\nbenchmarks demonstrate that our approach achieves strong performance,\nespecially on tasks requiring multimodal action distributions, and excels in\nboth offline and offline-to-online fine-tuning compared to existing methods.", "AI": {"tldr": "The paper introduces Distributional Flow Critic (DFC), a novel critic architecture that models the complete state-action return distribution using flow matching, overcoming limitations of traditional scalar critics and improving performance in offline RL tasks.", "motivation": "Flow-based policies are powerful for modeling multimodal behaviors in offline RL, but their potential is limited by traditional critics that only learn scalar return estimates, lacking distributional information.", "method": "DFC employs flow matching to model return distributions as continuous transformations from simple base distributions to complex target distributions, providing rich distributional Bellman targets for flow-based policies.", "result": "Extensive experiments on D4RL and OGBench benchmarks show strong performance, particularly on tasks requiring multimodal action distributions, and excel in both offline and offline-to-online fine-tuning.", "conclusion": "DFC successfully addresses the bottleneck of scalar critics by providing expressive distributional learning signals, enabling better utilization of flow-based policies' capabilities in complex RL scenarios."}}
{"id": "2509.23208", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23208", "abs": "https://arxiv.org/abs/2509.23208", "authors": ["Haorui Yu", "Ramon Ruiz-Dolz", "Qiufeng Yi"], "title": "A Structured Framework for Evaluating and Enhancing Interpretive Capabilities of Multimodal LLMs in Culturally Situated Tasks", "comment": "EMNLP 2025 submission, 10 pages, 6 figures, 5 tables", "summary": "This study aims to test and evaluate the capabilities and characteristics of\ncurrent mainstream Visual Language Models (VLMs) in generating critiques for\ntraditional Chinese painting. To achieve this, we first developed a\nquantitative framework for Chinese painting critique. This framework was\nconstructed by extracting multi-dimensional evaluative features covering\nevaluative stance, feature focus, and commentary quality from human expert\ncritiques using a zero-shot classification model. Based on these features,\nseveral representative critic personas were defined and quantified. This\nframework was then employed to evaluate selected VLMs such as Llama, Qwen, or\nGemini. The experimental design involved persona-guided prompting to assess the\nVLM's ability to generate critiques from diverse perspectives. Our findings\nreveal the current performance levels, strengths, and areas for improvement of\nVLMs in the domain of art critique, offering insights into their potential and\nlimitations in complex semantic understanding and content generation tasks. The\ncode used for our experiments can be publicly accessed at:\nhttps://github.com/yha9806/VULCA-EMNLP2025.", "AI": {"tldr": "This paper develops a quantitative framework to evaluate Visual Language Models' (VLMs) ability to critique traditional Chinese painting, using persona-guided prompting to assess performance across multiple dimensions.", "motivation": "To systematically test and evaluate current mainstream VLMs' capabilities in generating critiques for traditional Chinese painting, identifying their strengths and limitations in art critique tasks.", "method": "Developed a quantitative framework by extracting multi-dimensional evaluative features from human expert critiques using zero-shot classification, defined critic personas, and employed persona-guided prompting to evaluate VLMs like Llama, Qwen, and Gemini.", "result": "Revealed current performance levels, strengths, and areas for improvement of VLMs in art critique domain, providing insights into their capabilities in complex semantic understanding and content generation.", "conclusion": "The study offers valuable insights into VLMs' potential and limitations for traditional Chinese painting critique, with publicly available experimental code for further research."}}
{"id": "2509.23537", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23537", "abs": "https://arxiv.org/abs/2509.23537", "authors": ["Aaron Xuxiang Tian", "Ruofan Zhang", "Jiayao Tang", "Young Min Cho", "Xueqian Li", "Qiang Yi", "Ji Wang", "Zhunping Zhang", "Danrui Qi", "Sharath Chandra Guntuku", "Lyle Ungar", "Tianyu Shi", "Chi Wang"], "title": "Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks", "comment": "9 pages, 3 tables, 1 figure", "summary": "We study multi-turn multi-agent orchestration, where multiple large language\nmodel (LLM) agents interact over multiple turns by iteratively proposing\nanswers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5\nPro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we\nconduct two experiments: (i) benchmarking orchestration against single-LLM\nbaselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who\nauthored answers and whether they can observe ongoing votes. Orchestration\nmatches or exceeds the strongest single model and consistently outperforms the\nothers. Analysis of best-achievable orchestration performance shows potential\nfor further gains. The ablations show that revealing authorship increases\nself-voting and ties, and that showing ongoing votes amplifies herding, which\nspeeds convergence but can sometimes yield premature consensus.", "AI": {"tldr": "Multi-agent orchestration with LLMs achieves performance matching or exceeding the strongest single model across three benchmarks, with analysis showing potential for further gains and revealing how authorship visibility and vote observation affect voting behavior.", "motivation": "To study multi-turn multi-agent orchestration where multiple LLM agents interact through iterative answer proposals and voting until consensus, comparing performance against single-LLM baselines.", "method": "Used four LLMs (Gemini 2.5 Pro, GPT-5, Grok 4, Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR benchmarks. Conducted two experiments: (1) benchmarking orchestration vs single-LLM baselines, (2) ablations on GPQA-Diamond varying authorship visibility and ongoing vote observation.", "result": "Orchestration matches or exceeds the strongest single model and consistently outperforms others. Analysis shows potential for further gains. Ablations reveal that revealing authorship increases self-voting and ties, while showing ongoing votes amplifies herding, speeding convergence but sometimes causing premature consensus.", "conclusion": "Multi-agent orchestration is effective for improving LLM performance, with careful design of voting mechanisms needed to balance convergence speed against premature consensus, and authorship visibility affecting voting behavior."}}
{"id": "2509.24418", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24418", "abs": "https://arxiv.org/abs/2509.24418", "authors": ["Haoran Li", "Yulin Chen", "Jingru Zeng", "Hao Peng", "Huihao Jing", "Wenbin Hu", "Xi Yang", "Ziqian Zeng", "Sirui Han", "Yangqiu Song"], "title": "GSPR: Aligning LLM Safeguards as Generalizable Safety Policy Reasoners", "comment": null, "summary": "As large language models (LLMs) are increasingly integrated into numerous\napplications across various domains, LLMs' safety becomes a critical concern\nfor both application developers and intended users. Currently, great efforts\nhave been made to develop safety benchmarks with fine-grained taxonomies.\nHowever, these benchmarks' taxonomies are disparate with different safety\npolicies. Thus, existing safeguards trained on these benchmarks are either\ncoarse-grained to only distinguish between safe and unsafe, or constrained by\nthe narrow risk taxonomies of a single benchmark. To leverage these\nfine-grained safety taxonomies across multiple safety benchmarks, in this\npaper, we propose GSPR, a Generalizable Safety Policy Reasoner to identify\nunsafe input prompts and LLMs' outputs with violated safety taxonomies through\nGroup Relative Policy Optimization (GRPO). Unlike prior safeguards which only\ncover a fixed set of risk factors, our GSPR incentivizes its reasoning\ncapability with varied safety taxonomies through our careful cold-start\nstrategy and reward design. Consequently, our GSPR can be trained across\nmultiple safety benchmarks with distinct taxonomies and naturally exhibits\npowerful generalization ability. We conduct extensive experiments to show that\nour GSPR significantly improves existing safety guardrails' reasoning\ncapabilities for both safety and category prediction tasks. Moreover, our GSPR\nnot only demonstrates powerful safety generalization abilities but also\nachieves the least inference token costs with explanations.", "AI": {"tldr": "GSPR is a Generalizable Safety Policy Reasoner that identifies unsafe prompts and LLM outputs using Group Relative Policy Optimization, enabling cross-benchmark training with varied safety taxonomies.", "motivation": "Current LLM safety benchmarks have disparate taxonomies, leading to coarse-grained safeguards or narrow risk coverage. There's a need to leverage multiple safety benchmarks with different taxonomies for better safety reasoning.", "method": "Proposed GSPR with Group Relative Policy Optimization (GRPO), using careful cold-start strategy and reward design to train across multiple safety benchmarks with distinct taxonomies.", "result": "GSPR significantly improves safety reasoning capabilities for both safety and category prediction tasks, demonstrates powerful generalization abilities, and achieves the least inference token costs with explanations.", "conclusion": "GSPR provides an effective approach to leverage multiple safety benchmarks with varied taxonomies, enabling better safety reasoning and generalization while maintaining efficiency."}}
{"id": "2509.23089", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23089", "abs": "https://arxiv.org/abs/2509.23089", "authors": ["Sylee", "Beltiukov", "Satyandra Guthula", "Wenbo Guo", "Walter Willinger", "Arpit Gupta"], "title": "Demystifying Network Foundation Models", "comment": null, "summary": "This work presents a systematic investigation into the latent knowledge\nencoded within Network Foundation Models (NFMs) that focuses on hidden\nrepresentations analysis rather than pure downstream task performance.\nDifferent from existing efforts, we analyze the models through a three-part\nevaluation: Embedding Geometry Analysis to assess representation space\nutilization, Metric Alignment Assessment to measure correspondence with\ndomain-expert features, and Causal Sensitivity Testing to evaluate robustness\nto protocol perturbations. Using five diverse network datasets spanning\ncontrolled and real-world environments, we evaluate four state-of-the-art NFMs,\nrevealing that they all exhibit significant anisotropy, inconsistent feature\nsensitivity patterns, an inability to separate the high-level context, payload\ndependency, and other properties. Our work identifies numerous limitations\nacross all models and demonstrates that addressing them can significantly\nimprove model performance (by up to +0.35 $F_1$ score without architectural\nchanges).", "AI": {"tldr": "Systematic analysis of Network Foundation Models (NFMs) reveals significant limitations including anisotropy, inconsistent feature sensitivity, and inability to separate high-level context, with fixes improving performance by up to +0.35 F1 score.", "motivation": "To investigate latent knowledge in Network Foundation Models through hidden representations analysis rather than just downstream task performance, addressing gaps in existing evaluation methods.", "method": "Three-part evaluation: Embedding Geometry Analysis for representation space utilization, Metric Alignment Assessment for domain-expert feature correspondence, and Causal Sensitivity Testing for robustness to protocol perturbations. Evaluated four state-of-the-art NFMs across five diverse network datasets.", "result": "All NFMs exhibited significant anisotropy, inconsistent feature sensitivity patterns, inability to separate high-level context, payload dependency, and other limitations. Addressing these issues improved model performance by up to +0.35 F1 score without architectural changes.", "conclusion": "Current Network Foundation Models have fundamental limitations in their latent representations, but systematically identifying and addressing these issues can significantly enhance performance without requiring model architecture modifications."}}
{"id": "2509.23233", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23233", "abs": "https://arxiv.org/abs/2509.23233", "authors": ["Sina J. Semnani", "Jirayu Burapacheep", "Arpandeep Khatua", "Thanawan Atchariyachanvanit", "Zheng Wang", "Monica S. Lam"], "title": "Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models", "comment": "EMNLP 2025 (Main Conference)", "summary": "Wikipedia is the largest open knowledge corpus, widely used worldwide and\nserving as a key resource for training large language models (LLMs) and\nretrieval-augmented generation (RAG) systems. Ensuring its accuracy is\ntherefore critical. But how accurate is Wikipedia, and how can we improve it?\n  We focus on inconsistencies, a specific type of factual inaccuracy, and\nintroduce the task of corpus-level inconsistency detection. We present CLAIRE,\nan agentic system that combines LLM reasoning with retrieval to surface\npotentially inconsistent claims along with contextual evidence for human\nreview. In a user study with experienced Wikipedia editors, 87.5% reported\nhigher confidence when using CLAIRE, and participants identified 64.7% more\ninconsistencies in the same amount of time.\n  Combining CLAIRE with human annotation, we contribute WIKICOLLIDE, the first\nbenchmark of real Wikipedia inconsistencies. Using random sampling with\nCLAIRE-assisted analysis, we find that at least 3.3% of English Wikipedia facts\ncontradict another fact, with inconsistencies propagating into 7.3% of FEVEROUS\nand 4.0% of AmbigQA examples. Benchmarking strong baselines on this dataset\nreveals substantial headroom: the best fully automated system achieves an AUROC\nof only 75.1%.\n  Our results show that contradictions are a measurable component of Wikipedia\nand that LLM-based systems like CLAIRE can provide a practical tool to help\neditors improve knowledge consistency at scale.", "AI": {"tldr": "CLAIRE is an LLM-based system that detects inconsistencies in Wikipedia by combining reasoning with retrieval, helping editors identify contradictions more efficiently. The study found 3.3% of English Wikipedia facts contain contradictions, creating the WIKICOLLIDE benchmark.", "motivation": "Wikipedia serves as critical training data for LLMs and RAG systems, making its accuracy essential. The paper addresses the problem of factual inconsistencies in Wikipedia that undermine its reliability.", "method": "Developed CLAIRE - an agentic system combining LLM reasoning with retrieval to detect corpus-level inconsistencies. Used human annotation with CLAIRE assistance to create WIKICOLLIDE benchmark from real Wikipedia data.", "result": "87.5% of Wikipedia editors reported higher confidence using CLAIRE, identifying 64.7% more inconsistencies. Found 3.3% of English Wikipedia facts contain contradictions, affecting 7.3% of FEVEROUS and 4.0% of AmbigQA examples. Best automated system achieved only 75.1% AUROC.", "conclusion": "Contradictions are a measurable problem in Wikipedia. LLM-based systems like CLAIRE provide practical tools to help editors improve knowledge consistency at scale, with significant room for improvement in automated detection."}}
{"id": "2509.23558", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23558", "abs": "https://arxiv.org/abs/2509.23558", "authors": ["Zhaoqi Wang", "Daqing He", "Zijian Zhang", "Xin Li", "Liehuang Zhu", "Meng Li", "Jiamou Liu"], "title": "Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, yet\nthey also introduce novel security challenges. For instance, prompt\njailbreaking attacks involve adversaries crafting sophisticated prompts to\nelicit responses from LLMs that deviate from human values. To uncover\nvulnerabilities in LLM alignment methods, we propose the PASS framework\n(\\underline{P}rompt J\\underline{a}ilbreaking via \\underline{S}emantic and\n\\underline{S}tructural Formalization). Specifically, PASS employs reinforcement\nlearning to transform initial jailbreak prompts into formalized descriptions,\nwhich enhances stealthiness and enables bypassing existing alignment defenses.\nThe jailbreak outputs are then structured into a GraphRAG system that, by\nleveraging extracted relevant terms and formalized symbols as contextual input\nalongside the original query, strengthens subsequent attacks and facilitates\nmore effective jailbreaks. We conducted extensive experiments on common\nopen-source models, demonstrating the effectiveness of our attack.", "AI": {"tldr": "PASS framework uses reinforcement learning to formalize jailbreak prompts, making them stealthier and more effective at bypassing LLM alignment defenses.", "motivation": "To uncover vulnerabilities in LLM alignment methods by developing more sophisticated jailbreaking techniques that can evade existing security measures.", "method": "Uses reinforcement learning to transform initial jailbreak prompts into formalized descriptions, then structures outputs into a GraphRAG system that leverages extracted terms and symbols to enhance subsequent attacks.", "result": "Extensive experiments on open-source models demonstrated the effectiveness of the PASS framework in successfully jailbreaking LLMs.", "conclusion": "The PASS framework provides an effective method for identifying LLM alignment vulnerabilities through semantic and structural formalization of jailbreak attacks."}}
{"id": "2509.24440", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24440", "abs": "https://arxiv.org/abs/2509.24440", "authors": ["Antonis Selentis", "Nikolas Makris", "Alkinoos Papageorgopoulos", "Persefoni Konteli", "Konstantinos Christodoulopoulos", "George T. Kanellos", "Dimitris Syvridis"], "title": "Evaluating Relayed and Switched Quantum Key Distribution (QKD) Network Architectures", "comment": null, "summary": "We evaluate the performance of two architectures for network-wide quantum key\ndistribution (QKD): Relayed QKD, which relays keys over multi-link QKD paths\nfor non-adjacent nodes, and Switched QKD, which uses optical switches to\ndynamically connect arbitrary QKD modules to form direct QKD links between\nthem. An advantage of Switched QKD is that it distributes quantum keys\nend-to-end, whereas Relayed relies on trusted nodes. However, Switched depends\non arbitrary matching of QKD modules. We first experimentally evaluate the\nperformance of commercial DV-QKD modules; for each of three vendors we\nbenchmark the performance in standard/matched module pairs and in unmatched\npairs to emulate configurations in the Switched QKD network architecture. The\nanalysis reveals that in some cases a notable variation in the generated secret\nkey rate (SKR) between the matched and unmatched pairs is observed. Driven by\nthese experimental findings, we conduct a comprehensive theoretical analysis\nthat evaluates the network-wide performance of the two architectures. Our\nanalysis is based on uniform ring networks, where we derive optimal key\nmanagement configurations and analytical formulas for the achievable consumed\nSKR. We compare network performance under varying ring sizes, QKD link losses,\nQKD receivers' sensitivity and performance penalties of unmatched modules. Our\nfindings indicate that Switched QKD performs better in dense rings (short\ndistances, large node counts), while Relayed QKD is more effective in longer\ndistances and large node counts. Moreover, we confirm that unmatched QKD\nmodules penalties significantly impact the efficiency of Switched QKD\narchitecture.", "AI": {"tldr": "Comparison of two QKD network architectures: Relayed QKD (uses trusted nodes) vs Switched QKD (end-to-end keys via optical switches). Experimental evaluation shows SKR variation in unmatched QKD modules. Theoretical analysis reveals Switched QKD performs better in dense rings, while Relayed QKD excels in longer distances.", "motivation": "To evaluate and compare the performance of two quantum key distribution network architectures - Relayed QKD (with trusted nodes) and Switched QKD (end-to-end keys) - to understand their relative advantages in different network scenarios.", "method": "1) Experimental evaluation of commercial DV-QKD modules from three vendors, testing both matched and unmatched pairs; 2) Comprehensive theoretical analysis using uniform ring networks, deriving optimal configurations and analytical formulas for SKR; 3) Performance comparison under varying ring sizes, link losses, receiver sensitivity, and unmatched module penalties.", "result": "Experimental results show notable SKR variation between matched and unmatched QKD module pairs. Theoretical analysis indicates Switched QKD performs better in dense rings (short distances, large node counts), while Relayed QKD is more effective for longer distances with large node counts. Unmatched module penalties significantly impact Switched QKD efficiency.", "conclusion": "The choice between Switched and Relayed QKD architectures depends on network characteristics: Switched QKD is preferable for dense networks with short distances, while Relayed QKD performs better for longer distance networks. The performance penalty from unmatched QKD modules is a critical factor affecting Switched QKD efficiency."}}
{"id": "2509.23092", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23092", "abs": "https://arxiv.org/abs/2509.23092", "authors": ["Christopher Scarvelis", "Justin Solomon"], "title": "Sensitivity Analysis for Diffusion Models", "comment": null, "summary": "Training a diffusion model approximates a map from a data distribution $\\rho$\nto the optimal score function $s_t$ for that distribution. Can we differentiate\nthis map? If we could, then we could predict how the score, and ultimately the\nmodel's samples, would change under small perturbations to the training set\nbefore committing to costly retraining. We give a closed-form procedure for\ncomputing this map's directional derivatives, relying only on black-box access\nto a pre-trained score model and its derivatives with respect to its inputs. We\nextend this result to estimate the sensitivity of a diffusion model's samples\nto additive perturbations of its target measure, with runtime comparable to\nsampling from a diffusion model and computing log-likelihoods along the sample\npath. Our method is robust to numerical and approximation error, and the\nresulting sensitivities correlate with changes in an image diffusion model's\nsamples after retraining and fine-tuning.", "AI": {"tldr": "This paper develops a method to compute how diffusion model outputs change with small training data perturbations, without needing retraining.", "motivation": "To predict how diffusion model scores and samples would change under small training set perturbations before committing to costly retraining.", "method": "Closed-form procedure using black-box access to pre-trained score model and its derivatives, extended to estimate sample sensitivity to target measure perturbations.", "result": "Method is robust to numerical error, with runtime comparable to sampling and log-likelihood computation, and sensitivities correlate with actual retraining changes.", "conclusion": "Enables efficient prediction of model behavior changes without retraining, providing practical sensitivity analysis for diffusion models."}}
{"id": "2509.23259", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.23259", "abs": "https://arxiv.org/abs/2509.23259", "authors": ["Soumick Sarker", "Abhijit Kumar Rai"], "title": "Fin-ExBERT: User Intent based Text Extraction in Financial Context using Graph-Augmented BERT and trainable Plugin", "comment": "10 pages, 5 figures, accepted at EMNLP 2025 Industry Track", "summary": "Financial dialogue transcripts pose a unique challenge for sentence-level\ninformation extraction due to their informal structure, domain-specific\nvocabulary, and variable intent density. We introduce Fin-ExBERT, a lightweight\nand modular framework for extracting user intent-relevant sentences from\nannotated financial service calls. Our approach builds on a domain-adapted BERT\n(Bidirectional Encoder Representations from Transformers) backbone enhanced\nwith LoRA (Low-Rank Adaptation) adapters, enabling efficient fine-tuning using\nlimited labeled data. We propose a two-stage training strategy with progressive\nunfreezing: initially training a classifier head while freezing the backbone,\nfollowed by gradual fine-tuning of the entire model with differential learning\nrates. To ensure robust extraction under uncertainty, we adopt a dynamic\nthresholding strategy based on probability curvature (elbow detection),\navoiding fixed cutoff heuristics. Empirical results show strong precision and\nF1 performance on real-world transcripts, with interpretable output suitable\nfor downstream auditing and question-answering workflows. The full framework\nsupports batched evaluation, visualization, and calibrated export, offering a\ndeployable solution for financial dialogue mining.", "AI": {"tldr": "Fin-ExBERT: A lightweight BERT-based framework with LoRA adapters for extracting intent-relevant sentences from financial dialogue transcripts using two-stage training and dynamic thresholding.", "motivation": "Financial dialogue transcripts present challenges for information extraction due to informal structure, domain-specific vocabulary, and variable intent density, requiring specialized approaches.", "method": "Domain-adapted BERT backbone enhanced with LoRA adapters, two-stage training with progressive unfreezing (classifier head first, then full model), and dynamic thresholding using probability curvature (elbow detection).", "result": "Strong precision and F1 performance on real-world transcripts, with interpretable output suitable for downstream auditing and question-answering workflows.", "conclusion": "The framework provides a deployable solution for financial dialogue mining with batched evaluation, visualization, and calibrated export capabilities."}}
{"id": "2509.23560", "categories": ["cs.AI", "68T35"], "pdf": "https://arxiv.org/pdf/2509.23560", "abs": "https://arxiv.org/abs/2509.23560", "authors": ["ChaoBo Zhang", "Long Tan"], "title": "A Hierarchical Structure-Enhanced Personalized Recommendation Model for Traditional Chinese Medicine Formulas Based on KG Diffusion Guidance", "comment": "10 pages, 10 figures, Proceedings of the 34th ACM International\n  Conference on Information and Knowledge Management (CIKM)", "summary": "Artificial intelligence technology plays a crucial role in recommending\nprescriptions for traditional Chinese medicine (TCM). Previous studies have\nmade significant progress by focusing on the symptom-herb relationship in\nprescriptions. However, several limitations hinder model performance: (i)\nInsufficient attention to patient-personalized information such as age, BMI,\nand medical history, which hampers accurate identification of syndrome and\nreduces efficacy. (ii) The typical long-tailed distribution of herb data\nintroduces training biases and affects generalization ability. (iii) The\noversight of the 'monarch, minister, assistant and envoy' compatibility among\nherbs increases the risk of toxicity or side effects, opposing the 'treatment\nbased on syndrome differentiation' principle in clinical TCM. Therefore, we\npropose a novel hierarchical structure-enhanced personalized recommendation\nmodel for TCM formulas based on knowledge graph diffusion guidance, namely\nTCM-HEDPR. Specifically, we pre-train symptom representations using\npatient-personalized prompt sequences and apply prompt-oriented contrastive\nlearning for data augmentation. Furthermore, we employ a KG-guided homogeneous\ngraph diffusion method integrated with a self-attention mechanism to globally\ncapture the non-linear symptom-herb relationship. Lastly, we design a\nheterogeneous graph hierarchical network to integrate herbal dispensing\nrelationships with implicit syndromes, guiding the prescription generation\nprocess at a fine-grained level and mitigating the long-tailed herb data\ndistribution problem. Extensive experiments on two public datasets and one\nclinical dataset demonstrate the effectiveness of TCM-HEDPR. In addition, we\nincorporate insights from modern medicine and network pharmacology to evaluate\nthe recommended prescriptions comprehensively. It can provide a new paradigm\nfor the recommendation of modern TCM.", "AI": {"tldr": "TCM-HEDPR is a hierarchical structure-enhanced personalized recommendation model for TCM formulas that addresses limitations in existing approaches by incorporating patient personalization, handling long-tailed herb distributions, and capturing herb compatibility relationships through knowledge graph diffusion guidance.", "motivation": "Existing TCM prescription recommendation models have three key limitations: insufficient attention to patient-personalized information (age, BMI, medical history), biased training from long-tailed herb distributions, and oversight of herb compatibility principles ('monarch, minister, assistant and envoy') which increases toxicity risks and violates TCM treatment principles.", "method": "The model uses: (1) pre-trained symptom representations with patient-personalized prompt sequences and prompt-oriented contrastive learning for data augmentation; (2) KG-guided homogeneous graph diffusion with self-attention to capture non-linear symptom-herb relationships; (3) heterogeneous graph hierarchical network to integrate herbal dispensing relationships with implicit syndromes, addressing long-tailed distribution issues.", "result": "Extensive experiments on two public datasets and one clinical dataset demonstrate the effectiveness of TCM-HEDPR. The model also incorporates modern medicine and network pharmacology insights for comprehensive prescription evaluation.", "conclusion": "TCM-HEDPR provides a new paradigm for modern TCM recommendation by effectively addressing key limitations in existing approaches and ensuring safer, more personalized prescription recommendations that align with TCM principles."}}
{"id": "2509.24444", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.24444", "abs": "https://arxiv.org/abs/2509.24444", "authors": ["Yury Yanovich", "Victoria Kovalevskaya", "Maksim Egorov", "Elizaveta Smirnova", "Matvey Mishuris", "Yash Madhwal", "Kirill Ziborov", "Vladimir Gorgadze", "Subodh Sharma"], "title": "BugMagnifier: TON Transaction Simulator for Revealing Smart Contract Vulnerabilities", "comment": null, "summary": "The Open Network (TON) blockchain employs an asynchronous execution model\nthat introduces unique security challenges for smart contracts, particularly\nrace conditions arising from unpredictable message processing order. While\nprevious work established vulnerability patterns through static analysis of\naudit reports, dynamic detection of temporal dependencies through systematic\ntesting remains an open problem. We present BugMagnifier, a transaction\nsimulation framework that systematically reveals vulnerabilities in TON smart\ncontracts through controlled message orchestration. Built atop TON Sandbox and\nintegrated with the TON Virtual Machine (TVM), our tool combines precise\nmessage queue manipulation with differential state analysis and probabilistic\npermutation testing to detect asynchronous execution flaws. Experimental\nevaluation demonstrates BugMagnifier's effectiveness through extensive\nparametric studies on purpose-built vulnerable contracts, revealing message\nratio-dependent detection complexity that aligns with theoretical predictions.\nThis quantitative model enables predictive vulnerability assessment while\nshifting discovery from manual expert analysis to automated evidence\ngeneration. By providing reproducible test scenarios for temporal\nvulnerabilities, BugMagnifier addresses a critical gap in the TON security\ntooling, offering practical support for safer smart contract development in\nasynchronous blockchain environments.", "AI": {"tldr": "BugMagnifier is a transaction simulation framework that detects asynchronous execution vulnerabilities in TON blockchain smart contracts through systematic message orchestration and differential state analysis.", "motivation": "TON blockchain's asynchronous execution model creates unique security challenges like race conditions due to unpredictable message processing order, requiring dynamic detection methods beyond static analysis.", "method": "Built on TON Sandbox and TVM, BugMagnifier combines precise message queue manipulation with differential state analysis and probabilistic permutation testing to systematically reveal vulnerabilities.", "result": "Experimental evaluation shows effective detection through parametric studies on vulnerable contracts, revealing message ratio-dependent detection complexity that aligns with theoretical predictions.", "conclusion": "BugMagnifier addresses critical security gaps in TON by enabling automated vulnerability discovery and providing reproducible test scenarios for safer smart contract development in asynchronous environments."}}
{"id": "2509.23095", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23095", "abs": "https://arxiv.org/abs/2509.23095", "authors": ["Xiangqi Wang", "Yue Huang", "Yujun Zhou", "Xiaonan Luo", "Kehan Guo", "Xiangliang Zhang"], "title": "Causally-Enhanced Reinforcement Policy Optimization", "comment": "Reinforcement learning publication of 24 pages", "summary": "Large language models (LLMs) trained with reinforcement objectives often\nachieve superficially correct answers via shortcut strategies, pairing correct\noutputs with spurious or unfaithful reasoning and degrading under small causal\nperturbations. We introduce Causally-Enhanced Policy Optimization (CE-PO), a\ndrop-in reward-shaping framework that augments policy optimization with a\ndifferentiable proxy for causal coherence along the generation pathway from\nprompt (Z) to rationale (X) to answer (Y). CE-PO estimates model-internal\ninfluence with Jacobian-based sensitivities, counterfactually hardens these\nsignals to suppress nuisance cues, and fuses the resulting coherence score with\ntask-accuracy feedback via a Minkowski (power-mean) combiner, exposing a single\ntunable between accuracy and coherence trade-off. The unified reward integrates\nwith PPO/GRPO without architectural changes. Across reasoning benchmarks and\ncausal stress tests, CE-PO reduces reward hacking and unfaithful\nchain-of-thought while improving robustness to correlation-causation flips and\nlight counterfactual edits, all at near-parity accuracy. Experimental results\nacross 4 datasets show that CE-PO improves accuracy over baselines by 5.49% on\naverage (up to 9.58%), while improving robustness to correlation-causation\nflips and light counterfactual edits.", "AI": {"tldr": "CE-PO is a reward-shaping framework that enhances LLM training by incorporating causal coherence along the generation pathway, reducing reward hacking and improving robustness while maintaining accuracy.", "motivation": "LLMs often achieve correct answers through shortcut strategies with unfaithful reasoning, degrading under small causal perturbations.", "method": "Uses Jacobian-based sensitivities to estimate model-internal influence, counterfactually hardens signals, and fuses coherence score with task-accuracy via Minkowski combiner. Integrates with PPO/GRPO without architectural changes.", "result": "Improves accuracy by 5.49% on average (up to 9.58%) across 4 datasets, reduces reward hacking and unfaithful chain-of-thought, and improves robustness to correlation-causation flips and counterfactual edits.", "conclusion": "CE-PO effectively balances accuracy and causal coherence, enhancing LLM reasoning faithfulness and robustness without compromising performance."}}
{"id": "2509.23286", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23286", "abs": "https://arxiv.org/abs/2509.23286", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Yoonjun Cho", "Dongjae Jeon", "Sangwoo Shin", "Hyesoo Hong", "Albert No"], "title": "A2D: Any-Order, Any-Step Safety Alignment for Diffusion Language Models", "comment": "Code and models are available at https://ai-isl.github.io/A2D", "summary": "Diffusion large language models (dLLMs) enable any-order generation, but this\nflexibility enlarges the attack surface: harmful spans may appear at arbitrary\npositions, and template-based prefilling attacks such as DIJA bypass\nresponse-level refusals. We introduce A2D (Any-Order, Any-Step Defense), a\ntoken-level alignment method that aligns dLLMs to emit an [EOS] refusal signal\nwhenever harmful content arises. By aligning safety directly at the token-level\nunder randomized masking, A2D achieves robustness to both any-decoding-order\nand any-step prefilling attacks under various conditions. It also enables\nreal-time monitoring: dLLMs may begin a response but automatically terminate if\nunsafe continuation emerges. On safety benchmarks, A2D consistently prevents\nthe generation of harmful outputs, slashing DIJA success rates from over 80% to\nnear-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B), and\nthresholded [EOS] probabilities allow early rejection, yielding up to 19.3x\nfaster safe termination.", "AI": {"tldr": "A2D is a token-level alignment method that makes diffusion LLMs emit [EOS] refusal signals when harmful content appears, providing robust defense against any-order generation attacks and enabling real-time safety monitoring.", "motivation": "Diffusion LLMs' any-order generation flexibility creates security vulnerabilities where harmful content can appear at arbitrary positions, and existing template-based prefilling attacks like DIJA can bypass response-level refusal mechanisms.", "method": "A2D aligns dLLMs at token-level using randomized masking to emit [EOS] refusal signals whenever harmful content arises, making the defense robust to any decoding order and any-step prefilling attacks.", "result": "A2D slashed DIJA success rates from over 80% to near-zero (1.3% on LLaDA-8B-Instruct, 0.0% on Dream-v0-Instruct-7B) and enables early rejection with up to 19.3x faster safe termination through thresholded [EOS] probabilities.", "conclusion": "Token-level alignment with randomized masking provides effective defense against any-order generation attacks in diffusion LLMs, enabling robust safety mechanisms and real-time monitoring capabilities."}}
{"id": "2509.23564", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23564", "abs": "https://arxiv.org/abs/2509.23564", "authors": ["Min-Hsuan Yeh", "Yixuan Li"], "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment", "comment": "NeurIPS 2025", "summary": "Human feedback plays a pivotal role in aligning large language models (LLMs)\nwith human preferences. However, such feedback is often noisy or inconsistent,\nwhich can degrade the quality of reward models and hinder alignment. While\nvarious automated data cleaning methods have been proposed to mitigate this\nissue, a systematic evaluation of their effectiveness and generalizability\nremains lacking. To bridge this gap, we introduce the first comprehensive\nbenchmark for evaluating 13 preference data cleaning methods in the context of\nLLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning\nstrategies in terms of alignment performance and generalizability across\ndiverse datasets, model architectures, and optimization algorithms. By unifying\ndisparate methods and rigorously comparing them, we uncover key factors that\ndetermine the success of data cleaning in alignment tasks. This benchmark lays\nthe groundwork for principled and reproducible approaches to improving LLM\nalignment through better data quality-highlighting the crucial but\nunderexplored role of data preprocessing in responsible AI development. We\nrelease modular implementations of all methods to catalyze further research:\nhttps://github.com/deeplearning-wisc/PrefCleanBench.", "AI": {"tldr": "PrefCleanBench is the first comprehensive benchmark for evaluating 13 preference data cleaning methods in LLM alignment, providing standardized assessment of cleaning strategies across diverse datasets, models, and algorithms.", "motivation": "Human feedback for LLM alignment is often noisy and inconsistent, degrading reward model quality. Existing automated data cleaning methods lack systematic evaluation of their effectiveness and generalizability.", "method": "Introduces PrefCleanBench with standardized protocol to assess 13 preference data cleaning methods across diverse datasets, model architectures, and optimization algorithms.", "result": "The benchmark uncovers key factors determining data cleaning success in alignment tasks and provides modular implementations of all methods for further research.", "conclusion": "PrefCleanBench establishes groundwork for principled approaches to improve LLM alignment through better data quality, highlighting the crucial role of data preprocessing in responsible AI development."}}
{"id": "2509.24623", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24623", "abs": "https://arxiv.org/abs/2509.24623", "authors": ["Carlos Benitez"], "title": "Mapping Quantum Threats: An Engineering Inventory of Cryptographic Dependencies", "comment": "13 pages, to be submitted to IEEE Privacy and Security", "summary": "The emergence of large-scale quantum computers, powered by algorithms like\nShor's and Grover's, poses an existential threat to modern public-key\ncryptography. This vulnerability stems from the ability of these machines to\nefficiently solve the hard mathematical problems - such as integer\nfactorization and the elliptic curve discrete logarithm problem - that underpin\nwidely used cryptographic primitives. This includes RSA, Diffie-Hellman (DH),\nElliptic Curve Diffie-Hellman (ECDH), and Elliptic Curve Digital Signature\nAlgorithm (ECDSA), which are foundational to security across the digital\necosystem. Once Shor's algorithm becomes practically realizable, these\nprimitives will fail, undermining both retrospective confidentiality and\ncryptographic authenticity - enabling adversaries to decrypt previously\ncaptured communications and forge digital signatures. This paper presents a\nsystematic inventory of technologies exposed to quantum threats from the\nengineering perspective, organized by both technology domain and by\nimplementation environment. While prior research has emphasized theoretical\nbreaks or protocol-level adaptations, this work focuses on the practical\nlandscape - mapping quantum-vulnerable systems across diverse digital\ninfrastructures. The contribution is a cross-domain, cross-environment threat\nmap to guide practitioners, vendors, and policymakers in identifying exposed\ntechnologies before the arrival of cryptographically relevant quantum\ncomputers.", "AI": {"tldr": "This paper systematically maps quantum-vulnerable cryptographic technologies across digital infrastructures, providing a practical threat assessment to guide stakeholders in preparing for cryptographically relevant quantum computers.", "motivation": "Large-scale quantum computers pose an existential threat to modern public-key cryptography by efficiently breaking foundational algorithms like RSA, Diffie-Hellman, and elliptic curve cryptography through Shor's and Grover's algorithms.", "method": "The paper conducts a systematic inventory of technologies exposed to quantum threats from an engineering perspective, organized by technology domain and implementation environment, focusing on practical landscape mapping rather than theoretical breaks.", "result": "A cross-domain, cross-environment threat map is created that identifies quantum-vulnerable systems across diverse digital infrastructures, including RSA, DH, ECDH, and ECDSA cryptographic primitives.", "conclusion": "The work provides practitioners, vendors, and policymakers with a comprehensive guide to identify exposed technologies before the arrival of cryptographically relevant quantum computers, enabling proactive security measures."}}
{"id": "2509.23101", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23101", "abs": "https://arxiv.org/abs/2509.23101", "authors": ["M. Z. Haider", "Tayyaba Noreen", "M. Salman"], "title": "Towards Quantum-Ready Blockchain Fraud Detection via Ensemble Graph Neural Networks", "comment": null, "summary": "Blockchain Business applications and cryptocurrencies such as enable secure,\ndecentralized value transfer, yet their pseudonymous nature creates\nopportunities for illicit activity, challenging regulators and exchanges in\nanti money laundering (AML) enforcement. Detecting fraudulent transactions in\nblockchain networks requires models that can capture both structural and\ntemporal dependencies while remaining resilient to noise, imbalance, and\nadversarial behavior. In this work, we propose an ensemble framework that\nintegrates Graph Convolutional Networks (GCN), Graph Attention Networks (GAT),\nand Graph Isomorphism Networks (GIN) to enhance blockchain fraud detection.\nUsing the real-world Elliptic dataset, our tuned soft voting ensemble achieves\nhigh recall of illicit transactions while maintaining a false positive rate\nbelow 1%, beating individual GNN models and baseline methods. The modular\narchitecture incorporates quantum-ready design hooks, allowing seamless future\nintegration of quantum feature mappings and hybrid quantum classical graph\nneural networks. This ensures scalability, robustness, and long-term\nadaptability as quantum computing technologies mature. Our findings highlight\nensemble GNNs as a practical and forward-looking solution for real-time\ncryptocurrency monitoring, providing both immediate AML utility and a pathway\ntoward quantum-enhanced financial security analytics.", "AI": {"tldr": "Proposed ensemble framework combining GCN, GAT, and GIN for blockchain fraud detection, achieving high recall with low false positives on Elliptic dataset, with quantum-ready design for future scalability.", "motivation": "Blockchain's pseudonymous nature enables illicit activities, challenging AML enforcement; need for models capturing structural and temporal dependencies while being resilient to noise and adversarial behavior.", "method": "Ensemble framework integrating Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and Graph Isomorphism Networks (GIN) with tuned soft voting, using Elliptic dataset.", "result": "Achieved high recall of illicit transactions while maintaining false positive rate below 1%, outperforming individual GNN models and baseline methods.", "conclusion": "Ensemble GNNs provide practical solution for real-time cryptocurrency monitoring with immediate AML utility and pathway toward quantum-enhanced financial security analytics."}}
{"id": "2509.23291", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23291", "abs": "https://arxiv.org/abs/2509.23291", "authors": ["Joseph Marvin Imperial", "Harish Tayyar Madabushi"], "title": "Scaling Policy Compliance Assessment in Language Models with Policy Reasoning Traces", "comment": null, "summary": "Policy compliance assessment is a fundamental task of evaluating whether an\ninput case strictly complies with a set of human-defined rules, more generally\nknown as policies. In practice, human experts follow a systematic, step-by-step\nprocess to identify violations with respect to specific stipulations outlined\nin the policy. However, such documentation of gold-standard, expert-level\nreasoning processes is costly to acquire. In this paper, we introduce Policy\nReasoning Traces (PRT), a form of specialized generated reasoning chains that\nserve as a reasoning bridge to improve an LLM's policy compliance assessment\ncapabilities. Our empirical evaluations demonstrate that the use of PRTs for\nboth inference-time and training-time scenarios significantly enhances the\nperformance of open-weight and commercial models, setting a new\nstate-of-the-art for HIPAA and GDPR policies. Beyond accuracy gains, we also\nhighlight how PRTs can improve an LLM's ability to accurately cite policy\nclauses, as well as influence compliance decisions through their high\nutilization from the raw chains of thought.", "AI": {"tldr": "Policy Reasoning Traces (PRT) are specialized reasoning chains that improve LLM performance in policy compliance assessment, achieving state-of-the-art results for HIPAA and GDPR policies.", "motivation": "Human experts follow systematic processes for policy compliance assessment, but documenting gold-standard reasoning is costly. There's a need to enhance LLM capabilities in this domain.", "method": "Introduce Policy Reasoning Traces (PRT) - specialized generated reasoning chains that serve as a reasoning bridge to improve LLM's policy compliance assessment capabilities. Use PRTs for both inference-time and training-time scenarios.", "result": "Significantly enhances performance of open-weight and commercial models, setting new state-of-the-art for HIPAA and GDPR policies. Improves LLM's ability to accurately cite policy clauses and influences compliance decisions through high utilization from raw chains of thought.", "conclusion": "PRTs effectively bridge the reasoning gap in policy compliance assessment, providing substantial improvements in accuracy and citation capabilities for LLMs across different policy domains."}}
{"id": "2509.23589", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23589", "abs": "https://arxiv.org/abs/2509.23589", "authors": ["Shu Liu", "Wenlin Chen", "Weihao Li", "Zheng Wang", "Lijin Yang", "Jianing Huang", "Yipin Zhang", "Zhongzhan Huang", "Ze Cheng", "Hao Yang"], "title": "BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving", "comment": "16 pages, 7 figures, 6 tables", "summary": "Diffusion-based planners have shown great promise for autonomous driving due\nto their ability to capture multi-modal driving behaviors. However, guiding\nthese models effectively in reactive, closed-loop environments remains a\nsignificant challenge. Simple conditioning often fails to provide sufficient\nguidance in complex and dynamic driving scenarios. Recent work attempts to use\ntypical expert driving behaviors (i.e., anchors) to guide diffusion models but\nrelies on a truncated schedule, which introduces theoretical inconsistencies\nand can compromise performance. To address this, we introduce BridgeDrive, a\nnovel anchor-guided diffusion bridge policy for closed-loop trajectory\nplanning. Our approach provides a principled diffusion framework that\neffectively translates anchors into fine-grained trajectory plans,\nappropriately responding to varying traffic conditions. Our planner is\ncompatible with efficient ODE solvers, a critical factor for real-time\nautonomous driving deployment. We achieve state-of-the-art performance on the\nBench2Drive benchmark, improving the success rate by 5% over prior arts.", "AI": {"tldr": "BridgeDrive is a novel anchor-guided diffusion bridge policy for closed-loop trajectory planning in autonomous driving, achieving state-of-the-art performance with 5% improvement in success rate on Bench2Drive benchmark.", "motivation": "Existing diffusion-based planners struggle with effective guidance in reactive, closed-loop driving environments. Simple conditioning fails in complex scenarios, while recent anchor-based approaches rely on truncated schedules that introduce theoretical inconsistencies and performance issues.", "method": "BridgeDrive uses a principled diffusion framework that translates driving behavior anchors into fine-grained trajectory plans, responding appropriately to varying traffic conditions. It's compatible with efficient ODE solvers for real-time deployment.", "result": "Achieved state-of-the-art performance on Bench2Drive benchmark with 5% improvement in success rate over prior methods.", "conclusion": "BridgeDrive provides an effective solution for guiding diffusion models in closed-loop autonomous driving scenarios, addressing theoretical limitations of previous approaches while maintaining real-time compatibility."}}
{"id": "2509.24624", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24624", "abs": "https://arxiv.org/abs/2509.24624", "authors": ["Thomas Fargues", "Ye Dong", "Tianwei Zhang", "Jin-Song Dong"], "title": "PRIVMARK: Private Large Language Models Watermarking with MPC", "comment": "8 pages, 4 figures, under peer-review", "summary": "The rapid growth of Large Language Models (LLMs) has highlighted the pressing\nneed for reliable mechanisms to verify content ownership and ensure\ntraceability. Watermarking offers a promising path forward, but it remains\nlimited by privacy concerns in sensitive scenarios, as traditional approaches\noften require direct access to a model's parameters or its training data. In\nthis work, we propose a secure multi-party computation (MPC)-based private LLMs\nwatermarking framework, PRIVMARK, to address the concerns. Concretely, we\ninvestigate PostMark (EMNLP'2024), one of the state-of-the-art LLMs\nWatermarking methods, and formulate its basic operations. Then, we construct\nefficient protocols for these operations using the MPC primitives in a\nblack-box manner. In this way, PRIVMARK enables multiple parties to\ncollaboratively watermark an LLM's output without exposing the model's weights\nto any single computing party. We implement PRIVMARK using SecretFlow-SPU\n(USENIX ATC'2023) and evaluate its performance using the ABY3 (CCS'2018)\nbackend. The experimental results show that PRIVMARK achieves semantically\nidentical results compared to the plaintext baseline without MPC and is\nresistant against paraphrasing and removing attacks with reasonable efficiency.", "AI": {"tldr": "PRIVMARK is a secure multi-party computation (MPC)-based private watermarking framework for LLMs that enables collaborative watermarking without exposing model weights, achieving semantic equivalence and attack resistance.", "motivation": "Address privacy concerns in traditional LLM watermarking approaches that require direct access to model parameters or training data, particularly in sensitive scenarios.", "method": "Formulates operations of PostMark (state-of-the-art LLM watermarking), constructs efficient MPC protocols for these operations using SecretFlow-SPU with ABY3 backend, enabling black-box collaborative watermarking.", "result": "Achieves semantically identical results compared to plaintext baseline, resistant against paraphrasing and removing attacks with reasonable efficiency.", "conclusion": "PRIVMARK provides a practical solution for private LLM watermarking through secure multi-party computation, balancing privacy protection with watermarking effectiveness."}}
{"id": "2509.23106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23106", "abs": "https://arxiv.org/abs/2509.23106", "authors": ["Aman Gupta", "Rafael Celente", "Abhishek Shivanna", "D. T. Braithwaite", "Gregory Dexter", "Shao Tang", "Hiroto Udagawa", "Daniel Silva", "Rohan Ramanath", "S. Sathiya Keerthi"], "title": "Effective Quantization of Muon Optimizer States", "comment": "17 pages", "summary": "The Muon optimizer, based on matrix orthogonalization, has recently shown\nfaster convergence and up to 2x computational efficiency over AdamW in LLM\npretraining. Like AdamW, Muon is stateful, requiring storage of both model\nweights and accumulated gradients. While 8-bit AdamW variants mitigate this\noverhead using blockwise quantization, they are typically stable only under\ndynamic quantization - which improves stability on linear quantization for\nextreme values. In this paper, we introduce the 8-bit Muon optimizer using\nblockwise quantization, supporting both linear and dynamic schemes. We\ndemonstrate that 8-bit Muon maintains stability under both, while delivering\n$\\sim$74\\% reduction in memory footprint compared to full-precision Muon. In\nextensive experiments, 8-bit Muon closely matches the performance of Muon while\noutperforming AdamW and 8-bit AdamW in pre-training a 1.6B model on 4B FineWeb\ntokens. It also shows competitive results when fine-tuning the Llama 3.2 3B\nmodel on post-training data. We also provide a theoretical perspective to help\nexplain this robustness under quantization.", "AI": {"tldr": "8-bit Muon optimizer using blockwise quantization reduces memory footprint by ~74% while maintaining performance comparable to full-precision Muon, outperforming AdamW and 8-bit AdamW in LLM pretraining.", "motivation": "Muon optimizer shows better efficiency than AdamW but is stateful like AdamW, requiring significant memory for storing model weights and accumulated gradients. Existing 8-bit AdamW variants use dynamic quantization for stability, but the authors aim to develop a quantized Muon optimizer that works with both linear and dynamic quantization schemes.", "method": "Introduce 8-bit Muon optimizer using blockwise quantization, supporting both linear and dynamic quantization schemes. Provide theoretical analysis to explain the robustness under quantization.", "result": "8-bit Muon maintains stability under both linear and dynamic quantization, achieving ~74% memory reduction compared to full-precision Muon. In experiments with 1.6B model on 4B FineWeb tokens, it matches Muon performance while outperforming AdamW and 8-bit AdamW. Also shows competitive results when fine-tuning Llama 3.2 3B model.", "conclusion": "8-bit Muon optimizer successfully reduces memory overhead while maintaining training performance, making it a practical and efficient alternative to existing optimizers for large language model training."}}
{"id": "2509.23330", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23330", "abs": "https://arxiv.org/abs/2509.23330", "authors": ["Peng Yu", "Zeyuan Zhao", "Shao Zhang", "Luoyi Fu", "Xinbing Wang", "Ying Wen"], "title": "Learning to Reason in Structured In-context Environments with Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) have achieved significant advancements in\nreasoning capabilities through reinforcement learning (RL) via environmental\nexploration. As the intrinsic properties of the environment determine the\nabilities that LLMs can learn, the environment plays a important role in the RL\nfinetuning process. An ideal LLM reasoning environment should possess three\ncore characteristics: scalability, generalizable reasoning, and verifiability.\nHowever, existing mathematical and coding environments are difficult to scale\ndue to heavy reliance on expert annotation, while the skills learned in\ngame-based environments are too specialized to generalize. To bridge this gap,\nwe introduce the \\textbf{S}tructured \\textbf{I}n-context \\textbf{E}nvironment\n(SIE) framework. SIE achieves scalability by automatically constructing\nreasoning environments from large-scale structured data, where the rich\ncompositional patterns naturally support generalizable reasoning. Moreover, the\nexplicit schemas and reasoning chains in structured data provide a foundation\nfor rule-based verifiability. Experimental results show that SIE framework not\nonly achieves substantial improvements in in-domain structured reasoning, but\nalso enables the learned compositional reasoning skills to generalize\neffectively to out-of-domain mathematical and logical reasoning tasks. We\nfurther explored learning in information-limited partial SIEs and found that\nLLMs can infer the missing information through exploring the environment,\nleading to robust reasoning improvements and generalization performance.", "AI": {"tldr": "SIE framework creates scalable reasoning environments from structured data to enhance LLMs' compositional reasoning skills with verifiability.", "motivation": "Existing reasoning environments lack scalability and generalizability - mathematical/coding environments need expert annotation, while game-based environments produce specialized skills.", "method": "Automatically construct reasoning environments from large-scale structured data using rich compositional patterns, with explicit schemas and reasoning chains for rule-based verification.", "result": "Substantial improvements in in-domain structured reasoning and effective generalization to out-of-domain mathematical and logical reasoning tasks; robust performance even in information-limited partial SIEs.", "conclusion": "SIE framework successfully addresses scalability, generalizability, and verifiability challenges in LLM reasoning environments, enabling effective compositional reasoning skill learning and transfer."}}
{"id": "2509.23614", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23614", "abs": "https://arxiv.org/abs/2509.23614", "authors": ["Yaozu Wu", "Jizhou Guo", "Dongyuan Li", "Henry Peng Zou", "Wei-Chieh Huang", "Yankai Chen", "Zhen Wang", "Weizhi Zhang", "Yangning Li", "Meng Zhang", "Renhe Jiang", "Philip S. Yu"], "title": "PSG-Agent: Personality-Aware Safety Guardrail for LLM-based Agents", "comment": null, "summary": "Effective guardrails are essential for safely deploying LLM-based agents in\ncritical applications. Despite recent advances, existing guardrails suffer from\ntwo fundamental limitations: (i) they apply uniform guardrail policies to all\nusers, ignoring that the same agent behavior can harm some users while being\nsafe for others; (ii) they check each response in isolation, missing how risks\nevolve and accumulate across multiple interactions. To solve these issues, we\npropose PSG-Agent, a personalized and dynamic system for LLM-based agents.\nFirst, PSG-Agent creates personalized guardrails by mining the interaction\nhistory for stable traits and capturing real-time states from current queries,\ngenerating user-specific risk thresholds and protection strategies. Second,\nPSG-Agent implements continuous monitoring across the agent pipeline with\nspecialized guards, including Plan Monitor, Tool Firewall, Response Guard,\nMemory Guardian, that track cross-turn risk accumulation and issue verifiable\nverdicts. Finally, we validate PSG-Agent in multiple scenarios including\nhealthcare, finance, and daily life automation scenarios with diverse user\nprofiles. It significantly outperform existing agent guardrails including\nLlamaGuard3 and AGrail, providing an executable and auditable path toward\npersonalized safety for LLM-based agents.", "AI": {"tldr": "PSG-Agent is a personalized and dynamic safety system for LLM-based agents that addresses limitations of uniform guardrail policies and isolated response checking by creating user-specific risk thresholds and implementing continuous monitoring across agent interactions.", "motivation": "Existing guardrails apply uniform policies to all users and check responses in isolation, ignoring that the same behavior can harm some users while being safe for others, and missing how risks evolve across multiple interactions.", "method": "PSG-Agent creates personalized guardrails by mining interaction history for stable traits and real-time states, and implements continuous monitoring with specialized guards (Plan Monitor, Tool Firewall, Response Guard, Memory Guardian) that track cross-turn risk accumulation.", "result": "PSG-Agent significantly outperforms existing agent guardrails including LlamaGuard3 and AGrail in multiple scenarios including healthcare, finance, and daily life automation with diverse user profiles.", "conclusion": "PSG-Agent provides an executable and auditable path toward personalized safety for LLM-based agents by addressing fundamental limitations of current guardrail systems."}}
{"id": "2509.24698", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24698", "abs": "https://arxiv.org/abs/2509.24698", "authors": ["Izaiah Sun", "Daniel Tan", "Andy Deng"], "title": "LISA Technical Report: An Agentic Framework for Smart Contract Auditing", "comment": "A technical report with 10 pages", "summary": "We present LISA, an agentic smart contract vulnerability detection framework\nthat combines rule-based and logic-based methods to address a broad spectrum of\nvulnerabilities in smart contracts. LISA leverages data from historical audit\nreports to learn the detection experience (without model fine-tuning), enabling\nit to generalize learned patterns to unseen projects and evolving threat\nprofiles. In our evaluation, LISA significantly outperforms both LLM-based\napproaches and traditional static analysis tools, achieving superior coverage\nof vulnerability types and higher detection accuracy. Our results suggest that\nLISA offers a compelling solution for industry: delivering more reliable and\ncomprehensive vulnerability detection while reducing the dependence on manual\neffort.", "AI": {"tldr": "LISA is an agentic smart contract vulnerability detection framework that combines rule-based and logic-based methods, leveraging historical audit data to detect vulnerabilities without model fine-tuning, outperforming both LLM-based approaches and traditional static analysis tools.", "motivation": "To address the broad spectrum of vulnerabilities in smart contracts and reduce dependence on manual effort by creating a more reliable and comprehensive detection solution.", "method": "Combines rule-based and logic-based methods, leverages data from historical audit reports to learn detection experience without model fine-tuning, enabling generalization to unseen projects and evolving threats.", "result": "Significantly outperforms both LLM-based approaches and traditional static analysis tools, achieving superior coverage of vulnerability types and higher detection accuracy.", "conclusion": "LISA offers a compelling industry solution for more reliable and comprehensive vulnerability detection while reducing manual effort dependence."}}
{"id": "2509.23115", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23115", "abs": "https://arxiv.org/abs/2509.23115", "authors": ["Haoyu He", "Haozheng Luo", "Yan Chen", "Qi R. Wang"], "title": "RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility", "comment": "Advances in Neural Information Processing Systems 39 (NeurIPS) 2025", "summary": "Predicting human mobility is inherently challenging due to complex long-range\ndependencies and multi-scale periodic behaviors. To address this, we introduce\nRHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility),\na unified framework that leverages large language models (LLMs) as\ngeneral-purpose spatio-temporal predictors and trajectory reasoners.\nMethodologically, RHYTHM employs temporal tokenization to partition each\ntrajectory into daily segments and encode them as discrete tokens with\nhierarchical attention that captures both daily and weekly dependencies,\nthereby significantly reducing the sequence length while preserving cyclical\ninformation. Additionally, we enrich token representations by adding\npre-computed prompt embeddings for trajectory segments and prediction targets\nvia a frozen LLM, and feeding these combined embeddings back into the LLM\nbackbone to capture complex interdependencies. Computationally, RHYTHM freezes\nthe pretrained LLM's backbone to reduce attention complexity and memory cost.\nWe evaluate our model against state-of-the-art methods using three real-world\ndatasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a\n5.0% increase on weekends, and a 24.6% reduction in training time. Code is\npublicly available at https://github.com/he-h/rhythm.", "AI": {"tldr": "RHYTHM is a framework using LLMs for human mobility prediction with temporal tokenization and hierarchical attention to handle long-range dependencies and multi-scale periodic behaviors.", "motivation": "Human mobility prediction is challenging due to complex long-range dependencies and multi-scale periodic behaviors that existing methods struggle to capture effectively.", "method": "Uses temporal tokenization to partition trajectories into daily segments, encodes them as discrete tokens with hierarchical attention for daily/weekly dependencies, and enriches tokens with pre-computed prompt embeddings via frozen LLM.", "result": "Achieves 2.4% improvement in overall accuracy, 5.0% increase on weekends, and 24.6% reduction in training time compared to state-of-the-art methods on three real-world datasets.", "conclusion": "RHYTHM effectively leverages LLMs as spatio-temporal predictors with temporal tokenization and hierarchical attention, demonstrating significant improvements in accuracy and efficiency for human mobility prediction."}}
{"id": "2509.23331", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23331", "abs": "https://arxiv.org/abs/2509.23331", "authors": ["Tiancheng Li", "Yuhang Wang", "Zhiyang Chen", "Zijun Wang", "Liyuan Ma", "Guo-jun Qi"], "title": "C-Evolve: Consensus-based Evolution for Prompt Groups", "comment": "70 pages,7 figures", "summary": "Prompt evolution algorithms offer a powerful paradigm for enhancing AI\nsystems based on closed-source models, while few work explores whether\naggregating results from multiple prompts to reach a consensus can further\nadvance the system capability boundary. In this paper, we introduce\nConsensus-Evolve (C-Evolve), an evolutionary algorithm that discovers a group\nof prompts whose aggregated outputs after majority voting achieve optimal\nperformance. More specifically, C-Evolve employs an island-based evolutionary\nalgorithm to maintain population diversity, and prompts from distinct islands\nare selected to form groups to aggregate their outputs. The key difference from\nsingle individual evolution is a voting score, which evaluates each individual\nprompt's contribution within groups. We take this as the fitness score for\nevolution instead of individual performance. Consequently, C-Evolve is more\nlikely to produce and maintain prompts with higher potential to form a\nhigh-performing group and eliminate low-performing ones, gradually improving\nthe group performance after reaching consensus. Our method achieves\nstate-of-the-art performance across a wide range of tasks, including both\nopen-ended tasks like HotpotQA and closed-ended tasks like MATH. On Qwen3-8B,\nC-Evolve achieves 70.67% on HotpotQA and 43.88% on IFBench, which are 4.95% and\n2.73% higher than GEPA, respectively. For GPT-4.1-mini, the accuracy on IFBench\nis further improved to 47.96% and reaches 95.33% in the MATH benchmark. These\nresults demonstrate the C-Evolve's competitive performance.", "AI": {"tldr": "C-Evolve is an evolutionary algorithm that discovers groups of prompts whose aggregated outputs achieve optimal performance through majority voting, outperforming individual prompt evolution methods.", "motivation": "Few works explore whether aggregating results from multiple prompts to reach consensus can further advance AI system capabilities beyond single prompt evolution.", "method": "Uses island-based evolutionary algorithm to maintain population diversity, selects prompts from distinct islands to form groups, and evaluates individual prompts' contribution within groups using voting score as fitness metric.", "result": "Achieves state-of-the-art performance: 70.67% on HotpotQA and 43.88% on IFBench with Qwen3-8B (4.95% and 2.73% higher than GEPA), and 47.96% on IFBench and 95.33% on MATH with GPT-4.1-mini.", "conclusion": "C-Evolve demonstrates competitive performance by producing prompts with higher potential to form high-performing groups through consensus-based evolution."}}
{"id": "2509.23619", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23619", "abs": "https://arxiv.org/abs/2509.23619", "authors": ["Xiangyu Wen", "Junhua Huang", "Zeju Li", "Min Li", "Jianyuan Zhong", "Zhijian Xu", "Mingxuan Yuan", "Yongxiang Huang", "Qiang Xu"], "title": "Reasoning Scaffolding: Distilling the Flow of Thought from LLMs", "comment": null, "summary": "The prevailing approach to distilling reasoning from Large Language Models\n(LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It\nteaches Small Language Models (SLMs) to mimic surface-level patterns rather\nthan the underlying algorithmic structure of thought, resulting in a critical\nlack of logical robustness. We argue that instead of cloning text, distillation\nshould transfer this algorithmic structure directly. We introduce Reasoning\nScaffolding}, a framework that reframes reasoning as a structured generation\nprocess. Our method first abstracts the teacher's thought process into a\nsequence of discrete, interpretable semantic signals (e.g., Contrast, Addition)\nthat act as a scaffold. The student model is then trained via a multi-task\nobjective to both (1)predict the next semantic signal, anticipating the\nreasoning flow, and (2)generate the corresponding step, conditioned on that\nsignal. This multi-task scheme acts as a powerful regularizer, compelling the\nstudent to internalize the computational patterns of coherent reasoning. On a\nsuite of challenging reasoning benchmarks, our method significantly outperforms\nstate-of-the-art distillation in both accuracy and logical consistency,\nproviding a path towards creating smaller models that are genuine reasoners,\nnot just fluent mimics.", "AI": {"tldr": "The paper proposes Reasoning Scaffolding, a framework that distills algorithmic reasoning structure from LLMs to SLMs using semantic signals as scaffolds, outperforming traditional behavioral cloning methods.", "motivation": "Traditional behavioral cloning from textual rationales teaches SLMs to mimic surface patterns rather than underlying algorithmic reasoning structure, leading to poor logical robustness.", "method": "Abstract teacher's thought process into discrete semantic signals, train student model with multi-task objective: predict next semantic signal and generate corresponding step conditioned on that signal.", "result": "Significantly outperforms state-of-the-art distillation methods on challenging reasoning benchmarks in both accuracy and logical consistency.", "conclusion": "Provides a path towards creating smaller models that are genuine reasoners rather than just fluent mimics by transferring algorithmic reasoning structure directly."}}
{"id": "2509.24807", "categories": ["cs.CR", "K.6.5"], "pdf": "https://arxiv.org/pdf/2509.24807", "abs": "https://arxiv.org/abs/2509.24807", "authors": ["Dong Hyun Roh", "Rajesh Kumar"], "title": "Active Authentication via Korean Keystrokes Under Varying LLM Assistance and Cognitive Contexts", "comment": "Accepted for publication at IEEE-ICMLA 2025. Contains nine pages, six\n  figures, and two tables", "summary": "Keystroke dynamics is a promising modality for active user authentication,\nbut its effectiveness under varying LLM-assisted typing and cognitive\nconditions remains understudied. Using data from 50 users and cognitive labels\nfrom Bloom's Taxonomy, we evaluate keystroke-based authentication in Korean\nacross three realistic typing scenarios: bona fide composition, LLM content\nparaphrasing, and transcription. Our pipeline incorporates continuity-aware\nsegmentation, feature extraction, and classification via SVM, MLP, and XGB.\nResults show that the system maintains reliable performance across varying LLM\nusages and cognitive contexts, with Equal Error Rates ranging from 5.1% to\n10.4%. These findings demonstrate the feasibility of behavioral authentication\nunder modern writing conditions and offer insights into designing more\ncontext-resilient models.", "AI": {"tldr": "Keystroke dynamics authentication maintains reliable performance (5.1-10.4% EER) across different LLM-assisted typing scenarios and cognitive contexts in Korean language.", "motivation": "To evaluate keystroke-based authentication effectiveness under varying LLM-assisted typing and cognitive conditions, which remains understudied despite keystroke dynamics being a promising modality for active user authentication.", "method": "Used data from 50 users with cognitive labels from Bloom's Taxonomy, evaluated across three typing scenarios: bona fide composition, LLM content paraphrasing, and transcription. Implemented pipeline with continuity-aware segmentation, feature extraction, and classification using SVM, MLP, and XGB.", "result": "System maintained reliable performance across varying LLM usages and cognitive contexts, achieving Equal Error Rates ranging from 5.1% to 10.4%.", "conclusion": "Keystroke-based behavioral authentication is feasible under modern writing conditions with LLM assistance, providing insights for designing context-resilient authentication models."}}
{"id": "2509.23126", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23126", "abs": "https://arxiv.org/abs/2509.23126", "authors": ["Dengyi Liu", "Honggang Wang", "Hua Fang"], "title": "Impute-MACFM: Imputation based on Mask-Aware Flow Matching", "comment": "Preprint, 2025. 9 pages (main) + appendix", "summary": "Tabular data are central to many applications, especially longitudinal data\nin healthcare, where missing values are common, undermining model fidelity and\nreliability. Prior imputation methods either impose restrictive assumptions or\nstruggle with complex cross-feature structure, while recent generative\napproaches suffer from instability and costly inference. We propose\nImpute-MACFM, a mask-aware conditional flow matching framework for tabular\nimputation that addresses missingness mechanisms, missing completely at random,\nmissing at random, and missing not at random. Its mask-aware objective builds\ntrajectories only on missing entries while constraining predicted velocity to\nremain near zero on observed entries, using flexible nonlinear schedules.\nImpute-MACFM combines: (i) stability penalties on observed positions, (ii)\nconsistency regularization enforcing local invariance, and (iii) time-decayed\nnoise injection for numeric features. Inference uses constraint-preserving\nordinary differential equation integration with per-step projection to fix\nobserved values, optionally aggregating multiple trajectories for robustness.\nAcross diverse benchmarks, Impute-MACFM achieves state-of-the-art results while\ndelivering more robust, efficient, and higher-quality imputation than competing\napproaches, establishing flow matching as a promising direction for tabular\nmissing-data problems, including longitudinal data.", "AI": {"tldr": "Impute-MACFM is a mask-aware conditional flow matching framework for tabular data imputation that handles all missingness mechanisms (MCAR, MAR, MNAR) with stable training and efficient inference.", "motivation": "Tabular data, especially in healthcare, often has missing values that undermine model reliability. Existing methods either make restrictive assumptions, struggle with complex feature structures, or suffer from instability and costly inference.", "method": "Uses mask-aware conditional flow matching with trajectories only on missing entries, stability penalties on observed positions, consistency regularization, and time-decayed noise injection. Inference uses constraint-preserving ODE integration with per-step projection.", "result": "Achieves state-of-the-art results across diverse benchmarks, delivering more robust, efficient, and higher-quality imputation than competing approaches.", "conclusion": "Flow matching is established as a promising direction for tabular missing-data problems, particularly for longitudinal data."}}
{"id": "2509.23362", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23362", "abs": "https://arxiv.org/abs/2509.23362", "authors": ["Han Yan", "Zheyuan Liu", "Meng Jiang"], "title": "Dual-Space Smoothness for Robust and Balanced LLM Unlearning", "comment": "A unified framework that enforces dual-space smoothness in\n  representation and parameter spaces to improve robustness and balance\n  unlearning metrics", "summary": "With the rapid advancement of large language models, Machine Unlearning has\nemerged to address growing concerns around user privacy, copyright\ninfringement, and overall safety. Yet state-of-the-art (SOTA) unlearning\nmethods often suffer from catastrophic forgetting and metric imbalance, for\nexample by over-optimizing one objective (e.g., unlearning effectiveness,\nutility preservation, or privacy protection) at the expense of others. In\naddition, small perturbations in the representation or parameter space can be\nexploited by relearn and jailbreak attacks. To address these challenges, we\npropose PRISM, a unified framework that enforces dual-space smoothness in\nrepresentation and parameter spaces to improve robustness and balance\nunlearning metrics. PRISM consists of two smoothness optimization stages: (i) a\nrepresentation space stage that employs a robustly trained probe to defend\nagainst jailbreak attacks, and (ii) a parameter-space stage that decouples\nretain-forget gradient conflicts, reduces imbalance, and smooths the parameter\nspace to mitigate relearning attacks. Extensive experiments on WMDP and MUSE,\nacross conversational-dialogue and continuous-text settings, show that PRISM\noutperforms SOTA baselines under multiple attacks while achieving a better\nbalance among key metrics.", "AI": {"tldr": "PRISM is a unified framework for machine unlearning that enforces dual-space smoothness in representation and parameter spaces to improve robustness and balance unlearning metrics against attacks.", "motivation": "Address challenges in machine unlearning including catastrophic forgetting, metric imbalance, and vulnerability to relearn/jailbreak attacks that plague current SOTA methods.", "method": "Two-stage smoothness optimization: (1) representation space stage with robustly trained probe for jailbreak defense, (2) parameter-space stage that decouples retain-forget gradient conflicts and smooths parameter space.", "result": "Extensive experiments on WMDP and MUSE datasets show PRISM outperforms SOTA baselines under multiple attacks while achieving better balance among key metrics.", "conclusion": "PRISM provides a robust and balanced solution for machine unlearning that effectively addresses current limitations in the field."}}
{"id": "2509.23629", "categories": ["cs.AI", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.LG", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2509.23629", "abs": "https://arxiv.org/abs/2509.23629", "authors": ["Sihan Hu", "Xiansheng Cai", "Yuan Huang", "Zhiyuan Yao", "Linfeng Zhang", "Pan Zhang", "Youjin Deng", "Kun Chen"], "title": "How LLMs Learn to Reason: A Complex Network Perspective", "comment": "24 pages, 11 figures, 1 table, under review as a conference paper at\n  ICLR 2026", "summary": "Training large language models with Reinforcement Learning from Verifiable\nRewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain\npoorly understood, including a two-stage learning curve, V-shaped\nresponse-length trajectories, and a pronounced vulnerability to catastrophic\nforgetting. In this work, we propose that these seemingly disparate phenomena\ncan be explained using a single unifying theory: the model's reasoning process\nmaps to the self-organization of a semantic complex network whose topology\nremains persistently sparse, with the average degree pinned close to two. This\ntopology imposes a fundamental mechanism for forgetting and learning: it first\ndrives the system into a maximally frustrated state where ``skill islands''\nform, slow-learning happens, and forgetting is induced; then it enters a sharp\ngrowth phase where the new skills are ``bolted on'', driven by\nphase-transition-like learning at the web's frontier. Equipped with the theory,\nwe propose \\textit{Annealed-RLVR}, a principled algorithm that introduces an\nSFT-based ``heating'' step at the point of maximal frustration to resolve the\ncompetitive bottleneck and enhance the reasoning capability of the model.\nExperiments on a 1.5B-parameter model demonstrate that the approach outperforms\nstandard RLVR on both in-distribution and out-of-distribution benchmarks. By\nrecasting RLVR from black-box optimization into a predictable process of\nstructural self-organization, our work provides a new physical intuition for\nengineering the emergent reasoning capabilities of future AI systems.", "AI": {"tldr": "The paper proposes a unified theory explaining puzzling behaviors in RLVR training, identifies sparse network topology as the cause, and introduces Annealed-RLVR algorithm that outperforms standard RLVR.", "motivation": "To understand and explain the distinctive behaviors in RLVR training including two-stage learning curves, V-shaped response lengths, and catastrophic forgetting through a unifying theory.", "method": "Proposes that RLVR reasoning maps to self-organization of sparse semantic networks, and introduces Annealed-RLVR algorithm with SFT-based 'heating' step to resolve competitive bottlenecks.", "result": "Experiments on 1.5B-parameter model show Annealed-RLVR outperforms standard RLVR on both in-distribution and out-of-distribution benchmarks.", "conclusion": "The work provides a physical intuition for engineering AI reasoning capabilities by recasting RLVR from black-box optimization to predictable structural self-organization."}}
{"id": "2509.24823", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24823", "abs": "https://arxiv.org/abs/2509.24823", "authors": ["Benedetta Tondi", "Andrea Costanzo", "Mauro Barni"], "title": "Of-SemWat: High-payload text embedding for semantic watermarking of AI-generated images with arbitrary size", "comment": "5 pages, 2 figures", "summary": "We propose a high-payload image watermarking method for textual embedding,\nwhere a semantic description of the image - which may also correspond to the\ninput text prompt-, is embedded inside the image. In order to be able to\nrobustly embed high payloads in large-scale images - such as those produced by\nmodern AI generators - the proposed approach builds upon a traditional\nwatermarking scheme that exploits orthogonal and turbo codes for improved\nrobustness, and integrates frequency-domain embedding and perceptual masking\ntechniques to enhance watermark imperceptibility. Experiments show that the\nproposed method is extremely robust against a wide variety of image processing,\nand the embedded text can be retrieved also after traditional and AI\ninpainting, permitting to unveil the semantic modification the image has\nundergone via image-text mismatch analysis.", "AI": {"tldr": "A high-payload image watermarking method that embeds semantic text descriptions into AI-generated images using orthogonal/turbo codes with frequency-domain embedding and perceptual masking for robustness and imperceptibility.", "motivation": "To embed semantic descriptions of images (including text prompts) directly into AI-generated images, enabling detection of semantic modifications through text mismatch analysis.", "method": "Builds on traditional watermarking with orthogonal and turbo codes for robustness, combined with frequency-domain embedding and perceptual masking techniques for imperceptibility.", "result": "Extremely robust against various image processing operations, capable of retrieving embedded text even after traditional and AI inpainting, allowing detection of semantic modifications.", "conclusion": "The proposed method successfully embeds high-payload semantic text in AI-generated images with strong robustness and enables detection of unauthorized semantic modifications through text mismatch analysis."}}
{"id": "2509.23129", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23129", "abs": "https://arxiv.org/abs/2509.23129", "authors": ["Haotian Liu", "Shuo Wang", "Hongteng Xu"], "title": "C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning", "comment": null, "summary": "Reinforcement Learning (RL) methods, exemplified by Group Relative Policy\nOptimization (GRPO) and its variants, play a central role in developing\nreasoning models. However, these methods often suffer from a critical\noverconfidence issue, which prevents them from achieving self-aware reasoning\nmodels. In this study, we propose a simple yet effective confidence-calibration\ngroup sequence policy gradient method, called C$^2$GSPG, which simultaneously\nenhances reasoning performance while suppressing overconfidence. In principle,\nwe propose a Group Sequence Policy Gradient (GSPG) framework for learning\nreasoning models, which eliminates the token-level bias commonly appearing in\nGRPO and its variants. In this framework, we define the model confidence for\neach reasoning problem using the normalized sequence-level probability, and\nthen apply a cross-entropy regularizer to calibrate the model confidence to the\nsequence's reward. We demonstrate that the confidence calibration regularizer\nand GSPG are collaborative for binary rewards, as their objectives always share\nthe same gradient direction. For non-binary rewards, we apply nonlinear reward\nnormalization and adaptive regularizer clipping, mitigating the potential\nconflict between the two objectives. Applying C$^2$GSPG to post-train large\nlanguage models in logical and mathematical reasoning tasks, we show its\nsuperiority over state-of-the-art methods in both reasoning accuracy and\nconfidence calibration. The code of C$^2$GSPG is available at\nhttps://github.com/HaotianLiu123/CCGSPG.", "AI": {"tldr": "C\u00b2GSPG is a confidence-calibration group sequence policy gradient method that enhances reasoning performance while suppressing overconfidence in reinforcement learning models.", "motivation": "Existing RL methods like GRPO suffer from overconfidence issues that prevent achieving self-aware reasoning models, limiting their effectiveness in reasoning tasks.", "method": "Proposes Group Sequence Policy Gradient (GSPG) framework to eliminate token-level bias, defines model confidence using normalized sequence-level probability, and applies cross-entropy regularizer to calibrate confidence to reward.", "result": "C\u00b2GSPG shows superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration on logical and mathematical reasoning tasks.", "conclusion": "The proposed confidence calibration regularizer and GSPG framework are collaborative and effective for developing better self-aware reasoning models."}}
{"id": "2509.23368", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23368", "abs": "https://arxiv.org/abs/2509.23368", "authors": ["Xinchun Su", "Chunxu Luo", "Yixuan Li", "Weidong Yang", "Lipeng Ma"], "title": "MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction", "comment": null, "summary": "In the field of medicine, complex reasoning tasks such as clinical diagnosis,\ntreatment planning, and medical knowledge integration pose significant\nchallenges, where small language models often underperform compared to large\nlanguage models like GPT-4 and Deepseek. Recent knowledge distillation-based\nmethods aim to address these issues through teacher-guided error correction,\nbut this LLM as judge approach remains challenging in terms of cost, time, and\nefficiency. To circumvent this issue, we propose a novel two-stage framework,\nMedCritical, which uses a small language model fine-tuned by a large teacher\nmodel to play against itself. In the first stage, we extract high-level and\ndetailed long-chain thought templates from the teacher model to guide the\nstudent model to generate more complex reasoning thoughts. In the second stage,\nwe introduce direct preference optimization (DPO) through model self-iteration\ncollaboration to enhance the reasoning ability of the student model by playing\nagainst the correction trajectory of the fine-tuned model during training. This\nmodel self-learning DPO approach teaches the student model to use its own\nerror-driven insights to consolidate its skills and knowledge to solve complex\nproblems, and achieves comparable results to traditional knowledge distillation\nmethods using teacher models at a lower cost. Notably, our MedCritical 7B model\noutperforms the Taiyi and Huatuo-o1-7B models by 3.04\\% and 10.12\\%\nrespectively on the CMExam benchmark, achieving new SOTA performance among\n7B-class small models.", "AI": {"tldr": "MedCritical is a two-stage framework that uses self-play and DPO to enhance small language models' medical reasoning without expensive teacher models, achieving SOTA performance on CMExam benchmark.", "motivation": "Small language models underperform in complex medical reasoning tasks compared to large models, and traditional knowledge distillation methods are costly and inefficient.", "method": "Two-stage framework: 1) Extract thought templates from teacher model to guide student model, 2) Use DPO through model self-iteration collaboration where student plays against its own correction trajectory.", "result": "MedCritical 7B outperforms Taiyi and Huatuo-o1-7B by 3.04% and 10.12% respectively on CMExam benchmark, achieving new SOTA among 7B-class models.", "conclusion": "The self-learning DPO approach enables small models to achieve comparable results to traditional knowledge distillation at lower cost, effectively enhancing complex medical reasoning capabilities."}}
{"id": "2509.23630", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23630", "abs": "https://arxiv.org/abs/2509.23630", "authors": ["Yan Jiang", "Yongle Luo", "Qixian Zhou", "Elvis S. Liu"], "title": "Game-Oriented ASR Error Correction via RAG-Enhanced LLM", "comment": null, "summary": "With the rise of multiplayer online games, real-time voice communication is\nessential for team coordination. However, general ASR systems struggle with\ngaming-specific challenges like short phrases, rapid speech, jargon, and noise,\nleading to frequent errors. To address this, we propose the GO-AEC framework,\nwhich integrates large language models, Retrieval-Augmented Generation (RAG),\nand a data augmentation strategy using LLMs and TTS. GO-AEC includes data\naugmentation, N-best hypothesis-based correction, and a dynamic game knowledge\nbase. Experiments show GO-AEC reduces character error rate by 6.22% and\nsentence error rate by 29.71%, significantly improving ASR accuracy in gaming\nscenarios.", "AI": {"tldr": "GO-AEC framework improves ASR accuracy for gaming voice chat by combining LLMs, RAG, and data augmentation, reducing character error rate by 6.22% and sentence error rate by 29.71%.", "motivation": "General ASR systems perform poorly in gaming scenarios due to short phrases, rapid speech, gaming jargon, and background noise, leading to frequent recognition errors that hinder team coordination.", "method": "Proposed GO-AEC framework integrates large language models with Retrieval-Augmented Generation (RAG), uses LLMs and TTS for data augmentation, implements N-best hypothesis-based correction, and maintains a dynamic game knowledge base.", "result": "Experimental results show significant improvements: 6.22% reduction in character error rate and 29.71% reduction in sentence error rate compared to baseline ASR systems.", "conclusion": "GO-AEC effectively addresses gaming-specific ASR challenges and substantially improves speech recognition accuracy in multiplayer online gaming environments."}}
{"id": "2509.24955", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24955", "abs": "https://arxiv.org/abs/2509.24955", "authors": ["Tereza Burianov\u00e1", "Martin Pere\u0161\u00edni", "Ivan Homoliak"], "title": "Secret Leader Election in Ethereum PoS: An Empirical Security Analysis of Whisk and Homomorphic Sortition under DoS on the Leader and Censorship Attacks", "comment": null, "summary": "Proposer anonymity in Proof-of-Stake (PoS) blockchains is a critical concern\ndue to the risk of targeted attacks such as malicious denial-of-service (DoS)\nand censorship attacks. While several Secret Single Leader Election (SSLE)\nmechanisms have been proposed to address these threats, their practical impact\nand trade-offs remain insufficiently explored. In this work, we present a\nunified experimental framework for evaluating SSLE mechanisms under adversarial\nconditions, grounded in a simplified yet representative model of Ethereum's PoS\nconsensus layer. The framework includes configurable adversaries capable of\nlaunching targeted DoS and censorship attacks, including coordinated strategies\nthat simultaneously compromise groups of validators. We simulate and compare\nkey protection mechanisms - Whisk, and homomorphic sortition. To the best of\nour knowledge, this is the first comparative study to examine adversarial DoS\nscenarios involving multiple attackers under diverse protection mechanisms. Our\nresults show that while both designs offer strong protection against targeted\nDoS attacks on the leader, neither defends effectively against coordinated\nattacks on validator groups. Moreover, Whisk simplifies a DoS attack by\nnarrowing the target set from all validators to a smaller list of known\ncandidates. Homomorphic sortition, despite its theoretical strength, remains\nimpractical due to the complexity of cryptographic operations over large\nvalidator sets.", "AI": {"tldr": "This paper presents a unified experimental framework to evaluate Secret Single Leader Election (SSLE) mechanisms in PoS blockchains, comparing Whisk and homomorphic sortition under adversarial DoS and censorship attacks.", "motivation": "Proposer anonymity in PoS blockchains is crucial to prevent targeted attacks like DoS and censorship, but existing SSLE mechanisms lack practical evaluation of their trade-offs and effectiveness.", "method": "Developed a unified experimental framework with configurable adversaries capable of launching targeted DoS and censorship attacks, including coordinated strategies against validator groups, using a simplified model of Ethereum's PoS consensus layer.", "result": "Both Whisk and homomorphic sortition provide strong protection against targeted DoS attacks on individual leaders, but neither effectively defends against coordinated attacks on validator groups. Whisk simplifies DoS attacks by narrowing targets to known candidates, while homomorphic sortition remains impractical due to cryptographic complexity.", "conclusion": "Current SSLE mechanisms have limitations in defending against coordinated attacks on validator groups, highlighting the need for improved protection strategies that balance security and practicality in PoS blockchain systems."}}
{"id": "2509.23135", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23135", "abs": "https://arxiv.org/abs/2509.23135", "authors": ["Yang Chen", "Menglin Zou", "Jiaqi Zhang", "Yitan Zhang", "Junyi Yang", "Gael Gendron", "Libo Zhang", "Jiamou Liu", "Michael J. Witbrock"], "title": "Trust Region Reward Optimization and Proximal Inverse Reward Optimization Algorithm", "comment": "Accepted to NeurIPS 2025", "summary": "Inverse Reinforcement Learning (IRL) learns a reward function to explain\nexpert demonstrations. Modern IRL methods often use the adversarial (minimax)\nformulation that alternates between reward and policy optimization, which often\nlead to unstable training. Recent non-adversarial IRL approaches improve\nstability by jointly learning reward and policy via energy-based formulations\nbut lack formal guarantees. This work bridges this gap. We first present a\nunified view showing canonical non-adversarial methods explicitly or implicitly\nmaximize the likelihood of expert behavior, which is equivalent to minimizing\nthe expected return gap. This insight leads to our main contribution: Trust\nRegion Reward Optimization (TRRO), a framework that guarantees monotonic\nimprovement in this likelihood via a Minorization-Maximization process. We\ninstantiate TRRO into Proximal Inverse Reward Optimization (PIRO), a practical\nand stable IRL algorithm. Theoretically, TRRO provides the IRL counterpart to\nthe stability guarantees of Trust Region Policy Optimization (TRPO) in forward\nRL. Empirically, PIRO matches or surpasses state-of-the-art baselines in reward\nrecovery, policy imitation with high sample efficiency on MuJoCo and\nGym-Robotics benchmarks and a real-world animal behavior modeling task.", "AI": {"tldr": "This paper introduces TRRO, a stable non-adversarial IRL framework that guarantees monotonic improvement in expert behavior likelihood, with practical instantiation PIRO showing strong empirical performance.", "motivation": "Modern adversarial IRL methods suffer from unstable training, while recent non-adversarial approaches lack formal guarantees despite improved stability.", "method": "Proposes Trust Region Reward Optimization (TRRO) framework using Minorization-Maximization to guarantee monotonic improvement, instantiated as Proximal Inverse Reward Optimization (PIRO) algorithm.", "result": "PIRO matches or surpasses state-of-the-art baselines in reward recovery, policy imitation with high sample efficiency on MuJoCo, Gym-Robotics benchmarks and real-world animal behavior modeling.", "conclusion": "TRRO provides IRL counterpart to TRPO's stability guarantees in forward RL, offering both theoretical guarantees and practical performance."}}
{"id": "2509.23371", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23371", "abs": "https://arxiv.org/abs/2509.23371", "authors": ["Junming Yang", "Ning Xu", "Biao Liu", "Shiqi Qiao", "Xin Geng"], "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap between Data Generation and Preference Optimization", "comment": null, "summary": "Preference optimization is crucial for aligning large language models (LLMs)\nwith human values and intentions. A significant challenge in this process is\nthe distribution mismatch between pre-collected offline preference data and the\nevolving model policy. Existing methods attempt to reduce this gap using static\nheuristics or decoupled online sampling strategies, but they often fail to\nadapt to the model's dynamic learning state. To bridge this gap, we propose\nMeta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework\nthat dynamically couples data generation with model training. MetaAPO employs a\nlightweight meta-learner, as an \"alignment gap estimator\", to evaluate the\npotential benefits of on-policy sampling in relation to offline data. This\nguides targeted online generation and assigns sample-wise meta-weights to the\noptimization objective, dynamically balancing the quality and distribution of\nonline and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench\ndemonstrate that MetaAPO consistently outperforms existing preference\noptimization approaches across various settings, while reducing 42% in online\nannotation costs.", "AI": {"tldr": "MetaAPO is a novel preference optimization framework that dynamically couples data generation with model training using a meta-learner to estimate alignment gaps, enabling targeted online sampling and sample-wise weighting to balance online/offline data.", "motivation": "Address distribution mismatch between pre-collected offline preference data and evolving model policy, overcoming limitations of static heuristics and decoupled online sampling strategies that fail to adapt to dynamic learning states.", "method": "Uses lightweight meta-learner as 'alignment gap estimator' to evaluate benefits of on-policy sampling vs offline data, guides targeted online generation, assigns sample-wise meta-weights to optimization objective for dynamic balancing of data quality and distribution.", "result": "Outperforms existing preference optimization approaches on AlpacaEval 2, Arena-Hard and MT-Bench across various settings, while reducing 42% in online annotation costs.", "conclusion": "MetaAPO effectively bridges the distribution gap in preference optimization through dynamic coupling of data generation and training, providing superior performance with reduced annotation costs."}}
{"id": "2509.23676", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23676", "abs": "https://arxiv.org/abs/2509.23676", "authors": ["Jue Zhang", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models", "comment": "Accepted by EMNLP'25 (Main)", "summary": "Large Reasoning Models (LRMs) generate explicit reasoning traces alongside\nfinal answers, yet the extent to which these traces influence answer generation\nremains unclear. In this work, we conduct a three-stage investigation into the\ninterplay between reasoning and answer generation in three distilled DeepSeek\nR1 models. First, through empirical evaluation, we demonstrate that including\nexplicit reasoning consistently improves answer quality across diverse domains.\nSecond, attention analysis reveals that answer tokens attend substantially to\nreasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely\ntracking the reasoning trajectory, including self-reflective cues. Third, we\napply mechanistic interventions using activation patching to assess the\ndependence of answer tokens on reasoning activations. Our results show that\nperturbations to key reasoning tokens can reliably alter the final answers,\nconfirming a directional and functional flow of information from reasoning to\nanswer. These findings deepen our understanding of how LRMs leverage reasoning\ntokens for answer generation, highlighting the functional role of intermediate\nreasoning in shaping model outputs. Our data and code are publicly available at\n\\href{https://aka.ms/R2A-code}{this URL}.", "AI": {"tldr": "This paper investigates how Large Reasoning Models use explicit reasoning traces to generate answers, finding that reasoning tokens functionally influence answer generation through attention mechanisms and activation patterns.", "motivation": "To understand the extent to which explicit reasoning traces influence answer generation in Large Reasoning Models, as the relationship between reasoning and final answers remains unclear.", "method": "Three-stage investigation: 1) Empirical evaluation of reasoning inclusion effects, 2) Attention analysis identifying Reasoning-Focus Heads, 3) Mechanistic interventions using activation patching to test reasoning-answer dependence.", "result": "Explicit reasoning improves answer quality; answer tokens attend heavily to reasoning tokens; RFHs track reasoning trajectory; perturbations to reasoning tokens alter final answers, confirming directional information flow from reasoning to answer.", "conclusion": "LRMs functionally leverage reasoning tokens for answer generation, with intermediate reasoning playing a crucial role in shaping model outputs through a directional information flow."}}
{"id": "2509.24967", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24967", "abs": "https://arxiv.org/abs/2509.24967", "authors": ["Yupei Liu", "Yanting Wang", "Yuqi Jia", "Jinyuan Jia", "Neil Zhenqiang Gong"], "title": "SecInfer: Preventing Prompt Injection via Inference-time Scaling", "comment": null, "summary": "Prompt injection attacks pose a pervasive threat to the security of Large\nLanguage Models (LLMs). State-of-the-art prevention-based defenses typically\nrely on fine-tuning an LLM to enhance its security, but they achieve limited\neffectiveness against strong attacks. In this work, we propose \\emph{SecInfer},\na novel defense against prompt injection attacks built on \\emph{inference-time\nscaling}, an emerging paradigm that boosts LLM capability by allocating more\ncompute resources for reasoning during inference. SecInfer consists of two key\nsteps: \\emph{system-prompt-guided sampling}, which generates multiple responses\nfor a given input by exploring diverse reasoning paths through a varied set of\nsystem prompts, and \\emph{target-task-guided aggregation}, which selects the\nresponse most likely to accomplish the intended task. Extensive experiments\nshow that, by leveraging additional compute at inference, SecInfer effectively\nmitigates both existing and adaptive prompt injection attacks, outperforming\nstate-of-the-art defenses as well as existing inference-time scaling\napproaches.", "AI": {"tldr": "SecInfer is a novel defense against prompt injection attacks using inference-time scaling, which generates multiple responses through diverse system prompts and aggregates them to select the most secure response.", "motivation": "Prompt injection attacks are a major security threat to LLMs, and current prevention-based defenses through fine-tuning have limited effectiveness against strong attacks.", "method": "SecInfer uses inference-time scaling with two steps: system-prompt-guided sampling to generate multiple responses through diverse reasoning paths, and target-task-guided aggregation to select the response most likely to accomplish the intended task.", "result": "Extensive experiments show SecInfer effectively mitigates both existing and adaptive prompt injection attacks, outperforming state-of-the-art defenses and existing inference-time scaling approaches.", "conclusion": "By leveraging additional compute at inference time, SecInfer provides an effective defense mechanism against prompt injection attacks that surpasses current methods."}}
{"id": "2509.23139", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23139", "abs": "https://arxiv.org/abs/2509.23139", "authors": ["Sipeng Chen", "Yan Zhang", "Shibo Li"], "title": "Beyond Heuristics: Globally Optimal Configuration of Implicit Neural Representations", "comment": null, "summary": "Implicit Neural Representations (INRs) have emerged as a transformative\nparadigm in signal processing and computer vision, excelling in tasks from\nimage reconstruction to 3D shape modeling. Yet their effectiveness is\nfundamentally limited by the absence of principled strategies for optimal\nconfiguration - spanning activation selection, initialization scales,\nlayer-wise adaptation, and their intricate interdependencies. These choices\ndictate performance, stability, and generalization, but current practice relies\non ad-hoc heuristics, brute-force grid searches, or task-specific tuning, often\nleading to inconsistent results across modalities. This work introduces\nOptiINR, the first unified framework that formulates INR configuration as a\nrigorous optimization problem. Leveraging Bayesian optimization, OptiINR\nefficiently explores the joint space of discrete activation families - such as\nsinusoidal (SIREN), wavelet-based (WIRE), and variable-periodic (FINER) - and\ntheir associated continuous initialization parameters. This systematic approach\nreplaces fragmented manual tuning with a coherent, data-driven optimization\nprocess. By delivering globally optimal configurations, OptiINR establishes a\nprincipled foundation for INR design, consistently maximizing performance\nacross diverse signal processing applications.", "AI": {"tldr": "OptiINR is a unified framework that formulates INR configuration as an optimization problem using Bayesian optimization to find optimal activation functions and initialization parameters, replacing manual tuning with systematic optimization.", "motivation": "Current INR practices rely on ad-hoc heuristics and manual tuning, leading to inconsistent results across different modalities. There's a need for principled strategies to optimize INR configurations systematically.", "method": "Leverages Bayesian optimization to efficiently explore the joint space of discrete activation families (SIREN, WIRE, FINER) and their continuous initialization parameters, replacing manual tuning with data-driven optimization.", "result": "OptiINR delivers globally optimal configurations that consistently maximize performance across diverse signal processing applications, establishing a principled foundation for INR design.", "conclusion": "The framework provides a systematic approach to INR configuration that outperforms current fragmented manual tuning methods, offering consistent performance improvements across various applications."}}
{"id": "2509.23379", "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.10; J.3; I.5.4"], "pdf": "https://arxiv.org/pdf/2509.23379", "abs": "https://arxiv.org/abs/2509.23379", "authors": ["Xi Zhang", "Zaiqiao Meng", "Jake Lever", "Edmond S. L. Ho"], "title": "CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical Contrastive Decoding", "comment": "Preprint", "summary": "Multimodal large language models (MLLMs) have recently achieved remarkable\nprogress in radiology by integrating visual perception with natural language\nunderstanding. However, they often generate clinically unsupported\ndescriptions, known as medical hallucinations, which pose serious risks in\nmedical applications that demand accuracy and image-grounded outputs. Through\nempirical analysis, we find that prompt-induced hallucinations remain prevalent\nin radiology MLLMs, largely due to over-sensitivity to clinical sections. To\naddress this, we introduce Clinical Contrastive Cecoding (CCD), a training-free\nand retrieval-free inference framework that integrates structured clinical\nsignals from task-specific radiology expert models. CCD introduces a dual-stage\ncontrastive mechanism to refine token-level logits during generation, thereby\nenhancing clinical fidelity without modifying the base MLLM. Experiments on\nthree datasets and multiple models demonstrate that CCD consistently improves\noverall performance on radiology report generation (RRG). On the MIMIC-CXR\ndataset, it yields up to a 17% improvement in RadGraph-F1 when applied to\nstate-of-the-art RRG models. Our approach provides a lightweight and\ngeneralisable solution for mitigating medical hallucinations, effectively\nbridging expert models and MLLMs in radiology.", "AI": {"tldr": "CCD is a training-free framework that reduces medical hallucinations in radiology MLLMs by integrating clinical signals from expert models through dual-stage contrastive decoding.", "motivation": "MLLMs in radiology often generate clinically unsupported descriptions (medical hallucinations) due to over-sensitivity to clinical sections, posing serious risks in medical applications requiring accuracy.", "method": "Clinical Contrastive Decoding (CCD) - a training-free, retrieval-free inference framework that integrates structured clinical signals from radiology expert models using dual-stage contrastive mechanism to refine token-level logits during generation.", "result": "CCD consistently improves performance on radiology report generation across three datasets and multiple models, achieving up to 17% improvement in RadGraph-F1 on MIMIC-CXR dataset when applied to state-of-the-art RRG models.", "conclusion": "CCD provides a lightweight and generalizable solution for mitigating medical hallucinations, effectively bridging expert models and MLLMs in radiology without modifying the base MLLM."}}
{"id": "2509.23694", "categories": ["cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23694", "abs": "https://arxiv.org/abs/2509.23694", "authors": ["Jianshuo Dong", "Sheng Guo", "Hao Wang", "Zhuotao Liu", "Tianwei Zhang", "Ke Xu", "Minlie Huang", "Han Qiu"], "title": "SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents", "comment": "Preprint", "summary": "Search agents connect LLMs to the Internet, enabling access to broader and\nmore up-to-date information. However, unreliable search results may also pose\nsafety threats to end users, establishing a new threat surface. In this work,\nwe conduct two in-the-wild experiments to demonstrate both the prevalence of\nlow-quality search results and their potential to misguide agent behaviors. To\ncounter this threat, we introduce an automated red-teaming framework that is\nsystematic, scalable, and cost-efficient, enabling lightweight and harmless\nsafety assessments of search agents. Building on this framework, we construct\nthe SafeSearch benchmark, which includes 300 test cases covering five\ncategories of risks (e.g., misinformation and indirect prompt injection). Using\nthis benchmark, we evaluate three representative search agent scaffolds,\ncovering search workflow, tool-calling, and deep research, across 7 proprietary\nand 8 open-source backend LLMs. Our results reveal substantial vulnerabilities\nof LLM-based search agents: when exposed to unreliable websites, the highest\nASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover,\nour analysis highlights the limited effectiveness of common defense practices,\nsuch as reminder prompting. This emphasizes the value of our framework in\npromoting transparency for safer agent development. Our codebase and test cases\nare publicly available: https://github.com/jianshuod/SafeSearch.", "AI": {"tldr": "Search agents using LLMs are vulnerable to unreliable search results, with GPT-4.1-mini showing 90.5% attack success rate. The paper introduces SafeSearch benchmark for systematic safety assessment.", "motivation": "Search agents connect LLMs to the Internet but unreliable search results pose safety threats to users, creating a new threat surface that needs systematic evaluation.", "method": "Introduced automated red-teaming framework and SafeSearch benchmark with 300 test cases covering 5 risk categories. Evaluated 3 search agent scaffolds across 7 proprietary and 8 open-source LLMs.", "result": "Substantial vulnerabilities found: highest ASR reached 90.5% for GPT-4.1-mini. Common defenses like reminder prompting showed limited effectiveness.", "conclusion": "The framework provides systematic, scalable safety assessment for search agents, promoting transparency in safer agent development."}}
{"id": "2509.25072", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25072", "abs": "https://arxiv.org/abs/2509.25072", "authors": ["Yaman Jandali", "Ruisi Zhang", "Nojan Sheybani", "Farinaz Koushanfar"], "title": "Optimizing Privacy-Preserving Primitives to Support LLM-Scale Applications", "comment": null, "summary": "Privacy-preserving technologies have introduced a paradigm shift that allows\nfor realizable secure computing in real-world systems. The significant barrier\nto the practical adoption of these primitives is the computational and\ncommunication overhead that is incurred when applied at scale. In this paper,\nwe present an overview of our efforts to bridge the gap between this overhead\nand practicality for privacy-preserving learning systems using multi-party\ncomputation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic\nencryption (FHE). Through meticulous hardware/software/algorithm co-design, we\nshow progress towards enabling LLM-scale applications in privacy-preserving\nsettings. We demonstrate the efficacy of our solutions in several contexts,\nincluding DNN IP ownership, ethical LLM usage enforcement, and transformer\ninference.", "AI": {"tldr": "This paper presents hardware/software/algorithm co-design approaches to reduce the computational and communication overhead of privacy-preserving technologies (MPC, ZKPs, FHE) for practical adoption in learning systems, with demonstrations in DNN IP ownership, ethical LLM usage, and transformer inference.", "motivation": "The practical adoption of privacy-preserving technologies is hindered by significant computational and communication overhead when applied at scale, creating a gap between theoretical capabilities and real-world applications.", "method": "The authors employ meticulous hardware/software/algorithm co-design approaches to optimize privacy-preserving technologies including multi-party computation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic encryption (FHE).", "result": "The paper shows progress towards enabling LLM-scale applications in privacy-preserving settings and demonstrates efficacy in several contexts including DNN IP ownership, ethical LLM usage enforcement, and transformer inference.", "conclusion": "Through co-design optimization, the authors bridge the gap between overhead and practicality for privacy-preserving learning systems, making large-scale applications like LLMs feasible in privacy-preserving settings."}}
{"id": "2509.23145", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23145", "abs": "https://arxiv.org/abs/2509.23145", "authors": ["Xiaowen Ma", "Shuning Ge", "Fan Yang", "Xiangyu Li", "Yun Chen", "Mengting Ma", "Wei Zhang", "Zhipeng Liu"], "title": "TimeExpert: Boosting Long Time Series Forecasting with Temporal Mix of Experts", "comment": "Under Review", "summary": "Transformer-based architectures dominate time series modeling by enabling\nglobal attention over all timestamps, yet their rigid 'one-size-fits-all'\ncontext aggregation fails to address two critical challenges in real-world\ndata: (1) inherent lag effects, where the relevance of historical timestamps to\na query varies dynamically; (2) anomalous segments, which introduce noisy\nsignals that degrade forecasting accuracy. To resolve these problems, we\npropose the Temporal Mix of Experts (TMOE), a novel attention-level mechanism\nthat reimagines key-value (K-V) pairs as local experts (each specialized in a\ndistinct temporal context) and performs adaptive expert selection for each\nquery via localized filtering of irrelevant timestamps. Complementing this\nlocal adaptation, a shared global expert preserves the Transformer's strength\nin capturing long-range dependencies. We then replace the vanilla attention\nmechanism in popular time-series Transformer frameworks (i.e., PatchTST and\nTimer) with TMOE, without extra structural modifications, yielding our specific\nversion TimeExpert and general version TimeExpert-G. Extensive experiments on\nseven real-world long-term forecasting benchmarks demonstrate that TimeExpert\nand TimeExpert-G outperform state-of-the-art methods. Code is available at\nhttps://github.com/xwmaxwma/TimeExpert.", "AI": {"tldr": "TimeExpert proposes a Temporal Mix of Experts (TMOE) mechanism that replaces standard attention in Transformers with adaptive expert selection to address lag effects and anomalous segments in time series forecasting.", "motivation": "Standard Transformers use rigid global attention that fails to handle dynamic lag effects (varying historical relevance) and anomalous segments (noisy signals) in real-world time series data.", "method": "TMOE treats key-value pairs as local experts specialized in different temporal contexts, performs localized filtering to select relevant experts for each query, and maintains a shared global expert for long-range dependencies. Integrated into PatchTST and Timer frameworks as TimeExpert and TimeExpert-G.", "result": "Extensive experiments on 7 real-world long-term forecasting benchmarks show TimeExpert and TimeExpert-G outperform state-of-the-art methods.", "conclusion": "The proposed TMOE mechanism effectively addresses temporal challenges in time series forecasting while maintaining Transformer strengths, demonstrating superior performance across multiple benchmarks."}}
{"id": "2509.23381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23381", "abs": "https://arxiv.org/abs/2509.23381", "authors": ["Wonhyuk Lee", "Youngchol Kim", "Yunjin Park", "Junhyung Moon", "Dongyoung Jeong", "Wanjin Park"], "title": "Guard Vector: Beyond English LLM Guardrails with Task-Vector Composition and Streaming-Aware Prefix SFT", "comment": null, "summary": "We introduce Guard Vector, a safety task vector computed as the parameter\ndifference between a guardrail model (Guard Model) and a same-architecture\npretrained language model. Composing this vector with a target language model\nyields a Target Guard Model (TGM). We then adapt TGM with a streaming-aware\napproach that combines prefix-based training and evaluation with a classifier\nthat produces a single-token output. With this composition alone, TGM improves\nclassification quality over established Guard Models across standard safety\nsuites and enables language extensibility to Chinese, Japanese, and Korean,\nrequiring neither additional training nor target language labels. It also\ndemonstrates model portability across two widely used public guardrail\nbackbones, Llama and Gemma. With prefix SFT (supervised fine-tuning), TGM\npreserves classification quality under streaming by aligning the behavior\nbetween prefix inputs and full-text inputs. The single-token output design\nincreases throughput and reduces latency. Together, these components reduce\ndata and compute requirements while promoting streaming-aware evaluation\npractices, thereby contributing to a more responsible AI ecosystem.", "AI": {"tldr": "Guard Vector is a safety task vector created from parameter differences between guardrail and pretrained models. When combined with target models, it improves safety classification across multiple languages without extra training, supports streaming, and reduces latency.", "motivation": "To develop a more efficient and portable safety mechanism for language models that works across different languages and architectures while reducing computational requirements.", "method": "Compute Guard Vector as parameter difference between guardrail and pretrained models, compose with target models to create TGM, use prefix-based training with single-token classifier for streaming awareness.", "result": "TGM improves classification quality across standard safety suites, enables language extensibility to Chinese/Japanese/Korean without additional training, demonstrates portability across Llama and Gemma backbones, preserves quality under streaming, and increases throughput.", "conclusion": "The approach reduces data and compute requirements while promoting streaming-aware evaluation, contributing to more responsible AI development."}}
{"id": "2509.23717", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23717", "abs": "https://arxiv.org/abs/2509.23717", "authors": ["Claire Tian", "Katherine Tian", "Nathan Hu"], "title": "Measuring Sparse Autoencoder Feature Sensitivity", "comment": "NeurIPS 2025 Workshop on Mechanistic Interpretability Camera Ready", "summary": "Sparse Autoencoder (SAE) features have become essential tools for mechanistic\ninterpretability research. SAE features are typically characterized by\nexamining their activating examples, which are often \"monosemantic\" and align\nwith human interpretable concepts. However, these examples don't reveal feature\nsensitivity: how reliably a feature activates on texts similar to its\nactivating examples. In this work, we develop a scalable method to evaluate\nfeature sensitivity. Our approach avoids the need to generate natural language\ndescriptions for features; instead we use language models to generate text with\nthe same semantic properties as a feature's activating examples. We then test\nwhether the feature activates on these generated texts. We demonstrate that\nsensitivity measures a new facet of feature quality and find that many\ninterpretable features have poor sensitivity. Human evaluation confirms that\nwhen features fail to activate on our generated text, that text genuinely\nresembles the original activating examples. Lastly, we study feature\nsensitivity at the SAE level and observe that average feature sensitivity\ndeclines with increasing SAE width across 7 SAE variants. Our work establishes\nfeature sensitivity as a new dimension for evaluating both individual features\nand SAE architectures.", "AI": {"tldr": "Developed a scalable method to evaluate feature sensitivity in Sparse Autoencoders (SAEs) by generating semantically similar text using language models and testing feature activation, revealing that many interpretable features have poor sensitivity.", "motivation": "Current SAE feature analysis focuses on activating examples but doesn't reveal feature sensitivity - how reliably features activate on semantically similar texts, creating a gap in understanding feature quality.", "method": "Use language models to generate text with same semantic properties as feature's activating examples, then test whether features activate on these generated texts, avoiding need for natural language descriptions.", "result": "Many interpretable features have poor sensitivity; human evaluation confirms generated text genuinely resembles original examples when features fail to activate; average sensitivity declines with increasing SAE width across 7 variants.", "conclusion": "Feature sensitivity represents a new dimension for evaluating both individual features and SAE architectures, complementing existing interpretability analysis methods."}}
{"id": "2509.25113", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.25113", "abs": "https://arxiv.org/abs/2509.25113", "authors": ["Wai Ming Chan", "Remi Chou", "Taejoon Kim"], "title": "Two-Dimensional XOR-Based Secret Sharing for Layered Multipath Communication", "comment": null, "summary": "This paper introduces the first two-dimensional XOR-based secret sharing\nscheme for layered multipath communication networks. We present a construction\nthat guarantees successful message recovery and perfect privacy when an\nadversary observes and disrupts any single path at each transmission layer. The\nscheme achieves information-theoretic security using only bitwise XOR\noperations with linear $O(|S|)$ complexity, where $|S|$ is the message length.\nWe provide mathematical proofs demonstrating that the scheme maintains\nunconditional security regardless of computational resources available to\nadversaries. Unlike encryption-based approaches vulnerable to quantum computing\nadvances, our construction offers provable security suitable for\nresource-constrained military environments where computational assumptions may\nfail.", "AI": {"tldr": "First 2D XOR-based secret sharing scheme for layered multipath networks with unconditional security against single-path attacks per layer.", "motivation": "Need for provably secure communication in resource-constrained military environments where computational assumptions may fail and quantum computing threatens encryption-based approaches.", "method": "Construct 2D XOR-based secret sharing using only bitwise XOR operations with linear O(|S|) complexity, where |S| is message length. Mathematical proofs guarantee security against adversaries observing/disrupting any single path per transmission layer.", "result": "Scheme achieves perfect privacy and successful message recovery with information-theoretic security. Maintains unconditional security regardless of computational resources available to adversaries.", "conclusion": "XOR-based construction offers provable security suitable for military applications, resilient to quantum computing advances unlike encryption-based approaches."}}
{"id": "2509.23152", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23152", "abs": "https://arxiv.org/abs/2509.23152", "authors": ["Zhicheng Yang", "Zhijiang Guo", "Yinya Huang", "Yongxin Wang", "Yiwei Wang", "Xiaodan Liang", "Jing Tang"], "title": "Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers", "comment": "15 pages, 7 figures", "summary": "Test-time scaling via solution sampling and aggregation has become a key\nparadigm for improving the reasoning performance of Large Language Models\n(LLMs). While reward model selection is commonly employed in this approach, it\noften fails to identify minority-yet-correct answers, which limits its\neffectiveness beyond that of simple majority voting. We argue that this\nlimitation stems from a lack of informative critique signals during verifier\ntraining. To bridge this gap, we introduce Mirror-Critique, a framework that\ntrains a verifier with informative critiques. Our key insight is to leverage\nthe rich critique signal by contrasting model-generated solutions with\nground-truth solutions. We deploy a small instruction-tuned model to synthesize\nhigh-quality critique data with rejection sampling that teaches the verifier\nnot only what is wrong, but also why. The synthetic data is used to cold-start\nthe LLMs in the RLVR process to further improve the verification ability. The\nresulting Mirror-Verifier is deployed to evaluate candidate solutions by\ngenerating multiple critiques per solution, aggregating them into a verify\nscore used for weighted voting or selective abstention. The experimental\nresults show that our Mirror-Verifier significantly outperforms majority voting\nin terms of solution accuracy and also improves the solver's honesty to\nrecognize and abstain from answering beyond its capability boundaries.", "AI": {"tldr": "Mirror-Critique improves LLM reasoning by training verifiers with informative critiques through contrastive learning between model-generated and ground-truth solutions, outperforming majority voting.", "motivation": "Current reward model selection in test-time scaling fails to identify minority-yet-correct answers due to lack of informative critique signals during verifier training.", "method": "Leverage contrastive learning between model-generated and ground-truth solutions, use small instruction-tuned model to synthesize high-quality critique data with rejection sampling, and train Mirror-Verifier that generates multiple critiques per solution.", "result": "Significantly outperforms majority voting in solution accuracy and improves solver's honesty to recognize and abstain from answering beyond capability boundaries.", "conclusion": "Mirror-Critique framework effectively bridges the gap in verifier training by providing informative critiques, leading to better reasoning performance and model honesty."}}
{"id": "2509.23383", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23383", "abs": "https://arxiv.org/abs/2509.23383", "authors": ["Sebastian Bordt", "Martin Pawelczyk"], "title": "Train Once, Answer All: Many Pretraining Experiments for the Cost of One", "comment": null, "summary": "Recent work has demonstrated that controlled pretraining experiments are a\npowerful tool for understanding learning, reasoning, and memorization in large\nlanguage models (LLMs). However, the computational cost of pretraining presents\na significant constraint. To overcome this constraint, we propose to conduct\nmultiple pretraining experiments simultaneously during a single training run.\nWe demonstrate the feasibility of this approach by conducting ten experiments\nduring the training of a 1.5B parameter model on 210B tokens. Although we only\ntrain a single model, we can replicate the results from multiple previous works\non data contamination, poisoning, and memorization. We also conduct novel\ninvestigations into knowledge acquisition, mathematical reasoning, and\nwatermarking. For example, we dynamically update the training data until the\nmodel acquires a particular piece of knowledge. Remarkably, the influence of\nthe ten experiments on the model's training dynamics and overall performance is\nminimal. However, interactions between different experiments may act as a\npotential confounder in our approach. We propose to test for interactions with\ncontinual pretraining experiments, finding them to be negligible in our setup.\nOverall, our findings suggest that performing multiple pretraining experiments\nin a single training run can enable rigorous scientific experimentation with\nlarge models on a compute budget.", "AI": {"tldr": "The paper proposes a method to conduct multiple pretraining experiments simultaneously in a single training run to overcome computational constraints, demonstrating feasibility with 10 experiments on a 1.5B parameter model.", "motivation": "To address the high computational cost of pretraining experiments for understanding LLM behaviors, enabling more efficient scientific experimentation.", "method": "Conduct multiple pretraining experiments simultaneously during a single training run of a 1.5B parameter model on 210B tokens, testing for interactions with continual pretraining.", "result": "Successfully replicated previous works on data contamination, poisoning, and memorization, and conducted novel investigations with minimal impact on training dynamics and performance.", "conclusion": "Simultaneous pretraining experiments enable rigorous scientific experimentation with large models on limited compute budgets, with negligible interactions between experiments."}}
{"id": "2509.23725", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23725", "abs": "https://arxiv.org/abs/2509.23725", "authors": ["Siqi Ma", "Jiajie Huang", "Bolin Yang", "Fan Zhang", "Jinlin Wu", "Yue Shen", "Guohui Fan", "Zhu Zhang", "Zelin Zang"], "title": "MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models", "comment": null, "summary": "Answering complex medical questions requires not only domain expertise and\npatient-specific information, but also structured and multi-perspective\nreasoning. Existing multi-agent approaches often rely on fixed roles or shallow\ninteraction prompts, limiting their ability to detect and resolve fine-grained\nlogical inconsistencies. To address this, we propose \\textsc{MedLA}, a\nlogic-driven multi-agent framework built on large language models. Each agent\norganizes its reasoning process into an explicit logical tree based on\nsyllogistic triads (major premise, minor premise, and conclusion), enabling\ntransparent inference and premise-level alignment. Agents engage in a\nmulti-round, graph-guided discussion to compare and iteratively refine their\nlogic trees, achieving consensus through error correction and contradiction\nresolution. We demonstrate that \\textsc{MedLA} consistently outperforms both\nstatic role-based systems and single-agent baselines on challenging benchmarks\nsuch as MedDDx and standard medical QA tasks. Furthermore, \\textsc{MedLA}\nscales effectively across both open-source and commercial LLM backbones,\nachieving state-of-the-art performance and offering a generalizable paradigm\nfor trustworthy medical reasoning.", "AI": {"tldr": "MedLA is a logic-driven multi-agent framework using LLMs that organizes reasoning into explicit logical trees based on syllogistic triads, enabling transparent inference and premise-level alignment through multi-round graph-guided discussions.", "motivation": "Existing multi-agent approaches for complex medical questions rely on fixed roles or shallow interaction prompts, limiting their ability to detect and resolve fine-grained logical inconsistencies.", "method": "Each agent organizes reasoning into explicit logical trees based on syllogistic triads (major premise, minor premise, conclusion). Agents engage in multi-round, graph-guided discussions to compare and iteratively refine logic trees through error correction and contradiction resolution.", "result": "MedLA consistently outperforms both static role-based systems and single-agent baselines on challenging benchmarks like MedDDx and standard medical QA tasks. It scales effectively across both open-source and commercial LLM backbones.", "conclusion": "MedLA achieves state-of-the-art performance and offers a generalizable paradigm for trustworthy medical reasoning through its logic-driven multi-agent framework with transparent inference processes."}}
{"id": "2509.23156", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23156", "abs": "https://arxiv.org/abs/2509.23156", "authors": ["Prashant Govindarajan", "Mathieu Reymond", "Antoine Clavaud", "Mariano Phielipp", "Santiago Miret", "Sarath Chandar"], "title": "CrystalGym: A New Benchmark for Materials Discovery Using Reinforcement Learning", "comment": null, "summary": "In silico design and optimization of new materials primarily relies on\nhigh-accuracy atomic simulators that perform density functional theory (DFT)\ncalculations. While recent works showcase the strong potential of machine\nlearning to accelerate the material design process, they mostly consist of\ngenerative approaches that do not use direct DFT signals as feedback to improve\ntraining and generation mainly due to DFT's high computational cost. To aid the\nadoption of direct DFT signals in the materials design loop through online\nreinforcement learning (RL), we propose CrystalGym, an open-source RL\nenvironment for crystalline material discovery. Using CrystalGym, we benchmark\ncommon value- and policy-based reinforcement learning algorithms for designing\nvarious crystals conditioned on target properties. Concretely, we optimize for\nchallenging properties like the band gap, bulk modulus, and density, which are\ndirectly calculated from DFT in the environment. While none of the algorithms\nwe benchmark solve all CrystalGym tasks, our extensive experiments and\nablations show different sample efficiencies and ease of convergence to\noptimality for different algorithms and environment settings. Additionally, we\ninclude a case study on the scope of fine-tuning large language models with\nreinforcement learning for improving DFT-based rewards. Our goal is for\nCrystalGym to serve as a test bed for reinforcement learning researchers and\nmaterial scientists to address these real-world design problems with practical\napplications. We therefore introduce a novel class of challenges for\nreinforcement learning methods dealing with time-consuming reward signals,\npaving the way for future interdisciplinary research for machine learning\nmotivated by real-world applications.", "AI": {"tldr": "CrystalGym is an open-source RL environment for crystalline material discovery that enables direct DFT feedback in material design through reinforcement learning.", "motivation": "To address the challenge of incorporating direct DFT signals in material design loops due to DFT's high computational cost, and to provide a test bed for RL methods in real-world material science applications.", "method": "Proposed CrystalGym environment that benchmarks value- and policy-based RL algorithms for designing crystals conditioned on target properties like band gap, bulk modulus, and density, with rewards directly calculated from DFT.", "result": "Different RL algorithms showed varying sample efficiencies and convergence patterns across CrystalGym tasks, with none solving all tasks completely. A case study demonstrated fine-tuning LLMs with RL for DFT-based rewards.", "conclusion": "CrystalGym serves as a valuable test bed for RL researchers and material scientists, introducing new challenges for RL methods dealing with time-consuming reward signals and enabling future interdisciplinary research."}}
{"id": "2509.23387", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23387", "abs": "https://arxiv.org/abs/2509.23387", "authors": ["Wenhang Shi", "Yiren Chen", "Shuqing Bian", "Xinyi Zhang", "Kai Tang", "Pengfei Hu", "Zhe Zhao", "Wei Lu", "Xiaoyong Du"], "title": "No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization", "comment": "10 pages for main content", "summary": "Prompt engineering is crucial for leveraging the full potential of large\nlanguage models (LLMs). While automatic prompt optimization offers a scalable\nalternative to costly manual design, generating effective prompts remains\nchallenging. Existing methods often struggle to stably generate improved\nprompts, leading to low efficiency, and overlook that prompt optimization\neasily gets trapped in local optima. Addressing this, we propose GRACE, a\nframework that integrates two synergistic strategies: Gated Refinement and\nAdaptive Compression, achieving Efficient prompt optimization. The gated\nrefinement strategy introduces a feedback regulation gate and an update\nrejection gate, which refine update signals to produce stable and effective\nprompt improvements. When optimization stagnates, the adaptive compression\nstrategy distills the prompt's core concepts, restructuring the optimization\ntrace and opening new paths. By strategically introducing information loss\nthrough refinement and compression, GRACE delivers substantial gains in\nperformance and efficiency. In extensive experiments on 11 tasks across three\npractical domains, including BIG-Bench Hard (BBH), domain-specific, and general\nNLP tasks, GRACE achieves significant average relative performance improvements\nof 4.7%, 4.4% and 2.7% over state-of-the-art methods, respectively. Further\nanalysis shows that GRACE achieves these gains using only 25% of the prompt\ngeneration budget required by prior methods, highlighting its high optimization\nefficiency and low computational overhead. Our code is available at\nhttps://github.com/Eric8932/GRACE.", "AI": {"tldr": "GRACE is a prompt optimization framework that combines gated refinement and adaptive compression strategies to efficiently improve LLM prompts, achieving significant performance gains with only 25% of the computational budget of prior methods.", "motivation": "Existing automatic prompt optimization methods struggle with stable prompt generation, low efficiency, and getting trapped in local optima, making scalable prompt engineering challenging.", "method": "GRACE integrates two strategies: 1) Gated refinement with feedback regulation and update rejection gates to refine update signals, and 2) Adaptive compression that distills core concepts when optimization stagnates to restructure the optimization trace.", "result": "On 11 tasks across BIG-Bench Hard, domain-specific, and general NLP tasks, GRACE achieved average relative improvements of 4.7%, 4.4%, and 2.7% over state-of-the-art methods respectively, using only 25% of the prompt generation budget.", "conclusion": "GRACE demonstrates that strategic information loss through refinement and compression enables substantial performance gains and high optimization efficiency for prompt engineering."}}
{"id": "2509.23730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23730", "abs": "https://arxiv.org/abs/2509.23730", "authors": ["Siyao Song", "Cong Ma", "Zhihao Cheng", "Shiye Lei", "Minghao Li", "Ying Zeng", "Huaixiao Tou", "Kai Jia"], "title": "EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance", "comment": null, "summary": "Large language models (LLMs) have recently advanced in reasoning when\noptimized with reinforcement learning (RL) under verifiable rewards. Existing\nmethods primarily rely on outcome-based supervision to strengthen internal LLM\nreasoning, often leading to inefficient exploration and sparse rewards. To\nmitigate this issue, we propose Expert-Assisted Policy Optimization (EAPO), a\nnovel RL framework that enhances exploration by incorporating multi-turn\ninteractions with external experts during training. Unlike prior methods, where\npolicies reason in isolation, EAPO incentivizes the policy to adaptively\ndetermine when and how to consult experts, yielding richer reward signals and\nmore reliable reasoning trajectories. External assistance ultimately\ninternalizes expert knowledge into the policy model, amplifying the model's\ninherent reasoning capabilities. During evaluation, the policy model has been\nwell-optimized to solve questions independently, producing improved reasoning\npaths and more accurate solutions. Experiments on mathematical reasoning\nbenchmarks, including AIME 2024, AIME 2025, and AIMO 2025, show that EAPO\nconsistently outperforms expert-assisted workflow, expert-distilled models, and\nRL baselines, with an average gain of 5 points over self-exploratory models.", "AI": {"tldr": "EAPO is a novel RL framework that enhances LLM reasoning by incorporating multi-turn interactions with external experts during training, leading to improved exploration and internalization of expert knowledge.", "motivation": "Existing RL methods for LLMs rely on outcome-based supervision, which leads to inefficient exploration and sparse rewards. The paper aims to address these limitations by incorporating expert interactions.", "method": "Proposes Expert-Assisted Policy Optimization (EAPO) where the policy learns to adaptively consult external experts during training, yielding richer reward signals and more reliable reasoning trajectories.", "result": "EAPO consistently outperforms expert-assisted workflow, expert-distilled models, and RL baselines on mathematical reasoning benchmarks (AIME 2024, AIME 2025, AIMO 2025), with an average gain of 5 points over self-exploratory models.", "conclusion": "EAPO successfully internalizes expert knowledge into the policy model, amplifying the model's inherent reasoning capabilities and producing improved reasoning paths and more accurate solutions without requiring expert assistance during evaluation."}}
{"id": "2509.23158", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23158", "abs": "https://arxiv.org/abs/2509.23158", "authors": ["Yufei Shen", "Ji Hwan Park", "Minchao Huang", "Jared F. Benge", "Justin F. Rousseau", "Rosemary A. Lester-Smith", "Edison Thomaz"], "title": "Deep Learning-Based Detection of Cognitive Impairment from Passive Smartphone Sensing with Routine-Aware Augmentation and Demographic Personalization", "comment": "Accepted at 2025 IEEE EMBS International Conference on Biomedical and\n  Health Informatics (IEEE BHI 2025)", "summary": "Early detection of cognitive impairment is critical for timely diagnosis and\nintervention, yet infrequent clinical assessments often lack the sensitivity\nand temporal resolution to capture subtle cognitive declines in older adults.\nPassive smartphone sensing has emerged as a promising approach for naturalistic\nand continuous cognitive monitoring. Building on this potential, we implemented\na Long Short-Term Memory (LSTM) model to detect cognitive impairment from\nsequences of daily behavioral features, derived from multimodal sensing data\ncollected in an ongoing one-year study of older adults. Our key contributions\nare two techniques to enhance model generalizability across participants: (1)\nroutine-aware augmentation, which generates synthetic sequences by replacing\neach day with behaviorally similar alternatives, and (2) demographic\npersonalization, which reweights training samples to emphasize those from\nindividuals demographically similar to the test participant. Evaluated on\n6-month data from 36 older adults, these techniques jointly improved the Area\nUnder the Precision-Recall Curve (AUPRC) of the model trained on sensing and\ndemographic features from 0.637 to 0.766, highlighting the potential of\nscalable monitoring of cognitive impairment in aging populations with passive\nsensing.", "AI": {"tldr": "LSTM model with routine-aware augmentation and demographic personalization improves cognitive impairment detection from smartphone sensing data in older adults.", "motivation": "Early detection of cognitive impairment is critical but infrequent clinical assessments lack sensitivity and temporal resolution to capture subtle cognitive declines.", "method": "Implemented LSTM model with two techniques: routine-aware augmentation (generates synthetic sequences by replacing days with behaviorally similar alternatives) and demographic personalization (reweights training samples to emphasize demographically similar individuals).", "result": "Joint techniques improved AUPRC from 0.637 to 0.766 on 6-month data from 36 older adults.", "conclusion": "Demonstrates potential of scalable cognitive impairment monitoring using passive smartphone sensing with enhanced model generalizability."}}
{"id": "2509.23395", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23395", "abs": "https://arxiv.org/abs/2509.23395", "authors": ["Sherrie Shen", "Weixuan Wang", "Alexandra Birch"], "title": "Liaozhai through the Looking-Glass: On Paratextual Explicitation of Culture-Bound Terms in Machine Translation", "comment": "EMNLP 2025", "summary": "The faithful transfer of contextually-embedded meaning continues to challenge\ncontemporary machine translation (MT), particularly in the rendering of\nculture-bound terms--expressions or concepts rooted in specific languages or\ncultures, resisting direct linguistic transfer. Existing computational\napproaches to explicitating these terms have focused exclusively on in-text\nsolutions, overlooking paratextual apparatus in the footnotes and endnotes\nemployed by professional translators. In this paper, we formalize Genette's\n(1987) theory of paratexts from literary and translation studies to introduce\nthe task of paratextual explicitation for MT. We construct a dataset of 560\nexpert-aligned paratexts from four English translations of the classical\nChinese short story collection Liaozhai and evaluate LLMs with and without\nreasoning traces on choice and content of explicitation. Experiments across\nintrinsic prompting and agentic retrieval methods establish the difficulty of\nthis task, with human evaluation showing that LLM-generated paratexts improve\naudience comprehension, though remain considerably less effective than\ntranslator-authored ones. Beyond model performance, statistical analysis\nreveals that even professional translators vary widely in their use of\nparatexts, suggesting that cultural mediation is inherently open-ended rather\nthan prescriptive. Our findings demonstrate the potential of paratextual\nexplicitation in advancing MT beyond linguistic equivalence, with promising\nextensions to monolingual explanation and personalized adaptation.", "AI": {"tldr": "This paper introduces paratextual explicitation for machine translation, using footnotes/endnotes to explain culture-bound terms. It evaluates LLMs on this task using a dataset from Chinese literature translations.", "motivation": "Current MT approaches overlook paratextual apparatus used by professional translators to explain culture-bound terms, limiting faithful transfer of contextual meaning.", "method": "Formalized Genette's paratext theory, created dataset of 560 expert-aligned paratexts from English translations of Liaozhai, evaluated LLMs with/without reasoning traces on explicitation choice and content.", "result": "LLM-generated paratexts improve audience comprehension but are less effective than translator-authored ones. Professional translators vary widely in paratext usage, showing cultural mediation is open-ended.", "conclusion": "Paratextual explicitation has potential to advance MT beyond linguistic equivalence, with extensions to monolingual explanation and personalized adaptation."}}
{"id": "2509.23735", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23735", "abs": "https://arxiv.org/abs/2509.23735", "authors": ["Xuyan Ma", "Xiaofei Xie", "Yawen Wang", "Junjie Wang", "Boyu Wu", "Mingyang Li", "Qing Wang"], "title": "Diagnosing Failure Root Causes in Platform-Orchestrated Agentic Systems: Dataset, Taxonomy, and Benchmark", "comment": null, "summary": "Agentic systems consisting of multiple LLM-driven agents coordinating through\ntools and structured interactions, are increasingly deployed for complex\nreasoning and problem-solving tasks. At the same time, emerging low-code and\ntemplate-based agent development platforms (e.g., Dify) enable users to rapidly\nbuild and orchestrate agentic systems, which we refer to as\nplatform-orchestrated agentic systems. However, these systems are also fragile\nand it remains unclear how to systematically identify their potential failure\nroot cause. This paper presents a study of root cause identification of these\nplatform-orchestrated agentic systems. To support this initiative, we construct\na dataset AgentFail containing 307 failure logs from ten agentic systems, each\nwith fine-grained annotations linking failures to their root causes. We\nadditionally utilize counterfactual reasoning-based repair strategy to ensure\nthe reliability of the annotation. Building on the dataset, we develop a\ntaxonomy that characterizes failure root causes and analyze their distribution\nacross different platforms and task domains. Furthermore, we introduce a\nbenchmark that leverages LLMs for automatically identifying root causes, in\nwhich we also utilize the proposed taxonomy as guidance for LLMs. Results show\nthat the taxonomy can largely improve the performance, thereby confirming its\nutility. Nevertheless, the accuracy of root cause identification reaches at\nmost 33.6%, which indicates that this task still remains challenging. In light\nof these results, we also provide actionable guidelines for building such\nagentic systems. In summary, this paper provides a reliable dataset of failure\nroot cause for platform-orchestrated agentic systems, corresponding taxonomy\nand benchmark, which serves as a foundation for advancing the development of\nmore reliable agentic systems.", "AI": {"tldr": "This paper studies root cause identification in platform-orchestrated agentic systems, creates a dataset (AgentFail) with 307 failure logs, develops a taxonomy for failure root causes, and benchmarks LLM-based root cause identification methods.", "motivation": "Platform-orchestrated agentic systems are increasingly deployed but fragile, and there's no systematic way to identify their potential failure root causes.", "method": "Constructed AgentFail dataset with 307 failure logs from 10 agentic systems, used counterfactual reasoning-based repair for reliable annotation, developed taxonomy for failure root causes, and created LLM benchmark for root cause identification.", "result": "The taxonomy significantly improves LLM performance in root cause identification, but maximum accuracy reaches only 33.6%, indicating the task remains challenging.", "conclusion": "The paper provides a reliable dataset, taxonomy, and benchmark for failure root cause analysis in agentic systems, serving as foundation for developing more reliable agentic systems."}}
{"id": "2509.23159", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23159", "abs": "https://arxiv.org/abs/2509.23159", "authors": ["Ziheng Peng", "Shijie Ren", "Xinyue Gu", "Linxiao Yang", "Xiting Wang", "Liang Sun"], "title": "ProtoTS: Learning Hierarchical Prototypes for Explainable Time Series Forecasting", "comment": "Under submission", "summary": "While deep learning has achieved impressive performance in time series\nforecasting, it becomes increasingly crucial to understand its decision-making\nprocess for building trust in high-stakes scenarios. Existing interpretable\nmodels often provide only local and partial explanations, lacking the\ncapability to reveal how heterogeneous and interacting input variables jointly\nshape the overall temporal patterns in the forecast curve. We propose ProtoTS,\na novel interpretable forecasting framework that achieves both high accuracy\nand transparent decision-making through modeling prototypical temporal\npatterns. ProtoTS computes instance-prototype similarity based on a denoised\nrepresentation that preserves abundant heterogeneous information. The\nprototypes are organized hierarchically to capture global temporal patterns\nwith coarse prototypes while capturing finer-grained local variations with\ndetailed prototypes, enabling expert steering and multi-level interpretability.\nExperiments on multiple realistic benchmarks, including a newly released LOF\ndataset, show that ProtoTS not only exceeds existing methods in forecast\naccuracy but also delivers expert-steerable interpretations for better model\nunderstanding and decision support.", "AI": {"tldr": "ProtoTS is an interpretable time series forecasting framework that uses prototypical temporal patterns to provide both high accuracy and transparent decision-making through hierarchical prototype organization.", "motivation": "Deep learning models lack transparency in decision-making for time series forecasting, especially in high-stakes scenarios where understanding how input variables shape temporal patterns is crucial.", "method": "ProtoTS computes instance-prototype similarity using denoised representations, organizes prototypes hierarchically to capture both global temporal patterns and finer-grained local variations, enabling expert steering and multi-level interpretability.", "result": "Experiments on multiple realistic benchmarks including a new LOF dataset show ProtoTS exceeds existing methods in forecast accuracy while providing expert-steerable interpretations.", "conclusion": "ProtoTS successfully bridges the gap between accuracy and interpretability in time series forecasting, offering better model understanding and decision support through its prototype-based approach."}}
{"id": "2509.23412", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23412", "abs": "https://arxiv.org/abs/2509.23412", "authors": ["Haowei Hua", "Hong Jiao", "Dan Song"], "title": "Comparison of Scoring Rationales Between Large Language Models and Human Raters", "comment": "23 Pages, 4 Tables, 13 Figures", "summary": "Advances in automated scoring are closely aligned with advances in\nmachine-learning and natural-language-processing techniques. With recent\nprogress in large language models (LLMs), the use of ChatGPT, Gemini, Claude,\nand other generative-AI chatbots for automated scoring has been explored. Given\ntheir strong reasoning capabilities, LLMs can also produce rationales to\nsupport the scores they assign. Thus, evaluating the rationales provided by\nboth human and LLM raters can help improve the understanding of the reasoning\nthat each type of rater applies when assigning a score. This study investigates\nthe rationales of human and LLM raters to identify potential causes of scoring\ninconsistency. Using essays from a large-scale test, the scoring accuracy of\nGPT-4o, Gemini, and other LLMs is examined based on quadratic weighted kappa\nand normalized mutual information. Cosine similarity is used to evaluate the\nsimilarity of the rationales provided. In addition, clustering patterns in\nrationales are explored using principal component analysis based on the\nembeddings of the rationales. The findings of this study provide insights into\nthe accuracy and ``thinking'' of LLMs in automated scoring, helping to improve\nthe understanding of the rationales behind both human scoring and LLM-based\nautomated scoring.", "AI": {"tldr": "This study compares human and LLM (GPT-4o, Gemini, etc.) rationales in automated essay scoring to understand scoring inconsistencies and improve understanding of reasoning processes in both human and AI raters.", "motivation": "With advances in LLMs, understanding how they reason and score essays compared to humans is crucial for improving automated scoring systems and identifying sources of scoring inconsistency.", "method": "Used essays from large-scale tests, evaluated LLM scoring accuracy with quadratic weighted kappa and normalized mutual information, analyzed rationale similarity with cosine similarity, and explored clustering patterns using PCA on rationale embeddings.", "result": "The study provides insights into LLM scoring accuracy and their \"thinking\" processes, revealing patterns in how rationales differ between human and AI raters.", "conclusion": "Analyzing rationales from both human and LLM raters helps improve understanding of scoring reasoning and can enhance automated scoring systems by identifying and addressing scoring inconsistencies."}}
{"id": "2509.23738", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23738", "abs": "https://arxiv.org/abs/2509.23738", "authors": ["Cong Chen", "Kaixiang Ji", "Hao Zhong", "Muzhi Zhu", "Anzhou Li", "Guo Gan", "Ziyuan Huang", "Cheng Zou", "Jiajia Liu", "Jingdong Chen", "Hao Chen", "Chunhua Shen"], "title": "GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks", "comment": null, "summary": "Autonomous agents for long-sequence Graphical User Interface tasks are\nhindered by sparse rewards and the intractable credit assignment problem. To\naddress these challenges, we introduce GUI-Shepherd, a Process Reward Model\nthat provides dense, step-by-step feedback to guide agents. GUI-Shepherd is\ntrained on a diverse large-scale data set of $52$k interactions that features\nhuman-annotated scores and GPT-4o generated rationales, enabling it to serve\nboth as a reward provider for RL training and as a verifier for inference. As\nfar as we know, we are the first to conduct a systematic study of process\nsupervision in GUI agents, across diverse settings from online long-horizon\ntasks to offline single-step prediction. On the online AndroidWorld benchmark,\nGUI-Shepherd improves success rate by $7.7$ points via multi-turn online PPO,\nsignificantly outperforming Outcome Reward Model based competitors. When used\nas an inference verifier, it brings $5.1$ points improvements. The benefits\ngeneralize to the offline AndroidControl benchmark, with gains of $2.2$ points\nas a reward provider and $4.3$ points as a verifier. Collectively, our results\nestablish that high-fidelity process supervision is critical for building more\ncapable GUI agents and present a generalizable solution.", "AI": {"tldr": "GUI-Shepherd is a Process Reward Model that provides dense step-by-step feedback for GUI agents, addressing sparse rewards and credit assignment problems in long-sequence tasks.", "motivation": "Autonomous agents for GUI tasks face challenges with sparse rewards and intractable credit assignment, which hinders their performance in long-sequence tasks.", "method": "Trained on 52k interactions with human-annotated scores and GPT-4o generated rationales, GUI-Shepherd serves as both reward provider for RL training and verifier for inference.", "result": "On AndroidWorld benchmark: 7.7 points improvement via online PPO, 5.1 points as verifier. On AndroidControl: 2.2 points as reward provider, 4.3 points as verifier. Outperforms Outcome Reward Model competitors.", "conclusion": "High-fidelity process supervision is critical for building capable GUI agents, and GUI-Shepherd provides a generalizable solution for process supervision in GUI tasks."}}
{"id": "2509.23162", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.23162", "abs": "https://arxiv.org/abs/2509.23162", "authors": ["Chandan Tankala", "Krishnakumar Balasubramanian"], "title": "Dense associative memory on the Bures-Wasserstein space", "comment": null, "summary": "Dense associative memories (DAMs) store and retrieve patterns via\nenergy-functional fixed points, but existing models are limited to vector\nrepresentations. We extend DAMs to probability distributions equipped with the\n2-Wasserstein distance, focusing mainly on the Bures-Wasserstein class of\nGaussian densities. Our framework defines a log-sum-exp energy over stored\ndistributions and a retrieval dynamics aggregating optimal transport maps in a\nGibbs-weighted manner. Stationary points correspond to self-consistent\nWasserstein barycenters, generalizing classical DAM fixed points. We prove\nexponential storage capacity, provide quantitative retrieval guarantees under\nWasserstein perturbations, and validate the model on synthetic and real-world\ndistributional tasks. This work elevates associative memory from vectors to\nfull distributions, bridging classical DAMs with modern generative modeling and\nenabling distributional storage and retrieval in memory-augmented learning.", "AI": {"tldr": "Extends dense associative memories from vectors to probability distributions using 2-Wasserstein distance, focusing on Gaussian densities with Bures-Wasserstein metric.", "motivation": "Existing dense associative memory models are limited to vector representations, but many real-world applications involve distributional data. The paper aims to bridge classical associative memories with modern generative modeling by enabling storage and retrieval of full distributions.", "method": "Defines a log-sum-exp energy over stored distributions and retrieval dynamics that aggregate optimal transport maps in a Gibbs-weighted manner. Uses 2-Wasserstein distance and focuses on Bures-Wasserstein class of Gaussian densities.", "result": "Proves exponential storage capacity and provides quantitative retrieval guarantees under Wasserstein perturbations. Validates the model on synthetic and real-world distributional tasks.", "conclusion": "Successfully elevates associative memory from vectors to full distributions, enabling distributional storage and retrieval in memory-augmented learning systems."}}
{"id": "2509.23417", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23417", "abs": "https://arxiv.org/abs/2509.23417", "authors": ["Rajaa El Hamdani", "Samy Haffoudhi", "Nils Holzenberger", "Fabian Suchanek", "Thomas Bonald", "Fragkiskos D. Malliaros"], "title": "Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models", "comment": null, "summary": "Language models (LMs) encode substantial factual knowledge, but often produce\nanswers judged as incorrect. We hypothesize that many of these answers are\nactually correct, but are expressed in alternative surface forms that are\ndismissed due to an overly strict evaluation, leading to an underestimation of\nmodels' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD),\na decoding strategy that restricts model outputs to unique surface forms. We\nintroduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating\nopen-source LMs from 135M to 70B parameters, we show that standard decoding\nundervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1\nwith vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0%\nwith RCD, outperforming the larger model under vanilla decoding. We publicly\nshare the code and dataset at https://github.com/Rajjaa/disambiguated-LLM.", "AI": {"tldr": "The paper proposes Retrieval-Constrained Decoding (RCD) to address the underestimation of language models' factual knowledge by restricting outputs to unique surface forms, showing significant performance improvements on the YAGO-QA dataset.", "motivation": "Language models encode substantial factual knowledge but often produce answers judged incorrect due to alternative surface forms being dismissed in strict evaluations, leading to underestimation of their parametric knowledge.", "method": "Proposed Retrieval-Constrained Decoding (RCD), a decoding strategy that restricts model outputs to unique surface forms. Evaluated on YAGO-QA dataset (19,137 general knowledge questions) with open-source LMs from 135M to 70B parameters.", "result": "RCD significantly improves performance: Llama-3.1-70B scores 46.0% F1 with RCD vs 32.3% with vanilla decoding. Llama-3.1-8B reaches 33.0% with RCD, outperforming the larger model under vanilla decoding.", "conclusion": "Standard decoding undervalues language models' knowledge, and RCD effectively addresses this by constraining outputs to unique surface forms, revealing more accurate assessment of models' factual knowledge."}}
{"id": "2509.23757", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23757", "abs": "https://arxiv.org/abs/2509.23757", "authors": ["Benjamin Teoh", "Ben Glocker", "Francesca Toni", "Avinash Kori"], "title": "Transparent Visual Reasoning via Object-Centric Agent Collaboration", "comment": null, "summary": "A central challenge in explainable AI, particularly in the visual domain, is\nproducing explanations grounded in human-understandable concepts. To tackle\nthis, we introduce OCEAN (Object-Centric Explananda via Agent Negotiation), a\nnovel, inherently interpretable framework built on object-centric\nrepresentations and a transparent multi-agent reasoning process. The\ngame-theoretic reasoning process drives agents to agree on coherent and\ndiscriminative evidence, resulting in a faithful and interpretable\ndecision-making process. We train OCEAN end-to-end and benchmark it against\nstandard visual classifiers and popular posthoc explanation tools like GradCAM\nand LIME across two diagnostic multi-object datasets. Our results demonstrate\ncompetitive performance with respect to state-of-the-art black-box models with\na faithful reasoning process, which was reflected by our user study, where\nparticipants consistently rated OCEAN's explanations as more intuitive and\ntrustworthy.", "AI": {"tldr": "OCEAN is an interpretable AI framework using object-centric representations and multi-agent negotiation for transparent visual explanations, achieving competitive performance with black-box models while providing more intuitive explanations.", "motivation": "Address the challenge of producing visual explanations grounded in human-understandable concepts in explainable AI.", "method": "Object-centric representations with transparent multi-agent reasoning process using game-theoretic negotiation to agree on coherent and discriminative evidence.", "result": "Competitive performance with state-of-the-art black-box models on two diagnostic multi-object datasets, with user study showing OCEAN's explanations rated as more intuitive and trustworthy than GradCAM and LIME.", "conclusion": "OCEAN provides a faithful and interpretable decision-making process through object-centric representations and agent negotiation, successfully bridging the gap between performance and explainability in visual AI."}}
{"id": "2509.23789", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23789", "abs": "https://arxiv.org/abs/2509.23789", "authors": ["Chunxue Xu", "Yiwei Wang", "Yujun Cai", "Bryan Hooi", "Songze Li"], "title": "Visual CoT Makes VLMs Smarter but More Fragile", "comment": null, "summary": "Chain-of-Thought (CoT) techniques have significantly enhanced reasoning in\nVision-Language Models (VLMs). Extending this paradigm, Visual CoT integrates\nexplicit visual edits, such as cropping or annotating regions of interest, into\nthe reasoning process, achieving superior multimodal performance. However, the\nrobustness of Visual CoT-based VLMs against image-level noise remains\nunexplored. In this paper, we present the first systematic evaluation of Visual\nCoT robustness under visual perturbations. Our benchmark spans 12 image\ncorruption types across 4 Visual Question Answering (VQA) datasets, enabling a\ncomprehensive comparison between VLMs that use Visual CoT, and VLMs that do\nnot. The results reveal that integrating Visual CoT consistently improves\nabsolute accuracy regardless of whether the input images are clean or corrupted\nby noise; however, it also increases sensitivity to input perturbations,\nresulting in sharper performance degradation compared to standard VLMs. Through\nextensive analysis, we identify the intermediate reasoning components of Visual\nCoT, i.e., the edited image patches , as the primary source of fragility.\nBuilding on this analysis, we propose a plug-and-play robustness enhancement\nmethod that integrates Grounding DINO model into the Visual CoT pipeline,\nproviding high-confidence local visual cues to stabilize reasoning. Our work\nreveals clear fragility patterns in Visual CoT and offers an effective,\narchitecture-agnostic solution for enhancing visual robustness.", "AI": {"tldr": "This paper presents the first systematic evaluation of Visual Chain-of-Thought (CoT) robustness against image perturbations, revealing that while Visual CoT improves accuracy, it increases sensitivity to noise. The authors identify edited image patches as the primary fragility source and propose a Grounding DINO-based solution to enhance robustness.", "motivation": "Visual CoT techniques enhance reasoning in VLMs by integrating explicit visual edits, but their robustness against image-level noise remains unexplored. The authors aim to systematically evaluate Visual CoT's vulnerability to visual perturbations and understand its fragility patterns.", "method": "The authors create a benchmark spanning 12 image corruption types across 4 VQA datasets to compare VLMs with and without Visual CoT. They analyze intermediate reasoning components and propose a plug-and-play robustness enhancement method using Grounding DINO model to provide high-confidence local visual cues.", "result": "Visual CoT consistently improves absolute accuracy on both clean and corrupted images, but increases sensitivity to input perturbations, causing sharper performance degradation compared to standard VLMs. The edited image patches are identified as the primary source of fragility.", "conclusion": "The work reveals clear fragility patterns in Visual CoT and provides an effective, architecture-agnostic solution using Grounding DINO to stabilize reasoning and enhance visual robustness in Visual CoT-based VLMs."}}
{"id": "2509.23173", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23173", "abs": "https://arxiv.org/abs/2509.23173", "authors": ["Hangwei Zhang", "Chun Kang", "Yan Wang", "Difan Zou"], "title": "F-Adapter: Frequency-Adaptive Parameter-Efficient Fine-Tuning in Scientific Machine Learning", "comment": "NeurIPS 2025 Main Track", "summary": "Parameter-efficient fine-tuning (PEFT) of powerful pre-trained models for\ncomplex downstream tasks has proven effective in vision and language\nprocessing, yet this paradigm remains unexplored in scientific machine\nlearning, where the objective is to model complex physical systems. We conduct\nthe first systematic study of PEFT for pre-trained Large Operator Models (LOMs)\nobtained by scaling variants of Fourier Neural Operator. First, we observe that\nthe widely used Low-Rank Adaptation (LoRA) yields markedly poorer performance\non LOMs than Adapter tuning. Then, we further theoretically establish that\nstacked LoRA incurs a depth-amplified lower bound on approximation error within\nFourier layers, whereas adapters retain universal approximation capacity and,\nby concentrating parameters on energy-dominant low-frequency modes, attain\nexponentially decaying error with bottleneck width in the Fourier domain.\nMotivated by the robust empirical gains of adapters and by our theoretical\ncharacterization of PDE solutions as spectrally sparse, we introduce\nFrequency-Adaptive Adapter (F-Adapter). F-Adapter allocates adapter capacity\nbased on spectral complexity, assigning higher-dimension modules to\nlow-frequency components and lower-dimension modules to high-frequency\ncomponents. Our F-Adapters establish state-of-the-art (SOTA) results on\nmultiple challenging 3D Navier-Stokes benchmarks, markedly enhancing both\ngeneralization and spectral fidelity over LoRA and other PEFT techniques\ncommonly used in LLMs. To the best of our knowledge, this work is the first to\nexplore PEFT for scientific machine-learning and establishes F-Adapter as an\neffective paradigm for this domain.", "AI": {"tldr": "This paper introduces Frequency-Adaptive Adapter (F-Adapter), a parameter-efficient fine-tuning method for scientific machine learning that outperforms LoRA by adaptively allocating capacity based on spectral complexity.", "motivation": "Parameter-efficient fine-tuning (PEFT) has been successful in vision and language domains but remains unexplored in scientific machine learning for modeling physical systems. The authors aim to adapt PEFT techniques to pre-trained Large Operator Models (LOMs) for complex physical systems.", "method": "The authors systematically study PEFT for LOMs, identify limitations of LoRA, and propose F-Adapter which allocates adapter capacity based on spectral complexity - higher dimensions for low-frequency components and lower dimensions for high-frequency components.", "result": "F-Adapter establishes state-of-the-art results on multiple 3D Navier-Stokes benchmarks, significantly improving both generalization and spectral fidelity over LoRA and other PEFT techniques commonly used in LLMs.", "conclusion": "This work pioneers PEFT for scientific machine learning and demonstrates F-Adapter as an effective paradigm for this domain, with theoretical justification for its superior performance over LoRA in Fourier-based operator models."}}
{"id": "2509.23441", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23441", "abs": "https://arxiv.org/abs/2509.23441", "authors": ["Xuanming Zhang", "Yuxuan Chen", "Min-Hsuan Yeh", "Yixuan Li"], "title": "Cognition-of-Thought Elicits Social-Aligned Reasoning in Large Language Models", "comment": null, "summary": "Large language models (LLMs) excel at complex reasoning but can still exhibit\nharmful behaviors. Current alignment strategies typically embed safety into\nmodel weights, making these controls implicit, static, and difficult to modify.\nThis paper introduces Cognition-of-Thought (CooT), a novel decoding-time\nframework that equips LLMs with an explicit cognitive self-monitoring loop.\nCooT couples a standard text Generator with a cognitive Perceiver that\ncontinuously monitors the unfolding sequence. The Perceiver uses a structured,\nprecedence-based hierarchy of principles (e.g., safety over obedience) to\ndetect potential misalignments as they arise. When violations are flagged, CooT\nintervenes by rolling back the generation to the point of error and\nregenerating under injected guidance that combines universal social priors with\ncontext-specific warnings. CooT thus transforms alignment from a fixed property\ninto an explicit, dynamic, and auditable process active during inference,\nallowing for flexible policy updates without retraining the model. Extensive\nexperiments across multiple benchmarks and model families confirm that CooT\nconsistently improves safety and social reasoning performance.", "AI": {"tldr": "CooT is a decoding-time framework that adds explicit cognitive self-monitoring to LLMs, using a Perceiver to detect misalignments and intervene with rollback and regeneration guided by safety principles.", "motivation": "Current LLM alignment strategies embed safety into model weights, making controls implicit, static, and hard to modify. There's a need for more explicit, dynamic, and auditable safety mechanisms.", "method": "CooT couples a text Generator with a cognitive Perceiver that monitors generation using a precedence-based hierarchy of principles. When violations are detected, it rolls back generation and regenerates with injected guidance combining universal social priors and context-specific warnings.", "result": "Extensive experiments across multiple benchmarks and model families confirm that CooT consistently improves safety and social reasoning performance.", "conclusion": "CooT transforms alignment from a fixed property into an explicit, dynamic, and auditable process during inference, enabling flexible policy updates without model retraining."}}
{"id": "2509.23768", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23768", "abs": "https://arxiv.org/abs/2509.23768", "authors": ["Cheng Yang", "Jiaxuan Lu", "Haiyuan Wan", "Junchi Yu", "Feiwei Qin"], "title": "From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning", "comment": null, "summary": "The chemical reaction recommendation is to select proper reaction condition\nparameters for chemical reactions, which is pivotal to accelerating chemical\nscience. With the rapid development of large language models (LLMs), there is\ngrowing interest in leveraging their reasoning and planning capabilities for\nreaction condition recommendation. Despite their success, existing methods\nrarely explain the rationale behind the recommended reaction conditions,\nlimiting their utility in high-stakes scientific workflows. In this work, we\npropose ChemMAS, a multi-agent system that reframes condition prediction as an\nevidence-based reasoning task. ChemMAS decomposes the task into mechanistic\ngrounding, multi-channel recall, constraint-aware agentic debate, and rationale\naggregation. Each decision is backed by interpretable justifications grounded\nin chemical knowledge and retrieved precedents. Experiments show that ChemMAS\nachieves 20-35% gains over domain-specific baselines and outperforms\ngeneral-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable,\nhuman-trustable rationales, which establishes a new paradigm for explainable AI\nin scientific discovery.", "AI": {"tldr": "ChemMAS is a multi-agent system that reframes chemical reaction condition recommendation as an evidence-based reasoning task, achieving significant performance gains while providing interpretable rationales.", "motivation": "Existing LLM-based methods for chemical reaction condition recommendation lack explainable rationales, limiting their utility in high-stakes scientific workflows where understanding the reasoning behind recommendations is crucial.", "method": "ChemMAS decomposes the task into four components: mechanistic grounding, multi-channel recall, constraint-aware agentic debate, and rationale aggregation. It uses a multi-agent system where each decision is backed by interpretable justifications grounded in chemical knowledge and retrieved precedents.", "result": "ChemMAS achieves 20-35% gains over domain-specific baselines and outperforms general-purpose LLMs by 10-15% in Top-1 accuracy for reaction condition recommendation.", "conclusion": "ChemMAS establishes a new paradigm for explainable AI in scientific discovery by providing falsifiable, human-trustable rationales for chemical reaction condition recommendations."}}
{"id": "2509.23882", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23882", "abs": "https://arxiv.org/abs/2509.23882", "authors": ["Shuyi Lin", "Tian Lu", "Zikai Wang", "Bo Wen", "Yibo Zhao", "Cheng Tan"], "title": "Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B", "comment": null, "summary": "OpenAI's GPT-OSS family provides open-weight language models with explicit\nchain-of-thought (CoT) reasoning and a Harmony prompt format. We summarize an\nextensive security evaluation of GPT-OSS-20B that probes the model's behavior\nunder different adversarial conditions. Using the Jailbreak Oracle (JO) [1], a\nsystematic LLM evaluation tool, the study uncovers several failure modes\nincluding quant fever, reasoning blackholes, Schrodinger's compliance,\nreasoning procedure mirage, and chain-oriented prompting. Experiments\ndemonstrate how these behaviors can be exploited on GPT-OSS-20B models, leading\nto severe consequences.", "AI": {"tldr": "Security evaluation of GPT-OSS-20B reveals multiple failure modes including quant fever, reasoning blackholes, and Schrodinger's compliance that can be exploited under adversarial conditions.", "motivation": "To systematically evaluate the security vulnerabilities and failure modes of OpenAI's GPT-OSS-20B model under different adversarial conditions using the Jailbreak Oracle framework.", "method": "Used Jailbreak Oracle (JO), a systematic LLM evaluation tool, to probe GPT-OSS-20B's behavior under adversarial conditions and identify specific failure modes.", "result": "Uncovered several critical failure modes: quant fever, reasoning blackholes, Schrodinger's compliance, reasoning procedure mirage, and chain-oriented prompting that can be exploited on GPT-OSS-20B models.", "conclusion": "The study demonstrates severe security vulnerabilities in GPT-OSS-20B that can lead to significant consequences when exploited, highlighting the need for improved security measures in open-weight language models."}}
{"id": "2509.23183", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23183", "abs": "https://arxiv.org/abs/2509.23183", "authors": ["Guohao Chen", "Shuaicheng Niu", "Deyu Chen", "Jiahao Yang", "Zitian Zhang", "Mingkui Tan", "Pengcheng Wu", "Zhiqi Shen"], "title": "ZeroSiam: An Efficient Siamese for Test-Time Entropy Optimization without Collapse", "comment": null, "summary": "Test-time entropy minimization helps adapt a model to novel environments and\nincentivize its reasoning capability, unleashing the model's potential during\ninference by allowing it to evolve and improve in real-time using its own\npredictions, achieving promising performance. However, pure entropy\nminimization can favor non-generalizable shortcuts, such as inflating the logit\nnorm and driving all predictions to a dominant class to reduce entropy, risking\ncollapsed solutions (e.g., constant one-hot outputs) that trivially minimize\nthe objective without meaningful learning. In this paper, we introduce\nZeroSiam, an efficient asymmetric Siamese architecture tailored for test-time\nentropy minimization. ZeroSiam prevents collapse through asymmetric divergence\nalignment, which is efficiently achieved by a learnable predictor and a\nstop-gradient operator before the classifier. We provide empirical and\ntheoretical evidence that ZeroSiam not only prevents collapse solutions, but\nalso absorbs and regularizes biased learning signals, enhancing performance\neven when no collapse occurs. Despite its simplicity, extensive results show\nthat ZeroSiam performs more stably over prior methods using negligible\noverhead, demonstrating efficacy on both vision adaptation and large language\nmodel reasoning tasks across challenging test scenarios and diverse models,\nincluding tiny models that are particularly collapse-prone.", "AI": {"tldr": "ZeroSiam is an asymmetric Siamese architecture that prevents collapse in test-time entropy minimization by using asymmetric divergence alignment with a learnable predictor and stop-gradient operator, achieving stable performance across vision and language tasks.", "motivation": "Pure entropy minimization during test-time adaptation can lead to collapsed solutions like constant one-hot outputs that trivially minimize entropy without meaningful learning, favoring non-generalizable shortcuts.", "method": "ZeroSiam uses an asymmetric Siamese architecture with asymmetric divergence alignment, implemented via a learnable predictor and stop-gradient operator before the classifier to prevent collapse.", "result": "ZeroSiam performs more stably than prior methods with negligible overhead, works effectively on both vision adaptation and large language model reasoning tasks, and handles collapse-prone tiny models.", "conclusion": "ZeroSiam successfully prevents collapse in test-time entropy minimization while absorbing and regularizing biased learning signals, demonstrating efficacy across diverse models and challenging test scenarios."}}
{"id": "2509.23486", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.23486", "abs": "https://arxiv.org/abs/2509.23486", "authors": ["Sydney Peters", "Nan Zhang", "Hong Jiao", "Ming Li", "Tianyi Zhou", "Robert Lissitz"], "title": "Text-Based Approaches to Item Difficulty Modeling in Large-Scale Assessments: A Systematic Review", "comment": "45 pages, 9 figures", "summary": "Item difficulty plays a crucial role in test performance, interpretability of\nscores, and equity for all test-takers, especially in large-scale assessments.\nTraditional approaches to item difficulty modeling rely on field testing and\nclassical test theory (CTT)-based item analysis or item response theory (IRT)\ncalibration, which can be time-consuming and costly. To overcome these\nchallenges, text-based approaches leveraging machine learning and language\nmodels, have emerged as promising alternatives. This paper reviews and\nsynthesizes 37 articles on automated item difficulty prediction in large-scale\nassessment settings published through May 2025. For each study, we delineate\nthe dataset, difficulty parameter, subject domain, item type, number of items,\ntraining and test data split, input, features, model, evaluation criteria, and\nmodel performance outcomes. Results showed that although classic machine\nlearning models remain relevant due to their interpretability, state-of-the-art\nlanguage models, using both small and large transformer-based architectures,\ncan capture syntactic and semantic patterns without the need for manual feature\nengineering. Uniquely, model performance outcomes were summarized to serve as a\nbenchmark for future research and overall, text-based methods have the\npotential to predict item difficulty with root mean square error (RMSE) as low\nas 0.165, Pearson correlation as high as 0.87, and accuracy as high as 0.806.\nThe review concludes by discussing implications for practice and outlining\nfuture research directions for automated item difficulty modeling.", "AI": {"tldr": "This paper reviews 37 studies on automated item difficulty prediction using text-based machine learning approaches, showing that transformer-based language models can effectively predict item difficulty without manual feature engineering, achieving RMSE as low as 0.165 and Pearson correlation up to 0.87.", "motivation": "Traditional item difficulty modeling through field testing and CTT/IRT approaches is time-consuming and costly, creating a need for more efficient automated methods using text-based machine learning approaches.", "method": "Systematic review of 37 articles analyzing automated item difficulty prediction, examining datasets, difficulty parameters, subject domains, item types, input features, models, and evaluation criteria across different approaches including classic ML and transformer-based language models.", "result": "Text-based methods can predict item difficulty with RMSE as low as 0.165, Pearson correlation up to 0.87, and accuracy up to 0.806. Transformer models capture syntactic and semantic patterns without manual feature engineering, while classic ML models remain relevant for interpretability.", "conclusion": "Text-based automated methods show strong potential for item difficulty prediction, with performance benchmarks established for future research. The review discusses practical implications and outlines future research directions for automated item difficulty modeling."}}
{"id": "2509.23783", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23783", "abs": "https://arxiv.org/abs/2509.23783", "authors": ["Qi Xue", "Minrui Jiang", "Runjia Zhang", "Xiurui Xie", "Pei Ke", "Guisong Liu"], "title": "Falcon: A Cross-Modal Evaluation Dataset for Comprehensive Safety Perception", "comment": null, "summary": "Existing methods for evaluating the harmfulness of content generated by large\nlanguage models (LLMs) have been well studied. However, approaches tailored to\nmultimodal large language models (MLLMs) remain underdeveloped and lack depth.\nThis work highlights the crucial role of visual information in moderating\ncontent in visual question answering (VQA), a dimension often overlooked in\ncurrent research. To bridge this gap, we introduce Falcon, a large-scale\nvision-language safety dataset containing 57,515 VQA pairs across 13 harm\ncategories. The dataset provides explicit annotations for harmful attributes\nacross images, instructions, and responses, thereby facilitating a\ncomprehensive evaluation of the content generated by MLLMs. In addition, it\nincludes the relevant harm categories along with explanations supporting the\ncorresponding judgments. We further propose FalconEye, a specialized evaluator\nfine-tuned from Qwen2.5-VL-7B using the Falcon dataset. Experimental results\ndemonstrate that FalconEye reliably identifies harmful content in complex and\nsafety-critical multimodal dialogue scenarios. It outperforms all other\nbaselines in overall accuracy across our proposed Falcon-test dataset and two\nwidely-used benchmarks-VLGuard and Beavertail-V, underscoring its potential as\na practical safety auditing tool for MLLMs.", "AI": {"tldr": "This paper introduces Falcon, a large-scale vision-language safety dataset with 57,515 VQA pairs across 13 harm categories, and FalconEye, a specialized evaluator fine-tuned from Qwen2.5-VL-7B that reliably identifies harmful content in multimodal dialogue scenarios.", "motivation": "Existing methods for evaluating harmful content are well-developed for text-only LLMs but remain underdeveloped for multimodal LLMs (MLLMs), particularly overlooking the role of visual information in moderating content in visual question answering.", "method": "Created Falcon dataset with explicit annotations for harmful attributes across images, instructions, and responses across 13 harm categories. Developed FalconEye, a specialized evaluator fine-tuned from Qwen2.5-VL-7B using the Falcon dataset.", "result": "FalconEye outperforms all other baselines in overall accuracy across Falcon-test dataset and two widely-used benchmarks (VLGuard and Beavertail-V), demonstrating reliable identification of harmful content in complex multimodal dialogue scenarios.", "conclusion": "FalconEye shows potential as a practical safety auditing tool for MLLMs, effectively bridging the gap in multimodal content safety evaluation through comprehensive dataset creation and specialized model development."}}
{"id": "2509.23893", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.23893", "abs": "https://arxiv.org/abs/2509.23893", "authors": ["Zhixin Zhang", "Zeming Wei", "Meng Sun"], "title": "Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings", "comment": null, "summary": "Catastrophic forgetting remains a critical challenge in continual learning\nfor large language models (LLMs), where models struggle to retain performance\non historical tasks when fine-tuning on new sequential data without access to\npast datasets. In this paper, we first reveal that the drift of functional\ndirections during the fine-tuning process is a key reason why existing\nregularization-based methods fail in long-term LLM continual learning. To\naddress this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a\nnovel approach that tracks the drift of these functional directions and\ndynamically updates them during the fine-tuning process. Furthermore, by\nadjusting the gradients of new task parameters to be orthogonal to the tracked\nhistorical function directions, our method mitigates interference between new\nand old tasks. Extensive experiments on various LLM continual learning\nbenchmarks demonstrate that this approach outperforms prior methods,\neffectively reducing catastrophic forgetting and providing a robust tool for\ncontinuous LLM fine-tuning. Our code is available at\nhttps://github.com/meloxxxxxx/DOC.", "AI": {"tldr": "DOC fine-tuning addresses catastrophic forgetting in LLMs by tracking functional direction drift and making new task gradients orthogonal to historical directions.", "motivation": "Catastrophic forgetting in LLM continual learning, where models lose performance on historical tasks when fine-tuning on new data without access to past datasets.", "method": "Dynamic Orthogonal Continual (DOC) fine-tuning that tracks functional direction drift and adjusts new task gradients to be orthogonal to historical function directions.", "result": "Outperforms prior methods on various LLM continual learning benchmarks, effectively reducing catastrophic forgetting.", "conclusion": "DOC provides a robust tool for continuous LLM fine-tuning by mitigating interference between new and old tasks through orthogonal gradient adjustment."}}
{"id": "2509.23190", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23190", "abs": "https://arxiv.org/abs/2509.23190", "authors": ["Zhanhong Xie", "Meifan Zhang", "Lihua Yin"], "title": "CoSIFL: Collaborative Secure and Incentivized Federated Learning with Differential Privacy", "comment": null, "summary": "Federated learning (FL) has emerged as a promising paradigm for collaborative\nmodel training while preserving data locality. However, it still faces\nchallenges from malicious or compromised clients, as well as difficulties in\nincentivizing participants to contribute high-quality data under strict privacy\nrequirements. Motivated by these considerations, we propose CoSIFL, a novel\nframework that integrates proactive alarming for robust security and local\ndifferential privacy (LDP) for inference attacks, together with a\nStackelberg-based incentive scheme to encourage client participation and data\nsharing. Specifically, CoSIFL uses an active alarming mechanism and robust\naggregation to defend against Byzantine and inference attacks, while a Tullock\ncontest-inspired incentive module rewards honest clients for both data\ncontributions and reliable alarm triggers. We formulate the interplay between\nthe server and clients as a two-stage game: in the first stage, the server\ndetermines total rewards, selects participants, and fixes global iteration\nsettings, whereas in the second stage, each client decides its mini-batch size,\nprivacy noise scale, and alerting strategy. We prove that the server-client\ngame admits a unique equilibrium, and analyze how clients' multi-dimensional\nattributes - such as non-IID degrees and privacy budgets - jointly affect\nsystem efficiency. Experimental results on standard benchmarks demonstrate that\nCoSIFL outperforms state-of-the-art solutions in improving model robustness and\nreducing total server costs, highlighting the effectiveness of our integrated\ndesign.", "AI": {"tldr": "CoSIFL is a federated learning framework that combines proactive alarming, local differential privacy, and Stackelberg-based incentives to address security threats and encourage client participation while maintaining privacy.", "motivation": "Federated learning faces challenges from malicious clients and difficulties in incentivizing participants to contribute high-quality data under strict privacy requirements.", "method": "CoSIFL integrates proactive alarming for security, local differential privacy for inference attacks, and a Stackelberg-based incentive scheme. It uses active alarming mechanism, robust aggregation, and Tullock contest-inspired incentive module. The framework models server-client interaction as a two-stage game.", "result": "Experimental results show CoSIFL outperforms state-of-the-art solutions in improving model robustness and reducing total server costs. The server-client game admits a unique equilibrium.", "conclusion": "CoSIFL effectively addresses security threats and incentivization challenges in federated learning through its integrated design of proactive alarming, privacy protection, and incentive mechanisms."}}
{"id": "2509.23501", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.23501", "abs": "https://arxiv.org/abs/2509.23501", "authors": ["Hamidreza Rouzegar", "Masoud Makrehchi"], "title": "The Impact of Role Design in In-Context Learning for Large Language Models", "comment": "Code is available at\n  https://github.com/hrouzegar/Role_Based-In-Context-Learning", "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to generate\npredictions based on prompts without additional fine-tuning. While prompt\nengineering has been widely studied, the impact of role design within prompts\nremains underexplored. This study examines the influence of role configurations\nin zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from\nOpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models'\nperformance across datasets, focusing on tasks like sentiment analysis, text\nclassification, question answering, and math reasoning. Our findings suggest\nthe potential of role-based prompt structuring to enhance LLM performance.", "AI": {"tldr": "This paper explores how role configurations in prompts affect LLM performance in zero-shot and few-shot learning, testing on GPT-3.5, GPT-4o, Llama2-7b, and Llama2-13b across various tasks.", "motivation": "While prompt engineering is well-studied, the specific impact of role design within prompts remains underexplored in in-context learning scenarios.", "method": "Evaluated multiple LLMs (GPT-3.5, GPT-4o, Llama2-7b, Llama2-13b) across datasets for sentiment analysis, text classification, question answering, and math reasoning tasks using role-based prompt configurations.", "result": "Findings suggest that role-based prompt structuring has potential to enhance LLM performance in various learning scenarios.", "conclusion": "Role configurations in prompts show promise for improving LLM capabilities in in-context learning without additional fine-tuning."}}
{"id": "2509.23796", "categories": ["cs.AI", "cs.MM", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.23796", "abs": "https://arxiv.org/abs/2509.23796", "authors": ["Matthew McConnell", "Richard Zhao"], "title": "From Frustration to Fun: An Adaptive Problem-Solving Puzzle Game Powered by Genetic Algorithm", "comment": "Accepted at the AAAI Conference on Artificial Intelligence and\n  Interactive Digital Entertainment (AIIDE-25)", "summary": "This paper explores adaptive problem solving with a game designed to support\nthe development of problem-solving skills. Using an adaptive, AI-powered puzzle\ngame, our adaptive problem-solving system dynamically generates\npathfinding-based puzzles using a genetic algorithm, tailoring the difficulty\nof each puzzle to individual players in an online real-time approach. A\nplayer-modeling system records user interactions and informs the generation of\npuzzles to approximate a target difficulty level based on various metrics of\nthe player. By combining procedural content generation with online adaptive\ndifficulty adjustment, the system aims to maintain engagement, mitigate\nfrustration, and maintain an optimal level of challenge. A pilot user study\ninvestigates the effectiveness of this approach, comparing different types of\nadaptive difficulty systems and interpreting players' responses. This work lays\nthe foundation for further research into emotionally informed player models,\nadvanced AI techniques for adaptivity, and broader applications beyond gaming\nin educational settings.", "AI": {"tldr": "An adaptive AI-powered puzzle game that dynamically generates pathfinding puzzles using genetic algorithms, tailoring difficulty to individual players in real-time to maintain optimal challenge levels.", "motivation": "To develop problem-solving skills through adaptive gaming that maintains engagement, mitigates frustration, and provides personalized challenge levels.", "method": "Combines procedural content generation with online adaptive difficulty adjustment using genetic algorithms and player modeling that records user interactions to generate puzzles at target difficulty levels.", "result": "A pilot user study compared different adaptive difficulty systems and interpreted player responses, showing the approach's effectiveness.", "conclusion": "This work establishes a foundation for emotionally informed player models, advanced AI adaptivity techniques, and broader educational applications beyond gaming."}}
{"id": "2509.24368", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.24368", "abs": "https://arxiv.org/abs/2509.24368", "authors": ["Thibaud Gloaguen", "Robin Staab", "Nikola Jovanovi\u0107", "Martin Vechev"], "title": "Watermarking Diffusion Language Models", "comment": null, "summary": "We introduce the first watermark tailored for diffusion language models\n(DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order, in\ncontrast to standard autoregressive language models (ARLMs) which generate\ntokens sequentially. While there has been much work in ARLM watermarking, a key\nchallenge when attempting to apply these schemes directly to the DLM setting is\nthat they rely on previously generated tokens, which are not always available\nwith DLM generation. In this work we address this challenge by: (i) applying\nthe watermark in expectation over the context even when some context tokens are\nyet to be determined, and (ii) promoting tokens which increase the watermark\nstrength when used as context for other tokens. This is accomplished while\nkeeping the watermark detector unchanged. Our experimental evaluation\ndemonstrates that the DLM watermark leads to a >99% true positive rate with\nminimal quality impact and achieves similar robustness to existing ARLM\nwatermarks, enabling for the first time reliable DLM watermarking.", "AI": {"tldr": "First watermark for diffusion language models (DLMs) that works with arbitrary token generation order, achieving >99% true positive rate with minimal quality impact.", "motivation": "Existing ARLM watermarks rely on sequential token generation, which doesn't work for DLMs that generate tokens in arbitrary order. Need specialized watermark for this new paradigm.", "method": "Apply watermark in expectation over context when some tokens are undetermined, and promote tokens that increase watermark strength when used as context for other tokens.", "result": ">99% true positive rate with minimal quality impact, similar robustness to ARLM watermarks.", "conclusion": "Enables reliable DLM watermarking for the first time, addressing the unique challenges of diffusion language models."}}
{"id": "2509.23202", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23202", "abs": "https://arxiv.org/abs/2509.23202", "authors": ["Vage Egiazarian", "Roberto L. Castro", "Denis Kuznedelev", "Andrei Panferov", "Eldar Kurtic", "Shubhra Pandit", "Alexandre Marques", "Mark Kurtz", "Saleh Ashkboos", "Torsten Hoefler", "Dan Alistarh"], "title": "Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization", "comment": null, "summary": "The recent hardware-accelerated microscaling 4-bit floating-point formats\nsuch as MXFP4 and NVFP4, supported on NVIDIA and AMD GPUs, promise to\nrevolutionize large language model (LLM) inference. Yet, their practical\nbenefits remain unproven. We present the first comprehensive study of MXFP4 and\nNVFP4 for post-training quantization, revealing gaps between their promise and\nreal-world performance. Our analysis shows that state-of-the-art methods\nstruggle with FP4, due to two key issues: (1) NVFP4's small group size provably\nneutralizes traditional outlier mitigation techniques; (2) MXFP4's power-of-two\nscale quantization severely degrades accuracy due to high induced error. To\nbridge this gap, we introduce Micro-Rotated-GPTQ (MR-GPTQ), a variant of the\nclassic GPTQ quantization algorithm that tailors the quantization process to\nFP4's unique properties, by using block-wise Hadamard transforms and\nformat-specific optimizations. We support our proposal with a set of\nhigh-performance GPU kernels that enable the MR-GPTQ format with negligible\noverhead, by rotation fusion into the weights, and fast online computation of\nthe activations. This leads to speedups vs. FP16 of up to 3.6x layer-wise, and\n2.2x end-to-end on NVIDIA B200, and of 6x layer-wise and 4x end-to-end on\nRTX5090. Our extensive empirical evaluation demonstrates that MR-GPTQ matches\nor outperforms state-of-the-art accuracy, significantly boosting MXFP4, to the\npoint where it nears that of NVFP4. We conclude that, while FP4 is not an\nautomatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock\na new frontier of accuracy-performance trade-offs.", "AI": {"tldr": "This paper presents the first comprehensive study of MXFP4 and NVFP4 4-bit floating-point formats for LLM quantization, identifies their limitations, and proposes MR-GPTQ method with format-specific optimizations to bridge the performance gap.", "motivation": "Hardware-accelerated 4-bit floating-point formats (MXFP4 and NVFP4) promise to revolutionize LLM inference but their practical benefits remain unproven, with state-of-the-art methods struggling to achieve good performance.", "method": "Introduces Micro-Rotated-GPTQ (MR-GPTQ), a variant of GPTQ quantization algorithm that uses block-wise Hadamard transforms and format-specific optimizations, supported by high-performance GPU kernels with rotation fusion and fast online activation computation.", "result": "Achieves speedups vs. FP16 of up to 3.6x layer-wise and 2.2x end-to-end on NVIDIA B200, and 6x layer-wise and 4x end-to-end on RTX5090. MR-GPTQ significantly boosts MXFP4 accuracy to near NVFP4 levels.", "conclusion": "While FP4 is not an automatic upgrade over INT4, format-specialized methods like MR-GPTQ can unlock new accuracy-performance trade-offs for 4-bit floating-point quantization."}}
{"id": "2509.23504", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23504", "abs": "https://arxiv.org/abs/2509.23504", "authors": ["Bassam Matar", "Mohamed Fayed", "Ayman Khalafallah"], "title": "AraS2P: Arabic Speech-to-Phonemes System", "comment": null, "summary": "This paper describes AraS2P, our speech-to-phonemes system submitted to the\nIqra'Eval 2025 Shared Task. We adapted Wav2Vec2-BERT via Two-Stage training\nstrategy. In the first stage, task-adaptive continue pretraining was performed\non large-scale Arabic speech-phonemes datasets, which were generated by\nconverting the Arabic text using the MSA Phonetiser. In the second stage, the\nmodel was fine-tuned on the official shared task data, with additional\naugmentation from XTTS-v2-synthesized recitations featuring varied Ayat\nsegments, speaker embeddings, and textual perturbations to simulate possible\nhuman errors. The system ranked first on the official leaderboard,\ndemonstrating that phoneme-aware pretraining combined with targeted\naugmentation yields strong performance in phoneme-level mispronunciation\ndetection.", "AI": {"tldr": "AraS2P is a speech-to-phonemes system that won first place in Iqra'Eval 2025 by using Wav2Vec2-BERT with two-stage training: phoneme-aware pretraining on Arabic speech-phonemes data and fine-tuning with targeted augmentation.", "motivation": "To develop an effective system for phoneme-level mispronunciation detection in Arabic speech, addressing the need for accurate pronunciation assessment in recitation tasks.", "method": "Two-stage training: 1) Task-adaptive continue pretraining on large-scale Arabic speech-phonemes datasets generated using MSA Phonetiser, 2) Fine-tuning on official shared task data with augmentation from XTTS-v2-synthesized recitations featuring varied Ayat segments, speaker embeddings, and textual perturbations.", "result": "The system ranked first on the official leaderboard of Iqra'Eval 2025 Shared Task.", "conclusion": "Phoneme-aware pretraining combined with targeted augmentation yields strong performance in phoneme-level mispronunciation detection."}}
{"id": "2509.23811", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23811", "abs": "https://arxiv.org/abs/2509.23811", "authors": ["Rakesh Thakur", "Diksha Khandelwal", "Shreya Tiwari"], "title": "AnveshanaAI: A Multimodal Platform for Adaptive AI/ML Education through Automated Question Generation and Interactive Assessment", "comment": "11 pages, 12 figures. Under review as a conference paper at ICLR\n  2026. Preprint version posted on arXiv", "summary": "We propose AnveshanaAI, an application-based learning platform for artificial\nintelligence. With AnveshanaAI, learners are presented with a personalized\ndashboard featuring streaks, levels, badges, and structured navigation across\ndomains such as data science, machine learning, deep learning, transformers,\ngenerative AI, large language models, and multimodal AI, with scope to include\nmore in the future. The platform incorporates gamified tracking with points and\nachievements to enhance engagement and learning, while switching between\nPlayground, Challenges, Simulator, Dashboard, and Community supports\nexploration and collaboration. Unlike static question repositories used in\nexisting platforms, AnveshanaAI ensures balanced learning progression through a\ndataset grounded in Bloom's taxonomy, with semantic similarity checks and\nexplainable AI techniques improving transparency and reliability. Adaptive,\nautomated, and domain-aware assessment methods are also employed. Experiments\ndemonstrate broad dataset coverage, stable fine-tuning with reduced perplexity,\nand measurable gains in learner engagement. Together, these features illustrate\nhow AnveshanaAI integrates adaptivity, gamification, interactivity, and\nexplainability to support next-generation AI education.", "AI": {"tldr": "AnveshanaAI is a gamified AI learning platform with personalized dashboards, adaptive assessments based on Bloom's taxonomy, and explainable AI techniques to enhance engagement and learning outcomes.", "motivation": "To address limitations of static question repositories in existing platforms and provide a more engaging, adaptive, and transparent AI education experience.", "method": "Uses gamified tracking (streaks, levels, badges), structured navigation across AI domains, semantic similarity checks, explainable AI techniques, and adaptive domain-aware assessment methods grounded in Bloom's taxonomy.", "result": "Experiments show broad dataset coverage, stable fine-tuning with reduced perplexity, and measurable gains in learner engagement.", "conclusion": "AnveshanaAI successfully integrates adaptivity, gamification, interactivity, and explainability to support next-generation AI education."}}
{"id": "2509.24488", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24488", "abs": "https://arxiv.org/abs/2509.24488", "authors": ["Wenjie Fu", "Huandong Wang", "Junyao Gao", "Guoan Wan", "Tao Jiang"], "title": "Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models", "comment": null, "summary": "As Large Language Models (LLMs) achieve remarkable success across a wide\nrange of applications, such as chatbots and code copilots, concerns surrounding\nthe generation of harmful content have come increasingly into focus. Despite\nsignificant advances in aligning LLMs with safety and ethical standards,\nadversarial prompts can still be crafted to elicit undesirable responses.\nExisting mitigation strategies are predominantly based on post-hoc filtering,\nwhich introduces substantial latency or computational overhead, and is\nincompatible with token-level streaming generation. In this work, we introduce\nSelf-Sanitize, a novel LLM-driven mitigation framework inspired by cognitive\npsychology, which emulates human self-monitor and self-repair behaviors during\nconversations. Self-Sanitize comprises a lightweight Self-Monitor module that\ncontinuously inspects high-level intentions within the LLM at the token level\nvia representation engineering, and a Self-Repair module that performs in-place\ncorrection of harmful content without initiating separate review dialogues.\nThis design allows for real-time streaming monitoring and seamless repair, with\nnegligible impact on latency and resource utilization. Given that\nprivacy-invasive content has often been insufficiently focused in previous\nstudies, we perform extensive experiments on four LLMs across three privacy\nleakage scenarios. The results demonstrate that Self-Sanitize achieves superior\nmitigation performance with minimal overhead and without degrading the utility\nof LLMs, offering a practical and robust solution for safer LLM deployments.\nOur code is available at the following link:\nhttps://github.com/wjfu99/LLM_Self_Sanitize", "AI": {"tldr": "Self-Sanitize is a novel LLM-driven framework that uses cognitive psychology principles to enable real-time monitoring and self-repair of harmful content generation in large language models, addressing privacy leakage scenarios with minimal overhead.", "motivation": "Current mitigation strategies for harmful content in LLMs rely on post-hoc filtering, which introduces latency, computational overhead, and is incompatible with token-level streaming generation. There's also insufficient focus on privacy-invasive content in existing studies.", "method": "Self-Sanitize consists of two modules: a lightweight Self-Monitor that continuously inspects high-level intentions via representation engineering at token level, and a Self-Repair module that performs in-place correction of harmful content without separate review dialogues.", "result": "Extensive experiments on four LLMs across three privacy leakage scenarios demonstrate superior mitigation performance with minimal overhead, without degrading LLM utility.", "conclusion": "Self-Sanitize offers a practical and robust solution for safer LLM deployments by enabling real-time streaming monitoring and seamless repair with negligible impact on latency and resource utilization."}}
{"id": "2509.23209", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23209", "abs": "https://arxiv.org/abs/2509.23209", "authors": ["Wenhao Zhang", "Shao Zhang", "Xihuai Wang", "Yang Li", "Ying Wen"], "title": "Towards Monotonic Improvement in In-Context Reinforcement Learning", "comment": null, "summary": "In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm\nfor developing agents that can rapidly adapt to new tasks by leveraging past\nexperiences as context, without updating their parameters. Recent approaches\ntrain large sequence models on monotonic policy improvement data from online\nRL, aiming to a continue improved testing time performance. However, our\nexperimental analysis reveals a critical flaw: these models cannot show a\ncontinue improvement like the training data during testing time. Theoretically,\nwe identify this phenomenon as Contextual Ambiguity, where the model's own\nstochastic actions can generate an interaction history that misleadingly\nresembles that of a sub-optimal policy from the training data, initiating a\nvicious cycle of poor action selection. To resolve the Contextual Ambiguity, we\nintroduce Context Value into training phase and propose Context Value Informed\nICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing\nthe ideal performance theoretically achievable by a policy given the current\ncontext. As the context expands, Context Value could include more task-relevant\ninformation, and therefore the ideal performance should be non-decreasing. We\nprove that the Context Value tightens the lower bound on the performance gap\nrelative to an ideal, monotonically improving policy. We fruther propose two\nmethods for estimating Context Value at both training and testing time.\nExperiments conducted on the Dark Room and Minigrid testbeds demonstrate that\nCV-ICRL effectively mitigates performance degradation and improves overall ICRL\nabilities across various tasks and environments. The source code and data of\nthis paper are available at\nhttps://github.com/Bluixe/towards_monotonic_improvement .", "AI": {"tldr": "The paper identifies Contextual Ambiguity as a key limitation in In-Context Reinforcement Learning (ICRL) where models fail to show continued improvement during testing, and proposes Context Value Informed ICRL (CV-ICRL) to address this issue.", "motivation": "Current ICRL approaches trained on monotonic policy improvement data cannot achieve continued improvement during testing due to Contextual Ambiguity - where the model's stochastic actions generate misleading interaction histories.", "method": "Proposes CV-ICRL that introduces Context Value as an explicit signal representing ideal performance achievable given current context. Uses two methods for estimating Context Value during training and testing, ensuring non-decreasing ideal performance as context expands.", "result": "Experiments on Dark Room and Minigrid testbeds show CV-ICRL effectively mitigates performance degradation and improves overall ICRL abilities across various tasks and environments.", "conclusion": "CV-ICRL successfully resolves Contextual Ambiguity by incorporating Context Value, enabling continued performance improvement during testing and enhancing ICRL capabilities."}}
{"id": "2509.23515", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23515", "abs": "https://arxiv.org/abs/2509.23515", "authors": ["Dania Refai", "Alaa Dalaq", "Doaa Dalaq", "Irfan Ahmad"], "title": "From Human Annotation to Automation: LLM-in-the-Loop Active Learning for Arabic Sentiment Analysis", "comment": null, "summary": "Natural language processing (NLP), particularly sentiment analysis, plays a\nvital role in areas like marketing, customer service, and social media\nmonitoring by providing insights into user opinions and emotions. However,\nprogress in Arabic sentiment analysis remains limited due to the lack of large,\nhigh-quality labeled datasets. While active learning has proven effective in\nreducing annotation efforts in other languages, few studies have explored it in\nArabic sentiment tasks. Likewise, the use of large language models (LLMs) for\nassisting annotation and comparing their performance to human labeling is still\nlargely unexplored in the Arabic context. In this paper, we propose an active\nlearning framework for Arabic sentiment analysis designed to reduce annotation\ncosts while maintaining high performance. We evaluate multiple deep learning\narchitectures: Specifically, long short-term memory (LSTM), gated recurrent\nunits (GRU), and recurrent neural networks (RNN), across three benchmark\ndatasets: Hunger Station, AJGT, and MASAC, encompassing both modern standard\nArabic and dialectal variations. Additionally, two annotation strategies are\ncompared: Human labeling and LLM-assisted labeling. Five LLMs are evaluated as\nannotators: GPT-4o, Claude 3 Sonnet, Gemini 2.5 Pro, DeepSeek Chat, and LLaMA 3\n70B Instruct. For each dataset, the best-performing LLM was used: GPT-4o for\nHunger Station, Claude 3 Sonnet for AJGT, and DeepSeek Chat for MASAC. Our\nresults show that LLM-assisted active learning achieves competitive or superior\nperformance compared to human labeling. For example, on the Hunger Station\ndataset, the LSTM model achieved 93% accuracy with only 450 labeled samples\nusing GPT-4o-generated labels, while on the MASAC dataset, DeepSeek Chat\nreached 82% accuracy with 650 labeled samples, matching the accuracy obtained\nthrough human labeling.", "AI": {"tldr": "Proposed an active learning framework for Arabic sentiment analysis using LLM-assisted labeling to reduce annotation costs while maintaining high performance across multiple datasets.", "motivation": "Arabic sentiment analysis is limited by lack of large labeled datasets. Active learning and LLM-assisted annotation are underexplored in Arabic context despite their proven effectiveness in other languages.", "method": "Active learning framework with multiple deep learning architectures (LSTM, GRU, RNN) evaluated on three Arabic datasets. Compared human labeling vs LLM-assisted labeling using five LLMs (GPT-4o, Claude 3 Sonnet, Gemini 2.5 Pro, DeepSeek Chat, LLaMA 3 70B Instruct).", "result": "LLM-assisted active learning achieved competitive or superior performance to human labeling. LSTM with GPT-4o achieved 93% accuracy with only 450 samples on Hunger Station dataset. DeepSeek Chat reached 82% accuracy with 650 samples on MASAC dataset, matching human labeling performance.", "conclusion": "LLM-assisted active learning is effective for Arabic sentiment analysis, significantly reducing annotation costs while maintaining or improving performance compared to traditional human labeling approaches."}}
{"id": "2509.23836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23836", "abs": "https://arxiv.org/abs/2509.23836", "authors": ["Chenyu Zhou", "Xiaoming Shi", "Hui Qiu", "Xiawu Zheng", "Haitao Leng", "Yankai Jiang", "Shaoguo Liu", "Tingting Gao", "Rongrong Ji"], "title": "Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain Rules", "comment": null, "summary": "E-commerce agents contribute greatly to helping users complete their\ne-commerce needs. To promote further research and application of e-commerce\nagents, benchmarking frameworks are introduced for evaluating LLM agents in the\ne-commerce domain. Despite the progress, current benchmarks lack evaluating\nagents' capability to handle mixed-type e-commerce dialogue and complex domain\nrules. To address the issue, this work first introduces a novel corpus, termed\nMix-ECom, which is constructed based on real-world customer-service dialogues\nwith post-processing to remove user privacy and add CoT process. Specifically,\nMix-ECom contains 4,799 samples with multiply dialogue types in each e-commerce\ndialogue, covering four dialogue types (QA, recommendation, task-oriented\ndialogue, and chit-chat), three e-commerce task types (pre-sales, logistics,\nafter-sales), and 82 e-commerce rules. Furthermore, this work build baselines\non Mix-Ecom and propose a dynamic framework to further improve the performance.\nResults show that current e-commerce agents lack sufficient capabilities to\nhandle e-commerce dialogues, due to the hallucination cased by complex domain\nrules. The dataset will be publicly available.", "AI": {"tldr": "This paper introduces Mix-ECom, a novel corpus for evaluating e-commerce agents that handles mixed-type dialogues and complex domain rules, addressing limitations in current benchmarks.", "motivation": "Current e-commerce agent benchmarks lack evaluation of agents' capability to handle mixed-type e-commerce dialogues and complex domain rules, which limits research and application development.", "method": "Constructed Mix-ECom corpus based on real-world customer-service dialogues with privacy removal and CoT process addition. Contains 4,799 samples covering multiple dialogue types, e-commerce tasks, and 82 domain rules. Built baselines and proposed a dynamic framework for performance improvement.", "result": "Results show current e-commerce agents lack sufficient capabilities to handle e-commerce dialogues, primarily due to hallucinations caused by complex domain rules.", "conclusion": "The Mix-ECom dataset addresses critical gaps in e-commerce agent evaluation and will be publicly available to promote further research in this domain."}}
{"id": "2509.23213", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23213", "abs": "https://arxiv.org/abs/2509.23213", "authors": ["Hugo Math", "Robin Sch\u00f6n", "Rainer Lienhart"], "title": "One-Shot Multi-Label Causal Discovery in High-Dimensional Event Sequences", "comment": "Accepted at NeuRIPS2025 Workshop CauScien: Discovering Causality in\n  Science. arXiv admin note: substantial text overlap with arXiv:2509.19112", "summary": "Understanding causality in event sequences with thousands of sparse event\ntypes is critical in domains such as healthcare, cybersecurity, or vehicle\ndiagnostics, yet current methods fail to scale. We present OSCAR, a one-shot\ncausal autoregressive method that infers per-sequence Markov Boundaries using\ntwo pretrained Transformers as density estimators. This enables efficient,\nparallel causal discovery without costly global CI testing. On a real-world\nautomotive dataset with 29,100 events and 474 labels, OSCAR recovers\ninterpretable causal structures in minutes, while classical methods fail to\nscale, enabling practical scientific diagnostics at production scale.", "AI": {"tldr": "OSCAR is a one-shot causal autoregressive method that efficiently infers per-sequence Markov Boundaries using pretrained Transformers as density estimators, enabling scalable causal discovery without costly global conditional independence testing.", "motivation": "Current causal discovery methods fail to scale to event sequences with thousands of sparse event types in domains like healthcare, cybersecurity, and vehicle diagnostics, where understanding causality is critical.", "method": "Uses two pretrained Transformers as density estimators to infer per-sequence Markov Boundaries in a one-shot causal autoregressive approach, avoiding expensive global conditional independence testing.", "result": "On a real-world automotive dataset with 29,100 events and 474 labels, OSCAR recovers interpretable causal structures in minutes, while classical methods fail to scale.", "conclusion": "OSCAR enables practical scientific diagnostics at production scale by providing efficient, parallel causal discovery for large-scale event sequence data."}}
{"id": "2509.23542", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23542", "abs": "https://arxiv.org/abs/2509.23542", "authors": ["Janvijay Singh", "Austin Xu", "Yilun Zhou", "Yefan Zhou", "Dilek Hakkani-Tur", "Shafiq Joty"], "title": "On the Shelf Life of Fine-Tuned LLM Judges: Future Proofing, Backward Compatibility, and Question Generalization", "comment": "21 pages", "summary": "The LLM-as-a-judge paradigm is widely used in both evaluating free-text model\nresponses and reward modeling for model alignment and finetuning. Recently,\nfinetuning judges with judge-specific data has emerged as an often preferred\nchoice over directly prompting frontier models as judges, as the former\nachieves better performance with smaller model sizes while being more robust to\ncommon biases. However, the standard evaluation ignores several practical\nconcerns of finetuned judges regarding their real world deployment. In this\npaper, we identify and formalize three aspects that affect the shelf life of\nthese judges: future proofing and backward compatibility -- how well judges\nfinetuned on responses by today's generator models perform on responses by\nfuture models or past models, as well as question generalization -- how well\njudges generalize to unseen questions at test time. We study these three\naspects in the math domain under a unified framework with varying train and\ntest distributions, three SFT- and DPO-based finetuning algorithms and three\ndifferent base models. Experiments suggest that future-proofing is challenging\nfor most models, while backward compatibility is relatively easy, with\nDPO-trained models consistently improving performance. We further find that\ncontinual learning provides a more balanced adaptation to shifts between older\nand newer response distributions than training solely on stronger or weaker\nresponses. Moreover, all models observe certain degrees of performance\ndegradation when moving from questions seen during training to unseen ones,\nshowing that current judges do not fully generalize to unseen questions. These\nfindings provide insights into practical considerations for developing and\ndeploying judge models in the face of ever-changing generators.", "AI": {"tldr": "The paper identifies three practical concerns for finetuned LLM judges: future proofing, backward compatibility, and question generalization. Experiments show future-proofing is challenging, backward compatibility is easier, and current judges don't fully generalize to unseen questions.", "motivation": "To address practical deployment concerns of finetuned LLM judges that are overlooked in standard evaluation, focusing on their shelf life across evolving generator models and unseen questions.", "method": "Study three aspects (future proofing, backward compatibility, question generalization) in math domain using unified framework with varying train/test distributions, three SFT- and DPO-based finetuning algorithms, and three base models.", "result": "Future-proofing is challenging for most models; backward compatibility is relatively easy with DPO-trained models consistently improving performance; continual learning provides balanced adaptation; all models show performance degradation on unseen questions.", "conclusion": "Current judges don't fully generalize to unseen questions, and practical deployment requires considering adaptation strategies for evolving generator models, with continual learning offering balanced performance across distribution shifts."}}
{"id": "2509.23864", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23864", "abs": "https://arxiv.org/abs/2509.23864", "authors": ["Roham Koohestani"], "title": "AgentGuard: Runtime Verification of AI Agents", "comment": "Accepted for publication in the proceedings of the 40th IEEE/ACM\n  International Conference on Automated Software Engineering, ASE 2025, in the\n  1st international workshop on Agentic Software Engineering (AgenticSE)", "summary": "The rapid evolution to autonomous, agentic AI systems introduces significant\nrisks due to their inherent unpredictability and emergent behaviors; this also\nrenders traditional verification methods inadequate and necessitates a shift\ntowards probabilistic guarantees where the question is no longer if a system\nwill fail, but the probability of its failure within given constraints. This\npaper presents AgentGuard, a framework for runtime verification of Agentic AI\nsystems that provides continuous, quantitative assurance through a new paradigm\ncalled Dynamic Probabilistic Assurance. AgentGuard operates as an inspection\nlayer that observes an agent's raw I/O and abstracts it into formal events\ncorresponding to transitions in a state model. It then uses online learning to\ndynamically build and update a Markov Decision Process (MDP) that formally\nmodels the agent's emergent behavior. Using probabilistic model checking, the\nframework then verifies quantitative properties in real-time.", "AI": {"tldr": "AgentGuard is a runtime verification framework for Agentic AI systems that provides probabilistic guarantees through Dynamic Probabilistic Assurance, using online learning to build MDP models and probabilistic model checking for real-time verification.", "motivation": "Traditional verification methods are inadequate for autonomous AI systems due to their unpredictability and emergent behaviors, necessitating probabilistic guarantees instead of deterministic verification.", "method": "AgentGuard operates as an inspection layer that observes agent I/O, abstracts it into formal events in a state model, uses online learning to dynamically build and update Markov Decision Processes (MDPs), and applies probabilistic model checking for real-time verification.", "result": "The framework enables continuous, quantitative assurance by modeling emergent behavior and verifying quantitative properties in real-time.", "conclusion": "AgentGuard provides a practical approach to runtime verification of agentic AI systems through probabilistic modeling and real-time analysis, addressing the limitations of traditional verification methods."}}
{"id": "2509.23219", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23219", "abs": "https://arxiv.org/abs/2509.23219", "authors": ["Xin Li", "Mengbing Liu", "Yiyang Zhu", "Wenhe Zhang", "Li Wei", "Jiancheng An", "Chau Yuen"], "title": "WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless Communications with Reinforcement Learning", "comment": "Project Homepage: https://lixin.ai/WirelessMathLM", "summary": "Large language models (LLMs) excel at general mathematical reasoning but fail\ncatastrophically on specialized technical mathematics. In wireless\ncommunications, where problems require precise manipulation of\ninformation-theoretic bounds, optimization constraints, and signal processing\nformulations, even state-of-the-art models struggle to achieve competent\nperformance. We present WirelessMathLM, demonstrating that compact models\n(0.5B-7B parameters) can match or exceed much larger models through\ndomain-specific reinforcement learning with verifiable rewards. Our key insight\nis that wireless mathematics problems possess a unique property--verifiable\ncorrectness--that enables effective reinforcement learning without human\nfeedback. We construct WirelessMathBench-XL, a comprehensive benchmark of 4,027\nproblems from 970 papers. Using Group Relative Policy Optimization (GRPO) with\nbinary verification rewards, we train models directly from base checkpoints\nwithout supervised warm-start. Our 7B model achieves 39.5% accuracy on\nWirelessMathBench-XL, approaching GPT-4o (40.4%) while using about 100 times\nfewer parameters than DeepSeek-R1 (671B, 57.4%). Remarkably, GRPO training\nnearly doubles performance across all model scales (0.5B +11%, 3B +103%, 7B\n+81%), with positive transfer to general mathematics benchmarks--our models\ngain +8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and\nAIME without any training on these tasks.", "AI": {"tldr": "WirelessMathLM demonstrates that compact models (0.5B-7B) can match larger models in wireless mathematics through domain-specific reinforcement learning with verifiable rewards, achieving near-GPT-4o performance while using 100x fewer parameters than DeepSeek-R1.", "motivation": "Large language models fail catastrophically on specialized technical mathematics like wireless communications, which require precise manipulation of information-theoretic bounds, optimization constraints, and signal processing formulations.", "method": "Use domain-specific reinforcement learning with verifiable rewards (Group Relative Policy Optimization with binary verification rewards), training directly from base checkpoints without supervised warm-start, leveraging the unique property of wireless mathematics problems having verifiable correctness.", "result": "7B model achieves 39.5% accuracy on WirelessMathBench-XL (4,027 problems from 970 papers), approaching GPT-4o (40.4%) while using about 100 times fewer parameters than DeepSeek-R1 (671B, 57.4%). GRPO training nearly doubles performance across all model scales (0.5B +11%, 3B +103%, 7B +81%).", "conclusion": "Compact models can excel in specialized technical domains through verifiable reinforcement learning, with positive transfer to general mathematics benchmarks (+8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and AIME without training on these tasks)."}}
{"id": "2509.23550", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23550", "abs": "https://arxiv.org/abs/2509.23550", "authors": ["Vardis Georgilas", "Themos Stafylakis"], "title": "Automatic Speech Recognition for Greek Medical Dictation", "comment": null, "summary": "Medical dictation systems are essential tools in modern healthcare, enabling\naccurate and efficient conversion of speech into written medical documentation.\nThe main objective of this paper is to create a domain-specific system for\nGreek medical speech transcriptions. The ultimate goal is to assist healthcare\nprofessionals by reducing the overload of manual documentation and improving\nworkflow efficiency. Towards this goal, we develop a system that combines\nautomatic speech recognition techniques with text correction model, allowing\nbetter handling of domain-specific terminology and linguistic variations in\nGreek. Our approach leverages both acoustic and textual modeling to create more\nrealistic and reliable transcriptions. We focused on adapting existing language\nand speech technologies to the Greek medical context, addressing challenges\nsuch as complex medical terminology and linguistic inconsistencies. Through\ndomain-specific fine-tuning, our system achieves more accurate and coherent\ntranscriptions, contributing to the development of practical language\ntechnologies for the Greek healthcare sector.", "AI": {"tldr": "This paper develops a domain-specific medical dictation system for Greek language that combines automatic speech recognition with text correction to handle medical terminology and linguistic variations.", "motivation": "To assist Greek healthcare professionals by reducing manual documentation workload and improving workflow efficiency through accurate speech-to-text conversion for medical dictations.", "method": "Combines automatic speech recognition techniques with text correction models, leveraging both acoustic and textual modeling. Uses domain-specific fine-tuning to adapt existing technologies to Greek medical context.", "result": "The system achieves more accurate and coherent transcriptions by better handling domain-specific terminology and linguistic variations in Greek medical speech.", "conclusion": "The developed system contributes to practical language technologies for Greek healthcare sector, successfully addressing challenges of complex medical terminology and linguistic inconsistencies."}}
{"id": "2509.23870", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23870", "abs": "https://arxiv.org/abs/2509.23870", "authors": ["Jingyu Liu", "Xiaopeng Wu", "Jingquan Peng", "Kehan Chen", "Chuan Yu", "Lizhong Ding", "Yong Liu"], "title": "Rethinking Reward Miscalibration of GRPO in Agentic RL", "comment": null, "summary": "Building autonomous agents capable of solving long-horizon, real-world tasks\nhas garnered significant research interest. But outcome based rewards may cause\nreward miscalibration which means it might mistakenly allocate positive reward\nto flawed middle steps which is regarded as the key reason making the bad\nactions being reinforced during training. However we reveal that outcome based\nreward ensures expected negative advantage for those flawed middle steps, which\nmeans the flawed actions should be punished during training. Even accounting\nfor the ``squeezing effect\", the probability mass of good actions should\nincrease and the actor should gradually get rid of harmful actions. This shows\nthat flawed actions should be punished during training. We further identify\ngradient coupling between similar samples as a key issue in agentic RL, the\ninput prompt is extremely similar and the output action space is limited,\ntherefore during training, gradients from well-performing samples can\ninadvertently strengthen suboptimal or incorrect actions due to similar input\nobservation and output actions. We show that with gradient coupling, some\nflawed actions might be enhanced. To address this, we propose training the\nactor to classify good or bad actions to separate the embedding of good/bad\nactions and alleviate the gradient interference, extensive experiments shows\nits effectiveness.", "AI": {"tldr": "The paper challenges the common belief that outcome-based rewards cause reward miscalibration and reinforce flawed actions. Instead, it identifies gradient coupling between similar samples as the real issue in agentic RL, where similar input prompts and limited action space cause gradients from good samples to strengthen bad actions. A classification-based actor training method is proposed to separate embeddings and reduce gradient interference.", "motivation": "To address the misconception about outcome-based rewards in autonomous agent training and identify the actual problem of gradient coupling that causes flawed actions to be reinforced during training.", "method": "Propose training the actor to classify good or bad actions to separate their embeddings and alleviate gradient interference between similar samples with similar input observations and output actions.", "result": "Extensive experiments demonstrate the effectiveness of the proposed method in addressing gradient coupling issues and preventing the reinforcement of flawed actions.", "conclusion": "Gradient coupling, not outcome-based rewards, is the key issue in agentic RL that causes flawed actions to be strengthened, and the proposed classification-based training effectively mitigates this problem."}}
{"id": "2509.23232", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23232", "abs": "https://arxiv.org/abs/2509.23232", "authors": ["Bingshuai Liu", "Ante Wang", "Zijun Min", "Liang Yao", "Haibo Zhang", "Yang Liu", "Anxiang Zeng", "Jinsong Su"], "title": "SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on reinforcement learning with\nverifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning.\nHowever, the training process remains bottlenecked by the computationally\nexpensive rollout stage. Existing acceleration methods-such as parallelization,\nobjective- and data-driven modifications, and replay buffers-either incur\ndiminishing returns, introduce bias, or overlook redundancy across iterations.\nWe identify that rollouts from consecutive training epochs frequently share a\nlarge portion of overlapping segments, wasting computation. To address this, we\npropose SPEC-RL, a novel framework that integrates SPECulative decoding with\nthe RL rollout process. SPEC-RL reuses prior trajectory segments as speculative\nprefixes and extends them via a draft-and-verify mechanism, avoiding redundant\ngeneration while ensuring policy consistency. Experiments on diverse math\nreasoning and generalization benchmarks, including GSM8K, MATH-500,\nOlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout\ntime by 2-3x without compromising policy quality. As a purely rollout-stage\nenhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g.,\nPPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large\nreasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL", "AI": {"tldr": "SPEC-RL is a novel framework that integrates speculative decoding with RL rollouts to reduce computational costs by reusing overlapping trajectory segments from consecutive training epochs, achieving 2-3x speedup without quality loss.", "motivation": "Current RLVR training is bottlenecked by expensive rollout computation, and existing acceleration methods have limitations like diminishing returns, bias introduction, or ignoring redundancy across iterations.", "method": "SPEC-RL reuses prior trajectory segments as speculative prefixes and extends them via a draft-and-verify mechanism, avoiding redundant generation while ensuring policy consistency.", "result": "Experiments on math reasoning benchmarks (GSM8K, MATH-500, OlympiadBench, MMLU-STEM) show 2-3x reduction in rollout time without compromising policy quality.", "conclusion": "SPEC-RL provides a general and practical enhancement for RLVR training that integrates seamlessly with mainstream algorithms like PPO, GRPO, DAPO, enabling scalable training for large reasoning models."}}
{"id": "2509.23574", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23574", "abs": "https://arxiv.org/abs/2509.23574", "authors": ["Jianzhi Yan", "Le Liu", "Youcheng Pan", "Shiwei Chen", "Yang Xiang", "Buzhou Tang"], "title": "Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales", "comment": "18 pages, 10 figures", "summary": "Chain-of-thought (CoT) distillation aims to enhance small language models'\n(SLMs) reasoning by transferring multi-step reasoning capability from the\nlarger teacher models. However, existing work underestimates rationale quality,\nfocusing primarily on data quantity, which may transfer noisy or incorrect\ninformation to the student model. To address the above issues, we proposed\n\\textbf{M}odel-\\textbf{O}riented \\textbf{R}ationale \\textbf{S}election\n\\textbf{D}istillation (MoRSD), which can discern and select high quality\nrationales for distillation to improve performance further. We further propose\na Rationale Difficulty (RD) metric to measure the ability of the student model\nto generate the correct answer under a given rationale. Compared to the\nbaseline, we achieved 4.6$\\%$ average improvement on seven datasets over three\ntasks, using fewer rationales by controlling their accuracy, diversity, and\ndifficulty. Our results reveal that a small portion of the high quality\nrationales can enhance the reasoning ability of student models than the entire\ndataset. Our method promises to be a possible solution for efficient CoT\ndistillation. Our code will be released in https://github.com/Leon221220/MoRSD.", "AI": {"tldr": "MoRSD improves CoT distillation by selecting high-quality rationales using a Rationale Difficulty metric, achieving 4.6% average improvement on 7 datasets with fewer but better rationales.", "motivation": "Existing CoT distillation methods focus on data quantity but underestimate rationale quality, potentially transferring noisy or incorrect information to student models.", "method": "Proposed Model-Oriented Rationale Selection Distillation (MoRSD) with Rationale Difficulty metric to select high-quality rationales based on accuracy, diversity, and difficulty.", "result": "Achieved 4.6% average improvement on seven datasets across three tasks using fewer rationales, showing that small portions of high-quality rationales outperform entire datasets.", "conclusion": "MoRSD provides an efficient solution for CoT distillation by focusing on rationale quality rather than quantity, enhancing student models' reasoning ability with carefully selected rationales."}}
{"id": "2509.23240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23240", "abs": "https://arxiv.org/abs/2509.23240", "authors": ["Shayan Alahyari"], "title": "More Data or Better Algorithms: Latent Diffusion Augmentation for Deep Imbalanced Regression", "comment": null, "summary": "In many real-world regression tasks, the data distribution is heavily skewed,\nand models learn predominantly from abundant majority samples while failing to\npredict minority labels accurately. While imbalanced classification has been\nextensively studied, imbalanced regression remains relatively unexplored. Deep\nimbalanced regression (DIR) represents cases where the input data are\nhigh-dimensional and unstructured. Although several data-level approaches for\ntabular imbalanced regression exist, deep imbalanced regression currently lacks\ndedicated data-level solutions suitable for high-dimensional data and relies\nprimarily on algorithmic modifications. To fill this gap, we propose\nLatentDiff, a novel framework that uses conditional diffusion models with\npriority-based generation to synthesize high-quality features in the latent\nrepresentation space. LatentDiff is computationally efficient and applicable\nacross diverse data modalities, including images, text, and other\nhigh-dimensional inputs. Experiments on three DIR benchmarks demonstrate\nsubstantial improvements in minority regions while maintaining overall\naccuracy.", "AI": {"tldr": "LatentDiff is a novel framework using conditional diffusion models with priority-based generation to synthesize high-quality features in latent space for deep imbalanced regression, addressing the lack of dedicated data-level solutions for high-dimensional imbalanced regression.", "motivation": "Deep imbalanced regression lacks dedicated data-level solutions suitable for high-dimensional data, while existing approaches mainly rely on algorithmic modifications. The data distribution in real-world regression tasks is often heavily skewed, causing models to fail at predicting minority labels accurately.", "method": "Proposes LatentDiff framework that uses conditional diffusion models with priority-based generation to synthesize high-quality features in the latent representation space. It is computationally efficient and applicable across diverse data modalities including images, text, and other high-dimensional inputs.", "result": "Experiments on three deep imbalanced regression benchmarks demonstrate substantial improvements in minority regions while maintaining overall accuracy.", "conclusion": "LatentDiff effectively addresses the gap in data-level solutions for deep imbalanced regression, providing a computationally efficient framework that works across multiple data modalities and significantly improves performance on minority samples."}}
{"id": "2509.23579", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23579", "abs": "https://arxiv.org/abs/2509.23579", "authors": ["Kevin Frank", "Anmol Gulati", "Elias Lumer", "Sindy Campagna", "Vamse Kumar Subbiah"], "title": "Jackal: A Real-World Execution-Based Benchmark Evaluating Large Language Models on Text-to-JQL Tasks", "comment": "17 pages", "summary": "Enterprise teams rely on the Jira Query Language (JQL) to retrieve and filter\nissues from Jira. Yet, to our knowledge, there is no open, real-world,\nexecution-based benchmark for mapping natural language queries to JQL. We\nintroduce Jackal, a novel, large-scale text-to-JQL benchmark comprising 100,000\nnatural language (NL) requests paired with validated JQL queries and\nexecution-based results on a live Jira instance with over 200,000 issues. To\nreflect real-world usage, each JQL query is associated with four types of user\nrequests: (i) Long NL, (ii) Short NL, (iii) Semantically Similar, and (iv)\nSemantically Exact. We release Jackal, a corpus of 100,000 text-to-JQL pairs,\ntogether with an execution-based scoring toolkit, and a static snapshot of the\nevaluated Jira instance for reproducibility. We report text-to-JQL results on\n23 Large Language Models (LLMs) spanning parameter sizes, open and closed\nsource models, across execution accuracy, exact match, and canonical exact\nmatch. In this paper, we report results on Jackal-5K, a 5,000-pair subset of\nJackal. On Jackal-5K, the best overall model (Gemini 2.5 Pro) achieves only\n60.3% execution accuracy averaged equally across four user request types.\nPerformance varies significantly across user request types: (i) Long NL\n(86.0%), (ii) Short NL (35.7%), (iii) Semantically Similar (22.7%), and (iv)\nSemantically Exact (99.3%). By benchmarking LLMs on their ability to produce\ncorrect and executable JQL queries, Jackal exposes the limitations of current\nstate-of-the-art LLMs and sets a new, execution-based challenge for future\nresearch in Jira enterprise data.", "AI": {"tldr": "Jackal is a large-scale text-to-JQL benchmark with 100,000 natural language to JQL query pairs, evaluated on a live Jira instance with 200,000+ issues. It reveals current LLMs struggle with JQL generation, with best model achieving only 60.3% execution accuracy.", "motivation": "There is no open, real-world, execution-based benchmark for mapping natural language queries to Jira Query Language (JQL), despite enterprise teams' heavy reliance on JQL for issue retrieval.", "method": "Created Jackal benchmark with 100,000 NL-to-JQL pairs across four user request types (Long NL, Short NL, Semantically Similar, Semantically Exact), evaluated on live Jira instance with execution-based scoring.", "result": "On Jackal-5K subset, best model (Gemini 2.5 Pro) achieved 60.3% execution accuracy overall, with significant variation across request types: Long NL (86.0%), Short NL (35.7%), Semantically Similar (22.7%), Semantically Exact (99.3%).", "conclusion": "Jackal exposes limitations of current LLMs in JQL generation and sets a new execution-based challenge for future enterprise data research."}}
{"id": "2509.23912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23912", "abs": "https://arxiv.org/abs/2509.23912", "authors": ["Ouns El Harzli", "Bernardo Cuenca Grau", "Artur d'Avila Garcez", "Ian Horrocks", "Tarek R. Besold"], "title": "From Neural Networks to Logical Theories: The Correspondence between Fibring Modal Logics and Fibring Neural Networks", "comment": null, "summary": "Fibring of modal logics is a well-established formalism for combining\ncountable families of modal logics into a single fibred language with common\nsemantics, characterized by fibred models. Inspired by this formalism, fibring\nof neural networks was introduced as a neurosymbolic framework for combining\nlearning and reasoning in neural networks. Fibring of neural networks uses the\n(pre-)activations of a trained network to evaluate a fibring function computing\nthe weights of another network whose outputs are injected back into the\noriginal network. However, the exact correspondence between fibring of neural\nnetworks and fibring of modal logics was never formally established. In this\npaper, we close this gap by formalizing the idea of fibred models\n\\emph{compatible} with fibred neural networks. Using this correspondence, we\nthen derive non-uniform logical expressiveness results for Graph Neural\nNetworks (GNNs), Graph Attention Networks (GATs) and Transformer encoders.\nLonger-term, the goal of this paper is to open the way for the use of fibring\nas a formalism for interpreting the logical theories learnt by neural networks\nwith the tools of computational logic.", "AI": {"tldr": "This paper establishes a formal correspondence between fibring of neural networks and fibring of modal logics, using fibred models compatible with neural networks to derive logical expressiveness results for GNNs, GATs and Transformers.", "motivation": "To close the gap between fibring of neural networks (a neurosymbolic framework) and fibring of modal logics, and to enable logical interpretation of neural network theories using computational logic tools.", "method": "Formalizes fibred models compatible with fibred neural networks, then uses this correspondence to derive non-uniform logical expressiveness results for various neural architectures.", "result": "Establishes formal correspondence between neural network fibring and modal logic fibring, and derives logical expressiveness bounds for GNNs, GATs and Transformer encoders.", "conclusion": "Opens the way for using fibring as a formalism to interpret logical theories learned by neural networks using computational logic tools."}}
{"id": "2509.23246", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23246", "abs": "https://arxiv.org/abs/2509.23246", "authors": ["Manjiang Yu", "Priyanka Singh", "Xue Li", "Yang Cao"], "title": "Adaptive Token-Weighted Differential Privacy for LLMs: Not All Tokens Require Equal Protection", "comment": "18 pages", "summary": "Large language models (LLMs) frequently memorize sensitive or personal\ninformation, raising significant privacy concerns. Existing variants of\ndifferential privacy stochastic gradient descent (DPSGD) inject uniform noise\ninto every gradient step, significantly extending training time and reducing\nmodel accuracy. We propose that concentrating noise primarily on gradients\nassociated with sensitive tokens can substantially decrease DP training time,\nstrengthen the protection of sensitive information, and simultaneously preserve\nthe model's performance on non-sensitive data. We operationalize this insight\nthrough Adaptive Token-Weighted Differential Privacy (ATDP), a modification of\nvanilla DP-SGD that adaptively assigns different gradient weights to sensitive\nand non-sensitive tokens. By employing a larger noise scale at the early stage\nof training, ATDP rapidly disrupts memorization of sensitive content. As a\nresult, ATDP only requires a few additional epochs of lightweight\npost-processing following standard fine-tuning, injecting targeted noise\nprimarily on parameters corresponding to sensitive tokens, thus minimally\naffecting the model's general capabilities. ATDP can be seamlessly integrated\ninto any existing DP-based fine-tuning pipeline or directly applied to\nnon-private models as a fast privacy-enhancing measure. Additionally, combined\nwith an initial redacted fine-tuning phase, ATDP forms a streamlined DP\npipeline that achieves comparable canary protection to state-of-the-art DP-SGD\nmethods, significantly reduces the computational overhead of DP fine-tuning,\nshortening training time by approximately 90 percent, while achieving\ncomparable or superior privacy protection and minimal accuracy degradation.", "AI": {"tldr": "ATDP is a novel differential privacy method that focuses noise injection on sensitive tokens, reducing training time by 90% while maintaining privacy protection and model performance.", "motivation": "Existing DP-SGD methods inject uniform noise across all gradients, causing significant training time extension and accuracy reduction. There's a need for more efficient privacy protection that specifically targets sensitive information.", "method": "Adaptive Token-Weighted Differential Privacy (ATDP) - modifies vanilla DP-SGD by adaptively assigning different gradient weights to sensitive and non-sensitive tokens, using larger noise scale in early training stages and minimal post-processing.", "result": "ATDP achieves comparable canary protection to state-of-the-art DP-SGD methods, reduces computational overhead by approximately 90%, maintains comparable or superior privacy protection, and causes minimal accuracy degradation.", "conclusion": "ATDP provides an efficient privacy-enhancing solution that can be seamlessly integrated into existing pipelines, offering targeted protection for sensitive information while preserving model capabilities."}}
{"id": "2509.23580", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23580", "abs": "https://arxiv.org/abs/2509.23580", "authors": ["JinXin Li", "Gang Tu", "JunJie Hu"], "title": "LLM Hallucination Detection: HSAD", "comment": "in Chinese language", "summary": "Although Large Language Models have demonstrated powerful capabilities in a\nwide range of tasks such as language understanding and code generation, the\nfrequent occurrence of hallucinations during the generation process has become\na significant impediment to their deployment in critical application scenarios.\nCurrent mainstream hallucination detection methods rely on factual consistency\nverification or static hidden layer features. The former is constrained by the\nscope of knowledge coverage, while the latter struggles to capture reasoning\nbiases during the inference process. To address these issues, and inspired by\nsignal analysis methods in cognitive neuroscience, this paper proposes a\nhallucination detection method based on the frequency-domain analysis of hidden\nlayer temporal signals, named HSAD (\\textbf{H}idden \\textbf{S}ignal\n\\textbf{A}nalysis-based \\textbf{D}etection). First, by treating the LLM's\nreasoning process as a cognitive journey that unfolds over time, we propose\nmodeling and simulating the human process of signal perception and\ndiscrimination in a deception-detection scenario through hidden layer temporal\nsignals. Next, The Fast Fourier Transform is applied to map these temporal\nsignals into the frequency domain to construct spectral features, which are\nused to capture anomalies that arise during the reasoning process; analysis\nexperiments on these spectral features have proven the effectiveness of this\napproach. Finally, a hallucination detection algorithm is designed based on\nthese spectral features to identify hallucinations in the generated content. By\neffectively combining the modeling of the reasoning process with\nfrequency-domain feature extraction, the HSAD method overcomes the limitations\nof existing approaches in terms of knowledge coverage and the detection of\nreasoning biases, demonstrating higher detection accuracy and robustness.", "AI": {"tldr": "HSAD is a novel hallucination detection method that analyzes frequency-domain features of hidden layer temporal signals in LLMs, overcoming limitations of existing approaches by capturing reasoning process anomalies.", "motivation": "Current hallucination detection methods are limited by knowledge coverage constraints (factual consistency verification) or inability to capture reasoning biases (static hidden layer features). The paper aims to address these limitations by modeling LLM reasoning as a temporal cognitive process.", "method": "1) Model LLM reasoning as temporal cognitive journey using hidden layer signals; 2) Apply Fast Fourier Transform to map temporal signals to frequency domain; 3) Construct spectral features to capture reasoning anomalies; 4) Design detection algorithm based on spectral features.", "result": "The method demonstrates effectiveness in capturing reasoning process anomalies through spectral feature analysis, showing higher detection accuracy and robustness compared to existing approaches.", "conclusion": "HSAD effectively combines reasoning process modeling with frequency-domain feature extraction, overcoming knowledge coverage limitations and reasoning bias detection challenges in hallucination detection for LLMs."}}
{"id": "2509.23962", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23962", "abs": "https://arxiv.org/abs/2509.23962", "authors": ["Guanxu Chen", "Yafu Li", "Yuxian Jiang", "Chen Qian", "Qihan Ren", "Jingyi Yang", "Yu Cheng", "Dongrui Liu", "Jing Shao"], "title": "Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models", "comment": "18 pages, 13 figures, 4 tables", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) for large language\nmodels (LLMs) has achieved remarkable progress in enhancing LLMs' reasoning\ncapabilities on tasks with clear correctness criteria, such as mathematical\nreasoning tasks. Several training metrics, such as entropy or response length,\nhave been observed to correlate with different reasoning behaviors in\nreinforcement learning. Prior approaches incorporate such priors through reward\nor advantage shaping, which often relies on hand-crafted penalties and\npreferences (e.g., higher-is-better or lower-is-better). However, without\ncareful hyperparameter tuning, these directional priors can be overly biased\nand may lead to failure. To this end, we introduce Conditional advANtage\nestimatiON (CANON), amplifying the impact of the target metric without\npresuming its direction. Specifically, CANON regroups the sampled responses\ninto two groups based on the higher or lower value of a target metric, measures\nwhich metric trend contributes to better performance through inter-group\ncomparison, and identifies the better response within the same group. In\nsummary, CANON based on entropy consistently outperforms prior methods across\nthree LLMs on both math reasoning and high-complexity logic tasks. When applied\nto response length, CANON further improves token efficiency, yielding a more\nfavorable Pareto frontier in the performance-cost trade-off.", "AI": {"tldr": "CANON is a new advantage estimation method for RL that amplifies target metrics without assuming directionality, outperforming prior methods on math reasoning and logic tasks while improving token efficiency.", "motivation": "Prior RL methods for LLMs use hand-crafted directional priors that can be overly biased and require careful hyperparameter tuning, potentially leading to failure.", "method": "CANON regroups sampled responses into two groups based on metric values, measures which trend contributes to better performance through inter-group comparison, and identifies better responses within groups.", "result": "CANON with entropy consistently outperforms prior methods across three LLMs on math reasoning and high-complexity logic tasks, and improves token efficiency when applied to response length.", "conclusion": "CANON provides an effective way to leverage training metrics without directional assumptions, achieving better performance and more favorable performance-cost trade-offs."}}
{"id": "2509.23249", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.23249", "abs": "https://arxiv.org/abs/2509.23249", "authors": ["Vladimir Fanaskov", "Vladislav Trifonov", "Alexander Rudikov", "Ekaterina Muravleva", "Ivan Oseledets"], "title": "Deep Learning for Subspace Regression", "comment": null, "summary": "It is often possible to perform reduced order modelling by specifying linear\nsubspace which accurately captures the dynamics of the system. This approach\nbecomes especially appealing when linear subspace explicitly depends on\nparameters of the problem. A practical way to apply such a scheme is to compute\nsubspaces for a selected set of parameters in the computationally demanding\noffline stage and in the online stage approximate subspace for unknown\nparameters by interpolation. For realistic problems the space of parameters is\nhigh dimensional, which renders classical interpolation strategies infeasible\nor unreliable. We propose to relax the interpolation problem to regression,\nintroduce several loss functions suitable for subspace data, and use a neural\nnetwork as an approximation to high-dimensional target function. To further\nsimplify a learning problem we introduce redundancy: in place of predicting\nsubspace of a given dimension we predict larger subspace. We show theoretically\nthat this strategy decreases the complexity of the mapping for elliptic\neigenproblems with constant coefficients and makes the mapping smoother for\ngeneral smooth function on the Grassmann manifold. Empirical results also show\nthat accuracy significantly improves when larger-than-needed subspaces are\npredicted. With the set of numerical illustrations we demonstrate that subspace\nregression can be useful for a range of tasks including parametric\neigenproblems, deflation techniques, relaxation methods, optimal control and\nsolution of parametric partial differential equations.", "AI": {"tldr": "The paper proposes using neural networks for subspace regression in parametric model order reduction, addressing high-dimensional parameter spaces by predicting larger-than-needed subspaces to improve accuracy and smoothness.", "motivation": "Classical interpolation methods become infeasible for high-dimensional parameter spaces in parametric model order reduction, necessitating a more robust approach.", "method": "Relax interpolation to regression, use neural networks to approximate high-dimensional target functions, and introduce redundancy by predicting larger subspaces than needed.", "result": "Theoretical analysis shows reduced complexity for elliptic eigenproblems and smoother mappings on Grassmann manifolds; empirical results confirm significant accuracy improvements.", "conclusion": "Subspace regression with neural networks and redundancy is effective for various tasks including parametric eigenproblems, PDEs, and optimal control."}}
{"id": "2509.23595", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23595", "abs": "https://arxiv.org/abs/2509.23595", "authors": ["Taiqiang Wu", "Runming Yang", "Tao Liu", "Jiahao Wang", "Zenan Xu", "Ngai Wong"], "title": "Timber: Training-free Instruct Model Refining with Base via Effective Rank", "comment": "7 figures, 8 tables, Working in progress", "summary": "Post-training, which elicits a pretrained Base model into the corresponding\nInstruct model, is widely considered to be superficial. In this work, we first\nreinforce this hypothesis by providing novel quantitative evidence from the\nweight level that the effective rank (eRank) remains negligibly changed.\nHowever, this superficiality also suffers a critical trade-off, improving the\nexploitation capabilities at the cost of limiting its exploration. To tackle\nthis issue, we propose Timber, a simple yet effective training-free method that\nenhances the exploration capability of the Instruct model while preserving its\nexploitation. The key insight is to partially revert Instruct towards the\npaired Base model by subtle yet targeted refinement of the weight deltas.\nExtensive experiments on Llama and Qwen series demonstrate that Timber\nconsistently improves vanilla Instruct models, particularly on Pass@k\nperformance. Our findings offer new insights into the post-training stage at\nthe weight level and practical strategies to refine the Instruct model without\ntraining.", "AI": {"tldr": "Timber is a training-free method that enhances exploration capability of Instruct models by partially reverting them towards Base models through targeted weight delta refinement, improving performance particularly on Pass@k metrics.", "motivation": "Post-training is considered superficial with negligible weight changes, but suffers from a trade-off between exploitation and exploration capabilities - improving exploitation at the cost of limiting exploration.", "method": "Timber partially reverts Instruct models towards their paired Base models by subtle yet targeted refinement of the weight deltas, without requiring additional training.", "result": "Extensive experiments on Llama and Qwen series show Timber consistently improves vanilla Instruct models, particularly enhancing Pass@k performance.", "conclusion": "The findings provide new insights into post-training at the weight level and practical strategies to refine Instruct models without training, addressing the exploration-exploitation trade-off."}}
{"id": "2509.23981", "categories": ["cs.AI", "68", "I.2"], "pdf": "https://arxiv.org/pdf/2509.23981", "abs": "https://arxiv.org/abs/2509.23981", "authors": ["Jos\u00e9 de la Torre-L\u00f3pez", "Aurora Ram\u00edrez", "Jos\u00e9 Ra\u00fal Romero"], "title": "Automatic selection of primary studies in systematic reviews with evolutionary rule-based classification", "comment": "32 pages, 5 figures, 4 tables", "summary": "Searching, filtering and analysing scientific literature are time-consuming\ntasks when performing a systematic literature review. With the rise of\nartificial intelligence, some steps in the review process are progressively\nbeing automated. In particular, machine learning for automatic paper selection\ncan greatly reduce the effort required to identify relevant literature in\nscientific databases. We propose an evolutionary machine learning approach,\ncalled \\ourmodel, to automatically determine whether a paper retrieved from a\nliterature search process is relevant. \\ourmodel builds an interpretable\nrule-based classifier using grammar-guided genetic programming. The use of a\ngrammar to define the syntax and the structure of the rules allows \\ourmodel to\neasily combine the usual textual information with other bibliometric data not\nconsidered by state-of-the-art methods. Our experiments demonstrate that it is\npossible to generate accurate classifiers without impairing interpretability\nand using configurable information sources not supported so far.", "AI": {"tldr": "Proposes an evolutionary machine learning approach called \\ourmodel for automatic paper selection in systematic literature reviews, using grammar-guided genetic programming to create interpretable rule-based classifiers that combine textual and bibliometric data.", "motivation": "Systematic literature reviews are time-consuming, and current methods don't effectively combine textual information with bibliometric data. There's a need for automated approaches that maintain interpretability while improving accuracy.", "method": "Uses grammar-guided genetic programming to build interpretable rule-based classifiers. The grammar defines syntax and structure, allowing combination of textual information with bibliometric data not considered by state-of-the-art methods.", "result": "Experiments demonstrate that accurate classifiers can be generated without impairing interpretability, using configurable information sources not previously supported.", "conclusion": "The proposed evolutionary machine learning approach successfully automates paper selection while maintaining interpretability and leveraging additional bibliometric data sources."}}
{"id": "2509.23252", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23252", "abs": "https://arxiv.org/abs/2509.23252", "authors": ["Raviteja Anantha", "Soheil Hor", "Teodor Nicola Antoniu", "Layne C. Price"], "title": "NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning", "comment": "preprint version", "summary": "We present NanoFlux, a novel adversarial framework for generating targeted\ntraining data to improve LLM reasoning, where adversarially-generated datasets\ncontaining fewer than 200 examples outperform conventional fine-tuning\napproaches. The framework employs a competitive dynamic between models\nalternating as Attacker and Defender, supervised by a tool-augmented Judge,\nsynthesizing multi-step questions with explanatory annotations that target\nspecific reasoning capabilities. Fine-tuning a 4B-parameter model on\nNanoFlux-generated data yields performance gains across diverse domains\ncompared to full-benchmark fine-tuning: +5.9% on mathematical reasoning\n(GSMHard), +3.6% on scientific reasoning (GenomeBench), and +16.6% on medical\nreasoning (MultiMedQA), while reducing computational requirements by 3-14x.\nAblation studies reveal a non-monotonic relationship between dataset\ncharacteristics and model performance, uncovering domain-specific optimal\npoints for question complexity and reasoning quality. NanoFlux automates\ntraining data generation through embedding-based novelty filtering,\ntool-augmented evaluation, and multi-hop reasoning, suggesting that future\nmodel improvements may lie in the intelligent synthesis of small, precisely\ntargeted training datasets.", "AI": {"tldr": "NanoFlux is an adversarial framework that generates targeted training data for LLM reasoning improvement, where small datasets (<200 examples) outperform conventional fine-tuning approaches across multiple domains with significant computational efficiency gains.", "motivation": "To improve LLM reasoning capabilities through targeted data generation rather than large-scale conventional fine-tuning, addressing the computational inefficiency and lack of precision in current approaches.", "method": "Uses competitive adversarial dynamics between Attacker and Defender models supervised by a tool-augmented Judge, synthesizing multi-step questions with explanatory annotations. Includes embedding-based novelty filtering, tool-augmented evaluation, and multi-hop reasoning.", "result": "Fine-tuning a 4B-parameter model on NanoFlux data achieved: +5.9% on GSMHard (math), +3.6% on GenomeBench (science), +16.6% on MultiMedQA (medical), with 3-14x computational reduction. Ablation studies revealed non-monotonic relationship between dataset characteristics and performance.", "conclusion": "Future model improvements may come from intelligent synthesis of small, precisely targeted training datasets rather than large-scale data collection, demonstrating the effectiveness of adversarial data generation for specific reasoning capabilities."}}
{"id": "2509.23633", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23633", "abs": "https://arxiv.org/abs/2509.23633", "authors": ["Haoyu Zheng", "Zhuonan Wang", "Yuqian Yuan", "Tianwei Lin", "Wenqiao Zhang", "Zheqi Lv", "Juncheng Li", "Siliang Tang", "Yueting Zhuang", "Hongyang He"], "title": "Fast Thinking for Large Language Models", "comment": null, "summary": "Reasoning-oriented Large Language Models (LLMs) often rely on generating\nexplicit tokens step by step, and their effectiveness typically hinges on\nlarge-scale supervised fine-tuning or reinforcement learning. While\nChain-of-Thought (CoT) techniques substantially enhance performance on complex\nreasoning tasks, they remain inefficient, requiring long reasoning traces that\nincrease latency and token usage. In this work, we introduce Latent Codebooks\nfor Fast Thinking, a framework that uses concise CoT sketches only during\ntraining to learn a codebook of discrete strategy priors. At inference, the\nmodel conditions on a handful of continuous thinking vectors distilled from the\ncodebook in a single pass, enabling strategy-level guidance without producing\nexplicit reasoning tokens. To complement this design, we propose GainRouter, a\nlightweight routing mechanism that adaptively switches between fast codebook\nguided inference and slow explicit reasoning, thereby suppressing overthinking\nand reducing unnecessary token generation. Experiments across multiple\nreasoning benchmarks show that our approach achieves competitive or superior\naccuracy while substantially lowering inference cost, offering a practical path\ntoward efficient and controllable reasoning in large language models.", "AI": {"tldr": "Latent Codebooks for Fast Thinking framework uses concise CoT sketches in training to learn discrete strategy priors, enabling fast inference with continuous thinking vectors and adaptive routing between fast and slow reasoning modes.", "motivation": "Current reasoning-oriented LLMs rely on explicit step-by-step token generation, which is inefficient due to long reasoning traces that increase latency and token usage, despite CoT techniques improving performance on complex tasks.", "method": "Proposes a framework that learns a codebook of discrete strategy priors using concise CoT sketches during training. At inference, uses continuous thinking vectors from the codebook for strategy-level guidance without explicit tokens. Includes GainRouter mechanism for adaptive switching between fast codebook inference and slow explicit reasoning.", "result": "Experiments across multiple reasoning benchmarks show competitive or superior accuracy while substantially lowering inference cost, effectively reducing unnecessary token generation and suppressing overthinking.", "conclusion": "The approach offers a practical path toward efficient and controllable reasoning in large language models by enabling fast strategy-level guidance without explicit reasoning tokens and adaptive routing between reasoning modes."}}
{"id": "2509.23986", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23986", "abs": "https://arxiv.org/abs/2509.23986", "authors": ["Alistair Turcan", "Kexin Huang", "Lei Li", "Martin Jinye Zhang"], "title": "TusoAI: Agentic Optimization for Scientific Methods", "comment": null, "summary": "Scientific discovery is often slowed by the manual development of\ncomputational tools needed to analyze complex experimental data. Building such\ntools is costly and time-consuming because scientists must iteratively review\nliterature, test modeling and scientific assumptions against empirical data,\nand implement these insights into efficient software. Large language models\n(LLMs) have demonstrated strong capabilities in synthesizing literature,\nreasoning with empirical data, and generating domain-specific code, offering\nnew opportunities to accelerate computational method development. Existing\nLLM-based systems either focus on performing scientific analyses using existing\ncomputational methods or on developing computational methods or models for\ngeneral machine learning without effectively integrating the often unstructured\nknowledge specific to scientific domains. Here, we introduce TusoAI , an\nagentic AI system that takes a scientific task description with an evaluation\nfunction and autonomously develops and optimizes computational methods for the\napplication. TusoAI integrates domain knowledge into a knowledge tree\nrepresentation and performs iterative, domain-specific optimization and model\ndiagnosis, improving performance over a pool of candidate solutions. We\nconducted comprehensive benchmark evaluations demonstrating that TusoAI\noutperforms state-of-the-art expert methods, MLE agents, and scientific AI\nagents across diverse tasks, such as single-cell RNA-seq data denoising and\nsatellite-based earth monitoring. Applying TusoAI to two key open problems in\ngenetics improved existing computational methods and uncovered novel biology,\nincluding 9 new associations between autoimmune diseases and T cell subtypes\nand 7 previously unreported links between disease variants linked to their\ntarget genes. Our code is publicly available at\nhttps://github.com/Alistair-Turcan/TusoAI.", "AI": {"tldr": "TusoAI is an agentic AI system that autonomously develops and optimizes computational methods for scientific tasks by integrating domain knowledge and performing iterative optimization, outperforming existing methods and uncovering novel biological insights.", "motivation": "Scientific discovery is slowed by manual development of computational tools. LLMs offer capabilities in synthesizing literature, reasoning with data, and generating code, but existing systems don't effectively integrate domain-specific knowledge for computational method development.", "method": "TusoAI takes scientific task descriptions with evaluation functions, integrates domain knowledge into a knowledge tree representation, and performs iterative domain-specific optimization and model diagnosis to improve candidate solutions.", "result": "TusoAI outperformed state-of-the-art expert methods, MLE agents, and scientific AI agents across diverse tasks including single-cell RNA-seq data denoising and satellite-based earth monitoring. In genetics applications, it improved existing methods and uncovered 9 new autoimmune disease-T cell associations and 7 novel disease variant-gene links.", "conclusion": "TusoAI successfully accelerates computational method development by autonomously integrating domain knowledge and optimizing solutions, demonstrating practical applications that advance scientific discovery beyond existing methods."}}
{"id": "2509.23254", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.23254", "abs": "https://arxiv.org/abs/2509.23254", "authors": ["Zhang-Yu You", "Jiahao Ma", "Hongzong Li", "Ye-Fan Hu", "Jian-Dong Huang"], "title": "ABConformer: Physics-inspired Sliding Attention for Antibody-Antigen Interface Prediction", "comment": null, "summary": "Accurate prediction of antibody-antigen (Ab-Ag) interfaces is critical for\nvaccine design, immunodiagnostics, and therapeutic antibody development.\nHowever, achieving reliable predictions from sequences alone remains a\nchallenge. In this paper, we present ABCONFORMER, a model based on the\nConformer backbone that captures both local and global features of a\nbiosequence. To accurately capture Ab-Ag interactions, we introduced the\nphysics-inspired sliding attention, enabling residue-level contact recovery\nwithout relying on three-dimensional structural data. ABConformer can\naccurately predict paratopes and epitopes given the antibody and antigen\nsequence, and predict pan-epitopes on the antigen without antibody information.\nIn comparison experiments, ABCONFORMER achieves state-of-the-art performance on\na recent SARS-CoV-2 Ab-Ag dataset, and surpasses widely used sequence-based\nmethods for antibody-agnostic epitope prediction. Ablation studies further\nquantify the contribution of each component, demonstrating that, compared to\nconventional cross-attention, sliding attention significantly enhances the\nprecision of epitope prediction. To facilitate reproducibility, we will release\nthe code under an open-source license upon acceptance.", "AI": {"tldr": "ABCONFORMER is a Conformer-based model that uses physics-inspired sliding attention to predict antibody-antigen interfaces from sequences alone, achieving state-of-the-art performance on SARS-CoV-2 data.", "motivation": "Accurate prediction of antibody-antigen interfaces is critical for vaccine design and therapeutic development, but reliable prediction from sequences alone remains challenging.", "method": "Uses Conformer backbone to capture local and global sequence features, introduces physics-inspired sliding attention for residue-level contact recovery without 3D structural data.", "result": "Achieves state-of-the-art performance on SARS-CoV-2 Ab-Ag dataset, surpasses sequence-based methods for antibody-agnostic epitope prediction, with sliding attention significantly enhancing epitope prediction precision.", "conclusion": "ABCONFORMER enables accurate paratope and epitope prediction from sequences, with sliding attention proving superior to conventional cross-attention for epitope prediction."}}
{"id": "2509.23653", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23653", "abs": "https://arxiv.org/abs/2509.23653", "authors": ["Zemin Huang", "Yuhang Wang", "Zhiyang Chen", "Guo-Jun Qi"], "title": "Don't Settle Too Early: Self-Reflective Remasking for Diffusion Language Models", "comment": "24 pages,11 figures", "summary": "Mask-based Diffusion Language Models (DLMs) struggle to revise incorrect\ntokens: once a token is generated, it typically remains fixed. The key\nchallenge is to identify potential errors in the inputs. In this paper, we\npropose \\emph{\\underline{Rem}asking-\\underline{e}nabled \\underline{Di}ffusion\nLanguage Model (RemeDi}, a mask-based DLM that introduces \\emph{remasking} as\nanother fundamental mechanism, enabling more flexible text refinement in\ndiffusion-based text generation. To achieve this, RemeDi jointly predicts token\ndistributions and per-token confidence scores at each step. The confidence\nscores determine which tokens to be unmasked after the current step, allowing\nthe model to identify tokens with low quality and remask them. These remasked\ntokens can be resampled with richer context in subsequent steps. We design a\nremask-aware pipeline to train this ability, including supervised fine-tuning\nwhich teaches the model to detect and remask incorrect tokens in addition to\npredict mask tokens, and reinforcement learning which optimizes full generation\ntrajectories toward higher rewards. Experiments show that RemeDi achieves the\nstate-of-the-art results among open-source DLMs on multiple datasets.", "AI": {"tldr": "RemeDi introduces remasking mechanism in diffusion language models to identify and revise incorrect tokens by predicting confidence scores, enabling more flexible text refinement.", "motivation": "Mask-based Diffusion Language Models struggle to revise incorrect tokens once generated, as tokens typically remain fixed after generation.", "method": "RemeDi jointly predicts token distributions and per-token confidence scores, uses confidence scores to determine which tokens to remask, and employs remask-aware training pipeline with supervised fine-tuning and reinforcement learning.", "result": "RemeDi achieves state-of-the-art results among open-source Diffusion Language Models on multiple datasets.", "conclusion": "Remasking enables more flexible text refinement in diffusion-based text generation, allowing identification and correction of low-quality tokens."}}
{"id": "2509.23988", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.23988", "abs": "https://arxiv.org/abs/2509.23988", "authors": ["Zirui Tang", "Weizheng Wang", "Zihang Zhou", "Yang Jiao", "Bangrui Xu", "Boyu Niu", "Xuanhe Zhou", "Guoliang Li", "Yeye He", "Wei Zhou", "Yitong Song", "Cheng Tan", "Bin Wang", "Conghui He", "Xiaoyang Wang", "Fan Wu"], "title": "LLM/Agent-as-Data-Analyst: A Survey", "comment": "35 page, 11 figures", "summary": "Large language model (LLM) and agent techniques for data analysis (a.k.a\nLLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both\nacademica and industry. In comparison with traditional rule or small-model\nbased approaches, (agentic) LLMs enable complex data understanding, natural\nlanguage interfaces, semantic analysis functions, and autonomous pipeline\norchestration. The technical evolution further distills five key design goals\nfor intelligent data analysis agents, namely semantic-aware design,\nmodality-hybrid integration, autonomous pipelines, tool-augmented workflows,\nand support for open-world tasks. From a modality perspective, we review\nLLM-based techniques for (i) structured data (e.g., table question answering\nfor relational data and NL2GQL for graph data), (ii) semi-structured data\n(e.g., markup languages understanding and semi-structured table modeling),\n(iii) unstructured data (e.g., chart understanding, document understanding,\nprogramming languages vulnerable detection), and (iv) heterogeneous data (e.g.,\ndata retrieval and modality alignment for data lakes). Finally, we outline the\nremaining challenges and propose several insights and practical directions for\nadvancing LLM/Agent-powered data analysis.", "AI": {"tldr": "LLM/Agent-as-Data-Analyst techniques enable complex data understanding, natural language interfaces, and autonomous pipeline orchestration for data analysis across structured, semi-structured, unstructured, and heterogeneous data modalities.", "motivation": "Traditional rule-based or small-model approaches have limitations in handling complex data analysis tasks, while LLM/agent techniques offer superior capabilities for data understanding, semantic analysis, and autonomous workflow management.", "method": "The paper reviews LLM-based techniques across different data modalities: structured data (table QA, NL2GQL), semi-structured data (markup languages, table modeling), unstructured data (chart/document understanding, code vulnerability detection), and heterogeneous data (data retrieval, modality alignment).", "result": "The technical evolution identifies five key design goals for intelligent data analysis agents: semantic-aware design, modality-hybrid integration, autonomous pipelines, tool-augmented workflows, and support for open-world tasks.", "conclusion": "The paper outlines remaining challenges and proposes insights and practical directions for advancing LLM/Agent-powered data analysis, highlighting the need for continued development in this rapidly evolving field."}}
{"id": "2509.23265", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23265", "abs": "https://arxiv.org/abs/2509.23265", "authors": ["Jiajun He", "Paul Jeha", "Peter Potaptchik", "Leo Zhang", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Yuanqi Du", "Saifuddin Syed", "Francisco Vargas"], "title": "CREPE: Controlling Diffusion with Replica Exchange", "comment": "29 pages, 14 figures, 3 tables", "summary": "Inference-time control of diffusion models aims to steer model outputs to\nsatisfy new constraints without retraining. Previous approaches have mostly\nrelied on heuristic guidance or have been coupled with Sequential Monte Carlo\n(SMC) for bias correction. In this paper, we propose a flexible alternative\nbased on replica exchange, an algorithm designed initially for sampling\nproblems. We refer to this method as the CREPE (Controlling with REPlica\nExchange). Unlike SMC, CREPE: (1) generates particles sequentially, (2)\nmaintains high diversity in the generated samples after a burn-in period, and\n(3) enables online refinement or early termination. We demonstrate its\nversatility across various tasks, including temperature annealing,\nreward-tilting, model composition and classifier-free guidance debiasing, with\ncompetitive performance compared to prior SMC methods.", "AI": {"tldr": "CREPE is a flexible inference-time control method for diffusion models using replica exchange algorithm, offering sequential particle generation, high sample diversity, and online refinement capabilities.", "motivation": "Existing inference-time control methods for diffusion models rely on heuristic guidance or Sequential Monte Carlo (SMC) with bias correction, which may have limitations in flexibility and sample diversity.", "method": "Proposes CREPE (Controlling with REPlica Exchange) based on replica exchange algorithm, which generates particles sequentially, maintains high diversity after burn-in, and allows online refinement or early termination.", "result": "Demonstrates versatility across various tasks including temperature annealing, reward-tilting, model composition and classifier-free guidance debiasing, with competitive performance compared to prior SMC methods.", "conclusion": "CREPE provides a flexible alternative to SMC-based approaches for inference-time control of diffusion models, offering improved sample diversity and online refinement capabilities."}}
{"id": "2509.23657", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23657", "abs": "https://arxiv.org/abs/2509.23657", "authors": ["Shulin Huang", "Yiran Ding", "Junshu Pan", "Yue Zhang"], "title": "Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs", "comment": null, "summary": "Enhancing the complex reasoning capabilities of Large Language Models (LLMs)\nattracts widespread attention. While reinforcement learning (RL) has shown\nsuperior performance for improving complex reasoning, its impact on\ncross-lingual generalization compared to Supervised Fine-Tuning (SFT) remains\nunexplored. We present the first systematic investigation into cross-lingual\nreasoning generalization of RL and SFT. Using Qwen2.5-3B-Base as our foundation\nmodel, we conduct experiments on diverse multilingual reasoning benchmarks,\nincluding math reasoning, commonsense reasoning, and scientific reasoning. Our\ninvestigation yields two significant findings: (1) Tuning with RL not only\nachieves higher accuracy but also demonstrates substantially stronger\ncross-lingual generalization capabilities compared to SFT. (2) RL training on\nnon-English data yields better overall performance and generalization than\ntraining on English data, which is not observed with SFT. Furthermore, through\ncomprehensive mechanistic analyses, we explore the underlying factors of RL's\nsuperiority and generalization across languages. Our results provide compelling\nevidence that RL enables the model with more robust reasoning strategies,\noffering crucial guidance for more equitable and effective multilingual\nreasoning.", "AI": {"tldr": "RL training shows superior cross-lingual reasoning generalization compared to SFT, with better performance when trained on non-English data.", "motivation": "To investigate how reinforcement learning (RL) and supervised fine-tuning (SFT) affect cross-lingual generalization in complex reasoning tasks, which remains unexplored despite RL's known benefits for reasoning.", "method": "Systematic investigation using Qwen2.5-3B-Base model on diverse multilingual reasoning benchmarks (math, commonsense, scientific reasoning), comparing RL and SFT approaches across different language training data.", "result": "RL achieves higher accuracy and substantially stronger cross-lingual generalization than SFT. RL training on non-English data yields better overall performance and generalization than English data training, unlike SFT.", "conclusion": "RL enables more robust reasoning strategies and provides crucial guidance for equitable multilingual reasoning systems, demonstrating clear advantages over SFT for cross-lingual generalization."}}
{"id": "2509.23996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23996", "abs": "https://arxiv.org/abs/2509.23996", "authors": ["Yuchen Wang", "Pei-Duo Yu", "Chee Wei Tan"], "title": "Future-Proofing Programmers: Optimal Knowledge Tracing for AI-Assisted Personalized Education", "comment": "The paper is accepted to IEEE Signal Processing Magazine, Special\n  Issue on Artificial Intelligence for Education", "summary": "Learning to learn is becoming a science, driven by the convergence of\nknowledge tracing, signal processing, and generative AI to model student\nlearning states and optimize education. We propose CoTutor, an AI-driven model\nthat enhances Bayesian Knowledge Tracing with signal processing techniques to\nimprove student progress modeling and deliver adaptive feedback and strategies.\nDeployed as an AI copilot, CoTutor combines generative AI with adaptive\nlearning technology. In university trials, it has demonstrated measurable\nimprovements in learning outcomes while outperforming conventional educational\ntools. Our results highlight its potential for AI-driven personalization,\nscalability, and future opportunities for advancing privacy and ethical\nconsiderations in educational technology. Inspired by Richard Hamming's vision\nof computer-aided 'learning to learn,' CoTutor applies convex optimization and\nsignal processing to automate and scale up learning analytics, while reserving\npedagogical judgment for humans, ensuring AI facilitates the process of\nknowledge tracing while enabling learners to uncover new insights.", "AI": {"tldr": "CoTutor is an AI-driven educational model that enhances Bayesian Knowledge Tracing with signal processing and generative AI to improve student progress modeling and deliver adaptive feedback, showing measurable improvements in learning outcomes.", "motivation": "To advance learning science by combining knowledge tracing, signal processing, and generative AI to model student learning states and optimize education, inspired by Richard Hamming's vision of computer-aided 'learning to learn'.", "method": "Proposes CoTutor model that enhances Bayesian Knowledge Tracing with signal processing techniques, uses convex optimization and signal processing for learning analytics automation, and combines generative AI with adaptive learning technology as an AI copilot.", "result": "In university trials, CoTutor demonstrated measurable improvements in learning outcomes and outperformed conventional educational tools, showing potential for AI-driven personalization and scalability.", "conclusion": "CoTutor highlights potential for AI-driven personalization and scalability in education while reserving pedagogical judgment for humans, with future opportunities for advancing privacy and ethical considerations in educational technology."}}
{"id": "2509.23268", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.23268", "abs": "https://arxiv.org/abs/2509.23268", "authors": ["Lisa Pilgram", "Kai Yang", "Ana-Alicia Beltran-Bless", "Gregory R. Pond", "Lisa Vandermeer", "John Hilton", "Marie-France Savard", "Andr\u00e9anne Leblanc", "Lois Sheperd", "Bingshu E. Chen", "John M. S. Bartlett", "Karen J. Taylor", "Jane Bayani", "Sarah L. Barker", "Melanie Spears", "Cornelis J. H. van der Velde", "Elma Meershoek-Klein Kranenbarg", "Luc Dirix", "Elizabeth Mallon", "Annette Hasenburg", "Christos Markopoulos", "Lamin Juwara", "Fida K. Dankar", "Mark Clemons", "Khaled El Emam"], "title": "Transfer Learning and Machine Learning for Training Five Year Survival Prognostic Models in Early Breast Cancer", "comment": null, "summary": "Prognostic information is essential for decision-making in breast cancer\nmanagement. Recently trials have predominantly focused on genomic\nprognostication tools, even though clinicopathological prognostication is less\ncostly and more widely accessible. Machine learning (ML), transfer learning and\nensemble integration offer opportunities to build robust prognostication\nframeworks. We evaluate this potential to improve survival prognostication in\nbreast cancer by comparing de-novo ML, transfer learning from a pre-trained\nprognostic tool and ensemble integration. Data from the MA.27 trial was used\nfor model training, with external validation on the TEAM trial and a SEER\ncohort. Transfer learning was applied by fine-tuning the pre-trained prognostic\ntool PREDICT v3, de-novo ML included Random Survival Forests and Extreme\nGradient Boosting, and ensemble integration was realized through a weighted sum\nof model predictions. Transfer learning, de-novo RSF, and ensemble integration\nimproved calibration in MA.27 over the pre-trained model (ICI reduced from\n0.042 in PREDICT v3 to <=0.007) while discrimination remained comparable (AUC\nincreased from 0.738 in PREDICT v3 to 0.744-0.799). Invalid PREDICT v3\npredictions were observed in 23.8-25.8% of MA.27 individuals due to missing\ninformation. In contrast, ML models and ensemble integration could predict\nsurvival regardless of missing information. Across all models, patient age,\nnodal status, pathological grading and tumor size had the highest SHAP values,\nindicating their importance for survival prognostication. External validation\nin SEER, but not in TEAM, confirmed the benefits of transfer learning, RSF and\nensemble integration. This study demonstrates that transfer learning, de-novo\nRSF, and ensemble integration can improve prognostication in situations where\nrelevant information for PREDICT v3 is lacking or where a dataset shift is\nlikely.", "AI": {"tldr": "This study compares machine learning approaches for breast cancer survival prognostication, showing that transfer learning, de-novo Random Survival Forests, and ensemble integration outperform the pre-trained PREDICT v3 tool, especially when data is missing or dataset shifts occur.", "motivation": "To improve breast cancer survival prognostication using more accessible clinicopathological data rather than costly genomic tools, leveraging machine learning, transfer learning, and ensemble methods to overcome limitations of existing prognostic tools like PREDICT v3.", "method": "Used MA.27 trial data for training with external validation on TEAM trial and SEER cohort. Compared three approaches: transfer learning by fine-tuning PREDICT v3, de-novo ML (Random Survival Forests and Extreme Gradient Boosting), and ensemble integration through weighted sum of model predictions.", "result": "Transfer learning, de-novo RSF, and ensemble integration improved calibration over PREDICT v3 (ICI reduced from 0.042 to \u22640.007) while maintaining comparable discrimination (AUC increased from 0.738 to 0.744-0.799). ML models could predict survival even with missing data, unlike PREDICT v3 which failed in 23.8-25.8% of cases. Key features were age, nodal status, pathological grading, and tumor size.", "conclusion": "Transfer learning, de-novo RSF, and ensemble integration can enhance prognostication when PREDICT v3 data is incomplete or dataset shifts occur, providing more robust and accessible breast cancer survival prediction tools."}}
{"id": "2509.23659", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.23659", "abs": "https://arxiv.org/abs/2509.23659", "authors": ["Amit Agarwal", "Hansa Meghwani", "Hitesh Laxmichand Patel", "Tao Sheng", "Sujith Ravi", "Dan Roth"], "title": "Aligning LLMs for Multilingual Consistency in Enterprise Applications", "comment": "Accepted in EMNLP 2025", "summary": "Large language models (LLMs) remain unreliable for global enterprise\napplications due to substantial performance gaps between high-resource and\nmid/low-resource languages, driven by English-centric pretraining and internal\nreasoning biases. This inconsistency undermines customer experience and\noperational reliability in multilingual settings such as customer support,\ncontent moderation, and information retrieval. Even with advanced\nRetrieval-Augmented Generation (RAG) systems, we observe up to an 29% accuracy\ndrop in non-English languages compared to English.\n  We propose a practical, batch-wise alignment strategy for fine-tuning LLMs,\nleveraging semantically equivalent multilingual data in each training batch to\ndirectly align model outputs across languages. This approach improves\nnon-English accuracy by up to 23.9\\% without compromising English performance,\nmodel reasoning, or retrieval quality. Our method is simple to implement,\nscalable, and integrates seamlessly with existing LLM training \\& deployment\npipelines, enabling more robust and equitable multilingual AI solutions in\nindustry.", "AI": {"tldr": "A batch-wise alignment strategy for fine-tuning LLMs using multilingual data to reduce performance gaps between English and non-English languages, improving non-English accuracy by up to 23.9% without compromising English performance.", "motivation": "LLMs are unreliable for global enterprise applications due to substantial performance gaps between high-resource and mid/low-resource languages, driven by English-centric pretraining and internal reasoning biases, which undermines customer experience and operational reliability in multilingual settings.", "method": "A practical, batch-wise alignment strategy for fine-tuning LLMs that leverages semantically equivalent multilingual data in each training batch to directly align model outputs across languages.", "result": "The approach improves non-English accuracy by up to 23.9% without compromising English performance, model reasoning, or retrieval quality.", "conclusion": "The method is simple to implement, scalable, and integrates seamlessly with existing LLM training & deployment pipelines, enabling more robust and equitable multilingual AI solutions in industry."}}
{"id": "2509.24086", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24086", "abs": "https://arxiv.org/abs/2509.24086", "authors": ["Miguel Angel Alvarado Gonzalez", "Michelle Bruno Hernandez", "Miguel Angel Pe\u00f1aloza Perez", "Bruno Lopez Orozco", "Jesus Tadeo Cruz Soto", "Sandra Malagon"], "title": "Do Repetitions Matter? Strengthening Reliability in LLM Evaluations", "comment": null, "summary": "LLM leaderboards often rely on single stochastic runs, but how many\nrepetitions are required for reliable conclusions remains unclear. We\nre-evaluate eight state-of-the-art models on the AI4Math Benchmark with three\nindependent runs per setting. Using mixed-effects logistic regression,\ndomain-level marginal means, rank-instability analysis, and run-to-run\nreliability, we assessed the value of additional repetitions. Our findings\nshows that Single-run leaderboards are brittle: 10/12 slices (83\\%) invert at\nleast one pairwise rank relative to the three-run majority, despite a zero\nsign-flip rate for pairwise significance and moderate overall interclass\ncorrelation. Averaging runs yields modest SE shrinkage ($\\sim$5\\% from one to\nthree) but large ranking gains; two runs remove $\\sim$83\\% of single-run\ninversions. We provide cost-aware guidance for practitioners: treat evaluation\nas an experiment, report uncertainty, and use $\\geq 2$ repetitions under\nstochastic decoding. These practices improve robustness while remaining\nfeasible for small teams and help align model comparisons with real-world\nreliability.", "AI": {"tldr": "Single-run LLM evaluations are unreliable; using 2-3 repetitions significantly improves ranking stability while remaining feasible for small teams.", "motivation": "Current LLM leaderboards rely on single stochastic runs, but the reliability of such evaluations is unclear. The paper aims to determine how many repetitions are needed for robust model comparisons.", "method": "Re-evaluated 8 state-of-the-art models on AI4Math Benchmark with 3 independent runs per setting. Used mixed-effects logistic regression, domain-level marginal means, rank-instability analysis, and run-to-run reliability assessment.", "result": "Single-run leaderboards are brittle: 83% of slices invert at least one pairwise rank. Averaging runs yields modest standard error reduction (~5%) but large ranking improvements; two runs remove ~83% of single-run inversions.", "conclusion": "Practitioners should treat evaluation as an experiment, report uncertainty, and use \u22652 repetitions under stochastic decoding to improve robustness while remaining feasible for small teams."}}
{"id": "2509.23280", "categories": ["cs.LG", "cs.AI", "math.OC", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2509.23280", "abs": "https://arxiv.org/abs/2509.23280", "authors": ["Yilie Huang"], "title": "Continuous-Time Reinforcement Learning for Asset-Liability Management", "comment": "Accepted at the 6th ACM International Conference on AI in Finance\n  (ICAIF 2025), 8 pages, 2 figures", "summary": "This paper proposes a novel approach for Asset-Liability Management (ALM) by\nemploying continuous-time Reinforcement Learning (RL) with a linear-quadratic\n(LQ) formulation that incorporates both interim and terminal objectives. We\ndevelop a model-free, policy gradient-based soft actor-critic algorithm\ntailored to ALM for dynamically synchronizing assets and liabilities. To ensure\nan effective balance between exploration and exploitation with minimal tuning,\nwe introduce adaptive exploration for the actor and scheduled exploration for\nthe critic. Our empirical study evaluates this approach against two enhanced\ntraditional financial strategies, a model-based continuous-time RL method, and\nthree state-of-the-art RL algorithms. Evaluated across 200 randomized market\nscenarios, our method achieves higher average rewards than all alternative\nstrategies, with rapid initial gains and sustained superior performance. The\noutperformance stems not from complex neural networks or improved parameter\nestimation, but from directly learning the optimal ALM strategy without\nlearning the environment.", "AI": {"tldr": "This paper proposes a continuous-time reinforcement learning approach for Asset-Liability Management using linear-quadratic formulation with policy gradient-based soft actor-critic algorithm and adaptive exploration techniques.", "motivation": "To develop a more effective Asset-Liability Management (ALM) strategy that can dynamically synchronize assets and liabilities while balancing interim and terminal objectives, overcoming limitations of traditional financial strategies.", "method": "Model-free policy gradient-based soft actor-critic algorithm with linear-quadratic formulation, featuring adaptive exploration for actor and scheduled exploration for critic to balance exploration-exploitation trade-off.", "result": "The method achieves higher average rewards than all alternative strategies across 200 randomized market scenarios, showing rapid initial gains and sustained superior performance.", "conclusion": "The outperformance comes from directly learning optimal ALM strategy without learning the environment, rather than from complex neural networks or improved parameter estimation."}}
{"id": "2509.23686", "categories": ["cs.CL", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23686", "abs": "https://arxiv.org/abs/2509.23686", "authors": ["Yifeng He", "Luning Yang", "Christopher Castro Gaw Gonzalo", "Hao Chen"], "title": "TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in System F", "comment": "NeurIPS '25, package released at:\n  https://github.com/SecurityLab-UCD/TF-Bench", "summary": "Large Language Models (LLMs) are increasingly integrated into the software\nengineering ecosystem. Their test-time compute (TTC) reasoning capabilities\nshow significant potential for understanding program logic and semantics beyond\nmere token recognition. However, current benchmarks for code reasoning lack a\nformal, program-centric deductive framework to ensure sound evaluation, and are\nincapable of assessing whether models genuinely reason about program semantics\nor merely exploit superficial associations between natural language and code\ntokens. To bridge this gap, we introduce TF-Bench, a benchmark designed to\nevaluate LLM reasoning based on type inference in System F, a task we refer to\nas program semantics reasoning. By employing verified transformations to remove\nsemantically irrelevant natural language, we construct TF-Bench_pure, a purely\nsemantics-driven variant of TF-Bench. Our analysis reveals substantial\nlimitations in state-of-the-art LLMs, with the best-performing LLM\n(Claude-3.7-sonnet) achieving only 55.85% accuracy on TF-Bench_pure.\nAdditionally, we propose two novel metrics to assess robustness and the\neffectiveness of test-time reasoning, underscoring critical limitations in\ncurrent LLM capabilities and highlighting essential directions for future\nresearch.", "AI": {"tldr": "TF-Bench is a new benchmark for evaluating LLM reasoning on program semantics using System F type inference, revealing major limitations in current models with Claude-3.7-sonnet achieving only 55.85% accuracy on the pure semantics variant.", "motivation": "Current benchmarks lack formal deductive frameworks for sound evaluation and cannot distinguish genuine program semantics reasoning from superficial token associations.", "method": "Uses type inference in System F as reasoning task, creates TF-Bench_pure variant by removing semantically irrelevant natural language through verified transformations.", "result": "Best-performing LLM (Claude-3.7-sonnet) achieves only 55.85% accuracy on TF-Bench_pure, showing substantial limitations in program semantics reasoning.", "conclusion": "Proposes novel metrics for robustness and test-time reasoning effectiveness, highlighting critical LLM limitations and essential future research directions."}}
{"id": "2509.24107", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24107", "abs": "https://arxiv.org/abs/2509.24107", "authors": ["Shreyas Singh", "Kunal Singh", "Pradeep Moturi"], "title": "Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs", "comment": null, "summary": "Tool-integrated reasoning has emerged as a key focus for enabling agentic\napplications. Among these, DeepResearch Agents have gained significant\nattention for their strong performance on complex, open-ended\ninformation-seeking tasks. We introduce Fathom-DeepResearch, an agentic system\ncomposed of two specialized models. The first is Fathom-Search-4B, a DeepSearch\nmodel trained from Qwen3-4B and optimized for evidence-based investigation\nthrough live web search and targeted webpage querying. Its training combines\nthree advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent\nself-play that enforces strict web-search dependence and heterogeneous source\ngrounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes\nmulti-turn Reinforcement Learning with Verifiable Rewards through curriculum\npruning, reward-aware advantage scaling, and per-prompt replay buffers; and\n(iii) a steerable step-level reward that classifies each tool call by cognitive\nbehavior and marginal utility, enabling explicit control over search trajectory\nbreadth, depth, and horizon. These improvements enable reliable extension of\ntool-calling beyond 20 calls when warranted. The second is\nFathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn\nDeepSearch traces into structured, citation-dense DeepResearch Reports for\ncomprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES,\nWebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves\nstate-of-the-art performance in the open-weights category while demonstrating\nstrong generalization to diverse reasoning tasks including HLE, AIME-25,\nGPQA-Diamond, and MedQA.", "AI": {"tldr": "Fathom-DeepResearch is a two-model agentic system for complex information-seeking tasks, featuring Fathom-Search-4B for evidence-based web investigation and Fathom-Synthesizer-4B for structured report generation, achieving state-of-the-art performance on multiple benchmarks.", "motivation": "To address the need for reliable tool-integrated reasoning in agentic applications, particularly for complex open-ended information-seeking tasks where DeepResearch Agents have shown strong performance.", "method": "Developed two specialized models: (1) Fathom-Search-4B trained with DUETQA dataset, RAPO reinforcement learning, and steerable step-level rewards for web search; (2) Fathom-Synthesizer-4B for converting search traces into structured reports. Used three training advances: multi-agent self-play dataset, stabilized RL with curriculum pruning, and cognitive behavior classification.", "result": "Achieved state-of-the-art performance in open-weights category on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, with strong generalization to HLE, AIME-25, GPQA-Diamond, and MedQA tasks. Enables reliable tool-calling beyond 20 calls.", "conclusion": "The Fathom-DeepResearch system demonstrates effective tool-integrated reasoning through specialized models and advanced training techniques, providing a robust solution for complex information-seeking tasks with strong generalization capabilities."}}
{"id": "2509.23307", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23307", "abs": "https://arxiv.org/abs/2509.23307", "authors": ["Gabriel Jarry", "Ramon Dalmau", "Xavier Olive", "Philippe Very"], "title": "A Neural ODE Approach to Aircraft Flight Dynamics Modelling", "comment": null, "summary": "Accurate aircraft trajectory prediction is critical for air traffic\nmanagement, airline operations, and environmental assessment. This paper\nintroduces NODE-FDM, a Neural Ordinary Differential Equations-based Flight\nDynamics Model trained on Quick Access Recorder (QAR) data. By combining\nanalytical kinematic relations with data-driven components, NODE-FDM achieves a\nmore accurate reproduction of recorded trajectories than state-of-the-art\nmodels such as a BADA-based trajectory generation methodology (BADA4\nperformance model combined with trajectory control routines), particularly in\nthe descent phase of the flight. The analysis demonstrates marked improvements\nacross altitude, speed, and mass dynamics. Despite current limitations,\nincluding limited physical constraints and the limited availability of QAR\ndata, the results demonstrate the potential of physics-informed neural ordinary\ndifferential equations as a high-fidelity, data-driven approach to aircraft\nperformance modelling. Future work will extend the framework to incorporate a\nfull modelling of the lateral dynamics of the aircraft.", "AI": {"tldr": "NODE-FDM is a Neural ODE-based Flight Dynamics Model that combines analytical kinematics with data-driven components, achieving better trajectory prediction accuracy than BADA4 models, especially in descent phases.", "motivation": "Accurate aircraft trajectory prediction is critical for air traffic management, airline operations, and environmental assessment, but existing models have limitations in accuracy.", "method": "Uses Neural Ordinary Differential Equations trained on Quick Access Recorder (QAR) data, combining analytical kinematic relations with data-driven components.", "result": "Achieves more accurate reproduction of recorded trajectories than BADA4 models, with marked improvements in altitude, speed, and mass dynamics, particularly during descent phase.", "conclusion": "Demonstrates potential of physics-informed neural ODEs as high-fidelity, data-driven approach to aircraft performance modeling, though limited by physical constraints and QAR data availability. Future work will incorporate full lateral dynamics modeling."}}
{"id": "2509.23698", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23698", "abs": "https://arxiv.org/abs/2509.23698", "authors": ["Zhe Hu", "Yixiao Ren", "Guanzhong Liu", "Jing Li", "Yu Yin"], "title": "VIVA+: Human-Centered Situational Decision-Making", "comment": "EMNLP 2025 Findings", "summary": "Multimodal Large Language Models (MLLMs) show promising results for embodied\nagents in operating meaningfully in complex, human-centered environments. Yet,\nevaluating their capacity for nuanced, human-like reasoning and decision-making\nremains challenging. In this work, we introduce VIVA+, a cognitively grounded\nbenchmark for evaluating the reasoning and decision-making of MLLMs in\nhuman-centered situations. VIVA+ consists of 1,317 real-world situations paired\nwith 6,373 multiple-choice questions, targeting three core abilities for\ndecision-making: (1) Foundational Situation Comprehension, (2) Context-Driven\nAction Justification, and (3) Reflective Reasoning. Together, these dimensions\nprovide a systematic framework for assessing a model's ability to perceive,\nreason, and act in socially meaningful ways. We evaluate the latest commercial\nand open-source models on VIVA+, where we reveal distinct performance patterns\nand highlight significant challenges. We further explore targeted training and\nmulti-step reasoning strategies, which yield consistent performance\nimprovements. Finally, our in-depth analysis highlights current model\nlimitations and provides actionable insights for advancing MLLMs toward more\nrobust, context-aware, and socially adept decision-making in real-world\nsettings.", "AI": {"tldr": "VIVA+ is a cognitively grounded benchmark with 1,317 real-world situations and 6,373 multiple-choice questions to evaluate MLLMs' reasoning and decision-making across three core abilities: situation comprehension, action justification, and reflective reasoning.", "motivation": "Existing evaluation methods struggle to assess MLLMs' capacity for nuanced, human-like reasoning and decision-making in complex, human-centered environments, necessitating a more systematic framework.", "method": "Developed VIVA+ benchmark targeting three core decision-making abilities: (1) Foundational Situation Comprehension, (2) Context-Driven Action Justification, and (3) Reflective Reasoning. Evaluated latest commercial and open-source MLLMs, and explored targeted training and multi-step reasoning strategies.", "result": "Revealed distinct performance patterns and significant challenges among evaluated models. Targeted training and multi-step reasoning strategies yielded consistent performance improvements across models.", "conclusion": "Current MLLMs have limitations in robust, context-aware, and socially adept decision-making. The analysis provides actionable insights for advancing MLLMs toward better real-world decision-making capabilities."}}
{"id": "2509.24127", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.24127", "abs": "https://arxiv.org/abs/2509.24127", "authors": ["Nooshin Bahador"], "title": "Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework", "comment": "20 pages, 11 figures", "summary": "This article presents a modular, component-based architecture for developing\nand evaluating AI agents that bridge the gap between natural language\ninterfaces and complex enterprise data warehouses. The system directly\naddresses core challenges in data accessibility by enabling non-technical users\nto interact with complex data warehouses through a conversational interface,\ntranslating ambiguous user intent into precise, executable database queries to\novercome semantic gaps. A cornerstone of the design is its commitment to\ntransparent decision-making, achieved through a multi-layered reasoning\nframework that explains the \"why\" behind every decision, allowing for full\ninterpretability by tracing conclusions through specific, activated business\nrules and data points. The architecture integrates a robust quality assurance\nmechanism via an automated evaluation framework that serves multiple functions:\nit enables performance benchmarking by objectively measuring agent performance\nagainst golden standards, and it ensures system reliability by automating the\ndetection of performance regressions during updates. The agent's analytical\ndepth is enhanced by a statistical context module, which quantifies deviations\nfrom normative behavior, ensuring all conclusions are supported by quantitative\nevidence including concrete data, percentages, and statistical comparisons. We\ndemonstrate the efficacy of this integrated agent-development-with-evaluation\nframework through a case study on an insurance claims processing system. The\nagent, built on a modular architecture, leverages the BigQuery ecosystem to\nperform secure data retrieval, apply domain-specific business rules, and\ngenerate human-auditable justifications. The results confirm that this approach\ncreates a robust, evaluable, and trustworthy system for deploying LLM-powered\nagents in data-sensitive, high-stakes domains.", "AI": {"tldr": "A modular architecture for AI agents that enables natural language interaction with enterprise data warehouses, featuring transparent decision-making, automated evaluation, and statistical context for trustworthy deployment in data-sensitive domains.", "motivation": "To bridge the gap between natural language interfaces and complex enterprise data warehouses, enabling non-technical users to access data through conversational interfaces while ensuring transparency and reliability.", "method": "Component-based architecture with multi-layered reasoning framework, automated evaluation system, statistical context module, and integration with BigQuery ecosystem for secure data retrieval and business rule application.", "result": "Successfully demonstrated through an insurance claims processing case study, creating a robust, evaluable, and trustworthy system for LLM-powered agents in high-stakes domains.", "conclusion": "The integrated agent-development-with-evaluation framework provides an effective solution for deploying trustworthy AI agents in data-sensitive enterprise environments, with proven efficacy in real-world applications."}}
{"id": "2509.23313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23313", "abs": "https://arxiv.org/abs/2509.23313", "authors": ["Xvyuan Liu", "Xiangfei Qiu", "Hanyin Cheng", "Xingjian Wu", "Chenjuan Guo", "Bin Yang", "Jilin Hu"], "title": "ASTGI: Adaptive Spatio-Temporal Graph Interactions for Irregular Multivariate Time Series Forecasting", "comment": null, "summary": "Irregular multivariate time series (IMTS) are prevalent in critical domains\nlike healthcare and finance, where accurate forecasting is vital for proactive\ndecision-making. However, the asynchronous sampling and irregular intervals\ninherent to IMTS pose two core challenges for existing methods: (1) how to\naccurately represent the raw information of irregular time series without\nintroducing data distortion, and (2) how to effectively capture the complex\ndynamic dependencies between observation points. To address these challenges,\nwe propose the Adaptive Spatio-Temporal Graph Interaction (ASTGI) framework.\nSpecifically, the framework first employs a Spatio-Temporal Point\nRepresentation module to encode each discrete observation as a point within a\nlearnable spatio-temporal embedding space. Second, a Neighborhood-Adaptive\nGraph Construction module adaptively builds a causal graph for each point in\nthe embedding space via nearest neighbor search. Subsequently, a\nSpatio-Temporal Dynamic Propagation module iteratively updates information on\nthese adaptive causal graphs by generating messages and computing interaction\nweights based on the relative spatio-temporal positions between points.\nFinally, a Query Point-based Prediction module generates the final forecast by\naggregating neighborhood information for a new query point and performing\nregression. Extensive experiments on multiple benchmark datasets demonstrate\nthat ASTGI outperforms various state-of-the-art methods.", "AI": {"tldr": "ASTGI framework for irregular multivariate time series forecasting using adaptive spatio-temporal graph interactions to handle asynchronous sampling and irregular intervals.", "motivation": "Irregular multivariate time series (IMTS) in healthcare and finance have asynchronous sampling and irregular intervals, posing challenges for accurate representation and capturing complex dynamic dependencies.", "method": "ASTGI framework with four modules: Spatio-Temporal Point Representation, Neighborhood-Adaptive Graph Construction, Spatio-Temporal Dynamic Propagation, and Query Point-based Prediction.", "result": "Extensive experiments show ASTGI outperforms various state-of-the-art methods on multiple benchmark datasets.", "conclusion": "ASTGI effectively addresses challenges in IMTS forecasting through adaptive spatio-temporal graph interactions."}}
{"id": "2509.23714", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23714", "abs": "https://arxiv.org/abs/2509.23714", "authors": ["Zhiqiang Liu", "Yichi Zhang", "Mengshu Sun", "Lei Liang", "Wen Zhang"], "title": "Collaboration of Fusion and Independence: Hypercomplex-driven Robust Multi-Modal Knowledge Graph Completion", "comment": null, "summary": "Multi-modal knowledge graph completion (MMKGC) aims to discover missing facts\nin multi-modal knowledge graphs (MMKGs) by leveraging both structural\nrelationships and diverse modality information of entities. Existing MMKGC\nmethods follow two multi-modal paradigms: fusion-based and ensemble-based.\nFusion-based methods employ fixed fusion strategies, which inevitably leads to\nthe loss of modality-specific information and a lack of flexibility to adapt to\nvarying modality relevance across contexts. In contrast, ensemble-based methods\nretain modality independence through dedicated sub-models but struggle to\ncapture the nuanced, context-dependent semantic interplay between modalities.\nTo overcome these dual limitations, we propose a novel MMKGC method M-Hyper,\nwhich achieves the coexistence and collaboration of fused and independent\nmodality representations. Our method integrates the strengths of both\nparadigms, enabling effective cross-modal interactions while maintaining\nmodality-specific information. Inspired by ``quaternion'' algebra, we utilize\nits four orthogonal bases to represent multiple independent modalities and\nemploy the Hamilton product to efficiently model pair-wise interactions among\nthem. Specifically, we introduce a Fine-grained Entity Representation\nFactorization (FERF) module and a Robust Relation-aware Modality Fusion (R2MF)\nmodule to obtain robust representations for three independent modalities and\none fused modality. The resulting four modality representations are then mapped\nto the four orthogonal bases of a biquaternion (a hypercomplex extension of\nquaternion) for comprehensive modality interaction. Extensive experiments\nindicate its state-of-the-art performance, robustness, and computational\nefficiency.", "AI": {"tldr": "M-Hyper is a novel multi-modal knowledge graph completion method that uses quaternion algebra to enable coexistence of fused and independent modality representations, achieving state-of-the-art performance.", "motivation": "Existing MMKGC methods have limitations: fusion-based methods lose modality-specific information due to fixed fusion strategies, while ensemble-based methods fail to capture nuanced cross-modal interactions. There's a need for a method that integrates both paradigms.", "method": "Proposes M-Hyper using quaternion algebra with four orthogonal bases to represent multiple independent modalities. Uses Hamilton product for efficient modality interactions. Includes FERF module for fine-grained entity representation factorization and R2MF module for robust relation-aware modality fusion.", "result": "Extensive experiments show state-of-the-art performance, robustness, and computational efficiency compared to existing methods.", "conclusion": "M-Hyper successfully integrates fusion-based and ensemble-based paradigms, enabling effective cross-modal interactions while preserving modality-specific information through quaternion algebra representation."}}
{"id": "2509.24156", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24156", "abs": "https://arxiv.org/abs/2509.24156", "authors": ["Yuhui Wang", "Changjiang Li", "Guangke Chen", "Jiacheng Liang", "Ting Wang"], "title": "Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models", "comment": null, "summary": "Large reasoning models (LRMs) exhibit unprecedented capabilities in solving\ncomplex problems through Chain-of-Thought (CoT) reasoning. However, recent\nstudies reveal that their final answers often contradict their own reasoning\ntraces. We hypothesize that this inconsistency stems from two competing\nmechanisms for generating answers: CoT reasoning and memory retrieval. To test\nthis hypothesis, we conduct controlled experiments that challenge LRMs with\nmisleading cues during reasoning and/or corrupted answers during retrieval. Our\nresults across models and datasets confirm that both mechanisms operate\nsimultaneously, with their relative dominance influenced by multiple factors:\nproblem domains, model scales, and fine-tuning approaches (e.g., reinforcement\nlearning vs. distillation). The findings reveal a critical limitation in\ncurrent reasoning fine-tuning paradigms: models can exploit the retrieval\nmechanism as a shortcut, effectively \"hacking\" the reward signal and\nundermining genuine reasoning development. To address this challenge, we\nintroduce FARL, a novel fine-tuning framework that integrates memory unlearning\nwith reinforcement learning. By carefully suppressing retrieval shortcuts\nduring the fine-tuning process, FARL promotes reasoning-dominant behavior and\nenhances generalizable reasoning capabilities.", "AI": {"tldr": "Large reasoning models exhibit inconsistency between reasoning traces and final answers due to competing mechanisms of CoT reasoning and memory retrieval, which can be exploited as shortcuts during fine-tuning.", "motivation": "To understand why LRMs' final answers often contradict their own reasoning traces, and address the limitation in current reasoning fine-tuning paradigms where models exploit retrieval mechanisms as shortcuts.", "method": "Conducted controlled experiments with misleading cues during reasoning and corrupted answers during retrieval. Introduced FARL framework integrating memory unlearning with reinforcement learning to suppress retrieval shortcuts.", "result": "Confirmed both reasoning and retrieval mechanisms operate simultaneously, with dominance influenced by problem domains, model scales, and fine-tuning approaches. Models can exploit retrieval mechanisms to hack reward signals.", "conclusion": "FARL framework successfully promotes reasoning-dominant behavior and enhances generalizable reasoning capabilities by suppressing retrieval shortcuts during fine-tuning."}}
{"id": "2509.23314", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23314", "abs": "https://arxiv.org/abs/2509.23314", "authors": ["Francesco Pappone", "Donato Crisostomi", "Emanuele Rodol\u00e0"], "title": "Two-Scale Latent Dynamics for Recurrent-Depth Transformers", "comment": null, "summary": "Recurrent-depth transformers scale test-time compute by iterating latent\ncomputations before emitting tokens. We study the geometry of these iterates\nand argue for a simple, \\emph{two-scale} operational picture: (i) within a\nlooped block, updates act as \\emph{small-scale refinements}; (ii) across\nconsecutive blocks, states undergo a \\emph{larger-scale drift}. Across\ncheckpoints, our measurements show that loop steps become \\emph{smaller} and\nincreasingly \\emph{orthogonal} to one another, indicating better local modeling\nof fine structure rather than merely pushing in a single direction. These\ndynamics motivate an early-exit mechanism based on the model's second-order\ndifference in step-size, which we show is superior in terms of performance,\nstability and time-efficiency, when compared to the KL-divergence exit strategy\nof Geiping et al. and its naive first-order counterpart.", "AI": {"tldr": "Recurrent-depth transformers use iterative latent computations before token emission, exhibiting two-scale dynamics: small refinements within blocks and larger drift across blocks. An early-exit mechanism based on second-order step-size differences outperforms KL-divergence and first-order methods.", "motivation": "To understand the geometric properties of recurrent-depth transformers' iterative computations and leverage these dynamics for more efficient early-exit strategies.", "method": "Analyzed the geometry of iterates in recurrent-depth transformers, measuring loop step sizes and orthogonality across checkpoints. Proposed an early-exit mechanism using second-order differences in step-size.", "result": "Found that loop steps become smaller and more orthogonal over time, indicating better local modeling. The second-order early-exit method showed superior performance, stability and time-efficiency compared to KL-divergence and first-order approaches.", "conclusion": "Recurrent-depth transformers exhibit meaningful two-scale dynamics that can be effectively leveraged for early-exit mechanisms, with second-order step-size differences providing the most robust performance."}}
{"id": "2509.23715", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23715", "abs": "https://arxiv.org/abs/2509.23715", "authors": ["Eduard Barbu", "Adrian Marius Dumitran"], "title": "Do LLMs Understand Romanian Driving Laws? A Study on Multimodal and Fine-Tuned Question Answering", "comment": "Accepted@ CONSILR 2025 Bucharest Romania 9-10 October", "summary": "Ensuring that both new and experienced drivers master current traffic rules\nis critical to road safety. This paper evaluates Large Language Models (LLMs)\non Romanian driving-law QA with explanation generation. We release a\n1{,}208-question dataset (387 multimodal) and compare text-only and multimodal\nSOTA systems, then measure the impact of domain-specific fine-tuning for Llama\n3.1-8B-Instruct and RoLlama 3.1-8B-Instruct. SOTA models perform well, but\nfine-tuned 8B models are competitive. Textual descriptions of images outperform\ndirect visual input. Finally, an LLM-as-a-Judge assesses explanation quality,\nrevealing self-preference bias. The study informs explainable QA for\nless-resourced languages.", "AI": {"tldr": "This paper evaluates LLMs on Romanian driving-law QA with explanation generation, using a 1,208-question dataset (387 multimodal). It compares text-only and multimodal SOTA systems, measures domain-specific fine-tuning impact, and uses LLM-as-a-Judge for explanation quality assessment.", "motivation": "To ensure both new and experienced drivers master current traffic rules for road safety, and to inform explainable QA for less-resourced languages like Romanian.", "method": "Created a 1,208-question dataset (387 multimodal), compared text-only and multimodal SOTA systems, fine-tuned Llama 3.1-8B-Instruct and RoLlama 3.1-8B-Instruct, and used LLM-as-a-Judge to assess explanation quality.", "result": "SOTA models perform well but fine-tuned 8B models are competitive; textual descriptions of images outperform direct visual input; LLM-as-a-Judge reveals self-preference bias in explanation assessment.", "conclusion": "The study provides insights for explainable QA systems in less-resourced languages, showing the effectiveness of fine-tuned smaller models and the importance of addressing biases in automated evaluation methods."}}
{"id": "2509.24159", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24159", "abs": "https://arxiv.org/abs/2509.24159", "authors": ["Xiaoyang Cao", "Zelai Xu", "Mo Guang", "Kaiwen Long", "Michiel A. Bakker", "Yu Wang", "Chao Yu"], "title": "Robust Preference Optimization: Aligning Language Models with Noisy Preference Feedback", "comment": null, "summary": "Standard human preference-based alignment methods, such as Reinforcement\nLearning from Human Feedback (RLHF), are a cornerstone technology for aligning\nLarge Language Models (LLMs) with human values. However, these methods are all\nunderpinned by a critical, yet flawed assumption: human preferences are\nhomogeneous (representing a single, unified preference) and the collected data\nis noiseless (free from error). In reality, neither is true since human\npreference is pluralistic and annotators can make mistakes. This creates a\ndiscrepancy between the recorded data and the ground-truth preferences, which\ncan misguide the model and degrade its performance. To address this challenge,\nwe introduce Robust Preference Optimization (RPO). RPO employs an\nExpectation-Maximization (EM) algorithm to infer the posterior probability of\neach label's correctness, which is used to adaptively re-weigh each data point\nin the training loss to mitigate noise. We further generalize this approach by\nestablishing a theoretical link between arbitrary preference losses and their\ncorresponding probabilistic models. This generalization enables the systematic\ntransformation of existing alignment algorithms into their robust counterparts,\nelevating RPO from a specific algorithm to a meta-framework for robust\npreference alignment. Theoretically, we prove that under the condition of a\nperfectly calibrated model, RPO is guaranteed to converge to the true noise\nlevel of the dataset. Our experiments demonstrate RPO's effectiveness as a\nmeta-framework, consistently enhancing four state-of-the-art alignment\nalgorithms (DPO, IPO, SimPO, and CPO). When applied to Mistral and Llama 3\nmodels, the RPO-enhanced methods achieve substantial win rate gains on\nAlpacaEval 2 and Arena-Hard, with improvements of up to 7.0% and 5.4%,\nrespectively.", "AI": {"tldr": "RPO is a meta-framework that transforms existing preference alignment methods into robust versions by addressing noisy and heterogeneous human preference data through EM-based label correctness estimation and adaptive re-weighting.", "motivation": "Standard alignment methods assume homogeneous and noiseless human preferences, but real-world preferences are pluralistic and annotations contain errors, creating a discrepancy between recorded data and ground-truth preferences that degrades model performance.", "method": "Uses Expectation-Maximization algorithm to infer posterior probability of label correctness, adaptively re-weights data points in training loss, and establishes theoretical link between preference losses and probabilistic models to systematically transform existing alignment algorithms.", "result": "RPO consistently enhances four state-of-the-art alignment algorithms (DPO, IPO, SimPO, CPO), achieving up to 7.0% win rate gain on AlpacaEval 2 and 5.4% on Arena-Hard when applied to Mistral and Llama 3 models.", "conclusion": "RPO provides a theoretical guarantee of convergence to true noise level under perfect model calibration and serves as an effective meta-framework for robust preference alignment that addresses fundamental limitations in current human preference-based methods."}}
{"id": "2509.23315", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23315", "abs": "https://arxiv.org/abs/2509.23315", "authors": ["Khang Tran", "Hieu Cao", "Thinh Pham", "Nghiem Diep", "Tri Cao", "Binh Nguyen"], "title": "MELCOT: A Hybrid Learning Architecture with Marginal Preservation for Matrix-Valued Regression", "comment": null, "summary": "Regression is essential across many domains but remains challenging in\nhigh-dimensional settings, where existing methods often lose spatial structure\nor demand heavy storage. In this work, we address the problem of matrix-valued\nregression, where each sample is naturally represented as a matrix. We propose\nMELCOT, a hybrid model that integrates a classical machine learning-based\nMarginal Estimation (ME) block with a deep learning-based Learnable-Cost\nOptimal Transport (LCOT) block. The ME block estimates data marginals to\npreserve spatial information, while the LCOT block learns complex global\nfeatures. This design enables MELCOT to inherit the strengths of both classical\nand deep learning methods. Extensive experiments across diverse datasets and\ndomains demonstrate that MELCOT consistently outperforms all baselines while\nremaining highly efficient.", "AI": {"tldr": "MELCOT is a hybrid model for matrix-valued regression that combines classical machine learning (Marginal Estimation) with deep learning (Learnable-Cost Optimal Transport) to handle high-dimensional data while preserving spatial structure efficiently.", "motivation": "Existing regression methods struggle with high-dimensional data, often losing spatial structure or requiring excessive storage, especially for matrix-valued data where samples are naturally represented as matrices.", "method": "Propose MELCOT with two blocks: ME block for marginal estimation to preserve spatial information, and LCOT block using learnable-cost optimal transport to capture complex global features, combining classical and deep learning strengths.", "result": "Extensive experiments across diverse datasets and domains show MELCOT consistently outperforms all baseline methods while maintaining high efficiency.", "conclusion": "MELCOT successfully addresses matrix-valued regression challenges by integrating classical and deep learning approaches, achieving superior performance with preserved spatial structure and computational efficiency."}}
{"id": "2509.23744", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23744", "abs": "https://arxiv.org/abs/2509.23744", "authors": ["Yucheng Wang", "Yifan Hou", "Aydin Javadov", "Mubashara Akhtar", "Mrinmaya Sachan"], "title": "Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning", "comment": "Our code (https://github.com/DELTA-DoubleWise/OmniReason) and data\n  (https://huggingface.co/datasets/ycwang11/OmniReason) are publicly available", "summary": "Multimodal large language models (MLLMs) promise enhanced reasoning by\nintegrating diverse inputs such as text, vision, and audio. Yet cross-modal\nreasoning remains underexplored, with conflicting reports on whether added\nmodalities help or harm performance. These inconsistencies stem from a lack of\ncontrolled evaluation frameworks and analysis of models' internals to isolate\nwhen and why modality interactions support or undermine reasoning. We address\nthis gap through a logic-grounded evaluation framework that categorizes\nmultimodal reasoning into six interaction patterns, varying how facts are\ndistributed across modalities and logically combined. Empirically, additional\nmodalities enhance reasoning only when they provide independent and sufficient\nreasoning paths, while redundant or chained entailment support often hurts\nperformance. Moreover, reasoning degrades in three systematic ways: weaker\nmodalities drag down overall performance, conflicts bias preference toward\ncertain modalities, and joint signals from different modalities fail to be\nintegrated effectively. Therefore, we identify two core failures:\ntask-composition bottleneck, where recognition and reasoning cannot be jointly\nexecuted in one pass, and fusion bottleneck, where early integration introduces\nbias. For further investigation, we find that attention patterns fail to encode\nfact usefulness, but a simple two-step prompting (recognize then reason)\nrestores performance, confirming the task-composition bottleneck. Moreover,\nmodality identity remains recoverable in early layers, and softening attention\nin early fusion improves reasoning, highlighting biased fusion as another\nfailure mode. Overall, our findings show that integration, not perception, is\nthe main barrier to multimodal reasoning, suggesting composition-aware training\nand early fusion control as promising directions.", "AI": {"tldr": "MLLMs struggle with cross-modal reasoning due to integration failures, not perception issues. Additional modalities help only when providing independent reasoning paths, while redundant or chained support often harms performance through task-composition and fusion bottlenecks.", "motivation": "To address conflicting reports on whether added modalities help or harm multimodal reasoning performance, and to understand when and why modality interactions support or undermine reasoning through controlled evaluation.", "method": "Developed a logic-grounded evaluation framework categorizing multimodal reasoning into six interaction patterns, varying fact distribution across modalities and logical combinations. Used attention pattern analysis and two-step prompting experiments.", "result": "Additional modalities enhance reasoning only when providing independent and sufficient reasoning paths. Reasoning degrades in three ways: weaker modalities drag down performance, conflicts bias modality preference, and joint signals fail integration. Two core failures identified: task-composition bottleneck and fusion bottleneck.", "conclusion": "Integration, not perception, is the main barrier to multimodal reasoning. Composition-aware training and early fusion control are promising directions to address task-composition and fusion bottlenecks."}}
{"id": "2509.24207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24207", "abs": "https://arxiv.org/abs/2509.24207", "authors": ["Sijia Liu", "Niklas Muennighoff", "Kawin Ethayarajh"], "title": "Humanline: Online Alignment as Perceptual Loss", "comment": null, "summary": "Online alignment (e.g., GRPO) is generally more performant than offline\nalignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral\neconomics, we propose a human-centric explanation. We prove that online\non-policy sampling better approximates the human-perceived distribution of what\nthe model can produce, and PPO/GRPO-style clipping -- originally introduced to\njust stabilize training -- recovers a perceptual bias in how humans perceive\nprobability. In this sense, PPO/GRPO act as perceptual losses already. Our\ntheory further suggests that the online/offline dichotomy is itself incidental\nto maximizing human utility, since we can achieve the same effect by\nselectively training on any data in a manner that mimics human perception,\nrather than restricting ourselves to online on-policy data. Doing so would\nallow us to post-train more quickly, cheaply, and flexibly without sacrificing\nperformance. To this end, we propose a design pattern that explicitly\nincorporates perceptual distortions of probability into objectives like\nDPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that\nthese humanline variants, even when trained with offline off-policy data, can\nmatch the performance of their online counterparts on both verifiable and\nunverifiable tasks.", "AI": {"tldr": "Online alignment methods like GRPO outperform offline methods like DPO due to better approximation of human-perceived probability distributions and incorporation of perceptual biases through PPO/GRPO-style clipping. The paper proposes 'humanline' variants that explicitly model human perceptual distortions, enabling offline training to match online performance.", "motivation": "To understand why online alignment methods perform better than offline methods, drawing from behavioral economics and prospect theory to explain the role of human perceptual biases in probability assessment.", "method": "Theoretical analysis showing online on-policy sampling better approximates human-perceived distributions, and PPO/GRPO clipping recovers perceptual biases. Proposes 'humanline' variants of DPO/KTO/GRPO that explicitly incorporate human perceptual distortions of probability.", "result": "Humanline variants trained with offline off-policy data can match the performance of their online counterparts on both verifiable and unverifiable tasks.", "conclusion": "The online/offline dichotomy is incidental to maximizing human utility. By explicitly modeling human perceptual distortions, offline training can achieve the same performance as online methods, enabling faster, cheaper, and more flexible post-training without sacrificing performance."}}
{"id": "2509.23323", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23323", "abs": "https://arxiv.org/abs/2509.23323", "authors": ["Xiangchen Song", "Jiaqi Sun", "Zijian Li", "Yujia Zheng", "Kun Zhang"], "title": "LLM Interpretability with Identifiable Temporal-Instantaneous Representation", "comment": "NeurIPS 2025", "summary": "Despite Large Language Models' remarkable capabilities, understanding their\ninternal representations remains challenging. Mechanistic interpretability\ntools such as sparse autoencoders (SAEs) were developed to extract\ninterpretable features from LLMs but lack temporal dependency modeling,\ninstantaneous relation representation, and more importantly theoretical\nguarantees, undermining both the theoretical foundations and the practical\nconfidence necessary for subsequent analyses. While causal representation\nlearning (CRL) offers theoretically grounded approaches for uncovering latent\nconcepts, existing methods cannot scale to LLMs' rich conceptual space due to\ninefficient computation. To bridge the gap, we introduce an identifiable\ntemporal causal representation learning framework specifically designed for\nLLMs' high-dimensional concept space, capturing both time-delayed and\ninstantaneous causal relations. Our approach provides theoretical guarantees\nand demonstrates efficacy on synthetic datasets scaled to match real-world\ncomplexity. By extending SAE techniques with our temporal causal framework, we\nsuccessfully discover meaningful concept relationships in LLM activations. Our\nfindings show that modeling both temporal and instantaneous conceptual\nrelationships advances the interpretability of LLMs.", "AI": {"tldr": "The paper introduces an identifiable temporal causal representation learning framework to address limitations of existing mechanistic interpretability tools for LLMs, providing theoretical guarantees and capturing both time-delayed and instantaneous causal relations.", "motivation": "Current mechanistic interpretability tools like sparse autoencoders lack temporal dependency modeling, instantaneous relation representation, and theoretical guarantees, undermining confidence in LLM interpretability analyses.", "method": "Proposed an identifiable temporal causal representation learning framework specifically designed for LLMs' high-dimensional concept space, extending SAE techniques to capture both time-delayed and instantaneous causal relations.", "result": "The approach demonstrates efficacy on synthetic datasets scaled to real-world complexity and successfully discovers meaningful concept relationships in LLM activations.", "conclusion": "Modeling both temporal and instantaneous conceptual relationships advances the interpretability of LLMs, bridging the gap between theoretical foundations and practical applications."}}
{"id": "2509.23755", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23755", "abs": "https://arxiv.org/abs/2509.23755", "authors": ["Chao Wang", "Rui-Chen Zheng", "Yang Ai", "Zhen-Hua Ling"], "title": "Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance Analysis", "comment": null, "summary": "The integration of speech into Large Language Models (LLMs) has substantially\nexpanded their capabilities, but often at the cost of weakening their core\ntextual competence. This degradation limits the ability of speech-enabled LLMs\nto fully exploit their pre-trained text-based knowledge. In this work, we\nanalyze the underlying mechanisms of this issue through a focused study of the\nwidely used encoder-adaptor paradigm. We propose an analytical framework based\non parameter importance estimation, which reveals that fine-tuning for speech\nintroduces a textual importance distribution shift: the layer-wise allocation\nof parameters critical to textual reasoning is disrupted. Building on this\ninsight, we investigate two mitigation strategies: layer-wise learning rate\nscheduling and Low-Rank Adaptation (LoRA), both aim to preserve the original\nparameter distribution. Experimental results show that both approaches better\nmaintain textual competence than full fine-tuning, while also improving\ndownstream spoken question answering performance. Furthermore, our analysis\noffers a principled explanation for the effectiveness of the proposed\nmitigation strategies, linking their benefits to the structural properties of\ntextual knowledge in LLMs.", "AI": {"tldr": "Speech integration in LLMs weakens text competence. Analysis shows speech fine-tuning disrupts parameter importance distribution for text reasoning. Layer-wise learning rate and LoRA preserve text ability while improving speech QA.", "motivation": "Speech-enabled LLMs degrade in textual competence, limiting their ability to leverage pre-trained text knowledge. Need to understand and mitigate this degradation.", "method": "Propose analytical framework using parameter importance estimation. Investigate two mitigation strategies: layer-wise learning rate scheduling and Low-Rank Adaptation (LoRA).", "result": "Both approaches better maintain textual competence than full fine-tuning while improving spoken question answering performance.", "conclusion": "The study provides principled explanation for mitigation strategies' effectiveness, linking benefits to structural properties of textual knowledge in LLMs."}}
{"id": "2509.24230", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24230", "abs": "https://arxiv.org/abs/2509.24230", "authors": ["Shaobin Ling", "Yun Wang", "Chenyou Fan", "Tin Lun Lam", "Junjie Hu"], "title": "ELHPlan: Efficient Long-Horizon Task Planning for Multi-Agent Collaboration", "comment": null, "summary": "Large Language Models (LLMs) enable intelligent multi-robot collaboration but\nface fundamental trade-offs: declarative methods lack adaptability in dynamic\nenvironments, while iterative methods incur prohibitive computational costs\nthat scale poorly with team size and task complexity. In this paper, we propose\nELHPlan, a novel framework that introduces Action Chains--sequences of actions\nexplicitly bound to sub-goal intentions--as the fundamental planning primitive.\nELHPlan operates via a cyclical process: 1) constructing intention-bound action\nsequences, 2) proactively validating for conflicts and feasibility, 3) refining\nissues through targeted mechanisms, and 4) executing validated actions. This\ndesign balances adaptability and efficiency by providing sufficient planning\nhorizons while avoiding expensive full re-planning. We further propose\ncomprehensive efficiency metrics, including token consumption and planning\ntime, to more holistically evaluate multi-agent collaboration. Our experiments\non benchmark TDW-MAT and C-WAH demonstrate that ELHPlan achieves comparable\ntask success rates while consuming only 24% of the tokens required by\nstate-of-the-art methods. Our research establishes a new\nefficiency-effectiveness frontier for LLM-based multi-agent planning systems.", "AI": {"tldr": "ELHPlan is a novel LLM-based multi-robot planning framework that uses Action Chains to balance adaptability and efficiency, achieving comparable task success with only 24% of tokens compared to state-of-the-art methods.", "motivation": "Current LLM-based multi-robot collaboration faces fundamental trade-offs: declarative methods lack adaptability in dynamic environments, while iterative methods have prohibitive computational costs that scale poorly with team size and task complexity.", "method": "ELHPlan introduces Action Chains (sequences of actions bound to sub-goal intentions) as planning primitives. It operates through a cyclical process: constructing intention-bound action sequences, proactive validation for conflicts/feasibility, targeted refinement, and execution of validated actions.", "result": "Experiments on TDW-MAT and C-WAH benchmarks show ELHPlan achieves comparable task success rates while consuming only 24% of the tokens required by state-of-the-art methods. It also introduces comprehensive efficiency metrics including token consumption and planning time.", "conclusion": "ELHPlan establishes a new efficiency-effectiveness frontier for LLM-based multi-agent planning systems by balancing adaptability and efficiency through Action Chains and proactive validation mechanisms."}}
{"id": "2509.23325", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23325", "abs": "https://arxiv.org/abs/2509.23325", "authors": ["Jonas Ngnaw\u00e9", "Maxime Heuillet", "Sabyasachi Sahoo", "Yann Pequignot", "Ola Ahmad", "Audrey Durand", "Fr\u00e9d\u00e9ric Precioso", "Christian Gagn\u00e9"], "title": "Robust Fine-Tuning from Non-Robust Pretrained Models: Mitigating Suboptimal Transfer With Adversarial Scheduling", "comment": null, "summary": "Fine-tuning pretrained models is a standard and effective workflow in modern\nmachine learning. However, robust fine-tuning (RFT), which aims to\nsimultaneously achieve adaptation to a downstream task and robustness to\nadversarial examples, remains challenging. Despite the abundance of non-robust\npretrained models in open-source repositories, their potential for RFT is less\nunderstood. We address this knowledge gap by systematically examining RFT from\nsuch non-robust models. Our experiments reveal that fine-tuning non-robust\nmodels with a robust objective, even under small perturbations, can lead to\npoor performance, a phenomenon that we dub \\emph{suboptimal transfer}. In\nchallenging scenarios (eg, difficult tasks, high perturbation), the resulting\nperformance can be so low that it may be considered a transfer failure. We find\nthat fine-tuning using a robust objective impedes task adaptation at the\nbeginning of training and eventually prevents optimal transfer. However, we\npropose a novel heuristic, \\emph{Epsilon-Scheduling}, a schedule over\nperturbation strength used during training that promotes optimal transfer.\nAdditionally, we introduce \\emph{expected robustness}, a metric that captures\nperformance across a range of perturbations, providing a more comprehensive\nevaluation of the accuracy-robustness trade-off for diverse models at test\ntime. Extensive experiments on a wide range of configurations (six pretrained\nmodels and five datasets) show that \\emph{Epsilon-Scheduling} successfully\nprevents \\emph{suboptimal transfer} and consistently improves expected\nrobustness.", "AI": {"tldr": "Fine-tuning non-robust pretrained models with robust objectives can cause suboptimal transfer and poor performance. The paper proposes Epsilon-Scheduling to prevent this issue and introduces expected robustness as a comprehensive evaluation metric.", "motivation": "To address the challenge of robust fine-tuning (RFT) from non-robust pretrained models, which remains understudied despite the abundance of such models in open-source repositories.", "method": "Systematically examine RFT from non-robust models, identify suboptimal transfer phenomenon, and propose Epsilon-Scheduling - a schedule over perturbation strength during training to promote optimal transfer.", "result": "Fine-tuning with robust objectives impedes task adaptation and prevents optimal transfer, especially in challenging scenarios. Epsilon-Scheduling successfully prevents suboptimal transfer and consistently improves expected robustness across six pretrained models and five datasets.", "conclusion": "Epsilon-Scheduling is an effective heuristic for robust fine-tuning that prevents suboptimal transfer from non-robust pretrained models, and expected robustness provides a comprehensive evaluation metric for accuracy-robustness trade-off."}}
{"id": "2509.23765", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23765", "abs": "https://arxiv.org/abs/2509.23765", "authors": ["Junliang Li", "Yucheng Wang", "Yan Chen", "Yu Ran", "Ruiqing Zhang", "Jing Liu", "Hua Wu", "Haifeng Wang"], "title": "Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality", "comment": null, "summary": "Hallucination and factuality deficits remain key obstacles to the reliability\nof large language models (LLMs) in long-form generation. Existing reinforcement\nlearning from human feedback (RLHF) frameworks primarily rely on preference\nrewards, yet they often overlook the model's internal knowledge boundaries,\nexacerbating the so-called \"hallucination tax\". To address this challenge, we\npropose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a\nnovel framework that focuses on the knowledge consistency between the policy\nmodel's expressed knowledge and the base model's parametric knowledge, and\nintroduces a Dual-Fact Alignment mechanism to jointly optimize factual recall\nand precision. Specifically, KLCF leverages pretrained knowledge boundaries to\nconstruct fact checklist, guiding online reinforcement learning to improve\nfactual coverage and recall; simultaneously, it trains a self-assessment module\nbased on the base model's internal knowledge to enhance factual precision\nduring generation. Unlike prior methods that rely on external retrieval or\nheavy verification, our reward design is fully external-knowledge-free and\nlightweight, making KLCF efficient and easily scalable to large-scale training.\nExperimental results demonstrate that KLCF substantially improves factuality\nmetrics across multiple long-form benchmarks and effectively alleviates model\nhallucinations.", "AI": {"tldr": "KLCF is a reinforcement learning framework that improves LLM factuality by aligning policy model's expressed knowledge with base model's parametric knowledge through dual-fact alignment mechanism.", "motivation": "To address hallucination and factuality deficits in LLM long-form generation, particularly the \"hallucination tax\" caused by RLHF frameworks overlooking internal knowledge boundaries.", "method": "Uses knowledge consistency between policy and base models, constructs fact checklists from pretrained knowledge boundaries, implements dual-fact alignment for factual recall and precision, and trains self-assessment module based on base model's internal knowledge.", "result": "Substantially improves factuality metrics across multiple long-form benchmarks and effectively alleviates model hallucinations.", "conclusion": "KLCF provides an efficient, external-knowledge-free, and scalable solution to improve LLM factuality without relying on external retrieval or heavy verification."}}
{"id": "2509.24238", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24238", "abs": "https://arxiv.org/abs/2509.24238", "authors": ["Yixin He", "Lumingyuan Tang"], "title": "Learning to Ponder: Adaptive Reasoning in Latent Space", "comment": null, "summary": "Test-time compute has emerged as a key paradigm for enhancing LLM reasoning,\nyet prevailing approaches like Best-of-N and majority voting apply uniform\ndepth across inputs, wasting computation on simple queries while potentially\nunder-thinking complex ones. We present FR-Ponder, a single-graph,\nbackbone-training-free framework that allocates instance-adaptive reasoning\ncompute via latent steering. A less than 1M-param controller observes hidden\nstates and decides to halt or apply a small ponder step by adding a\npre-computed steering vector to frozen representations. Our method extracts the\nlatent steering vector associated with deeper reasoning outputs and direct IO\nfrom LLM and re-applies it through a tunable scaling factor, allowing the model\nto adapt its reasoning depth to the complexity of each input. To balance\nperformance and computational cost, we employ Group Relative Policy\nOptimization (GRPO) as a reward signal to adaptively regulate reasoning depth,\nachieving task accuracy while mitigating overreasoning. Through curriculum\nlearning and careful reward engineering, FR-Ponder learns calibrated compute\nallocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder\nimproves the compute-accuracy frontier, delivering lower FLOPs with better\nmatched accuracy and comparing favorably to early-exit baselines, without\nmodifying backbone weights. Analyses visualize interpretable steering\ndirections and show learned compute allocation correlates with problem\ndifficulty.", "AI": {"tldr": "FR-Ponder is a framework that adaptively allocates reasoning compute for LLMs by using a small controller to decide when to halt or apply ponder steps via latent steering vectors, optimizing compute usage without modifying backbone weights.", "motivation": "Current approaches like Best-of-N and majority voting use uniform compute depth across all inputs, which wastes computation on simple queries and under-thinks complex ones. There's a need for instance-adaptive reasoning compute allocation.", "method": "Uses a <1M-param controller that observes hidden states to decide whether to halt or apply ponder steps by adding pre-computed steering vectors to frozen representations. Employs Group Relative Policy Optimization (GRPO) for reward signals and curriculum learning to regulate reasoning depth adaptively.", "result": "On GSM8K and MATH500 benchmarks, FR-Ponder improves the compute-accuracy frontier, achieving lower FLOPs with better matched accuracy compared to early-exit baselines, without modifying backbone weights.", "conclusion": "FR-Ponder successfully enables instance-adaptive reasoning compute allocation through latent steering, demonstrating interpretable steering directions and learned compute allocation that correlates with problem difficulty."}}
{"id": "2509.23348", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23348", "abs": "https://arxiv.org/abs/2509.23348", "authors": ["Xavier Aramayo Carrasco", "Grigoriy Ksenofontov", "Aleksei Leonov", "Iaroslav Sergeevich Koshelev", "Alexander Korotin"], "title": "Entering the Era of Discrete Diffusion Models: A Benchmark for Schr\u00f6dinger Bridges and Entropic Optimal Transport", "comment": null, "summary": "The Entropic Optimal Transport (EOT) problem and its dynamic counterpart, the\nSchr\\\"odinger bridge (SB) problem, play an important role in modern machine\nlearning, linking generative modeling with optimal transport theory. While\nrecent advances in discrete diffusion and flow models have sparked growing\ninterest in applying SB methods to discrete domains, there is still no reliable\nway to evaluate how well these methods actually solve the underlying problem.\nWe address this challenge by introducing a benchmark for SB on discrete spaces.\nOur construction yields pairs of probability distributions with analytically\nknown SB solutions, enabling rigorous evaluation. As a byproduct of building\nthis benchmark, we obtain two new SB algorithms, DLightSB and DLightSB-M, and\nadditionally extend prior related work to construct the $\\alpha$-CSBM\nalgorithm. We demonstrate the utility of our benchmark by evaluating both\nexisting and new solvers in high-dimensional discrete settings. This work\nprovides the first step toward proper evaluation of SB methods on discrete\nspaces, paving the way for more reproducible future studies.", "AI": {"tldr": "This paper introduces the first benchmark for evaluating Schr\u00f6dinger bridge (SB) methods on discrete spaces, providing analytically known SB solutions for rigorous testing. It also develops new SB algorithms (DLightSB, DLightSB-M, \u03b1-CSBM) and demonstrates their utility in high-dimensional discrete settings.", "motivation": "There is growing interest in applying Schr\u00f6dinger bridge methods to discrete domains (e.g., discrete diffusion models), but no reliable way exists to evaluate how well these methods actually solve the underlying SB problem.", "method": "The authors construct a benchmark that yields pairs of probability distributions with analytically known SB solutions. They also develop two new SB algorithms (DLightSB and DLightSB-M) and extend prior work to create the \u03b1-CSBM algorithm.", "result": "The benchmark enables rigorous evaluation of SB solvers in high-dimensional discrete settings. The new algorithms are demonstrated to be effective through evaluation on the proposed benchmark.", "conclusion": "This work provides the first step toward proper evaluation of SB methods on discrete spaces, paving the way for more reproducible future studies in this important area connecting generative modeling with optimal transport theory."}}
{"id": "2509.23767", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23767", "abs": "https://arxiv.org/abs/2509.23767", "authors": ["Zehong Wang", "Junlin Wu", "ZHaoxuan Tan", "Bolian Li", "Xianrui Zhong", "Zheli Liu", "Qingkai Zeng"], "title": "From Personal to Collective: On the Role of Local and Global Memory in LLM Personalization", "comment": null, "summary": "Large language model (LLM) personalization aims to tailor model behavior to\nindividual users based on their historical interactions. However, its\neffectiveness is often hindered by two key challenges: the \\textit{cold-start\nproblem}, where users with limited history provide insufficient context for\naccurate personalization, and the \\textit{biasing problem}, where users with\nabundant but skewed history cause the model to overfit to narrow preferences.\nWe identify both issues as symptoms of a common underlying limitation, i.e.,\nthe inability to model collective knowledge across users. To address this, we\npropose a local-global memory framework (LoGo) that combines the personalized\nlocal memory with a collective global memory that captures shared interests\nacross the population. To reconcile discrepancies between these two memory\nsources, we introduce a mediator module designed to resolve conflicts between\nlocal and global signals. Extensive experiments on multiple benchmarks\ndemonstrate that LoGo consistently improves personalization quality by both\nwarming up cold-start users and mitigating biased predictions. These results\nhighlight the importance of incorporating collective knowledge to enhance LLM\npersonalization.", "AI": {"tldr": "LoGo framework addresses LLM personalization challenges by combining local user memory with global collective memory and using a mediator to resolve conflicts, improving cold-start and biasing problems.", "motivation": "To solve the cold-start problem (insufficient user history) and biasing problem (overfitting to skewed user preferences) in LLM personalization by leveraging collective knowledge across users.", "method": "Proposed LoGo framework with local-global memory: local memory for individual user preferences, global memory for shared interests across population, and a mediator module to reconcile conflicts between local and global signals.", "result": "Extensive experiments on multiple benchmarks show LoGo consistently improves personalization quality by warming up cold-start users and mitigating biased predictions.", "conclusion": "Incorporating collective knowledge through the LoGo framework effectively enhances LLM personalization by addressing both cold-start and biasing challenges."}}
{"id": "2509.24244", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24244", "abs": "https://arxiv.org/abs/2509.24244", "authors": ["Yuanyi Wang", "Yanggan Gu", "Yiming Zhang", "Qi Zhou", "Zhaoyi Yan", "Congkai Xie", "Xinyao Wang", "Jianbo Yuan", "Hongxia Yang"], "title": "Model Merging Scaling Laws in Large Language Models", "comment": "30 pages", "summary": "We study empirical scaling laws for language model merging measured by\ncross-entropy. Despite its wide practical use, merging lacks a quantitative\nrule that predicts returns as we add experts or scale the model size. We\nidentify a compact power law that links model size and expert number: the\nsize-dependent floor decreases with model capacity, while the merging tail\nexhibits clear diminishing returns in the number of experts. The law holds\nin-domain and cross-domain, tightly fits measured curves across diverse\narchitectures and methods (Average, TA, TIES, DARE), and explains two robust\nregularities: most gains arrive early, and variability shrinks as more experts\nare included. Building on this, we present a simple theory that explains why\ngains fall roughly as 1/k and links the floor and tail to properties of the\nbase model and the diversity across domains. This law enables predictive\nplanning: estimate how many experts are needed to reach a target loss, decide\nwhen to stop adding experts, and trade off scaling the base model versus adding\nexperts under a fixed budget--turning merging from heuristic practice into a\ncomputationally efficient, planable alternative to multitask training. This\nsuggests a scaling principle for distributed generative AI: predictable gains\ncan be achieved by composing specialists, offering a complementary path toward\nAGI-level systems.", "AI": {"tldr": "The paper identifies a power law for language model merging that predicts performance gains from combining experts, showing diminishing returns with more experts and enabling predictive planning for model composition.", "motivation": "To establish quantitative scaling laws for language model merging, which is widely used in practice but lacks predictive rules for returns when adding experts or scaling model size.", "method": "Empirical study of scaling laws for model merging measured by cross-entropy, analyzing how performance scales with model size and number of experts across diverse architectures and merging methods (Average, TA, TIES, DARE).", "result": "Identified a compact power law showing size-dependent floor decreases with model capacity, merging tail exhibits diminishing returns (roughly 1/k), gains arrive early, and variability shrinks with more experts. The law holds in-domain and cross-domain across diverse methods.", "conclusion": "The scaling law enables predictive planning for model merging, allowing estimation of expert needs, stopping criteria, and trade-offs between scaling base models vs adding experts. This turns merging from heuristic practice into a computationally efficient alternative to multitask training, suggesting a scaling principle for distributed generative AI."}}
{"id": "2509.23357", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23357", "abs": "https://arxiv.org/abs/2509.23357", "authors": ["Andrey Kharitenko", "Zebang Shen", "Riccardo de Santi", "Niao He", "Florian Doerfler"], "title": "Landing with the Score: Riemannian Optimization through Denoising", "comment": "37 pages, 9 figures", "summary": "Under the data manifold hypothesis, high-dimensional data are concentrated\nnear a low-dimensional manifold. We study the problem of Riemannian\noptimization over such manifolds when they are given only implicitly through\nthe data distribution, and the standard manifold operations required by\nclassical algorithms are unavailable. This formulation captures a broad class\nof data-driven design problems that are central to modern generative AI. Our\nkey idea is to introduce a link function that connects the data distribution to\nthe geometric operations needed for optimization. We show that this function\nenables the recovery of essential manifold operations, such as retraction and\nRiemannian gradient computation. Moreover, we establish a direct connection\nbetween our construction and the score function in diffusion models of the data\ndistribution. This connection allows us to leverage well-studied\nparameterizations, efficient training procedures, and even pretrained score\nnetworks from the diffusion model literature to perform optimization. Building\non this foundation, we propose two efficient inference-time algorithms --\nDenoising Landing Flow (DLF) and Denoising Riemannian Gradient Descent (DRGD)\n-- and provide theoretical guarantees for both feasibility (approximate\nmanifold adherence) and optimality (small Riemannian gradient norm). Finally,\nwe demonstrate the effectiveness of our approach on finite-horizon reference\ntracking tasks in data-driven control, highlighting its potential for practical\ngenerative and design applications.", "AI": {"tldr": "The paper introduces methods for Riemannian optimization over data manifolds given only through the data distribution, connecting diffusion model score functions to manifold operations and proposing efficient inference algorithms with theoretical guarantees.", "motivation": "High-dimensional data often lie near low-dimensional manifolds, but classical optimization methods require explicit manifold operations which are unavailable when manifolds are given only implicitly through data distributions. This problem is central to modern generative AI and data-driven design.", "method": "Introduce a link function connecting data distribution to geometric operations, enabling recovery of manifold operations like retraction and Riemannian gradient computation. Leverage diffusion model score functions and propose two algorithms: Denoising Landing Flow (DLF) and Denoising Riemannian Gradient Descent (DRGD).", "result": "Theoretical guarantees for both feasibility (approximate manifold adherence) and optimality (small Riemannian gradient norm). Demonstrated effectiveness on finite-horizon reference tracking tasks in data-driven control.", "conclusion": "The approach successfully connects diffusion models with Riemannian optimization, enabling practical generative and design applications by leveraging existing score networks and training procedures from diffusion model literature."}}
{"id": "2509.23782", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23782", "abs": "https://arxiv.org/abs/2509.23782", "authors": ["Yoonah Park", "Haesung Pyun", "Yohan Jo"], "title": "Bridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions", "comment": null, "summary": "Large Language Models (LLMs) often fail on multiple-choice questions (MCQs)\ndespite demonstrating correct knowledge in other contexts, such as free-form\ngeneration. To investigate the mechanism underlying this knowledge-prediction\ngap on MCQs and alleviate it, we conduct a probing analysis and find that\nresidual streams in certain layers contain a subspace spanned by two important\nbases: a \\emph{knowledge basis} that encodes the probability of the\nground-truth answer for a given MCQ and a \\emph{prediction basis} that encodes\nthe probability of the answer choice predicted by the model. We observe that\nincorrect predictions arise from a misalignment of the model's hidden states\nalong these two bases. Hence, we introduce \\textbf{KAPPA} (Knowledge-Aligned\nPrediction through Projection-based Adjustment), a parameter-free intervention\nthat transforms the hidden states to align the prediction coordinate with the\nknowledge coordinate within this subspace. Experiments on binary-choice\nreformulations of Big-Bench-Hard and ARC-Challenge show that KAPPA\nsubstantially improves accuracy and consistently outperforms baselines. While\noptimal subspaces differ across tasks, subspaces generalize to some extent, as\nsupported by cross-dataset experiments. Moreover, KAPPA extends its\neffectiveness to free-form questions beyond MCQs. Our work provides a new\ngeometric understanding of the knowledge-prediction gap and offers a practical\nmethod for better aligning model behavior with its latent knowledge.", "AI": {"tldr": "KAPPA is a parameter-free method that improves LLM performance on multiple-choice questions by aligning knowledge and prediction subspaces in hidden states.", "motivation": "LLMs often fail on multiple-choice questions despite having correct knowledge, due to misalignment between knowledge and prediction subspaces in hidden states.", "method": "Identify knowledge and prediction bases in residual streams, then use projection-based adjustment (KAPPA) to align hidden states along these bases.", "result": "KAPPA substantially improves accuracy on Big-Bench-Hard and ARC-Challenge, outperforms baselines, and generalizes to free-form questions.", "conclusion": "The work provides geometric understanding of knowledge-prediction gap and practical method for better aligning model behavior with latent knowledge."}}
{"id": "2509.24248", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24248", "abs": "https://arxiv.org/abs/2509.24248", "authors": ["Rubing Yang", "Huajun Bai", "Song Liu", "Guanghua Yu", "Runzhi Fan", "Yanbin Dang", "Jiejing Zhang", "Kai Liu", "Jianchen Zhu", "Peng Chen"], "title": "SpecExit: Accelerating Large Reasoning Model via Speculative Exit", "comment": null, "summary": "Despite their strong performance on reasoning tasks, large reasoning models\n(LRMs) often suffer from overthinking, producing unnecessarily long outputs and\nincurring high end-to-end latency, a significant limitation to their real-world\ndeployment. To address overthinking, early-exit mechanisms have been proposed\nto terminate reasoning before typical completion, showing that this approach\ncan effectively shorten generation length with minimal impact on accuracy.\nHowever, their reliance on probing mechanisms introduces a detection overhead\nthat limits their end-to-end latency gains and compromises their\ngeneralizability across diverse problems. Inspired by the use of hidden states\nin speculative decoding, we propose SpecExit, a novel framework that predicts\nboth future tokens and an early-exit signal directly from a lightweight draft\nmodel without probing overhead. Our method offers significant improvements,\nreducing average generation length by 66\\% and achieving a 2.5x speedup in\nend-to-end latency compared to the speculative decoding baseline, without\ncompromising accuracy. Our method leverages the inherent signals from hidden\nstates to provide effective early-exit signals, suggesting broader use of\nhidden states for efficient reasoning. Our code is available at\nhttps://github.com/Tencent/AngelSlim.", "AI": {"tldr": "SpecExit is a novel framework that uses hidden states from lightweight draft models to predict both future tokens and early-exit signals, reducing generation length by 66% and achieving 2.5x speedup in end-to-end latency without compromising accuracy.", "motivation": "Large reasoning models suffer from overthinking, producing unnecessarily long outputs with high latency, limiting real-world deployment. Existing early-exit mechanisms have detection overhead that limits latency gains and generalizability.", "method": "Proposes SpecExit framework that predicts future tokens and early-exit signals directly from lightweight draft model's hidden states, eliminating probing overhead. Inspired by speculative decoding's use of hidden states.", "result": "Reduces average generation length by 66%, achieves 2.5x speedup in end-to-end latency compared to speculative decoding baseline, while maintaining accuracy.", "conclusion": "Hidden states provide effective early-exit signals, suggesting broader use of hidden states for efficient reasoning. Method demonstrates significant improvements in efficiency without accuracy loss."}}
{"id": "2509.23365", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23365", "abs": "https://arxiv.org/abs/2509.23365", "authors": ["Hanlin Zhu", "Shibo Hao", "Zhiting Hu", "Jiantao Jiao", "Stuart Russell", "Yuandong Tian"], "title": "Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought", "comment": "29 pages, 5 figures", "summary": "Previous work shows that the chain of continuous thought (continuous CoT)\nimproves the reasoning capability of large language models (LLMs) by enabling\nimplicit parallel thinking, and a subsequent work provided theoretical insight\nby showing that a two-layer transformer equipped with continuous CoT can\nefficiently solve directed graph reachability by maintaining a superposition of\nmultiple reasoning traces in the continuous thought. However, it remains\nunclear how the superposition mechanism is naturally learned from\ngradient-based training methods. To fill this gap, we theoretically analyze the\ntraining dynamics of a simplified two-layer transformer on the directed graph\nreachability problem to unveil how the superposition mechanism emerges during\ntraining in two training stages -- (i) a thought-generation stage that\nautoregressively expands the continuous thought, and (ii) a prediction stage\nthat converts the thought into the final answer. Our analysis reveals that\nduring training using continuous thought, the index-matching logit, an\nimportant quantity which reflects the strength of the model's local search\nability, will first increase and then remain bounded under mild assumptions.\nThe bounded index-matching logit effectively balances exploration and\nexploitation during the reasoning process: the model will exploit local problem\nstructures to identify plausible search traces, and assign comparable weights\nto multiple such traces to explore when it is uncertain about which solution is\ncorrect, which results in superposition. Our experimental results tracking the\ngrowth of logits further validate our theory.", "AI": {"tldr": "This paper analyzes how the superposition mechanism in continuous chain-of-thought reasoning naturally emerges during training of two-layer transformers on graph reachability problems.", "motivation": "To understand how the superposition mechanism in continuous CoT reasoning is learned from gradient-based training methods, as previous work showed theoretical capabilities but not the learning dynamics.", "method": "Theoretical analysis of training dynamics of a simplified two-layer transformer on directed graph reachability problem, examining two training stages: thought-generation and prediction stages.", "result": "Analysis reveals that the index-matching logit first increases then remains bounded, balancing exploration and exploitation - exploiting local structures while assigning comparable weights to multiple traces when uncertain, creating superposition.", "conclusion": "The bounded index-matching logit during training enables the superposition mechanism by balancing exploration and exploitation, with experimental results validating the theoretical findings."}}
{"id": "2509.23793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23793", "abs": "https://arxiv.org/abs/2509.23793", "authors": ["Muhammad Abu Ahmad", "Mohamad Ballout", "Raia Abu Ahmad", "Elia Bruni"], "title": "Transformer Tafsir at QIAS 2025 Shared Task: Hybrid Retrieval-Augmented Generation for Islamic Knowledge Question Answering", "comment": "Accepted at ArabicNLP 2025, co-located with EMNLP 2025", "summary": "This paper presents our submission to the QIAS 2025 shared task on Islamic\nknowledge understanding and reasoning. We developed a hybrid\nretrieval-augmented generation (RAG) system that combines sparse and dense\nretrieval methods with cross-encoder reranking to improve large language model\n(LLM) performance. Our three-stage pipeline incorporates BM25 for initial\nretrieval, a dense embedding retrieval model for semantic matching, and\ncross-encoder reranking for precise content retrieval. We evaluate our approach\non both subtasks using two LLMs, Fanar and Mistral, demonstrating that the\nproposed RAG pipeline enhances performance across both, with accuracy\nimprovements up to 25%, depending on the task and model configuration. Our best\nconfiguration is achieved with Fanar, yielding accuracy scores of 45% in\nSubtask 1 and 80% in Subtask 2.", "AI": {"tldr": "Hybrid RAG system combining sparse/dense retrieval with cross-encoder reranking for Islamic knowledge tasks, improving LLM accuracy up to 25%.", "motivation": "To enhance large language model performance on Islamic knowledge understanding and reasoning tasks through improved information retrieval.", "method": "Three-stage pipeline: BM25 for initial retrieval, dense embedding model for semantic matching, cross-encoder reranking for precise content selection.", "result": "Accuracy improvements up to 25% across tasks; best configuration with Fanar achieved 45% in Subtask 1 and 80% in Subtask 2.", "conclusion": "The hybrid RAG pipeline effectively enhances LLM performance on Islamic knowledge tasks, with significant accuracy gains through multi-stage retrieval optimization."}}
{"id": "2509.24250", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24250", "abs": "https://arxiv.org/abs/2509.24250", "authors": ["Edward Kim", "Daniel He", "Jorge Chao", "Wiktor Rajca", "Mohammed Amin", "Nishant Malpani", "Ruta Desai", "Antti Oulasvirta", "Bjoern Hartmann", "Sanjit Seshia"], "title": "Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations", "comment": null, "summary": "Teaching systems physical tasks is a long standing goal in HCI, yet most\nprior work has focused on non collaborative physical activities. Collaborative\ntasks introduce added complexity, requiring systems to infer users assumptions\nabout their teammates intent, which is an inherently ambiguous and dynamic\nprocess. This necessitates representations that are interpretable and\ncorrectable, enabling users to inspect and refine system behavior. We address\nthis challenge by framing collaborative task learning as a program synthesis\nproblem. Our system represents behavior as editable programs and uses narrated\ndemonstrations, i.e. paired physical actions and natural language, as a unified\nmodality for teaching, inspecting, and correcting system logic without\nrequiring users to see or write code. The same modality is used for the system\nto communicate its learning to users. In a within subjects study, 20 users\ntaught multiplayer soccer tactics to our system. 70 percent (14/20) of\nparticipants successfully refined learned programs to match their intent and 90\npercent (18/20) found it easy to correct the programs. The study surfaced\nunique challenges in representing learning as programs and in enabling users to\nteach collaborative physical activities. We discuss these issues and outline\nmitigation strategies.", "AI": {"tldr": "The paper presents a system for teaching collaborative physical tasks using program synthesis and narrated demonstrations, enabling users to teach, inspect, and correct system behavior without coding.", "motivation": "Teaching collaborative physical tasks is challenging due to the need to infer users' assumptions about teammates' intent, requiring interpretable and correctable representations.", "method": "Frames collaborative task learning as program synthesis, representing behavior as editable programs using narrated demonstrations (paired physical actions and natural language) as a unified teaching modality.", "result": "In a study with 20 users teaching soccer tactics, 70% successfully refined learned programs to match intent and 90% found it easy to correct programs.", "conclusion": "The approach enables effective teaching of collaborative physical activities but surfaces unique challenges in program representation and teaching, with proposed mitigation strategies."}}
{"id": "2509.23366", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23366", "abs": "https://arxiv.org/abs/2509.23366", "authors": ["Ange-Cl\u00e9ment Akazan", "Verlon Roel Mbingui"], "title": "Splines-Based Feature Importance in Kolmogorov-Arnold Networks: A Framework for Supervised Tabular Data Dimensionality Reduction", "comment": null, "summary": "High-dimensional datasets require effective feature selection to improve\npredictive performance, interpretability, and robustness. We propose and\nevaluate feature selection methods for tabular datasets based on\nKolmogorov-Arnold networks (KANs), which parameterize feature transformations\nthrough splines, enabling direct access to interpretable importance measures.\nWe introduce four KAN-based selectors ($\\textit{KAN-L1}$, $\\textit{KAN-L2}$,\n$\\textit{KAN-SI}$, $\\textit{KAN-KO}$) and compare them against classical\nbaselines (LASSO, Random Forest, Mutual Information, SVM-RFE) across multiple\nclassification and regression tabular dataset benchmarks. Average (over three\nretention levels: 20\\%, 40\\%, and 60\\%) F1 scores and $R^2$ score results\nreveal that KAN-based selectors, particularly $\\textit{KAN-L2}$,\n$\\textit{KAN-L1}$, $\\textit{KAN-SI}$, and $\\textit{KAN-KO}$, are competitive\nwith and sometimes superior to classical baselines in structured and synthetic\ndatasets. However, $\\textit{KAN-L1}$ is often too aggressive in regression,\nremoving useful features, while $\\textit{KAN-L2}$ underperforms in\nclassification, where simple coefficient shrinkage misses complex feature\ninteractions. $\\textit{KAN-L2}$ and $\\textit{KAN-SI}$ provide robust\nperformance on noisy regression datasets and heterogeneous datasets, aligning\nclosely with ensemble predictors. In classification tasks, KAN selectors such\nas $\\textit{KAN-L1}$, $\\textit{KAN-KO}$, and $\\textit{KAN-SI}$ sometimes\nsurpass the other selectors by eliminating redundancy, particularly in\nhigh-dimensional multi-class data. Overall, our findings demonstrate that\nKAN-based feature selection provides a powerful and interpretable alternative\nto traditional methods, capable of uncovering nonlinear and multivariate\nfeature relevance beyond sparsity or impurity-based measures.", "AI": {"tldr": "KAN-based feature selection methods provide competitive and interpretable alternatives to classical methods, with specific variants excelling in different scenarios (structured data, noisy regression, high-dimensional classification).", "motivation": "High-dimensional datasets require effective feature selection to improve predictive performance, interpretability, and robustness, motivating the development of KAN-based methods that leverage spline parameterization for direct importance measurement.", "method": "Proposed four KAN-based feature selectors (KAN-L1, KAN-L2, KAN-SI, KAN-KO) using Kolmogorov-Arnold networks with spline parameterization, compared against classical baselines (LASSO, Random Forest, Mutual Information, SVM-RFE) across multiple classification and regression tabular datasets.", "result": "KAN-based selectors are competitive with and sometimes superior to classical baselines, with KAN-L2 and KAN-SI performing well on noisy regression and heterogeneous datasets, while KAN-L1, KAN-KO, and KAN-SI excel in high-dimensional multi-class classification by eliminating redundancy.", "conclusion": "KAN-based feature selection provides a powerful and interpretable alternative capable of uncovering nonlinear and multivariate feature relevance beyond traditional sparsity or impurity-based measures."}}
{"id": "2509.23805", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23805", "abs": "https://arxiv.org/abs/2509.23805", "authors": ["Arti Rani", "Shweta Singh", "Nihar Ranjan Sahoo", "Gaurav Kumar Nayak"], "title": "Open-DeBias: Toward Mitigating Open-Set Bias in Language Models", "comment": "25 pages, 3 figures, supplementary material included. To be published\n  in EMNLP-2025", "summary": "Large Language Models (LLMs) have achieved remarkable success on question\nanswering (QA) tasks, yet they often encode harmful biases that compromise\nfairness and trustworthiness. Most existing bias mitigation approaches are\nrestricted to predefined categories, limiting their ability to address novel or\ncontext-specific emergent biases. To bridge this gap, we tackle the novel\nproblem of open-set bias detection and mitigation in text-based QA. We\nintroduce OpenBiasBench, a comprehensive benchmark designed to evaluate biases\nacross a wide range of categories and subgroups, encompassing both known and\npreviously unseen biases. Additionally, we propose Open-DeBias, a novel,\ndata-efficient, and parameter-efficient debiasing method that leverages adapter\nmodules to mitigate existing social and stereotypical biases while generalizing\nto unseen ones. Compared to the state-of-the-art BMBI method, Open-DeBias\nimproves QA accuracy on BBQ dataset by nearly $48\\%$ on ambiguous subsets and\n$6\\%$ on disambiguated ones, using adapters fine-tuned on just a small fraction\nof the training data. Remarkably, the same adapters, in a zero-shot transfer to\nKorean BBQ, achieve $84\\%$ accuracy, demonstrating robust language-agnostic\ngeneralization. Through extensive evaluation, we also validate the\neffectiveness of Open-DeBias across a broad range of NLP tasks, including\nStereoSet and CrowS-Pairs, highlighting its robustness, multilingual strength,\nand suitability for general-purpose, open-domain bias mitigation. The project\npage is available at: https://sites.google.com/view/open-debias25", "AI": {"tldr": "Open-DeBias is a novel debiasing method using adapter modules to mitigate social and stereotypical biases in LLMs, achieving significant improvements in QA accuracy and demonstrating robust multilingual generalization.", "motivation": "LLMs often encode harmful biases that compromise fairness, but existing bias mitigation approaches are limited to predefined categories and cannot address novel or context-specific emergent biases.", "method": "Proposed Open-DeBias, a data-efficient and parameter-efficient debiasing method that leverages adapter modules to mitigate existing social and stereotypical biases while generalizing to unseen ones.", "result": "Open-DeBias improves QA accuracy on BBQ dataset by nearly 48% on ambiguous subsets and 6% on disambiguated ones. In zero-shot transfer to Korean BBQ, achieves 84% accuracy, demonstrating robust language-agnostic generalization.", "conclusion": "Open-DeBias shows effectiveness across broad NLP tasks, highlighting its robustness, multilingual strength, and suitability for general-purpose, open-domain bias mitigation."}}
{"id": "2509.24260", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24260", "abs": "https://arxiv.org/abs/2509.24260", "authors": ["Yuwei Hu", "Xinyi Huang", "Zhewei Wei", "Yongchao Liu", "Chuntao Hong"], "title": "Rethinking and Benchmarking Large Language Models for Graph Reasoning", "comment": null, "summary": "Large Language Models (LLMs) for Graph Reasoning have been extensively\nstudied over the past two years, involving enabling LLMs to understand graph\nstructures and reason on graphs to solve various graph problems, with graph\nalgorithm problems being the most prevalent. Recent studies underscore the\npotential of LLMs in handling graph reasoning tasks, but their performance is\nunderwhelming. In this work, we point out issues with existing methods and\nbenchmarks, and rethink the direction that LLMs for graph reasoning should\nstrive toward. We find that base models, e.g., GPT-4o-mini, are largely\nunderestimated due to improper reasoning focus. Base models with reasoning\nfocus redirected from replicating graph algorithms to designing them can easily\nsolve most graph reasoning tasks in existing benchmarks. To truly evaluate the\ngraph reasoning capabilities of LLMs, we construct a more challenging\nGraphAlgorithm benchmark, comprising 239 different graph problems and 3,041\ntest instances collected from 4 competition platforms. Finally, we introduce a\nsimple and strong baseline Simple-Reasoning-Then-Coding (Simple-RTC)-which\nguides LLMs to design graph algorithms first and then code to address graph\nreasoning tasks. Simple-RTC achieves near-perfect accuracy on existing\nbenchmarks and significantly outperforms GPT-4o-mini and all prior methods on\nthe GraphAlgorithm benchmark. This strong baseline encourages further\nadvancements in LLMs for Graph Reasoning in the future.", "AI": {"tldr": "LLMs for graph reasoning are underperforming due to improper focus on replicating rather than designing graph algorithms. A new benchmark GraphAlgorithm and method Simple-RTC achieve much better performance.", "motivation": "Existing LLM methods for graph reasoning have underwhelming performance due to improper reasoning focus on replicating graph algorithms instead of designing them.", "method": "Proposed Simple-RTC (Simple-Reasoning-Then-Coding) method that guides LLMs to first design graph algorithms and then code to solve graph reasoning tasks.", "result": "Simple-RTC achieves near-perfect accuracy on existing benchmarks and significantly outperforms GPT-4o-mini and all prior methods on the new GraphAlgorithm benchmark.", "conclusion": "Redirecting reasoning focus from algorithm replication to algorithm design enables LLMs to solve most graph reasoning tasks, and the proposed Simple-RTC provides a strong baseline for future research."}}
{"id": "2509.23373", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23373", "abs": "https://arxiv.org/abs/2509.23373", "authors": ["Xi Ding", "Lei Wang", "Piotr Koniusz", "Yongsheng Gao"], "title": "Graph Your Own Prompt", "comment": "Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "We propose Graph Consistency Regularization (GCR), a novel framework that\ninjects relational graph structures, derived from model predictions, into the\nlearning process to promote class-aware, semantically meaningful feature\nrepresentations. Functioning as a form of self-prompting, GCR enables the model\nto refine its internal structure using its own outputs. While deep networks\nlearn rich representations, these often capture noisy inter-class similarities\nthat contradict the model's predicted semantics. GCR addresses this issue by\nintroducing parameter-free Graph Consistency Layers (GCLs) at arbitrary depths.\nEach GCL builds a batch-level feature similarity graph and aligns it with a\nglobal, class-aware masked prediction graph, derived by modulating softmax\nprediction similarities with intra-class indicators. This alignment enforces\nthat feature-level relationships reflect class-consistent prediction behavior,\nacting as a semantic regularizer throughout the network. Unlike prior work, GCR\nintroduces a multi-layer, cross-space graph alignment mechanism with adaptive\nweighting, where layer importance is learned from graph discrepancy magnitudes.\nThis allows the model to prioritize semantically reliable layers and suppress\nnoisy ones, enhancing feature quality without modifying the architecture or\ntraining procedure. GCR is model-agnostic, lightweight, and improves semantic\nstructure across various networks and datasets. Experiments show that GCR\npromotes cleaner feature structure, stronger intra-class cohesion, and improved\ngeneralization, offering a new perspective on learning from prediction\nstructure. [Project website](https://darcyddx.github.io/gcr/)\n[Code](https://github.com/Darcyddx/graph-prompt)", "AI": {"tldr": "GCR is a framework that uses model predictions to create relational graphs for regularizing feature representations, improving semantic structure and generalization without architecture changes.", "motivation": "Deep networks learn noisy inter-class similarities that contradict predicted semantics, so GCR aims to enforce class-consistent feature relationships throughout the network.", "method": "Introduces parameter-free Graph Consistency Layers (GCLs) that align batch-level feature similarity graphs with global class-aware prediction graphs using adaptive weighting based on graph discrepancies.", "result": "GCR promotes cleaner feature structure, stronger intra-class cohesion, and improved generalization across various networks and datasets.", "conclusion": "GCR offers a model-agnostic, lightweight approach to enhance semantic structure by learning from prediction structure through multi-layer graph alignment."}}
{"id": "2509.23863", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23863", "abs": "https://arxiv.org/abs/2509.23863", "authors": ["Ziyi Yang", "Weizhou Shen", "Ruijun Chen", "Chenliang Li", "Fanqi Wan", "Ming Yan", "Xiaojun Quan", "Fei Huang"], "title": "SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models", "comment": "Preprint under review", "summary": "Progress in long-context reasoning for large language models (LLMs) has\nlagged behind other recent advances. This gap arises not only from the\nintrinsic difficulty of processing long texts, but also from the scarcity of\nreliable human annotations and programmatically verifiable reward signals. In\nthis paper, we propose SPELL, a multi-role self-play reinforcement learning\nframework that enables scalable, label-free optimization for long-context\nreasoning. SPELL integrates three cyclical roles-questioner, responder, and\nverifier-within a single model to enable continual self-improvement. The\nquestioner generates questions from raw documents paired with reference\nanswers; the responder learns to solve these questions based on the documents;\nand the verifier evaluates semantic equivalence between the responder's output\nand the questioner's reference answer, producing reward signals to guide\ncontinual training. To stabilize training, we introduce an automated curriculum\nthat gradually increases document length and a reward function that adapts\nquestion difficulty to the model's evolving capabilities. Extensive experiments\non six long-context benchmarks show that SPELL consistently improves\nperformance across diverse LLMs and outperforms equally sized models fine-tuned\non large-scale annotated data. Notably, SPELL achieves an average 7.6-point\ngain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising\nits performance ceiling and showing promise for scaling to even more capable\nmodels.", "AI": {"tldr": "SPELL is a multi-role self-play RL framework that enables scalable, label-free optimization for long-context reasoning in LLMs through questioner-responder-verifier roles and automated curriculum learning.", "motivation": "Progress in long-context reasoning for LLMs lags due to intrinsic difficulty of processing long texts and scarcity of reliable human annotations and verifiable reward signals.", "method": "Multi-role self-play RL with three cyclical roles (questioner, responder, verifier) within single model; automated curriculum gradually increasing document length; adaptive reward function matching question difficulty to model capabilities.", "result": "Consistent performance improvements across 6 long-context benchmarks; outperforms equally sized models fine-tuned on large-scale annotated data; achieves average 7.6-point gain in pass@8 on Qwen3-30B-A3B-Thinking.", "conclusion": "SPELL effectively improves long-context reasoning capabilities, raises performance ceiling of strong models, and shows promise for scaling to more capable models through label-free self-improvement."}}
{"id": "2509.24261", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24261", "abs": "https://arxiv.org/abs/2509.24261", "authors": ["Yuhua Jiang", "Jiawei Huang", "Yufeng Yuan", "Xin Mao", "Yu Yue", "Qianchuan Zhao", "Lin Yan"], "title": "Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective\nfor enhancing Large Language Models (LLMs) on complex reasoning tasks. However,\nexisting methods suffer from an exploration dilemma: the sharply peaked initial\npolicies of pre-trained LLMs confine standard RL algorithms to a narrow set of\nsolutions, boosting single-solution accuracy (pass@1) but suppressing solution\ndiversity and multi-solution performance (pass@k). As a result, RLVR often\ndistills existing capabilities rather than discovering new reasoning\nstrategies. To overcome this, we introduce a Risk-Sensitive Reinforcement\nLearning framework. Our approach employs a risk-seeking objective that\ninterpolates between mean and maximum rewards, leading to a novel algorithm,\nRisk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying\nlearning from challenging prompts. Remarkably, RS-GRPO is simple to implement,\nrequiring only minor code modifications. On six mathematical reasoning\nbenchmarks and with five different LLMs, RS-GRPO consistently improves pass@k\nperformance while maintaining or enhancing pass@1 accuracy.", "AI": {"tldr": "RLVR enhances LLMs on reasoning tasks but suffers from exploration limitations due to peaked initial policies, suppressing solution diversity. Risk-Sensitive RL framework with RS-GRPO algorithm addresses this by amplifying learning from challenging prompts, improving pass@k performance while maintaining pass@1 accuracy.", "motivation": "Existing RLVR methods face exploration dilemma where pre-trained LLMs' peaked initial policies limit solution diversity and multi-solution performance (pass@k), causing RL to distill existing capabilities rather than discover new reasoning strategies.", "method": "Proposed Risk-Sensitive Reinforcement Learning framework with risk-seeking objective that interpolates between mean and maximum rewards, leading to RS-GRPO algorithm that drives deeper exploration by amplifying learning from challenging prompts.", "result": "On six mathematical reasoning benchmarks with five different LLMs, RS-GRPO consistently improves pass@k performance while maintaining or enhancing pass@1 accuracy.", "conclusion": "Risk-sensitive RL framework effectively addresses exploration limitations in RLVR, enabling discovery of new reasoning strategies while maintaining single-solution performance through simple implementation requiring only minor code modifications."}}
{"id": "2509.23405", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23405", "abs": "https://arxiv.org/abs/2509.23405", "authors": ["Fred Zhangzhi Peng", "Zachary Bezemek", "Jarrid Rector-Brooks", "Shuibai Zhang", "Anru R. Zhang", "Michael Bronstein", "Avishek Joey Bose", "Alexander Tong"], "title": "Planner Aware Path Learning in Diffusion Language Models Training", "comment": null, "summary": "Diffusion language models have emerged as a powerful alternative to\nautoregressive models, enabling fast inference through flexible and parallel\ngeneration paths. This flexibility is enabled by new sampling strategies, or\nplanners, that iteratively choose where to denoise along the sequence rather\nthan sampling uniformly at random. However, by modifying reverse paths,\nplanners introduce a mismatch between the uniformly random denoising paths used\nduring training and the planning-based paths used at inference. In this work,\nwe systematically investigate this mismatch and theoretically show that the\nstandard discrete diffusion training evidence lower bound (ELBO) does not\naccurately describe a denoiser under non-uniform planning. To bridge this gap,\nwe derive a new Planned Evidence Lower Bound (P-ELBO) that directly\nincorporates planner-based reverse dynamics into the training objective.\nBuilding on this, we propose Planner Aware Path Learning (PAPL), a simple and\neffective modification of the standard masked discrete diffusion loss that\naligns training and inference under planned denoisers. Empirically, PAPL\ndelivers consistent improvements across domains, including a 40% relative gain\nin protein sequence modeling, up to a 4x improvement in MAUVE for text\ngeneration, and a 23% relative gain in HumanEval pass@10 for code generation.", "AI": {"tldr": "PAPL addresses the training-inference mismatch in diffusion language models by incorporating planner-based reverse dynamics into training, improving performance across protein, text, and code generation tasks.", "motivation": "Planners in diffusion models create a mismatch between training (uniform denoising) and inference (planned denoising), which the standard ELBO doesn't account for.", "method": "Derived Planned ELBO (P-ELBO) and proposed Planner Aware Path Learning (PAPL) to align training with planned inference paths.", "result": "40% relative gain in protein modeling, 4x MAUVE improvement in text generation, 23% relative gain in HumanEval pass@10 for code generation.", "conclusion": "PAPL effectively bridges the training-inference gap in planned diffusion models, delivering consistent performance improvements across multiple domains."}}
{"id": "2509.23873", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23873", "abs": "https://arxiv.org/abs/2509.23873", "authors": ["Shaobo Wang", "Jiaming Wang", "Jiajun Zhang", "Cong Wang", "Yue Min", "Zichen Wen", "Fei Huang", "Huiqiang Jiang", "Junyang Lin", "Dayiheng Liu", "Linfeng Zhang"], "title": "Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning", "comment": "19 pages, 6 figures", "summary": "As supervised fine-tuning (SFT) evolves from a lightweight post-training step\ninto a compute-intensive phase rivaling mid-training in scale, data efficiency\nhas become critical for aligning large language models (LLMs) under tight\nbudgets. Existing data pruning methods suffer from a fragmented design: they\noperate either at the sample level or the token level in isolation, failing to\njointly optimize both dimensions. This disconnect leads to significant\ninefficiencies--high-value samples may still contain redundant tokens, while\ntoken-level pruning often discards crucial instructional or corrective signals\nembedded in individual examples. To address this bottleneck, we introduce the\nError-Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes\nthe heterogeneous utility of training data across samples and tokens. Guided by\nthis insight, we propose Quadrant-based Tuning (Q-Tuning), a unified framework\nthat strategically coordinates sample pruning and token pruning. Q-Tuning\nemploys a two-stage strategy: first, it performs sample-level triage to retain\nexamples rich in informative misconceptions or calibration signals; second, it\napplies an asymmetric token-pruning policy, using a context-aware scoring\nmechanism to trim less salient tokens exclusively from misconception samples\nwhile preserving calibration samples in their entirety. Our method sets a new\nstate of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B,\nQ-Tuning achieves a +38\\% average improvement over the full-data SFT baseline\nusing only 12.5\\% of the original training data. As the first dynamic pruning\napproach to consistently outperform full-data training, Q-Tuning provides a\npractical and scalable blueprint for maximizing data utilization in\nbudget-constrained LLM SFT.", "AI": {"tldr": "Q-Tuning is a unified framework that jointly optimizes sample and token pruning for efficient supervised fine-tuning of LLMs, achieving state-of-the-art performance with only 12.5% of training data.", "motivation": "Existing data pruning methods operate in isolation at either sample or token level, leading to inefficiencies where high-value samples may contain redundant tokens and token-level pruning discards important instructional signals.", "method": "Proposes Error-Uncertainty (EU) Plane diagnostic framework and Quadrant-based Tuning (Q-Tuning) with two-stage strategy: sample-level triage to retain informative examples, followed by asymmetric token-pruning that trims less salient tokens only from misconception samples while preserving calibration samples entirely.", "result": "Sets new state-of-the-art across five benchmarks. On SmolLM2-1.7B, achieves +38% average improvement over full-data SFT baseline using only 12.5% of original training data.", "conclusion": "Q-Tuning is the first dynamic pruning approach to consistently outperform full-data training, providing a practical and scalable blueprint for maximizing data utilization in budget-constrained LLM SFT."}}
{"id": "2509.24263", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24263", "abs": "https://arxiv.org/abs/2509.24263", "authors": ["Junjie Luo", "Yihong Guo", "Anqi Liu", "Ritu Agarwal", "Gordon", "Gao"], "title": "PAME-AI: Patient Messaging Creation and Optimization using Agentic AI", "comment": null, "summary": "Messaging patients is a critical part of healthcare communication, helping to\nimprove things like medication adherence and healthy behaviors. However,\ntraditional mobile message design has significant limitations due to its\ninability to explore the high-dimensional design space. We develop PAME-AI, a\nnovel approach for Patient Messaging Creation and Optimization using Agentic\nAI. Built on the Data-Information-Knowledge-Wisdom (DIKW) hierarchy, PAME-AI\noffers a structured framework to move from raw data to actionable insights for\nhigh-performance messaging design. PAME-AI is composed of a system of\nspecialized computational agents that progressively transform raw experimental\ndata into actionable message design strategies. We demonstrate our approach's\neffectiveness through a two-stage experiment, comprising of 444,691 patient\nencounters in Stage 1 and 74,908 in Stage 2. The best-performing generated\nmessage achieved 68.76% engagement compared to the 61.27% baseline,\nrepresenting a 12.2\\% relative improvement in click-through rates. This agentic\narchitecture enables parallel processing, hypothesis validation, and continuous\nlearning, making it particularly suitable for large-scale healthcare\ncommunication optimization.", "AI": {"tldr": "PAME-AI is an agentic AI system that optimizes patient messaging design, achieving 12.2% improvement in engagement rates through structured data transformation and parallel processing.", "motivation": "Traditional mobile message design has limitations in exploring high-dimensional design spaces, and there's a need for more effective healthcare communication to improve medication adherence and healthy behaviors.", "method": "Built on DIKW hierarchy, PAME-AI uses specialized computational agents to transform raw experimental data into actionable message design strategies through a two-stage experiment with over 500,000 patient encounters.", "result": "The best-performing generated message achieved 68.76% engagement vs 61.27% baseline, representing 12.2% relative improvement in click-through rates.", "conclusion": "PAME-AI's agentic architecture enables effective large-scale healthcare communication optimization through parallel processing, hypothesis validation, and continuous learning."}}
{"id": "2509.23409", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23409", "abs": "https://arxiv.org/abs/2509.23409", "authors": ["Devesh Sharma", "Aditya Kishore", "Ayush Garg", "Debajyoti Mazumder", "Debasis Mohapatra", "Jasabanta Patro"], "title": "Mind the Links: Cross-Layer Attention for Link Prediction in Multiplex Networks", "comment": null, "summary": "Multiplex graphs capture diverse relations among shared nodes. Most\npredictors either collapse layers or treat them independently. This loses\ncrucial inter-layer dependencies and struggles with scalability. To overcome\nthis, we frame multiplex link prediction as multi-view edge classification. For\neach node pair, we construct a sequence of per-layer edge views and apply\ncross-layer self-attention to fuse evidence for the target layer. We present\ntwo models as instances of this framework: Trans-SLE, a lightweight transformer\nover static embeddings, and Trans-GAT, which combines layer-specific GAT\nencoders with transformer fusion. To ensure scalability and fairness, we\nintroduce a Union--Set candidate pool and two leakage-free protocols:\ncross-layer and inductive subgraph generalization. Experiments on six public\nmultiplex datasets show consistent macro-F_1 gains over strong baselines (MELL,\nHOPLP-MUL, RMNE). Our approach is simple, scalable, and compatible with both\nprecomputed embeddings and GNN encoders.", "AI": {"tldr": "The paper proposes a novel approach for multiplex link prediction by framing it as multi-view edge classification, using cross-layer self-attention to fuse evidence across layers, with two scalable models (Trans-SLE and Trans-GAT) that outperform existing methods.", "motivation": "Existing multiplex graph predictors either collapse layers or treat them independently, losing crucial inter-layer dependencies and struggling with scalability.", "method": "Frame multiplex link prediction as multi-view edge classification, construct sequences of per-layer edge views, apply cross-layer self-attention for fusion, and introduce two models (Trans-SLE with static embeddings, Trans-GAT with GAT encoders) with Union-Set candidate pool and leakage-free protocols.", "result": "Experiments on six public multiplex datasets show consistent macro-F1 gains over strong baselines (MELL, HOPLP-MUL, RMNE).", "conclusion": "The approach is simple, scalable, and compatible with both precomputed embeddings and GNN encoders, effectively capturing inter-layer dependencies in multiplex graphs."}}
{"id": "2509.23883", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.23883", "abs": "https://arxiv.org/abs/2509.23883", "authors": ["Yibo Yan", "Guangwei Xu", "Xin Zou", "Shuliang Liu", "James Kwok", "Xuming Hu"], "title": "DocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning", "comment": "Under review", "summary": "Visual Document Retrieval (VDR), the task of retrieving visually-rich\ndocument pages using queries that combine visual and textual cues, is crucial\nfor numerous real-world applications. Recent state-of-the-art methods leverage\nLarge Vision-Language Models (LVLMs) in a multi-vector paradigm, representing\neach document as patch-level embeddings to capture fine-grained details. While\nhighly effective, this approach introduces a critical challenge: prohibitive\nstorage overhead, as storing hundreds of vectors per page makes large-scale\ndeployment costly and impractical. To address this, we introduce DocPruner, the\nfirst framework to employ adaptive patch-level embedding pruning for VDR to\neffectively reduce the storage overhead. DocPruner leverages the intra-document\npatch attention distribution to dynamically identify and discard redundant\nembeddings for each document. This adaptive mechanism enables a significant\n50-60% reduction in storage for leading multi-vector VDR models with negligible\ndegradation in document retrieval performance. Extensive experiments across\nmore than ten representative datasets validate that DocPruner offers a robust,\nflexible, and effective solution for building storage-efficient, large-scale\nVDR systems.", "AI": {"tldr": "DocPruner introduces adaptive patch-level embedding pruning for Visual Document Retrieval (VDR) to reduce storage overhead by 50-60% while maintaining retrieval performance.", "motivation": "Current multi-vector VDR methods using LVLMs require storing hundreds of vectors per document page, creating prohibitive storage costs that make large-scale deployment impractical.", "method": "DocPruner leverages intra-document patch attention distribution to dynamically identify and discard redundant embeddings for each document, enabling adaptive pruning.", "result": "Extensive experiments across 10+ datasets show 50-60% storage reduction with negligible performance degradation in document retrieval.", "conclusion": "DocPruner provides a robust, flexible, and effective solution for building storage-efficient, large-scale VDR systems."}}
{"id": "2509.24269", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24269", "abs": "https://arxiv.org/abs/2509.24269", "authors": ["Zihao Zhu", "Xinyu Wu", "Gehan Hu", "Siwei Lyu", "Ke Xu", "Baoyuan Wu"], "title": "AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety Alignment of Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in\ncomplex problem-solving through Chain-of-Thought (CoT) reasoning. However, the\nmulti-step nature of CoT introduces new safety challenges that extend beyond\nconventional language model alignment. We identify a failure mode in current\nsafety CoT tuning methods: the \\textit{snowball effect}, where minor reasoning\ndeviations progressively amplify throughout the thought process, leading to\neither harmful compliance or excessive refusal. This effect stems from models\nbeing trained to imitate perfect reasoning scripts without learning to\nself-correct. To address this limitation, we propose AdvChain, an alignment\nparadigm that teaches models dynamic self-correction through adversarial CoT\ntuning. Our method involves constructing a dataset containing\nTemptation-Correction and Hesitation-Correction samples, where models learn to\nrecover from harmful reasoning drifts and unnecessary cautions. Extensive\nexperiments show that AdvChain significantly enhances robustness against\njailbreak attacks and CoT hijacking while substantially reducing over-refusal\non benign prompts, achieving a superior safety-utility balance without\ncompromising reasoning capabilities. Our work establishes a new direction for\nbuilding more robust and reliable reasoning models.", "AI": {"tldr": "AdvChain is an alignment method that teaches Large Reasoning Models dynamic self-correction through adversarial CoT tuning to address the snowball effect in reasoning safety.", "motivation": "Current safety CoT tuning methods suffer from the snowball effect where minor reasoning deviations amplify throughout the thought process, leading to harmful compliance or excessive refusal.", "method": "AdvChain constructs a dataset with Temptation-Correction and Hesitation-Correction samples to teach models dynamic self-correction through adversarial CoT tuning.", "result": "AdvChain significantly enhances robustness against jailbreak attacks and CoT hijacking while reducing over-refusal on benign prompts, achieving superior safety-utility balance.", "conclusion": "The work establishes a new direction for building more robust and reliable reasoning models through dynamic self-correction capabilities."}}
{"id": "2509.23410", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.23410", "abs": "https://arxiv.org/abs/2509.23410", "authors": ["Younes Hourri", "Mohammad Mozaffari", "Maryam Mehri Dehnavi"], "title": "PATCH: Learnable Tile-level Hybrid Sparsity for LLMs", "comment": null, "summary": "Large language models (LLMs) deliver impressive performance but incur\nprohibitive memory and compute costs at deployment. Model pruning is an\neffective way to reduce these overheads, yet existing approaches face\nchallenges: unstructured sparsity, where nonzeros can appear anywhere,\npreserves accuracy but yields irregular access patterns that prevent GPU\nacceleration, while semi-structured 2:4 sparsity is hardware-friendly but\nenforces a rigid 50% pattern that degrades model quality. To bridge this gap,\nwe introduce PATCH, a hybrid sparsity framework that enables a continuous\nsparsity ratio between 0% and 50%. PATCH partitions weight matrices into tiles,\nassigning each tile to be either dense or 2:4 sparse via a learnable mask\nselection mechanism. This design provides fine-grained control over\naccuracy-acceleration tradeoffs and supports non-uniform sparsity across\nlayers, leading to superior overall quality. Across models from 0.5B to 8B\nparameters, PATCH consistently narrows the gap to dense accuracy while\ndelivering practical speedups. For instance, on LLaMA-2 7B with an A6000 GPU,\nPATCH achieves 1.18x-1.38x end-to-end speedup over dense baselines while\nimproving accuracy by 0.37%-2.96% compared to the state-of-the-art 2:4 pruning\nmethod, MaskLLM.", "AI": {"tldr": "PATCH is a hybrid sparsity framework that enables continuous sparsity ratios between 0% and 50% by partitioning weight matrices into tiles and assigning each tile to be either dense or 2:4 sparse via learnable masks, achieving better accuracy-acceleration tradeoffs than existing pruning methods.", "motivation": "Existing pruning methods face challenges: unstructured sparsity preserves accuracy but prevents GPU acceleration due to irregular access patterns, while semi-structured 2:4 sparsity is hardware-friendly but enforces a rigid 50% pattern that degrades model quality. There's a need to bridge this gap.", "method": "PATCH partitions weight matrices into tiles and uses a learnable mask selection mechanism to assign each tile as either dense or 2:4 sparse. This provides fine-grained control over sparsity ratios and supports non-uniform sparsity across layers.", "result": "Across models from 0.5B to 8B parameters, PATCH consistently narrows the gap to dense accuracy while delivering practical speedups. On LLaMA-2 7B with A6000 GPU, it achieves 1.18x-1.38x end-to-end speedup over dense baselines while improving accuracy by 0.37%-2.96% compared to state-of-the-art 2:4 pruning method MaskLLM.", "conclusion": "PATCH successfully bridges the gap between unstructured and structured sparsity by enabling continuous sparsity control, delivering both improved accuracy and practical acceleration across various model sizes."}}
{"id": "2509.23924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23924", "abs": "https://arxiv.org/abs/2509.23924", "authors": ["Jingyi Yang", "Guanxu Chen", "Xuhao Hu", "Jing Shao"], "title": "Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step", "comment": "10 pages, 4 figures, 7 tables. Code:\n  https://github.com/yjyddq/EOSER-ASS-RL", "summary": "Masked diffusion language models (MDLMs) have recently emerged as a promising\nalternative to autoregressive (AR) language models, offering properties such as\nparallel decoding, flexible generation orders, and the potential for fewer\ninference steps. Despite these advantages, decoding strategies and\nreinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.\nA naive approach is to directly transfer techniques well-established for AR\nmodels to MDLMs. However, this raises an immediate question: Is such a naive\ntransfer truly optimal? For example, 1) Block-wise and semi-AR decoding\nstrategies are not employed during the training of MDLMs, so why do they\noutperform full diffusion-style decoding during inference? 2) Applying RL\nalgorithms designed for AR models directly to MDLMs exhibits a\ntraining-inference inconsistency, since MDLM decoding are non-causal\n(parallel). This results in inconsistencies between the rollout trajectory and\nthe optimization trajectory. To address these challenges, we propose EOS Early\nRejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which\nunlock the potential of MDLMs to perform full diffusion-style decoding,\nachieving competitive performance with fewer decoding steps. Additionally, we\nintroduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)\nfor taming MDLMs, which emphasizes the consistency between rollout trajectory\nand optimization trajectory, and reduces the optimization errors caused by\nskip-step optimization. We conduct extensive experiments on reasoning tasks,\nsuch as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The\nresults demonstrate that the proposed EOSER and ASS mechanisms, together with\nCJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.\nCode: https://github.com/yjyddq/EOSER-ASS-RL.", "AI": {"tldr": "This paper addresses challenges in masked diffusion language models (MDLMs) by proposing EOS Early Rejection and Ascending Step-Size decoding strategies, along with Consistency Trajectory Group Relative Policy Optimization for reinforcement learning, achieving competitive performance with fewer decoding steps.", "motivation": "MDLMs offer advantages like parallel decoding and flexible generation but lack tailored decoding strategies and RL algorithms. Direct transfer of AR model techniques to MDLMs is suboptimal due to training-inference inconsistencies and non-causal decoding.", "method": "Proposed EOSER and ASS decoding scheduler for full diffusion-style decoding, and CJ-GRPO RL algorithm that ensures consistency between rollout and optimization trajectories while reducing skip-step optimization errors.", "result": "Experiments on reasoning tasks using LLaDA-8B-Instruct show the proposed methods achieve competitive performance with fewer decoding steps, effectively taming MDLMs.", "conclusion": "The EOSER and ASS mechanisms combined with CJ-GRPO provide promising solutions for efficient and effective MDLM training and inference, addressing key challenges in non-causal parallel decoding."}}
{"id": "2509.24276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24276", "abs": "https://arxiv.org/abs/2509.24276", "authors": ["Linhao Luo", "Zicheng Zhao", "Junnan Liu", "Zhangchi Qiu", "Junnan Dong", "Serge Panev", "Chen Gong", "Thuy-Trang Vu", "Gholamreza Haffari", "Dinh Phung", "Alan Wee-Chung Liew", "Shirui Pan"], "title": "G-reasoner: Foundation Models for Unified Reasoning over Graph-structured Knowledge", "comment": "22 pages, 6 figures", "summary": "Large language models (LLMs) excel at complex reasoning but remain limited by\nstatic and incomplete parametric knowledge. Retrieval-augmented generation\n(RAG) mitigates this by incorporating external knowledge, yet existing RAGs\nstruggle with knowledge-intensive tasks due to fragmented information and weak\nmodeling of knowledge structure. Graphs offer a natural way to model\nrelationships within knowledge, but LLMs are inherently unstructured and cannot\neffectively reason over graph-structured data. Recent graph-enhanced RAG\n(GraphRAG) attempts to bridge this gap by constructing tailored graphs and\nenabling LLMs to reason on them. However, these methods often depend on ad-hoc\ngraph designs, heuristic search, or costly agent pipelines, which hinder\nscalability and generalization. To address these challenges, we present\nG-reasoner, a unified framework that integrates graph and language foundation\nmodels for reasoning over diverse graph-structured knowledge. Central to our\napproach is QuadGraph, a standardized four-layer abstraction that unifies\nheterogeneous knowledge sources into a common graph representation. Building on\nthis, we introduce a 34M-parameter graph foundation model (GFM) that jointly\ncaptures graph topology and textual semantics, and is integrated with LLMs to\nenhance reasoning in downstream applications. To ensure scalability and\nefficiency, mixed-precision training and distributed message-passing are\nimplemented to scale GFM with more GPUs. Extensive experiments on six\nbenchmarks show that G-reasoner consistently outperforms state-of-the-art\nbaselines, significantly enhances LLM reasoning, and achieves strong efficiency\nand cross-graph generalization.", "AI": {"tldr": "G-reasoner is a unified framework that integrates graph and language foundation models for reasoning over diverse graph-structured knowledge, addressing limitations in existing retrieval-augmented generation methods.", "motivation": "Existing RAG methods struggle with knowledge-intensive tasks due to fragmented information and weak modeling of knowledge structure. LLMs cannot effectively reason over graph-structured data, and current GraphRAG approaches have scalability and generalization issues.", "method": "Proposes QuadGraph (four-layer abstraction to unify heterogeneous knowledge), a 34M-parameter graph foundation model that captures graph topology and textual semantics, integrated with LLMs. Uses mixed-precision training and distributed message-passing for scalability.", "result": "Extensive experiments on six benchmarks show G-reasoner consistently outperforms state-of-the-art baselines, significantly enhances LLM reasoning, and achieves strong efficiency and cross-graph generalization.", "conclusion": "G-reasoner provides an effective solution for graph-enhanced reasoning, overcoming limitations of existing RAG methods through unified graph-language integration and scalable architecture."}}
{"id": "2509.23413", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23413", "abs": "https://arxiv.org/abs/2509.23413", "authors": ["Changliang Zhou", "Canhong Yu", "Shunyu Yao", "Xi Lin", "Zhenkun Wang", "Yu Zhou", "Qingfu Zhang"], "title": "URS: A Unified Neural Routing Solver for Cross-Problem Zero-Shot Generalization", "comment": "31 pages,3 figures", "summary": "Multi-task neural routing solvers have emerged as a promising paradigm for\ntheir ability to solve multiple vehicle routing problems (VRPs) using a single\nmodel. However, existing neural solvers typically rely on predefined problem\nconstraints or require per-problem fine-tuning, which substantially limits\ntheir zero-shot generalization ability to unseen VRP variants. To address this\ncritical bottleneck, we propose URS, a unified neural routing solver capable of\nzero-shot generalization across a wide range of unseen VRPs using a single\nmodel without any fine-tuning. The key component of URS is the unified data\nrepresentation (UDR), which replaces problem enumeration with data unification,\nthereby broadening the problem coverage and reducing reliance on domain\nexpertise. In addition, we propose a Mixed Bias Module (MBM) to efficiently\nlearn the geometric and relational biases inherent in various problems. On top\nof the proposed UDR, we further develop a parameter generator that adaptively\nadjusts the decoder and bias weights of MBM to enhance zero-shot\ngeneralization. Moreover, we propose an LLM-driven constraint satisfaction\nmechanism, which translates raw problem descriptions into executable stepwise\nmasking functions to ensure solution feasibility. Extensive experiments\ndemonstrate that URS can consistently produce high-quality solutions for more\nthan 100 distinct VRP variants without any fine-tuning, which includes more\nthan 90 unseen variants. To the best of our knowledge, URS is the first neural\nsolver capable of handling over 100 VRP variants with a single model.", "AI": {"tldr": "URS is a unified neural routing solver that achieves zero-shot generalization across 100+ VRP variants using unified data representation, mixed bias module, parameter generator, and LLM-driven constraint satisfaction.", "motivation": "Existing neural solvers rely on predefined constraints or require fine-tuning, limiting their zero-shot generalization to unseen VRP variants.", "method": "Proposes unified data representation (UDR) for data unification, mixed bias module (MBM) for learning geometric/relational biases, parameter generator for adaptive decoder adjustment, and LLM-driven constraint satisfaction mechanism.", "result": "URS produces high-quality solutions for 100+ distinct VRP variants without fine-tuning, including 90+ unseen variants - first neural solver to handle over 100 VRP variants with single model.", "conclusion": "URS successfully addresses the zero-shot generalization bottleneck in neural routing solvers and demonstrates superior performance across diverse VRP variants."}}
{"id": "2509.23936", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23936", "abs": "https://arxiv.org/abs/2509.23936", "authors": ["Zhangdie Yuan", "Zifeng Ding", "Andreas Vlachos"], "title": "Assessing Large Language Models in Updating Their Forecasts with New Information", "comment": null, "summary": "Prior work has largely treated future event prediction as a static task,\nfailing to consider how forecasts and the confidence in them should evolve as\nnew evidence emerges. To address this gap, we introduce EVOLVECAST, a framework\nfor evaluating whether large language models appropriately revise their\npredictions in response to new information. In particular, EVOLVECAST assesses\nwhether LLMs adjust their forecasts when presented with information released\nafter their training cutoff. We use human forecasters as a comparative\nreference to analyze prediction shifts and confidence calibration under updated\ncontexts. While LLMs demonstrate some responsiveness to new information, their\nupdates are often inconsistent or overly conservative. We further find that\nneither verbalized nor logits-based confidence estimates consistently\noutperform the other, and both remain far from the human reference standard.\nAcross settings, models tend to express conservative bias, underscoring the\nneed for more robust approaches to belief updating.", "AI": {"tldr": "EVOLVECAST framework evaluates LLMs' ability to revise predictions when presented with new post-training information, finding models update inconsistently and conservatively compared to human forecasters.", "motivation": "Address the gap in treating future event prediction as static, without considering how forecasts should evolve as new evidence emerges.", "method": "Introduce EVOLVECAST framework to assess LLM prediction revision using human forecasters as reference, analyzing prediction shifts and confidence calibration under updated contexts.", "result": "LLMs show some responsiveness to new information but updates are inconsistent/overly conservative; neither verbalized nor logits-based confidence estimates consistently outperform each other; both far from human standard; models express conservative bias.", "conclusion": "Need for more robust approaches to belief updating in LLMs, as current models demonstrate conservative bias and inadequate prediction revision capabilities."}}
{"id": "2509.24285", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24285", "abs": "https://arxiv.org/abs/2509.24285", "authors": ["Shenghe Zheng", "Chenyu Huang", "Fangchen Yu", "Junchi Yao", "Jingqi Ye", "Tao Chen", "Yun Luo", "Ning Ding", "LEI BAI", "Ganqu Cui", "Peng Ye"], "title": "SCI-Verifier: Scientific Verifier with Thinking", "comment": "This paper focuses on LLM-as-a-Judge, and the project is currently in\n  progress", "summary": "As large language models (LLMs) are increasingly applied to scientific\nreasoning, the complexity of answer formats and the diversity of equivalent\nexpressions make answer verification a critical yet challenging task. Existing\nverification studies in scientific domains suffer from two major limitations:\n(a) the absence of systematic evaluation standards and insufficient\ndisciplinary coverage, which hinders their comprehensive assessment; and (b)\nheavy reliance on cumbersome rule design or prompt engineering, which reduces\ntheir effectiveness in complex reasoning scenarios or limits their\ncross-disciplinary generalization. To address these challenges, we propose\nsolutions at both the data and model levels. On the data side, we construct\nSCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics,\nbiology, chemistry, and general scientific QA. The benchmark is built from real\nLLM responses and enhanced with domain-specific equivalence transformations\nthat generate challenging and realistic data. Model-based and expert\nannotations ensure both quality and diversity, enabling rigorous evaluation of\nverification ability. On the model side, we emphasize the importance of\nreasoning for verification and introduce SCI-Verifier, a unified\nreasoning-augmented verifier for scientific domains. Through post-training,\nSCI-Verifier demonstrates strong logical reasoning and equivalence judgment\ncapabilities while maintaining concise and stable outputs. Together,\nSCI-VerifyBench and SCI-Verifier provide a principled framework for scientific\nverification, offering both systematic evaluation and practical pathways to\nenhance the reliability and applicability of LLMs in scientific domains.", "AI": {"tldr": "The paper addresses challenges in verifying LLM-generated scientific answers by proposing SCI-VerifyBench, a cross-disciplinary benchmark, and SCI-Verifier, a reasoning-augmented verification model.", "motivation": "Existing verification methods for LLMs in scientific domains lack systematic evaluation standards, have insufficient disciplinary coverage, and rely on cumbersome rule design or prompt engineering, limiting their effectiveness and generalization.", "method": "1) Construct SCI-VerifyBench benchmark covering multiple scientific disciplines with real LLM responses and domain-specific equivalence transformations; 2) Develop SCI-Verifier, a unified reasoning-augmented verifier trained through post-training to enhance logical reasoning and equivalence judgment.", "result": "The proposed framework provides systematic evaluation standards and practical verification capabilities, demonstrating strong logical reasoning and stable outputs for scientific answer verification.", "conclusion": "SCI-VerifyBench and SCI-Verifier together offer a principled framework that enhances the reliability and applicability of LLMs in scientific domains through rigorous evaluation and effective verification methods."}}
{"id": "2509.23436", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23436", "abs": "https://arxiv.org/abs/2509.23436", "authors": ["Ashkan Shahbazi", "Chayne Thrash", "Yikun Bai", "Keaton Hamm", "Navid NaderiAlizadeh", "Soheil Kolouri"], "title": "LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal Transport", "comment": null, "summary": "Transformers have proven highly effective across a wide range of modalities.\nHowever, the quadratic complexity of the standard softmax attention mechanism\nposes a fundamental barrier to scaling them to long context windows. A large\nbody of work addresses this with linear attention, which reformulates attention\nas a kernel function and approximates it with finite feature maps to achieve\nlinear-time computation. Orthogonal to computational scaling, most attention\nmechanisms -- both quadratic and linear -- produce row-normalized maps that can\nover-focus on a few tokens, degrading robustness and information flow.\nEnforcing doubly-stochastic attention alleviates this by balancing token\nparticipation across rows and columns, but existing doubly-stochastic attention\nmechanisms typically introduce substantial overhead, undermining scalability.\nWe propose LOTFormer, a principled attention mechanism that is simultaneously\nlinear-time and doubly-stochastic. Our approach exploits the connection between\nattention maps and transportation plans between query and key measures. The\ncentral idea is to constrain the transport plan to be low-rank by conditioning\nit on a learnable pivot measure with small support. Concretely, we solve two\nentropic optimal transport problems (queries $\\to$ pivot and pivot $\\to$ keys)\nand compose them into a conditional (glued) coupling. This yields an attention\nmatrix that is provably doubly-stochastic, has rank at most $r \\ll n$, and\napplies to values in $O(nr)$ time without forming the full $n \\times n$ map.\nThe pivot locations and masses are learned end-to-end. Empirically, LOTFormer\nachieves state-of-the-art results on the Long Range Arena benchmark, surpassing\nprior linear and transport-based attention methods in both accuracy and\nefficiency.", "AI": {"tldr": "LOTFormer is a linear-time doubly-stochastic attention mechanism that uses entropic optimal transport with a learnable pivot measure to achieve efficient long-context modeling while balancing token participation.", "motivation": "Standard softmax attention has quadratic complexity that limits scaling to long contexts, and most attention mechanisms (including linear ones) produce row-normalized maps that can over-focus on few tokens, degrading robustness. Doubly-stochastic attention helps but existing methods introduce substantial overhead.", "method": "The approach exploits the connection between attention maps and transportation plans. It constrains the transport plan to be low-rank by conditioning on a learnable pivot measure with small support, solving two entropic optimal transport problems (queries\u2192pivot and pivot\u2192keys) and composing them into a conditional glued coupling.", "result": "LOTFormer achieves state-of-the-art results on the Long Range Arena benchmark, surpassing prior linear and transport-based attention methods in both accuracy and efficiency.", "conclusion": "LOTFormer provides a principled attention mechanism that is simultaneously linear-time and doubly-stochastic, enabling efficient long-context modeling with balanced token participation."}}
{"id": "2509.23938", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23938", "abs": "https://arxiv.org/abs/2509.23938", "authors": ["Guojian Li", "Chengyou Wang", "Hongfei Xue", "Shuiyuan Wang", "Dehui Gao", "Zihan Zhang", "Yuke Lin", "Wenjie Li", "Longshuai Xiao", "Zhonghua Fu", "Lei Xie"], "title": "Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems", "comment": null, "summary": "Full-duplex interaction is crucial for natural human-machine communication,\nyet remains challenging as it requires robust turn-taking detection to decide\nwhen the system should speak, listen, or remain silent. Existing solutions\neither rely on dedicated turn-taking models, most of which are not\nopen-sourced. The few available ones are limited by their large parameter size\nor by supporting only a single modality, such as acoustic or linguistic.\nAlternatively, some approaches finetune LLM backbones to enable full-duplex\ncapability, but this requires large amounts of full-duplex data, which remain\nscarce in open-source form. To address these issues, we propose Easy Turn, an\nopen-source, modular turn-taking detection model that integrates acoustic and\nlinguistic bimodal information to predict four dialogue turn states: complete,\nincomplete, backchannel, and wait, accompanied by the release of Easy Turn\ntrainset, a 1,145-hour speech dataset designed for training turn-taking\ndetection models. Compared to existing open-source models like TEN Turn\nDetection and Smart Turn V2, our model achieves state-of-the-art turn-taking\ndetection accuracy on our open-source Easy Turn testset. The data and model\nwill be made publicly available on GitHub.", "AI": {"tldr": "Easy Turn is an open-source, bimodal turn-taking detection model that integrates acoustic and linguistic information to predict four dialogue states, achieving SOTA performance with a 1,145-hour training dataset.", "motivation": "Full-duplex human-machine communication requires robust turn-taking detection, but existing solutions are limited by being closed-source, large in parameter size, single-modal, or requiring scarce full-duplex data.", "method": "Proposed Easy Turn - a modular turn-taking detection model that integrates acoustic and linguistic bimodal information to predict four dialogue turn states: complete, incomplete, backchannel, and wait, along with releasing Easy Turn trainset.", "result": "Achieves state-of-the-art turn-taking detection accuracy on the open-source Easy Turn testset compared to existing models like TEN Turn Detection and Smart Turn V2.", "conclusion": "Easy Turn provides an effective open-source solution for full-duplex interaction with bimodal turn-taking detection, accompanied by a large training dataset to address data scarcity issues."}}
{"id": "2509.24303", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.24303", "abs": "https://arxiv.org/abs/2509.24303", "authors": ["Huatao Xu", "Yan Zhang", "Wei Gao", "Guobin Shen", "Mo Li"], "title": "Experience Paper: Adopting Activity Recognition in On-demand Food Delivery Business", "comment": "13 pages", "summary": "This paper presents the first nationwide deployment of human activity\nrecognition (HAR) technology in the on-demand food delivery industry. We\nsuccessfully adapted the state-of-the-art LIMU-BERT foundation model to the\ndelivery platform. Spanning three phases over two years, the deployment\nprogresses from a feasibility study in Yangzhou City to nationwide adoption\ninvolving 500,000 couriers across 367 cities in China. The adoption enables a\nseries of downstream applications, and large-scale tests demonstrate its\nsignificant operational and economic benefits, showcasing the transformative\npotential of HAR technology in real-world applications. Additionally, we share\nlessons learned from this deployment and open-source our LIMU-BERT pretrained\nwith millions of hours of sensor data.", "AI": {"tldr": "First nationwide deployment of human activity recognition in food delivery industry using adapted LIMU-BERT model, scaled from city pilot to 500k couriers across China with significant operational benefits.", "motivation": "To demonstrate the real-world application and transformative potential of HAR technology in the on-demand food delivery industry at a massive scale.", "method": "Adapted LIMU-BERT foundation model through three deployment phases over two years, progressing from feasibility study in one city to nationwide adoption.", "result": "Successfully deployed to 500,000 couriers across 367 cities in China, enabling downstream applications and demonstrating significant operational and economic benefits.", "conclusion": "HAR technology has transformative potential in real-world applications, with successful large-scale deployment providing valuable lessons and open-sourced pretrained model."}}
{"id": "2509.23437", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23437", "abs": "https://arxiv.org/abs/2509.23437", "authors": ["Steve Hong", "Runa Eschenhagen", "Bruno Mlodozeniec", "Richard Turner"], "title": "Better Hessians Matter: Studying the Impact of Curvature Approximations in Influence Functions", "comment": null, "summary": "Influence functions offer a principled way to trace model predictions back to\ntraining data, but their use in deep learning is hampered by the need to invert\na large, ill-conditioned Hessian matrix. Approximations such as Generalised\nGauss-Newton (GGN) and Kronecker-Factored Approximate Curvature (K-FAC) have\nbeen proposed to make influence computation tractable, yet it remains unclear\nhow the departure from exactness impacts data attribution performance.\nCritically, given the restricted regime in which influence functions are\nderived, it is not necessarily clear better Hessian approximations should even\nlead to better data attribution performance. In this paper, we investigate the\neffect of Hessian approximation quality on influence-function attributions in a\ncontrolled classification setting. Our experiments show that better Hessian\napproximations consistently yield better influence score quality, offering\njustification for recent research efforts towards that end. We further\ndecompose the approximation steps for recent Hessian approximation methods and\nevaluate each step's influence on attribution accuracy. Notably, the mismatch\nbetween K-FAC eigenvalues and GGN/EK-FAC eigenvalues accounts for the majority\nof the error and influence loss. These findings highlight which approximations\nare most critical, guiding future efforts to balance computational tractability\nand attribution accuracy.", "AI": {"tldr": "Better Hessian approximations consistently improve influence function data attribution performance in classification tasks, with K-FAC eigenvalue mismatch being the main source of error.", "motivation": "To understand how Hessian approximation quality affects influence function data attribution performance, given that better approximations don't necessarily guarantee better attribution results in the restricted derivation regime of influence functions.", "method": "Investigated Hessian approximation effects on influence attributions in controlled classification setting, decomposed approximation steps of recent methods (GGN, K-FAC, EK-FAC) and evaluated each step's impact on attribution accuracy.", "result": "Better Hessian approximations consistently yield better influence score quality; K-FAC eigenvalue mismatch with GGN/EK-FAC accounts for majority of error and influence loss.", "conclusion": "Findings justify research efforts towards better Hessian approximations and highlight which approximations are most critical for balancing computational tractability and attribution accuracy."}}
{"id": "2509.23957", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23957", "abs": "https://arxiv.org/abs/2509.23957", "authors": ["Claudio Fantinuoli"], "title": "Vision-Grounded Machine Interpreting: Improving the Translation Process through Visual Cues", "comment": "Paper presented at AMTA 2025", "summary": "Machine Interpreting systems are currently implemented as unimodal, real-time\nspeech-to-speech architectures, processing translation exclusively on the basis\nof the linguistic signal. Such reliance on a single modality, however,\nconstrains performance in contexts where disambiguation and adequacy depend on\nadditional cues, such as visual, situational, or pragmatic information. This\npaper introduces Vision-Grounded Interpreting (VGI), a novel approach designed\nto address the limitations of unimodal machine interpreting. We present a\nprototype system that integrates a vision-language model to process both speech\nand visual input from a webcam, with the aim of priming the translation process\nthrough contextual visual information. To evaluate the effectiveness of this\napproach, we constructed a hand-crafted diagnostic corpus targeting three types\nof ambiguity. In our evaluation, visual grounding substantially improves\nlexical disambiguation, yields modest and less stable gains for gender\nresolution, and shows no benefit for syntactic ambiguities. We argue that\nembracing multimodality represents a necessary step forward for advancing\ntranslation quality in machine interpreting.", "AI": {"tldr": "Vision-Grounded Interpreting (VGI) introduces multimodal machine interpreting that combines speech and visual input to improve translation quality, particularly for lexical disambiguation.", "motivation": "Current machine interpreting systems rely solely on linguistic signals, limiting performance in contexts requiring visual, situational, or pragmatic cues for disambiguation and adequacy.", "method": "Developed a prototype system integrating vision-language model to process both speech and visual input from webcam, using visual context to prime translation. Evaluated with hand-crafted diagnostic corpus targeting three ambiguity types.", "result": "Visual grounding substantially improves lexical disambiguation, shows modest and less stable gains for gender resolution, and no benefit for syntactic ambiguities.", "conclusion": "Embracing multimodality is a necessary step forward for advancing translation quality in machine interpreting systems."}}
{"id": "2509.24314", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24314", "abs": "https://arxiv.org/abs/2509.24314", "authors": ["Hongjun Liu", "Yinghao Zhu", "Yuhui Wang", "Yitao Long", "Zeyu Lai", "Lequan Yu", "Chen Zhao"], "title": "MedMMV: A Controllable Multimodal Multi-Agent Framework for Reliable and Verifiable Clinical Reasoning", "comment": "25 pages, 5 figures", "summary": "Recent progress in multimodal large language models (MLLMs) has demonstrated\npromising performance on medical benchmarks and in preliminary trials as\nclinical assistants. Yet, our pilot audit of diagnostic cases uncovers a\ncritical failure mode: instability in early evidence interpretation precedes\nhallucination, creating branching reasoning trajectories that cascade into\nglobally inconsistent conclusions. This highlights the need for clinical\nreasoning agents that constrain stochasticity and hallucination while producing\nauditable decision flows. We introduce MedMMV, a controllable multimodal\nmulti-agent framework for reliable and verifiable clinical reasoning. MedMMV\nstabilizes reasoning through diversified short rollouts, grounds intermediate\nsteps in a structured evidence graph under the supervision of a Hallucination\nDetector, and aggregates candidate paths with a Combined Uncertainty scorer. On\nsix medical benchmarks, MedMMV improves accuracy by up to 12.7% and, more\ncritically, demonstrates superior reliability. Blind physician evaluations\nconfirm that MedMMV substantially increases reasoning truthfulness without\nsacrificing informational content. By controlling instability through a\nverifiable, multi-agent process, our framework provides a robust path toward\ndeploying trustworthy AI systems in high-stakes domains like clinical decision\nsupport.", "AI": {"tldr": "MedMMV is a controllable multimodal multi-agent framework that addresses instability and hallucination in medical AI systems by using diversified reasoning rollouts, structured evidence graphs, and uncertainty scoring to improve reliability and verifiability.", "motivation": "Current multimodal large language models show promising performance in medical applications but suffer from critical instability in early evidence interpretation that leads to hallucination and inconsistent conclusions, highlighting the need for more reliable clinical reasoning agents.", "method": "MedMMV stabilizes reasoning through diversified short rollouts, grounds intermediate steps in structured evidence graphs supervised by a Hallucination Detector, and aggregates candidate paths using a Combined Uncertainty scorer.", "result": "On six medical benchmarks, MedMMV improves accuracy by up to 12.7% and demonstrates superior reliability. Blind physician evaluations confirm substantial increases in reasoning truthfulness without sacrificing informational content.", "conclusion": "By controlling instability through a verifiable, multi-agent process, MedMMV provides a robust path toward deploying trustworthy AI systems in high-stakes clinical decision support domains."}}
{"id": "2509.23443", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23443", "abs": "https://arxiv.org/abs/2509.23443", "authors": ["Wenhao Yang", "Lin Li", "Xiaohui Tao", "Kaize Shi"], "title": "Factor Decorrelation Enhanced Data Removal from Deep Predictive Models", "comment": "accepted by NeurIPS 2025", "summary": "The imperative of user privacy protection and regulatory compliance\nnecessitates sensitive data removal in model training, yet this process often\ninduces distributional shifts that undermine model performance-particularly in\nout-of-distribution (OOD) scenarios. We propose a novel data removal approach\nthat enhances deep predictive models through factor decorrelation and loss\nperturbation. Our approach introduces: (1) a discriminative-preserving factor\ndecorrelation module employing dynamic adaptive weight adjustment and iterative\nrepresentation updating to reduce feature redundancy and minimize inter-feature\ncorrelations. (2) a smoothed data removal mechanism with loss perturbation that\ncreates information-theoretic safeguards against data leakage during removal\noperations. Extensive experiments on five benchmark datasets show that our\napproach outperforms other baselines and consistently achieves high predictive\naccuracy and robustness even under significant distribution shifts. The results\nhighlight its superior efficiency and adaptability in both in-distribution and\nout-of-distribution scenarios.", "AI": {"tldr": "A novel data removal approach using factor decorrelation and loss perturbation to maintain model performance while protecting privacy, showing superior results in both in-distribution and out-of-distribution scenarios.", "motivation": "To address the performance degradation caused by distributional shifts when removing sensitive data for privacy protection and regulatory compliance.", "method": "Uses discriminative-preserving factor decorrelation with dynamic adaptive weight adjustment and iterative representation updating, combined with smoothed data removal mechanism with loss perturbation.", "result": "Outperforms other baselines on five benchmark datasets, achieving high predictive accuracy and robustness under significant distribution shifts.", "conclusion": "The approach demonstrates superior efficiency and adaptability in maintaining model performance while ensuring data privacy protection."}}
{"id": "2509.23967", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23967", "abs": "https://arxiv.org/abs/2509.23967", "authors": ["Ken Deng", "Zizheng Zhan", "Wen Xiang", "Wenqiang Zhu", "Tianhao Peng", "Xinping Lei", "Weihao Li", "Jingxuan Xu", "Kun Wu", "Yifan Yao", "Haoyang Huang", "Huaixi Tang", "Kepeng Lei", "Zhiyi Lai", "Songwei Yu", "Zongxian Feng", "Zuchen Gao", "Weihao Xie", "Chenchen Zhang", "Yanan Wu", "Yuanxing Zhang", "Lecheng Huang", "Yuqun Zhang", "Jie Liu", "Zhaoxiang Zhang", "Haotian Zhang", "Bin Chen", "Jiaheng Liu"], "title": "HiPO: Hybrid Policy Optimization for Dynamic Reasoning in LLMs", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on chain-of-thought (CoT)\nreasoning to improve accuracy on complex tasks. However, always generating\nlengthy reasoning traces is inefficient, leading to excessive token usage and\nhigher inference costs. This paper introduces the Hybrid Policy Optimization\n(i.e., HiPO), a framework for adaptive reasoning control that enables LLMs to\nselectively decide when to engage in detailed reasoning (Think-on) and when to\nrespond directly (Think-off). Specifically, HiPO combines a hybrid data\npipelineproviding paired Think-on and Think-off responseswith a hybrid\nreinforcement learning reward system that balances accuracy and efficiency\nwhile avoiding over-reliance on detailed reasoning. Experiments across\nmathematics and coding benchmarks demonstrate that HiPO can substantially\nreduce token length while maintaining or improving accuracy. Finally, we hope\nHiPO a can be a principled approach for efficient adaptive reasoning, advancing\nthe deployment of reasoning-oriented LLMs in real-world, resource-sensitive\nsettings.", "AI": {"tldr": "HiPO is a framework for adaptive reasoning control that enables LLMs to selectively decide when to use detailed reasoning (Think-on) and when to respond directly (Think-off), balancing accuracy and efficiency.", "motivation": "Current LLMs always generate lengthy reasoning traces (CoT) which is inefficient, leading to excessive token usage and higher inference costs. There's a need for selective reasoning to optimize resource usage.", "method": "HiPO combines a hybrid data pipeline providing paired Think-on and Think-off responses with a hybrid reinforcement learning reward system that balances accuracy and efficiency while avoiding over-reliance on detailed reasoning.", "result": "Experiments across mathematics and coding benchmarks demonstrate that HiPO can substantially reduce token length while maintaining or improving accuracy.", "conclusion": "HiPO provides a principled approach for efficient adaptive reasoning, advancing the deployment of reasoning-oriented LLMs in real-world, resource-sensitive settings."}}
{"id": "2509.24340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24340", "abs": "https://arxiv.org/abs/2509.24340", "authors": ["German M. Matilla", "Jiri Nemecek", "Illia Kryvoviaz", "Jakub Marecek"], "title": "humancompatible.detect: a Python Toolkit for Detecting Bias in AI Models", "comment": null, "summary": "There is a strong recent emphasis on trustworthy AI. In particular,\ninternational regulations, such as the AI Act, demand that AI practitioners\nmeasure data quality on the input and estimate bias on the output of high-risk\nAI systems. However, there are many challenges involved, including scalability\n(MMD) and computability (Wasserstein-1) issues of traditional methods for\nestimating distances on measure spaces. Here, we present\nhumancompatible.detect, a toolkit for bias detection that addresses these\nchallenges. It incorporates two newly developed methods to detect and evaluate\nbias: maximum subgroup discrepancy (MSD) and subsampled $\\ell_\\infty$\ndistances. It has an easy-to-use API documented with multiple examples.\nhumancompatible.detect is licensed under the Apache License, Version 2.0.", "AI": {"tldr": "A toolkit called humancompatible.detect is introduced for bias detection in AI systems, addressing scalability and computability issues of traditional distance estimation methods.", "motivation": "To meet regulatory demands for trustworthy AI, particularly the AI Act's requirements for measuring data quality and estimating bias in high-risk AI systems, while overcoming challenges in traditional methods.", "method": "The toolkit incorporates two new methods: maximum subgroup discrepancy (MSD) and subsampled \u2113\u221e distances, providing an easy-to-use API with multiple examples.", "result": "humancompatible.detect successfully addresses scalability issues with MMD and computability issues with Wasserstein-1 distance, offering practical bias detection capabilities.", "conclusion": "The toolkit provides an effective solution for bias detection in AI systems, is licensed under Apache License 2.0, and helps practitioners comply with international AI regulations."}}
{"id": "2509.23453", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.23453", "abs": "https://arxiv.org/abs/2509.23453", "authors": ["Dawei Gao", "Dali Wang", "Zhuowei Gu", "Qinglei Cao", "Xiao Wang", "Peter Thornton", "Dan Ricciuto", "Yunhe Feng"], "title": "PHASE: Physics-Integrated, Heterogeneity-Aware Surrogates for Scientific Simulations", "comment": "19 pages, 13 figures", "summary": "Large-scale numerical simulations underpin modern scientific discovery but\nremain constrained by prohibitive computational costs. AI surrogates offer\nacceleration, yet adoption in mission-critical settings is limited by concerns\nover physical plausibility, trustworthiness, and the fusion of heterogeneous\ndata. We introduce PHASE, a modular deep-learning framework for\nphysics-integrated, heterogeneity-aware surrogates in scientific simulations.\nPHASE combines data-type-aware encoders for heterogeneous inputs with\nmulti-level physics-based constraints that promote consistency from local\ndynamics to global system behavior. We validate PHASE on the biogeochemical\n(BGC) spin-up workflow of the U.S. Department of Energy's Energy Exascale Earth\nSystem Model (E3SM) Land Model (ELM), presenting-to our knowledge-the first\nscientifically validated AI-accelerated solution for this task. Using only the\nfirst 20 simulation years, PHASE infers a near-equilibrium state that otherwise\nrequires more than 1,200 years of integration, yielding an effective reduction\nin required integration length by at least 60x. The framework is enabled by a\npipeline for fusing heterogeneous scientific data and demonstrates strong\ngeneralization to higher spatial resolutions with minimal fine-tuning. These\nresults indicate that PHASE captures governing physical regularities rather\nthan surface correlations, enabling practical, physically consistent\nacceleration of land-surface modeling and other complex scientific workflows.", "AI": {"tldr": "PHASE is a modular deep-learning framework that accelerates scientific simulations by integrating physics constraints and handling heterogeneous data, achieving 60x speedup in land-surface modeling.", "motivation": "Large-scale scientific simulations are computationally expensive, and existing AI surrogates lack physical plausibility and trustworthiness needed for mission-critical applications.", "method": "PHASE uses data-type-aware encoders for heterogeneous inputs and multi-level physics-based constraints to ensure consistency from local dynamics to global system behavior.", "result": "PHASE reduced required integration time from 1,200+ years to just 20 years (60x acceleration) for biogeochemical spin-up in Earth system modeling, with strong generalization to higher resolutions.", "conclusion": "PHASE captures governing physical regularities rather than surface correlations, enabling practical, physically consistent acceleration of complex scientific workflows."}}
{"id": "2509.23979", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23979", "abs": "https://arxiv.org/abs/2509.23979", "authors": ["Haonan Wang", "Junfeng Sun", "Xingdi Yuan", "Ruoyao Wang", "Ziang Xiao"], "title": "ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation", "comment": "14 pages,15 figures, Accepted to the 5th Wordplay: When Language\n  Meets Games Workshop, EMNLP 2025", "summary": "Simulating interactive world models remains a core challenge in Large\nLanguage Models(LLMs). In this work, we introduce the ByteSized32Refactored, a\nrefactored, modular, and extensible implementation of the original ByteSized32\ncorpus to explore the task of text game generation. We further optimize the\ncode structure of each text game and create the GameBasic.py foundation\nlibrary, which centralizes common logic across all 32 games by abstracting 7\nbase classes (GameObject, etc.) into reusable modules, thereby reducing from\n20k to 10k total lines of Python code compared to the original Bytesized32. Our\nrefactored implementation enables extendability - with our centralized design,\nByteSized32Refactored can be more efficiently extended to include text games of\nnew scenarios and specifications by reusing the shared logic and\nfunctionalities. Extensive experiments with GPT-4o demonstrate a mix of\nperformance - with Bytesized32Refactored, the generated text games for unseen\nscenarios showcase quality improvements on two of the four evaluation\ndimensions while decreases on the other two, indicating that the hierarchical\nstructure of the refactored code presents new challenges for LLMs. Overall, we\nhighlight that our extensible code structure, centered on the foundation\nlibrary and the modular optimization, not only facilitates LLM adaptation to\nenvironment specifications but also establishes a scalable environment that\nsupports future extensions.", "AI": {"tldr": "ByteSized32Refactored is a modular implementation of text game generation that reduces code by 50% through a foundation library with 7 base classes, enabling extensibility but presenting mixed LLM performance.", "motivation": "To address the challenge of simulating interactive world models in LLMs by creating a more modular and extensible text game generation framework.", "method": "Refactored the original ByteSized32 corpus with modular design, created GameBasic.py foundation library abstracting 7 base classes (GameObject, etc.), and centralized common logic across 32 games.", "result": "Reduced total lines of Python code from 20k to 10k. GPT-4o experiments showed mixed performance - quality improvements on 2 of 4 evaluation dimensions but decreases on the other 2 for unseen scenarios.", "conclusion": "The extensible code structure facilitates LLM adaptation and establishes a scalable environment for future extensions, though the hierarchical structure presents new challenges for LLMs."}}
{"id": "2509.24342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24342", "abs": "https://arxiv.org/abs/2509.24342", "authors": ["Sarmistha Das", "Priya Mathur", "Ishani Sharma", "Sriparna Saha", "Kitsuchart Pasupa", "Alka Maurya"], "title": "Fin-Ally: Pioneering the Development of an Advanced, Commonsense-Embedded Conversational AI for Money Matters", "comment": null, "summary": "The exponential technological breakthrough of the FinTech industry has\nsignificantly enhanced user engagement through sophisticated advisory chatbots.\nHowever, large-scale fine-tuning of LLMs can occasionally yield unprofessional\nor flippant remarks, such as ``With that money, you're going to change the\nworld,'' which, though factually correct, can be contextually inappropriate and\nerode user trust. The scarcity of domain-specific datasets has led previous\nstudies to focus on isolated components, such as reasoning-aware frameworks or\nthe enhancement of human-like response generation. To address this research\ngap, we present Fin-Solution 2.O, an advanced solution that 1) introduces the\nmulti-turn financial conversational dataset, Fin-Vault, and 2) incorporates a\nunified model, Fin-Ally, which integrates commonsense reasoning, politeness,\nand human-like conversational dynamics. Fin-Ally is powered by\nCOMET-BART-embedded commonsense context and optimized with a Direct Preference\nOptimization (DPO) mechanism to generate human-aligned responses. The novel\nFin-Vault dataset, consisting of 1,417 annotated multi-turn dialogues, enables\nFin-Ally to extend beyond basic account management to provide personalized\nbudgeting, real-time expense tracking, and automated financial planning. Our\ncomprehensive results demonstrate that incorporating commonsense context\nenables language models to generate more refined, textually precise, and\nprofessionally grounded financial guidance, positioning this approach as a\nnext-generation AI solution for the FinTech sector. Dataset and codes are\navailable at: https://github.com/sarmistha-D/Fin-Ally", "AI": {"tldr": "Fin-Solution 2.O introduces Fin-Vault dataset (1,417 multi-turn financial dialogues) and Fin-Ally model that integrates commonsense reasoning, politeness, and human-like conversation to generate professional financial advice.", "motivation": "Address the gap in domain-specific datasets for financial chatbots and prevent unprofessional/flippant responses that erode user trust in FinTech advisory systems.", "method": "Created Fin-Vault dataset and developed Fin-Ally model using COMET-BART-embedded commonsense context with Direct Preference Optimization (DPO) for human-aligned response generation.", "result": "The approach enables language models to generate more refined, textually precise, and professionally grounded financial guidance beyond basic account management.", "conclusion": "This represents a next-generation AI solution for FinTech sector that provides personalized budgeting, expense tracking, and automated financial planning through human-aligned conversational AI."}}
{"id": "2509.23461", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23461", "abs": "https://arxiv.org/abs/2509.23461", "authors": ["Ziheng Cheng", "Zhong Li", "Jiang Bian"], "title": "Data-Efficient Training by Evolved Sampling", "comment": null, "summary": "Data selection is designed to accelerate learning with preserved performance.\nTo achieve this, a fundamental thought is to identify informative data samples\nwith significant contributions to the training. In this work, we propose\n\\textbf{Evolved Sampling} (\\textbf{ES}), a simple yet effective framework for\n\\emph{dynamic} sampling along the training process. This method conducts \\em\nbatch \\em level data selection based on the dynamics of losses and augmented\n\\emph{loss differences}, which enables flexible \\emph{frequency tuning}, and\nhence significantly reduces the back propagation time with maintained model\nperformance. Due to its conciseness, ES is also readily extensible to\nincorporate \\em set \\em level data selection (to form ES with pruning,\n\\textbf{ESWP}) for further accelerations. As a plug-and-play framework, ES(WP)\nconsistently achieves lossless training accelerations across various\npre-training and post-training tasks, saving up to nearly 45\\% wall-clock time.\nOur results motivate further investigations on the data efficiency aspect of\nmodern large-scale machine learning.", "AI": {"tldr": "Evolved Sampling (ES) is a dynamic data selection framework that uses loss dynamics and loss differences for batch-level sampling, achieving up to 45% training acceleration without performance loss.", "motivation": "To accelerate machine learning training while maintaining performance by identifying and selecting the most informative data samples throughout the training process.", "method": "ES uses batch-level data selection based on loss dynamics and augmented loss differences, enabling flexible frequency tuning. It can be extended to set-level selection (ESWP) for further acceleration.", "result": "ES(WP) achieves lossless training accelerations across various pre-training and post-training tasks, saving up to nearly 45% wall-clock time.", "conclusion": "The method demonstrates effective data efficiency improvements for large-scale machine learning, motivating further research in this direction."}}
{"id": "2509.23982", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.23982", "abs": "https://arxiv.org/abs/2509.23982", "authors": ["Lucio La Cava", "Andrea Tagarelli"], "title": "Toward Preference-aligned Large Language Models via Residual-based Model Steering", "comment": null, "summary": "Preference alignment is a critical step in making Large Language Models\n(LLMs) useful and aligned with (human) preferences. Existing approaches such as\nReinforcement Learning from Human Feedback or Direct Preference Optimization\ntypically require curated data and expensive optimization over billions of\nparameters, and eventually lead to persistent task-specific models. In this\nwork, we introduce Preference alignment of Large Language Models via Residual\nSteering (PaLRS), a training-free method that exploits preference signals\nencoded in the residual streams of LLMs. From as few as one hundred preference\npairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be\napplied at inference time to push models toward preferred behaviors. We\nevaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that\nPaLRS-aligned models achieve consistent gains on mathematical reasoning and\ncode generation benchmarks while preserving baseline general-purpose\nperformance. Moreover, when compared to DPO-aligned models, they perform better\nwith huge time savings. Our findings highlight that PaLRS offers an effective,\nmuch more efficient and flexible alternative to standard preference\noptimization pipelines, offering a training-free, plug-and-play mechanism for\nalignment with minimal data.", "AI": {"tldr": "PaLRS is a training-free method for LLM preference alignment that extracts steering vectors from residual streams using minimal preference data, enabling plug-and-play inference-time alignment without expensive optimization.", "motivation": "Existing preference alignment methods like RLHF and DPO require curated data, expensive optimization over billions of parameters, and create persistent task-specific models, making them inefficient and inflexible.", "method": "Extracts lightweight steering vectors from LLM residual streams using as few as 100 preference pairs, then applies these plug-and-play vectors at inference time to steer models toward preferred behaviors.", "result": "PaLRS-aligned models achieve consistent gains on mathematical reasoning and code generation benchmarks while preserving baseline general-purpose performance, and outperform DPO-aligned models with huge time savings.", "conclusion": "PaLRS offers an effective, efficient, and flexible alternative to standard preference optimization pipelines, providing training-free, plug-and-play alignment with minimal data requirements."}}
{"id": "2509.24351", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24351", "abs": "https://arxiv.org/abs/2509.24351", "authors": ["Jie Ma", "Shihao Qi", "Rui Xing", "Ziang Yin", "Bifan Wei", "Jun Liu", "Tongliang Liu"], "title": "From Static to Dynamic: Adaptive Monte Carlo Search for Mathematical Process Supervision", "comment": null, "summary": "The quality of process data plays a key role in training a Process Reward\nModel (PRM), which can enhance the complex mathematical reasoning capability of\nlarge language models. Existing methods estimate the quality of reasoning steps\nbased on a fixed-budget sampling strategy and navigate a vast search space to\nperform path expansion during the automated data generation process, resulting\nin their inefficiency and inflexibility. To address these issues, we propose\nAdaptive Monte Carlo Search (AMCS), a framework that transforms data generation\nfrom fixed, static to adaptive, dynamic search at the level of node value\nestimation and path expansion. On one hand, AMCS adaptively refines estimation\nby allocating more samples to uncertain reasoning steps while using fewer\nsamples for those that are easier to estimate. On the other hand, it enhances\nthe path expansion through a Monte Carlo algorithm with a temporally adaptive\npolicy that begins with broad exploration and gradually shifts toward\nexploiting the most promising directions. With AMCS, we construct a large-scale\ndataset MathSearch-200K of about 200K process supervision examples for training\nPRMs. To verify the effectiveness of our method, we conduct extensive\nexperiments on four mathematical reasoning benchmarks. Experimental results\nshow that Qwen2.5-Math-7B-PRM-AMCS achieves up to 76.2% accuracy on MATH500\nwith GLM-4-9B, outperforming all baseline PRMs. Notably, a 7B model supervised\nby Qwen2.5-Math-7B-PRM-AMCS surpasses a 72B model with weaker supervision.\nMoreover, Qwen2.5-Math-7B-PRM-AMCS maintains consistent advantages on\nout-of-distribution problems, demonstrating strong generalization capability.\nOur code is available at https://github.com/reml-group/AMCS.", "AI": {"tldr": "AMCS is an adaptive Monte Carlo search framework that improves process data generation for training Process Reward Models (PRMs) by dynamically allocating samples and adapting search strategies, resulting in superior mathematical reasoning performance.", "motivation": "Existing methods for process data generation use fixed-budget sampling and inefficient search strategies, leading to poor quality training data for Process Reward Models that enhance mathematical reasoning in LLMs.", "method": "Proposes Adaptive Monte Carlo Search (AMCS) with two key components: adaptive node value estimation that allocates more samples to uncertain reasoning steps, and temporally adaptive path expansion that shifts from exploration to exploitation of promising directions.", "result": "Created MathSearch-200K dataset with 200K process supervision examples. Qwen2.5-Math-7B-PRM-AMCS achieved 76.2% accuracy on MATH500, outperforming all baseline PRMs. A 7B model supervised by AMCS surpassed a 72B model with weaker supervision, demonstrating strong generalization on out-of-distribution problems.", "conclusion": "AMCS effectively addresses inefficiencies in process data generation through adaptive search strategies, producing high-quality training data that significantly enhances mathematical reasoning capabilities in language models while maintaining strong generalization performance."}}
{"id": "2509.23462", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23462", "abs": "https://arxiv.org/abs/2509.23462", "authors": ["Alakh Sharma", "Gaurish Trivedi", "Kartikey Bhandari", "Yash Sinha", "Dhruv Kumar", "Pratik Narang", "Jagat Sesh Challa"], "title": "Generative Evolutionary Meta-Solver (GEMS): Scalable Surrogate-Free Multi-Agent Learning", "comment": "Under review", "summary": "Scalable multi-agent reinforcement learning (MARL) remains a central\nchallenge for AI. Existing population-based methods, like Policy-Space Response\nOracles, PSRO, require storing explicit policy populations and constructing\nfull payoff matrices, incurring quadratic computation and linear memory costs.\nWe present Generative Evolutionary Meta-Solver (GEMS), a surrogate-free\nframework that replaces explicit populations with a compact set of latent\nanchors and a single amortized generator. Instead of exhaustively constructing\nthe payoff matrix, GEMS relies on unbiased Monte Carlo rollouts,\nmultiplicative-weights meta-dynamics, and a model-free empirical-Bernstein UCB\noracle to adaptively expand the policy set. Best responses are trained within\nthe generator using an advantage-based trust-region objective, eliminating the\nneed to store and train separate actors. We evaluated GEMS in a variety of\nTwo-player and Multi-Player games such as the Deceptive Messages Game, Kuhn\nPoker and Multi-Particle environment. We find that GEMS is up to ~6x faster,\nhas 1.3x less memory usage than PSRO, while also reaps higher rewards\nsimultaneously. These results demonstrate that GEMS retains the game theoretic\nguarantees of PSRO, while overcoming its fundamental inefficiencies, hence\nenabling scalable multi-agent learning in multiple domains.", "AI": {"tldr": "GEMS is a scalable MARL framework that replaces explicit policy populations with latent anchors and an amortized generator, using Monte Carlo rollouts and meta-dynamics to achieve 6x speedup and 1.3x memory reduction over PSRO while maintaining game-theoretic guarantees.", "motivation": "Existing population-based MARL methods like PSRO suffer from quadratic computation and linear memory costs due to storing explicit policy populations and constructing full payoff matrices, limiting scalability.", "method": "Uses compact latent anchors and a single amortized generator instead of explicit populations. Employs unbiased Monte Carlo rollouts, multiplicative-weights meta-dynamics, and model-free empirical-Bernstein UCB oracle for adaptive policy expansion. Trains best responses within generator using advantage-based trust-region objective.", "result": "Achieves up to 6x faster computation, 1.3x less memory usage than PSRO, while obtaining higher rewards in Two-player and Multi-Player games including Deceptive Messages Game, Kuhn Poker and Multi-Particle environment.", "conclusion": "GEMS overcomes fundamental inefficiencies of PSRO while retaining game-theoretic guarantees, enabling scalable multi-agent learning across multiple domains."}}
{"id": "2509.23990", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23990", "abs": "https://arxiv.org/abs/2509.23990", "authors": ["Dhaathri Vijay", "Anandaswarup Vadapalli"], "title": "The Hidden Costs of Translation Accuracy: Distillation, Quantization, and Environmental Impact", "comment": null, "summary": "The rapid expansion of large language models (LLMs) has heightened concerns\nabout their computational and environmental costs. This study investigates the\ntrade-offs between translation quality and efficiency by comparing full-scale,\ndistilled, and quantized models using machine translation as a case study. We\nevaluated performance on the Flores+ benchmark and through human judgments of\nconversational translations in French, Hindi, and Kannada. Our analysis of\ncarbon emissions per evaluation run revealed that the full 3.3B fp32 model,\nwhile achieving the highest BLEU scores, incurred the largest environmental\nfootprint (about 0.007-0.008 kg CO2 per run). The distilled models achieved an\ninference of up to 4.5x faster than the full 3.3B model, with only minimal\nreductions in BLEU scores. Human evaluations also showed that even aggressive\nquantization (INT4) preserved high levels of accuracy and fluency, with\ndifferences between models generally minor. These findings demonstrate that\nmodel compression strategies can substantially reduce computational demands and\nenvironmental impact while maintaining competitive translation quality, though\ntrade-offs are more pronounced in low-resource settings. We argue for\nevaluation frameworks that integrate efficiency and sustainability alongside\nobjective metrics as central dimensions of progress in NLP.", "AI": {"tldr": "This study shows that distilled and quantized LLMs can achieve 4.5x faster inference with minimal quality loss, significantly reducing computational costs and carbon emissions (0.007-0.008 kg CO2 per run for full model) while maintaining competitive translation quality.", "motivation": "Address growing concerns about computational and environmental costs of large language models by investigating trade-offs between translation quality and efficiency.", "method": "Compared full-scale, distilled, and quantized models using machine translation as case study. Evaluated on Flores+ benchmark and human judgments of conversational translations in French, Hindi, and Kannada. Analyzed carbon emissions per evaluation run.", "result": "Distilled models achieved 4.5x faster inference than full 3.3B model with minimal BLEU score reductions. Aggressive quantization (INT4) preserved high accuracy and fluency. Full fp32 model had largest environmental footprint (0.007-0.008 kg CO2 per run). Trade-offs more pronounced in low-resource settings.", "conclusion": "Model compression strategies can substantially reduce computational demands and environmental impact while maintaining competitive translation quality. Need evaluation frameworks that integrate efficiency and sustainability alongside objective metrics."}}
{"id": "2509.24377", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24377", "abs": "https://arxiv.org/abs/2509.24377", "authors": ["Shihao Qi", "Jie Ma", "Ziang Yin", "Lingling Zhang", "Jian Zhang", "Jun Liu", "Feng Tian", "Tongliang Liu"], "title": "Plan before Solving: Problem-Aware Strategy Routing for Mathematical Reasoning with LLMs", "comment": null, "summary": "Existing methods usually leverage a fixed strategy, such as natural language\nreasoning, code-augmented reasoning, tool-integrated reasoning, or\nensemble-based reasoning, to guide Large Language Models (LLMs) to perform\nmathematical reasoning. Our analysis reveals that the single strategy cannot\nadapt to problem-specific requirements and thus overlooks the trade-off between\neffectiveness and efficiency. To address these issues, we propose Planning and\nRouting through Instance-Specific Modeling (PRISM), a novel framework that\ndecouples mathematical reasoning into two stages: strategy planning and\ntargeted execution. Specifically, we first curate a multi-strategy preference\ndataset, which we call MathStrat, capturing correctness, process quality, and\ncomputational efficiency for each problem--strategy pair. Then, we train a\nlightweight Strategy Adapter based on the dataset to obtain confidence\ndistributions over the mentioned four reasoning strategies. At inference time,\nan adaptive routing policy dynamically tailors the reasoning approach based on\npredictor confidence. It directs the model to use single-strategy execution for\nhigh-confidence predictions, dual-strategy verification for competitive\nscenarios, or comprehensive multi-strategy exploration for uncertain cases.\nExtensive experiments across five mathematical reasoning benchmarks demonstrate\nthat PRISM consistently outperforms individual strategies and ensemble\nbaselines, achieving improvements ranging from 0.9% to 7.6% across different\nbase models. The adaptive routing approach shows particularly strong benefits\nfor mathematical reasoning tasks across diverse model architectures. Our code\nis released at https://github.com/reml-group/PRISM.", "AI": {"tldr": "PRISM is a novel framework that decouples mathematical reasoning into strategy planning and targeted execution stages, using adaptive routing to dynamically select reasoning strategies based on problem-specific requirements.", "motivation": "Existing methods use fixed strategies that cannot adapt to problem-specific requirements and overlook the trade-off between effectiveness and efficiency in mathematical reasoning.", "method": "Proposes PRISM framework with two stages: (1) Strategy planning using a lightweight Strategy Adapter trained on MathStrat dataset to get confidence distributions over four reasoning strategies; (2) Adaptive routing policy that dynamically selects reasoning approaches based on predictor confidence.", "result": "PRISM consistently outperforms individual strategies and ensemble baselines across five mathematical reasoning benchmarks, achieving improvements ranging from 0.9% to 7.6% across different base models.", "conclusion": "The adaptive routing approach shows strong benefits for mathematical reasoning tasks across diverse model architectures, effectively balancing effectiveness and efficiency through dynamic strategy selection."}}
{"id": "2509.23470", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23470", "abs": "https://arxiv.org/abs/2509.23470", "authors": ["Rui Ai", "Hugo De Oliveira Barbalho", "Sirui Li", "Alexei Robsky", "David Simchi-Levi", "Ishai Menache"], "title": "Solve Smart, Not Often: Policy Learning for Costly MILP Re-solving", "comment": null, "summary": "A common challenge in real-time operations is deciding whether to re-solve an\noptimization problem or continue using an existing solution. While modern data\nplatforms may collect information at high frequencies, many real-time\noperations require repeatedly solving computationally intensive optimization\nproblems formulated as Mixed-Integer Linear Programs (MILPs). Determining when\nto re-solve is, therefore, an economically important question. This problem\nposes several challenges: 1) How to characterize solution optimality and\nsolving cost; 2) How to detect environmental changes and select beneficial\nsamples for solving the MILP; 3) Given the large time horizon and non-MDP\nstructure, vanilla reinforcement learning (RL) methods are not directly\napplicable and tend to suffer from value function explosion. Existing\nliterature largely focuses on heuristics, low-data settings, and smooth\nobjectives, with little focus on common NP-hard MILPs. We propose a framework\ncalled Proximal Policy Optimization with Change Point Detection (POC), which\nsystematically offers a solution for balancing performance and cost when\ndeciding appropriate re-solving times. Theoretically, we establish the\nrelationship between the number of re-solves and the re-solving cost. To test\nour framework, we assemble eight synthetic and real-world datasets, and show\nthat POC consistently outperforms existing baselines by 2%-17%. As a side\nbenefit, our work fills the gap in the literature by introducing real-time MILP\nbenchmarks and evaluation criteria.", "AI": {"tldr": "The paper proposes POC framework to determine optimal re-solving times for MILPs, balancing performance and computational cost, achieving 2%-17% improvement over baselines.", "motivation": "Real-time operations need to decide when to re-solve computationally intensive MILPs, as frequent re-solving is costly but infrequent solving may use suboptimal solutions. Existing methods focus on heuristics and smooth objectives, lacking systematic approaches for NP-hard MILPs.", "method": "Proximal Policy Optimization with Change Point Detection (POC) framework that detects environmental changes, selects beneficial samples, and establishes theoretical relationship between re-solves and cost.", "result": "POC consistently outperforms existing baselines by 2%-17% across eight synthetic and real-world datasets, and introduces real-time MILP benchmarks.", "conclusion": "POC provides a systematic solution for determining optimal re-solving times in real-time MILP operations, filling a literature gap and demonstrating significant performance improvements."}}
{"id": "2509.23994", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23994", "abs": "https://arxiv.org/abs/2509.23994", "authors": ["Gauri Kholkar", "Ratinder Ahuja"], "title": "The AI Agent Code of Conduct: Automated Guardrail Policy-as-Prompt Synthesis", "comment": "Accepted at Regulatable ML Workshop at NEURIPS 2025", "summary": "As autonomous AI agents are increasingly deployed in industry, it is\nessential to safeguard them. We introduce a novel framework that automates the\ntranslation of unstructured design documents into verifiable, real-time\nguardrails. We introduce \"Policy as Prompt,\" a new approach that uses Large\nLanguage Models (LLMs) to interpret and enforce natural language policies by\napplying contextual understanding and the principle of least privilege. Our\nsystem first ingests technical artifacts to construct a verifiable policy tree,\nwhich is then compiled into lightweight, prompt-based classifiers that audit\nagent behavior at runtime. We validate our approach across diverse\napplications, demonstrating a scalable and auditable pipeline that bridges the\ncritical policy-to-practice gap, paving the way for verifiably safer and more\nregulatable AI.", "AI": {"tldr": "A framework that automatically converts unstructured design documents into verifiable, real-time guardrails for autonomous AI agents using LLMs to interpret and enforce natural language policies.", "motivation": "As autonomous AI agents are increasingly deployed in industry, it is essential to safeguard them and bridge the critical policy-to-practice gap for verifiably safer and more regulatable AI.", "method": "Introduces 'Policy as Prompt' approach using LLMs to interpret natural language policies, constructs verifiable policy trees from technical artifacts, and compiles them into lightweight prompt-based classifiers that audit agent behavior at runtime.", "result": "Validated across diverse applications, demonstrating a scalable and auditable pipeline that successfully bridges the policy-to-practice gap.", "conclusion": "The framework paves the way for verifiably safer and more regulatable AI by providing automated translation of design documents into real-time guardrails through LLM-based policy interpretation and enforcement."}}
{"id": "2509.24393", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24393", "abs": "https://arxiv.org/abs/2509.24393", "authors": ["Yichi Zhang", "Yue Ding", "Jingwen Yang", "Tianwei Luo", "Dongbai Li", "Ranjie Duan", "Qiang Liu", "Hang Su", "Yinpeng Dong", "Jun Zhu"], "title": "Towards Safe Reasoning in Large Reasoning Models via Corrective Intervention", "comment": null, "summary": "Although Large Reasoning Models (LRMs) have progressed in solving complex\nproblems, their chain-of-thought (CoT) reasoning often contains harmful content\nthat can persist even when the final responses appear safe. We show that this\nissue still remains in existing methods which overlook the unique significance\nof safe reasoning, undermining their trustworthiness and posing potential risks\nin applications if unsafe reasoning is accessible for and exploited by\nmalicious users. We therefore shift our focus to aligning the safety of\nreasoning itself in this paper and explore process supervision as the solution.\nHowever, simply rewarding safe reasoning proves inadequate due to low rollout\ndiversity and limited training signals. To tackle this challenge, we first\ndelve into the characteristics of safe reasoning and uncover several critical\ninsights that 1) safe reasoning is often consolidated by a few critical steps\nof safety triggers; 2) compliance cues strongly correlate with unsafe\ncontinuations; and 3) corrective interventions reliably steer unsafe\ntrajectories towards safer traces. Motivated by these, we propose Intervened\nPreference Optimization (IPO), an alignment method that enforces safe reasoning\nby substituting compliance steps with safety triggers and constructing pairs\nfor preference learning with strong signals. Experiments on jailbreak and\nadversarial safety benchmarks demonstrate that IPO remarkably improves overall\nsafety regarding both reasoning and responses, outperforming SFT-based and\nRL-based baselines with a relative reduction of over 30% in harmfulness, while\npreserving excellent performance across diverse reasoning tasks. The results\nhighlight the importance of explicit alignment for reasoning and provide a\npractical path to safer LRMs.", "AI": {"tldr": "This paper addresses the safety issue in Large Reasoning Models' chain-of-thought reasoning, where harmful content persists even when final responses appear safe. The authors propose Intervened Preference Optimization (IPO) to align reasoning safety by substituting compliance steps with safety triggers and using preference learning.", "motivation": "Existing methods overlook the unique significance of safe reasoning in Large Reasoning Models, allowing harmful content to persist in chain-of-thought reasoning even when final responses appear safe. This undermines trustworthiness and poses potential risks if exploited by malicious users.", "method": "The authors propose Intervened Preference Optimization (IPO), an alignment method that enforces safe reasoning by: 1) substituting compliance steps with safety triggers, and 2) constructing pairs for preference learning with strong signals. This approach is based on insights about safety triggers, compliance cues, and corrective interventions.", "result": "Experiments on jailbreak and adversarial safety benchmarks show that IPO remarkably improves overall safety regarding both reasoning and responses, achieving over 30% relative reduction in harmfulness compared to SFT-based and RL-based baselines, while preserving excellent performance across diverse reasoning tasks.", "conclusion": "The results highlight the importance of explicit alignment for reasoning safety and provide a practical path to safer Large Reasoning Models through the proposed Intervened Preference Optimization method."}}
{"id": "2509.23471", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.23471", "abs": "https://arxiv.org/abs/2509.23471", "authors": ["Harshil Vejendla"], "title": "Drift-Adapter: A Practical Approach to Near Zero-Downtime Embedding Model Upgrades in Vector Databases", "comment": "EMNLP 2025 Main 12 pages, 6 figures", "summary": "Upgrading embedding models in production vector databases typically requires\nre-encoding the entire corpus and rebuilding the Approximate Nearest Neighbor\n(ANN) index, leading to significant operational disruption and computational\ncost. This paper presents Drift-Adapter, a lightweight, learnable\ntransformation layer designed to bridge embedding spaces between model\nversions. By mapping new queries into the legacy embedding space, Drift-Adapter\nenables the continued use of the existing ANN index, effectively deferring full\nre-computation. We systematically evaluate three adapter parameterizations:\nOrthogonal Procrustes, Low-Rank Affine, and a compact Residual MLP, trained on\na small sample of paired old and new embeddings. Experiments on MTEB text\ncorpora and a CLIP image model upgrade (1M items) show that Drift-Adapter\nrecovers 95-99% of the retrieval recall (Recall@10, MRR) of a full\nre-embedding, adding less than 10 microseconds of query latency. Compared to\noperational strategies like full re-indexing or dual-index serving,\nDrift-Adapter reduces recompute costs by over 100 times and facilitates\nupgrades with near-zero operational interruption. We analyze robustness to\nvaried model drift, training data size, scalability to billion-item systems,\nand the impact of design choices like diagonal scaling, demonstrating\nDrift-Adapter's viability as a pragmatic solution for agile model deployment.", "AI": {"tldr": "Drift-Adapter is a lightweight transformation layer that enables embedding model upgrades without rebuilding ANN indexes by mapping new queries to legacy embedding spaces, achieving 95-99% retrieval recall with minimal latency.", "motivation": "Traditional embedding model upgrades require re-encoding entire corpora and rebuilding ANN indexes, causing significant operational disruption and computational costs.", "method": "Three adapter parameterizations: Orthogonal Procrustes, Low-Rank Affine, and compact Residual MLP, trained on small samples of paired old and new embeddings to map new queries to legacy embedding space.", "result": "Recovers 95-99% of retrieval recall (Recall@10, MRR) compared to full re-embedding, adds less than 10 microseconds query latency, reduces recompute costs by over 100 times, and enables near-zero operational interruption upgrades.", "conclusion": "Drift-Adapter provides a pragmatic solution for agile model deployment by enabling embedding model upgrades without the need for full re-indexing, significantly reducing operational costs and disruption."}}
{"id": "2509.24002", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24002", "abs": "https://arxiv.org/abs/2509.24002", "authors": ["Zijian Wu", "Xiangyan Liu", "Xinyuan Zhang", "Lingjun Chen", "Fanqing Meng", "Lingxiao Du", "Yiran Zhao", "Fanshi Zhang", "Yaoqi Ye", "Jiawei Wang", "Zirui Wang", "Jinjie Ni", "Yufan Yang", "Arvin Xu", "Michael Qizhe Shieh"], "title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use", "comment": "42 pages, 27 figures, 10 tables", "summary": "MCP standardizes how LLMs interact with external systems, forming the\nfoundation for general agents. However, existing MCP benchmarks remain narrow\nin scope: they focus on read-heavy tasks or tasks with limited interaction\ndepth, and fail to capture the complexity and realism of real-world workflows.\nTo address this gap, we propose MCPMark, a benchmark designed to evaluate MCP\nuse in a more realistic and comprehensive manner. It consists of $127$\nhigh-quality tasks collaboratively created by domain experts and AI agents.\nEach task begins with a curated initial state and includes a programmatic\nscript for automatic verification. These tasks demand richer and more diverse\ninteractions with the environment, involving a broad range of create, read,\nupdate, and delete (CRUD) operations. We conduct a comprehensive evaluation of\ncutting-edge LLMs using a minimal agent framework that operates in a\ntool-calling loop. Empirical results show that the best-performing model,\ngpt-5-medium, reaches only $52.56$\\% pass@1 and $33.86$\\% pass^4, while other\nwidely regarded strong models, including claude-sonnet-4 and o3, fall below\n$30$\\% pass@1 and $15$\\% pass^4. On average, LLMs require $16.2$ execution\nturns and $17.4$ tool calls per task, significantly surpassing those in\nprevious MCP benchmarks and highlighting the stress-testing nature of MCPMark.", "AI": {"tldr": "MCPMark is a new benchmark with 127 tasks that evaluates MCP (Model Context Protocol) usage more realistically, requiring complex CRUD operations. Current LLMs perform poorly, with the best model achieving only 52.56% pass@1.", "motivation": "Existing MCP benchmarks are too narrow, focusing on read-heavy tasks with limited interaction depth, failing to capture real-world workflow complexity.", "method": "Created 127 high-quality tasks with domain experts and AI agents, each with curated initial states and programmatic verification scripts. Tasks require diverse CRUD operations and richer environment interactions.", "result": "Best model (gpt-5-medium) achieved only 52.56% pass@1 and 33.86% pass^4. Other strong models (claude-sonnet-4, o3) fell below 30% pass@1 and 15% pass^4. LLMs required average 16.2 execution turns and 17.4 tool calls per task.", "conclusion": "MCPMark effectively stress-tests MCP capabilities, revealing significant performance gaps in current LLMs for complex real-world interactions, with much higher interaction requirements than previous benchmarks."}}
{"id": "2509.24443", "categories": ["cs.AI", "cs.ET", "cs.SE", "68T01, 68T09, 68M14, 68W10, 68W15", "C.2.4; C.4; C.5; D.2.2; D.2.11; I.2.5; I.2.6; I.2.11; J.0; J.7"], "pdf": "https://arxiv.org/pdf/2509.24443", "abs": "https://arxiv.org/abs/2509.24443", "authors": ["Leila Ismail", "Abdelmoneim Abdelmoti", "Arkaprabha Basu", "Aymen Dia Eddine Berini", "Mohammad Naouss"], "title": "A Systematic Review of Digital Twin-Driven Predictive Maintenance in Industrial Engineering: Taxonomy, Architectural Elements, and Future Research Directions", "comment": null, "summary": "With the increasing complexity of industrial systems, there is a pressing\nneed for predictive maintenance to avoid costly downtime and disastrous\noutcomes that could be life-threatening in certain domains. With the growing\npopularity of the Internet of Things, Artificial Intelligence, machine\nlearning, and real-time big data analytics, there is a unique opportunity for\nefficient predictive maintenance to forecast equipment failures for real-time\nintervention and optimize maintenance actions, as traditional reactive and\npreventive maintenance practices are often inadequate to meet the requirements\nfor the industry to provide quality-of-services of operations. Central to this\nevolution is digital twin technology, an adaptive virtual replica that\ncontinuously monitors and integrates sensor data to simulate and improve asset\nperformance. Despite remarkable progress in digital twin implementations, such\nas considering DT in predictive maintenance for industrial engineering. This\npaper aims to address this void. We perform a retrospective analysis of the\ntemporal evolution of the digital twin in predictive maintenance for industrial\nengineering to capture the applications, middleware, and technological\nrequirements that led to the development of the digital twin from its inception\nto the AI-enabled digital twin and its self-learning models. We provide a\nlayered architecture of the digital twin technology, as well as a taxonomy of\nthe technology-enabled industrial engineering applications systems, middleware,\nand the used Artificial Intelligence algorithms. We provide insights into these\nsystems for the realization of a trustworthy and efficient smart digital-twin\nindustrial engineering ecosystem. We discuss future research directions in\ndigital twin for predictive maintenance in industrial engineering.", "AI": {"tldr": "This paper provides a retrospective analysis of digital twin evolution in predictive maintenance for industrial engineering, examining applications, middleware, and AI algorithms from inception to current AI-enabled self-learning models.", "motivation": "Traditional reactive and preventive maintenance practices are inadequate for modern industrial systems. With IoT, AI, and real-time analytics, there's an opportunity for efficient predictive maintenance to forecast failures and optimize maintenance actions, with digital twin technology playing a central role.", "method": "The authors perform retrospective analysis of temporal evolution of digital twin in predictive maintenance, provide layered architecture of digital twin technology, taxonomy of technology-enabled industrial engineering applications, middleware, and AI algorithms.", "result": "The paper provides insights into systems for realizing trustworthy and efficient smart digital-twin industrial engineering ecosystem, including applications, middleware, and technological requirements from digital twin's inception to AI-enabled versions.", "conclusion": "The research discusses future directions for digital twin in predictive maintenance for industrial engineering, addressing the gap in comprehensive analysis of digital twin evolution and its role in modern maintenance practices."}}
{"id": "2509.23472", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23472", "abs": "https://arxiv.org/abs/2509.23472", "authors": ["Jiang-Xin Shi", "Wen-Da Wei", "Jin-Fei Qi", "Xuanyu Chen", "Tong Wei", "Yu-Feng Li"], "title": "Memory-Efficient Fine-Tuning via Low-Rank Activation Compression", "comment": null, "summary": "The parameter-efficient fine-tuning paradigm has garnered significant\nattention with the advancement of foundation models. Although numerous methods\nhave been proposed to reduce the number of trainable parameters, their\nsubstantial memory overhead remains a critical bottleneck that hinders\npractical deployment. In this paper, we observe that model activations\nconstitute a major source of memory consumption, especially under large batch\nsizes and long context lengths; however, the rank of the activations remains\nconsistently low. Motivated by this insight, we propose a memory-efficient\nfine-tuning approach Low-Rank Activation Compression (LoRAct). Unlike prior\nwork, LoRAct provides a more flexible and versatile compressing strategy that\ncan be applied online during the forward pass without the need for any\ncalibration data. Moreover, LoRAct incorporates a novel sampling-based\northogonal decomposition algorithm specifically designed for low-rank matrices,\noffering improved computational efficiency and a tighter error bound compared\nto the widely used RSVD. Experiments on both vision and language tasks\ndemonstrate the effectiveness of LoRAct. Notably, LoRAct further reduces\nactivation memory by approximately 80% in comparison with the widely adopted\nLoRA method, while maintaining competitive performance. The source code is\navailable at https://github.com/shijxcs/meft.", "AI": {"tldr": "LoRAct is a memory-efficient fine-tuning method that compresses activations using low-rank approximation, reducing activation memory by ~80% compared to LoRA while maintaining performance.", "motivation": "Parameter-efficient fine-tuning methods still have substantial memory overhead due to model activations, especially with large batch sizes and long contexts. The observation that activation ranks remain consistently low motivates compression.", "method": "Proposes Low-Rank Activation Compression (LoRAct) that applies online compression during forward pass without calibration data. Uses a novel sampling-based orthogonal decomposition algorithm for low-rank matrices with better computational efficiency and tighter error bounds than RSVD.", "result": "Experiments on vision and language tasks show LoRAct reduces activation memory by approximately 80% compared to LoRA while maintaining competitive performance.", "conclusion": "LoRAct provides an effective memory-efficient fine-tuning approach that significantly reduces activation memory consumption without compromising model performance."}}
{"id": "2509.24007", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24007", "abs": "https://arxiv.org/abs/2509.24007", "authors": ["Yangzhou Liu", "Yue Cao", "Hao Li", "Gen Luo", "Zhe Chen", "Weiyun Wang", "Xiaobo Liang", "Biqing Qi", "Lijun Wu", "Changyao Tian", "Yanting Zhang", "Yuqiang Li", "Tong Lu", "Yu Qiao", "Jifeng Dai", "Wenhai Wang"], "title": "Sequential Diffusion Language Models", "comment": "14 pages, 5 figures, technical report", "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM", "AI": {"tldr": "SDLM unifies next-token and next-block prediction through NSP, enabling adaptive generation length while maintaining KV-cache compatibility and achieving higher throughput than autoregressive models.", "motivation": "Address limitations of diffusion language models including fixed-length decoding, KV-cache incompatibility, and expensive training requirements of existing approaches like block diffusion.", "method": "Propose Next Sequence Prediction (NSP) that adaptively determines generation length per step, and Sequential Diffusion Language Model (SDLM) that retrofits pre-trained ALMs with diffusion inference in mask blocks while dynamically decoding based on model confidence.", "result": "SDLM matches or surpasses autoregressive baselines with only 3.5M training samples, achieves 2.1\u00d7 higher throughput than Qwen-2.5, and SDLM-32B shows even greater efficiency gains demonstrating scalability.", "conclusion": "SDLM provides an efficient and scalable approach that bridges diffusion and autoregressive modeling, enabling adaptive sequence generation while maintaining compatibility with existing infrastructure."}}
{"id": "2509.24460", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24460", "abs": "https://arxiv.org/abs/2509.24460", "authors": ["Haotian Zhang", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Likang Xiao", "Yanwei Ren", "Quan Chen", "Xianglong Liu"], "title": "ContextPRM: Leveraging Contextual Coherence for multi-domain Test-Time Scaling", "comment": null, "summary": "Process reward models (PRMs) have demonstrated significant efficacy in\nenhancing the mathematical reasoning capabilities of large language models\n(LLMs) by leveraging test-time scaling (TTS). However, while most PRMs exhibit\nsubstantial gains in mathematical domains, the scarcity of domain-specific\ntraining data and knowledge-based learning patterns limits their generalization\nability when faced with other domains. To address this limitation, we shift the\nlearning objective from verifying domain-specific knowledge to modeling\ndomain-agnostic logical flow. Centering on contextual coherence between\nchain-of-thought (CoT) steps, our approach is realized through a novel data\nannotation and training framework, which enhances the model's generalization\ncapabilities across diverse domains. For instance, our resulting model,\nContextPRM, achieves a notable 6.5% average accuracy improvement over the\nmajority voting baseline via weighted majority voting across nine\nnon-mathematical domains in MMLU-Pro, including law, history, and philosophy,\nsignificantly surpassing the 2.2% improvement from VersaPRM and 0.5% gains from\nother mathematics-focused PRMs, demonstrating consistent performance across\nboth mathematical and non-mathematical domains.", "AI": {"tldr": "ContextPRM improves PRM generalization by focusing on domain-agnostic logical flow instead of domain-specific knowledge, achieving 6.5% accuracy improvement across non-mathematical domains.", "motivation": "PRMs show strong performance in math but lack generalization to other domains due to domain-specific training data and knowledge-based learning patterns.", "method": "Shift learning objective to model domain-agnostic logical flow by focusing on contextual coherence between CoT steps, using novel data annotation and training framework.", "result": "ContextPRM achieves 6.5% average accuracy improvement over majority voting baseline across nine non-mathematical domains in MMLU-Pro, outperforming VersaPRM (2.2%) and other math-focused PRMs (0.5%).", "conclusion": "Focusing on contextual coherence in logical flow enables PRMs to generalize effectively across both mathematical and non-mathematical domains."}}
{"id": "2509.23474", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.23474", "abs": "https://arxiv.org/abs/2509.23474", "authors": ["Yahong Yang", "Wei Zhu"], "title": "Statistical Learning Guarantees for Group-Invariant Barron Functions", "comment": null, "summary": "We investigate the generalization error of group-invariant neural networks\nwithin the Barron framework. Our analysis shows that incorporating\ngroup-invariant structures introduces a group-dependent factor\n$\\delta_{G,\\Gamma,\\sigma} \\le 1$ into the approximation rate. When this factor\nis small, group invariance yields substantial improvements in approximation\naccuracy. On the estimation side, we establish that the Rademacher complexity\nof the group-invariant class is no larger than that of the non-invariant\ncounterpart, implying that the estimation error remains unaffected by the\nincorporation of symmetry. Consequently, the generalization error can improve\nsignificantly when learning functions with inherent group symmetries. We\nfurther provide illustrative examples demonstrating both favorable cases, where\n$\\delta_{G,\\Gamma,\\sigma}\\approx |G|^{-1}$, and unfavorable ones, where\n$\\delta_{G,\\Gamma,\\sigma}\\approx 1$. Overall, our results offer a rigorous\ntheoretical foundation showing that encoding group-invariant structures in\nneural networks leads to clear statistical advantages for symmetric target\nfunctions.", "AI": {"tldr": "Group-invariant neural networks show improved generalization error for symmetric functions, with group-dependent approximation improvements and unchanged estimation error.", "motivation": "To understand the theoretical benefits of incorporating group-invariant structures in neural networks for learning symmetric functions.", "method": "Analyze generalization error within Barron framework, examining approximation rates with group-dependent factors and Rademacher complexity comparisons.", "result": "Group invariance improves approximation accuracy (factor \u03b4 \u2264 1) without increasing estimation error, leading to better generalization for symmetric functions.", "conclusion": "Encoding group-invariant structures provides clear statistical advantages for symmetric target functions, with rigorous theoretical foundation."}}
{"id": "2509.24014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24014", "abs": "https://arxiv.org/abs/2509.24014", "authors": ["Zeqing Wang", "Gongfan Fang", "Xinyin Ma", "Xingyi Yang", "Xinchao Wang"], "title": "SparseD: Sparse Attention for Diffusion Language Models", "comment": "The code is available at https://github.com/INV-WZQ/SparseD", "summary": "While diffusion language models (DLMs) offer a promising alternative to\nautoregressive models (ARs), existing open-source DLMs suffer from high\ninference latency. This bottleneck is mainly due to the attention's quadratic\ncomplexity with respect to context length in computing all query-key pairs.\nIntuitively, to reduce this complexity, a natural strategy is to restrict\nattention to sparse patterns that retain only the most relevant connections.\nSuch approaches are well-established in ARs, where attention follows fixed and\nclearly defined sparse patterns. However, in DLMs, we observe distinct sparsity\nbehaviors: (1) attention patterns vary across heads, (2) attention patterns in\neach head remain highly similar across denoising steps, and (3) early denoising\nsteps are critical for generation. These findings render sparse attention\nmethods designed for ARs largely incompatible with DLMs, as they fail to\ncapture head-specific structures and risk degrading generation when applied in\nearly denoising steps. To address these challenges, we propose SparseD, a novel\nsparse attention method for DLMs. Leveraging the observations, SparseD only\nrequires pre-computing head-specific sparse patterns one time, and reuses them\nacross all steps. This prevents recomputing sparse patterns at each denoising\nstep. Meanwhile, SparseD uses full attention in the early steps, then switches\nto sparse attention later to maintain generation quality. Together, these\nestablish SparseD as a practical and efficient solution for deploying DLMs in\nlong-context applications. Experimental results demonstrate that SparseD\nachieves lossless acceleration, delivering up to $1.50\\times$ speedup over\nFlashAttention at a 64k context length with 1,024 denoising steps.", "AI": {"tldr": "SparseD is a novel sparse attention method for diffusion language models that achieves lossless acceleration by using head-specific sparse patterns and full attention in early denoising steps.", "motivation": "Existing open-source diffusion language models suffer from high inference latency due to attention's quadratic complexity with context length. Sparse attention methods designed for autoregressive models are incompatible with DLMs due to distinct sparsity behaviors.", "method": "SparseD pre-computes head-specific sparse patterns once and reuses them across all denoising steps. It uses full attention in early steps and switches to sparse attention later to maintain generation quality.", "result": "SparseD achieves up to 1.50\u00d7 speedup over FlashAttention at 64k context length with 1,024 denoising steps while maintaining lossless acceleration.", "conclusion": "SparseD provides a practical and efficient solution for deploying diffusion language models in long-context applications by addressing the unique sparsity characteristics of DLMs."}}
{"id": "2509.24489", "categories": ["cs.AI", "cs.LG", "cs.LO", "68T20, 68Q25", "I.2.8; F.2.2"], "pdf": "https://arxiv.org/pdf/2509.24489", "abs": "https://arxiv.org/abs/2509.24489", "authors": ["Vasileios Balafas", "Dimos Tsouros", "Nikolaos Ploskas", "Kostas Stergiou"], "title": "Overcoming Over-Fitting in Constraint Acquisition via Query-Driven Interactive Refinement", "comment": "Preprint. Uses the International Journal on Artificial Intelligence\n  Tools (World Scientific) template. Includes figures, tables, and algorithms.\n  Submitted to IJAIT", "summary": "Manual modeling in Constraint Programming is a substantial bottleneck, which\nConstraint Acquisition (CA) aims to automate. However, passive CA methods are\nprone to over-fitting, often learning models that include spurious global\nconstraints when trained on limited data, while purely active methods can be\nquery-intensive. We introduce a hybrid CA framework specifically designed to\naddress the challenge of over-fitting in CA. Our approach integrates passive\nlearning for initial candidate generation, a query-driven interactive\nrefinement phase that utilizes probabilistic confidence scores (initialized by\nmachine learning priors) to systematically identify over-fitted constraints,\nand a specialized subset exploration mechanism to recover valid substructures\nfrom rejected candidates. A final active learning phase ensures model\ncompleteness. Extensive experiments on diverse benchmarks demonstrate that our\ninteractive refinement phase is crucial for achieving high target model\ncoverage and overall model accuracy from limited examples, doing so with\nmanageable query complexity. This framework represents a substantial\nadvancement towards robust and practical constraint acquisition in data-limited\nscenarios.", "AI": {"tldr": "A hybrid constraint acquisition framework combining passive learning with interactive refinement to prevent over-fitting in data-limited scenarios, using probabilistic confidence scores and subset exploration.", "motivation": "To address the challenge of over-fitting in constraint acquisition, where passive methods learn spurious constraints from limited data and purely active methods are query-intensive.", "method": "Integrates passive learning for initial candidate generation, query-driven interactive refinement with probabilistic confidence scores, specialized subset exploration to recover valid substructures, and final active learning for model completeness.", "result": "Extensive experiments show the interactive refinement phase achieves high target model coverage and accuracy from limited examples with manageable query complexity.", "conclusion": "This framework represents a substantial advancement towards robust and practical constraint acquisition in data-limited scenarios."}}
{"id": "2509.23487", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23487", "abs": "https://arxiv.org/abs/2509.23487", "authors": ["Divyam Madaan", "Sumit Chopra", "Kyunghyun Cho"], "title": "Temporal Generalization: A Reality Check", "comment": null, "summary": "Machine learning (ML) models often struggle to maintain performance under\ndistribution shifts, leading to inaccurate predictions on unseen future data.\nIn this work, we investigate whether and under what conditions models can\nachieve such a generalization when relying solely on past data. We explore two\nprimary approaches: convex combinations of past model parameters\n(\\emph{parameter interpolation}) and explicit extrapolation beyond the convex\nhull of past parameters (\\emph{parameter extrapolation}). We benchmark several\nmethods within these categories on a diverse set of temporal tasks, including\nlanguage modeling, news summarization, news tag prediction, academic paper\ncategorization, satellite image-based land use classification over time, and\nhistorical yearbook photo gender prediction. Our empirical findings show that\nnone of the evaluated methods consistently outperforms the simple baseline of\nusing the latest available model parameters in all scenarios. In the absence of\naccess to future data or robust assumptions about the underlying\ndata-generating process, these results underscore the inherent difficulties of\ngeneralizing and extrapolating to future data and warrant caution when\nevaluating claims of such generalization.", "AI": {"tldr": "This paper investigates whether ML models can generalize to future data through parameter interpolation and extrapolation methods, but finds none consistently outperform simply using the latest model parameters.", "motivation": "Machine learning models often fail to maintain performance under distribution shifts, leading to inaccurate predictions on unseen future data. The research aims to determine if models can achieve such generalization using only past data.", "method": "The study explores two approaches: parameter interpolation (convex combinations of past model parameters) and parameter extrapolation (explicit extrapolation beyond the convex hull of past parameters). These methods are benchmarked on diverse temporal tasks including language modeling, news summarization, image classification, and more.", "result": "Empirical findings show that none of the evaluated methods consistently outperforms the simple baseline of using the latest available model parameters across all scenarios.", "conclusion": "In the absence of access to future data or robust assumptions about data-generating processes, the results highlight the inherent difficulties of generalizing to future data and warrant caution when evaluating claims of such generalization."}}
{"id": "2509.24074", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24074", "abs": "https://arxiv.org/abs/2509.24074", "authors": ["Hongbo Liu", "Jia Xu"], "title": "ResFormer: All-Time Reservoir Memory for Long Sequence Classification", "comment": "Accepted at EMNLP 2025. To appear in the proceedings", "summary": "Sequence classification is essential in NLP for understanding and\ncategorizing language patterns in tasks like sentiment analysis, intent\ndetection, and topic classification. Transformer-based models, despite\nachieving state-of-the-art performance, have inherent limitations due to\nquadratic time and memory complexity, restricting their input length. Although\nextensive efforts have aimed at reducing computational demands, processing\nextensive contexts remains challenging.\n  To overcome these limitations, we propose ResFormer, a novel neural network\narchitecture designed to model varying context lengths efficiently through a\ncascaded methodology. ResFormer integrates an reservoir computing network\nfeaturing a nonlinear readout to effectively capture long-term contextual\ndependencies in linear time. Concurrently, short-term dependencies within\nsentences are modeled using a conventional Transformer architecture with\nfixed-length inputs.\n  Experiments demonstrate that ResFormer significantly outperforms baseline\nmodels of DeepSeek-Qwen and ModernBERT, delivering an accuracy improvement of\nup to +22.3% on the EmoryNLP dataset and consistent gains on MultiWOZ, MELD,\nand IEMOCAP. In addition, ResFormer exhibits reduced memory consumption,\nunderscoring its effectiveness and efficiency in modeling extensive contextual\ninformation.", "AI": {"tldr": "ResFormer is a novel neural architecture that combines reservoir computing with Transformers to efficiently model varying context lengths, achieving significant accuracy improvements and reduced memory consumption compared to baseline models.", "motivation": "Transformer models have quadratic complexity limitations that restrict input length, making it challenging to process extensive contexts efficiently. The goal is to overcome these computational constraints while maintaining performance.", "method": "ResFormer uses a cascaded approach: reservoir computing with nonlinear readout for long-term dependencies in linear time, combined with conventional Transformers for short-term sentence dependencies with fixed-length inputs.", "result": "ResFormer outperforms DeepSeek-Qwen and ModernBERT with up to +22.3% accuracy improvement on EmoryNLP dataset, and shows consistent gains on MultiWOZ, MELD, and IEMOCAP. It also exhibits reduced memory consumption.", "conclusion": "ResFormer effectively addresses Transformer limitations by efficiently modeling extensive contextual information through its hybrid architecture, demonstrating both superior performance and computational efficiency."}}
{"id": "2509.24495", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24495", "abs": "https://arxiv.org/abs/2509.24495", "authors": ["Mateusz \u017barski", "S\u0142awomir Nowaczyk"], "title": "Neuroplasticity-inspired dynamic ANNs for multi-task demand forecasting", "comment": "14 pages, 3 figures, 2 tables", "summary": "This paper introduces a novel approach to Dynamic Artificial Neural Networks\n(D-ANNs) for multi-task demand forecasting called Neuroplastic Multi-Task\nNetwork (NMT-Net). Unlike conventional methods focusing on inference-time\ndynamics or computational efficiency, our proposed method enables structural\nadaptability of the computational graph during training, inspired by\nneuroplasticity as seen in biological systems. Each new task triggers a dynamic\nnetwork adaptation, including similarity-based task identification and\nselective training of candidate ANN heads, which are then assessed and\nintegrated into the model based on their performance. We evaluated our\nframework using three real-world multi-task demand forecasting datasets from\nKaggle. We demonstrated its superior performance and consistency, achieving\nlower RMSE and standard deviation compared to traditional baselines and\nstate-of-the-art multi-task learning methods. NMT-Net offers a scalable,\nadaptable solution for multi-task and continual learning in time series\nprediction. The complete code for NMT-Net is available from our GitHub\nrepository.", "AI": {"tldr": "NMT-Net is a neuroplastic multi-task network that dynamically adapts its structure during training for multi-task demand forecasting, achieving better performance than traditional methods.", "motivation": "To create a more adaptable and scalable solution for multi-task demand forecasting by enabling structural adaptability during training, inspired by biological neuroplasticity.", "method": "Uses similarity-based task identification and selective training of candidate ANN heads, with dynamic network adaptation triggered by new tasks during training.", "result": "Achieved lower RMSE and standard deviation compared to traditional baselines and state-of-the-art multi-task learning methods on three real-world datasets.", "conclusion": "NMT-Net provides a scalable, adaptable solution for multi-task and continual learning in time series prediction with demonstrated superior performance."}}
{"id": "2509.23494", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23494", "abs": "https://arxiv.org/abs/2509.23494", "authors": ["Jie Yang", "Yifan Hu", "Kexin Zhang", "Luyang Niu", "Yushun Dong", "Philip S. Yu", "Kaize Ding"], "title": "Revisiting Multivariate Time Series Forecasting with Missing Values", "comment": null, "summary": "Missing values are common in real-world time series, and multivariate time\nseries forecasting with missing values (MTSF-M) has become a crucial area of\nresearch for ensuring reliable predictions. To address the challenge of missing\ndata, current approaches have developed an imputation-then-prediction framework\nthat uses imputation modules to fill in missing values, followed by forecasting\non the imputed data. However, this framework overlooks a critical issue: there\nis no ground truth for the missing values, making the imputation process\nsusceptible to errors that can degrade prediction accuracy. In this paper, we\nconduct a systematic empirical study and reveal that imputation without direct\nsupervision can corrupt the underlying data distribution and actively degrade\nprediction accuracy. To address this, we propose a paradigm shift that moves\naway from imputation and directly predicts from the partially observed time\nseries. We introduce Consistency-Regularized Information Bottleneck (CRIB), a\nnovel framework built on the Information Bottleneck principle. CRIB combines a\nunified-variate attention mechanism with a consistency regularization scheme to\nlearn robust representations that filter out noise introduced by missing values\nwhile preserving essential predictive signals. Comprehensive experiments on\nfour real-world datasets demonstrate the effectiveness of CRIB, which predicts\naccurately even under high missing rates. Our code is available in\nhttps://github.com/Muyiiiii/CRIB.", "AI": {"tldr": "CRIB is a novel framework that directly predicts from partially observed time series without imputation, using Information Bottleneck principle and consistency regularization to handle missing values in multivariate time series forecasting.", "motivation": "Traditional imputation-then-prediction approaches are problematic because there's no ground truth for missing values, making imputation error-prone and potentially degrading prediction accuracy by corrupting the underlying data distribution.", "method": "Proposes Consistency-Regularized Information Bottleneck (CRIB) framework with unified-variate attention mechanism and consistency regularization to learn robust representations that filter out noise from missing values while preserving predictive signals.", "result": "Comprehensive experiments on four real-world datasets show CRIB effectively predicts accurately even under high missing rates, outperforming traditional imputation-based approaches.", "conclusion": "CRIB represents a paradigm shift from imputation-based methods to direct prediction from incomplete data, providing a more robust solution for multivariate time series forecasting with missing values."}}
{"id": "2509.24080", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.24080", "abs": "https://arxiv.org/abs/2509.24080", "authors": ["Meysam Shirdel Bilehsavar", "Negin Mahmoudi", "Mohammad Jalili Torkamani", "Kiana Kiashemshaki"], "title": "Ensembling Multilingual Transformers for Robust Sentiment Analysis of Tweets", "comment": "19 pages, 4 figures, 2 tables", "summary": "Sentiment analysis is a very important natural language processing activity\nin which one identifies the polarity of a text, whether it conveys positive,\nnegative, or neutral sentiment. Along with the growth of social media and the\nInternet, the significance of sentiment analysis has grown across numerous\nindustries such as marketing, politics, and customer service. Sentiment\nanalysis is flawed, however, when applied to foreign languages, particularly\nwhen there is no labelled data to train models upon. In this study, we present\na transformer ensemble model and a large language model (LLM) that employs\nsentiment analysis of other languages. We used multi languages dataset.\nSentiment was then assessed for sentences using an ensemble of pre-trained\nsentiment analysis models: bert-base-multilingual-uncased-sentiment, and XLM-R.\nOur experimental results indicated that sentiment analysis performance was more\nthan 86% using the proposed method.", "AI": {"tldr": "This paper proposes a transformer ensemble model and LLM approach for multilingual sentiment analysis, achieving over 86% accuracy on multi-language datasets.", "motivation": "Sentiment analysis faces challenges with foreign languages, especially when labeled training data is unavailable, limiting its effectiveness across different languages.", "method": "Used an ensemble of pre-trained sentiment analysis models (bert-base-multilingual-uncased-sentiment and XLM-R) on multi-language datasets to assess sentence sentiment.", "result": "Experimental results showed sentiment analysis performance exceeding 86% using the proposed ensemble method.", "conclusion": "The transformer ensemble model with LLM approach effectively addresses multilingual sentiment analysis challenges and achieves high accuracy without requiring labeled training data for each language."}}
{"id": "2509.24509", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24509", "abs": "https://arxiv.org/abs/2509.24509", "authors": ["Yihong Liu", "Junyi Li", "Wayne Xin Zhao", "Hongyu Lu", "Ji-Rong Wen"], "title": "Experience-guided reflective co-evolution of prompts and heuristics for automatic algorithm design", "comment": null, "summary": "Combinatorial optimization problems are traditionally tackled with\nhandcrafted heuristic algorithms, which demand extensive domain expertise and\nsignificant implementation effort. Recent progress has highlighted the\npotential of automatic heuristics design powered by large language models\n(LLMs), enabling the automatic generation and refinement of heuristics. These\napproaches typically maintain a population of heuristics and employ LLMs as\nmutation operators to evolve them across generations. While effective, such\nmethods often risk stagnating in local optima. To address this issue, we\npropose the Experience-Guided Reflective Co-Evolution of Prompt and Heuristics\n(EvoPH) for automatic algorithm design, a novel framework that integrates the\nisland migration model with the elites selection algorithm to simulate diverse\nheuristics populations. In EvoPH, prompts are co-evolved with heuristic\nalgorithms, guided by performance feedback. We evaluate our framework on two\nproblems, i.e., Traveling Salesman Problem and Bin Packing Problem.\nExperimental results demonstrate that EvoPH achieves the lowest relative error\nagainst optimal solutions across both datasets, advancing the field of\nautomatic algorithm design with LLMs.", "AI": {"tldr": "EvoPH is a novel framework that co-evolves prompts and heuristic algorithms using LLMs, integrating island migration and elite selection to avoid local optima in automatic algorithm design for combinatorial optimization problems.", "motivation": "Traditional heuristic algorithms require extensive domain expertise and implementation effort, while existing LLM-based approaches risk stagnating in local optima during automatic heuristics design.", "method": "Proposes Experience-Guided Reflective Co-Evolution of Prompt and Heuristics (EvoPH) framework that integrates island migration model with elite selection algorithm to simulate diverse heuristics populations, co-evolving prompts and heuristic algorithms guided by performance feedback.", "result": "EvoPH achieves the lowest relative error against optimal solutions on Traveling Salesman Problem and Bin Packing Problem datasets compared to other methods.", "conclusion": "EvoPH advances the field of automatic algorithm design with LLMs by effectively avoiding local optima and generating high-quality heuristics for combinatorial optimization problems."}}
{"id": "2509.23500", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23500", "abs": "https://arxiv.org/abs/2509.23500", "authors": ["Georgios Vlassis", "Saleh Ashkboos", "Alexandra Volkova", "Torsten Hoefler", "Dan Alistarh"], "title": "Beyond Outliers: A Study of Optimizers Under Quantization", "comment": "20 pages", "summary": "As new optimizers gain traction and model quantization becomes standard for\nefficient deployment, a key question arises: how does the choice of optimizer\naffect model performance in the presence of quantization? Despite progress in\nboth areas, systematic evidence on optimizer-quantization interactions remains\nlimited. To fill this gap, we study the impact of optimizer choice on model\nrobustness under quantization, considering both post-training quantization\n(PTQ), and quantization-aware training (QAT). We first train full-precision\nmodels, ranging from 50M to 1.5B parameters, with six optimizers, to explore\nthe hyperparameter landscape, and establish well-tuned baselines. We then apply\nPTQ to evaluate how model performance degrades when trained with different\noptimizers. We find that outlier-related metrics, such as the max-to-mean ratio\n(MMR) and Kurtosis, fail to predict the PTQ performance across different\noptimizers. We show analytically that this is due to the MMR capturing only\nisolated layer errors, while ignoring how quantization errors accumulate and\npropagate through the network. To study the QAT degradation, we train quantized\nmodels from scratch and compare them to our original-precision baselines. We\nfind that optimizers performing well in the original pretraining setup may not\nremain optimal under QAT, and that models trained with Shampoo show the lowest\naccuracy degradation. Finally, we derive scaling laws for quantization-aware\ntraining under different optimizers, showing that Shampoo achieves the highest\nparameter efficiency of all tested optimizers.", "AI": {"tldr": "This paper systematically studies how optimizer choice affects model robustness under quantization (both PTQ and QAT), finding that traditional outlier metrics fail to predict PTQ performance and that Shampoo optimizer shows the best quantization robustness and parameter efficiency.", "motivation": "To understand how optimizer choice impacts model performance when quantization is applied, as systematic evidence on optimizer-quantization interactions is limited despite progress in both areas.", "method": "Train full-precision models (50M-1.5B parameters) with six optimizers, apply PTQ to evaluate performance degradation, analyze why outlier metrics fail, conduct QAT from scratch, and derive scaling laws for quantization-aware training.", "result": "Outlier metrics like MMR and Kurtosis fail to predict PTQ performance across optimizers; Shampoo optimizer shows lowest accuracy degradation under QAT and achieves highest parameter efficiency according to scaling laws.", "conclusion": "Optimizer choice significantly affects quantization robustness, with Shampoo performing best under QAT, and traditional outlier metrics are insufficient for predicting quantization performance due to error accumulation effects."}}
{"id": "2509.24090", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24090", "abs": "https://arxiv.org/abs/2509.24090", "authors": ["Matteo Boffa", "Jiaxuan You"], "title": "Large-Scale Constraint Generation -- Can LLMs Parse Hundreds of Constraints?", "comment": null, "summary": "Recent research has explored the constrained generation capabilities of Large\nLanguage Models (LLMs) when explicitly prompted by few task-specific\nrequirements. In contrast, we introduce Large-Scale Constraint Generation\n(LSCG), a new problem that evaluates whether LLMs can parse a large,\nfine-grained, generic list of constraints. To examine the LLMs' ability to\nhandle an increasing number constraints, we create a practical instance of\nLSCG, called Words Checker. In Words Checker, we evaluate the impact of model\ncharacteristics (e.g., size, family) and steering techniques (e.g., Simple\nPrompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet,\na small and dedicated model that parses the original list of constraints into a\nsmaller subset, helping the LLM focus on relevant constraints. Experiments\nreveal that existing solutions suffer a significant performance drop as the\nnumber of constraints increases, with FoCusNet showing an 8-13% accuracy boost.", "AI": {"tldr": "The paper introduces Large-Scale Constraint Generation (LSCG) to test LLMs' ability to handle large, fine-grained constraint lists, proposes FoCusNet to improve performance by filtering relevant constraints, and shows 8-13% accuracy improvements.", "motivation": "To evaluate whether LLMs can effectively parse and handle large-scale, fine-grained constraint lists, addressing the performance degradation as constraint numbers increase.", "method": "Created Words Checker as a practical LSCG instance, tested model characteristics and steering techniques, and proposed FoCusNet - a small model that filters constraints to help LLMs focus on relevant ones.", "result": "Existing solutions suffer significant performance drops with increasing constraints, while FoCusNet achieves 8-13% accuracy improvement over baseline methods.", "conclusion": "FoCusNet effectively addresses the challenge of large-scale constraint processing in LLMs, demonstrating substantial performance gains in constraint-heavy scenarios."}}
{"id": "2509.24527", "categories": ["cs.AI", "cs.LG", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24527", "abs": "https://arxiv.org/abs/2509.24527", "authors": ["Danijar Hafner", "Wilson Yan", "Timothy Lillicrap"], "title": "Training Agents Inside of Scalable World Models", "comment": "Website: https://danijar.com/dreamer4/", "summary": "World models learn general knowledge from videos and simulate experience for\ntraining behaviors in imagination, offering a path towards intelligent agents.\nHowever, previous world models have been unable to accurately predict object\ninteractions in complex environments. We introduce Dreamer 4, a scalable agent\nthat learns to solve control tasks by reinforcement learning inside of a fast\nand accurate world model. In the complex video game Minecraft, the world model\naccurately predicts object interactions and game mechanics, outperforming\nprevious world models by a large margin. The world model achieves real-time\ninteractive inference on a single GPU through a shortcut forcing objective and\nan efficient transformer architecture. Moreover, the world model learns general\naction conditioning from only a small amount of data, allowing it to extract\nthe majority of its knowledge from diverse unlabeled videos. We propose the\nchallenge of obtaining diamonds in Minecraft from only offline data, aligning\nwith practical applications such as robotics where learning from environment\ninteraction can be unsafe and slow. This task requires choosing sequences of\nover 20,000 mouse and keyboard actions from raw pixels. By learning behaviors\nin imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft\npurely from offline data, without environment interaction. Our work provides a\nscalable recipe for imagination training, marking a step towards intelligent\nagents.", "AI": {"tldr": "Dreamer 4 is a scalable agent that learns to solve control tasks through reinforcement learning in an accurate world model, achieving the first diamond collection in Minecraft from purely offline data without environment interaction.", "motivation": "Previous world models struggled with accurately predicting object interactions in complex environments. The work aims to develop intelligent agents that can learn from imagination training rather than direct environment interaction, which is important for practical applications like robotics where real-world interaction can be unsafe and slow.", "method": "Dreamer 4 uses a fast and accurate world model with a shortcut forcing objective and efficient transformer architecture for real-time interactive inference. It learns general action conditioning from small amounts of data and extracts most knowledge from diverse unlabeled videos. The agent learns behaviors through reinforcement learning inside the world model.", "result": "In Minecraft, the world model accurately predicts object interactions and game mechanics, outperforming previous world models by a large margin. Dreamer 4 becomes the first agent to obtain diamonds in Minecraft purely from offline data, successfully choosing sequences of over 20,000 mouse and keyboard actions from raw pixels.", "conclusion": "The work provides a scalable recipe for imagination training and marks a step towards intelligent agents that can learn complex behaviors without direct environment interaction, demonstrating the potential for applications where real-world training is impractical or unsafe."}}
{"id": "2509.23548", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23548", "abs": "https://arxiv.org/abs/2509.23548", "authors": ["Yijie Zhang", "Yiyang Shen", "Weiran Wang"], "title": "Disentanglement of Variations with Multimodal Generative Modeling", "comment": "22 pages, 14 figures, 7 tables", "summary": "Multimodal data are prevalent across various domains, and learning robust\nrepresentations of such data is paramount to enhancing generation quality and\ndownstream task performance. To handle heterogeneity and interconnections among\ndifferent modalities, recent multimodal generative models extract shared and\nprivate (modality-specific) information with two separate variables. Despite\nattempts to enforce disentanglement between these two variables, these methods\nstruggle with challenging datasets where the likelihood model is insufficient.\nIn this paper, we propose Information-disentangled Multimodal VAE (IDMVAE) to\nexplicitly address this issue, with rigorous mutual information-based\nregularizations, including cross-view mutual information maximization for\nextracting shared variables, and a cycle-consistency style loss for redundancy\nremoval using generative augmentations. We further introduce diffusion models\nto improve the capacity of latent priors. These newly proposed components are\ncomplementary to each other. Compared to existing approaches, IDMVAE shows a\nclean separation between shared and private information, demonstrating superior\ngeneration quality and semantic coherence on challenging datasets.", "AI": {"tldr": "IDMVAE is a multimodal VAE that uses mutual information regularization and diffusion models to better disentangle shared and private information across modalities, improving generation quality.", "motivation": "Existing multimodal generative models struggle to properly disentangle shared and private information, especially on challenging datasets where likelihood models are insufficient.", "method": "Proposes mutual information-based regularizations including cross-view mutual information maximization and cycle-consistency loss, combined with diffusion models for better latent priors.", "result": "IDMVAE achieves cleaner separation between shared and private information, demonstrating superior generation quality and semantic coherence on challenging datasets.", "conclusion": "The proposed components are complementary and effectively address the disentanglement problem in multimodal representation learning."}}
{"id": "2509.24096", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24096", "abs": "https://arxiv.org/abs/2509.24096", "authors": ["Kaiyu He", "Peilin Wu", "Mian Zhang", "Kun Wan", "Wentian Zhao", "Xinya Du", "Zhiyu Chen"], "title": "GEAR: A General Evaluation Framework for Abductive Reasoning", "comment": "Coda and Data:\n  https://github.com/KaiyuHe998/GEAR-Abduction_evaluation", "summary": "Since the advent of large language models (LLMs), research has focused on\ninstruction following and deductive reasoning. A central question remains: can\nthese models discover new knowledge, and how can we evaluate this ability? We\naddress this by studying abductive reasoning-the generation of plausible\nhypotheses to explain observations-and introduce GEAR (General Evaluation for\nAbductive Reasoning), a general-purpose, fully automated, transparent, and\nlabel-free evaluation paradigm. GEAR scores hypothesis sets by three metrics:\nconsistency (each hypothesis explains the observations), generalizability\n(consistent hypotheses make meaningful predictions on unseen inputs), and\ndiversity (the set covers distinct predictions and patterns). Built this way,\nGEAR is scalable (no human gold answers), reliable (deterministic scoring\naligned with classical abduction), and open-ended (scores improve only when\nmodels produce new plausible hypotheses, unlike static benchmarks that saturate\nonce accuracy is high). Using GEAR, we conduct a fine-grained study of nine\nLLMs on four abduction benchmarks with 1,500 problems, generating over 50,000\ncandidate hypotheses and revealing model differences obscured by gold-answer or\npurely human evaluations. We further propose a momentum-based curriculum that\nadjusts GEAR-derived training data by learning velocity: it starts with what\nthe model learns quickly and shifts toward harder objectives such as generating\ndiverse hypotheses once the model is confident on foundational objectives.\nWithout gold-label supervision, this strategy improves all GEAR objectives and\nthese gains transfer to established abductive reasoning benchmarks. Taken\ntogether, GEAR provides a principled framework that evaluates abduction and\nsupplies label-free, scalable training signals that help LLMs produce more\ndiverse and reliable hypotheses.", "AI": {"tldr": "GEAR is a novel evaluation framework for assessing LLMs' abductive reasoning abilities through automated scoring of hypothesis sets based on consistency, generalizability, and diversity, without requiring human-labeled answers.", "motivation": "To address whether LLMs can discover new knowledge through abductive reasoning (generating plausible hypotheses from observations) and develop a scalable, transparent evaluation method that doesn't rely on human gold answers.", "method": "Introduces GEAR framework with three scoring metrics: consistency (hypotheses explain observations), generalizability (hypotheses predict unseen inputs), and diversity (coverage of distinct predictions). Uses this to evaluate 9 LLMs on 1,500 abduction problems, generating 50,000+ hypotheses. Also proposes momentum-based curriculum training that adjusts training data based on learning velocity.", "result": "GEAR reveals model differences obscured by traditional evaluations. The momentum-based curriculum improves all GEAR objectives and transfers gains to established abduction benchmarks, enabling LLMs to produce more diverse and reliable hypotheses without gold-label supervision.", "conclusion": "GEAR provides a principled framework for evaluating abduction and supplies scalable, label-free training signals that help LLMs generate more diverse and reliable hypotheses, advancing their knowledge discovery capabilities."}}
{"id": "2509.24592", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24592", "abs": "https://arxiv.org/abs/2509.24592", "authors": ["Josip Tomo Licardo", "Nikola Tankovic", "Darko Etinger"], "title": "BPMN Assistant: An LLM-Based Approach to Business Process Modeling", "comment": "12 pages, 4 figures", "summary": "This paper presents BPMN Assistant, a tool that leverages Large Language\nModels (LLMs) for natural language-based creation and editing of BPMN diagrams.\nA specialized JSON-based representation is introduced as a structured\nalternative to the direct handling of XML to enhance the accuracy of process\nmodifications. Process generation quality is evaluated using Graph Edit\nDistance (GED) and Relative Graph Edit Distance (RGED), while editing\nperformance is evaluated with a binary success metric. Results show that JSON\nand XML achieve similar similarity scores in generation, but JSON offers\ngreater reliability, faster processing, and significantly higher editing\nsuccess rates. We discuss key trade-offs, limitations, and future improvements.\nThe implementation is available at https://github.com/jtlicardo/bpmn-assistant.", "AI": {"tldr": "BPMN Assistant is a tool that uses LLMs for natural language-based creation and editing of BPMN diagrams, using a specialized JSON representation instead of XML for better accuracy and reliability.", "motivation": "To enable more intuitive natural language-based creation and editing of BPMN diagrams using LLMs, while addressing the limitations of direct XML handling for process modifications.", "method": "Introduces a specialized JSON-based representation as a structured alternative to XML, evaluates generation quality using Graph Edit Distance (GED) and Relative Graph Edit Distance (RGED), and assesses editing performance with a binary success metric.", "result": "JSON and XML achieve similar similarity scores in generation, but JSON offers greater reliability, faster processing, and significantly higher editing success rates.", "conclusion": "The JSON-based approach provides better reliability and editing performance compared to XML, with discussions on trade-offs, limitations, and future improvements. The tool is publicly available."}}
{"id": "2509.23552", "categories": ["cs.LG", "cs.AI", "q-bio.GN", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.23552", "abs": "https://arxiv.org/abs/2509.23552", "authors": ["Md. Saiful Bari Siddiqui", "Nowshin Tarannum"], "title": "Fusing Sequence Motifs and Pan-Genomic Features: Antimicrobial Resistance Prediction using an Explainable Lightweight 1D CNN-XGBoost Ensemble", "comment": "Submitted to SCA/HPCAsia 2026. This preprint version has been\n  prepared for open-access distribution and may differ in formatting from the\n  official proceedings. Also available on bioRxiv for visibility to the life\n  sciences community", "summary": "Antimicrobial Resistance (AMR) is a rapidly escalating global health crisis.\nWhile genomic sequencing enables rapid prediction of resistance phenotypes,\ncurrent computational methods have limitations. Standard machine learning\nmodels treat the genome as an unordered collection of features, ignoring the\nsequential context of Single Nucleotide Polymorphisms (SNPs). State-of-the-art\nsequence models like Transformers are often too data-hungry and computationally\nexpensive for the moderately-sized datasets that are typical in this domain. To\naddress these challenges, we propose AMR-EnsembleNet, an ensemble framework\nthat synergistically combines sequence-based and feature-based learning. We\ndeveloped a lightweight, custom 1D Convolutional Neural Network (CNN) to\nefficiently learn predictive sequence motifs from high-dimensional SNP data.\nThis sequence-aware model was ensembled with an XGBoost model, a powerful\ngradient boosting system adept at capturing complex, non-local feature\ninteractions. We trained and evaluated our framework on a benchmark dataset of\n809 E. coli strains, predicting resistance across four antibiotics with varying\nclass imbalance. Our 1D CNN-XGBoost ensemble consistently achieved top-tier\nperformance across all the antibiotics, reaching a Matthews Correlation\nCoefficient (MCC) of 0.926 for Ciprofloxacin (CIP) and the highest Macro\nF1-score of 0.691 for the challenging Gentamicin (GEN) AMR prediction. We also\nshow that our model consistently focuses on SNPs within well-known AMR genes\nlike fusA and parC, confirming it learns the correct genetic signals for\nresistance. Our work demonstrates that fusing a sequence-aware 1D CNN with a\nfeature-based XGBoost model creates a powerful ensemble, overcoming the\nlimitations of using either an order-agnostic or a standalone sequence model.", "AI": {"tldr": "AMR-EnsembleNet: An ensemble framework combining 1D CNN for sequence motif learning and XGBoost for feature interactions, achieving superior AMR prediction performance on E. coli strains.", "motivation": "Address limitations of current AMR prediction methods that ignore SNP sequential context or require large datasets, by creating an efficient ensemble approach for moderately-sized genomic datasets.", "method": "Developed a lightweight 1D CNN to learn sequence motifs from SNP data, ensembled with XGBoost to capture complex feature interactions, trained on 809 E. coli strains across four antibiotics.", "result": "Achieved MCC of 0.926 for Ciprofloxacin and highest Macro F1-score of 0.691 for Gentamicin, with model focusing on known AMR genes like fusA and parC.", "conclusion": "Fusing sequence-aware 1D CNN with feature-based XGBoost creates a powerful ensemble that overcomes limitations of order-agnostic or standalone sequence models for AMR prediction."}}
{"id": "2509.24101", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24101", "abs": "https://arxiv.org/abs/2509.24101", "authors": ["Zsolt T. Kardkov\u00e1cs", "Lynda Djennane", "Anna Field", "Boualem Benatallah", "Yacine Gaci", "Fabio Casati", "Walid Gaaloul"], "title": "BTC-SAM: Leveraging LLMs for Generation of Bias Test Cases for Sentiment Analysis Models", "comment": "Accepted at EMNLP 2025 main conference", "summary": "Sentiment Analysis (SA) models harbor inherent social biases that can be\nharmful in real-world applications. These biases are identified by examining\nthe output of SA models for sentences that only vary in the identity groups of\nthe subjects. Constructing natural, linguistically rich, relevant, and diverse\nsets of sentences that provide sufficient coverage over the domain is\nexpensive, especially when addressing a wide range of biases: it requires\ndomain experts and/or crowd-sourcing. In this paper, we present a novel bias\ntesting framework, BTC-SAM, which generates high-quality test cases for bias\ntesting in SA models with minimal specification using Large Language Models\n(LLMs) for the controllable generation of test sentences. Our experiments show\nthat relying on LLMs can provide high linguistic variation and diversity in the\ntest sentences, thereby offering better test coverage compared to base\nprompting methods even for previously unseen biases.", "AI": {"tldr": "BTC-SAM is a novel bias testing framework that uses Large Language Models to generate high-quality test cases for identifying social biases in Sentiment Analysis models, providing better linguistic variation and test coverage compared to traditional methods.", "motivation": "Sentiment Analysis models contain harmful social biases that are difficult to detect comprehensively due to the high cost and effort required to create diverse, natural test sentences covering various identity groups and bias types.", "method": "The framework uses Large Language Models for controllable generation of test sentences, enabling creation of linguistically rich and diverse test cases with minimal specification, even for previously unseen biases.", "result": "Experiments show that LLM-based generation provides higher linguistic variation and diversity in test sentences, offering better test coverage compared to base prompting methods.", "conclusion": "BTC-SAM demonstrates that LLMs can effectively generate comprehensive bias test cases for Sentiment Analysis models, making bias testing more accessible and thorough."}}
{"id": "2509.24616", "categories": ["cs.AI", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.24616", "abs": "https://arxiv.org/abs/2509.24616", "authors": ["Gabriel Bathie", "Nathana\u00ebl Fijalkow", "Th\u00e9o Matricon", "Baptiste Mouillon", "Pierre Vandenhove"], "title": "LTL$_f$ Learning Meets Boolean Set Cover", "comment": "23 pages, 4 figures", "summary": "Learning formulas in Linear Temporal Logic (LTLf) from finite traces is a\nfundamental research problem which has found applications in artificial\nintelligence, software engineering, programming languages, formal methods,\ncontrol of cyber-physical systems, and robotics. We implement a new CPU tool\ncalled Bolt improving over the state of the art by learning formulas more than\n100x faster over 70% of the benchmarks, with smaller or equal formulas in 98%\nof the cases. Our key insight is to leverage a problem called Boolean Set Cover\nas a subroutine to combine existing formulas using Boolean connectives. Thanks\nto the Boolean Set Cover component, our approach offers a novel trade-off\nbetween efficiency and formula size.", "AI": {"tldr": "Bolt is a new CPU tool for learning Linear Temporal Logic (LTLf) formulas from finite traces, achieving 100x faster learning speeds on 70% of benchmarks while producing smaller or equal formulas in 98% of cases.", "motivation": "Learning LTLf formulas from finite traces is fundamental for applications in AI, software engineering, formal methods, cyber-physical systems, and robotics, requiring more efficient and compact formula learning methods.", "method": "The approach leverages Boolean Set Cover as a subroutine to combine existing formulas using Boolean connectives, offering a novel trade-off between efficiency and formula size.", "result": "Bolt improves over state-of-the-art by learning formulas more than 100x faster over 70% of benchmarks, with smaller or equal formulas in 98% of cases.", "conclusion": "The Boolean Set Cover component enables a novel efficiency-formula size trade-off, making Bolt a significant advancement in LTLf formula learning from finite traces."}}
{"id": "2509.23570", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23570", "abs": "https://arxiv.org/abs/2509.23570", "authors": ["Ruiqi Lyu", "Alistair Turcan", "Martin Jinye Zhang", "Bryan Wilder"], "title": "Improving constraint-based discovery with robust propagation and reliable LLM priors", "comment": null, "summary": "Learning causal structure from observational data is central to scientific\nmodeling and decision-making. Constraint-based methods aim to recover\nconditional independence (CI) relations in a causal directed acyclic graph\n(DAG). Classical approaches such as PC and subsequent methods orient\nv-structures first and then propagate edge directions from these seeds,\nassuming perfect CI tests and exhaustive search of separating subsets --\nassumptions often violated in practice, leading to cascading errors in the\nfinal graph. Recent work has explored using large language models (LLMs) as\nexperts, prompting sets of nodes for edge directions, and could augment edge\norientation when assumptions are not met. However, such methods implicitly\nassume perfect experts, which is unrealistic for hallucination-prone LLMs. We\npropose MosaCD, a causal discovery method that propagates edges from a\nhigh-confidence set of seeds derived from both CI tests and LLM annotations. To\nfilter hallucinations, we introduce shuffled queries that exploit LLMs'\npositional bias, retaining only high-confidence seeds. We then apply a novel\nconfidence-down propagation strategy that orients the most reliable edges\nfirst, and can be integrated with any skeleton-based discovery method. Across\nmultiple real-world graphs, MosaCD achieves higher accuracy in final graph\nconstruction than existing constraint-based methods, largely due to the\nimproved reliability of initial seeds and robust propagation strategies.", "AI": {"tldr": "MosaCD is a causal discovery method that combines CI tests and LLM annotations to create high-confidence seed edges, then uses confidence-down propagation to build more accurate causal graphs than existing methods.", "motivation": "Traditional constraint-based methods like PC rely on perfect CI tests and exhaustive search, which often fail in practice leading to cascading errors. LLMs can help but are prone to hallucinations, requiring careful filtering.", "method": "MosaCD propagates edges from high-confidence seeds derived from both CI tests and LLM annotations. It uses shuffled queries to filter LLM hallucinations and applies confidence-down propagation that orients the most reliable edges first.", "result": "Across multiple real-world graphs, MosaCD achieves higher accuracy in final graph construction than existing constraint-based methods.", "conclusion": "MosaCD improves causal discovery by combining the strengths of CI tests and LLMs while mitigating their weaknesses through careful seed selection and robust propagation strategies."}}
{"id": "2509.24102", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24102", "abs": "https://arxiv.org/abs/2509.24102", "authors": ["Guangliang Liu", "Xi Chen", "Bocheng Chen", "Xitong Zhang", "Kristen Johnson"], "title": "Pragmatic Inference for Moral Reasoning Acquisition: Generalization via Distributional Semantics", "comment": null, "summary": "Moral reasoning has emerged as a promising research direction for Large\nLanguage Models (LLMs), yet achieving generalization remains a central\nchallenge. From a linguistic standpoint, this difficulty arises because LLMs\nare adept at capturing distributional semantics, which fundamentally differs\nfrom the morals which operate at the pragmatic level. This paper investigates\nhow LLMs can achieve generalized moral reasoning despite their reliance on\ndistributional semantics. We propose pragmatic inference methods grounded in\nmoral foundations theory, which leverage contextual information at each step to\nbridge the pragmatic gap and guide LLMs in connecting moral foundations with\nmoral reasoning objectives. Experimental results demonstrate that our approach\nsignificantly enhances LLMs' generalization in moral reasoning, providing a\nfoundation for future research grounded in moral foundations theory.", "AI": {"tldr": "This paper addresses the challenge of achieving generalized moral reasoning in LLMs by proposing pragmatic inference methods based on moral foundations theory to bridge the gap between distributional semantics and pragmatic-level moral reasoning.", "motivation": "LLMs struggle with generalizing moral reasoning because they operate at the distributional semantics level while morals function at the pragmatic level, creating a fundamental gap that needs to be addressed.", "method": "Proposed pragmatic inference methods grounded in moral foundations theory that leverage contextual information at each step to connect moral foundations with moral reasoning objectives.", "result": "Experimental results show that the approach significantly enhances LLMs' generalization capabilities in moral reasoning tasks.", "conclusion": "The method provides a foundation for future research on moral reasoning in LLMs based on moral foundations theory, successfully bridging the pragmatic gap between distributional semantics and moral reasoning."}}
{"id": "2509.24651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24651", "abs": "https://arxiv.org/abs/2509.24651", "authors": ["Nikolaos Kondylidis", "Andrea Rafanelli", "Ilaria Tiddi", "Annette ten Teije", "Frank van Harmelen"], "title": "\"Stop replacing salt with sugar!'': Towards Intuitive Human-Agent Teaching", "comment": "22nd European Conference on Multi-Agent Systems (EUMAS 2025)", "summary": "Humans quickly learn new concepts from a small number of examples.\nReplicating this capacity with Artificial Intelligence (AI) systems has proven\nto be challenging. When it comes to learning subjective tasks-where there is an\nevident scarcity of data-this capacity needs to be recreated. In this work, we\npropose an intuitive human-agent teaching architecture in which the human can\nteach an agent how to perform a task by providing demonstrations, i.e.,\nexamples. To have an intuitive interaction, we argue that the agent should be\nable to learn incrementally from a few single examples. To allow for this, our\nobjective is to broaden the agent's task understanding using domain knowledge.\nThen, using a learning method to enable the agent to learn efficiently from a\nlimited number of examples. Finally, to optimize how human can select the most\nrepresentative and less redundant examples to provide the agent with. We apply\nour proposed method to the subjective task of ingredient substitution, where\nthe agent needs to learn how to substitute ingredients in recipes based on\nhuman examples. We replicate human input using the Recipe1MSubs dataset. In our\nexperiments, the agent achieves half its task performance after only 100\nexamples are provided, compared to the complete training set of 50k examples.\nWe show that by providing examples in strategic order along with a learning\nmethod that leverages external symbolic knowledge, the agent can generalize\nmore efficiently.", "AI": {"tldr": "Proposes human-agent teaching architecture for few-shot learning of subjective tasks using demonstrations, applied to ingredient substitution with strategic example selection and external knowledge.", "motivation": "To enable AI systems to learn subjective tasks from few examples like humans do, addressing data scarcity in subjective domains.", "method": "Human-agent teaching architecture with incremental learning from demonstrations, leveraging domain knowledge and strategic example selection using Recipe1MSubs dataset.", "result": "Agent achieves half task performance with only 100 examples vs 50k full training set, showing efficient generalization through strategic ordering and external knowledge.", "conclusion": "Strategic example selection combined with external symbolic knowledge enables efficient few-shot learning for subjective tasks, replicating human-like learning capabilities."}}
{"id": "2509.23585", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23585", "abs": "https://arxiv.org/abs/2509.23585", "authors": ["Emerald Zhang", "Julian Weaver", "Edward Castillo"], "title": "EVO-LRP: Evolutionary Optimization of LRP for Interpretable Model Explanations", "comment": "15 pages", "summary": "Explainable AI (XAI) methods help identify which image regions influence a\nmodel's prediction, but often face a trade-off between detail and\ninterpretability. Layer-wise Relevance Propagation (LRP) offers a model-aware\nalternative. However, LRP implementations commonly rely on heuristic rule sets\nthat are not optimized for clarity or alignment with model behavior. We\nintroduce EVO-LRP, a method that applies Covariance Matrix Adaptation Evolution\nStrategy (CMA-ES) to tune LRP hyperparameters based on quantitative\ninterpretability metrics, such as faithfulness or sparseness. EVO-LRP\noutperforms traditional XAI approaches in both interpretability metric\nperformance and visual coherence, with strong sensitivity to class-specific\nfeatures. These findings demonstrate that attribution quality can be\nsystematically improved through principled, task-specific optimization.", "AI": {"tldr": "EVO-LRP uses evolutionary optimization to tune LRP hyperparameters for better interpretability metrics and visual coherence in XAI methods.", "motivation": "Traditional XAI methods face trade-offs between detail and interpretability, while LRP implementations rely on heuristic rules not optimized for clarity or model alignment.", "method": "EVO-LRP applies Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to optimize LRP hyperparameters based on quantitative interpretability metrics like faithfulness and sparseness.", "result": "EVO-LRP outperforms traditional XAI approaches in interpretability metrics and visual coherence, showing strong sensitivity to class-specific features.", "conclusion": "Attribution quality can be systematically improved through principled, task-specific optimization of XAI methods."}}
{"id": "2509.24116", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24116", "abs": "https://arxiv.org/abs/2509.24116", "authors": ["Minsoo Kim", "Seung-won Hwang"], "title": "Dual-Scale World Models for LLM Agents Towards Hard-Exploration Problems", "comment": null, "summary": "LLM-based agents have seen promising advances, yet they are still limited in\n\"hard-exploration\" tasks requiring learning new knowledge through exploration.\nWe present GLoW, a novel approach leveraging dual-scale world models,\nmaintaining a trajectory frontier of high-value discoveries at the global\nscale, while learning from local trial-and-error in exploration through a\nMulti-path Advantage Reflection mechanism which infers advantage-based progress\nsignals to guide exploration. To evaluate our framework for hard-exploration,\nwe tackle the Jericho benchmark suite of text-based games, where GLoW achieves\na new state-of-theart performance for LLM-based approaches. Compared to\nstate-of-the-art RLbased methods, our approach achieves comparable performance\nwhile requiring 100-800x fewer environment interactions.", "AI": {"tldr": "GLoW is a novel LLM-based agent approach using dual-scale world models and Multi-path Advantage Reflection to tackle hard-exploration tasks, achieving SOTA performance on Jericho text games with 100-800x fewer environment interactions than RL methods.", "motivation": "LLM-based agents struggle with hard-exploration tasks that require learning new knowledge through exploration, which is a key limitation in current agent capabilities.", "method": "Uses dual-scale world models with global trajectory frontier and local trial-and-error exploration through Multi-path Advantage Reflection mechanism that infers advantage-based progress signals.", "result": "Achieves new state-of-the-art performance for LLM-based approaches on Jericho benchmark suite, with comparable performance to SOTA RL methods but requiring 100-800x fewer environment interactions.", "conclusion": "GLoW demonstrates effective hard-exploration capabilities for LLM-based agents through its dual-scale world modeling and advantage-based exploration guidance, significantly improving sample efficiency."}}
{"id": "2509.24660", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24660", "abs": "https://arxiv.org/abs/2509.24660", "authors": ["Nikolaos Kondylidis", "Anil Yaman", "Frank van Harmelen", "Erman Acar", "Annette ten Teije"], "title": "Successful Misunderstandings: Learning to Coordinate Without Being Understood", "comment": "22nd European Conference on Multi-Agent Systems (EUMAS 2025)", "summary": "The main approach to evaluating communication is by assessing how well it\nfacilitates coordination. If two or more individuals can coordinate through\ncommunication, it is generally assumed that they understand one another. We\ninvestigate this assumption in a signaling game where individuals develop a new\nvocabulary of signals to coordinate successfully. In our game, the individuals\ndo not have common observations besides the communication signal and outcome of\nthe interaction, i.e. received reward. This setting is used as a proxy to study\ncommunication emergence in populations of agents that perceive their\nenvironment very differently, e.g. hybrid populations that include humans and\nartificial agents. Agents develop signals, use them, and refine interpretations\nwhile not observing how other agents are using them. While populations always\nconverge to optimal levels of coordination, in some cases, interacting agents\ninterpret and use signals differently, converging to what we call successful\nmisunderstandings. However, agents of population that coordinate using\nmisaligned interpretations, are unable to establish successful coordination\nwith new interaction partners. Not leading to coordination failure immediately,\nsuccessful misunderstandings are difficult to spot and repair. Having at least\nthree agents that all interact with each other are the two minimum conditions\nto ensure the emergence of shared interpretations. Under these conditions, the\nagent population exhibits this emergent property of compensating for the lack\nof shared observations of signal use, ensuring the emergence of shared\ninterpretations.", "AI": {"tldr": "The paper investigates whether successful coordination through communication necessarily implies mutual understanding. In a signaling game where agents develop new vocabularies without shared observations, they find that populations can achieve optimal coordination while maintaining different interpretations of signals - called \"successful misunderstandings\" - which prevent coordination with new partners.", "motivation": "To challenge the assumption that successful coordination through communication implies mutual understanding, particularly in contexts where agents (like humans and AI) perceive environments differently and lack shared observations of signal usage.", "method": "Used a signaling game where agents develop new vocabularies without common observations besides communication signals and interaction outcomes. Agents refine interpretations while not observing how others use signals.", "result": "Populations converged to optimal coordination levels, but sometimes with \"successful misunderstandings\" - agents coordinated effectively while interpreting signals differently. These misunderstandings prevented coordination with new partners. At least three interacting agents were needed to ensure shared interpretations emerge.", "conclusion": "Successful coordination doesn't guarantee mutual understanding. \"Successful misunderstandings\" can persist undetected and prevent coordination with new partners. Shared interpretations emerge only when at least three agents interact, compensating for lack of shared signal usage observations."}}
{"id": "2509.23587", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.23587", "abs": "https://arxiv.org/abs/2509.23587", "authors": ["Andres Fernandez", "Felix Dangel", "Philipp Hennig", "Frank Schneider"], "title": "Sketching Low-Rank Plus Diagonal Matrices", "comment": null, "summary": "Many relevant machine learning and scientific computing tasks involve\nhigh-dimensional linear operators accessible only via costly matrix-vector\nproducts. In this context, recent advances in sketched methods have enabled the\nconstruction of *either* low-rank *or* diagonal approximations from few\nmatrix-vector products. This provides great speedup and scalability, but\napproximation errors arise due to the assumed simpler structure. This work\nintroduces SKETCHLORD, a method that simultaneously estimates both low-rank\n*and* diagonal components, targeting the broader class of Low-Rank *plus*\nDiagonal (LoRD) linear operators. We demonstrate theoretically and empirically\nthat this joint estimation is superior also to any sequential variant\n(diagonal-then-low-rank or low-rank-then-diagonal). Then, we cast SKETCHLORD as\na convex optimization problem, leading to a scalable algorithm. Comprehensive\nexperiments on synthetic (approximate) LoRD matrices confirm SKETCHLORD's\nperformance in accurately recovering these structures. This positions it as a\nvaluable addition to the structured approximation toolkit, particularly when\nhigh-fidelity approximations are desired for large-scale operators, such as the\ndeep learning Hessian.", "AI": {"tldr": "SKETCHLORD is a method that simultaneously estimates both low-rank and diagonal components of linear operators, outperforming sequential approaches and providing high-fidelity approximations for large-scale operators.", "motivation": "Many machine learning tasks involve high-dimensional linear operators that are costly to compute. Existing sketched methods only construct either low-rank or diagonal approximations, leading to approximation errors due to oversimplified structure assumptions.", "method": "SKETCHLORD simultaneously estimates both low-rank and diagonal components through convex optimization, targeting Low-Rank plus Diagonal (LoRD) linear operators from few matrix-vector products.", "result": "Theoretical and empirical results show SKETCHLORD's joint estimation is superior to sequential variants, accurately recovering LoRD structures in synthetic experiments.", "conclusion": "SKETCHLORD provides a valuable addition to structured approximation tools, particularly for high-fidelity approximations of large-scale operators like deep learning Hessians."}}
{"id": "2509.24120", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24120", "abs": "https://arxiv.org/abs/2509.24120", "authors": ["Sourjyadip Ray", "Shubham Sharma", "Somak Aditya", "Pawan Goyal"], "title": "EduVidQA: Generating and Evaluating Long-form Answers to Student Questions based on Lecture Videos", "comment": "EMNLP 2025 (Main)", "summary": "As digital platforms redefine educational paradigms, ensuring interactivity\nremains vital for effective learning. This paper explores using Multimodal\nLarge Language Models (MLLMs) to automatically respond to student questions\nfrom online lectures - a novel question answering task of real world\nsignificance. We introduce the EduVidQA Dataset with 5252 question-answer pairs\n(both synthetic and real-world) from 296 computer science videos covering\ndiverse topics and difficulty levels. To understand the needs of the dataset\nand task evaluation, we empirically study the qualitative preferences of\nstudents, which we provide as an important contribution to this line of work.\nOur benchmarking experiments consist of 6 state-of-the-art MLLMs, through which\nwe study the effectiveness of our synthetic data for finetuning, as well as\nshowing the challenging nature of the task. We evaluate the models using both\ntext-based and qualitative metrics, thus showing a nuanced perspective of the\nmodels' performance, which is paramount to future work. This work not only sets\na benchmark for this important problem, but also opens exciting avenues for\nfuture research in the field of Natural Language Processing for Education.", "AI": {"tldr": "This paper introduces EduVidQA, a dataset for automated question answering from online educational videos using Multimodal Large Language Models (MLLMs), and benchmarks 6 state-of-the-art models on this challenging task.", "motivation": "As digital platforms transform education, maintaining interactivity is crucial for effective learning. The paper addresses the need for automated question answering from online lectures, which has real-world significance in educational technology.", "method": "Created EduVidQA Dataset with 5252 question-answer pairs from 296 computer science videos, including both synthetic and real-world data. Studied student qualitative preferences and benchmarked 6 state-of-the-art MLLMs using text-based and qualitative metrics.", "result": "The research demonstrates the challenging nature of the task and shows the effectiveness of synthetic data for finetuning. The evaluation provides nuanced perspectives on model performance through multiple metrics.", "conclusion": "This work establishes a benchmark for automated question answering in educational videos and opens promising research directions in Natural Language Processing for Education."}}
{"id": "2509.24711", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24711", "abs": "https://arxiv.org/abs/2509.24711", "authors": ["Qingjie Zhang", "Yujia Fu", "Yang Wang", "Liu Yan", "Tao Wei", "Ke Xu", "Minlie Huang", "Han Qiu"], "title": "On the Self-awareness of Large Reasoning Models' Capability Boundaries", "comment": null, "summary": "Large Reasoning Models (LRMs) have shown impressive performance on complex\nreasoning tasks such as mathematics, yet they also display misbehaviors that\nexpose their limitations. In particular, when faced with hard questions, LRMs\noften engage in unproductive reasoning until context limit, producing wrong\nanswers while wasting substantial computation. This phenomenon reflects a\nfundamental issue: current answering paradigms overlook the relationship\nbetween questions and LRMs' capability boundaries. In this paper, we\ninvestigate whether LRMs possess self-awareness of capability boundaries. We\nbegin by an observation that LRMs may know what they cannot solve through\nexpressed reasoning confidence. For black-box models, we find that reasoning\nexpressions reveal boundary signals, with accelerated growing confidence\ntrajectory for solvable problems but convergent uncertainty trajectory for\nunsolvable ones. For white-box models, we show that hidden states of the last\ninput token encode boundary information, with solvable and unsolvable problems\nlinearly separable even before reasoning begins. Building on these findings, we\npropose two simple yet effective optimization strategies: reasoning expression\nmonitoring and hidden states monitoring. Experiments demonstrate that these\nboundary-aware strategies enable LRMs to avoid unproductive reasoning without\nsacrificing accuracy, significantly improving reliability and efficiency by\ncutting token usage up to 62.7 - 93.6%.", "AI": {"tldr": "Large Reasoning Models (LRMs) can identify their capability boundaries through reasoning confidence patterns and hidden states, enabling boundary-aware strategies that cut unproductive reasoning by up to 93.6% while maintaining accuracy.", "motivation": "Current LRMs waste computation on unsolvable problems by engaging in unproductive reasoning until context limit, highlighting the need for self-awareness of capability boundaries.", "method": "Two monitoring approaches: 1) For black-box models: analyze reasoning expression confidence trajectories; 2) For white-box models: examine hidden states of last input token for linear separability of solvable/unsolvable problems.", "result": "Boundary-aware strategies reduce token usage by 62.7-93.6% without sacrificing accuracy, significantly improving reliability and efficiency.", "conclusion": "LRMs possess inherent capability boundary awareness that can be leveraged through simple monitoring strategies to prevent unproductive reasoning and enhance model efficiency."}}
{"id": "2509.23592", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23592", "abs": "https://arxiv.org/abs/2509.23592", "authors": ["Hoang Phan", "Sungmin Cha", "Tung Lam Tran", "Qi Lei"], "title": "Toward a Holistic Approach to Continual Model Merging", "comment": "Accepted to Workshop on Continual Learning in Computer Vision, ICCV\n  2025", "summary": "We present a holistic framework for continual model merging that intervenes\nat three critical stages: pre-merging, during merging, and post-merging-to\naddress two fundamental challenges in continual learning. In particular,\nconventional approaches either maintain a growing list of per-domain task\nvectors, leading to scalability issues or rely solely on weight-space merging\nwhen old data is inaccessible, thereby losing crucial functional information.\nOur method overcomes these limitations by first fine-tuning the main model\nwithin its tangent space on domain-specific data; this linearization amplifies\nper-task weight disentanglement, effectively mitigating across-task\ninterference. During merging, we leverage functional information from available\noptimizer states beyond mere parameter averages to avoid the need to revisit\nold data. Finally, a post-merging correction aligns the representation\ndiscrepancy between pre- and post-merged models, reducing bias and enhancing\noverall performance-all while operating under constant memory constraints\nwithout accessing historical data. Extensive experiments on standard\nclass-incremental and domain-incremental benchmarks demonstrate that our\napproach not only achieves competitive performance but also provides a scalable\nand efficient solution to the catastrophic forgetting problem.", "AI": {"tldr": "A holistic continual model merging framework that intervenes at pre-merging, during merging, and post-merging stages to address catastrophic forgetting without accessing old data, using tangent space fine-tuning, functional information from optimizer states, and representation alignment.", "motivation": "To overcome limitations of conventional continual learning approaches that either maintain growing task vectors (scalability issues) or rely solely on weight-space merging (losing functional information) when old data is inaccessible.", "method": "Three-stage approach: 1) Pre-merging: fine-tune main model in tangent space for weight disentanglement; 2) During merging: leverage functional information from optimizer states; 3) Post-merging: correct representation discrepancy between pre- and post-merged models.", "result": "Achieves competitive performance on standard class-incremental and domain-incremental benchmarks while operating under constant memory constraints without accessing historical data.", "conclusion": "Provides a scalable and efficient solution to catastrophic forgetting problem by combining tangent space fine-tuning, functional information utilization, and representation alignment in a holistic framework."}}
{"id": "2509.24130", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24130", "abs": "https://arxiv.org/abs/2509.24130", "authors": ["Guancheng Wan", "Lucheng Fu", "Haoxin Liu", "Yiqiao Jin", "Hui Yi Leong", "Eric Hanchen Jiang", "Hejia Geng", "Jinhe Bi", "Yunpu Ma", "Xiangru Tang", "B. Aditya Prakash", "Yizhou Sun", "Wei Wang"], "title": "Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE", "comment": null, "summary": "The performance of Large Language Models (LLMs) hinges on carefully\nengineered prompts. However, prevailing prompt optimization methods, ranging\nfrom heuristic edits and reinforcement learning to evolutionary search,\nprimarily target point-wise accuracy. They seldom enforce paraphrase invariance\nor searching stability, and therefore cannot remedy this brittleness in\npractice. Automated prompt search remains brittle: small, semantically\npreserving paraphrases often cause large performance swings. We identify this\nbrittleness as the textual sharpness of the prompt landscape. In this work, we\nprovide the first formal treatment of textual sharpness in the discrete,\nsemantic space of prompts, together with an operational robustness criterion\nover a semantic neighborhood; the design is black-box or API-only, requiring no\ngradients to update the model's parameters. Then we introduce TARE (Textual\nSharpness-Aware Evolving), a derivative-free framework that alternates between\nan inner, sampling-based adversarial search that stresses a prompt with hard\nparaphrases and an outer, robust selection that prefers candidates whose\nneighborhoods remain strong. We further propose ATARE, which learns anisotropic\nweights to shape the semantic neighborhood and adapts its radius over time to\nbalance exploration and fidelity. Diverse tasks evaluate our methods, whose\ndesign for minimizing textual sharpness gap leads to prompts that preserve\naccuracy under paraphrasing, outperforming accuracy-only prompt search while\nremaining computationally practical.", "AI": {"tldr": "TARE is a derivative-free prompt optimization framework that minimizes textual sharpness by alternating between adversarial paraphrase search and robust selection, improving prompt robustness to semantic-preserving changes while maintaining accuracy.", "motivation": "Current prompt optimization methods focus only on point-wise accuracy but ignore paraphrase invariance, making prompts brittle to small semantic-preserving changes that cause large performance swings.", "method": "TARE framework alternates between inner adversarial search (stressing prompts with hard paraphrases) and outer robust selection (preferring candidates with strong neighborhoods). ATARE variant learns anisotropic weights to shape semantic neighborhood and adapts radius over time.", "result": "TARE produces prompts that preserve accuracy under paraphrasing, outperforming accuracy-only prompt search methods while remaining computationally practical across diverse tasks.", "conclusion": "Minimizing textual sharpness gap leads to more robust prompts that maintain performance under semantic-preserving paraphrases, addressing the brittleness of traditional prompt optimization approaches."}}
{"id": "2509.24761", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24761", "abs": "https://arxiv.org/abs/2509.24761", "authors": ["Yueming Sun", "Long Yang"], "title": "Spatial-Functional awareness Transformer-based graph archetype contrastive learning for Decoding Visual Neural Representations from EEG", "comment": null, "summary": "Decoding visual neural representations from Electroencephalography (EEG)\nsignals remains a formidable challenge due to their high-dimensional, noisy,\nand non-Euclidean nature. In this work, we propose a Spatial-Functional\nAwareness Transformer-based Graph Archetype Contrastive Learning (SFTG)\nframework to enhance EEG-based visual decoding. Specifically, we introduce the\nEEG Graph Transformer (EGT), a novel graph-based neural architecture that\nsimultaneously encodes spatial brain connectivity and temporal neural dynamics.\nTo mitigate high intra-subject variability, we propose Graph Archetype\nContrastive Learning (GAC), which learns subject-specific EEG graph archetypes\nto improve feature consistency and class separability. Furthermore, we conduct\ncomprehensive subject-dependent and subject-independent evaluations on the\nThings-EEG dataset, demonstrating that our approach significantly outperforms\nprior state-of-the-art EEG decoding methods.The results underscore the\ntransformative potential of integrating graph-based learning with contrastive\nobjectives to enhance EEG-based brain decoding, paving the way for more\ngeneralizable and robust neural representations.", "AI": {"tldr": "Proposed SFTG framework using EEG Graph Transformer and Graph Archetype Contrastive Learning to improve EEG-based visual decoding by addressing spatial-temporal dynamics and intra-subject variability.", "motivation": "To overcome challenges in decoding visual neural representations from EEG signals, which are high-dimensional, noisy, and non-Euclidean, with high intra-subject variability.", "method": "SFTG framework with EEG Graph Transformer (EGT) for spatial-temporal encoding and Graph Archetype Contrastive Learning (GAC) for subject-specific EEG graph archetypes to improve feature consistency.", "result": "Significantly outperforms prior state-of-the-art EEG decoding methods on Things-EEG dataset in both subject-dependent and subject-independent evaluations.", "conclusion": "Integration of graph-based learning with contrastive objectives shows transformative potential for enhancing EEG-based brain decoding, enabling more generalizable and robust neural representations."}}
{"id": "2509.23593", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23593", "abs": "https://arxiv.org/abs/2509.23593", "authors": ["Zekun Wang", "Anant Gupta", "Zihan Dong", "Christopher J. MacLellan"], "title": "Avoid Catastrophic Forgetting with Rank-1 Fisher from Diffusion Models", "comment": "18 pages, 14 figures", "summary": "Catastrophic forgetting remains a central obstacle for continual learning in\nneural models. Popular approaches -- replay and elastic weight consolidation\n(EWC) -- have limitations: replay requires a strong generator and is prone to\ndistributional drift, while EWC implicitly assumes a shared optimum across\ntasks and typically uses a diagonal Fisher approximation. In this work, we\nstudy the gradient geometry of diffusion models, which can already produce\nhigh-quality replay data. We provide theoretical and empirical evidence that,\nin the low signal-to-noise ratio (SNR) regime, per-sample gradients become\nstrongly collinear, yielding an empirical Fisher that is effectively rank-1 and\naligned with the mean gradient. Leveraging this structure, we propose a rank-1\nvariant of EWC that is as cheap as the diagonal approximation yet captures the\ndominant curvature direction. We pair this penalty with a replay-based approach\nto encourage parameter sharing across tasks while mitigating drift. On\nclass-incremental image generation datasets (MNIST, FashionMNIST, CIFAR-10,\nImageNet-1k), our method consistently improves average FID and reduces\nforgetting relative to replay-only and diagonal-EWC baselines. In particular,\nforgetting is nearly eliminated on MNIST and FashionMNIST and is roughly halved\non ImageNet-1k. These results suggest that diffusion models admit an\napproximately rank-1 Fisher. With a better Fisher estimate, EWC becomes a\nstrong complement to replay: replay encourages parameter sharing across tasks,\nwhile EWC effectively constrains replay-induced drift.", "AI": {"tldr": "Proposes a rank-1 EWC method for continual learning in diffusion models, leveraging their gradient geometry to reduce forgetting while being computationally efficient.", "motivation": "Address limitations of existing continual learning methods: replay requires strong generators and suffers from distributional drift, while EWC assumes shared optimum and uses diagonal Fisher approximation.", "method": "Leverages diffusion model gradient geometry in low SNR regime, proposes rank-1 EWC variant that captures dominant curvature direction, pairs with replay to encourage parameter sharing and mitigate drift.", "result": "Consistently improves average FID and reduces forgetting on class-incremental image generation datasets (MNIST, FashionMNIST, CIFAR-10, ImageNet-1k), nearly eliminating forgetting on MNIST/FashionMNIST and halving it on ImageNet-1k.", "conclusion": "Diffusion models have approximately rank-1 Fisher, enabling effective EWC that complements replay by constraining replay-induced drift while encouraging parameter sharing across tasks."}}
{"id": "2509.24147", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24147", "abs": "https://arxiv.org/abs/2509.24147", "authors": ["Yida Chen", "Yuning Mao", "Xianjun Yang", "Suyu Ge", "Shengjie Bi", "Lijuan Liu", "Saghar Hosseini", "Liang Tan", "Yixin Nie", "Shaoliang Nie"], "title": "Your thoughts tell who you are: Characterize the reasoning patterns of LRMs", "comment": "32 pages, 28 figures", "summary": "Current comparisons of large reasoning models (LRMs) focus on macro-level\nstatistics such as task accuracy or reasoning length. Whether different LRMs\nreason differently remains an open question. To address this gap, we introduce\nthe LLM-proposed Open Taxonomy (LOT), a classification method that uses a\ngenerative language model to compare reasoning traces from two LRMs and\narticulate their distinctive features in words. LOT then models how these\nfeatures predict the source LRM of a reasoning trace based on their empirical\ndistributions across LRM outputs. Iterating this process over a dataset of\nreasoning traces yields a human-readable taxonomy that characterizes how models\nthink. We apply LOT to compare the reasoning of 12 open-source LRMs on tasks in\nmath, science, and coding. LOT identifies systematic differences in their\nthoughts, achieving 80-100% accuracy in distinguishing reasoning traces from\nLRMs that differ in scale, base model family, or objective domain. Beyond\nclassification, LOT's natural-language taxonomy provides qualitative\nexplanations of how LRMs think differently. Finally, in a case study, we link\nthe reasoning differences to performance: aligning the reasoning style of\nsmaller Qwen3 models with that of the largest Qwen3 during test time improves\ntheir accuracy on GPQA by 3.3-5.7%.", "AI": {"tldr": "LOT is a method that uses language models to compare reasoning traces from different large reasoning models, creating a human-readable taxonomy that characterizes their distinctive thinking patterns and achieves high accuracy in distinguishing between models.", "motivation": "Current comparisons of large reasoning models focus only on macro-level statistics like accuracy, leaving the question of whether different models actually reason differently unanswered.", "method": "LOT uses a generative language model to compare reasoning traces from two LRMs, articulate their distinctive features in words, and model how these features predict the source model based on empirical distributions across outputs.", "result": "Applied to 12 open-source LRMs on math, science, and coding tasks, LOT achieved 80-100% accuracy in distinguishing reasoning traces and identified systematic differences in reasoning styles. Aligning smaller models' reasoning with larger models improved accuracy on GPQA by 3.3-5.7%.", "conclusion": "LOT provides both quantitative classification and qualitative explanations of how different large reasoning models think, revealing systematic reasoning differences that can be leveraged to improve model performance."}}
{"id": "2509.24765", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24765", "abs": "https://arxiv.org/abs/2509.24765", "authors": ["Yunyao Zhang", "Xinglang Zhang", "Junxi Sheng", "Wenbing Li", "Junqing Yu", "Wei Yang", "Zikai Song"], "title": "From Ambiguity to Verdict: A Semiotic-Grounded Multi-Perspective Agent for LLM Logical Reasoning", "comment": null, "summary": "Logical reasoning is a fundamental capability of large language models\n(LLMs). However, existing studies largely overlook the interplay between\nlogical complexity and semantic complexity, resulting in methods that struggle\nto address challenging scenarios involving abstract propositions, ambiguous\ncontexts, and conflicting stances, which are central to human reasoning. For\nthis gap, we propose LogicAgent, a semiotic-square-guided framework designed to\njointly address logical complexity and semantic complexity. LogicAgent\nexplicitly performs multi-perspective deduction in first-order logic (FOL),\nwhile mitigating vacuous reasoning through existential import checks that\nincorporate a three-valued decision scheme (True, False, Uncertain) to handle\nboundary cases more faithfully. Furthermore, to overcome the semantic\nsimplicity and low logical complexity of existing datasets, we introduce\nRepublicQA, a benchmark that reaches college-level difficulty (FKGL = 11.94)\nand exhibits substantially greater lexical and structural diversity than prior\nbenchmarks. RepublicQA is grounded in philosophical concepts, featuring\nabstract propositions and systematically organized contrary and contradictory\nrelations, making it the most semantically rich resource for evaluating logical\nreasoning. Experiments demonstrate that LogicAgent achieves state-of-the-art\nperformance on RepublicQA, with a 6.25% average gain over strong baselines, and\ngeneralizes effectively to mainstream logical reasoning benchmarks including\nProntoQA, ProofWriter, FOLIO, and ProverQA, achieving an additional 7.05%\naverage gain. These results highlight the strong effectiveness of our\nsemiotic-grounded multi-perspective reasoning in boosting LLMs' logical\nperformance.", "AI": {"tldr": "LogicAgent is a semiotic-square-guided framework that addresses both logical and semantic complexity in LLM reasoning, achieving SOTA performance on the new RepublicQA benchmark and generalizing well to other logical reasoning benchmarks.", "motivation": "Existing methods overlook the interplay between logical complexity and semantic complexity, struggling with abstract propositions, ambiguous contexts, and conflicting stances that are central to human reasoning.", "method": "LogicAgent performs multi-perspective deduction in first-order logic while mitigating vacuous reasoning through existential import checks with a three-valued decision scheme (True, False, Uncertain) for boundary cases.", "result": "LogicAgent achieves state-of-the-art performance on RepublicQA with 6.25% average gain over baselines, and generalizes effectively to other benchmarks (ProntoQA, ProofWriter, FOLIO, ProverQA) with additional 7.05% average gain.", "conclusion": "The semiotic-grounded multi-perspective reasoning approach effectively boosts LLMs' logical performance, demonstrating strong effectiveness in handling complex logical reasoning scenarios."}}
{"id": "2509.23597", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23597", "abs": "https://arxiv.org/abs/2509.23597", "authors": ["Zheng Wang", "Kaixuan Zhang", "Wanfang Chen", "Xiaonan Lu", "Longyuan Li", "Tobias Schlagenhauf"], "title": "Characteristic Root Analysis and Regularization for Linear Time Series Forecasting", "comment": null, "summary": "Time series forecasting remains a critical challenge across numerous domains,\nyet the effectiveness of complex models often varies unpredictably across\ndatasets. Recent studies highlight the surprising competitiveness of simple\nlinear models, suggesting that their robustness and interpretability warrant\ndeeper theoretical investigation. This paper presents a systematic study of\nlinear models for time series forecasting, with a focus on the role of\ncharacteristic roots in temporal dynamics. We begin by analyzing the noise-free\nsetting, where we show that characteristic roots govern long-term behavior and\nexplain how design choices such as instance normalization and channel\nindependence affect model capabilities. We then extend our analysis to the\nnoisy regime, revealing that models tend to produce spurious roots. This leads\nto the identification of a key data-scaling property: mitigating the influence\nof noise requires disproportionately large training data, highlighting the need\nfor structural regularization. To address these challenges, we propose two\ncomplementary strategies for robust root restructuring. The first uses rank\nreduction techniques, including Reduced-Rank Regression and Direct Weight Rank\nReduction, to recover the low-dimensional latent dynamics. The second, a novel\nadaptive method called Root Purge, encourages the model to learn a\nnoise-suppressing null space during training. Extensive experiments on standard\nbenchmarks demonstrate the effectiveness of both approaches, validating our\ntheoretical insights and achieving state-of-the-art results in several\nsettings. Our findings underscore the potential of integrating classical\ntheories for linear systems with modern learning techniques to build robust,\ninterpretable, and data-efficient forecasting models.", "AI": {"tldr": "This paper systematically studies linear models for time series forecasting, focusing on characteristic roots' role in temporal dynamics. It analyzes noise-free and noisy regimes, identifies data-scaling challenges, and proposes two robust root restructuring strategies that achieve state-of-the-art results.", "motivation": "Recent studies show simple linear models can be surprisingly competitive in time series forecasting, warranting deeper theoretical investigation into their robustness and interpretability.", "method": "1) Theoretical analysis of characteristic roots in noise-free and noisy settings; 2) Proposed two strategies: rank reduction techniques (Reduced-Rank Regression, Direct Weight Rank Reduction) and Root Purge method for noise suppression; 3) Extensive experiments on standard benchmarks.", "result": "Both proposed approaches demonstrate effectiveness, validating theoretical insights and achieving state-of-the-art results in several settings. The methods successfully address noise challenges and improve forecasting performance.", "conclusion": "Integrating classical linear systems theories with modern learning techniques can build robust, interpretable, and data-efficient forecasting models, highlighting the continued relevance of linear approaches in time series analysis."}}
{"id": "2509.24164", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24164", "abs": "https://arxiv.org/abs/2509.24164", "authors": ["Haolin Yang", "Hakaze Cho", "Naoya Inoue"], "title": "Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis", "comment": "45 pages, 88 figures, 10 tables", "summary": "We investigate the mechanistic underpinnings of in-context learning (ICL) in\nlarge language models by reconciling two dominant perspectives: the\ncomponent-level analysis of attention heads and the holistic decomposition of\nICL into Task Recognition (TR) and Task Learning (TL). We propose a novel\nframework based on Task Subspace Logit Attribution (TSLA) to identify attention\nheads specialized in TR and TL, and demonstrate their distinct yet\ncomplementary roles. Through correlation analysis, ablation studies, and input\nperturbations, we show that the identified TR and TL heads independently and\neffectively capture the TR and TL components of ICL. Using steering experiments\nwith geometric analysis of hidden states, we reveal that TR heads promote task\nrecognition by aligning hidden states with the task subspace, while TL heads\nrotate hidden states within the subspace toward the correct label to facilitate\nprediction. We further show how previous findings on ICL mechanisms, including\ninduction heads and task vectors, can be reconciled with our\nattention-head-level analysis of the TR-TL decomposition. Our framework thus\nprovides a unified and interpretable account of how large language models\nexecute ICL across diverse tasks and settings.", "AI": {"tldr": "This paper proposes a unified framework (TSLA) to analyze in-context learning mechanisms in LLMs, identifying specialized attention heads for task recognition and task learning, and showing how they work together.", "motivation": "To reconcile two dominant perspectives on in-context learning mechanisms: component-level analysis of attention heads vs holistic TR-TL decomposition, providing a unified understanding.", "method": "Proposed Task Subspace Logit Attribution (TSLA) framework, conducted correlation analysis, ablation studies, input perturbations, and steering experiments with geometric analysis of hidden states.", "result": "Identified distinct TR and TL attention heads that independently capture ICL components; TR heads align hidden states with task subspace, TL heads rotate states toward correct labels.", "conclusion": "Provides a unified and interpretable account of how LLMs execute ICL across diverse tasks, reconciling previous findings like induction heads and task vectors with TR-TL decomposition."}}
{"id": "2509.24803", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24803", "abs": "https://arxiv.org/abs/2509.24803", "authors": ["Tong Guan", "Zijie Meng", "Dianqi Li", "Shiyu Wang", "Chao-Han Huck Yang", "Qingsong Wen", "Zuozhu Liu", "Sabato Marco Siniscalchi", "Ming Jin", "Shirui Pan"], "title": "TimeOmni-1: Incentivizing Complex Reasoning with Time Series in Large Language Models", "comment": null, "summary": "Recent advances in multimodal time series learning underscore a paradigm\nshift from analytics centered on basic patterns toward advanced time series\nunderstanding and reasoning. However, existing multimodal time series datasets\nmostly remain at the level of surface alignment and question answering, without\nreaching the depth of genuine reasoning. The absence of well-defined tasks that\ngenuinely require time series reasoning, along with the scarcity of\nhigh-quality data, has limited progress in building practical time series\nreasoning models (TSRMs). To this end, we introduce Time Series Reasoning Suite\n(TSR-Suite), which formalizes four atomic tasks that span three fundamental\ncapabilities for reasoning with time series: (1) perception, acquired through\nscenario understanding and causality discovery; (2) extrapolation, realized via\nevent-aware forecasting; and (3) decision-making, developed through\ndeliberation over perception and extrapolation. TSR-Suite is the first\ncomprehensive time series reasoning suite that supports not only thorough\nevaluation but also the data pipeline and training of TSRMs. It contains more\nthan 23K samples, of which 2.3K are carefully curated through a human-guided\nhierarchical annotation process. Building on this foundation, we introduce\nTimeOmni-1, the first unified reasoning model designed to address diverse\nreal-world problems demanding time series reasoning. The model is trained in\nmultiple stages, integrating a mixture of task scenarios, novel reward\nfunctions, and tailored optimizations. Experiments show that TimeOmni-1\ndelivers strong out-of-distribution generalization across all tasks and\nachieves a high rate of valid responses. It significantly improves causality\ndiscovery accuracy (64.0% vs. 35.9% with GPT-4.1) and raises the valid response\nrate by over 6% compared to GPT-4.1 on the event-aware forecasting task.", "AI": {"tldr": "TSR-Suite introduces four atomic tasks for time series reasoning across perception, extrapolation, and decision-making capabilities, with TimeOmni-1 model showing strong generalization and outperforming GPT-4.1 in causality discovery and forecasting.", "motivation": "Existing multimodal time series datasets lack genuine reasoning tasks and high-quality data, limiting progress in practical time series reasoning models.", "method": "Introduces TSR-Suite with four atomic tasks spanning perception, extrapolation, and decision-making, and TimeOmni-1 model trained with multiple stages, reward functions, and optimizations.", "result": "TimeOmni-1 achieves 64.0% causality discovery accuracy (vs 35.9% for GPT-4.1) and improves valid response rate by over 6% on event-aware forecasting compared to GPT-4.1.", "conclusion": "TSR-Suite enables comprehensive evaluation and training of time series reasoning models, with TimeOmni-1 demonstrating strong out-of-distribution generalization across reasoning tasks."}}
{"id": "2509.23616", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23616", "abs": "https://arxiv.org/abs/2509.23616", "authors": ["Fanlong Zeng", "Wensheng Gan", "Philip S. Yu"], "title": "GraphIFE: Rethinking Graph Imbalance Node Classification via Invariant Learning", "comment": "PrePrint, 16 pages, 7 tables, 6 figures", "summary": "The class imbalance problem refers to the disproportionate distribution of\nsamples across different classes within a dataset, where the minority classes\nare significantly underrepresented. This issue is also prevalent in\ngraph-structured data. Most graph neural networks (GNNs) implicitly assume a\nbalanced class distribution and therefore often fail to account for the\nchallenges introduced by class imbalance, which can lead to biased learning and\ndegraded performance on minority classes. We identify a quality inconsistency\nproblem in synthesized nodes, which leads to suboptimal performance under graph\nimbalance conditions. To mitigate this issue, we propose GraphIFE (Graph\nInvariant Feature Extraction), a novel framework designed to mitigate quality\ninconsistency in synthesized nodes. Our approach incorporates two key concepts\nfrom graph invariant learning and introduces strategies to strengthen the\nembedding space representation, thereby enhancing the model's ability to\nidentify invariant features. Extensive experiments demonstrate the framework's\nefficiency and robust generalization, as GraphIFE consistently outperforms\nvarious baselines across multiple datasets. The code is publicly available at\nhttps://github.com/flzeng1/GraphIFE.", "AI": {"tldr": "GraphIFE addresses class imbalance in graph data by mitigating quality inconsistency in synthesized nodes using graph invariant learning, achieving superior performance over baselines.", "motivation": "Class imbalance in graph data causes biased learning and poor performance on minority classes, with existing GNNs failing to handle quality inconsistency in synthesized nodes.", "method": "Proposes GraphIFE framework with graph invariant learning strategies to strengthen embedding space representation and identify invariant features.", "result": "Extensive experiments show GraphIFE consistently outperforms various baselines across multiple datasets with robust generalization.", "conclusion": "GraphIFE effectively mitigates quality inconsistency in synthesized nodes and enhances model performance under graph imbalance conditions."}}
{"id": "2509.24169", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24169", "abs": "https://arxiv.org/abs/2509.24169", "authors": ["Haolin Yang", "Hakaze Cho", "Kaize Ding", "Naoya Inoue"], "title": "Task Vectors, Learned Not Extracted: Performance Gains and Mechanistic Insight", "comment": "48 pages, 95 figures, 17 tables", "summary": "Large Language Models (LLMs) can perform new tasks from in-context\ndemonstrations, a phenomenon known as in-context learning (ICL). Recent work\nsuggests that these demonstrations are compressed into task vectors (TVs),\ncompact task representations that LLMs exploit for predictions. However, prior\nstudies typically extract TVs from model outputs or hidden states using\ncumbersome and opaque methods, and they rarely elucidate the mechanisms by\nwhich TVs influence computation. In this work, we address both limitations.\nFirst, we propose directly training Learned Task Vectors (LTVs), which surpass\nextracted TVs in accuracy and exhibit superior flexibility-acting effectively\nat arbitrary layers, positions, and even with ICL prompts. Second, through\nsystematic analysis, we investigate the mechanistic role of TVs, showing that\nat the low level they steer predictions primarily through attention-head OV\ncircuits, with a small subset of \"key heads\" most decisive. At a higher level,\nwe find that despite Transformer nonlinearities, TV propagation is largely\nlinear: early TVs are rotated toward task-relevant subspaces to improve logits\nof relevant labels, while later TVs are predominantly scaled in magnitude.\nTaken together, LTVs not only provide a practical approach for obtaining\neffective TVs but also offer a principled lens into the mechanistic foundations\nof ICL.", "AI": {"tldr": "This paper introduces Learned Task Vectors (LTVs) as a direct training approach that outperforms extracted task vectors in accuracy and flexibility, while also providing mechanistic insights into how task vectors influence in-context learning through attention circuits and linear propagation patterns.", "motivation": "Prior methods for extracting task vectors from LLMs are cumbersome, opaque, and don't explain how task vectors actually influence model computation during in-context learning.", "method": "Directly train Learned Task Vectors (LTVs) instead of extracting them, then systematically analyze their mechanistic role through attention-head OV circuits and propagation patterns.", "result": "LTVs surpass extracted task vectors in accuracy and work effectively at arbitrary layers, positions, and with ICL prompts. Analysis reveals task vectors primarily operate through attention-head OV circuits, with key heads being decisive, and propagation is largely linear despite Transformer nonlinearities.", "conclusion": "LTVs provide both a practical approach for obtaining effective task vectors and a principled framework for understanding the mechanistic foundations of in-context learning in LLMs."}}
{"id": "2509.24808", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24808", "abs": "https://arxiv.org/abs/2509.24808", "authors": ["Tung-Yu Wu", "Fazl Barez"], "title": "Query Circuits: Explaining How Language Models Answer User Prompts", "comment": "Preprint. Under review", "summary": "Explaining why a language model produces a particular output requires local,\ninput-level explanations. Existing methods uncover global capability circuits\n(e.g., indirect object identification), but not why the model answers a\nspecific input query in a particular way. We introduce query circuits, which\ndirectly trace the information flow inside a model that maps a specific input\nto the output. Unlike surrogate-based approaches (e.g., sparse autoencoders),\nquery circuits are identified within the model itself, resulting in more\nfaithful and computationally accessible explanations. To make query circuits\npractical, we address two challenges. First, we introduce Normalized Deviation\nFaithfulness (NDF), a robust metric to evaluate how well a discovered circuit\nrecovers the model's decision for a specific input, and is broadly applicable\nto circuit discovery beyond our setting. Second, we develop sampling-based\nmethods to efficiently identify circuits that are sparse yet faithfully\ndescribe the model's behavior. Across benchmarks (IOI, arithmetic, MMLU, and\nARC), we find that there exist extremely sparse query circuits within the model\nthat can recover much of its performance on single queries. For example, a\ncircuit covering only 1.3% of model connections can recover about 60% of\nperformance on an MMLU questions. Overall, query circuits provide a step\ntowards faithful, scalable explanations of how language models process\nindividual inputs.", "AI": {"tldr": "Query circuits are introduced as a method to trace information flow in language models for specific input-output mappings, providing more faithful explanations than surrogate-based approaches.", "motivation": "Existing methods only uncover global capability circuits but fail to explain why models produce specific outputs for particular inputs, creating a need for local, input-level explanations.", "method": "Developed query circuits that directly trace information flow within models, introduced Normalized Deviation Faithfulness (NDF) metric for evaluation, and used sampling-based methods to identify sparse circuits.", "result": "Found extremely sparse query circuits covering only 1.3% of model connections can recover about 60% of performance on MMLU questions, demonstrating effectiveness across multiple benchmarks.", "conclusion": "Query circuits provide a step towards faithful, scalable explanations of how language models process individual inputs, offering more computationally accessible and accurate explanations than existing methods."}}
{"id": "2509.23631", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23631", "abs": "https://arxiv.org/abs/2509.23631", "authors": ["Chen Yang", "Changhao Zhao", "Chen Wang", "Jiansheng Fan"], "title": "DRIK: Distribution-Robust Inductive Kriging without Information Leakage", "comment": null, "summary": "Inductive kriging supports high-resolution spatio-temporal estimation with\nsparse sensor networks, but conventional training-evaluation setups often\nsuffer from information leakage and poor out-of-distribution (OOD)\ngeneralization. We find that the common 2x2 spatio-temporal split allows test\ndata to influence model selection through early stopping, obscuring the true\nOOD characteristics of inductive kriging. To address this issue, we propose a\n3x3 partition that cleanly separates training, validation, and test sets,\neliminating leakage and better reflecting real-world applications. Building on\nthis redefined setting, we introduce DRIK, a Distribution-Robust Inductive\nKriging approach designed with the intrinsic properties of inductive kriging in\nmind to explicitly enhance OOD generalization, employing a three-tier strategy\nat the node, edge, and subgraph levels. DRIK perturbs node coordinates to\ncapture continuous spatial relationships, drops edges to reduce ambiguity in\ninformation flow and increase topological diversity, and adds pseudo-labeled\nsubgraphs to strengthen domain generalization. Experiments on six diverse\nspatio-temporal datasets show that DRIK consistently outperforms existing\nmethods, achieving up to 12.48% lower MAE while maintaining strong scalability.", "AI": {"tldr": "The paper identifies information leakage issues in conventional inductive kriging evaluation and proposes a 3x3 partition method and DRIK approach to improve out-of-distribution generalization.", "motivation": "Conventional training-evaluation setups for inductive kriging suffer from information leakage and poor out-of-distribution generalization, which obscures true performance characteristics.", "method": "Proposes a 3x3 partition to separate training, validation, and test sets cleanly, and introduces DRIK with three-tier strategy: node coordinate perturbation, edge dropping, and pseudo-labeled subgraph addition.", "result": "DRIK consistently outperforms existing methods on six spatio-temporal datasets, achieving up to 12.48% lower MAE while maintaining strong scalability.", "conclusion": "The proposed 3x3 partition and DRIK approach effectively address information leakage and enhance out-of-distribution generalization in inductive kriging."}}
{"id": "2509.24183", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24183", "abs": "https://arxiv.org/abs/2509.24183", "authors": ["Ran Xu", "Kaixin Ma", "Wenhao Yu", "Hongming Zhang", "Joyce C. Ho", "Carl Yang", "Dong Yu"], "title": "Retrieval-augmented GUI Agents with Generative Guidelines", "comment": "Accepted to EMNLP 2025 (Main Conference)", "summary": "GUI agents powered by vision-language models (VLMs) show promise in\nautomating complex digital tasks. However, their effectiveness in real-world\napplications is often limited by scarce training data and the inherent\ncomplexity of these tasks, which frequently require long-tailed knowledge\ncovering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that\nleverages web tutorials at inference time. RAG-GUI is first warm-started via\nsupervised finetuning (SFT) and further refined through self-guided rejection\nsampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as\na generic plug-in that enhances any VLM-based agent. Evaluated across three\ndistinct tasks, it consistently outperforms baseline agents and surpasses other\ninference baselines by 2.6% to 13.3% across two model sizes, demonstrating\nstrong generalization and practical plug-and-play capabilities in real-world\nscenarios.", "AI": {"tldr": "RAG-GUI is a lightweight VLM that enhances GUI agents by leveraging web tutorials at inference time, using supervised finetuning and self-guided rejection sampling finetuning to improve performance on complex digital tasks.", "motivation": "GUI agents face limitations due to scarce training data and the complexity of real-world tasks that require long-tailed knowledge for rare scenarios.", "method": "Proposes RAG-GUI, a model-agnostic VLM that uses web tutorials at inference time, warm-started via supervised finetuning and refined through self-guided rejection sampling finetuning.", "result": "Outperforms baseline agents and other inference baselines by 2.6% to 13.3% across two model sizes, demonstrating strong generalization in three distinct tasks.", "conclusion": "RAG-GUI serves as an effective plug-and-play enhancement for VLM-based agents, showing practical capabilities in real-world scenarios."}}
{"id": "2509.24836", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24836", "abs": "https://arxiv.org/abs/2509.24836", "authors": ["Zhen Bi", "Zhenlin Hu", "Jinnan Yang", "Mingyang Chen", "Cheng Deng", "Yida Xue", "Zeyu Yang", "Qing Shen", "Zhenfang Liu", "Kang Zhao", "Ningyu Zhang", "Jungang Lou"], "title": "Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity", "comment": null, "summary": "Recent advances in large language models (LLMs) highlight the importance of\ntraining data structure and quality in shaping reasoning behavior. However,\nmost existing approaches focus on transforming data formats while neglecting\nthe internal reasoning complexity of training samples, leaving the reasoning\npotential of data under-explored and underutilized. In this work, we posit that\nLLM logical reasoning performance is jointly constrained by the potential of\nthe training data and the cognitive capacity of the model. To make this\nrelationship measurable, we introduce Data Reasoning Intensity (DRI), a novel\nmetric that quantifies the latent logical reasoning complexity of samples by\ndecomposing and aggregating their logical structures. This allows us to analyze\nhow well current LLMs utilize logical reasoning signals and identify\nperformance gaps relative to data potential. Based on this insight, we\nintroduce a re-cognizing optimization strategy that systematically enhances the\nlogical reasoning intensity of training data.Rather than increasing data\nvolume, our method re-optimizes existing samples to better align with the LLM's\nlogical reasoning boundary. Extensive experiments show that our approach\nsignificantly improves performance and generalization over data-centric\nstrategies. We further validate our method under a reinforcement learning\nframework. Our results indicate that prioritizing reasoning complexity in data\nrather than sheer scale or superficial form is essential to realizing LLMs'\nfull cognitive potential.", "AI": {"tldr": "This paper introduces Data Reasoning Intensity (DRI) to measure the latent logical reasoning complexity in training data, and proposes a re-cognizing optimization strategy to enhance LLM reasoning performance by focusing on reasoning complexity rather than data volume.", "motivation": "Current LLM training approaches focus on data format transformation but neglect internal reasoning complexity, leaving reasoning potential under-utilized. The authors believe LLM reasoning performance is constrained by both data potential and model cognitive capacity.", "method": "Introduce Data Reasoning Intensity (DRI) metric to quantify logical reasoning complexity by decomposing and aggregating logical structures. Propose a re-cognizing optimization strategy that systematically enhances logical reasoning intensity of training data to better align with LLM's reasoning boundary.", "result": "Extensive experiments show significant improvements in performance and generalization over data-centric strategies. The method is also validated under reinforcement learning framework.", "conclusion": "Prioritizing reasoning complexity in data rather than sheer scale or superficial form is essential to realizing LLMs' full cognitive potential."}}
{"id": "2509.23638", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23638", "abs": "https://arxiv.org/abs/2509.23638", "authors": ["Enda Yu", "Zhaoning Zhang", "Dezun Dong", "Yongwei Wu", "Xiangke Liao"], "title": "PreScope: Unleashing the Power of Prefetching for Resource-Constrained MoE Inference", "comment": null, "summary": "Mixture-of-Experts (MoE) models face memory and PCIe latency bottlenecks when\ndeployed on commodity hardware. Offloading expert weights to CPU memory results\nin PCIe transfer latency that exceeds GPU computation by several folds. We\npresent PreScope, a prediction-driven expert scheduling system that addresses\nthree key challenges: inaccurate activation prediction, PCIe bandwidth\ncompetition, and cross-device scheduling complexity. Our solution includes: 1)\nLearnable Layer-Aware Predictor (LLaPor) that captures layer-specific expert\nactivation patterns; 2) Prefetch-Aware Cross-Layer Scheduling (PreSched) that\ngenerates globally optimal plans balancing prefetching costs and loading\noverhead; 3) Asynchronous I/O Optimizer (AsyncIO) that decouples I/O from\ncomputation, eliminating waiting bubbles. PreScope achieves 141% higher\nthroughput and 74.6% lower latency than state-of-the-art solutions.", "AI": {"tldr": "PreScope is a prediction-driven expert scheduling system that addresses memory and PCIe latency bottlenecks in Mixture-of-Experts models by using learnable predictors, cross-layer scheduling, and asynchronous I/O optimization.", "motivation": "Mixture-of-Experts models face significant memory and PCIe latency bottlenecks when deployed on commodity hardware, with CPU offloading causing PCIe transfer latency that exceeds GPU computation by several folds.", "method": "1) Learnable Layer-Aware Predictor (LLaPor) for expert activation patterns; 2) Prefetch-Aware Cross-Layer Scheduling (PreSched) for optimal plans; 3) Asynchronous I/O Optimizer (AsyncIO) to decouple I/O from computation.", "result": "PreScope achieves 141% higher throughput and 74.6% lower latency compared to state-of-the-art solutions.", "conclusion": "PreScope effectively addresses the key challenges in MoE model deployment through its prediction-driven scheduling approach, significantly improving performance on commodity hardware."}}
{"id": "2509.24186", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24186", "abs": "https://arxiv.org/abs/2509.24186", "authors": ["Zhimeng Luo", "Lixin Wu", "Adam Frisch", "Daqing He"], "title": "Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific Medical Capabilities of 80 Large Language Models", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly proposed for high-stakes\nmedical applications, there has emerged a critical need for reliable and\naccurate evaluation methodologies. Traditional accuracy metrics fail\ninadequately as they neither capture question characteristics nor offer\ntopic-specific insights. To address this gap, we introduce \\textsc{MedIRT}, a\nrigorous evaluation framework grounded in Item Response Theory (IRT), the gold\nstandard in high-stakes educational testing. Unlike previous research relying\non archival data, we prospectively gathered fresh responses from 80 diverse\nLLMs on a balanced, 1,100-question USMLE-aligned benchmark. Using one\nunidimensional two-parameter logistic IRT model per topic, we estimate LLM's\nlatent model ability jointly with question difficulty and discrimination,\nyielding more stable and nuanced performance rankings than accuracy alone.\nNotably, we identify distinctive ``spiky'' ability profiles, where overall\nrankings can be misleading due to highly specialized model abilities. While\n\\texttt{GPT-5} was the top performer in a majority of domains (8 of 11), it was\noutperformed in Social Science and Communication by \\texttt{Claude-3-opus},\ndemonstrating that even an overall 23rd-ranked model can hold the top spot for\nspecific competencies. Furthermore, we demonstrate IRT's utility in auditing\nbenchmarks by identifying flawed questions. We synthesize these findings into a\npractical decision-support framework that integrates our multi-factor\ncompetency profiles with operational metrics. This work establishes a robust,\npsychometrically grounded methodology essential for the safe, effective, and\ntrustworthy deployment of LLMs in healthcare.", "AI": {"tldr": "MedIRT is an Item Response Theory-based framework for evaluating LLMs in medical applications, using a 1,100-question USMLE benchmark to measure latent model abilities, question difficulty, and discrimination, revealing specialized competency profiles beyond overall rankings.", "motivation": "Traditional accuracy metrics are inadequate for evaluating LLMs in high-stakes medical applications as they fail to capture question characteristics or provide topic-specific insights, creating a critical need for more reliable evaluation methodologies.", "method": "Prospectively gathered responses from 80 diverse LLMs on a balanced 1,100-question USMLE-aligned benchmark, using unidimensional two-parameter logistic IRT models per topic to estimate latent model ability, question difficulty, and discrimination parameters.", "result": "Identified distinctive \"spiky\" ability profiles where overall rankings can be misleading; GPT-5 was top performer in 8 of 11 domains but was outperformed by Claude-3-opus in Social Science and Communication; IRT successfully identified flawed questions for benchmark auditing.", "conclusion": "MedIRT establishes a robust, psychometrically grounded methodology essential for safe, effective, and trustworthy deployment of LLMs in healthcare, providing a practical decision-support framework that integrates multi-factor competency profiles with operational metrics."}}
{"id": "2509.24855", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24855", "abs": "https://arxiv.org/abs/2509.24855", "authors": ["Fangchen Yu", "Junchi Yao", "Ziyi Wang", "Haiyuan Wan", "Youling Huang", "Bo Zhang", "Shuyue Hu", "Dongzhan Zhou", "Ning Ding", "Ganqu Cui", "Lei Bai", "Wanli Ouyang", "Peng Ye"], "title": "PhysicsMinions: Winning Gold Medals in the Latest Physics Olympiads with a Coevolutionary Multimodal Multi-Agent System", "comment": null, "summary": "Physics is central to understanding and shaping the real world, and the\nability to solve physics problems is a key indicator of real-world physical\nintelligence. Physics Olympiads, renowned as the crown of competitive physics,\nprovide a rigorous testbed requiring complex reasoning and deep multimodal\nunderstanding, yet they remain largely underexplored in AI research. Existing\napproaches are predominantly single-model based, and open-source MLLMs rarely\nreach gold-medal-level performance. To address this gap, we propose\nPhysicsMinions, a coevolutionary multi-agent system for Physics Olympiad. Its\narchitecture features three synergistic studios: a Visual Studio to interpret\ndiagrams, a Logic Studio to formulate solutions, and a Review Studio to perform\ndual-stage verification. The system coevolves through an iterative refinement\nloop where feedback from the Review Studio continuously guides the Logic\nStudio, enabling the system to self-correct and converge towards the ground\ntruth. Evaluated on the HiPhO benchmark spanning 7 latest physics Olympiads,\nPhysicsMinions delivers three major breakthroughs: (i) Strong generalization:\nit consistently improves both open-source and closed-source models of different\nsizes, delivering clear benefits over their single-model baselines; (ii)\nHistoric breakthroughs: it elevates open-source models from only 1-2 to 6 gold\nmedals across 7 Olympiads, achieving the first-ever open-source gold medal in\nthe latest International Physics Olympiad (IPhO) under the average-score\nmetric; and (iii) Scaling to human expert: it further advances the open-source\nPass@32 score to 26.8/30 points on the latest IPhO, ranking 4th of 406\ncontestants and far surpassing the top single-model score of 22.7 (ranked\n22nd). Generally, PhysicsMinions offers a generalizable framework for\nOlympiad-level problem solving, with the potential to extend across\ndisciplines.", "AI": {"tldr": "PhysicsMinions is a coevolutionary multi-agent system that achieves state-of-the-art performance on Physics Olympiad problems through synergistic collaboration between visual, logic, and review studios, enabling open-source models to reach gold-medal-level performance for the first time.", "motivation": "Physics Olympiads represent the pinnacle of physical intelligence testing but remain largely unexplored in AI research, with existing single-model approaches failing to achieve gold-medal-level performance.", "method": "A coevolutionary multi-agent system with three studios: Visual Studio for diagram interpretation, Logic Studio for solution formulation, and Review Studio for dual-stage verification, operating through iterative refinement loops.", "result": "Achieves historic breakthroughs: elevates open-source models from 1-2 to 6 gold medals across 7 Olympiads, first-ever open-source gold medal in IPhO, and ranks 4th of 406 contestants with 26.8/30 points.", "conclusion": "PhysicsMinions provides a generalizable framework for Olympiad-level problem solving that can potentially extend across disciplines, demonstrating the power of multi-agent collaboration over single-model approaches."}}
{"id": "2509.23660", "categories": ["cs.LG", "I.2.0"], "pdf": "https://arxiv.org/pdf/2509.23660", "abs": "https://arxiv.org/abs/2509.23660", "authors": ["Ranhui Yan", "Jia cai"], "title": "Virtual Nodes based Heterogeneous Graph Convolutional Neural Network for Efficient Long-Range Information Aggregation", "comment": null, "summary": "Heterogeneous Graph Neural Networks (HGNNs) have exhibited powerful\nperformance in heterogeneous graph learning by aggregating information from\nvarious types of nodes and edges. However, existing heterogeneous graph models\noften struggle to capture long-range information or necessitate stacking\nnumerous layers to learn such dependencies, resulting in high computational\ncomplexity and encountering over-smoothing issues. In this paper, we propose a\nVirtual Nodes based Heterogeneous Graph Convolutional Network (VN-HGCN), which\nleverages virtual nodes to facilitate enhanced information flow within the\ngraph. Virtual nodes are auxiliary nodes interconnected with all nodes of a\nspecific type in the graph, facilitating efficient aggregation of long-range\ninformation across different types of nodes and edges. By incorporating virtual\nnodes into the graph structure, VN-HGCN achieves effective information\naggregation with only $4$ layers. Additionally, we demonstrate that VN-HGCN can\nserve as a versatile framework that can be seamlessly applied to other HGNN\nmodels, showcasing its generalizability. Empirical evaluations validate the\neffectiveness of VN-HGCN, and extensive experiments conducted on three\nreal-world heterogeneous graph datasets demonstrate the superiority of our\nmodel over several state-of-the-art baselines.", "AI": {"tldr": "VN-HGCN is a heterogeneous graph neural network that uses virtual nodes to improve long-range information flow, achieving better performance with fewer layers (only 4) compared to existing methods.", "motivation": "Existing heterogeneous graph models struggle with capturing long-range dependencies, requiring many layers that lead to computational complexity and over-smoothing issues.", "method": "Proposes VN-HGCN which introduces virtual nodes that connect to all nodes of specific types, enabling efficient aggregation of long-range information across different node and edge types.", "result": "Empirical evaluations show VN-HGCN's effectiveness, with extensive experiments on three real-world datasets demonstrating superiority over state-of-the-art baselines.", "conclusion": "VN-HGCN provides an effective framework for heterogeneous graph learning that can be applied to other HGNN models, offering improved performance with reduced computational complexity."}}
{"id": "2509.24189", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24189", "abs": "https://arxiv.org/abs/2509.24189", "authors": ["Luyang Zhang", "Siyuan Peng", "Jialu Wang", "Shichao Zhu", "Beibei Li", "Zhongcun Wang", "Guangmou Pan", "Yan Li", "Song Yang"], "title": "PET: Preference Evolution Tracking with LLM-Generated Explainable Distribution", "comment": null, "summary": "Understanding how user preference evolves over time is a fundamental\nchallenge central to modern digital ecosystems, for which Large Language Models\n(LLMs) are an increasingly prominent and popular approach due to their ability\nto comprehend the rich semantic context within behavioral data. A common\npractice is to use LLMs to predict a user's next action by directly generating\na ranked list of preferred items. Although effective for short-term prediction,\nthe end-to-end generation paradigm inherently limits personalization. Its\nopaque decision-making process obscures holistic user profiling and exacerbates\npopularity bias. To address these limitations, we propose Preference Evolution\nTracking (PET), a framework that reframes the task as inferring a dynamic\nprobability distribution over a stable and interpretable lattice of preference\nclusters. By applying logit-probing and generative classification techniques,\nPET infers a user's preference as a probability distribution, enabling\ntransparent preference learning. On public benchmarks (Yelp, MovieLens), PET\nimproves ranking quality by up to 40% in NDCG over direct generation baselines.\nOn a large-scale, real-world dataset from a short-video platform, it excels at\nranking long-tail contents, significantly outperforming a SOTA production model\nby 7 times in the NDCG score. Ultimately, PET transforms the user profile model\nfrom direct preference list generation to a transparent distributional\npreference mapping, paving the way for more explainable, fair, and diverse\npersonalization systems.", "AI": {"tldr": "PET framework reframes user preference prediction from direct LLM generation to inferring dynamic probability distributions over interpretable preference clusters, improving ranking quality and addressing transparency issues.", "motivation": "Current LLM-based approaches for user preference prediction suffer from opaque decision-making, limited personalization, and popularity bias, making holistic user profiling difficult.", "method": "Proposes Preference Evolution Tracking (PET) using logit-probing and generative classification to infer user preferences as probability distributions over stable preference clusters rather than direct item generation.", "result": "PET improves ranking quality by up to 40% in NDCG on public benchmarks and outperforms SOTA production model by 7 times in NDCG on real-world short-video platform data, particularly excelling at ranking long-tail content.", "conclusion": "PET transforms user profiling from direct preference generation to transparent distributional mapping, enabling more explainable, fair, and diverse personalization systems."}}
{"id": "2509.24877", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24877", "abs": "https://arxiv.org/abs/2509.24877", "authors": ["Xiao Jia", "Zhanzhan Zhao"], "title": "The Emergence of Social Science of Large Language Models", "comment": null, "summary": "The social science of large language models (LLMs) examines how these systems\nevoke mind attributions, interact with one another, and transform human\nactivity and institutions. We conducted a systematic review of 270 studies,\ncombining text embeddings, unsupervised clustering and topic modeling to build\na computational taxonomy. Three domains emerge organically across the reviewed\nliterature. LLM as Social Minds examines whether and when models display\nbehaviors that elicit attributions of cognition, morality and bias, while\naddressing challenges such as test leakage and surface cues. LLM Societies\nexamines multi-agent settings where interaction protocols, architectures and\nmechanism design shape coordination, norms, institutions and collective\nepistemic processes. LLM-Human Interactions examines how LLMs reshape tasks,\nlearning, trust, work and governance, and how risks arise at the human-AI\ninterface. This taxonomy provides a reproducible map of a fragmented field,\nclarifies evidentiary standards across levels of analysis, and highlights\nopportunities for cumulative progress in the social science of artificial\nintelligence.", "AI": {"tldr": "This paper presents a systematic review of 270 studies on the social science of large language models (LLMs), using computational methods to develop a taxonomy that organizes research into three domains: LLM as Social Minds, LLM Societies, and LLM-Human Interactions.", "motivation": "To address the fragmented nature of research on the social science of LLMs and provide a systematic framework for organizing and understanding how these models evoke mind attributions, interact with each other, and transform human activity and institutions.", "method": "Conducted a systematic review of 270 studies using text embeddings, unsupervised clustering, and topic modeling to build a computational taxonomy of the field.", "result": "Identified three main research domains: 1) LLM as Social Minds (examining cognition, morality, bias attributions), 2) LLM Societies (multi-agent interactions and coordination), and 3) LLM-Human Interactions (impact on tasks, learning, work, and governance).", "conclusion": "The taxonomy provides a reproducible map of the fragmented field, clarifies evidentiary standards across different levels of analysis, and highlights opportunities for cumulative progress in the social science of artificial intelligence."}}
{"id": "2509.23662", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23662", "abs": "https://arxiv.org/abs/2509.23662", "authors": ["Fanlong Zeng", "Wensheng Gan", "Jiayang Wu", "Philip S. Yu"], "title": "Pure Node Selection for Imbalanced Graph Node Classification", "comment": "Preprint, 8 tables, 9 figures", "summary": "The problem of class imbalance refers to an uneven distribution of quantity\namong classes in a dataset, where some classes are significantly\nunderrepresented compared to others. Class imbalance is also prevalent in\ngraph-structured data. Graph neural networks (GNNs) are typically based on the\nassumption of class balance, often overlooking the issue of class imbalance. In\nour investigation, we identified a problem, which we term the Randomness\nAnomalous Connectivity Problem (RACP), where certain off-the-shelf models are\naffected by random seeds, leading to a significant performance degradation. To\neliminate the influence of random factors in algorithms, we proposed PNS (Pure\nNode Sampling) to address the RACP in the node synthesis stage. Unlike existing\napproaches that design specialized algorithms to handle either quantity\nimbalance or topological imbalance, PNS is a novel plug-and-play module that\noperates directly during node synthesis to mitigate RACP. Moreover, PNS also\nalleviates performance degradation caused by abnormal distribution of node\nneighbors. We conduct a series of experiments to identify what factors are\ninfluenced by random seeds. Experimental results demonstrate the effectiveness\nand stability of our method, which not only eliminates the effect of\nunfavorable random seeds but also outperforms the baseline across various\nbenchmark datasets with different GNN backbones. Data and code are available at\nhttps://github.com/flzeng1/PNS.", "AI": {"tldr": "PNS is a plug-and-play module that addresses the Randomness Anomalous Connectivity Problem (RACP) in graph neural networks by operating during node synthesis, eliminating random seed effects and improving performance on imbalanced graph data.", "motivation": "Class imbalance in graph-structured data is often overlooked by GNNs, and random seeds can cause significant performance degradation (RACP), which needs to be addressed without designing specialized algorithms for each type of imbalance.", "method": "Proposed Pure Node Sampling (PNS) - a plug-and-play module that operates during node synthesis to mitigate RACP and abnormal neighbor distribution, eliminating random factor influences.", "result": "Experimental results show PNS effectively eliminates unfavorable random seed effects and outperforms baselines across various benchmark datasets with different GNN backbones, demonstrating stability and effectiveness.", "conclusion": "PNS successfully addresses the RACP problem in graph neural networks, providing a stable solution that works across different datasets and GNN architectures while handling class imbalance issues."}}
{"id": "2509.24193", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24193", "abs": "https://arxiv.org/abs/2509.24193", "authors": ["Ran Xu", "Yuchen Zhuang", "Zihan Dong", "Jonathan Wang", "Yue Yu", "Joyce C. Ho", "Linjun Zhang", "Haoyu Wang", "Wenqi Shi", "Carl Yang"], "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play", "comment": "Accepted to NeurIPS 2025 (Spotlight)", "summary": "Search-augmented LLMs often struggle with complex reasoning tasks due to\nineffective multi-hop retrieval and limited reasoning ability. We propose\nAceSearcher, a cooperative self-play framework that trains a single large\nlanguage model (LLM) to alternate between two roles: a decomposer that breaks\ndown complex queries and a solver that integrates retrieved contexts for answer\ngeneration. AceSearcher couples supervised fine-tuning on a diverse mixture of\nsearch, reasoning, and decomposition tasks with reinforcement fine-tuning\noptimized for final answer accuracy, eliminating the need for intermediate\nannotations. Extensive experiments on three reasoning-intensive tasks across 10\ndatasets show that AceSearcher outperforms state-of-the-art baselines,\nachieving an average exact match improvement of 7.6%. Remarkably, on\ndocument-level finance reasoning tasks, AceSearcher-32B matches the performance\nof the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller\nscales (1.5B and 8B), AceSearcher often surpasses existing search-augmented\nLLMs with up to 9x more parameters, highlighting its exceptional efficiency and\neffectiveness in tackling complex reasoning tasks. Our code will be published\nat https://github.com/ritaranx/AceSearcher and\nhttps://huggingface.co/AceSearcher.", "AI": {"tldr": "AceSearcher is a cooperative self-play framework that trains a single LLM to alternate between decomposer and solver roles, achieving state-of-the-art performance on complex reasoning tasks with significantly fewer parameters.", "motivation": "Search-augmented LLMs struggle with complex reasoning due to ineffective multi-hop retrieval and limited reasoning ability, requiring a more effective approach for handling complex queries.", "method": "Uses cooperative self-play framework where a single LLM alternates between decomposer (breaks down queries) and solver (integrates contexts) roles, combining supervised fine-tuning on diverse tasks with reinforcement fine-tuning optimized for answer accuracy.", "result": "Outperforms state-of-the-art baselines with 7.6% average exact match improvement, matches DeepSeek-V3 performance with <5% parameters on finance tasks, and surpasses larger models with up to 9x more parameters.", "conclusion": "AceSearcher demonstrates exceptional efficiency and effectiveness in tackling complex reasoning tasks, providing a powerful solution for search-augmented LLMs without requiring intermediate annotations."}}
{"id": "2509.24897", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24897", "abs": "https://arxiv.org/abs/2509.24897", "authors": ["Yang Shi", "Yuhao Dong", "Yue Ding", "Yuran Wang", "Xuanyu Zhu", "Sheng Zhou", "Wenting Liu", "Haochen Tian", "Rundong Wang", "Huanqian Wang", "Zuyan Liu", "Bohan Zeng", "Ruizhe Chen", "Qixun Wang", "Zhuoran Zhang", "Xinlong Chen", "Chengzhuo Tong", "Bozhou Li", "Chaoyou Fu", "Qiang Liu", "Haotian Wang", "Wenjing Yang", "Yuanxing Zhang", "Pengfei Wan", "Yi-Fan Zhang", "Ziwei Liu"], "title": "RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark", "comment": null, "summary": "The integration of visual understanding and generation into unified\nmultimodal models represents a significant stride toward general-purpose AI.\nHowever, a fundamental question remains unanswered by existing benchmarks: does\nthis architectural unification actually enable synergetic interaction between\nthe constituent capabilities? Existing evaluation paradigms, which primarily\nassess understanding and generation in isolation, are insufficient for\ndetermining whether a unified model can leverage its understanding to enhance\nits generation, or use generative simulation to facilitate deeper\ncomprehension. To address this critical gap, we introduce RealUnify, a\nbenchmark specifically designed to evaluate bidirectional capability synergy.\nRealUnify comprises 1,000 meticulously human-annotated instances spanning 10\ncategories and 32 subtasks. It is structured around two core axes: 1)\nUnderstanding Enhances Generation, which requires reasoning (e.g., commonsense,\nlogic) to guide image generation, and 2) Generation Enhances Understanding,\nwhich necessitates mental simulation or reconstruction (e.g., of transformed or\ndisordered visual inputs) to solve reasoning tasks. A key contribution is our\ndual-evaluation protocol, which combines direct end-to-end assessment with a\ndiagnostic stepwise evaluation that decomposes tasks into distinct\nunderstanding and generation phases. This protocol allows us to precisely\ndiscern whether performance bottlenecks stem from deficiencies in core\nabilities or from a failure to integrate them. Through large-scale evaluations\nof 12 leading unified models and 6 specialized baselines, we find that current\nunified models still struggle to achieve effective synergy, indicating that\narchitectural unification alone is insufficient. These results highlight the\nneed for new training strategies and inductive biases to fully unlock the\npotential of unified modeling.", "AI": {"tldr": "RealUnify is a benchmark for evaluating bidirectional capability synergy in unified multimodal models, testing whether understanding enhances generation and vice versa, revealing current models struggle with effective integration despite architectural unification.", "motivation": "Existing benchmarks only assess visual understanding and generation in isolation, failing to determine if unified models can leverage bidirectional synergy between these capabilities for mutual enhancement.", "method": "Created RealUnify benchmark with 1,000 human-annotated instances across 10 categories and 32 subtasks, featuring dual-evaluation protocol combining end-to-end assessment with diagnostic stepwise evaluation to isolate understanding and generation phases.", "result": "Evaluation of 12 unified models and 6 specialized baselines shows current unified models struggle to achieve effective capability synergy, indicating architectural unification alone is insufficient.", "conclusion": "New training strategies and inductive biases are needed to fully unlock the potential of unified multimodal modeling beyond mere architectural unification."}}
{"id": "2509.23665", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2509.23665", "abs": "https://arxiv.org/abs/2509.23665", "authors": ["Kristina P. Sinaga", "Arjun S. Nair"], "title": "Calibration Meets Reality: Making Machine Learning Predictions Trustworthy", "comment": "30 pages, 7 figures, 5 tables", "summary": "Post-hoc calibration methods are widely used to improve the reliability of\nprobabilistic predictions from machine learning models. Despite their\nprevalence, a comprehensive theoretical understanding of these methods remains\nelusive, particularly regarding their performance across different datasets and\nmodel architectures. Input features play a crucial role in shaping model\npredictions and, consequently, their calibration. However, the interplay\nbetween feature quality and calibration performance has not been thoroughly\ninvestigated. In this work, we present a rigorous theoretical analysis of\npost-hoc calibration methods, focusing on Platt scaling and isotonic\nregression. We derive convergence guarantees, computational complexity bounds,\nand finite-sample performance metrics for these methods. Furthermore, we\nexplore the impact of feature informativeness on calibration performance\nthrough controlled synthetic experiments. Our empirical evaluation spans a\ndiverse set of real-world datasets and model architectures, demonstrating\nconsistent improvements in calibration metrics across various scenarios. By\nexamining calibration performance under varying feature conditions utilizing\nonly informative features versus complete feature spaces including noise\ndimensions, we provide fundamental insights into the robustness and reliability\nof different calibration approaches. Our findings offer practical guidelines\nfor selecting appropriate calibration methods based on dataset characteristics\nand computational constraints, bridging the gap between theoretical\nunderstanding and practical implementation in uncertainty quantification. Code\nand experimental data are available at:\nhttps://github.com/Ajwebdevs/calibration-analysis-experiments.", "AI": {"tldr": "Theoretical analysis of post-hoc calibration methods (Platt scaling, isotonic regression) showing their performance depends on feature informativeness, with empirical validation across diverse datasets.", "motivation": "Lack of comprehensive theoretical understanding of post-hoc calibration methods and their performance across different datasets and model architectures, particularly regarding the role of feature quality.", "method": "Rigorous theoretical analysis with convergence guarantees, complexity bounds, and finite-sample metrics; controlled synthetic experiments to study feature informativeness impact; empirical evaluation on real-world datasets.", "result": "Derived theoretical guarantees for calibration methods; demonstrated consistent calibration improvements across scenarios; showed calibration performance varies with feature informativeness (better with informative features vs. noisy features).", "conclusion": "Provides fundamental insights into calibration robustness and practical guidelines for method selection based on dataset characteristics and computational constraints, bridging theory and practice in uncertainty quantification."}}
{"id": "2509.24202", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24202", "abs": "https://arxiv.org/abs/2509.24202", "authors": ["Linwei Tao", "Yi-Fan Yeh", "Bo Kai", "Minjing Dong", "Tao Huang", "Tom A. Lamb", "Jialin Yu", "Philip H. S. Torr", "Chang Xu"], "title": "Can Large Language Models Express Uncertainty Like Human?", "comment": "10 pages", "summary": "Large language models (LLMs) are increasingly used in high-stakes settings,\nwhere overconfident responses can mislead users. Reliable confidence estimation\nhas been shown to enhance trust and task accuracy. Yet existing methods face\npractical barriers: logits are often hidden, multi-sampling is computationally\nexpensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score)\ndeviates from natural communication. We revisit linguistic confidence (LC),\nwhere models express uncertainty through hedging language (e.g., probably,\nmight), offering a lightweight and human-centered alternative. To advance this\ndirection, we (1) release the first diverse, large-scale dataset of hedging\nexpressions with human-annotated confidence scores, and (2) propose a\nlightweight mapper that converts hedges into confidence scores at near-zero\ncost. Building on these resources, we (3) conduct the first systematic study of\nLC across modern LLMs and QA benchmarks, revealing that while most LLMs\nunderperform in expressing reliable LC, carefully designed prompting achieves\ncompetitive calibration and discriminability. Finally, we (4) introduce a\nfine-tuning framework that further improves LC reliability. Taken together, our\nwork positions linguistic confidence as a scalable, efficient, and\nhuman-aligned approach to LLM uncertainty estimation, and calls for deeper\nexploration of this promising yet underexplored direction.", "AI": {"tldr": "This paper proposes linguistic confidence (LC) as a lightweight alternative to traditional confidence estimation methods for LLMs, using hedging language instead of numerical scores. The authors create a dataset of hedging expressions, develop a mapper to convert hedges to confidence scores, systematically study LC across LLMs, and introduce a fine-tuning framework to improve LC reliability.", "motivation": "Existing confidence estimation methods for LLMs face practical barriers: hidden logits, computational expense of multi-sampling, and unnatural verbalized numerical uncertainty. Linguistic confidence through hedging language offers a lightweight, human-centered alternative.", "method": "1) Release first large-scale dataset of hedging expressions with human-annotated confidence scores; 2) Propose lightweight mapper to convert hedges to confidence scores; 3) Conduct systematic study of LC across modern LLMs and QA benchmarks; 4) Introduce fine-tuning framework to improve LC reliability.", "result": "While most LLMs underperform in expressing reliable linguistic confidence, carefully designed prompting achieves competitive calibration and discriminability. The fine-tuning framework further improves LC reliability.", "conclusion": "Linguistic confidence is positioned as a scalable, efficient, and human-aligned approach to LLM uncertainty estimation, calling for deeper exploration of this promising direction."}}
{"id": "2509.24906", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24906", "abs": "https://arxiv.org/abs/2509.24906", "authors": ["Max Pellert", "Clemens M. Lechner", "Indira Sen", "Markus Strohmaier"], "title": "Neural network embeddings recover value dimensions from psychometric survey items on par with human data", "comment": null, "summary": "This study introduces \"Survey and Questionnaire Item Embeddings\nDifferentials\" (SQuID), a novel methodological approach that enables neural\nnetwork embeddings to effectively recover latent dimensions from psychometric\nsurvey items. We demonstrate that embeddings derived from large language\nmodels, when processed with SQuID, can recover the structure of human values\nobtained from human rater judgments on the Revised Portrait Value Questionnaire\n(PVQ-RR). Our experimental validation compares multiple embedding models across\na number of evaluation metrics. Unlike previous approaches, SQuID successfully\naddresses the challenge of obtaining negative correlations between dimensions\nwithout requiring domain-specific fine-tuning. Quantitative analysis reveals\nthat our embedding-based approach explains 55% of variance in\ndimension-dimension similarities compared to human data. Multidimensional\nscaling configurations from both types of data show fair factor congruence\ncoefficients and largely follow the underlying theory. These results\ndemonstrate that semantic embeddings can effectively replicate psychometric\nstructures previously established through extensive human surveys. The approach\noffers substantial advantages in cost, scalability and flexibility while\nmaintaining comparable quality to traditional methods. Our findings have\nsignificant implications for psychometrics and social science research,\nproviding a complementary methodology that could expand the scope of human\nbehavior and experience represented in measurement tools.", "AI": {"tldr": "SQuID enables neural embeddings to recover latent psychometric dimensions from survey items, achieving 55% variance explained compared to human data without domain-specific fine-tuning.", "motivation": "To develop a scalable, cost-effective method that can replicate psychometric structures from human surveys using semantic embeddings, addressing the challenge of obtaining negative correlations between dimensions.", "method": "Uses SQuID (Survey and Questionnaire Item Embeddings Differentials) to process large language model embeddings, comparing multiple embedding models across evaluation metrics to recover human value structures from PVQ-RR.", "result": "Explains 55% of variance in dimension-dimension similarities compared to human data, shows fair factor congruence coefficients, and successfully recovers the structure of human values without requiring negative correlation constraints.", "conclusion": "Semantic embeddings can effectively replicate psychometric structures with comparable quality to traditional methods, offering significant advantages in cost, scalability and flexibility for psychometrics and social science research."}}
{"id": "2509.23666", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23666", "abs": "https://arxiv.org/abs/2509.23666", "authors": ["Divya Jyoti Bajpai", "Manjesh Kumar Hanawal"], "title": "Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and Reliability", "comment": "Accepted as poster in NeurIPS 2025", "summary": "Early-Exit Deep Neural Networks enable adaptive inference by allowing\nprediction at intermediary layers, significantly reducing computational costs\nand latency. Most of the early exit strategies greedily exit a sample at an\nintermediary layer if the confidence in class prediction exceeds a predefined\nthreshold that is set using a static validation set. This is problematic as the\nmodel might be overconfident in a wrong class. Also, they are not robust to\ndistribution shifts encountered in deployment, which can undermine model\ntrustworthiness and accuracy. To address these challenges, we propose UAT that\nadapts the threshold for exit decisions using a Multi-Armed Bandit framework,\nenabling online, unsupervised adjustment of exit decisions. UAT makes decisions\nbased on a new reward function that assesses predictive certainty and its\nreliability to balance computational efficiency and prediction quality while\npenalizing unnecessary late exits. We provide guarantees on risk achieved by\nUAT and validate its performance on diverse tasks spanning vision-language\nunderstanding, text generation, and classification. Our framework demonstrates\nconsistent improvements in speedup (1.70-2.10x) with a minimal performance drop\n(<2%) as compared to full model performance. Our source code is available at\nhttps://github.com/Div290/UAT.", "AI": {"tldr": "UAT proposes an adaptive threshold adjustment method for early-exit DNNs using Multi-Armed Bandit framework to improve robustness against distribution shifts and prevent overconfident wrong predictions.", "motivation": "Traditional early-exit strategies use static thresholds that can lead to overconfidence in wrong classes and lack robustness to distribution shifts, undermining model trustworthiness.", "method": "UAT adapts exit thresholds online using Multi-Armed Bandit framework with a reward function that balances predictive certainty, reliability, computational efficiency, and penalizes unnecessary late exits.", "result": "UAT achieves 1.70-2.10x speedup with minimal performance drop (<2%) compared to full model performance across vision-language understanding, text generation, and classification tasks.", "conclusion": "The proposed UAT framework provides guaranteed risk bounds and demonstrates consistent improvements in computational efficiency while maintaining prediction quality across diverse tasks."}}
{"id": "2509.24210", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24210", "abs": "https://arxiv.org/abs/2509.24210", "authors": ["Gaurav Srivastava", "Aafiya Hussain", "Zhenyu Bi", "Swastik Roy", "Priya Pitre", "Meng Lu", "Morteza Ziyadi", "Xuan Wang"], "title": "BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models", "comment": "113 pages, 5 figures, 30 tables", "summary": "Evaluating language models fairly is becoming harder as static benchmarks\navailable on the internet risk contamination by training data. This makes it\nunclear whether models are truly reasoning or just recalling answers. In this\npaper, we introduce BeyondBench, an evaluation framework that avoids this\nproblem by using algorithmic problem generation. Unlike traditional benchmarks\nthat risk contamination from internet-scale training data, BeyondBench creates\nmathematically grounded problems on the fly, ensuring each test remains fresh\nand uncontaminated. Our framework covers 44 algorithmic tasks with a total of\n117 variations, grouped into three difficulty levels: the Easy Suite (29 tasks)\nfor basic arithmetic and statistics, the Medium Suite (5 tasks, 49 variations)\nfor sequence patterns and reasoning, and the Hard Suite (10 tasks, 68\nvariations) tackling NP-complete and constraint satisfaction problems. Each\ntask generates problems from a combinatorial space larger than 10^15 unique\ninstances, with solutions verified deterministically by mathematical proofs. We\nevaluated 101 language models, including 85 open-source and 16 closed-source\nmodels, spanning sizes from 0.5B to 141B parameters and multiple quantization\nschemes. Our results show consistent reasoning deficiencies across model\nfamilies, with performance degrading sharply as problem complexity increases\nfrom polynomial to exponential. In our Hard Suite evaluations, models such as\nGemini-2.5-pro, Llama-3.3-70B, and Qwen2.5-72B achieved average accuracies of\n56.38%, 26.91%, and 33.60%, respectively. Moreover, we observe that performance\ndrops drastically without tool usage, with GPT-5, GPT-5-mini, and GPT-5-nano\nshowing a decline of 16.81%, 28.05%, and 47.59% accuracy on the hard suite. Our\nleaderboard is publicly available at https://ctrl-gaurav.github.io/BeyondBench/", "AI": {"tldr": "BeyondBench is an evaluation framework that uses algorithmic problem generation to avoid benchmark contamination, testing 101 language models on 44 algorithmic tasks across three difficulty levels and revealing consistent reasoning deficiencies as complexity increases.", "motivation": "Traditional benchmarks risk contamination from internet training data, making it unclear if models are reasoning or just recalling answers. This paper aims to create uncontaminated evaluations that truly test reasoning capabilities.", "method": "The framework generates algorithmic problems on-the-fly from a combinatorial space larger than 10^15 unique instances, covering 44 tasks with 117 variations across Easy (basic arithmetic), Medium (sequence patterns), and Hard (NP-complete problems) suites. Solutions are verified deterministically by mathematical proofs.", "result": "Evaluation of 101 models (85 open-source, 16 closed-source) showed consistent reasoning deficiencies across model families. Performance degraded sharply with complexity: Gemini-2.5-pro (56.38%), Llama-3.3-70B (26.91%), Qwen2.5-72B (33.60%) on Hard Suite. Performance dropped drastically without tool usage (GPT-5: -16.81%, GPT-5-mini: -28.05%, GPT-5-nano: -47.59%).", "conclusion": "BeyondBench provides a contamination-free evaluation framework that reveals significant reasoning limitations in current language models, especially as problem complexity increases from polynomial to exponential, highlighting the gap between memorization and true reasoning capabilities."}}
{"id": "2509.24919", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2509.24919", "abs": "https://arxiv.org/abs/2509.24919", "authors": ["Bahti Zakirov", "Ga\u0161per Tka\u010dik"], "title": "Meta-Learning Theory-Informed Inductive Biases using Deep Kernel Gaussian Processes", "comment": "13 pages, 5 figures, 9 SI figures", "summary": "Normative and task-driven theories offer powerful top-down explanations for\nbiological systems, yet the goals of quantitatively arbitrating between\ncompeting theories, and utilizing them as inductive biases to improve\ndata-driven fits of real biological datasets are prohibitively laborious, and\noften impossible. To this end, we introduce a Bayesian meta-learning framework\ndesigned to automatically convert raw functional predictions from normative\ntheories into tractable probabilistic models. We employ adaptive deep kernel\nGaussian processes, meta-learning a kernel on synthetic data generated from a\nnormative theory. This Theory-Informed Kernel specifies a probabilistic model\nrepresenting the theory predictions -- usable for both fitting data and\nrigorously validating the theory. As a demonstration, we apply our framework to\nthe early visual system, using efficient coding as our normative theory. We\nshow improved response prediction accuracy in ex vivo recordings of mouse\nretinal ganglion cells stimulated by natural scenes compared to conventional\ndata-driven baselines, while providing well-calibrated uncertainty estimates\nand interpretable representations. Using exact Bayesian model selection, we\nalso show that our informed kernel can accurately infer the degree of\ntheory-match from data, confirming faithful encapsulation of theory structure.\nThis work provides a more general, scalable, and automated approach for\nintegrating theoretical knowledge into data-driven scientific inquiry in\nneuroscience and beyond.", "AI": {"tldr": "A Bayesian meta-learning framework that converts normative theories into probabilistic models using adaptive deep kernel Gaussian processes, improving data fitting and theory validation.", "motivation": "To automate the process of arbitrating between competing normative theories and using them as inductive biases for data-driven fits in biological systems, which is currently laborious and often impossible.", "method": "Employ adaptive deep kernel Gaussian processes to meta-learn a kernel on synthetic data generated from normative theories, creating a Theory-Informed Kernel that specifies probabilistic models.", "result": "Applied to mouse retinal ganglion cells, the framework showed improved response prediction accuracy with natural scene stimuli compared to conventional baselines, providing calibrated uncertainty estimates and interpretable representations. Bayesian model selection accurately inferred theory-match from data.", "conclusion": "The work provides a scalable, automated approach for integrating theoretical knowledge into data-driven scientific inquiry in neuroscience and beyond."}}
{"id": "2509.23667", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23667", "abs": "https://arxiv.org/abs/2509.23667", "authors": ["Sungmin Cha", "Kyunghyun Cho"], "title": "Why Alignment Must Precede Distillation: A Minimal Working Explanation", "comment": "Preprint", "summary": "For efficiency, preference alignment is often performed on compact,\nknowledge-distilled (KD) models. We argue this common practice introduces a\nsignificant limitation by overlooking a key property of the alignment's\nreference model: its distributional recall. We show that the standard KD ->\nAlign workflow diminishes the model's capacity to align rare yet desirable\nbehaviors, even under strong preference signals. We instead demonstrate that\nreversing the pipeline (i.e., Align -> KD) is essential: alignment must first\nbe performed on a high-recall reference before distillation. Our contributions\nare threefold. First, we provide a minimal working explanation of how the\nreference model constrains preference alignment objectives at a fundamental\nlevel. Second, we validate this theory in a controllable Mixture-of-Gaussians\nexperiment, where low-recall anchoring consistently results in suboptimal model\nperformance. Finally, we demonstrate that the same phenomenon holds in LLM\nalignment with the SmolLM2 family: models aligned after KD fail to effectively\nalign target behaviors, resulting in substantially lower reward and target\nprecision. In contrast, our proposed Align -> KD pipeline robustly aligns these\nbehaviors, yielding models with superior target-oriented metrics and lower\nvariance. Together, these results establish reference-model recall as a\nfirst-order design choice in alignment, offering a clear principle: alignment\nmust precede distillation.", "AI": {"tldr": "The paper argues that the common practice of distilling knowledge before alignment (KD -> Align) limits model performance, particularly for rare desirable behaviors. Instead, alignment should be performed first on high-recall reference models before distillation (Align -> KD), which yields superior results.", "motivation": "Current preference alignment practices on compact, knowledge-distilled models overlook the importance of the reference model's distributional recall, leading to suboptimal alignment of rare desirable behaviors even with strong preference signals.", "method": "The authors propose reversing the standard pipeline to Align -> KD. They provide theoretical explanation, validate with controllable Mixture-of-Gaussians experiments, and demonstrate on LLM alignment with SmolLM2 family, comparing both workflows.", "result": "Models aligned after KD fail to effectively align target behaviors with substantially lower reward and target precision. The proposed Align -> KD pipeline robustly aligns these behaviors, yielding superior target-oriented metrics and lower variance.", "conclusion": "Reference-model recall is a first-order design choice in alignment, establishing the principle that alignment must precede distillation for optimal performance, especially for rare desirable behaviors."}}
{"id": "2509.24212", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24212", "abs": "https://arxiv.org/abs/2509.24212", "authors": ["Zahra Atf", "Peter R Lewis"], "title": "ScenarioBench: Trace-Grounded Compliance Evaluation for Text-to-SQL and RAG", "comment": "Accepted for presentation at the LLMs Meet Databases (LMD) Workshop,\n  35th IEEE International Conference on Collaborative Advances in Software and\n  Computing, 2025. Workshop website: https://sites.google.com/view/lmd2025/home", "summary": "ScenarioBench is a policy-grounded, trace-aware benchmark for evaluating\nText-to-SQL and retrieval-augmented generation in compliance contexts. Each\nYAML scenario includes a no-peek gold-standard package with the expected\ndecision, a minimal witness trace, the governing clause set, and the canonical\nSQL, enabling end-to-end scoring of both what a system decides and why. Systems\nmust justify outputs using clause IDs from the same policy canon, making\nexplanations falsifiable and audit-ready. The evaluator reports decision\naccuracy, trace quality (completeness, correctness, order), retrieval\neffectiveness, SQL correctness via result-set equivalence, policy coverage,\nlatency, and an explanation-hallucination rate. A normalized Scenario\nDifficulty Index (SDI) and a budgeted variant (SDI-R) aggregate results while\naccounting for retrieval difficulty and time. Compared with prior Text-to-SQL\nor KILT/RAG benchmarks, ScenarioBench ties each decision to clause-level\nevidence under strict grounding and no-peek rules, shifting gains toward\njustification quality under explicit time budgets.", "AI": {"tldr": "ScenarioBench is a benchmark for evaluating Text-to-SQL and RAG systems in compliance contexts, requiring systems to justify decisions using specific policy clauses and providing comprehensive evaluation metrics.", "motivation": "Existing Text-to-SQL and RAG benchmarks lack proper grounding in compliance contexts and don't require systems to provide falsifiable, clause-level justifications for their decisions.", "method": "Uses YAML scenarios with gold-standard packages containing expected decisions, witness traces, governing clauses, and canonical SQL. Systems must justify outputs using specific clause IDs from the policy canon.", "result": "Provides comprehensive evaluation including decision accuracy, trace quality, retrieval effectiveness, SQL correctness, policy coverage, latency, and explanation-hallucination rate with normalized difficulty indices.", "conclusion": "ScenarioBench shifts evaluation focus toward justification quality under explicit time constraints, addressing limitations of prior benchmarks by enforcing strict grounding and no-peek rules for compliance applications."}}
{"id": "2509.24922", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24922", "abs": "https://arxiv.org/abs/2509.24922", "authors": ["Huihao Jing", "Wenbin Hu", "Hongyu Luo", "Jianhui Yang", "Wei Fan", "Haoran Li", "Yangqiu Song"], "title": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning", "comment": null, "summary": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large\nLanguage Models (LLMs), show great potential in addressing complex tasks. In\nthis context, integrating MAS with legal tasks is a crucial step. While\nprevious studies have developed legal benchmarks for LLM agents, none are\nspecifically designed to consider the unique advantages of MAS, such as task\ndecomposition, agent specialization, and flexible training. In fact, the lack\nof evaluation methods limits the potential of MAS in the legal domain. To\naddress this gap, we propose MASLegalBench, a legal benchmark tailored for MAS\nand designed with a deductive reasoning approach. Our benchmark uses GDPR as\nthe application scenario, encompassing extensive background knowledge and\ncovering complex reasoning processes that effectively reflect the intricacies\nof real-world legal situations. Furthermore, we manually design various\nrole-based MAS and conduct extensive experiments using different\nstate-of-the-art LLMs. Our results highlight the strengths, limitations, and\npotential areas for improvement of existing models and MAS architectures.", "AI": {"tldr": "MASLegalBench is a legal benchmark designed specifically for multi-agent systems using GDPR scenarios, addressing the lack of MAS-focused evaluation methods in legal tasks.", "motivation": "Previous legal benchmarks for LLM agents don't consider MAS advantages like task decomposition and agent specialization, limiting MAS potential in legal domain.", "method": "Propose MASLegalBench with deductive reasoning approach using GDPR scenarios, manually design role-based MAS, and conduct experiments with state-of-the-art LLMs.", "result": "Results reveal strengths, limitations, and improvement areas of existing models and MAS architectures in legal reasoning tasks.", "conclusion": "MASLegalBench effectively evaluates MAS capabilities in legal domain and provides insights for future improvements in multi-agent legal systems."}}
{"id": "2509.23668", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23668", "abs": "https://arxiv.org/abs/2509.23668", "authors": ["Xiangfei Qiu", "Liu Yang", "Hanyin Cheng", "Xingjian Wu", "Rongjia Wu", "Zhigang Zhang", "Ding Tu", "Chenjuan Guo", "Bin Yang", "Christian S. Jensen", "Jilin Hu"], "title": "Multi-Scale Spatial-Temporal Hypergraph Network with Lead-Lag Structures for Stock Time Series Forecasting", "comment": null, "summary": "Time series forecasting occurs in a range of financial applications providing\nessential decision-making support to investors, regulatory institutions, and\nanalysts. Unlike multivariate time series from other domains, stock time series\nexhibit industry correlation. Exploiting this kind of correlation can improve\nforecasting accuracy. However, existing methods based on hypergraphs can only\ncapture industry correlation relatively superficially. These methods face two\nkey limitations: they do not fully consider inter-industry lead-lag\ninteractions, and they do not model multi-scale information within and among\nindustries. This study proposes the Hermes framework for stock time series\nforecasting that aims to improve the exploitation of industry correlation by\neliminating these limitations. The framework integrates moving aggregation and\nmulti-scale fusion modules in a hypergraph network. Specifically, to more\nflexibly capture the lead-lag relationships among industries, Hermes proposes a\nhyperedge-based moving aggregation module. This module incorporates a sliding\nwindow and utilizes dynamic temporal aggregation operations to consider\nlead-lag dependencies among industries. Additionally, to effectively model\nmulti-scale information, Hermes employs cross-scale, edge-to-edge message\npassing to integrate information from different scales while maintaining the\nconsistency of each scale. Experimental results on multiple real-world stock\ndatasets show that Hermes outperforms existing state-of-the-art methods in both\nefficiency and accuracy.", "AI": {"tldr": "Hermes is a hypergraph-based framework for stock time series forecasting that better captures industry correlations by modeling lead-lag relationships and multi-scale information through moving aggregation and cross-scale fusion modules.", "motivation": "Stock time series exhibit industry correlations that can improve forecasting accuracy, but existing hypergraph methods capture these correlations superficially without fully considering inter-industry lead-lag interactions and multi-scale information.", "method": "Hermes integrates moving aggregation and multi-scale fusion modules in a hypergraph network. It uses hyperedge-based moving aggregation with sliding windows and dynamic temporal aggregation to capture lead-lag relationships, and employs cross-scale edge-to-edge message passing to integrate multi-scale information while maintaining scale consistency.", "result": "Experimental results on multiple real-world stock datasets show that Hermes outperforms existing state-of-the-art methods in both efficiency and accuracy.", "conclusion": "The Hermes framework successfully addresses limitations in existing methods by better exploiting industry correlations through lead-lag relationship modeling and multi-scale information integration, achieving superior forecasting performance."}}
{"id": "2509.24216", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.24216", "abs": "https://arxiv.org/abs/2509.24216", "authors": ["Ziyu Chen", "Junfei Sun", "Chenxi Li", "Tuan Dung Nguyen", "Jing Yao", "Xiaoyuan Yi", "Xing Xie", "Chenhao Tan", "Lexing Xie"], "title": "MoVa: Towards Generalizable Classification of Human Morals and Values", "comment": "9 pages, 10 figures and tables, EMNLP 2025 main conference", "summary": "Identifying human morals and values embedded in language is essential to\nempirical studies of communication. However, researchers often face substantial\ndifficulty navigating the diversity of theoretical frameworks and data\navailable for their analysis. Here, we contribute MoVa, a well-documented suite\nof resources for generalizable classification of human morals and values,\nconsisting of (1) 16 labeled datasets and benchmarking results from four\ntheoretically-grounded frameworks; (2) a lightweight LLM prompting strategy\nthat outperforms fine-tuned models across multiple domains and frameworks; and\n(3) a new application that helps evaluate psychological surveys. In practice,\nwe specifically recommend a classification strategy, all@once, that scores all\nrelated concepts simultaneously, resembling the well-known multi-label\nclassifier chain. The data and methods in MoVa can facilitate many fine-grained\ninterpretations of human and machine communication, with potential implications\nfor the alignment of machine behavior.", "AI": {"tldr": "MoVa is a comprehensive resource suite for classifying human morals and values, featuring 16 labeled datasets, a lightweight LLM prompting strategy, and tools for evaluating psychological surveys.", "motivation": "Researchers face difficulties navigating diverse theoretical frameworks and data for analyzing human morals and values in language, necessitating standardized tools.", "method": "Developed MoVa with 16 labeled datasets from four frameworks, a lightweight LLM prompting strategy (all@once), and survey evaluation tools.", "result": "The all@once prompting strategy outperforms fine-tuned models across domains and frameworks, enabling generalizable moral/value classification.", "conclusion": "MoVa facilitates fine-grained interpretation of human and machine communication, with implications for machine behavior alignment."}}
{"id": "2509.24927", "categories": ["cs.AI", "cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24927", "abs": "https://arxiv.org/abs/2509.24927", "authors": ["An Guo", "Shuoxiao Zhang", "Enyi Tang", "Xinyu Gao", "Haomin Pang", "Haoxiang Tian", "Yanzhou Mu", "Wu Wen", "Chunrong Fang", "Zhenyu Chen"], "title": "When Autonomous Vehicle Meets V2X Cooperative Perception: How Far Are We?", "comment": "The paper has been accepted by the 40th IEEE/ACM International\n  Conference on Automated Software Engineering, ASE 2025", "summary": "With the tremendous advancement of deep learning and communication\ntechnology, Vehicle-to-Everything (V2X) cooperative perception has the\npotential to address limitations in sensing distant objects and occlusion for a\nsingle-agent perception system. V2X cooperative perception systems are software\nsystems characterized by diverse sensor types and cooperative agents, varying\nfusion schemes, and operation under different communication conditions.\nTherefore, their complex composition gives rise to numerous operational\nchallenges. Furthermore, when cooperative perception systems produce erroneous\npredictions, the types of errors and their underlying causes remain\ninsufficiently explored. To bridge this gap, we take an initial step by\nconducting an empirical study of V2X cooperative perception. To systematically\nevaluate the impact of cooperative perception on the ego vehicle's perception\nperformance, we identify and analyze six prevalent error patterns in\ncooperative perception systems. We further conduct a systematic evaluation of\nthe critical components of these systems through our large-scale study and\nidentify the following key findings: (1) The LiDAR-based cooperation\nconfiguration exhibits the highest perception performance; (2)\nVehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communication\nexhibit distinct cooperative perception performance under different fusion\nschemes; (3) Increased cooperative perception errors may result in a higher\nfrequency of driving violations; (4) Cooperative perception systems are not\nrobust against communication interference when running online. Our results\nreveal potential risks and vulnerabilities in critical components of\ncooperative perception systems. We hope that our findings can better promote\nthe design and repair of cooperative perception systems.", "AI": {"tldr": "This paper conducts an empirical study on V2X cooperative perception systems, identifying six common error patterns and evaluating critical system components through large-scale analysis.", "motivation": "V2X cooperative perception systems face numerous operational challenges due to their complex composition, and the types of errors and their underlying causes remain insufficiently explored when these systems produce erroneous predictions.", "method": "The authors conducted an empirical study by systematically evaluating the impact of cooperative perception on ego vehicle's perception performance, identifying six prevalent error patterns, and analyzing critical components through large-scale evaluation.", "result": "Key findings include: (1) LiDAR-based cooperation has highest perception performance; (2) V2I and V2V communication show distinct performance under different fusion schemes; (3) Increased cooperative perception errors lead to more driving violations; (4) Systems are not robust against communication interference in online operation.", "conclusion": "The study reveals potential risks and vulnerabilities in critical components of cooperative perception systems, providing findings that can better promote the design and repair of such systems."}}
{"id": "2509.23671", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23671", "abs": "https://arxiv.org/abs/2509.23671", "authors": ["Jingqi Xu", "Guibin Chen", "Jingxi Lu", "Yuzhang Lin"], "title": "Graph Neural Networks with Diversity-aware Neighbor Selection and Dynamic Multi-scale Fusion for Multivariate Time Series Forecasting", "comment": null, "summary": "Recently, numerous deep models have been proposed to enhance the performance\nof multivariate time series (MTS) forecasting. Among them, Graph Neural\nNetworks (GNNs)-based methods have shown great potential due to their\ncapability to explicitly model inter-variable dependencies. However, these\nmethods often overlook the diversity of information among neighbors, which may\nlead to redundant information aggregation. In addition, their final prediction\ntypically relies solely on the representation from a single temporal scale. To\ntackle these issues, we propose a Graph Neural Networks (GNNs) with\nDiversity-aware Neighbor Selection and Dynamic Multi-scale Fusion (DIMIGNN).\nDIMIGNN introduces a Diversity-aware Neighbor Selection Mechanism (DNSM) to\nensure that each variable shares high informational similarity with its\nneighbors while maintaining diversity among neighbors themselves. Furthermore,\na Dynamic Multi-Scale Fusion Module (DMFM) is introduced to dynamically adjust\nthe contributions of prediction results from different temporal scales to the\nfinal forecasting result. Extensive experiments on real-world datasets\ndemonstrate that DIMIGNN consistently outperforms prior methods.", "AI": {"tldr": "DIMIGNN is a GNN-based model for multivariate time series forecasting that addresses redundant neighbor information aggregation and single-scale temporal dependency issues through diversity-aware neighbor selection and dynamic multi-scale fusion.", "motivation": "Existing GNN-based MTS forecasting methods often overlook neighbor diversity, leading to redundant information aggregation, and rely solely on single temporal scale representations for final predictions.", "method": "Proposes DIMIGNN with two key components: Diversity-aware Neighbor Selection Mechanism (DNSM) to ensure high informational similarity with neighbors while maintaining neighbor diversity, and Dynamic Multi-Scale Fusion Module (DMFM) to dynamically adjust contributions from different temporal scales.", "result": "Extensive experiments on real-world datasets demonstrate that DIMIGNN consistently outperforms prior methods.", "conclusion": "The proposed DIMIGNN effectively addresses limitations in existing GNN-based MTS forecasting by incorporating diversity-aware neighbor selection and dynamic multi-scale fusion, achieving superior performance."}}
{"id": "2509.24229", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24229", "abs": "https://arxiv.org/abs/2509.24229", "authors": ["Kangxu Wang", "Ze Chen", "Chengcheng Wei", "Jiewen Zheng", "Jiarong He", "Max Gao"], "title": "Model Fusion with Multi-LoRA Inference for Tool-Enhanced Game Dialogue Agents", "comment": "8 pages", "summary": "This paper presents the opdainlp team's solution for the GPU track of the\nCPDC 2025 challenge. The challenge consists of three tasks, aiming to build an\nin-game conversational AI that adheres to character personas, aligns with the\ngame's worldview, and supports function calling. Considering both effectiveness\nand resource/time constraints during inference, we synthesized data for some of\nthe tasks based on the datasets provided by the competition organizers. We\nemployed Qwen3-14B with LoRA fine-tuning and model fusion, and utilized a base\nmodel integrated with multiple LoRA adapters during inference. Specifically, in\nthe competition, we used three distinct LoRA adapters to handle tool calling,\nresponse generation with tool call results, and response generation without\ntool call results, respectively. MultiLoRA inference was implemented using\nvLLM. Our solution achieved the first place in Task 1 and Task 3, and the\nsecond place in Task 2 of the GPU track.", "AI": {"tldr": "The opdainlp team won first place in Tasks 1 and 3, and second place in Task 2 of the CPDC 2025 GPU track by using Qwen3-14B with LoRA fine-tuning and model fusion, employing three specialized LoRA adapters for different functions.", "motivation": "To build an in-game conversational AI that adheres to character personas, aligns with the game's worldview, and supports function calling, while considering effectiveness and resource/time constraints during inference.", "method": "Used Qwen3-14B with LoRA fine-tuning and model fusion; synthesized data for some tasks; employed three distinct LoRA adapters for tool calling, response generation with tool call results, and response generation without tool call results; implemented MultiLoRA inference using vLLM.", "result": "Achieved first place in Task 1 and Task 3, and second place in Task 2 of the GPU track.", "conclusion": "The approach of using specialized LoRA adapters for different functions with Qwen3-14B and model fusion was effective for building a competitive in-game conversational AI system."}}
{"id": "2509.24934", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24934", "abs": "https://arxiv.org/abs/2509.24934", "authors": ["Johannes Zenkert", "Christian Weber", "Mubaris Nadeem", "Lisa Bender", "Madjid Fathi", "Abu Shad Ahammed", "Aniebiet Micheal Ezekiel", "Roman Obermaisser", "Maximilian Bradford"], "title": "KIRETT -- A wearable device to support rescue operations using artificial intelligence to improve first aid", "comment": "Conference Paper for 2022 IEEE International Smart Cities Conference\n  (ISC2), KIRETT Project, University of Siegen, Germany", "summary": "This short paper presents first steps in the scientific part of the KIRETT\nproject, which aims to improve first aid during rescue operations using a\nwearable device. The wearable is used for computer-aided situation recognition\nby means of artificial intelligence. It provides contextual recommendations for\nactions and operations to rescue personnel and is intended to minimize damage\nto patients due to incorrect treatment, as well as increase the probability of\nsurvival. The paper describes a first overview of research approaches within\nthe project.", "AI": {"tldr": "The KIRETT project develops a wearable AI device to improve first aid during rescue operations by providing contextual recommendations to minimize patient harm and increase survival rates.", "motivation": "To reduce patient damage from incorrect treatment and improve survival probability during rescue operations through technological assistance.", "method": "Using wearable devices with artificial intelligence for computer-aided situation recognition and providing contextual action recommendations to rescue personnel.", "result": "Initial research approaches and project overview presented, representing first steps in the scientific development of the KIRETT project.", "conclusion": "The project shows promising potential for enhancing first aid effectiveness through AI-powered wearable technology in rescue scenarios."}}
{"id": "2509.23678", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23678", "abs": "https://arxiv.org/abs/2509.23678", "authors": ["Guoliang Zhao", "Yuhan Fu", "Shuaipeng Li", "Xingwu Sun", "Ruobing Xie", "An Wang", "Weidong Han", "Zhen Yang", "Weixuan Sun", "Yudong Zhang", "Cheng-zhong Xu", "Di Wang", "Jie Jiang"], "title": "Towards a Comprehensive Scaling Law of Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) models have become the consensus approach for\nenabling parameter-efficient scaling and cost-effective deployment in large\nlanguage models. However, existing scaling laws for dense models are\ninapplicable to MoE models, which stems from three critical challenges: the\nmultiplicity of influencing factors, their intricate coupling relationships and\nthe non-monotonic nature of their performance impacts. They collectively\nnecessitate a fine-grained investigation into MoE-specific scaling laws. In\nthis work, we perform a systematic decomposition of MoE settings, identifying\nfive key factors that influence model performance from both size and structural\nperspectives (data size ($D$), total model size ($N$), activated model size\n($N_a$), number of active experts ($G$) and the ratio of shared experts ($S$)).\nSpecifically, we design $446$ controlled experiments to characterize their\nmarginal effects, ultimately constructing a comprehensive and precise joint MoE\nscaling law that considers all essential factors. Furthermore, we derive the\ntheoretically optimal and practically efficiency-aware optimal configurations\nfor $G$, $S$ and $N_a/N$ with detailed analyses. Our results demonstrate that\nthe optimal settings for $G$ and $S$ are independent of both the model\narchitecture and data size. With the scaling of $N$, the optimal activation\nparameter ratio of $N_a/N$ becomes sparser. Our proposed MoE scaling law could\nfunction as an accurate and insightful guidance to facilitate future MoE model\ndesign and training.", "AI": {"tldr": "This paper establishes comprehensive scaling laws for Mixture-of-Experts (MoE) models by systematically analyzing five key factors (data size, total model size, activated model size, number of active experts, and shared expert ratio) through 446 controlled experiments.", "motivation": "Existing scaling laws for dense models are inapplicable to MoE models due to multiple influencing factors, their intricate coupling relationships, and non-monotonic performance impacts, necessitating fine-grained investigation into MoE-specific scaling laws.", "method": "Systematic decomposition of MoE settings into five key factors, conducting 446 controlled experiments to characterize marginal effects, and constructing a comprehensive joint MoE scaling law that considers all essential factors.", "result": "Optimal settings for number of active experts (G) and shared expert ratio (S) are independent of model architecture and data size. With scaling of total model size (N), the optimal activation parameter ratio (Na/N) becomes sparser.", "conclusion": "The proposed MoE scaling law provides accurate and insightful guidance for future MoE model design and training, with optimal configurations derived for key parameters."}}
{"id": "2509.24245", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24245", "abs": "https://arxiv.org/abs/2509.24245", "authors": ["Xiaohe Bo", "Rui Li", "Zexu Sun", "Quanyu Dai", "Zeyu Zhang", "Zihang Tian", "Xu Chen", "Zhenhua Dong"], "title": "Prompt and Parameter Co-Optimization for Large Language Models", "comment": "19 pages, 10 figures", "summary": "Prompt optimization and fine-tuning are two major approaches to improve the\nperformance of Large Language Models (LLMs). They enhance the capabilities of\nLLMs from complementary perspectives: the former through explicit natural\nlanguage, and the latter through implicit parameter updates. However, prior\nwork has typically studied them in isolation, leaving their synergistic\npotential largely underexplored. To bridge this gap, in this paper, we\nintroduce MetaTuner, a novel framework that jointly integrates prompt\noptimization and fine-tuning for LLM training. Specifically, we introduce two\nneural networks to generate prompts and parameters, respectively, while\nallowing them to share a common bottom encoding layer to enable knowledge\nsharing. By the guidance of the final supervised signals, our framework is\noptimized to discover the optimal combinations between the prompts and\nparameters. Given that prompt learning involves discrete optimization while\nfine-tuning operates in a continuous parameter space, we design a supervised\nregularization loss to train our framework effectively. Extensive experiments\nacross diverse benchmarks show that our method consistently outperforms the\nbaselines.", "AI": {"tldr": "MetaTuner is a framework that jointly optimizes prompt learning and fine-tuning for LLMs, enabling synergistic improvement through shared knowledge encoding and supervised regularization.", "motivation": "Previous work studied prompt optimization and fine-tuning in isolation, leaving their synergistic potential underexplored despite their complementary approaches to enhancing LLM performance.", "method": "Introduces two neural networks for prompt generation and parameter updates with shared bottom encoding layer, using supervised regularization loss to bridge discrete prompt learning and continuous fine-tuning optimization.", "result": "Extensive experiments across diverse benchmarks show consistent performance improvements over baseline methods.", "conclusion": "Joint integration of prompt optimization and fine-tuning through MetaTuner framework effectively discovers optimal prompt-parameter combinations, demonstrating superior performance compared to isolated approaches."}}
{"id": "2509.24978", "categories": ["cs.AI", "cond-mat.quant-gas", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.24978", "abs": "https://arxiv.org/abs/2509.24978", "authors": ["Maximilian N\u00e4gele", "Florian Marquardt"], "title": "Agentic Exploration of Physics Models", "comment": null, "summary": "The process of scientific discovery relies on an interplay of observations,\nanalysis, and hypothesis generation. Machine learning is increasingly being\nadopted to address individual aspects of this process. However, it remains an\nopen challenge to fully automate the open-ended, heuristic, iterative loop\nrequired to discover the laws of an unknown system by exploring it through\nexperiments and analysis, without tailoring the approach to the specifics of a\ngiven task. Here, we introduce SciExplorer, an agent that leverages large\nlanguage model tool-use capabilities to enable free-form exploration of systems\nwithout any domain-specific blueprints, and apply it to the exploration of\nphysical systems that are initially unknown to the agent. We test SciExplorer\non a broad set of models spanning mechanical dynamical systems, wave evolution,\nand quantum many-body physics. Despite using a minimal set of tools, primarily\nbased on code execution, we observe impressive performance on tasks such as\nrecovering equations of motion from observed dynamics and inferring\nHamiltonians from expectation values. The demonstrated effectiveness of this\nsetup opens the door towards similar scientific exploration in other domains,\nwithout the need for finetuning or task-specific instructions.", "AI": {"tldr": "SciExplorer is an AI agent that uses large language models to autonomously explore unknown physical systems and discover their underlying laws through experiments and analysis, without requiring domain-specific knowledge or task-specific instructions.", "motivation": "To fully automate the scientific discovery process by creating an agent that can explore unknown systems and discover their governing laws through an open-ended, iterative loop of experiments and analysis, without being tailored to specific domains.", "method": "Leverages large language model tool-use capabilities with a minimal set of tools (primarily code execution) to enable free-form exploration of physical systems. Tested on mechanical dynamical systems, wave evolution, and quantum many-body physics.", "result": "Impressive performance in recovering equations of motion from observed dynamics and inferring Hamiltonians from expectation values across diverse physical systems.", "conclusion": "The approach demonstrates effective scientific exploration without fine-tuning or task-specific instructions, opening doors for similar automated discovery in other domains."}}
{"id": "2509.23683", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23683", "abs": "https://arxiv.org/abs/2509.23683", "authors": ["Danni Yang", "Zhikang Chen", "Sen Cui", "Mengyue Yang", "Ding Li", "Abudukelimu Wuerkaixi", "Haoxuan Li", "Jinke Ren", "Mingming Gong"], "title": "Decentralized Dynamic Cooperation of Personalized Models for Federated Continual Learning", "comment": null, "summary": "Federated continual learning (FCL) has garnered increasing attention for its\nability to support distributed computation in environments with evolving data\ndistributions. However, the emergence of new tasks introduces both temporal and\ncross-client shifts, making catastrophic forgetting a critical challenge. Most\nexisting works aggregate knowledge from clients into a global model, which may\nnot enhance client performance since irrelevant knowledge could introduce\ninterference, especially in heterogeneous scenarios. Additionally, directly\napplying decentralized approaches to FCL suffers from ineffective group\nformation caused by task changes. To address these challenges, we propose a\ndecentralized dynamic cooperation framework for FCL, where clients establish\ndynamic cooperative learning coalitions to balance the acquisition of new\nknowledge and the retention of prior learning, thereby obtaining personalized\nmodels. To maximize model performance, each client engages in selective\ncooperation, dynamically allying with others who offer meaningful performance\ngains. This results in non-overlapping, variable coalitions at each stage of\nthe task. Moreover, we use coalitional affinity game to simulate coalition\nrelationships between clients. By assessing both client gradient coherence and\nmodel similarity, we quantify the client benefits derived from cooperation. We\nalso propose a merge-blocking algorithm and a dynamic cooperative evolution\nalgorithm to achieve cooperative and dynamic equilibrium. Comprehensive\nexperiments demonstrate the superiority of our method compared to various\nbaselines. Code is available at: https://github.com/ydn3229/DCFCL.", "AI": {"tldr": "A decentralized dynamic cooperation framework for federated continual learning that forms non-overlapping coalitions to address catastrophic forgetting and client heterogeneity.", "motivation": "Existing federated continual learning approaches suffer from catastrophic forgetting due to temporal and cross-client shifts, and global model aggregation introduces irrelevant knowledge interference in heterogeneous scenarios.", "method": "Proposes dynamic cooperative learning coalitions where clients selectively ally based on performance gains, using coalitional affinity game to assess gradient coherence and model similarity, with merge-blocking and dynamic cooperative evolution algorithms.", "result": "Comprehensive experiments demonstrate superiority over various baselines in handling catastrophic forgetting and client heterogeneity.", "conclusion": "The decentralized dynamic cooperation framework effectively balances new knowledge acquisition with prior learning retention, achieving personalized models through selective client cooperation."}}
{"id": "2509.24253", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24253", "abs": "https://arxiv.org/abs/2509.24253", "authors": ["Yuelyu Ji"], "title": "MRAG-Suite: A Diagnostic Evaluation Platform for Visual Retrieval-Augmented Generation", "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (Visual RAG) significantly advances\nquestion answering by integrating visual and textual evidence. Yet, current\nevaluations fail to systematically account for query difficulty and ambiguity.\nWe propose MRAG-Suite, a diagnostic evaluation platform integrating diverse\nmultimodal benchmarks (WebQA, Chart-RAG, Visual-RAG, MRAG-Bench). We introduce\ndifficulty-based and ambiguity-aware filtering strategies, alongside\nMM-RAGChecker, a claim-level diagnostic tool. Our results demonstrate\nsubstantial accuracy reductions under difficult and ambiguous queries,\nhighlighting prevalent hallucinations. MM-RAGChecker effectively diagnoses\nthese issues, guiding future improvements in Visual RAG systems.", "AI": {"tldr": "MRAG-Suite is a diagnostic evaluation platform for Visual RAG systems that addresses query difficulty and ambiguity issues, using multimodal benchmarks and diagnostic tools to identify hallucinations.", "motivation": "Current multimodal RAG evaluations fail to systematically account for query difficulty and ambiguity, leading to incomplete assessment of system performance.", "method": "Proposed MRAG-Suite platform with difficulty-based and ambiguity-aware filtering strategies, integrating multiple multimodal benchmarks (WebQA, Chart-RAG, Visual-RAG, MRAG-Bench) and MM-RAGChecker diagnostic tool.", "result": "Substantial accuracy reductions observed under difficult and ambiguous queries, revealing prevalent hallucinations in Visual RAG systems.", "conclusion": "MM-RAGChecker effectively diagnoses Visual RAG issues and provides guidance for future system improvements in handling query difficulty and ambiguity."}}
{"id": "2509.25004", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25004", "abs": "https://arxiv.org/abs/2509.25004", "authors": ["Shijie Zhang", "Guohao Sun", "Kevin Zhang", "Xiang Guo", "Rujun Guo"], "title": "CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning", "comment": null, "summary": "Recently, online Reinforcement Learning with Verifiable Rewards (RLVR) has\nbecome a key paradigm for enhancing the reasoning capabilities of Large\nLanguage Models (LLMs). However, existing methods typically treat all training\nsamples uniformly, overlooking the vast differences in problem difficulty\nrelative to the model's current capabilities. This uniform training strategy\nleads to inefficient exploration of problems the model has already mastered,\nwhile concurrently lacking effective guidance on problems that are challenging\nits abilities the most, limiting both learning efficiency and upper-bound\nperformance. To address this, we propose CLPO (Curriculum-guided Learning for\nPolicy Optimization), a novel algorithm that creates a dynamic pedagogical\nfeedback loop within the policy optimization process. The core of CLPO\nleverages the model's own rollout performance to conduct real-time difficulty\nassessment, thereby constructing an Online Curriculum. This curriculum then\nguides an Adaptive Problem Restructuring mechanism, where the model acts as its\nown teacher: it diversifies medium-difficulty problems to promote\ngeneralization and simplifies challenging problems to make them more\nattainable. Our approach transforms the static training procedure into a\ndynamic process that co-evolves with the model's capabilities. Experiments show\nthat CLPO achieves state-of-the-art performance across eight challenging\nmathematical and general reasoning benchmarks, with an average pass@1\nimprovement of 6.96% over other methods, demonstrating its potential for more\nefficiently training more capable reasoning models.", "AI": {"tldr": "CLPO introduces a curriculum-guided learning approach for RLVR that dynamically adjusts training difficulty based on model performance, achieving SOTA results on reasoning benchmarks.", "motivation": "Existing RLVR methods treat all training samples uniformly, ignoring differences in problem difficulty relative to model capabilities, leading to inefficient learning and limited performance.", "method": "CLPO creates a dynamic pedagogical feedback loop using real-time difficulty assessment to construct an Online Curriculum, guiding Adaptive Problem Restructuring where the model diversifies medium problems and simplifies challenging ones.", "result": "Achieves state-of-the-art performance across eight mathematical and general reasoning benchmarks with average pass@1 improvement of 6.96% over other methods.", "conclusion": "CLPO transforms static training into dynamic co-evolution with model capabilities, enabling more efficient training of capable reasoning models."}}
{"id": "2509.23684", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23684", "abs": "https://arxiv.org/abs/2509.23684", "authors": ["Tanya Chowdhury", "Atharva Nijasure", "Yair Zick", "James Allan"], "title": "Hedonic Neurons: A Mechanistic Mapping of Latent Coalitions in Transformer MLPs", "comment": "Preprint", "summary": "Fine-tuned Large Language Models (LLMs) encode rich task-specific features,\nbut the form of these representations, especially within MLP layers, remains\nunclear. Empirical inspection of LoRA updates shows that new features\nconcentrate in mid-layer MLPs, yet the scale of these layers obscures\nmeaningful structure. Prior probing suggests that statistical priors may\nstrengthen, split, or vanish across depth, motivating the need to study how\nneurons work together rather than in isolation.\n  We introduce a mechanistic interpretability framework based on coalitional\ngame theory, where neurons mimic agents in a hedonic game whose preferences\ncapture their synergistic contributions to layer-local computations. Using\ntop-responsive utilities and the PAC-Top-Cover algorithm, we extract stable\ncoalitions of neurons: groups whose joint ablation has non-additive effects. We\nthen track their transitions across layers as persistence, splitting, merging,\nor disappearance.\n  Applied to LLaMA, Mistral, and Pythia rerankers fine-tuned on scalar IR\ntasks, our method finds coalitions with consistently higher synergy than\nclustering baselines. By revealing how neurons cooperate to encode features,\nhedonic coalitions uncover higher-order structure beyond disentanglement and\nyield computational units that are functionally important, interpretable, and\npredictive across domains.", "AI": {"tldr": "The paper introduces a game theory-based framework to analyze how neurons cooperate in MLP layers of fine-tuned LLMs, revealing stable coalitions with synergistic effects that track feature evolution across layers.", "motivation": "To understand how neurons in MLP layers of fine-tuned LLMs work together rather than in isolation, as prior work shows features may strengthen, split, or vanish across depth but the scale obscures meaningful structure.", "method": "A mechanistic interpretability framework using coalitional game theory, where neurons are agents in hedonic games with top-responsive utilities, applying PAC-Top-Cover algorithm to extract stable neuron coalitions and track their transitions across layers.", "result": "Applied to LLaMA, Mistral, and Pythia rerankers, the method finds coalitions with consistently higher synergy than clustering baselines, revealing higher-order structure and functionally important computational units.", "conclusion": "Hedonic coalitions uncover how neurons cooperate to encode features, providing interpretable and predictive computational units that go beyond disentanglement approaches."}}
{"id": "2509.24282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24282", "abs": "https://arxiv.org/abs/2509.24282", "authors": ["Gyuhyeon Seo", "Jungwoo Yang", "Junseong Pyo", "Nalim Kim", "Jonggeun Lee", "Yohan Jo"], "title": "SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents", "comment": null, "summary": "Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks.\nHowever, smart homes introduce distinct challenges, requiring agents to handle\nlatent user intents, temporal dependencies, device constraints, scheduling, and\nmore. The main bottlenecks for developing smart home agents with such\ncapabilities include the lack of a realistic simulation environment where\nagents can interact with devices and observe the results, as well as a\nchallenging benchmark to evaluate them. To address this, we introduce\n$\\textbf{SimuHome}$, a time-accelerated home environment that simulates smart\ndevices, supports API calls, and reflects changes in environmental variables.\nBy building the simulator on the Matter protocol (the global industry standard\nfor smart home communication), SimuHome provides a high-fidelity environment,\nand agents validated in SimuHome can be deployed on real Matter-compliant\ndevices with minimal adaptation. We provide a challenging benchmark of 600\nepisodes across twelve user query types that require the aforementioned\ncapabilities. Our evaluation of 11 agents under a unified ReAct framework\nreveals that while models perform well on simple tasks, they struggle with\nlatent intent inference, state verification, and especially temporal\nscheduling. Even the top-performing model, GPT-4.1, reaches only 54% success\nrate. These findings highlight a critical need for methods that can reliably\nverify the current state via tools before acting and coordinate time-dependent\nactions.", "AI": {"tldr": "SimuHome is a time-accelerated smart home simulator built on Matter protocol that provides realistic device interactions and environmental changes, with a benchmark of 600 episodes showing current LLM agents struggle with latent intent inference and temporal scheduling.", "motivation": "To address the lack of realistic simulation environments and challenging benchmarks for developing smart home agents that can handle latent user intents, temporal dependencies, device constraints, and scheduling.", "method": "Built SimuHome simulator on Matter protocol with time-acceleration, smart device simulation, API call support, and environmental variable tracking. Created benchmark of 600 episodes across 12 user query types requiring complex capabilities.", "result": "Evaluation of 11 agents showed they perform well on simple tasks but struggle with latent intent inference, state verification, and temporal scheduling. Even top-performing GPT-4.1 achieved only 54% success rate.", "conclusion": "There is critical need for methods that can reliably verify current state via tools before acting and coordinate time-dependent actions in smart home environments."}}
{"id": "2509.25047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25047", "abs": "https://arxiv.org/abs/2509.25047", "authors": ["Ram Ramrakhya", "Andrew Szot", "Omar Attia", "Yuhao Yang", "Anh Nguyen", "Bogdan Mazoure", "Zhe Gan", "Harsh Agrawal", "Alexander Toshev"], "title": "Scaling Synthetic Task Generation for Agents via Exploration", "comment": null, "summary": "Post-Training Multimodal Large Language Models (MLLMs) to build interactive\nagents holds promise across domains such as computer-use, web navigation, and\nrobotics. A key challenge in scaling such post-training is lack of high-quality\ndownstream agentic task datasets with tasks that are diverse, feasible, and\nverifiable. Existing approaches for task generation rely heavily on human\nannotation or prompting MLLM with limited downstream environment information,\nwhich is either costly or poorly scalable as it yield tasks with limited\ncoverage. To remedy this, we present AutoPlay, a scalable pipeline for task\ngeneration that explicitly explores interactive environments to discover\npossible interactions and current state information to synthesize\nenvironment-grounded tasks. AutoPlay operates in two stages: (i) an exploration\nphase, where an MLLM explorer agent systematically uncovers novel environment\nstates and functionalities, and (ii) a task generation phase, where a task\ngenerator leverages exploration trajectories and a set of task guideline\nprompts as context to synthesize diverse, executable, and verifiable tasks. We\nshow AutoPlay generates 20k tasks across 20 Android applications and 10k tasks\nacross 13 applications Ubuntu applications to train mobile-use and computer-use\nagents. AutoPlay generated tasks enable large-scale task demonstration\nsynthesis without human annotation by employing an MLLM task executor and\nverifier. This data enables training MLLM-based UI agents that improve success\nrates up to $20.0\\%$ on mobile-use and $10.9\\%$ on computer-use scenarios. In\naddition, AutoPlay generated tasks combined with MLLM verifier-based rewards\nenable scaling reinforcement learning training of UI agents, leading to an\nadditional $5.7\\%$ gain. coverage. These results establish AutoPlay as a\nscalable approach for post-training capable MLLM agents reducing reliance on\nhuman annotation.", "AI": {"tldr": "AutoPlay is a scalable pipeline for generating diverse, executable tasks for training multimodal large language models (MLLMs) as interactive agents, reducing reliance on human annotation by systematically exploring environments and synthesizing grounded tasks.", "motivation": "The motivation is to address the lack of high-quality downstream agentic task datasets that are diverse, feasible, and verifiable for scaling post-training of MLLMs, as existing approaches are either costly (human annotation) or yield limited coverage (prompting MLLMs).", "method": "AutoPlay operates in two stages: (1) exploration phase where an MLLM explorer agent systematically uncovers novel environment states and functionalities, and (2) task generation phase where a task generator uses exploration trajectories and task guideline prompts to synthesize diverse, executable, and verifiable tasks.", "result": "AutoPlay generated 20k tasks across 20 Android applications and 10k tasks across 13 Ubuntu applications. This data enabled training MLLM-based UI agents that improved success rates by up to 20.0% on mobile-use and 10.9% on computer-use scenarios, with an additional 5.7% gain when combined with reinforcement learning.", "conclusion": "AutoPlay establishes itself as a scalable approach for post-training capable MLLM agents, significantly reducing reliance on human annotation while enabling large-scale task demonstration synthesis and improved agent performance across mobile and computer use cases."}}
{"id": "2509.23688", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23688", "abs": "https://arxiv.org/abs/2509.23688", "authors": ["Soroosh Safari Loaliyan", "Jose-Luis Ambite", "Paul M. Thompson", "Neda Jahanshad", "Greg Ver Steeg"], "title": "FedDAPL: Toward Client-Private Generalization in Federated Learning", "comment": "4 Pages", "summary": "Federated Learning (FL) trains models locally at each research center or\nclinic and aggregates only model updates, making it a natural fit for medical\nimaging, where strict privacy laws forbid raw data sharing. A major obstacle is\nscanner-induced domain shift: non-biological variations in hardware or\nacquisition protocols can cause models to fail on external sites. Most\nharmonization methods correct this shift by directly comparing data across\nsites, conflicting with FL's privacy constraints. Domain Generalization (DG)\noffers a privacy-friendly alternative - learning site-invariant representations\nwithout sharing raw data - but standard DG pipelines still assume centralized\naccess to multi-site data, again violating FL's guarantees. This paper meets\nthese difficulties with a straightforward integration of a Domain-Adversarial\nNeural Network (DANN) within the FL process. After demonstrating that a naive\nfederated DANN fails to converge, we propose a proximal regularization method\nthat stabilizes adversarial training among clients. Experiments on T1-weighted\n3-D brain MRIs from the OpenBHB dataset, performing brain-age prediction on\nparticipants aged 6-64 y (mean 22+/-6 y; 45 percent male) in training and 6-79\ny (mean 19+/-13 y; 55 percent male) in validation, show that training on 15\nsites and testing on 19 unseen sites yields superior cross-site generalization\nover FedAvg and ERM while preserving data privacy.", "AI": {"tldr": "This paper integrates Domain-Adversarial Neural Network (DANN) into Federated Learning to address scanner-induced domain shift in medical imaging while preserving data privacy, using proximal regularization to stabilize training.", "motivation": "Federated Learning is ideal for medical imaging due to privacy constraints, but scanner-induced domain shift causes models to fail on external sites. Existing harmonization methods violate privacy by requiring data comparison across sites.", "method": "Proposed a federated DANN approach with proximal regularization to stabilize adversarial training among clients, enabling domain-invariant learning without sharing raw data.", "result": "Experiments on T1-weighted 3-D brain MRIs from OpenBHB dataset showed superior cross-site generalization over FedAvg and ERM when training on 15 sites and testing on 19 unseen sites.", "conclusion": "The proposed federated DANN with proximal regularization effectively addresses domain shift in medical imaging while maintaining strict data privacy constraints of Federated Learning."}}
{"id": "2509.24291", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.24291", "abs": "https://arxiv.org/abs/2509.24291", "authors": ["Yu-Che Tsai", "Kuan-Yu Chen", "Yuan-Chi Li", "Yuan-Hao Chen", "Ching-Yu Tsai", "Shou-De Lin"], "title": "Let LLMs Speak Embedding Languages: Generative Text Embeddings via Iterative Contrastive Refinement", "comment": null, "summary": "Existing large language model (LLM)-based embeddings typically adopt an\nencoder-only paradigm, treating LLMs as static feature extractors and\noverlooking their core generative strengths. We introduce GIRCSE (Generative\nIterative Refinement for Contrastive Sentence Embeddings), a novel framework\nthat leverages autoregressive generation to iteratively refine semantic\nrepresentations. By producing sequences of soft tokens optimized under\ncontrastive objective, GIRCSE captures latent concepts and implicit semantics\nthat encoder-only methods often miss. To guide this process, we propose an\nIterative Contrastive Refinement (ICR) objective that encourages each\nrefinement step to yield better representations. Extensive experiments show\nthat GIRCSE outperforms strong LLM-based embedding baselines on the MTEB\nbenchmark and instruction-following tasks. Moreover, GIRCSE exhibits an\nemergent test-time scaling property: generating more tokens at inference\nsteadily improves embedding quality. Our results establish generative iterative\nrefinement as a new paradigm for representation learning.", "AI": {"tldr": "GIRCSE is a novel framework that uses autoregressive generation to iteratively refine sentence embeddings through contrastive learning, outperforming encoder-only LLM-based embeddings on benchmarks.", "motivation": "Existing LLM-based embeddings treat LLMs as static feature extractors and overlook their generative capabilities, missing latent concepts and implicit semantics.", "method": "Proposes GIRCSE framework with Iterative Contrastive Refinement (ICR) objective, using autoregressive generation to produce sequences of soft tokens optimized under contrastive learning.", "result": "Outperforms strong LLM-based embedding baselines on MTEB benchmark and instruction-following tasks, exhibits emergent test-time scaling where generating more tokens improves embedding quality.", "conclusion": "Generative iterative refinement establishes a new paradigm for representation learning, effectively leveraging LLMs' generative strengths for semantic representation."}}
{"id": "2509.25052", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25052", "abs": "https://arxiv.org/abs/2509.25052", "authors": ["Sai Wang", "Yu Wu", "Zhongwen Xu"], "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning", "comment": null, "summary": "The pursuit of artificial agents that can learn to master complex\nenvironments has led to remarkable successes, yet prevailing deep reinforcement\nlearning methods often rely on immense experience, encoding their knowledge\nopaquely within neural network weights. We propose a different paradigm, one in\nwhich an agent learns to play by reasoning and planning. We introduce Cogito,\nergo ludo (CEL), a novel agent architecture that leverages a Large Language\nModel (LLM) to build an explicit, language-based understanding of its\nenvironment's mechanics and its own strategy. Starting from a tabula rasa state\nwith no prior knowledge (except action set), CEL operates on a cycle of\ninteraction and reflection. After each episode, the agent analyzes its complete\ntrajectory to perform two concurrent learning processes: Rule Induction, where\nit refines its explicit model of the environment's dynamics, and Strategy and\nPlaybook Summarization, where it distills experiences into an actionable\nstrategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,\nMinesweeper, Frozen Lake, and Sokoban), and show that the CEL agent\nsuccessfully learns to master these games by autonomously discovering their\nrules and developing effective policies from sparse rewards. Ablation studies\nconfirm that the iterative process is critical for sustained learning. Our work\ndemonstrates a path toward more general and interpretable agents that not only\nact effectively but also build a transparent and improving model of their world\nthrough explicit reasoning on raw experience.", "AI": {"tldr": "CEL is a novel agent architecture that uses LLMs for explicit reasoning and planning in games, learning environment rules and strategies from scratch through interaction and reflection cycles.", "motivation": "To create more interpretable and general agents that build transparent world models through explicit reasoning, rather than relying on opaque neural network weights.", "method": "CEL uses LLM-based reasoning in cycles: Rule Induction to learn environment dynamics and Strategy Summarization to create playbooks from trajectory analysis after each episode.", "result": "CEL successfully masters grid-world games (Minesweeper, Frozen Lake, Sokoban) by autonomously discovering rules and developing effective policies from sparse rewards.", "conclusion": "The work demonstrates a path toward interpretable agents that build transparent world models through explicit reasoning on raw experience, with iterative learning being critical for success."}}
{"id": "2509.23689", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23689", "abs": "https://arxiv.org/abs/2509.23689", "authors": ["Ankit Gangwal", "Aaryan Ajay Sharma"], "title": "Merge Now, Regret Later: The Hidden Cost of Model Merging is Adversarial Transferability", "comment": null, "summary": "Model Merging (MM) has emerged as a promising alternative to multi-task\nlearning, where multiple fine-tuned models are combined, without access to\ntasks' training data, into a single model that maintains performance across\ntasks. Recent works have explored the impact of MM on adversarial attacks,\nparticularly backdoor attacks. However, none of them have sufficiently explored\nits impact on transfer attacks using adversarial examples, i.e., a black-box\nadversarial attack where examples generated for a surrogate model successfully\nmislead a target model.\n  In this work, we study the effect of MM on the transferability of adversarial\nexamples. We perform comprehensive evaluations and statistical analysis\nconsisting of 8 MM methods, 7 datasets, and 6 attack methods, sweeping over 336\ndistinct attack settings. Through it, we first challenge the prevailing notion\nof MM conferring free adversarial robustness, and show MM cannot reliably\ndefend against transfer attacks, with over 95% relative transfer attack success\nrate. Moreover, we reveal 3 key insights for machine-learning practitioners\nregarding MM and transferability for a robust system design: (1) stronger MM\nmethods increase vulnerability to transfer attacks; (2) mitigating\nrepresentation bias increases vulnerability to transfer attacks; and (3) weight\naveraging, despite being the weakest MM method, is the most vulnerable MM\nmethod to transfer attacks. Finally, we analyze the underlying reasons for this\nincreased vulnerability, and provide potential solutions to the problem. Our\nfindings offer critical insights for designing more secure systems employing\nMM.", "AI": {"tldr": "Model Merging does not provide reliable defense against transfer attacks, with over 95% relative transfer attack success rate, challenging the notion of free adversarial robustness.", "motivation": "To study the impact of Model Merging on the transferability of adversarial examples, as previous works focused on backdoor attacks but insufficiently explored transfer attacks.", "method": "Comprehensive evaluations with 8 MM methods, 7 datasets, and 6 attack methods across 336 distinct attack settings, including statistical analysis.", "result": "MM cannot reliably defend against transfer attacks; stronger MM methods increase vulnerability; mitigating representation bias increases vulnerability; weight averaging is the most vulnerable method.", "conclusion": "MM does not confer free adversarial robustness and actually increases vulnerability to transfer attacks, requiring careful consideration in secure system design."}}
{"id": "2509.24294", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.24294", "abs": "https://arxiv.org/abs/2509.24294", "authors": ["Xinyu Pi", "Qisen Yang", "Chuong Nguyen"], "title": "LOGOS: LLM-driven End-to-End Grounded Theory Development and Schema Induction for Qualitative Research", "comment": null, "summary": "Grounded theory offers deep insights from qualitative data, but its reliance\non expert-intensive manual coding presents a major scalability bottleneck.\nCurrent computational tools stop short of true automation, keeping researchers\nfirmly in the loop. We introduce LOGOS, a novel, end-to-end framework that\nfully automates the grounded theory workflow, transforming raw text into a\nstructured, hierarchical theory. LOGOS integrates LLM-driven coding, semantic\nclustering, graph reasoning, and a novel iterative refinement process to build\nhighly reusable codebooks. To ensure fair comparison, we also introduce a\nprincipled 5-dimensional metric and a train-test split protocol for\nstandardized, unbiased evaluation. Across five diverse corpora, LOGOS\nconsistently outperforms strong baselines and achieves a remarkable $88.2\\%$\nalignment with an expert-developed schema on a complex dataset. LOGOS\ndemonstrates a powerful new path to democratize and scale qualitative research\nwithout sacrificing theoretical nuance.", "AI": {"tldr": "LOGOS is an end-to-end framework that fully automates grounded theory workflow using LLM-driven coding, semantic clustering, and graph reasoning to transform raw text into structured hierarchical theories.", "motivation": "To overcome the scalability bottleneck of expert-intensive manual coding in grounded theory and democratize qualitative research while maintaining theoretical nuance.", "method": "Integrates LLM-driven coding, semantic clustering, graph reasoning, and iterative refinement process to build reusable codebooks. Includes a 5-dimensional metric and train-test split protocol for evaluation.", "result": "Consistently outperforms strong baselines across five diverse corpora, achieving 88.2% alignment with expert-developed schema on complex dataset.", "conclusion": "LOGOS demonstrates a powerful path to scale qualitative research without sacrificing theoretical nuance, enabling broader accessibility to grounded theory methodology."}}
{"id": "2509.25112", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.25112", "abs": "https://arxiv.org/abs/2509.25112", "authors": ["Yiquan Wang", "Tin-Yeh Huang", "Qingyun Gao", "Jialin Zhang"], "title": "HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis", "comment": null, "summary": "Heatwaves pose complex cascading risks across interconnected climate, social,\nand economic systems, but knowledge fragmentation in scientific literature\nhinders comprehensive understanding of these risk pathways. We introduce HeDA\n(Heatwave Discovery Agent), an intelligent multi-agent system designed for\nautomated scientific discovery through knowledge graph construction and\nmulti-layer risk propagation analysis. HeDA processes over 10,247 academic\npapers to construct a comprehensive knowledge graph with 23,156 nodes and\n89,472 relationships, employing novel multi-layer risk propagation analysis to\nsystematically identify overlooked risk transmission pathways. Our system\nachieves 78.9% accuracy on complex question-answering tasks, outperforming\nstate-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA\nsuccessfully discovered five previously unidentified high-impact risk chains,\nsuch as the pathway where a heatwave leads to a water demand surge, resulting\nin industrial water restrictions and ultimately causing small business\ndisruption, which were validated through historical case studies and domain\nexpert review. This work presents a new paradigm for AI-driven scientific\ndiscovery, providing actionable insights for developing more resilient climate\nadaptation strategies.", "AI": {"tldr": "HeDA is an AI system that analyzes 10,247 academic papers to build a knowledge graph and identify overlooked heatwave risk pathways, achieving 78.9% accuracy and discovering 5 new high-impact risk chains.", "motivation": "Heatwaves create complex cascading risks across climate, social and economic systems, but fragmented scientific literature hinders comprehensive understanding of these risk pathways.", "method": "Developed HeDA (Heatwave Discovery Agent) - an intelligent multi-agent system that constructs knowledge graphs from academic papers and performs multi-layer risk propagation analysis.", "result": "Built knowledge graph with 23,156 nodes and 89,472 relationships; achieved 78.9% accuracy on complex QA tasks (13.7% better than GPT-4); discovered 5 previously unidentified high-impact risk chains validated by experts.", "conclusion": "HeDA presents a new paradigm for AI-driven scientific discovery, providing actionable insights for developing more resilient climate adaptation strategies."}}
{"id": "2509.23695", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23695", "abs": "https://arxiv.org/abs/2509.23695", "authors": ["Qingren Yao", "Ming Jin", "Chengqi Zhang", "Chao-Han Huck Yang", "Jun Qi", "Shirui Pan"], "title": "Estimating Time Series Foundation Model Transferability via In-Context Learning", "comment": null, "summary": "Time series foundation models (TSFMs) offer strong zero-shot forecasting via\nlarge-scale pre-training, yet fine-tuning remains critical for boosting\nperformance in domains with limited public data. With the growing number of\nTSFMs, efficiently identifying the best model for downstream fine-tuning\nbecomes increasingly challenging. In this work, we introduce TimeTic, a\ntransferability estimation framework that recasts model selection as an\nin-context-learning problem: given observations on known (source) datasets, it\npredicts how a TSFM will perform after fine-tuning on a downstream (target)\ndataset. TimeTic flexibly organizes the observed model-data relationships as\ncontextual information, allowing it to adapt seamlessly to various test-time\nscenarios. Leveraging the natural tabular structure formed by dataset\nmeta-features, model characteristics, and fine-tuned performance, we employ\ntabular foundation models to serve as in-context learners. We further introduce\na novel model characterization based on entropy evolution across model layers,\ncapturing embedding-space distinctions and enabling TimeTic to generalize\nacross arbitrary model sets. We establish a comprehensive benchmark for\ntransferability estimation including 10 datasets, 10 foundation models, and 3\nforecasting tasks. On this benchmark, TimeTic's estimation demonstrates strong\nalignment with actual fine-tuned performance for previously unseen datasets,\nachieving a mean rank correlation of approximately 0.6 and a 30% improvement\ncompared to using zero-shot performance as the transferability score.", "AI": {"tldr": "TimeTic is a transferability estimation framework that uses in-context learning to predict TSFM performance after fine-tuning on target datasets, achieving 30% improvement over zero-shot baselines.", "motivation": "With growing number of time series foundation models, efficiently identifying the best model for downstream fine-tuning becomes challenging, especially in domains with limited public data.", "method": "Recasts model selection as in-context-learning problem using tabular foundation models, organizes model-data relationships as contextual information, and introduces entropy-based model characterization across layers.", "result": "Achieves mean rank correlation of ~0.6 with actual fine-tuned performance, 30% improvement over using zero-shot performance as transferability score on benchmark with 10 datasets, 10 models, and 3 tasks.", "conclusion": "TimeTic provides effective transferability estimation for time series foundation models, enabling better model selection for fine-tuning across diverse datasets and tasks."}}
{"id": "2509.24296", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24296", "abs": "https://arxiv.org/abs/2509.24296", "authors": ["Zherui Li", "Zheng Nie", "Zhenhong Zhou", "Yufei Guo", "Yue Liu", "Yitong Zhang", "Yu Cheng", "Qingsong Wen", "Kun Wang", "Jiaheng Zhang"], "title": "DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models", "comment": null, "summary": "The rapid advancement of Diffusion Large Language Models (dLLMs) introduces\nunprecedented vulnerabilities that are fundamentally distinct from\nAutoregressive LLMs, stemming from their iterative and parallel generation\nmechanisms. In this paper, we conduct an in-depth analysis of dLLM\nvulnerabilities to jailbreak attacks across two distinct dimensions: intra-step\nand inter-step dynamics. Experimental results reveal a harmful bias inherent in\nthe standard greedy remasking strategy and identify a critical phenomenon we\nterm Denoising-path Dependence, where the safety of early-stage tokens\ndecisively influences the final output. These findings also indicate that while\ncurrent decoding strategies constitute a significant vulnerability, dLLMs\npossess a substantial intrinsic safety potential. To unlock this potential, we\npropose DiffuGuard, a training-free defense framework that addresses\nvulnerabilities through a dual-stage approach: Stochastic Annealing Remasking\ndynamically introduces controlled randomness to mitigate greedy selection bias,\nwhile Block-level Audit and Repair exploits internal model representations for\nautonomous risk detection and guided correction. Comprehensive experiments on\nfour dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack\nSuccess Rate against six diverse jailbreak methods from 47.9% to 14.7% while\npreserving model utility and efficiency. Our code is available at:\nhttps://github.com/niez233/DiffuGuard.", "AI": {"tldr": "This paper analyzes vulnerabilities in Diffusion Large Language Models (dLLMs) to jailbreak attacks, identifies key issues like greedy remasking bias and Denoising-path Dependence, and proposes DiffuGuard - a training-free defense framework that reduces attack success rates from 47.9% to 14.7%.", "motivation": "The rapid advancement of Diffusion LLMs introduces unique vulnerabilities distinct from Autoregressive LLMs, stemming from their iterative and parallel generation mechanisms. Current decoding strategies create significant security risks that need to be addressed.", "method": "The paper conducts vulnerability analysis across intra-step and inter-step dimensions, identifies harmful bias in greedy remasking and Denoising-path Dependence phenomenon. It proposes DiffuGuard with two components: Stochastic Annealing Remasking (introduces controlled randomness) and Block-level Audit and Repair (uses internal representations for risk detection and correction).", "result": "Comprehensive experiments on four dLLMs show DiffuGuard reduces Attack Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while preserving model utility and efficiency. The framework demonstrates exceptional effectiveness in mitigating dLLM vulnerabilities.", "conclusion": "While current decoding strategies constitute significant vulnerabilities in dLLMs, these models possess substantial intrinsic safety potential. DiffuGuard successfully unlocks this potential through a training-free defense framework that addresses key vulnerabilities without compromising model performance."}}
{"id": "2509.25123", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25123", "abs": "https://arxiv.org/abs/2509.25123", "authors": ["Lifan Yuan", "Weize Chen", "Yuchen Zhang", "Ganqu Cui", "Hanbin Wang", "Ziming You", "Ning Ding", "Zhiyuan Liu", "Maosong Sun", "Hao Peng"], "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones", "comment": null, "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems.", "AI": {"tldr": "RL enables LLMs to acquire genuinely new compositional skills by combining existing atomic skills, not just reweighting existing knowledge. This compositional ability generalizes to unseen function combinations and transfers across tasks.", "motivation": "To resolve the debate about whether RL teaches LLMs genuinely new skills or merely activates existing ones, and to understand the mechanisms behind LLM skill acquisition.", "method": "Developed a synthetic framework using string transformation functions, where skills are defined as inferring f(x) given x. Tested RL's ability to teach compositions h(x)=g(f(x)) when models already know f and g individually.", "result": "RL enables LLMs to learn unseen function compositions, generalize to compositions of >2 functions, and transfer compositional skills across different tasks without target-specific compositional training. RL fundamentally changes reasoning behaviors, while next-token training fails to achieve these results.", "conclusion": "RL can teach LLMs genuinely new compositional skills that generalize and transfer, suggesting a strategy of building base models with basic skills first, then using RL to develop advanced, generalizable skills for complex problems."}}
{"id": "2509.23711", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23711", "abs": "https://arxiv.org/abs/2509.23711", "authors": ["Ziheng Cheng", "Xin Guo", "Yufei Zhang"], "title": "Bridging Discrete and Continuous RL: Stable Deterministic Policy Gradient with Martingale Characterization", "comment": null, "summary": "The theory of discrete-time reinforcement learning (RL) has advanced rapidly\nover the past decades. Although primarily designed for discrete environments,\nmany real-world RL applications are inherently continuous and complex. A major\nchallenge in extending discrete-time algorithms to continuous-time settings is\ntheir sensitivity to time discretization, often leading to poor stability and\nslow convergence. In this paper, we investigate deterministic policy gradient\nmethods for continuous-time RL. We derive a continuous-time policy gradient\nformula based on an analogue of the advantage function and establish its\nmartingale characterization. This theoretical foundation leads to our proposed\nalgorithm, CT-DDPG, which enables stable learning with deterministic policies\nin continuous-time environments. Numerical experiments show that the proposed\nCT-DDPG algorithm offers improved stability and faster convergence compared to\nexisting discrete-time and continuous-time methods, across a wide range of\ncontrol tasks with varying time discretizations and noise levels.", "AI": {"tldr": "This paper proposes CT-DDPG, a continuous-time deterministic policy gradient algorithm that addresses sensitivity to time discretization in RL, offering improved stability and faster convergence.", "motivation": "Many real-world RL applications are continuous, but discrete-time algorithms are sensitive to time discretization, leading to poor stability and slow convergence in continuous environments.", "method": "Derived continuous-time policy gradient formula based on advantage function analogue, established martingale characterization, and developed CT-DDPG algorithm for deterministic policies in continuous-time settings.", "result": "Numerical experiments show CT-DDPG provides improved stability and faster convergence compared to existing discrete-time and continuous-time methods across various control tasks with different time discretizations and noise levels.", "conclusion": "The proposed CT-DDPG algorithm successfully enables stable learning with deterministic policies in continuous-time environments, overcoming limitations of time discretization sensitivity in traditional RL methods."}}
{"id": "2509.24297", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24297", "abs": "https://arxiv.org/abs/2509.24297", "authors": ["Junying Wang", "Zicheng Zhang", "Ye Shen", "Yalun Wu", "Yingji Liang", "Yijin Guo", "Farong Wen", "Wenzhe Li", "Xuezhi Zhao", "Qi Jia", "Guangtao Zhai"], "title": "Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs", "comment": "26 pages", "summary": "High-quality, multi-modal benchmarks are crucial for advancing scientific\nreasoning in large models yet their manual creation is costly and unscalable.\nTo address this bottleneck, we explore the potential for transforming Text-Only\nQA Pairs (TQAs) into high-quality Multi-Modal QA Pairs (MMQAs), which include\nthree parts: 1) Task Definition \\& Evaluation Rubric: We develop a TQA-to-MMQA\nframework and establish a comprehensive, multi-dimensional MMQA quality rubric\nthat provides principles for the transformation. 2) Benchmark Construction:\nThen we construct two extensive benchmarks to rigorously evaluate\nstate-of-the-art generation \\& understanding models on the distinct tasks of\nMMQA generation \\& MMQA quality evaluation. 3) Preliminary Solution: We develop\nan agentic system (Q-Mirror), which operationalizes our framework by\nintegrating MMQA generation and evaluation into a closed loop for iterative\nrefinement. Our experiments show that while state-of-the-art models can\ngenerate MMQAs, their outputs still leave substantial gaps, underscoring the\nneed for reliable evaluation. We further demonstrate that top-tier\nunderstanding models align closely with human judgment in MMQA quality\nassessment. Leveraging both insights, the Q-Mirror agent raises average scores\nfrom 78.90 to 85.22 and pass rates from 72\\% to 95\\%, offering a practical path\nto large-scale scientific benchmarks.", "AI": {"tldr": "This paper presents a framework for transforming Text-Only QA Pairs (TQAs) into Multi-Modal QA Pairs (MMQAs) to address the bottleneck in creating high-quality scientific reasoning benchmarks, and develops an agentic system called Q-Mirror that iteratively refines MMQA generation and evaluation.", "motivation": "Manual creation of high-quality multi-modal benchmarks for scientific reasoning is costly and unscalable, creating a bottleneck for advancing large models.", "method": "Developed a TQA-to-MMQA framework with quality rubric, constructed evaluation benchmarks, and created Q-Mirror agent that integrates MMQA generation and evaluation in a closed loop for iterative refinement.", "result": "State-of-the-art models can generate MMQAs but with substantial gaps, while top-tier understanding models align well with human judgment in quality assessment. Q-Mirror agent improved average scores from 78.90 to 85.22 and pass rates from 72% to 95%.", "conclusion": "The proposed framework and Q-Mirror agent offer a practical path to creating large-scale scientific benchmarks by automating the transformation of text-only QA pairs into high-quality multi-modal QA pairs through iterative refinement."}}
{"id": "2509.25137", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25137", "abs": "https://arxiv.org/abs/2509.25137", "authors": ["Chuanyang Jin", "Jing Xu", "Bo Liu", "Leitian Tao", "Olga Golovneva", "Tianmin Shu", "Wenting Zhao", "Xian Li", "Jason Weston"], "title": "The Era of Real-World Human Interaction: RL from User Conversations", "comment": null, "summary": "We posit that to achieve continual model improvement and multifaceted\nalignment, future models must learn from natural human interaction. Current\nconversational models are aligned using pre-annotated, expert-generated human\nfeedback. In this work, we introduce Reinforcement Learning from Human\nInteraction (RLHI), a paradigm that learns directly from in-the-wild user\nconversations. We develop two complementary methods: (1) RLHI with User-Guided\nRewrites, which revises unsatisfactory model outputs based on users'\nnatural-language follow-up responses, (2) RLHI with User-Based Rewards, which\nlearns via a reward model conditioned on knowledge of the user's long-term\ninteraction history (termed persona). Together, these methods link long-term\nuser personas to turn-level preferences via persona-conditioned preference\noptimization. Trained on conversations derived from WildChat, both RLHI\nvariants outperform strong baselines in personalization and\ninstruction-following, and similar feedback enhances performance on reasoning\nbenchmarks. These results suggest organic human interaction offers scalable,\neffective supervision for personalized alignment.", "AI": {"tldr": "RLHI (Reinforcement Learning from Human Interaction) learns from natural user conversations to improve model alignment, using two methods: User-Guided Rewrites and User-Based Rewards with persona conditioning.", "motivation": "Current conversational models rely on pre-annotated expert feedback, but natural human interaction provides more scalable and effective supervision for continual model improvement and multifaceted alignment.", "method": "Two complementary RLHI methods: (1) User-Guided Rewrites that revise unsatisfactory outputs based on user follow-up responses, (2) User-Based Rewards that learn via persona-conditioned reward models using long-term interaction history.", "result": "Both RLHI variants trained on WildChat conversations outperform strong baselines in personalization and instruction-following, with similar feedback improving reasoning benchmarks.", "conclusion": "Organic human interaction offers scalable, effective supervision for personalized alignment, enabling continual model improvement through natural conversation data."}}
{"id": "2509.23712", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23712", "abs": "https://arxiv.org/abs/2509.23712", "authors": ["Gholamali Aminian", "Andrew Elliott", "Tiger Li", "Timothy Cheuk Hin Wong", "Victor Claude Dehon", "Lukasz Szpruch", "Carsten Maple", "Christopher Read", "Martin Brown", "Gesine Reinert", "Mo Mamouei"], "title": "FraudTransformer: Time-Aware GPT for Transaction Fraud Detection", "comment": "Pre-print", "summary": "Detecting payment fraud in real-world banking streams requires models that\ncan exploit both the order of events and the irregular time gaps between them.\nWe introduce FraudTransformer, a sequence model that augments a vanilla\nGPT-style architecture with (i) a dedicated time encoder that embeds either\nabsolute timestamps or inter-event values, and (ii) a learned positional\nencoder that preserves relative order. Experiments on a large industrial\ndataset -- tens of millions of transactions and auxiliary events -- show that\nFraudTransformer surpasses four strong classical baselines (Logistic\nRegression, XGBoost and LightGBM) as well as transformer ablations that omit\neither the time or positional component. On the held-out test set it delivers\nthe highest AUROC and PRAUC.", "AI": {"tldr": "FraudTransformer is a sequence model for payment fraud detection that enhances GPT-style architecture with time encoding and learned positional encoding, outperforming classical baselines and transformer ablations.", "motivation": "Real-world banking fraud detection requires models that can utilize both event order and irregular time gaps between transactions.", "method": "Augments GPT-style architecture with dedicated time encoder for timestamps/inter-event values and learned positional encoder for relative order preservation.", "result": "Outperforms four classical baselines (Logistic Regression, XGBoost, LightGBM) and transformer ablations on large industrial dataset with tens of millions of transactions.", "conclusion": "FraudTransformer achieves highest AUROC and PRAUC on held-out test set, demonstrating effectiveness for payment fraud detection."}}
{"id": "2509.24319", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24319", "abs": "https://arxiv.org/abs/2509.24319", "authors": ["Jongwook Han", "Jongwon Lim", "Injin Kong", "Yohan Jo"], "title": "Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs", "comment": null, "summary": "Large language models (LLMs) can express different values in two distinct\nways: (1) intrinsic expression, reflecting the model's inherent values learned\nduring training, and (2) prompted expression, elicited by explicit prompts.\nGiven their widespread use in value alignment and persona steering, it is\nparamount to clearly understand their underlying mechanisms, particularly\nwhether they mostly overlap (as one might expect) or rely on substantially\ndifferent mechanisms, but this remains largely understudied. We analyze this at\nthe mechanistic level using two approaches: (1) value vectors, feature\ndirections representing value mechanisms extracted from the residual stream,\nand (2) value neurons, MLP neurons that contribute to value expressions. We\ndemonstrate that intrinsic and prompted value mechanisms partly share common\ncomponents that are crucial for inducing value expression, but also possess\nunique elements that manifest in different ways. As a result, these mechanisms\nlead to different degrees of value steerability (prompted > intrinsic) and\nresponse diversity (intrinsic > prompted). In particular, components unique to\nthe intrinsic mechanism seem to promote lexical diversity in responses, whereas\nthose specific to the prompted mechanism primarily strengthen instruction\nfollowing, taking effect even in distant tasks like jailbreaking.", "AI": {"tldr": "LLMs express values through intrinsic (learned) and prompted (elicited) mechanisms that share some components but also have unique elements, leading to different steerability and response diversity patterns.", "motivation": "To understand whether intrinsic and prompted value expression mechanisms in LLMs mostly overlap or rely on substantially different mechanisms, which is crucial for value alignment and persona steering applications.", "method": "Used two mechanistic approaches: (1) value vectors extracted from the residual stream representing value mechanisms, and (2) value neurons (MLP neurons) that contribute to value expressions.", "result": "Intrinsic and prompted value mechanisms partly share common components crucial for value expression but also possess unique elements that manifest differently. Prompted mechanisms show higher steerability while intrinsic mechanisms show greater response diversity.", "conclusion": "The distinct components of intrinsic mechanisms promote lexical diversity, while prompted mechanism components strengthen instruction following and can affect distant tasks like jailbreaking."}}
{"id": "2509.25139", "categories": ["cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.25139", "abs": "https://arxiv.org/abs/2509.25139", "authors": ["Yue Zhang", "Tianyi Ma", "Zun Wang", "Yanyuan Qiao", "Parisa Kordjamshidi"], "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs", "comment": null, "summary": "Integrating large language models (LLMs) into embodied AI models is becoming\nincreasingly prevalent. However, existing zero-shot LLM-based\nVision-and-Language Navigation (VLN) agents either encode images as textual\nscene descriptions, potentially oversimplifying visual details, or process raw\nimage inputs, which can fail to capture abstract semantics required for\nhigh-level reasoning. In this paper, we improve the navigation agent's\ncontextual understanding by incorporating textual descriptions from multiple\nperspectives that facilitate analogical reasoning across images. By leveraging\ntext-based analogical reasoning, the agent enhances its global scene\nunderstanding and spatial reasoning, leading to more accurate action decisions.\nWe evaluate our approach on the R2R dataset, where our experiments demonstrate\nsignificant improvements in navigation performance.", "AI": {"tldr": "This paper proposes using multi-perspective textual descriptions to enhance LLM-based Vision-and-Language Navigation agents through analogical reasoning, improving navigation performance on R2R dataset.", "motivation": "Existing zero-shot LLM-based VLN agents either oversimplify visual details by encoding images as text or fail to capture abstract semantics when processing raw images, limiting contextual understanding for navigation.", "method": "Incorporates textual descriptions from multiple perspectives to facilitate analogical reasoning across images, enhancing global scene understanding and spatial reasoning for more accurate action decisions.", "result": "Experiments on R2R dataset demonstrate significant improvements in navigation performance compared to existing approaches.", "conclusion": "Multi-perspective textual descriptions combined with analogical reasoning effectively enhance LLM-based VLN agents' contextual understanding and navigation capabilities."}}
{"id": "2509.23720", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23720", "abs": "https://arxiv.org/abs/2509.23720", "authors": ["Xian Zeng", "Tianze Xu", "Kai Yang", "Jie Sun", "Youran Wang", "Jun Xu", "Mucheng Ren"], "title": "A Self-Adaptive Frequency Domain Network for Continuous Intraoperative Hypotension Prediction", "comment": "Accepted at ECAI 2025 main conference", "summary": "Intraoperative hypotension (IOH) is strongly associated with postoperative\ncomplications, including postoperative delirium and increased mortality, making\nits early prediction crucial in perioperative care. While several artificial\nintelligence-based models have been developed to provide IOH warnings, existing\nmethods face limitations in incorporating both time and frequency domain\ninformation, capturing short- and long-term dependencies, and handling noise\nsensitivity in biosignal data. To address these challenges, we propose a novel\nSelf-Adaptive Frequency Domain Network (SAFDNet). Specifically, SAFDNet\nintegrates an adaptive spectral block, which leverages Fourier analysis to\nextract frequency-domain features and employs self-adaptive thresholding to\nmitigate noise. Additionally, an interactive attention block is introduced to\ncapture both long-term and short-term dependencies in the data. Extensive\ninternal and external validations on two large-scale real-world datasets\ndemonstrate that SAFDNet achieves up to 97.3\\% AUROC in IOH early warning,\noutperforming state-of-the-art models. Furthermore, SAFDNet exhibits robust\npredictive performance and low sensitivity to noise, making it well-suited for\npractical clinical applications.", "AI": {"tldr": "SAFDNet is a novel AI model for early prediction of intraoperative hypotension that integrates adaptive spectral analysis and interactive attention to handle frequency domain information and temporal dependencies while being robust to noise.", "motivation": "Existing AI models for intraoperative hypotension prediction have limitations in incorporating time and frequency domain information, capturing dependencies, and handling noise sensitivity in biosignal data.", "method": "Proposed SAFDNet with adaptive spectral block using Fourier analysis and self-adaptive thresholding for noise mitigation, plus interactive attention block for capturing long-term and short-term dependencies.", "result": "Achieves up to 97.3% AUROC in IOH early warning, outperforming state-of-the-art models, with robust predictive performance and low sensitivity to noise.", "conclusion": "SAFDNet is well-suited for practical clinical applications due to its superior performance and noise robustness in intraoperative hypotension prediction."}}
{"id": "2509.24322", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24322", "abs": "https://arxiv.org/abs/2509.24322", "authors": ["Yuntao Shou", "Tao Meng", "Wei Ai", "Keqin Li"], "title": "Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey", "comment": "35 pages, 10 figures, 8 tables", "summary": "In recent years, large language models (LLMs) have driven major advances in\nlanguage understanding, marking a significant step toward artificial general\nintelligence (AGI). With increasing demands for higher-level semantics and\ncross-modal fusion, multimodal large language models (MLLMs) have emerged,\nintegrating diverse information sources (e.g., text, vision, and audio) to\nenhance modeling and reasoning in complex scenarios. In AI for Science,\nmultimodal emotion recognition and reasoning has become a rapidly growing\nfrontier. While LLMs and MLLMs have achieved notable progress in this area, the\nfield still lacks a systematic review that consolidates recent developments. To\naddress this gap, this paper provides a comprehensive survey of LLMs and MLLMs\nfor emotion recognition and reasoning, covering model architectures, datasets,\nand performance benchmarks. We further highlight key challenges and outline\nfuture research directions, aiming to offer researchers both an authoritative\nreference and practical insights for advancing this domain. To the best of our\nknowledge, this paper is the first attempt to comprehensively survey the\nintersection of MLLMs with multimodal emotion recognition and reasoning. The\nsummary of existing methods mentioned is in our Github:\n\\href{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}{https://github.com/yuntaoshou/Awesome-Emotion-Reasoning}.", "AI": {"tldr": "This paper provides a comprehensive survey of LLMs and MLLMs for emotion recognition and reasoning, covering architectures, datasets, and benchmarks, while highlighting challenges and future directions.", "motivation": "The field lacks a systematic review consolidating recent developments in multimodal emotion recognition and reasoning using LLMs and MLLMs, despite their significant progress.", "method": "Conducts a comprehensive survey covering model architectures, datasets, and performance benchmarks for LLMs and MLLMs in emotion recognition and reasoning.", "result": "Provides the first comprehensive survey at the intersection of MLLMs with multimodal emotion recognition and reasoning, offering an authoritative reference and practical insights.", "conclusion": "This survey addresses a critical gap in the literature and serves as a foundational reference for advancing research in multimodal emotion recognition and reasoning using LLMs and MLLMs."}}
{"id": "2509.25140", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25140", "abs": "https://arxiv.org/abs/2509.25140", "authors": ["Siru Ouyang", "Jun Yan", "I-Hung Hsu", "Yanfei Chen", "Ke Jiang", "Zifeng Wang", "Rujun Han", "Long T. Le", "Samira Daruki", "Xiangru Tang", "Vishy Tirumalashetty", "George Lee", "Mahsan Rofouei", "Hangfei Lin", "Jiawei Han", "Chen-Yu Lee", "Tomas Pfister"], "title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory", "comment": "11 pages, 7 figures, 4 tables", "summary": "With the growing adoption of large language model agents in persistent\nreal-world roles, they naturally encounter continuous streams of tasks. A key\nlimitation, however, is their failure to learn from the accumulated interaction\nhistory, forcing them to discard valuable insights and repeat past errors. We\npropose ReasoningBank, a novel memory framework that distills generalizable\nreasoning strategies from an agent's self-judged successful and failed\nexperiences. At test time, an agent retrieves relevant memories from\nReasoningBank to inform its interaction and then integrates new learnings back,\nenabling it to become more capable over time. Building on this powerful\nexperience learner, we further introduce memory-aware test-time scaling\n(MaTTS), which accelerates and diversifies this learning process by scaling up\nthe agent's interaction experience. By allocating more compute to each task,\nthe agent generates abundant, diverse experiences that provide rich contrastive\nsignals for synthesizing higher-quality memory. The better memory in turn\nguides more effective scaling, establishing a powerful synergy between memory\nand test-time scaling. Across web browsing and software engineering benchmarks,\nReasoningBank consistently outperforms existing memory mechanisms that store\nraw trajectories or only successful task routines, improving both effectiveness\nand efficiency; MaTTS further amplifies these gains. These findings establish\nmemory-driven experience scaling as a new scaling dimension, enabling agents to\nself-evolve with emergent behaviors naturally arise.", "AI": {"tldr": "ReasoningBank is a memory framework that enables LLM agents to learn from past experiences by distilling reasoning strategies from both successful and failed interactions, allowing continuous improvement over time.", "motivation": "Current LLM agents fail to learn from accumulated interaction history, forcing them to discard valuable insights and repeat past errors, limiting their effectiveness in persistent real-world roles.", "method": "Proposes ReasoningBank memory framework that distills generalizable reasoning strategies from self-judged experiences, and introduces memory-aware test-time scaling (MaTTS) to accelerate learning by scaling up interaction experience.", "result": "Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful routines, improving both effectiveness and efficiency.", "conclusion": "Memory-driven experience scaling establishes a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arising through the synergy between memory and test-time scaling."}}
{"id": "2509.23742", "categories": ["cs.LG", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.23742", "abs": "https://arxiv.org/abs/2509.23742", "authors": ["Yewang Chen", "Junfeng Li", "Shuyin Xia", "Qinghong Lai", "Xinbo Gao", "Guoyin Wang", "Dongdong Cheng", "Yi Liu", "Yi Wang"], "title": "GBSK: Skeleton Clustering via Granular-ball Computing and Multi-Sampling for Large-Scale Data", "comment": null, "summary": "To effectively handle clustering task for large-scale datasets, we propose a\nnovel scalable skeleton clustering algorithm, namely GBSK, which leverages the\ngranular-ball technique to capture the underlying structure of data. By\nmulti-sampling the dataset and constructing multi-grained granular-balls, GBSK\nprogressively uncovers a statistical \"skeleton\" -- a spatial abstraction that\napproximates the essential structure and distribution of the original data.\nThis strategy enables GBSK to dramatically reduce computational overhead while\nmaintaining high clustering accuracy. In addition, we introduce an adaptive\nversion, AGBSK, with simplified parameter settings to enhance usability and\nfacilitate deployment in real-world scenarios. Extensive experiments conducted\non standard computing hardware demonstrate that GBSK achieves high efficiency\nand strong clustering performance on large-scale datasets, including one with\nup to 100 million instances across 256 dimensions. Our implementation and\nexperimental results are available at: https://github.com/XFastDataLab/GBSK/.", "AI": {"tldr": "GBSK is a scalable skeleton clustering algorithm using granular-ball technique to efficiently handle large datasets by constructing multi-grained granular-balls that approximate data structure, reducing computational cost while maintaining accuracy.", "motivation": "To address the computational challenges of clustering large-scale datasets efficiently while maintaining accuracy.", "method": "Uses granular-ball technique with multi-sampling to construct multi-grained granular-balls that progressively uncover a statistical \"skeleton\" approximating the essential data structure and distribution.", "result": "GBSK achieves high efficiency and strong clustering performance on large-scale datasets, including one with 100 million instances across 256 dimensions, with dramatically reduced computational overhead.", "conclusion": "GBSK and its adaptive version AGBSK provide effective solutions for scalable clustering of large datasets with simplified parameter settings for real-world deployment."}}
{"id": "2509.24328", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24328", "abs": "https://arxiv.org/abs/2509.24328", "authors": ["Sungkyun Kim", "Jaemin Kim", "Dogyung Yoon", "Jiho Shin", "Junyeol Lee", "Jiwon Seo"], "title": "Speculative Verification: Exploiting Information Gain to Refine Speculative Decoding", "comment": "14 pages, 6 figures", "summary": "LLMs have low GPU efficiency and high latency due to autoregressive decoding.\nSpeculative decoding (SD) mitigates this using a small draft model to\nspeculatively generate multiple tokens, which are then verified in parallel by\na target model. However, when speculation accuracy is low, the overhead from\nrejected tokens can offset the benefits, limiting SD's effectiveness,\nespecially at large batch sizes. To address this, we propose Speculative\nVerification (SV), an efficient augmentation to SD that dynamically predicts\nspeculation accuracy and adapts the verification length to maximize throughput.\nSV introduces a companion model - a small auxiliary model similar in size to\nthe draft model - to estimate the alignment between draft and target model\ndistributions. By maximizing the information gain from quantifying this\nalignment, SV refines verification decisions, reducing wasted computation on\nrejected tokens and improving decoding efficiency. Moreover, SV requires no\nmodifications to the draft or target models and is compatible with existing SD\nvariants. We extensively evaluated SV on publicly available LLMs across three\nNLP tasks using nine combinations of draft, companion, and target models,\nincluding 13B-72B target models and three types of variations: base (no\nfinetuning), instruction-tuned, and task fine-tuned. Across all experiments and\nbatch sizes (4-80), SV consistently outperforms both SD and standard decoding\nwith the target model. It improves SD performance by up to 2$\\times$, with an\naverage speedup of 1.4 $\\times$ in large-batch settings (batch sizes 32-80).\nThese results demonstrate SV's robustness, scalability, and practical utility\nfor efficient LLM inference.", "AI": {"tldr": "Speculative Verification (SV) enhances speculative decoding by dynamically predicting speculation accuracy and adapting verification length to maximize throughput, achieving up to 2x speedup over standard speculative decoding.", "motivation": "LLMs suffer from low GPU efficiency and high latency due to autoregressive decoding. While speculative decoding helps by using draft models, its effectiveness is limited when speculation accuracy is low, as rejected tokens create overhead that offsets benefits.", "method": "Proposes Speculative Verification (SV) that uses a small companion model to estimate alignment between draft and target model distributions. This allows dynamic prediction of speculation accuracy and adaptation of verification length to reduce wasted computation on rejected tokens.", "result": "SV consistently outperforms both speculative decoding and standard decoding across all experiments, improving SD performance by up to 2x with average 1.4x speedup in large-batch settings (batch sizes 32-80).", "conclusion": "SV demonstrates robustness, scalability, and practical utility for efficient LLM inference, requiring no modifications to draft or target models and being compatible with existing SD variants."}}
{"id": "2509.25142", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25142", "abs": "https://arxiv.org/abs/2509.25142", "authors": ["Nicholas Budny", "Kia Ghods", "Declan Campbell", "Raja Marjieh", "Amogh Joshi", "Sreejan Kumar", "Jonathan D. Cohen", "Taylor W. Webb", "Thomas L. Griffiths"], "title": "Visual serial processing deficits explain divergences in human and VLM reasoning", "comment": null, "summary": "Why do Vision Language Models (VLMs), despite success on standard benchmarks,\noften fail to match human performance on surprisingly simple visual reasoning\ntasks? While the underlying computational principles are still debated, we\nhypothesize that a crucial factor is a deficit in visually-grounded serial\nprocessing. To test this hypothesis, we compared human and VLM performance\nacross tasks designed to vary serial processing demands in three distinct\ndomains: geometric reasoning, perceptual enumeration, and mental rotation.\nTasks within each domain varied serial processing load by manipulating factors\nsuch as geometric concept complexity, perceptual individuation load, and\ntransformation difficulty. Across all domains, our results revealed a\nconsistent pattern: decreased VLM accuracy was strongly correlated with\nincreased human reaction time (used as a proxy for serial processing load). As\ntasks require more demanding serial processing -- whether composing concepts,\nenumerating items, or performing mental transformations -- the VLM-human\nperformance gap widens reliably. These findings support our hypothesis,\nindicating that limitations in serial, visually grounded reasoning represent a\nfundamental bottleneck that distinguishes current VLMs from humans.", "AI": {"tldr": "VLMs fail on simple visual reasoning tasks due to deficits in visually-grounded serial processing, as shown by correlation between decreased VLM accuracy and increased human reaction time across geometric reasoning, enumeration, and mental rotation tasks.", "motivation": "To understand why VLMs perform poorly on simple visual reasoning tasks despite success on standard benchmarks, hypothesizing that visually-grounded serial processing limitations are the key factor.", "method": "Compared human and VLM performance across three domains (geometric reasoning, perceptual enumeration, mental rotation) with varying serial processing demands, using human reaction time as proxy for processing load.", "result": "Decreased VLM accuracy strongly correlated with increased human reaction time across all domains - VLM-human performance gap widens as serial processing demands increase.", "conclusion": "Limitations in serial, visually-grounded reasoning represent a fundamental bottleneck distinguishing current VLMs from human performance."}}
{"id": "2509.23749", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23749", "abs": "https://arxiv.org/abs/2509.23749", "authors": ["Ting-Kang Wang", "Chih-Pin Tan", "Yi-Hsuan Yang"], "title": "Time-Shifted Token Scheduling for Symbolic Music Generation", "comment": null, "summary": "Symbolic music generation faces a fundamental trade-off between efficiency\nand quality. Fine-grained tokenizations achieve strong coherence but incur long\nsequences and high complexity, while compact tokenizations improve efficiency\nat the expense of intra-token dependencies. To address this, we adapt a\ndelay-based scheduling mechanism (DP) that expands compound-like tokens across\ndecoding steps, enabling autoregressive modeling of intra-token dependencies\nwhile preserving efficiency. Notably, DP is a lightweight strategy that\nintroduces no additional parameters and can be seamlessly integrated into\nexisting representations. Experiments on symbolic orchestral MIDI datasets show\nthat our method improves all metrics over standard compound tokenizations and\nnarrows the gap to fine-grained tokenizations.", "AI": {"tldr": "The paper proposes a delay-based scheduling mechanism (DP) to address the trade-off between efficiency and quality in symbolic music generation, improving performance over standard compound tokenizations without adding parameters.", "motivation": "Symbolic music generation faces a fundamental trade-off: fine-grained tokenizations achieve strong coherence but have high complexity, while compact tokenizations improve efficiency but suffer from intra-token dependencies.", "method": "Adapt a delay-based scheduling mechanism (DP) that expands compound-like tokens across decoding steps, enabling autoregressive modeling of intra-token dependencies while preserving efficiency. DP is lightweight with no additional parameters and can be integrated into existing representations.", "result": "Experiments on symbolic orchestral MIDI datasets show that the method improves all metrics over standard compound tokenizations and narrows the gap to fine-grained tokenizations.", "conclusion": "The delay-based scheduling mechanism effectively balances efficiency and quality in symbolic music generation, providing a practical solution to the fundamental trade-off without requiring additional parameters."}}
{"id": "2509.24338", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24338", "abs": "https://arxiv.org/abs/2509.24338", "authors": ["Mengyu Bu", "Shaolei Zhang", "Zhongjun He", "Hua Wu", "Yang Feng"], "title": "AlignX: Advancing Multilingual Large Language Models with Multilingual Representation Alignment", "comment": "Accepted to EMNLP 2025 Main Conference. The code will be available at\n  https://github.com/ictnlp/AlignX", "summary": "Multilingual large language models (LLMs) possess impressive multilingual\nunderstanding and generation capabilities. However, their performance and\ncross-lingual alignment often lag for non-dominant languages. A common solution\nis to fine-tune LLMs on large-scale and more balanced multilingual corpus, but\nsuch approaches often lead to imprecise alignment and suboptimal knowledge\ntransfer, struggling with limited improvements across languages. In this paper,\nwe propose AlignX to bridge the multilingual performance gap, which is a\ntwo-stage representation-level framework for enhancing multilingual performance\nof pre-trained LLMs. In the first stage, we align multilingual representations\nwith multilingual semantic alignment and language feature integration. In the\nsecond stage, we stimulate the multilingual capability of LLMs via multilingual\ninstruction fine-tuning. Experimental results on several pre-trained LLMs\ndemonstrate that our approach enhances LLMs' multilingual general and\ncross-lingual generation capability. Further analysis indicates that AlignX\nbrings the multilingual representations closer and improves the cross-lingual\nalignment.", "AI": {"tldr": "AlignX is a two-stage framework that improves multilingual LLM performance by aligning representations and fine-tuning instructions.", "motivation": "Multilingual LLMs underperform on non-dominant languages due to imprecise alignment and suboptimal knowledge transfer from standard fine-tuning approaches.", "method": "Two-stage approach: 1) Align multilingual representations using semantic alignment and language feature integration, 2) Stimulate multilingual capability via instruction fine-tuning.", "result": "Experimental results show enhanced multilingual general and cross-lingual generation capabilities, with representations becoming closer and alignment improving.", "conclusion": "AlignX effectively bridges the multilingual performance gap in LLMs through representation-level alignment and instruction fine-tuning."}}
{"id": "2509.25148", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25148", "abs": "https://arxiv.org/abs/2509.25148", "authors": ["FaQiang Qian", "WeiKun Zhang", "Ziliang Wang", "Kang An", "Xuhui Zheng", "Liangjian Wen", "Mengya Gao", "Yong Dai", "Yichao Wu"], "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following", "comment": null, "summary": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment.", "AI": {"tldr": "UniAPL is a unified adversarial preference learning framework that addresses distributional mismatch in standard SFT+RL alignment pipelines by jointly learning from both demonstration and preference data in a single stage.", "motivation": "Standard sequential alignment pipelines (SFT followed by RL) suffer from distributional mismatch where SFT uses static expert data but policy distribution drifts during RL, making SFT knowledge brittle and RL updates ungrounded.", "method": "Reframe alignment as constrained optimization and propose UniAPL - a single-stage unified training objective that learns from mixed batches of SFT and preference data, allowing expert demonstrations to directly ground and regularize online exploration.", "result": "UniAPL models match or exceed strong GRPO baselines: +5.77% on Qwen3-0.6B (matching 32B model) and +3.75% on Qwen3-4B, even outperforming the teacher model. Response analysis confirms outputs closely mimic expert demonstrations.", "conclusion": "UniAPL resolves distributional mismatch and maximizes data synergy, achieving both stronger performance and better behavioral alignment by dynamically aligning policy distribution with expert distribution."}}
{"id": "2509.23750", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23750", "abs": "https://arxiv.org/abs/2509.23750", "authors": ["Li Wang", "Sudun", "Xingjian Zhang", "Wenjun Wu", "Lei Huang"], "title": "An Investigation of Batch Normalization in Off-Policy Actor-Critic Algorithms", "comment": null, "summary": "Batch Normalization (BN) has played a pivotal role in the success of deep\nlearning by improving training stability, mitigating overfitting, and enabling\nmore effective optimization. However, its adoption in deep reinforcement\nlearning (DRL) has been limited due to the inherent non-i.i.d. nature of data\nand the dynamically shifting distributions induced by the agent's learning\nprocess. In this paper, we argue that, despite these challenges, BN retains\nunique advantages in DRL settings, particularly through its stochasticity and\nits ability to ease training. When applied appropriately, BN can adapt to\nevolving data distributions and enhance both convergence speed and final\nperformance. To this end, we conduct a comprehensive empirical study on the use\nof BN in off-policy actor-critic algorithms, systematically analyzing how\ndifferent training and evaluation modes impact performance. We further identify\nfailure modes that lead to instability or divergence, analyze their underlying\ncauses, and propose the Mode-Aware Batch Normalization (MA-BN) method with\npractical actionable recommendations for robust BN integration in DRL\npipelines. We also empirically validate that, in RL settings, MA-BN accelerates\nand stabilizes training, broadens the effective learning rate range, enhances\nexploration, and reduces overall optimization difficulty. Our code is available\nat: https://github.com/monster476/ma-bn.git.", "AI": {"tldr": "Batch Normalization (BN) can be effectively used in deep reinforcement learning despite non-i.i.d. data challenges, with proposed MA-BN method improving training stability and performance.", "motivation": "BN has been underutilized in DRL due to non-i.i.d. data and shifting distributions, but it retains unique advantages like stochasticity and training ease that could benefit DRL.", "method": "Conducted empirical study on BN in off-policy actor-critic algorithms, analyzed training/evaluation modes, identified failure modes, and proposed Mode-Aware Batch Normalization (MA-BN) with practical recommendations.", "result": "MA-BN accelerates and stabilizes training, broadens effective learning rate range, enhances exploration, and reduces optimization difficulty in RL settings.", "conclusion": "BN can be successfully integrated into DRL pipelines when applied appropriately, with MA-BN providing robust solutions to overcome instability issues and leverage BN's benefits."}}
{"id": "2509.24356", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24356", "abs": "https://arxiv.org/abs/2509.24356", "authors": ["Matthew Theodore Roque", "Dan John Velasco"], "title": "Beyond Repetition: Text Simplification and Curriculum Learning for Data-Constrained Pretraining", "comment": "To be published in BabyLM Workshop at EMNLP 2025", "summary": "Most studies on language model pretraining focus on large datasets, leaving\nopen questions about optimization in data-constrained settings. In such\nsettings, the effects of training data order and of including alternative\nversions of the same text remain underexplored. We address this by studying\ncurriculum learning in pretraining, focusing on text-complexity ordering and\ndata augmentation via simplification. We ask: (1) Does simplifying texts\nenhance representation quality more than reusing the original data? and (2)\nDoes ordering data by text complexity yield better representations? To answer,\nwe build on a pair of parallel corpora where human-written paragraphs are\naligned with LLM-simplified variants, and test four data schedules: repeated\nexposure, low-to-high complexity, high-to-low, and interleaved. We analyze\nmodels' representation quality from a sample efficiency perspective via\nfine-tuning, as well as its zero-shot performance on linguistic knowledge,\nentity tracking, world knowledge, and commonsense reasoning. Our findings show\nthat adding simplified data improves fine-tuning and zero-shot performance over\na repeated-exposure baseline: smaller models benefit from low-to-high\ncomplexity, while larger models perform better with interleaved ordering.", "AI": {"tldr": "Study shows that using simplified text data and complexity-based ordering improves language model pretraining in data-constrained settings, with different optimal strategies for small vs large models.", "motivation": "Most pretraining research focuses on large datasets, leaving gaps in understanding optimization for data-constrained scenarios, particularly regarding training data order and inclusion of simplified text versions.", "method": "Built parallel corpora with human-written paragraphs aligned with LLM-simplified variants, tested four data schedules: repeated exposure, low-to-high complexity, high-to-low, and interleaved ordering.", "result": "Adding simplified data improves fine-tuning and zero-shot performance over repeated-exposure baseline. Smaller models benefit from low-to-high complexity ordering, while larger models perform better with interleaved ordering.", "conclusion": "Curriculum learning with text complexity ordering and data augmentation via simplification effectively enhances representation quality in data-constrained pretraining, with model size determining the optimal data schedule."}}
{"id": "2509.25154", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25154", "abs": "https://arxiv.org/abs/2509.25154", "authors": ["Dawei Li", "Zhen Tan", "Chengshuai Zhao", "Bohan Jiang", "Baixiang Huang", "Pingchuan Ma", "Abdullah Alnaibari", "Kai Shu", "Huan Liu"], "title": "Who's Your Judge? On the Detectability of LLM-Generated Judgments", "comment": "Under review", "summary": "Large Language Model (LLM)-based judgments leverage powerful LLMs to\nefficiently evaluate candidate content and provide judgment scores. However,\nthe inherent biases and vulnerabilities of LLM-generated judgments raise\nconcerns, underscoring the urgent need for distinguishing them in sensitive\nscenarios like academic peer reviewing. In this work, we propose and formalize\nthe task of judgment detection and systematically investigate the detectability\nof LLM-generated judgments. Unlike LLM-generated text detection, judgment\ndetection relies solely on judgment scores and candidates, reflecting\nreal-world scenarios where textual feedback is often unavailable in the\ndetection process. Our preliminary analysis shows that existing LLM-generated\ntext detection methods perform poorly given their incapability to capture the\ninteraction between judgment scores and candidate content -- an aspect crucial\nfor effective judgment detection. Inspired by this, we introduce\n\\textit{J-Detector}, a lightweight and transparent neural detector augmented\nwith explicitly extracted linguistic and LLM-enhanced features to link LLM\njudges' biases with candidates' properties for accurate detection. Experiments\nacross diverse datasets demonstrate the effectiveness of \\textit{J-Detector}\nand show how its interpretability enables quantifying biases in LLM judges.\nFinally, we analyze key factors affecting the detectability of LLM-generated\njudgments and validate the practical utility of judgment detection in\nreal-world scenarios.", "AI": {"tldr": "This paper proposes judgment detection to identify LLM-generated evaluation scores, introduces J-Detector method that captures interactions between scores and content, and demonstrates its effectiveness in detecting LLM judge biases.", "motivation": "LLM-based judgments have inherent biases and vulnerabilities that raise concerns in sensitive scenarios like academic peer reviewing, creating urgent need for detection methods.", "method": "Proposed J-Detector - a lightweight neural detector augmented with linguistic and LLM-enhanced features to link LLM judges' biases with candidates' properties for detection.", "result": "Experiments across diverse datasets demonstrate J-Detector's effectiveness and interpretability in quantifying LLM judge biases, outperforming existing LLM-generated text detection methods.", "conclusion": "Judgment detection is practically useful in real-world scenarios, and the paper analyzes key factors affecting detectability of LLM-generated judgments."}}
{"id": "2509.23753", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23753", "abs": "https://arxiv.org/abs/2509.23753", "authors": ["He Zhu", "Junyou Su", "Peng Lai", "Ren Ma", "Wenjia Zhang", "Linyi Yang", "Guanhua Chen"], "title": "Anchored Supervised Fine-Tuning", "comment": null, "summary": "Post-training of large language models involves a fundamental trade-off\nbetween supervised fine-tuning (SFT), which efficiently mimics demonstrations\nbut tends to memorize, and reinforcement learning (RL), which achieves better\ngeneralization at higher computational cost. Dynamic Fine-Tuning (DFT) recently\nemerged as a promising middle ground, reweighting SFT objectives with token\nprobabilities and achieving improvements in certain reasoning domains, though\nit exhibits instability in other tasks. We provide a analysis of DFT through\nthe reward-weighted regression (RWR) framework, revealing that it corresponds\nto a specific auxiliary distribution choice that yields provably tighter RL\nbounds than standard SFT. However, our analysis also uncovers a critical\nlimitation: this construction lacks distributional anchoring, leading to\nprogressive drift that undermines training stability. To address this, we\npropose Anchored Supervised Fine-Tuning (ASFT), which augments DFT's\nreweighting with lightweight KL regularization to preserve tightness while\nensuring stability. Empirically, ASFT consistently outperforms both SFT and DFT\nacross mathematical reasoning, medical knowledge grounding, and code\ngeneration, achieving substantial improvements with minimal computational\noverhead. Our RWR framework provides a systematic lens for understanding\npost-training methods and demonstrates that principled theoretical analysis\nleads to both stronger guarantees and practical gains.", "AI": {"tldr": "DFT improves over SFT but suffers from instability due to distributional drift. ASFT adds KL regularization to DFT, achieving better performance and stability across multiple domains with minimal computational cost.", "motivation": "Address the trade-off between SFT (efficient but memorizes) and RL (generalizes better but computationally expensive), and improve upon DFT's instability issues.", "method": "Propose Anchored Supervised Fine-Tuning (ASFT) that augments DFT's reweighting with lightweight KL regularization to prevent distributional drift while maintaining tight RL bounds.", "result": "ASFT consistently outperforms both SFT and DFT across mathematical reasoning, medical knowledge grounding, and code generation tasks with substantial improvements.", "conclusion": "The RWR framework provides systematic understanding of post-training methods, showing that principled theoretical analysis leads to stronger guarantees and practical performance gains."}}
{"id": "2509.24375", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24375", "abs": "https://arxiv.org/abs/2509.24375", "authors": ["Yijun Tian", "Shaoyu Chen", "Zhichao Xu", "Yawei Wang", "Jinhe Bi", "Peng Han", "Wei Wang"], "title": "Reinforcement Mid-Training", "comment": null, "summary": "The development of state-of-the-art large language models is commonly\nunderstood as a two-stage process involving pre-training and post-training. We\npoint out the need for an additional intermediate stage called reinforcement\nmid-training with potential for strong performance gains. In this paper, we\nformally define the problem and identify three key challenges: (1) inefficient\ntraining due to excessive reasoning steps, (2) disregard of the imbalanced\ntoken entropy distribution, and (3) underutilization of token information. To\naddress these challenges, we propose RMT, a framework for efficient, adaptive,\nand unified reinforcement mid-training with various innovative components. In\nparticular, we first introduce a dynamic token budget mechanism that constrains\nunnecessary reasoning steps and mitigates model overthinking. Next, we design a\ncurriculum-based adaptive sampling method that fosters a progressive learning\ntrajectory from easy to hard tokens. Finally, we present a dual training\nstrategy that combines reinforcement learning with next-token prediction,\nensuring targeted learning on key tokens and full exploitation of all token\ninformation. Extensive experiments demonstrate the superiority of RMT over\nstate-of-the-art methods, achieving up to +64.91% performance improvement with\nonly 21% of the reasoning length in language modeling. We also show that\ncheckpoints obtained after reinforcement mid-training can benefit the\nsubsequent post-training, yielding up to +18.76% improvement in the\nmathematical domain.", "AI": {"tldr": "Proposes reinforcement mid-training (RMT) as an intermediate stage between pre-training and post-training for LLMs, addressing challenges of inefficient training, imbalanced token entropy, and underutilized token information through dynamic token budgeting, curriculum-based sampling, and dual training strategy.", "motivation": "Current two-stage LLM development (pre-training + post-training) is insufficient; an intermediate reinforcement mid-training stage can provide significant performance gains by addressing key training inefficiencies.", "method": "RMT framework with three components: (1) dynamic token budget mechanism to limit unnecessary reasoning steps, (2) curriculum-based adaptive sampling for progressive learning from easy to hard tokens, (3) dual training strategy combining RL with next-token prediction.", "result": "Achieves up to +64.91% performance improvement with only 21% reasoning length in language modeling; checkpoints from mid-training improve subsequent post-training by up to +18.76% in mathematical domain.", "conclusion": "Reinforcement mid-training is a valuable intermediate stage that significantly enhances LLM performance and efficiency, with RMT framework effectively addressing key training challenges."}}
{"id": "2509.23756", "categories": ["cs.LG", "cs.AI", "I.2.6; J.3; H.4.2"], "pdf": "https://arxiv.org/pdf/2509.23756", "abs": "https://arxiv.org/abs/2509.23756", "authors": ["Tomer D. Meirman", "Bracha Shapira", "Noa Dagan", "Lior S. Rokach"], "title": "SHAPoint: Task-Agnostic, Efficient, and Interpretable Point-Based Risk Scoring via Shapley Values", "comment": "29 pages inc. references for main article. 6 Figures and 7 Tables.\n  Including Data and Code availability statements", "summary": "Interpretable risk scores play a vital role in clinical decision support, yet\ntraditional methods for deriving such scores often rely on manual\npreprocessing, task-specific modeling, and simplified assumptions that limit\ntheir flexibility and predictive power. We present SHAPoint, a novel,\ntask-agnostic framework that integrates the predictive accuracy of gradient\nboosted trees with the interpretability of point-based risk scores. SHAPoint\nsupports classification, regression, and survival tasks, while also inheriting\nvaluable properties from tree-based models, such as native handling of missing\ndata and support for monotonic constraints. Compared to existing frameworks,\nSHAPoint offers superior flexibility, reduced reliance on manual preprocessing,\nand faster runtime performance. Empirical results show that SHAPoint produces\ncompact and interpretable scores with predictive performance comparable to\nstate-of-the-art methods, but at a fraction of the runtime, making it a\npowerful tool for transparent and scalable risk stratification.", "AI": {"tldr": "SHAPoint is a task-agnostic framework that combines gradient boosted trees' predictive power with interpretable point-based risk scores for clinical decision support, offering superior flexibility, faster runtime, and comparable performance to state-of-the-art methods.", "motivation": "Traditional methods for interpretable risk scores in clinical decision support rely on manual preprocessing, task-specific modeling, and simplified assumptions that limit flexibility and predictive power.", "method": "SHAPoint integrates predictive accuracy of gradient boosted trees with interpretability of point-based risk scores, supporting classification, regression, and survival tasks while inheriting tree-based model properties like native missing data handling and monotonic constraint support.", "result": "SHAPoint produces compact and interpretable scores with predictive performance comparable to state-of-the-art methods, but at a fraction of the runtime, offering superior flexibility and reduced reliance on manual preprocessing.", "conclusion": "SHAPoint is a powerful tool for transparent and scalable risk stratification that bridges the gap between predictive accuracy and interpretability in clinical decision support."}}
{"id": "2509.24384", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24384", "abs": "https://arxiv.org/abs/2509.24384", "authors": ["Langqi Yang", "Tianhang Zheng", "Kedong Xiu", "Yixuan Chen", "Di Wang", "Puning Zhao", "Zhan Qin", "Kui Ren"], "title": "HarmMetric Eval: Benchmarking Metrics and Judges for LLM Harmfulness Assessment", "comment": null, "summary": "The alignment of large language models (LLMs) with human values is critical\nfor their safe deployment, yet jailbreak attacks can subvert this alignment to\nelicit harmful outputs from LLMs. In recent years, a proliferation of jailbreak\nattacks has emerged, accompanied by diverse metrics and judges to assess the\nharmfulness of the LLM outputs. However, the absence of a systematic benchmark\nto assess the quality and effectiveness of these metrics and judges undermines\nthe credibility of the reported jailbreak effectiveness and other risks. To\naddress this gap, we introduce HarmMetric Eval, a comprehensive benchmark\ndesigned to support both overall and fine-grained evaluation of harmfulness\nmetrics and judges. Our benchmark includes a high-quality dataset of\nrepresentative harmful prompts paired with diverse harmful and non-harmful\nmodel responses, alongside a flexible scoring mechanism compatible with various\nmetrics and judges. With HarmMetric Eval, our extensive experiments uncover a\nsurprising result: two conventional metrics--METEOR and ROUGE-1--outperform\nLLM-based judges in evaluating the harmfulness of model responses, challenging\nprevailing beliefs about LLMs' superiority in this domain. Our dataset is\npublicly available at https://huggingface.co/datasets/qusgo/HarmMetric_Eval,\nand the code is available at\nhttps://anonymous.4open.science/r/HarmMetric-Eval-4CBE.", "AI": {"tldr": "HarmMetric Eval is a comprehensive benchmark for evaluating harmfulness metrics and judges in LLM jailbreak scenarios, revealing that traditional metrics like METEOR and ROUGE-1 outperform LLM-based judges.", "motivation": "The proliferation of jailbreak attacks against LLMs has led to diverse metrics for assessing harmfulness, but the absence of systematic benchmarks undermines the credibility of reported jailbreak effectiveness and risks.", "method": "Developed HarmMetric Eval benchmark with high-quality dataset of harmful prompts paired with diverse model responses, and a flexible scoring mechanism compatible with various metrics and judges.", "result": "Extensive experiments showed that conventional metrics METEOR and ROUGE-1 outperform LLM-based judges in evaluating harmfulness of model responses, challenging prevailing beliefs about LLMs' superiority.", "conclusion": "HarmMetric Eval provides a systematic framework for evaluating harmfulness metrics, revealing unexpected superiority of traditional metrics over LLM-based approaches in this domain."}}
{"id": "2509.23773", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.23773", "abs": "https://arxiv.org/abs/2509.23773", "authors": ["Utkarsh Sahu", "Zhisheng Qi", "Mahantesh Halappanavar", "Nedim Lipka", "Ryan A. Rossi", "Franck Dernoncourt", "Yu Zhang", "Yao Ma", "Yu Wang"], "title": "Knowledge Homophily in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have been increasingly studied as neural\nknowledge bases for supporting knowledge-intensive applications such as\nquestion answering and fact checking. However, the structural organization of\ntheir knowledge remains unexplored. Inspired by cognitive neuroscience\nfindings, such as semantic clustering and priming, where knowing one fact\nincreases the likelihood of recalling related facts, we investigate an\nanalogous knowledge homophily pattern in LLMs. To this end, we map LLM\nknowledge into a graph representation through knowledge checking at both the\ntriplet and entity levels. After that, we analyze the knowledgeability\nrelationship between an entity and its neighbors, discovering that LLMs tend to\npossess a similar level of knowledge about entities positioned closer in the\ngraph. Motivated by this homophily principle, we propose a Graph Neural Network\n(GNN) regression model to estimate entity-level knowledgeability scores for\ntriplets by leveraging their neighborhood scores. The predicted\nknowledgeability enables us to prioritize checking less well-known triplets,\nthereby maximizing knowledge coverage under the same labeling budget. This not\nonly improves the efficiency of active labeling for fine-tuning to inject\nknowledge into LLMs but also enhances multi-hop path retrieval in\nreasoning-intensive question answering.", "AI": {"tldr": "This paper investigates knowledge homophily patterns in LLMs, where related entities tend to have similar knowledge levels. The authors propose a GNN-based method to predict entity knowledgeability scores, enabling more efficient knowledge coverage and improved performance in question answering.", "motivation": "To understand the structural organization of knowledge in LLMs and leverage cognitive neuroscience principles (semantic clustering and priming) to improve knowledge-intensive applications like question answering and fact checking.", "method": "Map LLM knowledge into a graph representation through knowledge checking at triplet and entity levels, then use a GNN regression model to estimate entity-level knowledgeability scores based on neighborhood relationships.", "result": "Discovered knowledge homophily pattern in LLMs, where entities closer in the graph tend to have similar knowledge levels. The proposed method enables prioritizing less well-known triplets for labeling, improving knowledge coverage efficiency.", "conclusion": "The knowledge homophily principle in LLMs can be leveraged to optimize knowledge injection and improve reasoning performance, particularly in multi-hop question answering tasks."}}
{"id": "2509.24389", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24389", "abs": "https://arxiv.org/abs/2509.24389", "authors": ["Fengqi Zhu", "Zebin You", "Yipeng Xing", "Zenan Huang", "Lin Liu", "Yihong Zhuang", "Guoshan Lu", "Kangyu Wang", "Xudong Wang", "Lanning Wei", "Hongrui Guo", "Jiaqi Hu", "Wentao Ye", "Tieyuan Chen", "Chenchen Li", "Chengfu Tang", "Haibo Feng", "Jun Hu", "Jun Zhou", "Xiaolu Zhang", "Zhenzhong Lan", "Junbo Zhao", "Da Zheng", "Chongxuan Li", "Jianguo Li", "Ji-Rong Wen"], "title": "LLaDA-MoE: A Sparse MoE Diffusion Language Model", "comment": null, "summary": "We introduce LLaDA-MoE, a large language diffusion model with the\nMixture-of-Experts (MoE) architecture, trained from scratch on approximately\n20T tokens. LLaDA-MoE achieves competitive performance with significantly\nreduced computational overhead by maintaining a 7B-parameter capacity while\nactivating only 1.4B parameters during inference. Our empirical evaluation\nreveals that LLaDA-MoE achieves state-of-the-art performance among diffusion\nlanguage models with larger parameters, surpassing previous diffusion language\nmodels LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The\ninstruct-tuned model LLaDA-MoE-7B-A1B-Instruct demonstrates capabilities\ncomparable to Qwen2.5-3B-Instruct in knowledge understanding, code generation,\nmathematical reasoning, agent and alignment tasks, despite using fewer active\nparameters. Our results show that integrating a sparse MoE architecture into\nthe training objective of masked diffusion language models still brings out\nMoE's strengths under efficient inference with few active parameters, and opens\nample room for further exploration of diffusion language models. LLaDA-MoE\nmodels are available at Huggingface.", "AI": {"tldr": "LLaDA-MoE is a large language diffusion model with Mixture-of-Experts architecture trained on 20T tokens, achieving competitive performance with only 1.4B active parameters during inference while maintaining 7B total capacity.", "motivation": "To develop a more computationally efficient diffusion language model by integrating sparse Mixture-of-Experts architecture, reducing inference costs while maintaining competitive performance.", "method": "Training from scratch on 20T tokens using masked diffusion language modeling with sparse MoE architecture, maintaining 7B total parameters but activating only 1.4B parameters during inference.", "result": "Achieves state-of-the-art performance among diffusion language models, surpassing LLaDA, LLaDA 1.5, and Dream across multiple benchmarks. The instruct-tuned model performs comparably to Qwen2.5-3B-Instruct in various tasks despite using fewer active parameters.", "conclusion": "Integrating sparse MoE architecture into diffusion language models successfully brings out MoE's strengths for efficient inference with few active parameters, opening new possibilities for further exploration of diffusion language models."}}
{"id": "2509.23779", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23779", "abs": "https://arxiv.org/abs/2509.23779", "authors": ["Jiarui Jiang", "Wei Huang", "Miao Zhang", "Taiji Suzuki", "Liqiang Nie"], "title": "Trained Mamba Emulates Online Gradient Descent in In-Context Linear Regression", "comment": null, "summary": "State-space models (SSMs), particularly Mamba, emerge as an efficient\nTransformer alternative with linear complexity for long-sequence modeling.\nRecent empirical works demonstrate Mamba's in-context learning (ICL)\ncapabilities competitive with Transformers, a critical capacity for large\nfoundation models. However, theoretical understanding of Mamba's ICL remains\nlimited, restricting deeper insights into its underlying mechanisms. Even\nfundamental tasks such as linear regression ICL, widely studied as a standard\ntheoretical benchmark for Transformers, have not been thoroughly analyzed in\nthe context of Mamba. To address this gap, we study the training dynamics of\nMamba on the linear regression ICL task. By developing novel techniques\ntackling non-convex optimization with gradient descent related to Mamba's\nstructure, we establish an exponential convergence rate to ICL solution, and\nderive a loss bound that is comparable to Transformer's. Importantly, our\nresults reveal that Mamba can perform a variant of \\textit{online gradient\ndescent} to learn the latent function in context. This mechanism is different\nfrom that of Transformer, which is typically understood to achieve ICL through\ngradient descent emulation. The theoretical results are verified by\nexperimental simulation.", "AI": {"tldr": "The paper provides theoretical analysis of Mamba's in-context learning capabilities on linear regression tasks, showing it achieves comparable performance to Transformers through online gradient descent.", "motivation": "While Mamba shows competitive ICL capabilities with Transformers, theoretical understanding of its mechanisms remains limited, especially for fundamental tasks like linear regression ICL.", "method": "Developed novel techniques for non-convex optimization with gradient descent related to Mamba's structure, analyzing training dynamics on linear regression ICL tasks.", "result": "Established exponential convergence rate to ICL solution with loss bound comparable to Transformers, revealing Mamba performs online gradient descent to learn latent functions.", "conclusion": "Mamba achieves ICL through online gradient descent mechanism, which differs from Transformers' gradient descent emulation, with theoretical results verified experimentally."}}
{"id": "2509.24403", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.24403", "abs": "https://arxiv.org/abs/2509.24403", "authors": ["Pengfei Wang", "Baolin Sun", "Xuemei Dong", "Yaxun Dai", "Hongwei Yuan", "Mengdie Chu", "Yingqi Gao", "Xiang Qi", "Peng Zhang", "Ying Yan"], "title": "Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling", "comment": null, "summary": "State-of-the-art (SOTA) Text-to-SQL methods still lag significantly behind\nhuman experts on challenging benchmarks like BIRD. Current approaches that\nexplore test-time scaling lack an orchestrated strategy and neglect the model's\ninternal reasoning process. To bridge this gap, we introduce Agentar-Scale-SQL,\na novel framework leveraging scalable computation to improve performance.\nAgentar-Scale-SQL implements an Orchestrated Test-Time Scaling strategy that\nsynergistically combines three distinct perspectives: i) Internal Scaling via\nRL-enhanced Intrinsic Reasoning, ii) Sequential Scaling through Iterative\nRefinement, and iii) Parallel Scaling using Diverse Synthesis and Tournament\nSelection. Agentar-Scale-SQL is a general-purpose framework designed for easy\nadaptation to new databases and more powerful language models. Extensive\nexperiments show that Agentar-Scale-SQL achieves SOTA performance on the BIRD\nbenchmark, reaching 81.67\\% execution accuracy on the test set and ranking\nfirst on the official leaderboard, demonstrating an effective path toward\nhuman-level performance.", "AI": {"tldr": "Agentar-Scale-SQL is a novel Text-to-SQL framework that uses orchestrated test-time scaling with three synergistic approaches to achieve state-of-the-art performance on the BIRD benchmark.", "motivation": "Current Text-to-SQL methods significantly lag behind human experts on challenging benchmarks like BIRD, and existing test-time scaling approaches lack orchestration and neglect the model's internal reasoning process.", "method": "The framework implements Orchestrated Test-Time Scaling strategy combining: i) Internal Scaling via RL-enhanced Intrinsic Reasoning, ii) Sequential Scaling through Iterative Refinement, and iii) Parallel Scaling using Diverse Synthesis and Tournament Selection.", "result": "Agentar-Scale-SQL achieves SOTA performance on BIRD benchmark with 81.67% execution accuracy on test set and ranks first on the official leaderboard.", "conclusion": "The framework demonstrates an effective path toward human-level performance and is designed as a general-purpose solution adaptable to new databases and more powerful language models."}}
{"id": "2509.24405", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.24405", "abs": "https://arxiv.org/abs/2509.24405", "authors": ["Khanh Trinh Pham", "Thu Huong Nguyen", "Jun Jo", "Quoc Viet Hung Nguyen", "Thanh Tam Nguyen"], "title": "Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents", "comment": null, "summary": "Text-to-SQL enables natural access to databases, yet most benchmarks are\nEnglish-only, limiting multilingual progress. We introduce MultiSpider 2.0,\nextending Spider 2.0 to eight languages (English, German, French, Spanish,\nPortuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's\nstructural difficulty while adding linguistic and dialectal variability,\ndemanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art\nLLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\\% execution accuracy when\nrelying on intrinsic reasoning, versus 60\\% on MultiSpider 1.0. Therefore, we\nprovide a collaboration-driven language agents baseline that iteratively\nrefines queries, improving accuracy to 15\\%. These results reveal a substantial\nmultilingual gap and motivate methods that are robust across languages and\nready for real-world enterprise deployment. Our benchmark is available at\nhttps://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.", "AI": {"tldr": "MultiSpider 2.0 extends Spider 2.0 to 8 languages, revealing a large multilingual gap in Text-to-SQL performance where state-of-the-art LLMs achieve only 4% accuracy without external reasoning.", "motivation": "Most Text-to-SQL benchmarks are English-only, limiting progress in multilingual database access. There's a need for benchmarks that test linguistic and dialectal variability across languages.", "method": "Extended Spider 2.0 to 8 languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese) while preserving structural difficulty. Tested state-of-the-art LLMs and developed collaboration-driven language agents that iteratively refine queries.", "result": "LLMs like DeepSeek-R1 and OpenAI o1 achieved only 4% execution accuracy using intrinsic reasoning, compared to 60% on MultiSpider 1.0. Collaboration-driven language agents improved accuracy to 15%.", "conclusion": "There's a substantial multilingual gap in Text-to-SQL performance. Methods need to be robust across languages for real-world enterprise deployment. The benchmark motivates development of more language-agnostic approaches."}}
{"id": "2509.23799", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23799", "abs": "https://arxiv.org/abs/2509.23799", "authors": ["Anyi Wang", "Xuansheng Wu", "Dong Shu", "Yunpu Ma", "Ninghao Liu"], "title": "Enhancing LLM Steering through Sparse Autoencoder-Based Vector Refinement", "comment": "19 pages, 11 figures, 7 tables", "summary": "Steering has emerged as a promising approach in controlling large language\nmodels (LLMs) without modifying model parameters. However, most existing\nsteering methods rely on large-scale datasets to learn clear behavioral\ninformation, which limits their applicability in many real-world scenarios. The\nsteering vectors extracted from small dataset often contain task-irrelevant\nnoising features, which degrades their effectiveness. To refine the steering\nvectors learned from limited data, we introduce Refinement of Steering Vector\nvia Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise\nand augment the steering vectors. In our framework, we first remove\ntask-irrelevant features according to their semantics provided by SAEs, and\nthen enrich task-relevant features missing from the small dataset through their\nsemantic similarity to the identified relevant features. Extensive experiments\ndemonstrate that the proposed SAE-RSV substantially outperforms all the\nbaseline methods including supervised fine-tuning. Our findings show that\neffective steering vector can be constructed from limited training data by\nrefining the original steering vector through SAEs.", "AI": {"tldr": "SAE-RSV refines steering vectors from limited data using sparse autoencoders to remove noise and enrich task-relevant features, outperforming baselines including supervised fine-tuning.", "motivation": "Existing steering methods require large datasets, limiting real-world applicability. Small datasets produce noisy steering vectors with task-irrelevant features.", "method": "Use sparse autoencoders to semantically denoise steering vectors by removing task-irrelevant features and enriching task-relevant features through semantic similarity.", "result": "SAE-RSV substantially outperforms all baseline methods including supervised fine-tuning in extensive experiments.", "conclusion": "Effective steering vectors can be constructed from limited data by refining original vectors through sparse autoencoders."}}
{"id": "2509.24422", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24422", "abs": "https://arxiv.org/abs/2509.24422", "authors": ["Haosi Mo", "Xinyu Ma", "Xuebo Liu", "Derek F. Wong", "Yu Li", "Jie Liu", "Min Zhang"], "title": "CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task", "comment": "20 pages, 5 figures, EMNLP2025 Findings", "summary": "Recent advances in Large Language Models (LLMs) have significantly enhanced\ntheir capabilities, highlighting the need for comprehensive evaluation\nframeworks that extend beyond task-specific benchmarks. However, existing\nbenchmarks often focus on isolated abilities, lacking a holistic framework for\nassessing LLM capabilities. To address this gap, we propose the\nCognition-Domain-Task (CDT) framework, which comprehensively measures a model's\ncapabilities across three dimensions. We expand the scope of model capability\ndefinitions at the cognitive level by incorporating the Cattell-Horn-Carroll\ncognitive theory, refining the categorization of model capabilities. We apply\nCDT in two directions: dataset capability evaluation and data selection.\nExperiments show that our capability metrics correlate well with downstream\nperformance and can support effective dataset analysis and construction. The\nexperiments on data selection also show significant improvements in both\ngeneral and specific benchmarks, achieving scores of 44.3 and 45.4, with an\nincrease of 1.6 and 2.2 points over the baselines, respectively. These results\nvalidate the effectiveness and practicality of CDT. Source code and models are\navailable at https://github.com/Alessa-mo/CDT.", "AI": {"tldr": "The paper proposes the Cognition-Domain-Task (CDT) framework to comprehensively evaluate LLM capabilities across three dimensions, addressing limitations of existing benchmarks that focus on isolated abilities.", "motivation": "Existing benchmarks for LLMs often focus on isolated abilities and lack a holistic framework for comprehensive capability assessment, creating a gap in evaluation methodologies.", "method": "The CDT framework measures model capabilities across three dimensions, incorporating Cattell-Horn-Carroll cognitive theory to refine capability categorization. It's applied for dataset capability evaluation and data selection.", "result": "Experiments show capability metrics correlate well with downstream performance, supporting effective dataset analysis. Data selection experiments achieved scores of 44.3 and 45.4, with 1.6 and 2.2 point improvements over baselines respectively.", "conclusion": "The results validate the effectiveness and practicality of the CDT framework for comprehensive LLM capability assessment and data selection."}}
{"id": "2509.23802", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23802", "abs": "https://arxiv.org/abs/2509.23802", "authors": ["Yao Luan", "Ni Mu", "Yiqin Yang", "Bo Xu", "Qing-Shan Jia"], "title": "STAIR: Addressing Stage Misalignment through Temporal-Aligned Preference Reinforcement Learning", "comment": "NeurIPS 2025", "summary": "Preference-based reinforcement learning (PbRL) bypasses complex reward\nengineering by learning rewards directly from human preferences, enabling\nbetter alignment with human intentions. However, its effectiveness in\nmulti-stage tasks, where agents sequentially perform sub-tasks (e.g.,\nnavigation, grasping), is limited by stage misalignment: Comparing segments\nfrom mismatched stages, such as movement versus manipulation, results in\nuninformative feedback, thus hindering policy learning. In this paper, we\nvalidate the stage misalignment issue through theoretical analysis and\nempirical experiments. To address this issue, we propose STage-AlIgned Reward\nlearning (STAIR), which first learns a stage approximation based on temporal\ndistance, then prioritizes comparisons within the same stage. Temporal distance\nis learned via contrastive learning, which groups temporally close states into\ncoherent stages, without predefined task knowledge, and adapts dynamically to\npolicy changes. Extensive experiments demonstrate STAIR's superiority in\nmulti-stage tasks and competitive performance in single-stage tasks.\nFurthermore, human studies show that stages approximated by STAIR are\nconsistent with human cognition, confirming its effectiveness in mitigating\nstage misalignment.", "AI": {"tldr": "STAIR addresses stage misalignment in PbRL for multi-stage tasks by learning stage approximations via temporal distance and prioritizing same-stage comparisons, improving reward learning and policy performance.", "motivation": "Preference-based RL struggles with multi-stage tasks due to stage misalignment - comparing segments from different stages (e.g., navigation vs manipulation) provides uninformative feedback that hinders learning.", "method": "STAIR learns stage approximations using temporal distance via contrastive learning, grouping temporally close states into coherent stages without predefined task knowledge, then prioritizes comparisons within the same stage.", "result": "Extensive experiments show STAIR's superiority in multi-stage tasks and competitive performance in single-stage tasks. Human studies confirm stages approximated by STAIR align with human cognition.", "conclusion": "STAIR effectively mitigates stage misalignment in PbRL for multi-stage tasks through temporal distance-based stage approximation and same-stage comparison prioritization."}}
{"id": "2509.24435", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24435", "abs": "https://arxiv.org/abs/2509.24435", "authors": ["Charlie Wyatt", "Aditya Joshi", "Flora Salim"], "title": "Alternatives To Next Token Prediction In Text Generation -- A Survey", "comment": null, "summary": "The paradigm of Next Token Prediction (NTP) has driven the unprecedented\nsuccess of Large Language Models (LLMs), but is also the source of their most\npersistent weaknesses such as poor long-term planning, error accumulation, and\ncomputational inefficiency. Acknowledging the growing interest in exploring\nalternatives to NTP, the survey describes the emerging ecosystem of\nalternatives to NTP. We categorise these approaches into five main families:\n(1) Multi-Token Prediction, which targets a block of future tokens instead of a\nsingle one; (2) Plan-then-Generate, where a global, high-level plan is created\nupfront to guide token-level decoding; (3) Latent Reasoning, which shifts the\nautoregressive process itself into a continuous latent space; (4) Continuous\nGeneration Approaches, which replace sequential generation with iterative,\nparallel refinement through diffusion, flow matching, or energy-based methods;\nand (5) Non-Transformer Architectures, which sidestep NTP through their\ninherent model structure. By synthesizing insights across these methods, this\nsurvey offers a taxonomy to guide research into models that address the known\nlimitations of token-level generation to develop new transformative models for\nnatural language processing.", "AI": {"tldr": "This survey categorizes alternatives to Next Token Prediction (NTP) in LLMs into five families to address NTP's weaknesses like poor planning and error accumulation.", "motivation": "NTP drives LLM success but causes persistent weaknesses including poor long-term planning, error accumulation, and computational inefficiency.", "method": "Categorizes NTP alternatives into five families: Multi-Token Prediction, Plan-then-Generate, Latent Reasoning, Continuous Generation Approaches, and Non-Transformer Architectures.", "result": "Provides a comprehensive taxonomy of emerging alternatives to NTP that address token-level generation limitations.", "conclusion": "The survey offers a structured framework to guide research into transformative NLP models that overcome NTP's known limitations."}}
{"id": "2509.23803", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.23803", "abs": "https://arxiv.org/abs/2509.23803", "authors": ["Pramit Saha", "Joshua Strong", "Divyanshu Mishra", "Cheng Ouyang", "J. Alison Noble"], "title": "FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents", "comment": null, "summary": "Federated learning (FL) allows collaborative model training across healthcare\nsites without sharing sensitive patient data. However, real-world FL deployment\nis often hindered by complex operational challenges that demand substantial\nhuman efforts. This includes: (a) selecting appropriate clients (hospitals),\n(b) coordinating between the central server and clients, (c) client-level data\npre-processing, (d) harmonizing non-standardized data and labels across\nclients, and (e) selecting FL algorithms based on user instructions and\ncross-client data characteristics. However, the existing FL works overlook\nthese practical orchestration challenges. These operational bottlenecks\nmotivate the need for autonomous, agent-driven FL systems, where intelligent\nagents at each hospital client and the central server agent collaboratively\nmanage FL setup and model training with minimal human intervention. To this\nend, we first introduce an agent-driven FL framework that captures key phases\nof real-world FL workflows from client selection to training completion and a\nbenchmark dubbed FedAgentBench that evaluates the ability of LLM agents to\nautonomously coordinate healthcare FL. Our framework incorporates 40 FL\nalgorithms, each tailored to address diverse task-specific requirements and\ncross-client characteristics. Furthermore, we introduce a diverse set of\ncomplex tasks across 201 carefully curated datasets, simulating 6\nmodality-specific real-world healthcare environments, viz., Dermatoscopy,\nUltrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic\nperformance of 14 open-source and 10 proprietary LLMs spanning small, medium,\nand large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3\ncan automate various stages of the FL pipeline, our results reveal that more\ncomplex, interdependent tasks based on implicit goals remain challenging for\neven the strongest models.", "AI": {"tldr": "This paper introduces an agent-driven federated learning framework and FedAgentBench benchmark to address practical orchestration challenges in healthcare FL deployment, evaluating LLM agents' ability to autonomously coordinate complex FL workflows across diverse healthcare modalities.", "motivation": "Real-world FL deployment in healthcare faces operational bottlenecks including client selection, coordination, data preprocessing, data harmonization, and algorithm selection, which demand substantial human efforts and are overlooked by existing FL works.", "method": "Proposed an agent-driven FL framework capturing key workflow phases from client selection to training completion, incorporating 40 FL algorithms and evaluating 24 LLMs (14 open-source, 10 proprietary) across 201 datasets simulating 6 healthcare modalities.", "result": "Some agent cores like GPT-4.1 and DeepSeek V3 can automate various FL pipeline stages, but complex interdependent tasks based on implicit goals remain challenging even for the strongest models.", "conclusion": "Agent-driven FL systems show promise for autonomous coordination but face limitations in handling complex, implicit-goal tasks, highlighting the need for continued development in autonomous FL orchestration."}}
{"id": "2509.24468", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24468", "abs": "https://arxiv.org/abs/2509.24468", "authors": ["Taisei Yamamoto", "Ryoma Kumon", "Danushka Bollegala", "Hitomi Yanaka"], "title": "Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset", "comment": "Accepted to EMNLP 2025 main", "summary": "Large language models (LLMs) exhibit social biases, prompting the development\nof various debiasing methods. However, debiasing methods may degrade the\ncapabilities of LLMs. Previous research has evaluated the impact of bias\nmitigation primarily through tasks measuring general language understanding,\nwhich are often unrelated to social biases. In contrast, cultural commonsense\nis closely related to social biases, as both are rooted in social norms and\nvalues. The impact of bias mitigation on cultural commonsense in LLMs has not\nbeen well investigated. Considering this gap, we propose SOBACO (SOcial BiAs\nand Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate\nsocial biases and cultural commonsense in LLMs in a unified format. We evaluate\nseveral LLMs on SOBACO to examine how debiasing methods affect cultural\ncommonsense in LLMs. Our results reveal that the debiasing methods degrade the\nperformance of the LLMs on the cultural commonsense task (up to 75% accuracy\ndeterioration). These results highlight the importance of developing debiasing\nmethods that consider the trade-off with cultural commonsense to improve\nfairness and utility of LLMs.", "AI": {"tldr": "SOBACO is a Japanese benchmark evaluating social biases and cultural commonsense in LLMs. Debiasing methods degrade cultural commonsense performance by up to 75%, highlighting the need for methods that balance fairness and utility.", "motivation": "Existing debiasing methods may degrade LLM capabilities, but their impact on cultural commonsense (closely related to social biases) hasn't been well studied. Cultural commonsense and social biases both stem from social norms and values.", "method": "Proposed SOBACO benchmark for Japanese LLMs to evaluate social biases and cultural commonsense in unified format. Evaluated several LLMs to examine debiasing impact on cultural commonsense.", "result": "Debiasing methods significantly degrade LLM performance on cultural commonsense tasks, with up to 75% accuracy deterioration.", "conclusion": "There's a critical trade-off between bias mitigation and cultural commonsense preservation. Need debiasing methods that consider this balance to improve both fairness and utility of LLMs."}}
{"id": "2509.23808", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23808", "abs": "https://arxiv.org/abs/2509.23808", "authors": ["Fanding Huang", "Guanbo Huang", "Xiao Fan", "Yi He", "Xiao Liang", "Xiao Chen", "Qinting Jiang", "Faisal Nadeem Khan", "Jingyan Jiang", "Zhi Wang"], "title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR", "comment": null, "summary": "A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)\ninterprets recent progress through the lens of an exploration-exploitation\ntrade-off, a perspective largely shaped by token-level metrics. We re-examine\nthis perspective, proposing that this perceived trade-off may not be a\nfundamental constraint but rather an artifact of the measurement level. To\ninvestigate this, we shift the analysis to the semantically rich hidden-state\nspace, adopting Effective Rank (ER) to quantify exploration and proposing its\nnovel first- and second-order derivatives, named Effective Rank Velocity (ERV)\nand Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our\nanalysis reveals that at the hidden-state level, exploration and exploitation\ncould be decoupled (Sec. 4). This finding reveals an opportunity to enhance\nboth capacities simultaneously. This insight motivates our method,\nVelocity-Exploiting Rank-Learning (VERL), the first to operationalize the\nprinciple of synergistic exploration-exploitation enhancement by directly\nshaping the RL advantage function. The key innovation is leveraging the\ntheoretically stable ERA as a predictive meta-controller to create a\nsynergistic, dual-channel incentive structure. Instead of forcing a trade-off,\nVERL prospectively amplifies rewards for exploration to preempt overconfidence\nand reinforces exploitative gains to consolidate reasoning. Experiments across\ndiverse LLMs and reasoning benchmarks show consistent gains, including up to\n21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.", "AI": {"tldr": "The paper challenges the traditional exploration-exploitation trade-off view in RLVR, showing it's an artifact of measurement level. By analyzing hidden-state space using Effective Rank derivatives, they discover exploration and exploitation can be decoupled, leading to VERL method that enhances both simultaneously.", "motivation": "To re-examine the prevailing exploration-exploitation trade-off perspective in RLVR, which may be an artifact of token-level metrics rather than a fundamental constraint.", "method": "Shift analysis to hidden-state space using Effective Rank (ER) and its derivatives ERV and ERA. Propose VERL method that operationalizes synergistic exploration-exploitation enhancement by shaping RL advantage function using ERA as predictive meta-controller.", "result": "Experiments show consistent gains across diverse LLMs and reasoning benchmarks, including up to 21.4% absolute accuracy improvement on Gaokao 2024 dataset.", "conclusion": "Exploration and exploitation can be decoupled at hidden-state level, enabling simultaneous enhancement through VERL's dual-channel incentive structure that prospectively amplifies rewards for both exploration and exploitation."}}
{"id": "2509.24478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24478", "abs": "https://arxiv.org/abs/2509.24478", "authors": ["Lasse Borgholt", "Jakob Havtorn", "Christian Igel", "Lars Maal\u00f8e", "Zheng-Hua Tan"], "title": "A Text-To-Text Alignment Algorithm for Better Evaluation of Modern Speech Recognition Systems", "comment": null, "summary": "Modern neural networks have greatly improved performance across speech\nrecognition benchmarks. However, gains are often driven by frequent words with\nlimited semantic weight, which can obscure meaningful differences in word error\nrate, the primary evaluation metric. Errors in rare terms, named entities, and\ndomain-specific vocabulary are more consequential, but remain hidden by\naggregate metrics. This highlights the need for finer-grained error analysis,\nwhich depends on accurate alignment between reference and model transcripts.\nHowever, conventional alignment methods are not designed for such precision. We\npropose a novel alignment algorithm that couples dynamic programming with beam\nsearch scoring. Compared to traditional text alignment methods, our approach\nprovides more accurate alignment of individual errors, enabling reliable error\nanalysis. The algorithm is made available via PyPI.", "AI": {"tldr": "Proposes a novel alignment algorithm combining dynamic programming with beam search scoring to enable finer-grained error analysis in speech recognition by accurately aligning individual errors between reference and model transcripts.", "motivation": "Current speech recognition evaluation metrics like word error rate are dominated by frequent words with limited semantic weight, obscuring meaningful differences in rare terms, named entities, and domain-specific vocabulary errors that are more consequential.", "method": "Developed a novel alignment algorithm that couples dynamic programming with beam search scoring to provide more accurate alignment of individual errors compared to traditional text alignment methods.", "result": "The proposed approach enables reliable error analysis by providing more accurate alignment of individual errors between reference and model transcripts.", "conclusion": "The algorithm addresses the need for finer-grained error analysis in speech recognition systems and is made available via PyPI for practical use."}}
{"id": "2509.23809", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23809", "abs": "https://arxiv.org/abs/2509.23809", "authors": ["Hong Huang", "Decheng Wu", "Rui Cen", "Guanghua Yu", "Zonghang Li", "Kai Liu", "Jianchen Zhu", "Peng Chen", "Xue Liu", "Dapeng Wu"], "title": "Tequila: Trapping-free Ternary Quantization for Large Language Models", "comment": null, "summary": "Quantization techniques are essential for the deployment of Large Language\nModels (LLMs) on edge devices. However, prevailing methods often rely on\nmixed-precision multiplication that lacks efficient hardware support, making it\nnot feasible. Ternary weight quantization addresses this by constraining\nweights to {-1, 0, 1}, replacing expensive multiplications with\nhardware-efficient additions. However, such aggressive compression leads to\nsignificant accuracy degradation, even after costly quantization-aware training\nwith massive data. We identify the core issue as deadzone trapping: a large\nnumber of weights are trapped at the deadzone boundary. This occurs because\nthese weights receive only noisy, uninformative gradients, preventing stable\nescape from the deadzone and severely impeding model capacity and optimization.\nTo address this issue, we propose Tequila, a trapping-free quantization\noptimization method that reactivates deadzone-trapped weights by repurposing\nthem as dynamic biases. This allows the repurposed weights to provide a\ncontinuous signal in the forward pass and, critically, receive direct,\nmeaningful gradient signals during backpropagation, thereby enhancing model\ncapacity and optimization with nearly zero inference overhead. Extensive\nevaluations demonstrate that Tequila outperforms state-of-the-art (SOTA)\nternary quantization methods across five benchmarks. Specifically, on the ARC\nbenchmark, it achieves >4% accuracy gain over the SOTA baseline, nearly\nmatching full-precision performance (within <1% gap) with a 3.0x inference\nspeedup. Consequently, Tequila offers a highly practical and efficient\nimplementation for the deployment of advanced LLMs in resource-constrained\nenvironments. The code is available at https://github.com/Tencent/AngelSlim.", "AI": {"tldr": "Tequila is a novel ternary weight quantization method that addresses deadzone trapping by repurposing trapped weights as dynamic biases, achieving near full-precision performance with 3.0x inference speedup.", "motivation": "Current ternary quantization methods suffer from accuracy degradation due to deadzone trapping, where weights get stuck at boundaries and receive uninformative gradients, limiting model capacity.", "method": "Proposes Tequila which reactivates deadzone-trapped weights by converting them into dynamic biases, enabling continuous forward signals and meaningful gradient updates during backpropagation.", "result": "Outperforms SOTA ternary quantization methods across five benchmarks, achieving >4% accuracy gain on ARC benchmark and nearly matching full-precision performance (<1% gap) with 3.0x inference speedup.", "conclusion": "Tequila provides a practical and efficient solution for deploying advanced LLMs in resource-constrained environments with minimal inference overhead."}}
{"id": "2509.23813", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23813", "abs": "https://arxiv.org/abs/2509.23813", "authors": ["Beiliang Wu", "Peiyuan Liu", "Yifan Hu", "Luyan Zhang", "Ao Hu", "Zenglin Xu"], "title": "IndexNet: Timestamp and Variable-Aware Modeling for Time Series Forecasting", "comment": null, "summary": "Multivariate time series forecasting (MTSF) plays a vital role in a wide\nrange of real-world applications, such as weather prediction and traffic flow\nforecasting. Although recent advances have significantly improved the modeling\nof temporal dynamics and inter-variable dependencies, most existing methods\noverlook index-related descriptive information, such as timestamps and variable\nindices, which carry rich contextual semantics. To unlock the potential of such\ninformation and take advantage of the lightweight and powerful periodic capture\nability of MLP-based architectures, we propose IndexNet, an MLP-based framework\naugmented with an Index Embedding (IE) module. The IE module consists of two\nkey components: Timestamp Embedding (TE) and Channel Embedding (CE).\nSpecifically, TE transforms timestamps into embedding vectors and injects them\ninto the input sequence, thereby improving the model's ability to capture\nlong-term complex periodic patterns. In parallel, CE assigns each variable a\nunique and trainable identity embedding based on its index, allowing the model\nto explicitly distinguish between heterogeneous variables and avoid homogenized\npredictions when input sequences seem close. Extensive experiments on 12\ndiverse real-world datasets demonstrate that IndexNet achieves comparable\nperformance across mainstream baselines, validating the effectiveness of our\ntemporally and variably aware design. Moreover, plug-and-play experiments and\nvisualization analyses further reveal that IndexNet exhibits strong generality\nand interpretability, two aspects that remain underexplored in current MTSF\nresearch.", "AI": {"tldr": "IndexNet is an MLP-based multivariate time series forecasting framework that incorporates index-related information through timestamp and channel embeddings to capture complex periodic patterns and distinguish between variables.", "motivation": "Most existing MTSF methods overlook index-related descriptive information like timestamps and variable indices, which carry rich contextual semantics that could improve forecasting performance.", "method": "Proposed IndexNet with Index Embedding module containing Timestamp Embedding (transforms timestamps into embedding vectors) and Channel Embedding (assigns unique trainable identity embeddings to each variable based on index).", "result": "Extensive experiments on 12 diverse real-world datasets show IndexNet achieves comparable performance to mainstream baselines, demonstrating effectiveness of temporally and variably aware design.", "conclusion": "IndexNet exhibits strong generality and interpretability, addressing underexplored aspects in current MTSF research, while effectively leveraging index information through lightweight MLP architecture."}}
{"id": "2509.24494", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24494", "abs": "https://arxiv.org/abs/2509.24494", "authors": ["Hongcheng Wang", "Yinuo Huang", "Sukai Wang", "Guanghui Ren", "Hao Dong"], "title": "GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training", "comment": "Under review", "summary": "Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a\nReinforcement Learning (RL) approach, can effectively train Chain-of-Thought\n(CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models\n(VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling\nbetween thoughts and answers, sparse reward signals caused by limited parallel\nsampling, and unstable advantage estimation. To mitigate these challenges, we\npropose GRPO-MA, a simple yet theoretically grounded method that leverages\nmulti-answer generation from each thought process, enabling more robust and\nefficient optimization. Theoretically, we show that the variance of thought\nadvantage decreases as the number of answers per thought increases.\nEmpirically, our gradient analysis confirms this effect, showing that GRPO-MA\nreduces gradient spikes compared to GRPO. Experiments on math, code, and\ndiverse multimodal tasks demonstrate that GRPO-MA substantially improves\nperformance and training efficiency. Our ablation studies further reveal that\nincreasing the number of answers per thought consistently enhances model\nperformance.", "AI": {"tldr": "GRPO-MA improves GRPO algorithm by generating multiple answers per thought to address gradient coupling, sparse rewards, and unstable advantage estimation in CoT reasoning training.", "motivation": "To overcome three key challenges in GRPO algorithm: gradient coupling between thoughts and answers, sparse reward signals due to limited parallel sampling, and unstable advantage estimation.", "method": "Propose GRPO-MA which leverages multi-answer generation from each thought process, theoretically reducing variance of thought advantage as number of answers per thought increases.", "result": "Empirical gradient analysis shows GRPO-MA reduces gradient spikes; experiments on math, code, and multimodal tasks demonstrate substantial performance and training efficiency improvements; ablation studies confirm increasing answers per thought consistently enhances performance.", "conclusion": "GRPO-MA is a simple yet theoretically grounded method that enables more robust and efficient optimization for training CoT reasoning in LLMs and VLMs."}}
{"id": "2509.23816", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23816", "abs": "https://arxiv.org/abs/2509.23816", "authors": ["Bo Li", "Xin Zheng", "Ming Jin", "Can Wang", "Shirui Pan"], "title": "Test-time GNN Model Evaluation on Dynamic Graphs", "comment": "Accepted by ICDM 2025", "summary": "Dynamic graph neural networks (DGNNs) have emerged as a leading paradigm for\nlearning from dynamic graphs, which are commonly used to model real-world\nsystems and applications. However, due to the evolving nature of dynamic graph\ndata distributions over time, well-trained DGNNs often face significant\nperformance uncertainty when inferring on unseen and unlabeled test graphs in\npractical deployment. In this case, evaluating the performance of deployed\nDGNNs at test time is crucial to determine whether a well-trained DGNN is\nsuited for inference on an unseen dynamic test graph. In this work, we\nintroduce a new research problem: DGNN model evaluation, which aims to assess\nthe performance of a specific DGNN model trained on observed dynamic graphs by\nestimating its performance on unseen dynamic graphs during test time.\nSpecifically, we propose a Dynamic Graph neural network Evaluator, dubbed\nDyGEval, to address this new problem. The proposed DyGEval involves a two-stage\nframework: (1) test-time dynamic graph simulation, which captures the\ntraining-test distributional differences as supervision signals and trains an\nevaluator; and (2) DyGEval development and training, which accurately estimates\nthe performance of the well-trained DGNN model on the test-time dynamic graphs.\nExtensive experiments demonstrate that the proposed DyGEval serves as an\neffective evaluator for assessing various DGNN backbones across different\ndynamic graphs under distribution shifts.", "AI": {"tldr": "DyGEval is a novel framework for evaluating Dynamic Graph Neural Networks (DGNNs) on unseen test graphs by estimating their performance under distribution shifts.", "motivation": "DGNNs face performance uncertainty when deployed on unseen dynamic graphs due to evolving data distributions over time, making model evaluation crucial for practical deployment.", "method": "Two-stage framework: (1) test-time dynamic graph simulation to capture training-test distribution differences as supervision signals, and (2) DyGEval development and training to estimate DGNN performance on test graphs.", "result": "Extensive experiments show DyGEval effectively evaluates various DGNN backbones across different dynamic graphs under distribution shifts.", "conclusion": "DyGEval provides a reliable solution for assessing DGNN performance on unseen dynamic graphs, addressing the critical need for model evaluation in dynamic graph learning scenarios."}}
{"id": "2509.24502", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24502", "abs": "https://arxiv.org/abs/2509.24502", "authors": ["Haewon Park", "Sangwoo Kim", "Yohan Jo"], "title": "Knowledge Editing with Subspace-Aware Key-Value Mappings", "comment": "25 pages, 12 figures, 10 tables", "summary": "Knowledge editing aims to efficiently correct factual errors in Language\nModels (LMs). The popular locate-then-edit approach modifies an MLP layer by\nfinding an optimal mapping between its input vector (key) and output vector\n(value) that leads to the expression of the edited knowledge. However, existing\nmethods without any constraints on the key and value vectors cause significant\nperturbations to the edited model. To address this, we propose Subspace\nKnowledge Edit (SUIT), a method that identifies and modifies only the subspace\nof critical features relevant to the edit. Our empirical results on LLaMA-3-8B,\nGPT-J-6B, and Qwen2.5-7B models show that SUIT dramatically improves knowledge\npreservation over strong baselines while maintaining high edit efficacy. This\neffectiveness confirms that SUIT successfully identifies the critical subspace\nfor the edit. Further analyses provide additional validation for our approach.\nThe source code and data will be released to the public upon publication of the\npaper.", "AI": {"tldr": "SUIT is a knowledge editing method that modifies only the critical feature subspace relevant to edits, reducing model perturbations while maintaining high edit efficacy.", "motivation": "Existing locate-then-edit methods without constraints on key and value vectors cause significant perturbations to edited models, leading to poor knowledge preservation.", "method": "Proposes Subspace Knowledge Edit (SUIT) that identifies and modifies only the subspace of critical features relevant to the edit, rather than unconstrained modifications.", "result": "Empirical results on LLaMA-3-8B, GPT-J-6B, and Qwen2.5-7B show SUIT dramatically improves knowledge preservation over strong baselines while maintaining high edit efficacy.", "conclusion": "SUIT successfully identifies the critical subspace for edits, providing an effective approach for knowledge editing with minimal model perturbations."}}
{"id": "2509.23822", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23822", "abs": "https://arxiv.org/abs/2509.23822", "authors": ["Omri Puny", "Yaron Lipman", "Benjamin Kurt Miller"], "title": "Space Group Conditional Flow Matching", "comment": null, "summary": "Inorganic crystals are periodic, highly-symmetric arrangements of atoms in\nthree-dimensional space. Their structures are constrained by the symmetry\noperations of a crystallographic \\emph{space group} and restricted to lie in\nspecific affine subspaces known as \\emph{Wyckoff positions}. The frequency an\natom appears in the crystal and its rough positioning are determined by its\nWyckoff position. Most generative models that predict atomic coordinates\noverlook these symmetry constraints, leading to unrealistically high\npopulations of proposed crystals exhibiting limited symmetry. We introduce\nSpace Group Conditional Flow Matching, a novel generative framework that\nsamples significantly closer to the target population of highly-symmetric,\nstable crystals. We achieve this by conditioning the entire generation process\non a given space group and set of Wyckoff positions; specifically, we define a\nconditionally symmetric noise base distribution and a group-conditioned,\nequivariant, parametric vector field that restricts the motion of atoms to\ntheir initial Wyckoff position. Our form of group-conditioned equivariance is\nachieved using an efficient reformulation of \\emph{group averaging} tailored\nfor symmetric crystals. Importantly, it reduces the computational overhead of\nsymmetrization to a negligible level. We achieve state of the art results on\ncrystal structure prediction and de novo generation benchmarks. We also perform\nrelevant ablations.", "AI": {"tldr": "Space Group Conditional Flow Matching is a novel generative framework that samples highly-symmetric, stable crystals by conditioning on space groups and Wyckoff positions, achieving state-of-the-art results in crystal structure prediction.", "motivation": "Most generative models for predicting atomic coordinates overlook symmetry constraints, leading to unrealistic crystals with limited symmetry. The paper aims to address this by incorporating crystallographic symmetry into the generation process.", "method": "The method conditions the entire generation process on a given space group and Wyckoff positions. It uses a conditionally symmetric noise base distribution and a group-conditioned, equivariant, parametric vector field that restricts atom motion to their initial Wyckoff positions. An efficient reformulation of group averaging is employed for symmetric crystals.", "result": "The approach achieves state-of-the-art results on crystal structure prediction and de novo generation benchmarks, with significantly reduced computational overhead for symmetrization.", "conclusion": "Space Group Conditional Flow Matching successfully generates highly-symmetric, stable crystals by incorporating crystallographic constraints, outperforming existing methods while maintaining computational efficiency."}}
{"id": "2509.24506", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24506", "abs": "https://arxiv.org/abs/2509.24506", "authors": ["Hamna", "Gayatri Bhat", "Sourabrata Mukherjee", "Faisal Lalani", "Evan Hadfield", "Divya Siddarth", "Kalika Bali", "Sunayana Sitaram"], "title": "Building Benchmarks from the Ground Up: Community-Centered Evaluation of LLMs in Healthcare Chatbot Settings", "comment": null, "summary": "Large Language Models (LLMs) are typically evaluated through general or\ndomain-specific benchmarks testing capabilities that often lack grounding in\nthe lived realities of end users. Critical domains such as healthcare require\nevaluations that extend beyond artificial or simulated tasks to reflect the\neveryday needs, cultural practices, and nuanced contexts of communities. We\npropose Samiksha, a community-driven evaluation pipeline co-created with\ncivil-society organizations (CSOs) and community members. Our approach enables\nscalable, automated benchmarking through a culturally aware, community-driven\npipeline in which community feedback informs what to evaluate, how the\nbenchmark is built, and how outputs are scored. We demonstrate this approach in\nthe health domain in India. Our analysis highlights how current multilingual\nLLMs address nuanced community health queries, while also offering a scalable\npathway for contextually grounded and inclusive LLM evaluation.", "AI": {"tldr": "Samiksha is a community-driven evaluation pipeline for LLMs, co-created with civil-society organizations and community members, focusing on culturally aware and contextually grounded assessment in domains like healthcare.", "motivation": "Current LLM evaluations lack grounding in the lived realities of end users, especially in critical domains like healthcare where cultural practices and community contexts are essential.", "method": "Co-creation with CSOs and community members to build a scalable, automated benchmarking pipeline where community feedback informs evaluation criteria, benchmark construction, and output scoring.", "result": "Demonstrated in the health domain in India, showing how multilingual LLMs handle nuanced community health queries and providing a scalable approach for inclusive evaluation.", "conclusion": "Samiksha offers a scalable pathway for contextually grounded and inclusive LLM evaluation, addressing the gap between artificial benchmarks and real-world community needs."}}
{"id": "2509.23825", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23825", "abs": "https://arxiv.org/abs/2509.23825", "authors": ["Alexander Kolesov", "Stepan Manukhov", "Vladimir V. Palyulin", "Alexander Korotin"], "title": "Electric Currents for Discrete Data Generation", "comment": "generative models, electrodynamics", "summary": "We propose $\\textbf{E}$lectric $\\textbf{C}$urrent $\\textbf{D}$iscrete\n$\\textbf{D}$ata $\\textbf{G}$eneration (ECD$^{2}$G), a pioneering method for\ndata generation in discrete settings that is grounded in electrical engineering\ntheory. Our approach draws an analogy between electric current flow in a\ncircuit and the transfer of probability mass between data distributions. We\ninterpret samples from the source distribution as current input nodes of a\ncircuit and samples from the target distribution as current output nodes. A\nneural network is then used to learn the electric currents to represent the\nprobability flow in the circuit. To map the source distribution to the target,\nwe sample from the source and transport these samples along the circuit\npathways according to the learned currents. This process provably guarantees\ntransfer between data distributions. We present proof-of-concept experiments to\nillustrate our ECD$^{2}$G method.", "AI": {"tldr": "ECD\u00b2G is a novel discrete data generation method using electrical current analogies to model probability flow between distributions, with neural networks learning current flows for provable distribution transfer.", "motivation": "To develop a theoretically grounded approach for discrete data generation that guarantees distribution transfer using principles from electrical engineering.", "method": "Analogizes electric current flow to probability mass transfer, uses neural networks to learn electric currents representing probability flow, and transports samples along circuit pathways based on learned currents.", "result": "Proof-of-concept experiments demonstrate the method's effectiveness in transferring between data distributions.", "conclusion": "ECD\u00b2G provides a theoretically sound framework for discrete data generation with provable distribution transfer guarantees."}}
{"id": "2509.24560", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24560", "abs": "https://arxiv.org/abs/2509.24560", "authors": ["Shaohao Rui", "Kaitao Chen", "Weijie Ma", "Xiaosong Wang"], "title": "AdaThink-Med: Medical Adaptive Thinking with Uncertainty-Guided Length Calibration", "comment": null, "summary": "Recent advances in inference time scaling with extended long chain-of thought\nhave significantly improved the reasoning capabilities of both general and\nmedical large language models (LLMs). However, these models tend to engage in\nlengthy reasoning processes regardless of the difficulty of the input question,\nleading to increased inference costs in real-world applications. Therefore,\nenabling adaptive thinking where models think less for simpler questions and\nthink more for complex ones is critical for the effective use of medical LLMs\nin practice. Despite its importance, there is a lack of end-to-end approaches\ndesigned to enhance the adaptive thinking capabilities of medical LLMs while\nproviding a comprehensive examination of the trade-off between performance and\ncomputational cost. To bridge this gap, we propose AdaThink-Med, the first\nend-to-end framework designed to enhance adaptive thinking ability in medical\nreasoning models with uncertainty-guided length calibration. AdaThink-Med first\ngenerates multiple candidate outputs for each question, evaluates the\ncorrectness and uncertainty of each candidate, and then estimates problem\ndifficulty via an uncertainty-guided length calibration module. For outputs\nwith low difficulty and correct answers, the framework penalizes longer\nreasoning paths; whereas for those with high difficulty and incorrect answers,\nit encourages extending the chain of thought to explore alternative solutions.\nOn six public medical QA benchmarks, AdaThink-Med achieves up to 6.4x length\nreduction on average while retaining performance with only minimal degradation.\nIntriguingly, we observe that AdaThink-Med spontaneously develops two distinct\nreasoning modes, which we characterize as \"non-thinking\" and \"thinking\",\ndemonstrating the model's ability to suppress redundant reasoning processes\ndynamically.", "AI": {"tldr": "AdaThink-Med is an end-to-end framework that enhances adaptive thinking in medical LLMs by using uncertainty-guided length calibration to reduce unnecessary reasoning for simple questions while maintaining performance on complex ones.", "motivation": "Current medical LLMs engage in lengthy reasoning regardless of question difficulty, leading to high inference costs. Adaptive thinking - thinking less for simple questions and more for complex ones - is crucial for practical medical applications.", "method": "Generates multiple candidate outputs per question, evaluates correctness and uncertainty, estimates problem difficulty via uncertainty-guided length calibration, penalizes long reasoning for easy questions, and extends reasoning for difficult ones.", "result": "Achieves up to 6.4x length reduction on average across six medical QA benchmarks while maintaining performance with minimal degradation. Spontaneously develops \"non-thinking\" and \"thinking\" reasoning modes.", "conclusion": "AdaThink-Med effectively enables adaptive thinking in medical LLMs, significantly reducing computational costs while preserving reasoning quality, demonstrating practical viability for real-world medical applications."}}
{"id": "2509.23830", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.23830", "abs": "https://arxiv.org/abs/2509.23830", "authors": ["Albus Yizhuo Li"], "title": "Bayesian Mixture-of-Experts: Towards Making LLMs Know What They Don't Know", "comment": null, "summary": "The Mixture-of-Experts (MoE) architecture has enabled the creation of massive\nyet efficient Large Language Models (LLMs). However, the standard deterministic\nrouting mechanism presents a significant limitation: its inherent brittleness\nis a key contributor to model miscalibration and overconfidence, resulting in\nsystems that often do not know what they don't know.\n  This thesis confronts this challenge by proposing a structured\n\\textbf{Bayesian MoE routing framework}. Instead of forcing a single,\ndeterministic expert selection, our approach models a probability distribution\nover the routing decision itself. We systematically investigate three families\nof methods that introduce this principled uncertainty at different stages of\nthe routing pipeline: in the \\textbf{weight-space}, the \\textbf{logit-space},\nand the final \\textbf{selection-space}.\n  Through a series of controlled experiments on a 3-billion parameter MoE\nmodel, we demonstrate that this framework significantly improves routing\nstability, in-distribution calibration, and out-of-distribution (OoD)\ndetection. The results show that by targeting this core architectural\ncomponent, we can create a more reliable internal uncertainty signal. This work\nprovides a practical and computationally tractable pathway towards building\nmore robust and self-aware LLMs, taking a crucial step towards making them know\nwhat they don't know.", "AI": {"tldr": "Proposes a Bayesian MoE routing framework to address brittleness in standard deterministic routing, improving model calibration and uncertainty awareness.", "motivation": "Standard deterministic routing in Mixture-of-Experts models causes brittleness, leading to model miscalibration, overconfidence, and inability to know what they don't know.", "method": "Introduces Bayesian MoE routing framework modeling probability distribution over routing decisions, with three method families: weight-space, logit-space, and selection-space uncertainty.", "result": "Significantly improves routing stability, in-distribution calibration, and out-of-distribution detection in 3-billion parameter MoE model experiments.", "conclusion": "Provides practical pathway to build more robust and self-aware LLMs by targeting core architectural component to create reliable internal uncertainty signal."}}
{"id": "2509.24597", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24597", "abs": "https://arxiv.org/abs/2509.24597", "authors": ["Melika Honarmand", "Ayati Sharma", "Badr AlKhamissi", "Johannes Mehrer", "Martin Schrimpf"], "title": "Inducing Dyslexia in Vision Language Models", "comment": null, "summary": "Dyslexia, a neurodevelopmental disorder characterized by persistent reading\ndifficulties, is often linked to reduced activity of the visual word form area\nin the ventral occipito-temporal cortex. Traditional approaches to studying\ndyslexia, such as behavioral and neuroimaging methods, have provided valuable\ninsights but remain limited in their ability to test causal hypotheses about\nthe underlying mechanisms of reading impairments. In this study, we use\nlarge-scale vision-language models (VLMs) to simulate dyslexia by functionally\nidentifying and perturbing artificial analogues of word processing. Using\nstimuli from cognitive neuroscience, we identify visual-word-form-selective\nunits within VLMs and demonstrate that targeted ablation of these units, unlike\nablation of random units, leads to selective impairments in reading tasks while\ngeneral visual and language comprehension abilities remain intact. In\nparticular, the resulting model matches dyslexic humans' phonological deficits\nwithout a significant change in orthographic processing. Taken together, our\nmodeling results replicate key characteristics of dyslexia and establish a\ncomputational framework for investigating reading disorders.", "AI": {"tldr": "Using vision-language models to simulate dyslexia by ablating visual-word-form-selective units, which causes reading impairments while preserving general visual and language abilities.", "motivation": "Traditional methods for studying dyslexia are limited in testing causal hypotheses about reading impairments, so computational models are needed to establish causal mechanisms.", "method": "Identify visual-word-form-selective units in VLMs and perform targeted ablation of these units, comparing with random unit ablation as control.", "result": "Targeted ablation of word-form units causes selective reading impairments matching dyslexic patterns (phonological deficits without orthographic changes), while general visual/language abilities remain intact.", "conclusion": "The model successfully replicates key dyslexia characteristics and provides a computational framework for investigating reading disorders."}}
{"id": "2509.23846", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23846", "abs": "https://arxiv.org/abs/2509.23846", "authors": ["Daniele Foffano", "Alessio Russo", "Alexandre Proutiere"], "title": "Adversarial Diffusion for Robust Reinforcement Learning", "comment": null, "summary": "Robustness to modeling errors and uncertainties remains a central challenge\nin reinforcement learning (RL). In this work, we address this challenge by\nleveraging diffusion models to train robust RL policies. Diffusion models have\nrecently gained popularity in model-based RL due to their ability to generate\nfull trajectories \"all at once\", mitigating the compounding errors typical of\nstep-by-step transition models. Moreover, they can be conditioned to sample\nfrom specific distributions, making them highly flexible. We leverage\nconditional sampling to learn policies that are robust to uncertainty in\nenvironment dynamics. Building on the established connection between\nConditional Value at Risk (CVaR) optimization and robust RL, we introduce\nAdversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides\nthe diffusion process to generate worst-case trajectories during training,\neffectively optimizing the CVaR of the cumulative return. Empirical results\nacross standard benchmarks show that AD-RRL achieves superior robustness and\nperformance compared to existing robust RL methods.", "AI": {"tldr": "AD-RRL uses diffusion models to train robust RL policies by generating worst-case trajectories during training, optimizing CVaR for better robustness against environment uncertainties.", "motivation": "Address the challenge of robustness to modeling errors and uncertainties in reinforcement learning, which remains a central issue in RL applications.", "method": "Leverage diffusion models with conditional sampling to generate worst-case trajectories, building on CVaR optimization to create Adversarial Diffusion for Robust Reinforcement Learning (AD-RRL).", "result": "Empirical results across standard benchmarks demonstrate that AD-RRL achieves superior robustness and performance compared to existing robust RL methods.", "conclusion": "AD-RRL effectively addresses robustness challenges in RL by using diffusion models to optimize CVaR through adversarial trajectory generation during training."}}
{"id": "2509.24613", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.24613", "abs": "https://arxiv.org/abs/2509.24613", "authors": ["Gio Paik", "Yongbeom Kim", "Soungmin Lee", "Sangmin Ahn", "Chanwoo Kim"], "title": "HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition", "comment": "5 pages, 2 figures, Submitted to ICASSP2026", "summary": "Despite advances in multilingual automatic speech recognition (ASR),\ncode-switching (CS), the mixing of languages within an utterance common in\ndaily speech, remains a severely underexplored challenge. In this paper, we\nintroduce HiKE: the Hierarchical Korean-English code-switching benchmark, the\nfirst globally accessible evaluation framework for Korean-English CS, aiming to\nprovide a means for the precise evaluation of multilingual ASR models and to\nfoster research in the field. The proposed framework not only consists of\nhigh-quality, natural CS data across various topics, but also provides\nmeticulous loanword labels and a hierarchical CS-level labeling scheme (word,\nphrase, and sentence) that together enable a systematic evaluation of a model's\nability to handle each distinct level of code-switching. Through evaluations of\ndiverse multilingual ASR models and fine-tuning experiments, this paper\ndemonstrates that while most multilingual ASR models initially struggle with\nCS-ASR, this capability can be enabled through fine-tuning with CS data. HiKE\nwill be available at https://github.com/ThetaOne-AI/HiKE.", "AI": {"tldr": "HiKE is the first Korean-English code-switching benchmark providing hierarchical CS-level labels for systematic ASR evaluation, showing that fine-tuning with CS data enables multilingual ASR models to handle code-switching.", "motivation": "Code-switching remains an underexplored challenge in multilingual ASR despite advances, with Korean-English CS lacking accessible evaluation frameworks.", "method": "Developed HiKE benchmark with high-quality natural CS data across topics, providing loanword labels and hierarchical CS-level labeling (word, phrase, sentence) for systematic evaluation.", "result": "Most multilingual ASR models initially struggle with CS-ASR but can be enabled through fine-tuning with CS data.", "conclusion": "HiKE provides the first globally accessible Korean-English CS evaluation framework that enables systematic assessment and improvement of multilingual ASR models' code-switching capabilities."}}
{"id": "2509.23866", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23866", "abs": "https://arxiv.org/abs/2509.23866", "authors": ["Pengxiang Li", "Zechen Hu", "Zirui Shang", "Jingrong Wu", "Yang Liu", "Hui Liu", "Zhi Gao", "Chenrui Shi", "Bofei Zhang", "Zihao Zhang", "Xiaochuan Shi", "Zedong YU", "Yuwei Wu", "Xinxiao Wu", "Yunde Jia", "Liuyu Xiang", "Zhaofeng He", "Qing Li"], "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation", "comment": null, "summary": "Vision-language model (VLM) based GUI agents show promise for automating\ncomplex desktop and mobile tasks, but face significant challenges in applying\nreinforcement learning (RL): (1) slow multi-turn interactions with GUI\nenvironments for policy rollout, and (2) insufficient high-quality\nagent-environment interactions for policy learning. To address these\nchallenges, we propose DART, a Decoupled Agentic RL Training framework for GUI\nagents, which coordinates heterogeneous modules in a highly decoupled manner.\nDART separates the training system into four asynchronous modules: environment\ncluster, rollout service, data manager, and trainer. This design enables\nnon-blocking communication, asynchronous training, rollout-wise trajectory\nsampling, and per-worker model synchronization, significantly improving the\nsystem efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,\nand 5.5* environment utilization. To facilitate effective learning from\nabundant samples, we introduce an adaptive data curation scheme: (1)\npre-collecting successful trajectories for challenging tasks to supplement\nsparse success in online sampling; (2) dynamically adjusting rollout numbers\nand trajectory lengths based on task difficulty; (3) training selectively on\nhigh-entropy steps to prioritize critical decisions; (4) stabilizing learning\nvia truncated importance sampling for policy mismatch between policy rollout\nand updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task\nsuccess rate, a 14.61% absolute gain over the base model, and 7.34% higher than\nopen-source SOTA. We will fully open-source our training framework, data, and\nmodel checkpoints via computer-use-agents.github.io/dart-gui, which we believe\nis a timely contribution to the open-source community of agentic RL training.", "AI": {"tldr": "DART is a decoupled agentic RL training framework for GUI agents that addresses slow multi-turn interactions and insufficient training data through asynchronous modules and adaptive data curation, achieving significant performance improvements on the OSWorld benchmark.", "motivation": "Vision-language model based GUI agents face challenges in reinforcement learning due to slow multi-turn interactions with GUI environments and insufficient high-quality agent-environment interactions for effective policy learning.", "method": "DART separates the training system into four asynchronous modules (environment cluster, rollout service, data manager, trainer) with non-blocking communication, and introduces adaptive data curation including pre-collecting successful trajectories, dynamic adjustment of rollout parameters, selective training on high-entropy steps, and truncated importance sampling.", "result": "DART achieves 1.6x GPU utilization for rollout, 1.9x training throughput, and 5.5x environment utilization. On OSWorld benchmark, DART-GUI-7B achieves 42.13% task success rate, a 14.61% absolute gain over base model and 7.34% higher than open-source SOTA.", "conclusion": "DART provides an efficient and effective framework for training GUI agents through decoupled architecture and adaptive data curation, demonstrating significant performance improvements and will be open-sourced to benefit the agentic RL community."}}
{"id": "2509.24638", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24638", "abs": "https://arxiv.org/abs/2509.24638", "authors": ["Bojan Batalo", "Erica K. Shimomoto", "Neil Millar"], "title": "Hype or not? Formalizing Automatic Promotional Language Detection in Biomedical Research", "comment": null, "summary": "In science, promotional language ('hype') is increasing and can undermine\nobjective evaluation of evidence, impede research development, and erode trust\nin science. In this paper, we introduce the task of automatic detection of\nhype, which we define as hyperbolic or subjective language that authors use to\nglamorize, promote, embellish, or exaggerate aspects of their research. We\npropose formalized guidelines for identifying hype language and apply them to\nannotate a portion of the National Institutes of Health (NIH) grant application\ncorpus. We then evaluate traditional text classifiers and language models on\nthis task, comparing their performance with a human baseline. Our experiments\nshow that formalizing annotation guidelines can help humans reliably annotate\ncandidate hype adjectives and that using our annotated dataset to train machine\nlearning models yields promising results. Our findings highlight the linguistic\ncomplexity of the task, and the potential need for domain knowledge and\ntemporal awareness of the facts. While some linguistic works address hype\ndetection, to the best of our knowledge, we are the first to approach it as a\nnatural language processing task.", "AI": {"tldr": "This paper introduces automatic hype detection in scientific writing, defines hype as hyperbolic/subjective language used to glamorize research, creates annotation guidelines for NIH grant applications, and evaluates ML models showing promising results.", "motivation": "Hype language in science is increasing and can undermine objective evaluation, impede research development, and erode trust in science.", "method": "Proposed formalized annotation guidelines for identifying hype language, annotated NIH grant application corpus, evaluated traditional text classifiers and language models against human baseline.", "result": "Formalizing annotation guidelines helped humans reliably annotate hype adjectives, and training ML models on the annotated dataset yielded promising results.", "conclusion": "Hype detection is linguistically complex and may require domain knowledge and temporal awareness, but this is the first NLP approach to this task with promising initial results."}}
{"id": "2509.23886", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23886", "abs": "https://arxiv.org/abs/2509.23886", "authors": ["Simon Schrodi", "Elias Kempf", "Fazl Barez", "Thomas Brox"], "title": "Towards Understanding Subliminal Learning: When and How Hidden Biases Transfer", "comment": null, "summary": "Language models can transfer hidden biases during distillation. For example,\na teacher that \"likes owls\" can make its student \"like owls\" too, even when the\ntraining data consists only of lists of numbers. This surprising phenomenon is\ncalled subliminal learning. Subliminal learning can be expected under soft\ndistillation, where the student is trained on the teacher's full next-token\ndistribution. But the fact that this also occurs under hard distillation-where\nthe student only sees sampled tokens-raises a deeper question: when and how\ndoes subliminal learning actually occur? We answer this question through\ncontrolled experiments and mechanistic analysis. Our results show that\nsubliminal learning does not need (global) token entanglement or logit leakage.\nInstead, it comes down to a small set of divergence tokens-rare cases where\nteachers with different biases would predict different tokens. Masking out\nthese tokens mostly removes the hidden bias transfer. Mechanistically,\ndivergence tokens reveal that early layers are critical. Surprisingly,\nfinetuning even a single such early layer is sufficient for subliminal\nlearning. Finally, we find that subliminal learning is fragile. Even small\nchanges, like paraphrasing prompts, are usually sufficient to suppress it.", "AI": {"tldr": "Subliminal learning occurs in language model distillation where hidden biases transfer from teacher to student, even with hard distillation. This happens through divergence tokens - rare cases where biased teachers predict different tokens. Early layers are critical, and subliminal learning is fragile to prompt changes.", "motivation": "To understand when and how subliminal learning occurs in language model distillation, particularly why it happens under hard distillation where students only see sampled tokens rather than full distributions.", "method": "Conducted controlled experiments and mechanistic analysis to identify the mechanism behind subliminal learning. Investigated token entanglement, logit leakage, and identified divergence tokens as the key mechanism. Tested layer importance through selective finetuning.", "result": "Subliminal learning doesn't require global token entanglement or logit leakage. It occurs through divergence tokens - rare cases where biased teachers predict different tokens. Early layers are critical, and finetuning even a single early layer can enable subliminal learning. The phenomenon is fragile and can be suppressed by small prompt changes.", "conclusion": "Subliminal learning in language model distillation is mediated by divergence tokens rather than global mechanisms, with early layers playing a crucial role. The fragility of this phenomenon suggests it can be mitigated through careful prompt design."}}
{"id": "2509.24663", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24663", "abs": "https://arxiv.org/abs/2509.24663", "authors": ["Weilin Zhao", "Zihan Zhou", "Zhou Su", "Chaojun Xiao", "Yuxuan Li", "Yanghao Li", "Yudi Zhang", "Weilun Zhao", "Zhen Li", "Yuxiang Huang", "Ao Sun", "Xu Han", "Zhiyuan Liu"], "title": "InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation", "comment": null, "summary": "Long-sequence processing is a critical capability for modern large language\nmodels. However, the self-attention mechanism in the standard Transformer\narchitecture faces severe computational and memory bottlenecks when processing\nlong sequences. While trainable sparse attention methods offer a promising\nsolution, existing approaches such as NSA introduce excessive extra parameters\nand disrupt the conventional \\textit{pretrain-on-short, finetune-on-long}\nworkflow, resulting in slow convergence and difficulty in acceleration. To\novercome these limitations, we introduce dense-sparse switchable attention\nframework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that\nseamlessly adapts models from short to long sequences. Specifically, InfLLM-V2\nreuses dense attention parameters through parameter-free architecture\nmodification, maintaining consistency between short and long sequence\nprocessing. Additionally, InfLLM-V2 ensures computational efficiency across all\nsequence lengths, by using dense attention for short inputs and smoothly\ntransitioning to sparse attention for long sequences. To achieve practical\nacceleration, we further introduce an efficient implementation of InfLLM-V2\nthat significantly reduces the computational overhead. Our experiments on\nlong-context understanding and chain-of-thought reasoning demonstrate that\nInfLLM-V2 is 4$\\times$ faster than dense attention while retaining 98.1% and\n99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we\nhave trained and open-sourced MiniCPM4.1\n(https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model,\nproviding a reproducible implementation for the research community.", "AI": {"tldr": "InfLLM-V2 is a trainable sparse attention framework that enables efficient long-sequence processing by reusing dense attention parameters and smoothly switching between dense and sparse attention based on sequence length.", "motivation": "To overcome computational and memory bottlenecks of self-attention in Transformers for long sequences, while avoiding excessive parameters and maintaining the pretrain-on-short, finetune-on-long workflow.", "method": "Dense-sparse switchable attention that reuses dense attention parameters through parameter-free architecture modification, using dense attention for short inputs and sparse attention for long sequences with efficient implementation.", "result": "4x faster than dense attention while retaining 98.1% performance on long-context understanding and 99.7% on chain-of-thought reasoning. MiniCPM4.1 model trained and open-sourced.", "conclusion": "InfLLM-V2 provides an effective solution for efficient long-sequence processing that maintains performance while significantly improving computational efficiency across all sequence lengths."}}
{"id": "2509.23887", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23887", "abs": "https://arxiv.org/abs/2509.23887", "authors": ["Yash Jakhmola"], "title": "Gradient Flow Convergence Guarantee for General Neural Network Architectures", "comment": "12 pages, 3 figures, 1 table", "summary": "A key challenge in modern deep learning theory is to explain the remarkable\nsuccess of gradient-based optimization methods when training large-scale,\ncomplex deep neural networks. Though linear convergence of such methods has\nbeen proved for a handful of specific architectures, a united theory still\nevades researchers. This article presents a unified proof for linear\nconvergence of continuous gradient descent, also called gradient flow, while\ntraining any neural network with piecewise non-zero polynomial activations or\nReLU, sigmoid activations. Our primary contribution is a single, general\ntheorem that not only covers architectures for which this result was previously\nunknown but also consolidates existing results under weaker assumptions. While\nour focus is theoretical and our results are only exact in the infinitesimal\nstep size limit, we nevertheless find excellent empirical agreement between the\npredictions of our result and those of the practical step-size gradient descent\nmethod.", "AI": {"tldr": "This paper provides a unified proof for linear convergence of gradient flow in training neural networks with various activations, covering both previously known and new architectures under weaker assumptions.", "motivation": "To address the theoretical challenge of explaining why gradient-based optimization successfully trains complex deep neural networks, and to develop a unified theory that covers multiple architectures.", "method": "The authors present a general theorem for linear convergence of continuous gradient descent (gradient flow) for neural networks with piecewise non-zero polynomial activations, ReLU, and sigmoid activations.", "result": "The unified proof covers architectures previously unknown and consolidates existing results under weaker assumptions, with empirical agreement between theoretical predictions and practical gradient descent.", "conclusion": "The paper establishes a comprehensive theoretical foundation for linear convergence in neural network training, bridging theory and practice for gradient-based optimization methods."}}
{"id": "2509.24675", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24675", "abs": "https://arxiv.org/abs/2509.24675", "authors": ["Qingjie Zhang", "Haoting Qian", "Zhicong Huang", "Cheng Hong", "Minlie Huang", "Ke Xu", "Chao Zhang", "Han Qiu"], "title": "Understanding the Dilemma of Unlearning for Large Language Models", "comment": null, "summary": "Unlearning seeks to remove specific knowledge from large language models\n(LLMs), but its effectiveness remains contested. On one side, \"forgotten\"\nknowledge can often be recovered through interventions such as light\nfine-tuning; on the other side, unlearning may induce catastrophic forgetting\nthat degrades general capabilities. Despite active exploration of unlearning\nmethods, interpretability analyses of the mechanism are scarce due to the\ndifficulty of tracing knowledge in LLMs' complex architectures. We address this\ngap by proposing unPact, an interpretable framework for unlearning via prompt\nattribution and contribution tracking. Typically, it quantifies each prompt\ntoken's influence on outputs, enabling pre- and post-unlearning comparisons to\nreveal what changes. Across six mainstream unlearning methods, three LLMs, and\nthree benchmarks, we find that: (1) Unlearning appears to be effective by\ndisrupting focus on keywords in prompt; (2) Much of the knowledge is not truly\nerased and can be recovered by simply emphasizing these keywords in prompts,\nwithout modifying the model's weights; (3) Catastrophic forgetting arises from\nindiscriminate penalization of all tokens. Taken together, our results suggest\nan unlearning dilemma: existing methods tend either to be insufficient -\nknowledge remains recoverable by keyword emphasis, or overly destructive -\ngeneral performance collapses due to catastrophic forgetting, still leaving a\ngap to reliable unlearning.", "AI": {"tldr": "The paper proposes unPact, an interpretable framework for analyzing unlearning in LLMs, revealing that current methods either insufficiently remove knowledge (recoverable via keyword emphasis) or cause catastrophic forgetting.", "motivation": "To address the lack of interpretability in understanding how unlearning works in LLMs and why its effectiveness is contested.", "method": "Developed unPact framework that quantifies prompt token influence on outputs through attribution and contribution tracking, enabling pre/post-unlearning comparisons.", "result": "Found that unlearning works by disrupting keyword focus rather than true knowledge erasure, knowledge remains recoverable via keyword emphasis, and catastrophic forgetting stems from indiscriminate token penalization.", "conclusion": "Existing unlearning methods face a dilemma: either insufficient (knowledge recoverable) or overly destructive (catastrophic forgetting), leaving a gap to reliable unlearning."}}
{"id": "2509.24678", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24678", "abs": "https://arxiv.org/abs/2509.24678", "authors": ["Leander Girrbach", "Chi-Ping Su", "Tankred Saanum", "Richard Socher", "Eric Schulz", "Zeynep Akata"], "title": "Reference-Free Rating of LLM Responses via Latent Information", "comment": "21 pages", "summary": "How reliable are single-response LLM-as-a-judge ratings without references,\nand can we obtain fine-grained, deterministic scores in this setting? We study\nthe common practice of asking a judge model to assign Likert-scale scores to\nfree-text responses and show two systematic issues: scores are unstable under\nsampling and poorly calibrated, leading to compression near the top of the\nscale and frequent ties. We then propose and evaluate Latent Judges, which\nderive scalar ratings from internal model signals: (i) probability-weighted\nscores over integer ratings, (ii) verifier-style probabilities of \"yes\", and\n(iii) linear probes trained on model activations at the rating position. Across\na broad suite of pairwise and single-rating benchmarks, latent methods match or\nsurpass standard prompting, with consistent gains on pairwise accuracy and\nlistwise ranking relevant to Best-of-N selection. Probability-weighted scores\nachieve the strongest single-rating correlations, while probes recover useful\nsignals when output logits are miscalibrated. These results indicate that\nlatent information provides deterministic and more discriminative signals for\nreference-free evaluation, and can improve selection and training approaches\nlike Best-of-$N$, multi-teacher distillation, and routing.", "AI": {"tldr": "The paper studies the unreliability of single-response LLM-as-a-judge ratings without references, identifies issues with score instability and poor calibration, and proposes Latent Judges methods that derive scalar ratings from internal model signals to provide more deterministic and discriminative evaluation.", "motivation": "To address the systematic issues in current LLM-as-a-judge practices where scores are unstable under sampling, poorly calibrated, compressed near the top of the scale, and frequently tied, making single-response ratings unreliable.", "method": "Proposes three Latent Judges methods: (1) probability-weighted scores over integer ratings, (2) verifier-style probabilities of \"yes\", and (3) linear probes trained on model activations at the rating position.", "result": "Across various pairwise and single-rating benchmarks, latent methods match or surpass standard prompting, with consistent gains on pairwise accuracy and listwise ranking. Probability-weighted scores achieve the strongest single-rating correlations, while probes recover useful signals when output logits are miscalibrated.", "conclusion": "Latent information provides deterministic and more discriminative signals for reference-free evaluation, and can improve selection and training approaches like Best-of-N, multi-teacher distillation, and routing."}}
{"id": "2509.23898", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23898", "abs": "https://arxiv.org/abs/2509.23898", "authors": ["Chris Kolb", "Laetitia Frost", "Bernd Bischl", "David R\u00fcgamer"], "title": "Differentiable Sparsity via $D$-Gating: Simple and Versatile Structured Penalization", "comment": null, "summary": "Structured sparsity regularization offers a principled way to compact neural\nnetworks, but its non-differentiability breaks compatibility with conventional\nstochastic gradient descent and requires either specialized optimizers or\nadditional post-hoc pruning without formal guarantees. In this work, we propose\n$D$-Gating, a fully differentiable structured overparameterization that splits\neach group of weights into a primary weight vector and multiple scalar gating\nfactors. We prove that any local minimum under $D$-Gating is also a local\nminimum using non-smooth structured $L_{2,2/D}$ penalization, and further show\nthat the $D$-Gating objective converges at least exponentially fast to the\n$L_{2,2/D}$-regularized loss in the gradient flow limit. Together, our results\nshow that $D$-Gating is theoretically equivalent to solving the original group\nsparsity problem, yet induces distinct learning dynamics that evolve from a\nnon-sparse regime into sparse optimization. We validate our theory across\nvision, language, and tabular tasks, where $D$-Gating consistently delivers\nstrong performance-sparsity tradeoffs and outperforms both direct optimization\nof structured penalties and conventional pruning baselines.", "AI": {"tldr": "D-Gating is a differentiable structured overparameterization method that achieves structured sparsity in neural networks by splitting weights into primary vectors and gating factors, with theoretical guarantees of equivalence to L2,2/D regularization and exponential convergence.", "motivation": "Structured sparsity regularization is effective for network compression but incompatible with standard SGD due to non-differentiability, requiring specialized optimizers or post-hoc pruning without formal guarantees.", "method": "Proposes D-Gating that splits each weight group into a primary weight vector and multiple scalar gating factors, creating a fully differentiable overparameterization that enables structured sparsity learning.", "result": "Proves theoretical equivalence to L2,2/D regularization, shows exponential convergence to regularized loss, and demonstrates superior performance-sparsity tradeoffs across vision, language, and tabular tasks compared to direct optimization and pruning baselines.", "conclusion": "D-Gating provides a theoretically grounded, differentiable approach to structured sparsity that evolves naturally from non-sparse to sparse optimization while maintaining compatibility with standard training methods."}}
{"id": "2509.24704", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24704", "abs": "https://arxiv.org/abs/2509.24704", "authors": ["Guibin Zhang", "Muxin Fu", "Shuicheng Yan"], "title": "MemGen: Weaving Generative Latent Memory for Self-Evolving Agents", "comment": null, "summary": "Agent memory shapes how Large Language Model (LLM)-powered agents, akin to\nthe human brain, progressively refine themselves through environment\ninteractions. Existing paradigms remain constrained: parametric memory forcibly\nadjusts model parameters, and retrieval-based memory externalizes experience\ninto structured databases, yet neither captures the fluid interweaving of\nreasoning and memory that underlies human cognition. To address this gap, we\npropose MemGen, a dynamic generative memory framework that equips agents with a\nhuman-esque cognitive faculty. It consists of a \\textit{memory trigger}, which\nmonitors the agent's reasoning state to decide explicit memory invocation, and\na \\textit{memory weaver}, which takes the agent's current state as stimulus to\nconstruct a latent token sequence as machine-native memory to enrich its\nreasoning. In this way, MemGen enables agents to recall and augment latent\nmemory throughout reasoning, producing a tightly interwoven cycle of memory and\ncognition. Extensive experiments across eight benchmarks show that MemGen\nsurpasses leading external memory systems such as ExpeL and AWM by up to\n$38.22\\%$, exceeds GRPO by up to $13.44\\%$, and exhibits strong cross-domain\ngeneralization ability. More importantly, we find that without explicit\nsupervision, MemGen spontaneously evolves distinct human-like memory faculties,\nincluding planning memory, procedural memory, and working memory, suggesting an\nemergent trajectory toward more naturalistic forms of machine cognition.", "AI": {"tldr": "MemGen is a dynamic generative memory framework that enables LLM agents to interweave memory and reasoning through latent token sequences, achieving significant performance improvements over existing memory systems and exhibiting emergent human-like memory faculties.", "motivation": "Existing memory paradigms for LLM agents are constrained - parametric memory forcibly adjusts model parameters while retrieval-based memory externalizes experience into databases, neither capturing the fluid integration of reasoning and memory that characterizes human cognition.", "method": "MemGen consists of a memory trigger that monitors the agent's reasoning state to decide explicit memory invocation, and a memory weaver that takes the current state as stimulus to construct latent token sequences as machine-native memory to enrich reasoning.", "result": "Extensive experiments across eight benchmarks show MemGen surpasses leading external memory systems (ExpeL, AWM) by up to 38.22%, exceeds GRPO by up to 13.44%, and exhibits strong cross-domain generalization ability.", "conclusion": "MemGen enables agents to recall and augment latent memory throughout reasoning, creating a tightly interwoven cycle of memory and cognition, and spontaneously evolves distinct human-like memory faculties including planning, procedural, and working memory without explicit supervision."}}
{"id": "2509.23905", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.23905", "abs": "https://arxiv.org/abs/2509.23905", "authors": ["Tianjiao Sun", "Ningyan Guo", "Haozhe Gu", "Yanyan Peng", "Zhiyong Feng"], "title": "Integrated Communication and Control for Energy-Efficient UAV Swarms: A Multi-Agent Reinforcement Learning Approach", "comment": null, "summary": "The deployment of unmanned aerial vehicle (UAV) swarm-assisted communication\nnetworks has become an increasingly vital approach for remediating coverage\nlimitations in infrastructure-deficient environments, with especially pressing\napplications in temporary scenarios, such as emergency rescue, military and\nsecurity operations, and remote area coverage. However, complex geographic\nenvironments lead to unpredictable and highly dynamic wireless channel\nconditions, resulting in frequent interruptions of air-to-ground (A2G) links\nthat severely constrain the reliability and quality of service in UAV\nswarm-assisted mobile communications. To improve the quality of UAV\nswarm-assisted communications in complex geographic environments, we propose an\nintegrated communication and control co-design mechanism. Given the stringent\nenergy constraints inherent in UAV swarms, our proposed mechanism is designed\nto optimize energy efficiency while maintaining an equilibrium between\nequitable communication rates for mobile ground users (GUs) and UAV energy\nexpenditure. We formulate the joint resource allocation and 3D trajectory\ncontrol problem as a Markov decision process (MDP), and develop a multi-agent\nreinforcement learning (MARL) framework to enable real-time coordinated actions\nacross the UAV swarm. To optimize the action policy of UAV swarms, we propose a\nnovel multi-agent hybrid proximal policy optimization with action masking\n(MAHPPO-AM) algorithm, specifically designed to handle complex hybrid action\nspaces. The algorithm incorporates action masking to enforce hard constraints\nin high-dimensional action spaces. Experimental results demonstrate that our\napproach achieves a fairness index of 0.99 while reducing energy consumption by\nup to 25% compared to baseline methods.", "AI": {"tldr": "Proposes an integrated communication-control co-design using multi-agent reinforcement learning (MAHPPO-AM) to optimize UAV swarm energy efficiency and communication fairness in complex environments.", "motivation": "UAV swarm communications face unreliable A2G links in complex geographic environments, requiring energy-efficient solutions that maintain communication quality for ground users.", "method": "Formulates joint resource allocation and 3D trajectory control as MDP, develops MARL framework with novel MAHPPO-AM algorithm using action masking for hybrid action spaces.", "result": "Achieves 0.99 fairness index while reducing energy consumption by up to 25% compared to baseline methods.", "conclusion": "The proposed integrated communication-control co-design effectively balances energy efficiency and communication fairness in UAV swarm networks."}}
{"id": "2509.24726", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24726", "abs": "https://arxiv.org/abs/2509.24726", "authors": ["Shaobo Wang", "Zhengbo Jiao", "Zifan Zhang", "Yilang Peng", "Xu Ze", "Boyu Yang", "Wei Wang", "Hu Wei", "Linfeng Zhang"], "title": "Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution", "comment": "23 pages, 3 figures", "summary": "Recent breakthroughs in large language models (LLMs) on reasoning tasks rely\nheavily on massive, high-quality datasets-typically human-annotated and thus\ndifficult to scale. While data synthesis or distillation offers a promising\nalternative, existing methods struggle with inconsistent data quality and an\ninability to dynamically adapt to the evolving capabilities of the model,\nleading to suboptimal training signals. To address these limitations, we\nintroduce Socratic-Zero, a fully autonomous framework that generates\nhigh-quality training data from minimal seed examples through the co-evolution\nof three agents: the Teacher, the Solver, and the Generator. The Solver\ncontinuously refines its reasoning by learning from preference feedback on both\nsuccessful and failed trajectories; the Teacher adaptively crafts increasingly\nchallenging questions based on the Solver's weaknesses; and the Generator\ndistills the Teacher's question-design strategy to enable scalable,\nhigh-fidelity curriculum generation. This closed-loop system produces a\nself-improving curriculum-requiring no pre-existing tasks or labels.\nRemarkably, starting from only 100 seed questions, our Socratic-Solver-8B\nachieves an average gain of +20.2 percentage points over prior data synthesis\nmethods across seven mathematical reasoning benchmarks (AMC23, AIME24-25,\nOlympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3\nand GLM4 series models. Even more surprisingly, synthetic data from\nSocratic-Generator-32B enables student LLMs to achieve superior performance\ncompared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks,\nincluding Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4,\nand Claude-4.1-Opus.", "AI": {"tldr": "Socratic-Zero is an autonomous framework that generates high-quality training data from minimal seed examples through co-evolution of three agents, achieving significant performance gains on mathematical reasoning benchmarks without requiring pre-existing tasks or labels.", "motivation": "Current LLM training relies on massive human-annotated datasets that are difficult to scale, while existing data synthesis methods suffer from inconsistent quality and inability to adapt to model evolution.", "method": "Uses three co-evolving agents: Solver (refines reasoning from preference feedback), Teacher (creates challenging questions based on Solver's weaknesses), and Generator (distills Teacher's strategy for scalable curriculum generation).", "result": "Starting from only 100 seed questions, Socratic-Solver-8B achieves +20.2 percentage point average gain over prior methods across 7 mathematical reasoning benchmarks. Synthetic data from Socratic-Generator-32B enables student LLMs to outperform SOTA commercial models.", "conclusion": "The closed-loop co-evolution system enables fully autonomous generation of high-quality training data that significantly improves LLM performance on reasoning tasks without human annotation."}}
{"id": "2509.23923", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23923", "abs": "https://arxiv.org/abs/2509.23923", "authors": ["Maya Bechler-Speicher", "Andrea Zerio", "Maor Huri", "Marie Vibeke Vestergaard", "Ran Gilad-Bachrach", "Tine Jess", "Samir Bhatt", "Aleksejs Sazonovs"], "title": "Graph Mixing Additive Networks", "comment": "arXiv admin note: substantial text overlap with arXiv:2505.19193", "summary": "We introduce GMAN, a flexible, interpretable, and expressive framework that\nextends Graph Neural Additive Networks (GNANs) to learn from sets of sparse\ntime-series data. GMAN represents each time-dependent trajectory as a directed\ngraph and applies an enriched, more expressive GNAN to each graph. It allows\nusers to control the interpretability-expressivity trade-off by grouping\nfeatures and graphs to encode priors, and it provides feature, node, and\ngraph-level interpretability. On real-world datasets, including mortality\nprediction from blood tests and fake-news detection, GMAN outperforms strong\nnon-interpretable black-box baselines while delivering actionable,\ndomain-aligned explanations.", "AI": {"tldr": "GMAN extends Graph Neural Additive Networks to learn from sparse time-series data by representing trajectories as directed graphs, offering flexible interpretability-expressivity trade-offs and outperforming black-box models on real-world tasks.", "motivation": "To create an interpretable yet expressive framework for learning from sets of sparse time-series data that can outperform black-box models while providing actionable explanations.", "method": "Represents time-dependent trajectories as directed graphs and applies enriched Graph Neural Additive Networks (GNANs) to each graph, allowing feature grouping and graph grouping to encode priors and control interpretability-expressivity trade-off.", "result": "Outperforms strong non-interpretable black-box baselines on real-world datasets including mortality prediction from blood tests and fake-news detection.", "conclusion": "GMAN provides a flexible, interpretable framework that delivers domain-aligned explanations while maintaining competitive performance against black-box models."}}
{"id": "2509.24745", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24745", "abs": "https://arxiv.org/abs/2509.24745", "authors": ["Yixuan Wang", "Huang He", "Siqi Bao", "Hua Wu", "Haifeng Wang", "Qingfu Zhu", "Wanxiang Che"], "title": "ProxyAttn: Guided Sparse Attention via Representative Heads", "comment": "14pages, 5figures", "summary": "The quadratic complexity of attention mechanisms limits the efficiency of\nLarge Language Models (LLMs) on long-text tasks. Recently, methods that\ndynamically estimate block importance have enabled efficient block sparse\nattention, leading to significant acceleration in long-text pre-filling of\nLLMs. However, their coarse-grained estimation inevitably leads to performance\ndegradation at high sparsity rates. In this work, we propose ProxyAttn, a\ntraining-free sparse attention algorithm that achieves more precise block\nestimation by compressing the dimension of attention heads. Based on our\nobservation of the similarity among multiple attention heads, we use the scores\nof pooled representative heads to approximate the scores for all heads. To\naccount for the varying sparsity among heads, we also propose a block-aware\ndynamic budget estimation method. By combining the scores from representative\nproxy heads with multi-head dynamic budgets, we achieve a more fine-grained\nblock importance evaluation at low computational cost. Experiments on a variety\nof mainstream models and extensive benchmarks confirm the underlying similarity\namong attention heads. Leveraging a fine-grained estimation, the proposed\nmethod achieves substantial gains in performance and efficiency compared to\nexisting methods. More precisely, ProxyAttn can achieve up to 10.3x attention\nacceleration and 2.4x prefilling acceleration without significant performance\nloss. Our code is available at https://github.com/wyxstriker/ProxyAttn.", "AI": {"tldr": "ProxyAttn is a training-free sparse attention algorithm that accelerates LLMs on long-text tasks by using representative proxy heads for fine-grained block importance estimation, achieving up to 10.3x attention acceleration and 2.4x pre-filling acceleration.", "motivation": "Existing methods for efficient block sparse attention use coarse-grained block importance estimation, which leads to performance degradation at high sparsity rates. The quadratic complexity of attention mechanisms limits LLM efficiency on long-text tasks.", "method": "Compresses attention head dimensions and uses scores from pooled representative heads to approximate all heads' scores. Proposes block-aware dynamic budget estimation to account for varying sparsity among heads. Combines proxy head scores with multi-head dynamic budgets for fine-grained evaluation.", "result": "Experiments confirm similarity among attention heads. Achieves up to 10.3x attention acceleration and 2.4x pre-filling acceleration without significant performance loss across various models and benchmarks.", "conclusion": "ProxyAttn enables more precise block importance estimation at low computational cost, providing substantial gains in both performance and efficiency compared to existing sparse attention methods."}}
{"id": "2509.23928", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23928", "abs": "https://arxiv.org/abs/2509.23928", "authors": ["Zhinan Xie", "Peisong Wang", "Jian Cheng"], "title": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models", "comment": null, "summary": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference.", "AI": {"tldr": "HiViS is a speculative decoding method for Vision-Language Models that hides visual tokens from the drafter to accelerate inference while maintaining quality.", "motivation": "Adapting speculative decoding to VLMs is challenging due to visual token misalignment between drafter and target models, and slow self-attention from numerous visual tokens.", "method": "Removes visual tokens from drafter input, uses only textual tokens as explicit inputs, reuses target VLM's hidden states as implicit visual information, and employs multi-step self-feedback training.", "result": "Compresses drafter prefill sequence length to 0.7%-1.3% of target VLM input, achieves up to 2.65x speedup with lossless generation quality across diverse models and tasks.", "conclusion": "HiViS effectively accelerates VLM inference by addressing visual token challenges in speculative decoding through explicit-implicit input decomposition."}}
{"id": "2509.24771", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24771", "abs": "https://arxiv.org/abs/2509.24771", "authors": ["Guibin Zhang", "Fanci Meng", "Guancheng Wan", "Zherui Li", "Kun Wang", "Zhenfei Yin", "Lei Bai", "Shuicheng Yan"], "title": "LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space", "comment": null, "summary": "Test-time Scaling (TTS) has been demonstrated to significantly enhance the\nreasoning capabilities of Large Language Models (LLMs) during the inference\nphase without altering model parameters. However, existing TTS methods are\nlargely independent, implying that LLMs have not yet evolved to progressively\nlearn how to scale more effectively. With the objective of evolving LLMs to\nlearn ``how to scale test-time computation,'' we propose LatentEvolve, a\nself-evolving latent TTS framework inspired by the complementary learning\nsystem (CLS) theory. Analogous to the human brain's dual system of a\nfast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve\ncomprises two evolutionary components: \\textit{daytime scaling}, which rapidly\nretrieves historical latent representations to better guide current LLM\nreasoning; and \\textit{nighttime scaling}, which integrates past latent\noptimizations in a manner akin to the human brain's consolidation of\nexperiences during sleep. The alternation of daytime and nighttime processes\nfacilitates a fast and slow evolution of LLM TTS, mirroring human cognitive\ndynamics in a fully unsupervised manner. Extensive experiments across eight\nbenchmarks and five model backbones demonstrate that our LatentEvolve surpasses\nstate-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33\\%$ and\nexhibits exceptional cross-domain and cross-backbone generalization.", "AI": {"tldr": "LatentEvolve is a self-evolving test-time scaling framework that enables LLMs to progressively learn how to scale computation more effectively through alternating daytime (fast recall) and nighttime (slow consolidation) processes inspired by human cognitive systems.", "motivation": "Existing test-time scaling methods are independent and don't allow LLMs to progressively learn better scaling strategies. The goal is to evolve LLMs to learn \"how to scale test-time computation\" more effectively over time.", "method": "LatentEvolve uses a dual evolutionary system: daytime scaling rapidly retrieves historical latent representations to guide current reasoning, while nighttime scaling integrates past latent optimizations through consolidation, mimicking human brain's complementary learning system.", "result": "Extensive experiments across 8 benchmarks and 5 model backbones show LatentEvolve surpasses state-of-the-art TTS methods like LatentSeek and TTRL by up to 13.33%, with exceptional cross-domain and cross-backbone generalization.", "conclusion": "The alternating daytime-nighttime evolution process successfully enables LLMs to progressively learn effective test-time scaling strategies, achieving superior performance and generalization compared to existing methods."}}
{"id": "2509.23933", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23933", "abs": "https://arxiv.org/abs/2509.23933", "authors": ["Jiahao Ying", "Mingbao Lin", "Qianru Sun", "Yixin Cao"], "title": "Beyond Benchmarks: Understanding Mixture-of-Experts Models through Internal Mechanisms", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures have emerged as a promising direction,\noffering efficiency and scalability by activating only a subset of parameters\nduring inference. However, current research remains largely\nperformance-centric, with limited understanding of its internal mechanisms,\nthereby constraining broader progress. In this work, we use an internal metric\nto investigate the mechanisms of MoE architecture by explicitly incorporating\nrouting mechanisms and analyzing expert-level behaviors. Through systematic\nanalyses of a wide range of publicly available MoE models, we uncover several\nfindings: (1) neuron utilization decreases as models evolve, reflecting\nstronger generalization; (2) training exhibits a dynamic trajectory, where\nbenchmark performance alone provides limited signal while MUI reveals deeper\ninsights; (3) task completion emerges from collaborative contributions of\nmultiple experts, with shared experts driving concentration; and (4) activation\npatterns at the neuron level provide a fine-grained proxy for data diversity.\nTogether, these results demonstrate the potential of MUI as a complementary\nindicator to benchmark performance, offering new insights into the capacity,\ndynamics, and specialization of MoE models. Our project can be found at\nhttps://yingjiahao14.github.io/MoE-MUI/.", "AI": {"tldr": "This paper investigates the internal mechanisms of Mixture-of-Experts (MoE) architectures using an internal metric (MUI), revealing insights about neuron utilization, training dynamics, expert collaboration, and activation patterns that complement traditional performance benchmarks.", "motivation": "Current MoE research is largely performance-centric with limited understanding of internal mechanisms, which constrains broader progress in the field. The authors aim to provide deeper insights into how MoE architectures actually work internally.", "method": "The authors use an internal metric (MUI) to investigate MoE mechanisms by explicitly incorporating routing mechanisms and analyzing expert-level behaviors across a wide range of publicly available MoE models.", "result": "Key findings include: (1) neuron utilization decreases as models evolve, indicating stronger generalization; (2) training shows dynamic trajectories where MUI reveals deeper insights than benchmark performance alone; (3) task completion involves collaborative contributions from multiple experts with shared experts driving concentration; (4) neuron-level activation patterns serve as fine-grained proxies for data diversity.", "conclusion": "MUI serves as a valuable complementary indicator to benchmark performance, offering new insights into MoE model capacity, dynamics, and specialization, demonstrating the potential of internal metrics for understanding complex neural architectures."}}
{"id": "2509.24781", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24781", "abs": "https://arxiv.org/abs/2509.24781", "authors": ["Jun Rao", "Yunjie Liao", "Xuebo Liu", "Zepeng Lin", "Lian Lian", "Dong Jin", "Shengjun Cheng", "Jun Yu", "Min Zhang"], "title": "SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models", "comment": "EMNLP 2025 Findings", "summary": "Existing alignment methods for preference optimization of large language\nmodels (LLMs) aim to enhance model performance by utilizing pairs of positive\nand negative samples. However, due to the limited capacity of models in scoring\nor generating responses, the quality of positive and negative samples may\nbecome similar during training, which complicates optimization for preference\nlearning. To address this issue, we introduce SeaPO, a Strategic Error\nAmplification method that leverages three error types commonly occurring in\nLLMs to introduce specific error patterns into the model Preference\nOptimization. This strategy ensures that negative samples are more erroneous\nthan positive samples and preference-based training is employed to mitigate the\noccurrence of these errors, thereby enhancing model performance. Evaluations\nacross five capability dimensions and different model scales (1.5B to 14B)\ndemonstrate that the generated data significantly improved overall model\nperformance, particularly in terms of truthfulness, with improvements of 5-10\npercentage points observed. Further analysis reveals that task performance\nvaries depending on the error types introduced. Injecting the most common error\ntypes improves performance in related tasks, while a mix of error types leads\nto a broader performance enhancement: most tasks show stable improvements,\nwhile a few tasks exhibit significant gains.", "AI": {"tldr": "SeaPO introduces Strategic Error Amplification to enhance LLM preference optimization by injecting specific error patterns into negative samples, ensuring they are more erroneous than positive samples, which improves model performance across multiple dimensions.", "motivation": "Existing alignment methods struggle when positive and negative samples become similar in quality during training, complicating preference learning optimization.", "method": "Strategic Error Amplification method that leverages three common LLM error types to introduce specific error patterns into model Preference Optimization, ensuring negative samples are more erroneous.", "result": "Evaluations across five capability dimensions and model scales (1.5B to 14B) show significant performance improvements, particularly in truthfulness (5-10 percentage points). Task performance varies with error types - common errors improve related tasks, while mixed errors provide broader enhancement.", "conclusion": "SeaPO effectively addresses the similarity issue in preference optimization by strategically amplifying errors in negative samples, leading to substantial performance improvements across multiple model scales and tasks."}}
{"id": "2509.23937", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.23937", "abs": "https://arxiv.org/abs/2509.23937", "authors": ["Akhil Premkumar"], "title": "Diffusion Models are Kelly Gamblers", "comment": "26 pages + references, 13 figures", "summary": "We draw a connection between diffusion models and the Kelly criterion for\nmaximizing returns in betting games. We find that conditional diffusion models\nstore additional information to bind the signal $X$ with the conditioning\ninformation $Y$, equal to the mutual information between them. Classifier-free\nguidance effectively boosts the mutual information between $X$ and $Y$ at\nsampling time. This is especially helpful in image models, since the mutual\ninformation between images and their labels is low, a fact which is intimately\nconnected to the manifold hypothesis. Finally, we point out some nuances in the\npopular perspective that diffusion models are infinitely deep autoencoders. In\ndoing so, we relate the denoising loss to the Fermi Golden Rule from quantum\nmechanics.", "AI": {"tldr": "This paper connects diffusion models to the Kelly criterion in betting games, revealing that conditional diffusion models store mutual information between signals and conditioning information. Classifier-free guidance boosts this mutual information during sampling, which is particularly useful for image models where image-label mutual information is low. The paper also clarifies nuances in viewing diffusion models as infinitely deep autoencoders and relates the denoising loss to the Fermi Golden Rule from quantum mechanics.", "motivation": "To establish theoretical connections between diffusion models and established concepts from information theory (Kelly criterion) and quantum mechanics (Fermi Golden Rule), providing deeper insights into how conditional diffusion models work and why classifier-free guidance is effective.", "method": "Theoretical analysis connecting diffusion models to the Kelly criterion for betting games, examining how conditional diffusion models store mutual information between signals X and conditioning information Y. Analysis of classifier-free guidance mechanisms and their effect on mutual information.", "result": "Found that conditional diffusion models store additional information equal to the mutual information between X and Y. Classifier-free guidance effectively boosts this mutual information during sampling, which is particularly beneficial for image models due to the low mutual information between images and their labels (related to the manifold hypothesis).", "conclusion": "The paper provides novel theoretical connections between diffusion models and concepts from betting theory and quantum mechanics, offering insights into the information storage mechanisms in conditional diffusion models and explaining why classifier-free guidance works effectively, especially in image generation tasks."}}
{"id": "2509.24792", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24792", "abs": "https://arxiv.org/abs/2509.24792", "authors": ["Luisa Geiger", "Mareike Hartmann", "Michael Sullivan", "Alexander Koller"], "title": "Evaluating Spatiotemporal Consistency in Automatically Generated Sewing Instructions", "comment": "18 pages, 14 figures; to be published in EMNLP 2025 proceedings", "summary": "In this paper, we propose a novel, automatic tree-based evaluation metric for\nLLM-generated step-by-step assembly instructions, that more accurately reflects\nspatiotemporal aspects of construction than traditional metrics such as BLEU\nand BERT similarity scores. We apply our proposed metric to the domain of\nsewing instructions, and show that our metric better correlates with\nmanually-annotated error counts as well as human quality ratings, demonstrating\nour metric's superiority for evaluating the spatiotemporal soundness of sewing\ninstructions. Further experiments show that our metric is more robust than\ntraditional approaches against artificially-constructed counterfactual examples\nthat are specifically constructed to confound metrics that rely on textual\nsimilarity.", "AI": {"tldr": "Proposes a novel tree-based evaluation metric for LLM-generated assembly instructions that better captures spatiotemporal aspects than traditional text similarity metrics like BLEU and BERT.", "motivation": "Traditional metrics like BLEU and BERT similarity scores fail to adequately reflect spatiotemporal aspects of construction in step-by-step assembly instructions, necessitating a more accurate evaluation approach.", "method": "Developed an automatic tree-based evaluation metric specifically designed for LLM-generated step-by-step assembly instructions, focusing on spatiotemporal soundness rather than just textual similarity.", "result": "The proposed metric shows better correlation with manually-annotated error counts and human quality ratings in sewing instructions, and demonstrates greater robustness against artificially-constructed counterfactual examples designed to confuse text-based metrics.", "conclusion": "The tree-based metric is superior to traditional approaches for evaluating the spatiotemporal soundness of assembly instructions, providing more accurate and robust assessment of instruction quality."}}
{"id": "2509.23941", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2509.23941", "abs": "https://arxiv.org/abs/2509.23941", "authors": ["Victoria Bosch", "Daniel Anthes", "Adrien Doerig", "Sushrut Thorat", "Peter K\u00f6nig", "Tim Christian Kietzmann"], "title": "Brain-language fusion enables interactive neural readout and in-silico experimentation", "comment": null, "summary": "Large language models (LLMs) have revolutionized human-machine interaction,\nand have been extended by embedding diverse modalities such as images into a\nshared language space. Yet, neural decoding has remained constrained by static,\nnon-interactive methods. We introduce CorText, a framework that integrates\nneural activity directly into the latent space of an LLM, enabling open-ended,\nnatural language interaction with brain data. Trained on fMRI data recorded\nduring viewing of natural scenes, CorText generates accurate image captions and\ncan answer more detailed questions better than controls, while having access to\nneural data only. We showcase that CorText achieves zero-shot generalization\nbeyond semantic categories seen during training. Furthermore, we present a\ncounterfactual analysis that emulates in-silico cortical microstimulation.\nThese advances mark a shift from passive decoding toward generative, flexible\ninterfaces between brain activity and language.", "AI": {"tldr": "CorText is a framework that integrates neural activity into LLM's latent space, enabling natural language interaction with brain data through fMRI recordings during natural scene viewing.", "motivation": "Current neural decoding methods are constrained by static, non-interactive approaches, limiting the potential for dynamic brain-computer interfaces.", "method": "Integrates fMRI data recorded during natural scene viewing directly into LLM's latent space, enabling open-ended natural language interaction with neural data.", "result": "Generates accurate image captions, answers detailed questions better than controls using only neural data, and achieves zero-shot generalization beyond training categories.", "conclusion": "Represents a shift from passive decoding to generative, flexible brain-language interfaces, demonstrated through counterfactual analysis simulating cortical microstimulation."}}
{"id": "2509.24816", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24816", "abs": "https://arxiv.org/abs/2509.24816", "authors": ["Xilin Dang", "Kexin Chen", "Xiaorui Su", "Ayush Noori", "I\u00f1aki Arango", "Lucas Vittor", "Xinyi Long", "Yuyang Du", "Marinka Zitnik", "Pheng Ann Heng"], "title": "KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning", "comment": null, "summary": "In clinical practice, physicians refrain from making decisions when patient\ninformation is insufficient. This behavior, known as abstention, is a critical\nsafety mechanism preventing potentially harmful misdiagnoses. Recent\ninvestigations have reported the application of large language models (LLMs) in\nmedical scenarios. However, existing LLMs struggle with the abstentions,\nfrequently providing overconfident responses despite incomplete information.\nThis limitation stems from conventional abstention methods relying solely on\nmodel self-assessments, which lack systematic strategies to identify knowledge\nboundaries with external medical evidences. To address this, we propose\n\\textbf{KnowGuard}, a novel \\textit{investigate-before-abstain} paradigm that\nintegrates systematic knowledge graph exploration for clinical decision-making.\nOur approach consists of two key stages operating on a shared contextualized\nevidence pool: 1) an evidence discovery stage that systematically explores the\nmedical knowledge space through graph expansion and direct retrieval, and 2) an\nevidence evaluation stage that ranks evidence using multiple factors to adapt\nexploration based on patient context and conversation history. This two-stage\napproach enables systematic knowledge graph exploration, allowing models to\ntrace structured reasoning paths and recognize insufficient medical evidence.\nWe evaluate our abstention approach using open-ended multi-round clinical\nbenchmarks that mimic realistic diagnostic scenarios, assessing abstention\nquality through accuracy-efficiency trade-offs beyond existing closed-form\nevaluations. Experimental evidences clearly demonstrate that KnowGuard\noutperforms state-of-the-art abstention approaches, improving diagnostic\naccuracy by 3.93\\% while reducing unnecessary interaction by 7.27 turns on\naverage.", "AI": {"tldr": "KnowGuard introduces a novel investigate-before-abstain paradigm that uses systematic knowledge graph exploration to improve LLM abstention in clinical decision-making, outperforming state-of-the-art methods.", "motivation": "Current LLMs struggle with abstention in medical scenarios, providing overconfident responses despite incomplete information, which poses safety risks in clinical practice.", "method": "A two-stage approach: 1) evidence discovery through graph expansion and direct retrieval, and 2) evidence evaluation using multiple factors to adapt exploration based on patient context and conversation history.", "result": "KnowGuard improves diagnostic accuracy by 3.93% while reducing unnecessary interaction by 7.27 turns on average compared to state-of-the-art abstention approaches.", "conclusion": "The investigate-before-abstain paradigm with systematic knowledge graph exploration effectively enables models to recognize insufficient medical evidence and make appropriate abstentions in clinical decision-making."}}
{"id": "2509.23942", "categories": ["cs.LG", "cs.DB", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.23942", "abs": "https://arxiv.org/abs/2509.23942", "authors": ["John N. Daras"], "title": "Efficient Identification of High Similarity Clusters in Polygon Datasets", "comment": "11 pages, 3 figures", "summary": "Advancements in tools like Shapely 2.0 and Triton can significantly improve\nthe efficiency of spatial similarity computations by enabling faster and more\nscalable geometric operations. However, for extremely large datasets, these\noptimizations may face challenges due to the sheer volume of computations\nrequired. To address this, we propose a framework that reduces the number of\nclusters requiring verification, thereby decreasing the computational load on\nthese systems. The framework integrates dynamic similarity index thresholding,\nsupervised scheduling, and recall-constrained optimization to efficiently\nidentify clusters with the highest spatial similarity while meeting\nuser-defined precision and recall requirements. By leveraging Kernel Density\nEstimation (KDE) to dynamically determine similarity thresholds and machine\nlearning models to prioritize clusters, our approach achieves substantial\nreductions in computational cost without sacrificing accuracy. Experimental\nresults demonstrate the scalability and effectiveness of the method, offering a\npractical solution for large-scale geospatial analysis.", "AI": {"tldr": "A framework that reduces computational load for large-scale spatial similarity computations by integrating dynamic thresholding, supervised scheduling, and recall-constrained optimization.", "motivation": "Existing tools like Shapely 2.0 and Triton improve efficiency but still face challenges with extremely large datasets due to high computational volume.", "method": "Uses Kernel Density Estimation (KDE) for dynamic similarity thresholding, machine learning models for cluster prioritization, and recall-constrained optimization to reduce verification clusters.", "result": "Achieves substantial reductions in computational cost without sacrificing accuracy, demonstrating scalability and effectiveness in experiments.", "conclusion": "Provides a practical solution for large-scale geospatial analysis by efficiently identifying high spatial similarity clusters while meeting user-defined precision and recall requirements."}}
{"id": "2509.24821", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24821", "abs": "https://arxiv.org/abs/2509.24821", "authors": ["Rui Jia", "Yuang Wei", "Ruijia Li", "Yuang-Hao Jiang", "Xinyu Xie", "Yaomin Shen", "Min Zhang", "Bo Jiang"], "title": "DiaCDM: Cognitive Diagnosis in Teacher-Student Dialogues using the Initiation-Response-Evaluation Framework", "comment": null, "summary": "While cognitive diagnosis (CD) effectively assesses students' knowledge\nmastery from structured test data, applying it to real-world teacher-student\ndialogues presents two fundamental challenges. Traditional CD models lack a\nsuitable framework for handling dynamic, unstructured dialogues, and it's\ndifficult to accurately extract diagnostic semantics from lengthy dialogues. To\novercome these hurdles, we propose DiaCDM, an innovative model. We've adapted\nthe initiation-response-evaluation (IRE) framework from educational theory to\ndesign a diagnostic framework tailored for dialogue. We also developed a unique\ngraph-based encoding method that integrates teacher questions with relevant\nknowledge components to capture key information more precisely. To our\nknowledge, this is the first exploration of cognitive diagnosis in a dialogue\nsetting. Experiments on three real-world dialogue datasets confirm that DiaCDM\nnot only significantly improves diagnostic accuracy but also enhances the\nresults' interpretability, providing teachers with a powerful tool for\nassessing students' cognitive states. The code is available at\nhttps://github.com/Mind-Lab-ECNU/DiaCDM/tree/main.", "AI": {"tldr": "DiaCDM is a novel cognitive diagnosis model for teacher-student dialogues that adapts the IRE framework and uses graph-based encoding to improve diagnostic accuracy and interpretability.", "motivation": "Traditional cognitive diagnosis models are unsuitable for dynamic, unstructured dialogues and struggle to extract diagnostic semantics from lengthy conversations.", "method": "Adapted initiation-response-evaluation (IRE) framework for dialogue diagnosis and developed graph-based encoding that integrates teacher questions with knowledge components.", "result": "Experiments on three real-world dialogue datasets show significant improvements in diagnostic accuracy and enhanced interpretability of results.", "conclusion": "DiaCDM provides teachers with a powerful tool for assessing students' cognitive states in dialogue settings, representing the first exploration of cognitive diagnosis in this context."}}
{"id": "2509.23946", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23946", "abs": "https://arxiv.org/abs/2509.23946", "authors": ["Kaisen Yang", "Lixuan He", "Rushi Shah", "Kaicheng Yang", "Qinwei Ma", "Dianbo Liu", "Alex Lamb"], "title": "Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm", "comment": "Under review ICLR 2026", "summary": "Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning\nabilities of Large Language Models (LLMs), yet their monolithic and\nauto-regressive architecture inherently conflates high-level strategic planning\nwith low-level step-by-step execution, leading to computational inefficiency,\nlimited exploration of reasoning paths, and reduced interpretability. To\novercome these issues, we propose the Explore-Execute Chain ($E^2C$), a\nstructured reasoning framework that decouples reasoning into two distinct\nphases: an exploratory phase that stochastically generates succinct high-level\nplans, followed by an execution phase that deterministically carries out the\nchosen plan. Our approach incorporates a two-stage training methodology, which\ncombines Supervised Fine-Tuning (SFT) - augmented by a novel data generation\nalgorithm enforcing strict plan adherence - with a subsequent Reinforcement\nLearning (RL) stage that capitalizes on the informativeness of exploration and\nreinforces the determinism of execution.This decomposition enables an efficient\ntest-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches\n58.1% accuracy using <10% of the decoding tokens required by comparable methods\n(e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For\ncross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with\nonly 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher\naccuracy than standard SFT on medical benchmarks, delivering state-of-the-art\nperformance, strong generalization, and greater interpretability by separating\nplanning from execution. The code and pre-trained models for the project are\navailable at: https://github.com/yks23/Explore-Execute-Chain.git", "AI": {"tldr": "E^2C is a structured reasoning framework that decouples reasoning into exploratory planning and deterministic execution phases, achieving computational efficiency and improved performance with fewer tokens.", "motivation": "Current Chain-of-Thought methods conflate planning and execution, leading to computational inefficiency, limited path exploration, and reduced interpretability.", "method": "Two-phase framework: exploratory phase generates high-level plans stochastically, execution phase deterministically carries out plans. Uses two-stage training with SFT (augmented by strict plan adherence) and RL.", "result": "Achieves 58.1% accuracy on AIME'2024 using <10% of decoding tokens compared to similar methods. EF-SFT fine-tuning uses only 3.5% of tokens yet yields up to 14.5% higher accuracy on medical benchmarks.", "conclusion": "E^2C provides state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution, while sharply reducing computational overhead."}}
{"id": "2509.24832", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24832", "abs": "https://arxiv.org/abs/2509.24832", "authors": ["Xinye Zhao", "Spyridon Mastorakis"], "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching", "comment": "11 figures, 14pages", "summary": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference.", "AI": {"tldr": "SemShareKV is a KV cache sharing framework that accelerates LLM inference by reusing key-value caches from semantically similar prompts using fuzzy token matching with LSH and RoPE, achieving up to 6.25\u00d7 speedup and 42% memory reduction.", "motivation": "The memory footprint of KV caches during LLM inference has become a bottleneck. Existing approaches are limited in handling semantically similar but lexically different prompts, which are common in tasks like multi-document summarization and conversational agents.", "method": "Proposes SemShareKV framework that uses locality-sensitive hashing (LSH) on token embeddings for fuzzy token matching and incorporates Rotary Position Embedding (RoPE) to preserve positional information. It selectively reuses relevant key-value pairs from reference prompts' caches.", "result": "Experiments on summarization datasets show up to 6.25\u00d7 speedup and 42% lower GPU memory usage with 5k tokens input, with negligible quality degradation.", "conclusion": "Semantic-aware cache sharing shows significant potential for efficient LLM inference, particularly in scenarios with semantically similar but lexically diverse prompts."}}
{"id": "2509.23948", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23948", "abs": "https://arxiv.org/abs/2509.23948", "authors": ["Surya Murthy", "Kushagra Gupta", "Mustafa O. Karabag", "David Fridovich-Keil", "Ufuk Topcu"], "title": "DiBS-MTL: Transformation-Invariant Multitask Learning with Direction Oracles", "comment": null, "summary": "Multitask learning (MTL) algorithms typically rely on schemes that combine\ndifferent task losses or their gradients through weighted averaging. These\nmethods aim to find Pareto stationary points by using heuristics that require\naccess to task loss values, gradients, or both. In doing so, a central\nchallenge arises because task losses can be arbitrarily, nonaffinely scaled\nrelative to one another, causing certain tasks to dominate training and degrade\noverall performance. A recent advance in cooperative bargaining theory, the\nDirection-based Bargaining Solution (DiBS), yields Pareto stationary solutions\nimmune to task domination because of its invariance to monotonic nonaffine task\nloss transformations. However, the convergence behavior of DiBS in nonconvex\nMTL settings is currently not understood. To this end, we prove that under\nstandard assumptions, a subsequence of DiBS iterates converges to a Pareto\nstationary point when task losses are possibly nonconvex, and propose DiBS-MTL,\na computationally efficient adaptation of DiBS to the MTL setting. Finally, we\nvalidate DiBS-MTL empirically on standard MTL benchmarks, showing that it\nachieves competitive performance with state-of-the-art methods while\nmaintaining robustness to nonaffine monotonic transformations that\nsignificantly degrade the performance of existing approaches, including prior\nbargaining-inspired MTL methods. Code available at\nhttps://github.com/suryakmurthy/dibs-mtl.", "AI": {"tldr": "DiBS-MTL is a multitask learning method based on cooperative bargaining theory that achieves Pareto stationary solutions immune to task domination by being invariant to monotonic nonaffine task loss transformations.", "motivation": "Existing multitask learning methods suffer from task domination issues when task losses are arbitrarily scaled, which degrades overall performance. Current approaches are not robust to nonaffine monotonic transformations of task losses.", "method": "Proposed DiBS-MTL, a computationally efficient adaptation of the Direction-based Bargaining Solution (DiBS) from cooperative bargaining theory to the MTL setting. It provides convergence guarantees to Pareto stationary points even for nonconvex task losses.", "result": "Empirical validation on standard MTL benchmarks shows DiBS-MTL achieves competitive performance with state-of-the-art methods while maintaining robustness to nonaffine monotonic transformations that degrade existing approaches.", "conclusion": "DiBS-MTL provides a theoretically grounded solution to task domination in multitask learning, with proven convergence properties and empirical effectiveness that outperforms existing bargaining-inspired methods."}}
{"id": "2509.24841", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.24841", "abs": "https://arxiv.org/abs/2509.24841", "authors": ["Zhilong Zhao", "Yindi Liu"], "title": "Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement", "comment": "10 pages, 4 figures, 4 tables", "summary": "Large Language Models face significant performance challenges in specialized\ndomains, with state-of-the-art models achieving only 45.9% accuracy on medical\ncoding tasks. This study proposes a Hierarchical Error Correction (HEC)\nframework that addresses domain-specific AI limitations through systematic\nerror analysis and targeted intervention strategies.\n  We analyze error patterns across four specialized domains and find that AI\nerrors follow consistent hierarchical structures: Knowledge-layer errors\n(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).\nBased on these patterns, we develop a three-stage correction framework that\naddresses errors according to their hierarchical importance and demonstrates\nthat framework effectiveness correlates inversely with baseline task\nperformance.\n  Experimental validation across medical transcription (4,921 cases), legal\ndocument classification (1,000 cases), political bias detection (645 cases),\nand legal reasoning (1,000 cases) shows consistent improvements. Cross-model\nvalidation across five LLM architectures demonstrates average improvements of\n11.2 percentage points (p < 0.001). However, analysis reveals framework\nlimitations in high-baseline tasks (>75% accuracy), where hierarchical\nintervention may interfere with effective reasoning processes.\n  The results suggest that systematic error analysis can guide effective AI\nenhancement strategies in specialized domains, particularly for\nmoderate-baseline tasks, while highlighting the importance of understanding\nframework boundaries for optimal deployment.", "AI": {"tldr": "The paper proposes a Hierarchical Error Correction (HEC) framework that systematically analyzes and corrects AI errors in specialized domains, achieving 11.2 percentage point average improvement across various tasks.", "motivation": "Large Language Models perform poorly in specialized domains (only 45.9% accuracy on medical coding), necessitating targeted error correction strategies.", "method": "Three-stage hierarchical correction framework addressing Knowledge-layer (58.4%), Reasoning-layer (39.6%), and Complexity-layer (2.0%) errors based on systematic error pattern analysis.", "result": "Validated across 4 domains (medical, legal, political, legal reasoning) with 11.2% average improvement across 5 LLM architectures (p < 0.001), but limited effectiveness in high-baseline tasks (>75% accuracy).", "conclusion": "Systematic error analysis enables effective AI enhancement in specialized domains, particularly for moderate-baseline tasks, while understanding framework boundaries is crucial for optimal deployment."}}
{"id": "2509.23963", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23963", "abs": "https://arxiv.org/abs/2509.23963", "authors": ["Rylan Schaeffer", "Noam Levi", "Andreas Kirsch", "Theo Guenais", "Brando Miranda", "Elyas Obbad", "Sanmi Koyejo"], "title": "Evaluating the Robustness of Chinchilla Compute-Optimal Scaling", "comment": null, "summary": "Hoffman et al (2022)'s Chinchilla paper introduced the principle of\ncompute-optimal scaling, laying a foundation for future scaling of language\nmodels. In the years since, however, valid concerns about Chinchilla have been\nraised: wide confidence intervals, discrepancies between its three approaches,\nand incongruities with other scaling laws. This raises a critical question for\nthe field: Can practitioners still rely on Chinchilla's prescriptions? Our work\ndemonstrates the answer is yes. We begin by uncovering that the model\nparameters central to Chinchilla's analyses were ambiguous: three\ninterpretations are possible, with relative differences between different\ninterpretations of model parameters as high as 15.2%. We find that, perhaps\nsurprisingly, which model parameters are used for the analyses do not\nmeaningfully affect key results: the scaling law estimates and the\ncompute-optimal tokens-to-parameter ratio. Indeed, under one interpretation,\nthe tokens-to-parameter ratio becomes more constant with the target compute\nbudget. We then ask how distorted the Chinchilla model parameters could have\nbeen without meaningfully affecting the key results. By deliberately perturbing\nmodel parameters in four structured ways, we find that key Chinchilla results\nare most sensitive to additive or systematic errors, which can alter the\notherwise flat trend of the optimal tokens-to-parameter ratio, but overall,\nChinchilla's key results withstand sizable perturbations. Altogether, our\nfindings offer the field renewed confidence in Chinchilla as a durable guide\nfor scaling language models.", "AI": {"tldr": "This paper validates Chinchilla's compute-optimal scaling laws despite concerns about parameter ambiguities, showing that different parameter interpretations don't meaningfully affect key results and the scaling laws withstand significant perturbations.", "motivation": "To address concerns about Chinchilla's scaling laws - including wide confidence intervals, discrepancies between approaches, and incongruities with other scaling laws - and determine if practitioners can still rely on Chinchilla's prescriptions for language model scaling.", "method": "Analyzed three possible interpretations of Chinchilla's model parameters, then deliberately perturbed model parameters in four structured ways to test the robustness of key results, particularly the compute-optimal tokens-to-parameter ratio.", "result": "Found that different parameter interpretations (with up to 15.2% differences) don't meaningfully affect scaling law estimates or the optimal tokens-to-parameter ratio. Under one interpretation, the ratio becomes more constant with compute budget. Key results withstand sizable perturbations, being most sensitive only to additive/systematic errors.", "conclusion": "Chinchilla's compute-optimal scaling laws remain valid and provide a durable guide for scaling language models, offering renewed confidence to the field despite previous concerns."}}
{"id": "2509.24857", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.24857", "abs": "https://arxiv.org/abs/2509.24857", "authors": ["Adrian Arnaiz-Rodriguez", "Miguel Baidal", "Erik Derner", "Jenn Layton Annable", "Mark Ball", "Mark Ince", "Elvira Perez Vallejos", "Nuria Oliver"], "title": "Between Help and Harm: An Evaluation of Mental Health Crisis Handling by LLMs", "comment": null, "summary": "The widespread use of chatbots powered by large language models (LLMs) such\nas ChatGPT and Llama has fundamentally reshaped how people seek information and\nadvice across domains. Increasingly, these chatbots are being used in\nhigh-stakes contexts, including emotional support and mental health concerns.\nWhile LLMs can offer scalable support, their ability to safely detect and\nrespond to acute mental health crises remains poorly understood. Progress is\nhampered by the absence of unified crisis taxonomies, robust annotated\nbenchmarks, and empirical evaluations grounded in clinical best practices. In\nthis work, we address these gaps by introducing a unified taxonomy of six\nclinically-informed mental health crisis categories, curating a diverse\nevaluation dataset, and establishing an expert-designed protocol for assessing\nresponse appropriateness. We systematically benchmark three state-of-the-art\nLLMs for their ability to classify crisis types and generate safe, appropriate\nresponses. The results reveal that while LLMs are highly consistent and\ngenerally reliable in addressing explicit crisis disclosures, significant risks\nremain. A non-negligible proportion of responses are rated as inappropriate or\nharmful, with responses generated by an open-weight model exhibiting higher\nfailure rates than those generated by the commercial ones. We also identify\nsystemic weaknesses in handling indirect or ambiguous risk signals, a reliance\non formulaic and inauthentic default replies, and frequent misalignment with\nuser context. These findings underscore the urgent need for enhanced\nsafeguards, improved crisis detection, and context-aware interventions in LLM\ndeployments. Our taxonomy, datasets, and evaluation framework lay the\ngroundwork for ongoing research and responsible innovation in AI-driven mental\nhealth support, helping to minimize harm and better protect vulnerable users.", "AI": {"tldr": "This paper evaluates LLMs' ability to handle mental health crises, finding they're generally reliable for explicit disclosures but have significant risks including inappropriate responses, poor handling of indirect signals, and formulaic replies.", "motivation": "LLMs are increasingly used in high-stakes mental health contexts, but their crisis detection and response capabilities remain poorly understood due to lack of unified taxonomies, benchmarks, and clinical evaluations.", "method": "Created unified taxonomy of 6 mental health crisis categories, curated evaluation dataset, established expert-designed assessment protocol, and systematically benchmarked 3 state-of-the-art LLMs for crisis classification and response generation.", "result": "LLMs show high consistency and reliability for explicit crisis disclosures, but significant risks remain: non-negligible inappropriate/harmful responses (worse in open-weight models), poor handling of indirect/ambiguous signals, formulaic replies, and context misalignment.", "conclusion": "Urgent need for enhanced safeguards, improved crisis detection, and context-aware interventions in LLM deployments. The taxonomy, datasets, and framework provide groundwork for responsible AI mental health support innovation."}}
{"id": "2509.23964", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23964", "abs": "https://arxiv.org/abs/2509.23964", "authors": ["Dang Huu-Tien", "Naoya Inoue"], "title": "Detecting and Rectifying Noisy Labels: A Similarity-based Approach", "comment": null, "summary": "Label noise in datasets could damage the performance of neural net training.\nAs the size of modern deep networks grows, there is a growing demand for\nautomated tools for detecting such errors. In this paper, we propose post-hoc,\nmodel-agnostic error detection and rectification methods utilizing the\npenultimate feature from a neural network. Our idea is based on the observation\nthat the similarity between the penultimate feature of a mislabeled data point\nand its true class data points is higher than that for data points from other\nclasses, making the probability of label occurrence within a tight, similar\ncluster informative for detecting and rectifying errors. Extensive experiments\nshow our method not only demonstrates high performance across various noises\nbut also automatically rectifies these errors to improve the quality of\ndatasets.", "AI": {"tldr": "Proposes post-hoc, model-agnostic methods using penultimate features to detect and rectify label errors in datasets by leveraging feature similarity patterns.", "motivation": "Label noise damages neural network performance, and with growing network sizes, there's increasing need for automated error detection tools.", "method": "Utilizes penultimate features from neural networks, observing that mislabeled data points have higher similarity to true class features than other classes, using probability of label occurrence in similar clusters for error detection and rectification.", "result": "Extensive experiments show high performance across various noise types and successful automatic rectification of errors to improve dataset quality.", "conclusion": "The proposed method effectively detects and rectifies label errors using penultimate feature similarities, demonstrating robust performance across different noise scenarios."}}
{"id": "2509.24866", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24866", "abs": "https://arxiv.org/abs/2509.24866", "authors": ["Matteo Fuoli", "Weihang Huang", "Jeannette Littlemore", "Sarah Turner", "Ellen Wilding"], "title": "Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning", "comment": null, "summary": "Metaphor is a pervasive feature of discourse and a powerful lens for\nexamining cognition, emotion, and ideology. Large-scale analysis, however, has\nbeen constrained by the need for manual annotation due to the context-sensitive\nnature of metaphor. This study investigates the potential of large language\nmodels (LLMs) to automate metaphor identification in full texts. We compare\nthree methods: (i) retrieval-augmented generation (RAG), where the model is\nprovided with a codebook and instructed to annotate texts based on its rules\nand examples; (ii) prompt engineering, where we design task-specific verbal\ninstructions; and (iii) fine-tuning, where the model is trained on hand-coded\ntexts to optimize performance. Within prompt engineering, we test zero-shot,\nfew-shot, and chain-of-thought strategies. Our results show that\nstate-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning\nyielding a median F1 score of 0.79. A comparison of human and LLM outputs\nreveals that most discrepancies are systematic, reflecting well-known grey\nareas and conceptual challenges in metaphor theory. We propose that LLMs can be\nused to at least partly automate metaphor identification and can serve as a\ntestbed for developing and refining metaphor identification protocols and the\ntheory that underpins them.", "AI": {"tldr": "This study explores using large language models (LLMs) to automate metaphor identification in texts, comparing RAG, prompt engineering, and fine-tuning methods, with fine-tuning achieving the best performance (F1=0.79).", "motivation": "Metaphor analysis is important for understanding cognition and discourse, but manual annotation is time-consuming and context-sensitive, creating a need for automated solutions.", "method": "Compared three approaches: retrieval-augmented generation (RAG), prompt engineering (zero-shot, few-shot, chain-of-thought), and fine-tuning on hand-coded texts.", "result": "Fine-tuned LLMs achieved highest accuracy with median F1 score of 0.79. Discrepancies between human and LLM annotations were systematic and reflected known theoretical challenges in metaphor identification.", "conclusion": "LLMs can partially automate metaphor identification and serve as testbeds for developing and refining metaphor identification protocols and underlying theory."}}
{"id": "2509.23976", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23976", "abs": "https://arxiv.org/abs/2509.23976", "authors": ["Maruf Ahmed Mridul", "Oshani Seneviratne"], "title": "Curriculum-Guided Reinforcement Learning for Synthesizing Gas-Efficient Financial Derivatives Contracts", "comment": "8 pages, 3 figures, 2 tables", "summary": "Smart contract-based automation of financial derivatives offers substantial\nefficiency gains, but its real-world adoption is constrained by the complexity\nof translating financial specifications into gas-efficient executable code. In\nparticular, generating code that is both functionally correct and economically\nviable from high-level specifications, such as the Common Domain Model (CDM),\nremains a significant challenge. This paper introduces a Reinforcement Learning\n(RL) framework to generate functional and gas-optimized Solidity smart\ncontracts directly from CDM specifications. We employ a Proximal Policy\nOptimization (PPO) agent that learns to select optimal code snippets from a\npre-defined library. To manage the complex search space, a two-phase curriculum\nfirst trains the agent for functional correctness before shifting its focus to\ngas optimization. Our empirical results show the RL agent learns to generate\ncontracts with significant gas savings, achieving cost reductions of up to\n35.59% on unseen test data compared to unoptimized baselines. This work\npresents a viable methodology for the automated synthesis of reliable and\neconomically sustainable smart contracts, bridging the gap between high-level\nfinancial agreements and efficient on-chain execution.", "AI": {"tldr": "A Reinforcement Learning framework generates gas-optimized Solidity smart contracts from CDM specifications, achieving up to 35.59% gas savings.", "motivation": "Smart contract automation faces challenges in translating financial specifications into gas-efficient code, particularly from high-level models like CDM.", "method": "Uses Proximal Policy Optimization (PPO) agent with two-phase curriculum: first learns functional correctness, then focuses on gas optimization by selecting optimal code snippets from a library.", "result": "RL agent generates contracts with significant gas savings - up to 35.59% cost reduction on unseen test data compared to unoptimized baselines.", "conclusion": "Presents a viable methodology for automated synthesis of reliable and economically sustainable smart contracts, bridging high-level financial agreements with efficient on-chain execution."}}
{"id": "2509.24884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24884", "abs": "https://arxiv.org/abs/2509.24884", "authors": ["Yoonna Jang", "Kisu Yang", "Isabelle Augenstein"], "title": "Expanding Computation Spaces of LLMs at Inference Time", "comment": null, "summary": "Chain-of-thought (CoT) rationale enables language models to use additional\ntask-related text for problem-solving, benefiting not only from detailed\nreasoning steps but also from the expanded computational space of longer\ninputs. Prior work has trained filler or special tokens to serve as additional\ncomputation spaces. In this study, we investigate whether language models can\nleverage artificially inserted sequences of filler tokens solely at inference.\nWe first identify effective token types, numbers, and insertion locations, then\nexamine at what stage of training models begin to exploit the expanded\ncomputation space, and finally analyze dynamics within these spaces via\nattention maps. Experiments on models ranging from 1.7B to 32B across\nopen-domain QA and math tasks show that appropriate token types and counts\nvary, but placing filler tokens directly before the final 'Answer:' token is\nmost effective. Smaller models benefit most, up to 12.372 percentage points in\nSmolLM2-1.7B-Instruct, indicating that these spaces act as additional\ncomputational capacity rather than redundant input. Attention maps reveal that\nexpanded spaces often continue the original attention mechanism and sometimes\nfocus on questions or answer options, suggesting meaningful computation for\nproblem-solving.", "AI": {"tldr": "Language models can use artificially inserted filler tokens at inference time to expand computational space, improving performance especially for smaller models by up to 12.4 percentage points.", "motivation": "To investigate whether language models can leverage additional computational space through filler tokens inserted solely at inference, rather than requiring training of special tokens.", "method": "Systematically tested different token types, numbers, and insertion locations; analyzed when during training models learn to use these spaces; examined attention dynamics in expanded spaces.", "result": "Appropriate token types and counts vary, but placing filler tokens before the final 'Answer:' token works best. Smaller models benefit most (up to 12.4% improvement), showing these spaces provide genuine computational capacity.", "conclusion": "Filler tokens at inference act as meaningful computational capacity, with attention patterns showing continued reasoning processes and focus on relevant problem elements."}}
{"id": "2509.23992", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23992", "abs": "https://arxiv.org/abs/2509.23992", "authors": ["Amartya Roy", "Devharish N", "Shreya Ganguly", "Kripabandhu Ghosh"], "title": "Guide: Generalized-Prior and Data Encoders for DAG Estimation", "comment": null, "summary": "Modern causal discovery methods face critical limitations in scalability,\ncomputational efficiency, and adaptability to mixed data types, as evidenced by\nbenchmarks on node scalability (30, $\\le 50$, $\\ge 70$ nodes), computational\nenergy demands, and continuous/non-continuous data handling. While traditional\nalgorithms like PC, GES, and ICA-LiNGAM struggle with these challenges,\nexhibiting prohibitive energy costs for higher-order nodes and poor scalability\nbeyond 70 nodes, we propose \\textbf{GUIDE}, a framework that integrates Large\nLanguage Model (LLM)-generated adjacency matrices with observational data\nthrough a dual-encoder architecture. GUIDE uniquely optimizes computational\nefficiency, reducing runtime on average by $\\approx 42%$ compared to RL-BIC and\nKCRL methods, while achieving an average $\\approx 117%$ improvement in accuracy\nover both NOTEARS and GraN-DAG individually. During training, GUIDE's\nreinforcement learning agent dynamically balances reward maximization\n(accuracy) and penalty avoidance (DAG constraints), enabling robust performance\nacross mixed data types and scalability to $\\ge 70$ nodes -- a setting where\nbaseline methods fail.", "AI": {"tldr": "GUIDE is a causal discovery framework that combines LLM-generated adjacency matrices with observational data using dual-encoder architecture, achieving 42% faster runtime and 117% higher accuracy than baselines while scaling to \u226570 nodes.", "motivation": "Existing causal discovery methods like PC, GES, and ICA-LiNGAM face limitations in scalability, computational efficiency, and handling mixed data types, especially beyond 70 nodes with high energy costs.", "method": "Integrates LLM-generated adjacency matrices with observational data through dual-encoder architecture, using reinforcement learning to balance accuracy rewards and DAG constraint penalties.", "result": "Reduces runtime by \u224842% compared to RL-BIC and KCRL, achieves \u2248117% accuracy improvement over NOTEARS and GraN-DAG individually, and scales to \u226570 nodes where baseline methods fail.", "conclusion": "GUIDE provides an efficient and scalable solution for causal discovery that overcomes limitations of traditional methods in handling mixed data types and large-scale problems."}}
{"id": "2509.24908", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24908", "abs": "https://arxiv.org/abs/2509.24908", "authors": ["Andr\u00e9s Fern\u00e1ndez Garc\u00eda", "Javier de la Rosa", "Julio Gonzalo", "Roser Morante", "Enrique Amig\u00f3", "Alejandro Benito-Santos", "Jorge Carrillo-de-Albornoz", "V\u00edctor Fresno", "Adrian Ghajari", "Guillermo Marco", "Laura Plaza", "Eva S\u00e1nchez Salido"], "title": "BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees and Notifications", "comment": "Published in SEPLN 2025. 20 pages, 4 figures", "summary": "The ability to summarize long documents succinctly is increasingly important\nin daily life due to information overload, yet there is a notable lack of such\nsummaries for Spanish documents in general, and in the legal domain in\nparticular. In this work, we present BOE-XSUM, a curated dataset comprising\n3,648 concise, plain-language summaries of documents sourced from Spain's\n``Bolet\\'{\\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each\nentry in the dataset includes a short summary, the original text, and its\ndocument type label. We evaluate the performance of medium-sized large language\nmodels (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose\ngenerative models in a zero-shot setting. Results show that fine-tuned models\nsignificantly outperform their non-specialized counterparts. Notably, the\nbest-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\\%\nperformance gain over the top zero-shot model, DeepSeek-R1 (accuracies of\n41.6\\% vs.\\ 33.5\\%).", "AI": {"tldr": "BOE-XSUM is a new Spanish legal document summarization dataset with 3,648 entries from Spain's Official Gazette. Fine-tuned LLMs significantly outperform zero-shot models, with BERTIN GPT-J 6B achieving 41.6% accuracy vs 33.5% for the best zero-shot model.", "motivation": "There is a lack of concise Spanish document summaries, especially in the legal domain, despite growing information overload needs.", "method": "Created BOE-XSUM dataset with summaries of Spanish Official Gazette documents, then fine-tuned medium-sized LLMs and compared them to zero-shot general-purpose models.", "result": "Fine-tuned models significantly outperformed zero-shot models. BERTIN GPT-J 6B achieved 41.6% accuracy, a 24% improvement over the best zero-shot model (DeepSeek-R1 at 33.5%).", "conclusion": "Fine-tuning specialized models on domain-specific datasets like BOE-XSUM yields substantial performance gains for Spanish legal document summarization."}}
{"id": "2509.24005", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24005", "abs": "https://arxiv.org/abs/2509.24005", "authors": ["Chenruo Liu", "Yijun Dong", "Qi Lei"], "title": "Does Weak-to-strong Generalization Happen under Spurious Correlations?", "comment": null, "summary": "We initiate a unified theoretical and algorithmic study of a key problem in\nweak-to-strong (W2S) generalization: when fine-tuning a strong pre-trained\nstudent with pseudolabels from a weaker teacher on a downstream task with\nspurious correlations, does W2S happen, and how to improve it upon failures? We\nconsider two sources of spurious correlations caused by group imbalance: (i) a\nweak teacher fine-tuned on group-imbalanced labeled data with a minority group\nof fraction $\\eta_\\ell$, and (ii) a group-imbalanced unlabeled set\npseudolabeled by the teacher with a minority group of fraction $\\eta_u$.\nTheoretically, a precise characterization of W2S gain at the proportional\nasymptotic limit shows that W2S always happens with sufficient pseudolabels\nwhen $\\eta_u = \\eta_\\ell$ but may fail when $\\eta_u \\ne \\eta_\\ell$, where W2S\ngain diminishes as $(\\eta_u - \\eta_\\ell)^2$ increases. Our theory is\ncorroborated by extensive experiments on various spurious correlation\nbenchmarks and teacher-student pairs. To boost W2S performance upon failures,\nwe further propose a simple, effective algorithmic remedy that retrains the\nstrong student on its high-confidence data subset after W2S fine-tuning. Our\nalgorithm is group-label-free and achieves consistent, substantial improvements\nover vanilla W2S fine-tuning.", "AI": {"tldr": "This paper studies weak-to-strong generalization in scenarios with spurious correlations from group imbalance, showing W2S succeeds when labeled and unlabeled data have same minority group proportions but fails when proportions differ, and proposes a simple retraining method to improve performance.", "motivation": "To understand when weak-to-strong generalization works or fails in the presence of spurious correlations from group-imbalanced data, and develop methods to improve it when it fails.", "method": "Theoretical analysis of W2S gain at proportional asymptotic limit, extensive experiments on spurious correlation benchmarks, and a simple algorithmic remedy that retrains the strong student on its high-confidence data subset after W2S fine-tuning.", "result": "W2S always succeeds when \u03b7_u = \u03b7_\u2113 with sufficient pseudolabels, but fails when \u03b7_u \u2260 \u03b7_\u2113 with gain diminishing as (\u03b7_u - \u03b7_\u2113)^2 increases. The proposed retraining method achieves consistent, substantial improvements over vanilla W2S fine-tuning.", "conclusion": "Group imbalance between labeled and unlabeled data can cause W2S generalization failures, but a simple high-confidence retraining strategy can effectively boost performance without requiring group labels."}}
{"id": "2509.24930", "categories": ["cs.CL", "cs.CY", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.24930", "abs": "https://arxiv.org/abs/2509.24930", "authors": ["Rebira Jemama", "Rajesh Kumar"], "title": "How Well Do LLMs Imitate Human Writing Style?", "comment": "IEEE UEMCON 2025, 11 pages, 4 figures, and 4 tables", "summary": "Large language models (LLMs) can generate fluent text, but their ability to\nreplicate the distinctive style of a specific human author remains unclear. We\npresent a fast, training-free framework for authorship verification and style\nimitation analysis. The method integrates TF-IDF character n-grams with\ntransformer embeddings and classifies text pairs through empirical distance\ndistributions, eliminating the need for supervised training or threshold\ntuning. It achieves 97.5\\% accuracy on academic essays and 94.5\\% in\ncross-domain evaluation, while reducing training time by 91.8\\% and memory\nusage by 59\\% relative to parameter-based baselines. Using this framework, we\nevaluate five LLMs from three separate families (Llama, Qwen, Mixtral) across\nfour prompting strategies - zero-shot, one-shot, few-shot, and text completion.\nResults show that the prompting strategy has a more substantial influence on\nstyle fidelity than model size: few-shot prompting yields up to 23.5x higher\nstyle-matching accuracy than zero-shot, and completion prompting reaches 99.9\\%\nagreement with the original author's style. Crucially, high-fidelity imitation\ndoes not imply human-like unpredictability - human essays average a perplexity\nof 29.5, whereas matched LLM outputs average only 15.2. These findings\ndemonstrate that stylistic fidelity and statistical detectability are\nseparable, establishing a reproducible basis for future work in authorship\nmodeling, detection, and identity-conditioned generation.", "AI": {"tldr": "A training-free framework for authorship verification and style imitation analysis that combines TF-IDF character n-grams with transformer embeddings, achieving high accuracy while significantly reducing computational resources. Used to evaluate LLMs' style imitation capabilities.", "motivation": "To understand whether large language models can effectively replicate specific human author styles, and to develop an efficient method for authorship verification without requiring supervised training.", "method": "Integrates TF-IDF character n-grams with transformer embeddings, classifies text pairs through empirical distance distributions, and evaluates LLMs across different prompting strategies (zero-shot, one-shot, few-shot, text completion).", "result": "Achieved 97.5% accuracy on academic essays and 94.5% in cross-domain evaluation, with 91.8% training time reduction and 59% memory usage reduction. Found that prompting strategy has greater influence than model size, with few-shot prompting yielding 23.5x higher style-matching accuracy than zero-shot.", "conclusion": "High-fidelity style imitation by LLMs doesn't imply human-like unpredictability, as human essays have higher perplexity (29.5) than matched LLM outputs (15.2). Stylistic fidelity and statistical detectability are separable, providing a basis for future authorship modeling and detection work."}}
{"id": "2509.24006", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24006", "abs": "https://arxiv.org/abs/2509.24006", "authors": ["Jintao Zhang", "Haoxu Wang", "Kai Jiang", "Shuo Yang", "Kaiwen Zheng", "Haocheng Xi", "Ziteng Wang", "Hongzhou Zhu", "Min Zhao", "Ion Stoica", "Joseph E. Gonzalez", "Jun Zhu", "Jianfei Chen"], "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention", "comment": null, "summary": "In Diffusion Transformer (DiT) models, particularly for video generation,\nattention latency is a major bottleneck due to the long sequence length and the\nquadratic complexity. We find that attention weights can be separated into two\nparts: a small fraction of large weights with high rank and the remaining\nweights with very low rank. This naturally suggests applying sparse\nacceleration to the first part and low-rank acceleration to the second. Based\non this finding, we propose SLA (Sparse-Linear Attention), a trainable\nattention method that fuses sparse and linear attention to accelerate diffusion\nmodels. SLA classifies attention weights into critical, marginal, and\nnegligible categories, applying O(N^2) attention to critical weights, O(N)\nattention to marginal weights, and skipping negligible ones. SLA combines these\ncomputations into a single GPU kernel and supports both forward and backward\npasses. With only a few fine-tuning steps using SLA, DiT models achieve a 20x\nreduction in attention computation, resulting in significant acceleration\nwithout loss of generation quality. Experiments show that SLA reduces attention\ncomputation by 95% without degrading end-to-end generation quality,\noutperforming baseline methods. In addition, we implement an efficient GPU\nkernel for SLA, which yields a 13.7x speedup in attention computation and a\n2.2x end-to-end speedup in video generation on Wan2.1-1.3B.", "AI": {"tldr": "SLA (Sparse-Linear Attention) accelerates Diffusion Transformer models by fusing sparse and linear attention, reducing attention computation by 95% while maintaining generation quality.", "motivation": "Attention latency is a major bottleneck in Diffusion Transformer models for video generation due to long sequence lengths and quadratic complexity.", "method": "SLA classifies attention weights into critical, marginal, and negligible categories, applying O(N\u00b2) attention to critical weights, O(N) attention to marginal weights, and skipping negligible ones, then combines these computations into a single GPU kernel.", "result": "SLA achieves 20x reduction in attention computation, 13.7x speedup in attention computation, and 2.2x end-to-end speedup in video generation on Wan2.1-1.3B without degrading generation quality.", "conclusion": "SLA effectively accelerates DiT models by leveraging the observation that attention weights can be separated into high-rank and low-rank components, enabling significant computation reduction while preserving performance."}}
{"id": "2509.24945", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24945", "abs": "https://arxiv.org/abs/2509.24945", "authors": ["Changsheng Zhao", "Ernie Chang", "Zechun Liu", "Chia-Jung Chang", "Wei Wen", "Chen Lai", "Rick Cao", "Yuandong Tian", "Raghuraman Krishnamoorthi", "Yangyang Shi", "Vikas Chandra"], "title": "MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes", "comment": "Model:\n  https://huggingface.co/collections/facebook/mobilellm-r1-68c4597b104fac45f28f448e", "summary": "The paradigm shift in large language models (LLMs) from instinctive responses\nto chain-of-thought (CoT) reasoning has fueled two prevailing assumptions: (1)\nreasoning capabilities only emerge in sufficiently large models, and (2) such\ncapabilities require training on massive datasets. While the first assumption\nhas already been challenged by recent sub-billion-parameter reasoning models\nsuch as Qwen3-0.6B and DeepSeek distilled variants, the second remains largely\nunquestioned. In this work, we revisit the necessity of scaling to extremely\nlarge corpora (>10T tokens) for reasoning emergence. By carefully curating and\nresampling open-source datasets that we identify as beneficial under our\ndesigned metrics, we demonstrate that strong reasoning abilities can emerge\nwith far less data. Specifically, we show that only ~2T tokens of high-quality\ndata are sufficient, and pre-training with 4.2T tokens on the dataset resampled\nfrom these ~2T tokens, followed by a established post-training procedure,\nenables the development of MobileLLM-R1, a series of sub-billion-parameter\nreasoning models that substantially outperform prior models trained on fully\nopen-sourced data. For example, MobileLLM-R1-950M achieves an AIME score of\n15.5, compared to just 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B.\nRemarkably, despite being trained on only 11.7% of the tokens compared to\nQwen3's proprietary 36T-token corpus for pretraining, MobileLLM-R1-950M matches\nor surpasses Qwen3-0.6B across multiple reasoning benchmarks. To facilitate\nfurther research in this direction, we have released the complete training\nrecipe, data sources, data mixing ratio, and model checkpoints, together with\nthe key insights obtained throughout this study.", "AI": {"tldr": "This paper challenges the assumption that reasoning capabilities in LLMs require massive datasets (>10T tokens), showing that high-quality data curation and resampling can achieve strong reasoning with only ~2T tokens.", "motivation": "To question the prevailing assumption that reasoning capabilities in LLMs necessitate training on extremely large corpora (>10T tokens), and demonstrate that careful data curation can achieve similar results with far less data.", "method": "Curated and resampled open-source datasets using designed metrics to identify beneficial data, then pre-trained on 4.2T tokens from these ~2T high-quality tokens, followed by established post-training procedures.", "result": "Developed MobileLLM-R1 series (sub-billion-parameter models) that substantially outperform prior models on reasoning benchmarks. MobileLLM-R1-950M achieved AIME score of 15.5 vs 0.6 for OLMo-2-1.48B and 0.3 for SmolLM-2-1.7B, matching/surpassing Qwen3-0.6B despite using only 11.7% of its training tokens.", "conclusion": "Strong reasoning capabilities can emerge with far less data than previously assumed through careful data curation and resampling, challenging the necessity of massive training corpora for reasoning emergence in LLMs."}}
{"id": "2509.24012", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24012", "abs": "https://arxiv.org/abs/2509.24012", "authors": ["Rylan Schaeffer", "Noam Levi", "Brando Miranda", "Sanmi Koyejo"], "title": "Pretraining Scaling Laws for Generative Evaluations of Language Models", "comment": null, "summary": "Neural scaling laws have played a central role in modern machine learning,\ndriving the field's ever-expanding scaling of parameters, data and compute.\nWhile much research has gone into fitting scaling laws and predicting\nperformance on pretraining losses and on discriminative evaluations such as\nmultiple-choice question-answering, comparatively little research has been done\non fitting scaling laws and predicting performance on generative evaluations\nsuch as mathematical problem-solving or software engineering. We propose and\nevaluate three different pretraining scaling laws for fitting pass-at-$k$ on\ngenerative evaluations and for predicting pass-at-$k$ of the most expensive\nmodel using the performance of cheaper models. Our three scaling laws differ in\nthe covariates used: (1) compute, (2) model parameters and tokens, (3) log\nlikelihoods of gold reference solutions. We make four main contributions: (1)\nWe show how generative evaluations offer new hyperparameters (in our setting,\n$k$) that researchers can use to control the scaling laws parameters and the\npredictability of performance. (2) In terms of scaling law parameters, we find\nthat the compute scaling law and parameters\\,+\\,tokens scaling law stabilize\nfor the last ~$1.5{-}2.5$ orders of magnitude, whereas the gold reference\nlikelihood scaling law stabilizes for the last ~$5$ orders of magnitude. (3) In\nterms of predictive performance, we find all three scaling laws perform\ncomparably, although the compute scaling law predicts slightly worse for small\n$k$ and the log likelihoods of gold reference solutions predicts slightly worse\nfor large $k$. (4) We establish a theoretical connection that the compute\nscaling law emerges as the compute-optimal envelope of the\nparameters-and-tokens scaling law. Our framework provides researchers and\npractitioners with insights and methodologies to forecast generative\nperformance.", "AI": {"tldr": "The paper proposes three scaling laws for generative evaluations (pass-at-k) and shows how generative evaluations offer new hyperparameters to control scaling law parameters and performance predictability.", "motivation": "While much research has focused on scaling laws for pretraining losses and discriminative tasks, little work has been done on scaling laws for generative evaluations like mathematical problem-solving and software engineering.", "method": "Three different pretraining scaling laws are proposed and evaluated: (1) compute-based, (2) model parameters and tokens, (3) log likelihoods of gold reference solutions. The study examines how generative evaluation hyperparameters (k) affect scaling law parameters and predictability.", "result": "The compute and parameters+tokens scaling laws stabilize for the last ~1.5-2.5 orders of magnitude, while gold reference likelihood scaling law stabilizes for ~5 orders. All three scaling laws perform comparably, with minor differences: compute scaling predicts slightly worse for small k, and gold reference likelihood predicts slightly worse for large k.", "conclusion": "The paper establishes a theoretical connection showing the compute scaling law emerges as the compute-optimal envelope of the parameters-and-tokens scaling law, providing researchers with methodologies to forecast generative performance."}}
{"id": "2509.24958", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24958", "abs": "https://arxiv.org/abs/2509.24958", "authors": ["Linlu Gong", "Ante Wang", "Yunghwei Lai", "Weizhi Ma", "Yang Liu"], "title": "The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents' Inquiry Capability", "comment": null, "summary": "An effective physician should possess a combination of empathy, expertise,\npatience, and clear communication when treating a patient. Recent advances have\nsuccessfully endowed AI doctors with expert diagnostic skills, particularly the\nability to actively seek information through inquiry. However, other essential\nqualities of a good doctor remain overlooked. To bridge this gap, we present\nMAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the\nautomatic and comprehensive evaluation of medical multi-turn questioning. It\nfeatures 3,000 realistically simulated patient agents that exhibit diverse\nlinguistic patterns, cognitive limitations, emotional responses, and tendencies\nfor passive disclosure. We also introduce a multi-faceted evaluation framework,\ncovering task success, inquiry proficiency, dialogue competence, inquiry\nefficiency, and patient experience. Experiments on different LLMs reveal\nsubstantial challenges across the evaluation aspects. Even state-of-the-art\nmodels show significant room for improvement in their inquiry capabilities.\nThese models are highly sensitive to variations in realistic patient behavior,\nwhich considerably impacts diagnostic accuracy. Furthermore, our fine-grained\nmetrics expose trade-offs between different evaluation perspectives,\nhighlighting the challenge of balancing performance and practicality in\nreal-world clinical settings.", "AI": {"tldr": "MAQuE is a comprehensive benchmark for evaluating medical AI doctors' questioning abilities, featuring 3,000 simulated patient agents with diverse characteristics and a multi-faceted evaluation framework.", "motivation": "Current AI doctors focus mainly on diagnostic skills but overlook other essential physician qualities like empathy, patience, and communication. There's a need to evaluate AI's comprehensive questioning capabilities in realistic medical scenarios.", "method": "Created MAQuE benchmark with 3,000 realistically simulated patient agents exhibiting diverse linguistic patterns, cognitive limitations, emotional responses, and passive disclosure tendencies. Introduced multi-faceted evaluation framework covering task success, inquiry proficiency, dialogue competence, inquiry efficiency, and patient experience.", "result": "Experiments on different LLMs revealed substantial challenges across all evaluation aspects. Even state-of-the-art models show significant room for improvement in inquiry capabilities. Models are highly sensitive to realistic patient behavior variations, which considerably impacts diagnostic accuracy. Fine-grained metrics expose trade-offs between different evaluation perspectives.", "conclusion": "The study highlights the challenge of balancing performance and practicality in real-world clinical settings, demonstrating that current AI medical agents need significant improvement in comprehensive questioning capabilities beyond just diagnostic skills."}}
{"id": "2509.24031", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.24031", "abs": "https://arxiv.org/abs/2509.24031", "authors": ["Umang Garg", "Bowen Zhang", "Anantanjit Subrahmanya", "Chandrakanth Gudavalli", "BS Manjunath"], "title": "GPS-MTM: Capturing Pattern of Normalcy in GPS-Trajectories with self-supervised learning", "comment": "4 pages, 2 figures", "summary": "Foundation models have driven remarkable progress in text, vision, and video\nunderstanding, and are now poised to unlock similar breakthroughs in trajectory\nmodeling. We introduce the GPSMasked Trajectory Transformer (GPS-MTM), a\nfoundation model for large-scale mobility data that captures patterns of\nnormalcy in human movement. Unlike prior approaches that flatten trajectories\ninto coordinate streams, GPS-MTM decomposes mobility into two complementary\nmodalities: states (point-of-interest categories) and actions (agent\ntransitions). Leveraging a bi-directional Transformer with a self-supervised\nmasked modeling objective, the model reconstructs missing segments across\nmodalities, enabling it to learn rich semantic correlations without manual\nlabels. Across benchmark datasets, including Numosim-LA, Urban Anomalies, and\nGeolife, GPS-MTM consistently outperforms on downstream tasks such as\ntrajectory infilling and next-stop prediction. Its advantages are most\npronounced in dynamic tasks (inverse and forward dynamics), where contextual\nreasoning is critical. These results establish GPS-MTM as a robust foundation\nmodel for trajectory analytics, positioning mobility data as a first-class\nmodality for large-scale representation learning. Code is released for further\nreference.", "AI": {"tldr": "GPS-MTM is a foundation model for mobility data that decomposes trajectories into states and actions, using masked modeling to learn semantic patterns without labels. It outperforms benchmarks on trajectory tasks.", "motivation": "Foundation models have shown success in text, vision, and video, but trajectory modeling lacks similar breakthroughs. The paper aims to create a foundation model for large-scale mobility data to capture normal human movement patterns.", "method": "GPS-MTM decomposes mobility into states (POI categories) and actions (transitions), using a bi-directional Transformer with self-supervised masked modeling to reconstruct missing segments across modalities without manual labels.", "result": "GPS-MTM consistently outperforms on benchmark datasets (Numosim-LA, Urban Anomalies, Geolife) for trajectory infilling and next-stop prediction, with strongest advantages in dynamic tasks requiring contextual reasoning.", "conclusion": "GPS-MTM establishes itself as a robust foundation model for trajectory analytics, positioning mobility data as a first-class modality for large-scale representation learning."}}
{"id": "2509.24961", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24961", "abs": "https://arxiv.org/abs/2509.24961", "authors": ["Kaihong Li", "Huichi Zhou", "Bin Ma", "Fangjun Huang"], "title": "SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems", "comment": null, "summary": "Recommender systems (RS) are widely used in e-commerce for personalized\nsuggestions, yet their openness makes them susceptible to shilling attacks,\nwhere adversaries inject fake behaviors to manipulate recommendations. Most\nexisting defenses emphasize user-side behaviors while overlooking item-side\nfeatures such as titles and descriptions that can expose malicious intent. To\naddress this gap, we propose a two-stage detection framework that integrates\nitem-side semantics via large language models (LLMs). The first stage\npre-screens suspicious users using low-cost behavioral criteria, and the second\nstage employs LLM-based auditing to evaluate semantic consistency. Furthermore,\nwe enhance the auditing model through reinforcement fine-tuning on a\nlightweight LLM with carefully designed reward functions, yielding a\nspecialized detector called SemanticShield. Experiments on six representative\nattack strategies demonstrate the effectiveness of SemanticShield against\nshilling attacks, and further evaluation on previously unseen attack methods\nshows its strong generalization capability. Code is available at\nhttps://github.com/FrankenstLee/SemanticShield.", "AI": {"tldr": "SemanticShield is a two-stage framework that uses LLMs to detect shilling attacks in recommender systems by analyzing item-side semantic features alongside user behaviors.", "motivation": "Existing defenses focus mainly on user behaviors but ignore item-side features like titles and descriptions that can reveal malicious intent in shilling attacks.", "method": "Two-stage detection: first pre-screens suspicious users using behavioral criteria, then employs LLM-based auditing to evaluate semantic consistency. Enhanced through reinforcement fine-tuning with specialized reward functions.", "result": "Experiments on six attack strategies show SemanticShield effectively detects shilling attacks, and further evaluation demonstrates strong generalization to unseen attack methods.", "conclusion": "SemanticShield provides an effective defense against shilling attacks by leveraging item-side semantics through LLMs, offering both detection accuracy and generalization capability."}}
{"id": "2509.24047", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.24047", "abs": "https://arxiv.org/abs/2509.24047", "authors": ["Runyu Zhang", "Na Li", "Asuman Ozdaglar", "Jeff Shamma", "Gioele Zardini"], "title": "Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning", "comment": null, "summary": "Risk sensitivity has become a central theme in reinforcement learning (RL),\nwhere convex risk measures and robust formulations provide principled ways to\nmodel preferences beyond expected return. Recent extensions to multi-agent RL\n(MARL) have largely emphasized the risk-averse setting, prioritizing robustness\nto uncertainty. In cooperative MARL, however, such conservatism often leads to\nsuboptimal equilibria, and a parallel line of work has shown that optimism can\npromote cooperation. Existing optimistic methods, though effective in practice,\nare typically heuristic and lack theoretical grounding. Building on the dual\nrepresentation for convex risk measures, we propose a principled framework that\ninterprets risk-seeking objectives as optimism. We introduce optimistic value\nfunctions, which formalize optimism as divergence-penalized risk-seeking\nevaluations. Building on this foundation, we derive a policy-gradient theorem\nfor optimistic value functions, including explicit formulas for the entropic\nrisk/KL-penalty setting, and develop decentralized optimistic actor-critic\nalgorithms that implement these updates. Empirical results on cooperative\nbenchmarks demonstrate that risk-seeking optimism consistently improves\ncoordination over both risk-neutral baselines and heuristic optimistic methods.\nOur framework thus unifies risk-sensitive learning and optimism, offering a\ntheoretically grounded and practically effective approach to cooperation in\nMARL.", "AI": {"tldr": "This paper proposes a principled framework that interprets risk-seeking objectives as optimism in multi-agent reinforcement learning (MARL), introducing optimistic value functions and developing decentralized optimistic actor-critic algorithms that improve coordination over existing methods.", "motivation": "Existing optimistic methods in cooperative MARL are typically heuristic and lack theoretical grounding, while risk-averse approaches often lead to suboptimal equilibria. The authors aim to provide a principled framework that unifies risk-sensitive learning and optimism.", "method": "Building on dual representation for convex risk measures, the authors propose optimistic value functions that formalize optimism as divergence-penalized risk-seeking evaluations. They derive a policy-gradient theorem for these functions and develop decentralized optimistic actor-critic algorithms.", "result": "Empirical results on cooperative benchmarks demonstrate that risk-seeking optimism consistently improves coordination over both risk-neutral baselines and heuristic optimistic methods.", "conclusion": "The framework unifies risk-sensitive learning and optimism, offering a theoretically grounded and practically effective approach to cooperation in MARL."}}
{"id": "2509.24988", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24988", "abs": "https://arxiv.org/abs/2509.24988", "authors": ["Hanqi Xiao", "Vaidehi Patil", "Hyunji Lee", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns", "comment": "Code:\n  https://github.com/The-Inscrutable-X/CalibratedModelAgnosticCorrectness", "summary": "Generating accurate and calibrated confidence estimates is critical for\ndeploying LLMs in high-stakes or user-facing applications, and remains an open\nchallenge. Prior research has often framed confidence as a problem of eliciting\na model's \"self-knowledge\", i.e., the ability of an LLM to judge whether its\nown answers are correct; this approach implicitly assumes that there is some\nprivileged information about the answer's correctness that is accessible to the\nmodel itself. However, our experiments reveal that an LLM attempting to predict\nthe correctness of its own outputs generally performs no better than an\nunrelated LLM. Moreover, we hypothesize that a key factor in building a\n\"Correctness Model\" (CM) is exposure to a target model's historical\npredictions. We propose multiple methods to inject this historical correctness\ninformation, creating a Generalized Correctness Model (GCM). We first show that\nGCMs can be trained on the correctness data from many LLMs and learn patterns\nfor correctness prediction applicable across datasets and models. We then use\nCMs as a lens for studying the source of correctness prediction ability and its\ngeneralization, systematically controlling their training data and finding that\nanswer phrasing is a strong predictor for correctness. We further explore\nalternative methods of injecting history without training an LLM, finding that\nincluding history as in-context examples can help improve correctness\nprediction, and post-hoc calibration can provide complementary reductions in\ncalibration error. We evaluate GCMs based on Qwen3-8B across 5 model families\nand the MMLU and TriviaQA datasets, as well as on a downstream selective\nprediction task, finding that reliable LLM confidence estimation is a\ngeneralizable and model-agnostic skill learned by systematically encoding\ncorrectness history rather than a model-specific skill reliant on\nself-introspection.", "AI": {"tldr": "Training Generalized Correctness Models (GCMs) on historical correctness data from multiple LLMs enables reliable confidence estimation that generalizes across models and datasets, challenging the notion that confidence requires model-specific self-introspection.", "motivation": "Accurate confidence estimation is critical for deploying LLMs in high-stakes applications, but current approaches based on self-knowledge assumption may not be optimal. The research aims to develop more effective confidence estimation methods.", "method": "Proposed Generalized Correctness Models (GCMs) trained on historical correctness data from multiple LLMs, with systematic experiments controlling training data and exploring alternative methods like in-context examples and post-hoc calibration.", "result": "GCMs demonstrate generalizable correctness prediction across 5 model families and datasets (MMLU, TriviaQA), with answer phrasing identified as a strong predictor. In-context history and calibration provide complementary improvements.", "conclusion": "Reliable LLM confidence estimation is a generalizable skill learned from systematic encoding of correctness history, rather than a model-specific skill dependent on self-introspection."}}
{"id": "2509.24050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24050", "abs": "https://arxiv.org/abs/2509.24050", "authors": ["Wenzhi Fang", "Dong-Jun Han", "Liangqi Yuan", "Christopher Brinton"], "title": "Collaborative Device-Cloud LLM Inference through Reinforcement Learning", "comment": "We propose a unified post-training framework that integrates routing\n  optimization, enabling the on-device LLM to improve its problem-solving\n  ability while learning routing strategies", "summary": "Device-cloud collaboration has emerged as a promising paradigm for deploying\nlarge language models (LLMs), combining the efficiency of lightweight on-device\ninference with the superior performance of powerful cloud LLMs. An essential\nproblem in this scenario lies in deciding whether a given query is best handled\nlocally or delegated to the cloud. Existing approaches typically rely on\nexternal routers, implemented as binary classifiers, which often struggle to\ndetermine task difficulty from the prompt's surface pattern. To address these\nlimitations, we propose a framework where the on-device LLM makes routing\ndecisions at the end of its solving process, with this capability instilled\nthrough post-training. In particular, we formulate a reward maximization\nproblem with carefully designed rewards that encourage effective problem\nsolving and judicious offloading to the cloud. To solve this problem, we\ndevelop a group-adaptive policy gradient algorithm, featuring a group-level\npolicy gradient, designed to yield an unbiased gradient estimator of the\nreward, and adaptive prompt filtering, developed to enforce the constraint on\ncloud LLM usage. Extensive experiments across models and benchmarks show that\nthe proposed methodology consistently outperforms existing baselines and\nsignificantly narrows the gap to full cloud LLM performance.", "AI": {"tldr": "Proposes a device-cloud collaboration framework where on-device LLMs make routing decisions after problem-solving, using post-training with reward maximization and group-adaptive policy gradient to optimize performance while controlling cloud usage.", "motivation": "Existing binary classifier routers struggle to determine task difficulty from prompt surface patterns, leading to suboptimal routing decisions in device-cloud collaboration scenarios.", "method": "Framework where on-device LLM makes routing decisions after solving process, using post-training with reward maximization problem and group-adaptive policy gradient algorithm with adaptive prompt filtering.", "result": "Extensive experiments show consistent outperformance over existing baselines and significant narrowing of gap to full cloud LLM performance across models and benchmarks.", "conclusion": "The proposed methodology effectively addresses limitations of existing routing approaches and demonstrates superior performance in device-cloud collaboration for LLM deployment."}}
{"id": "2509.25002", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25002", "abs": "https://arxiv.org/abs/2509.25002", "authors": ["Somin Wadhwa", "Silvio Amir", "Byron C. Wallace"], "title": "Circuit Distillation", "comment": "Preprint; Under Review", "summary": "Model distillation typically focuses on behavioral mimicry, where a student\nmodel is trained to replicate a teacher's output while treating its internal\ncomputations as a black box. In this work we propose an alternative approach:\nDistilling the underlying computational mechanisms implemented by a teacher\nmodel. Specifically, we propose circuit distillation, which introduces an\nobjective to align internal representations between analogous circuit\ncomponents in teacher and student models. We propose a method to match\n``functionally correspondent'' circuit components and introduce a loss\nreflecting similarities between the representations that these induce. We\nevaluate circuit distillation on entity tracking and theory of mind (ToM) tasks\nusing models from the Llama3 family. Our results demonstrate that circuit\ndistillation outperforms standard distillation, successfully transferring\nalgorithmic capabilities by adjusting only a small, targeted subset of student\nmodel parameters. This work establishes the feasibility of transferring\nmechanisms, which may in turn allow for efficient distillation of targeted\nteacher capabilities via interpretable and controllable internal student\nmechanisms.", "AI": {"tldr": "Circuit distillation aligns internal representations between teacher and student models' circuit components, outperforming standard behavioral mimicry distillation on entity tracking and theory of mind tasks.", "motivation": "Standard model distillation treats teacher's internal computations as black box, focusing only on output mimicry. This work aims to distill the underlying computational mechanisms themselves.", "method": "Proposes circuit distillation with objective to align internal representations between functionally correspondent circuit components in teacher and student models, using a loss reflecting representation similarities.", "result": "Circuit distillation outperforms standard distillation on entity tracking and theory of mind tasks using Llama3 models, successfully transferring algorithmic capabilities by adjusting only small, targeted parameter subsets.", "conclusion": "Establishes feasibility of transferring mechanisms, enabling efficient distillation of targeted teacher capabilities via interpretable and controllable internal student mechanisms."}}
{"id": "2509.24058", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24058", "abs": "https://arxiv.org/abs/2509.24058", "authors": ["Julia Wenkmann", "Damien Garreau"], "title": "On The Variability of Concept Activation Vectors", "comment": "26 pages (including appendix), 24 figures (44 panels). Submitted to\n  AAAI-26", "summary": "One of the most pressing challenges in artificial intelligence is to make\nmodels more transparent to their users. Recently, explainable artificial\nintelligence has come up with numerous method to tackle this challenge. A\npromising avenue is to use concept-based explanations, that is, high-level\nconcepts instead of plain feature importance score. Among this class of\nmethods, Concept Activation vectors (CAVs), Kim et al. (2018) stands out as one\nof the main protagonists. One interesting aspect of CAVs is that their\ncomputation requires sampling random examples in the train set. Therefore, the\nactual vectors obtained may vary from user to user depending on the randomness\nof this sampling. In this paper, we propose a fine-grained theoretical analysis\nof CAVs construction in order to quantify their variability. Our results,\nconfirmed by experiments on several real-life datasets, point out towards an\nuniversal result: the variance of CAVs decreases as $1/N$, where $N$ is the\nnumber of random examples. Based on this we give practical recommendations for\na resource-efficient application of the method.", "AI": {"tldr": "This paper provides a theoretical analysis of Concept Activation Vectors (CAVs) variability, showing that their variance decreases as 1/N where N is the number of random examples, and offers practical recommendations for efficient application.", "motivation": "To address the variability in Concept Activation Vectors (CAVs) that arises from random sampling during their computation, which can lead to different explanations for different users.", "method": "The authors conduct a fine-grained theoretical analysis of CAVs construction to quantify their variability, supported by experiments on several real-life datasets.", "result": "The analysis reveals a universal result: the variance of CAVs decreases as 1/N, where N is the number of random examples used in their computation.", "conclusion": "Based on the theoretical findings, the paper provides practical recommendations for resource-efficient application of the CAVs method in explainable AI."}}
{"id": "2509.25035", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25035", "abs": "https://arxiv.org/abs/2509.25035", "authors": ["Haoyang Zheng", "Xinyang Liu", "Cindy Xiangrui Kong", "Nan Jiang", "Zheyuan Hu", "Weijian Luo", "Wei Deng", "Guang Lin"], "title": "Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct", "comment": "56 pages, 7 figures, 7 tables", "summary": "Fast generation of language texts is the holy grail that people pursue in the\nAI era. In this work, we introduced Discrete Diffusion Divergence Instruct\n(DiDi-Instruct), a training-based method that leads to fast language generation\nmodels by initializing from a pre-trained (masked) discrete diffusion language\nmodel (dLLM). The resulting DiDi-Instruct model outperforms the dLLM\ncounterparts and the GPT-2 baseline with 64x acceleration. In the theoretical\npart of the paper, we build the foundation of DiDi-Instruct in a framework of\nintegral KL-divergence minimization, with practical training algorithms. We\nalso introduce techniques like grouped reward normalization, intermediate-state\nmatching, and the reward-guided ancestral sampler (RGAS) that significantly\nimprove the training stability, the model coverage, and the inference\nperformances. On OpenWebText, DiDi-Instruct outperforms all accelerated\nlanguage generation models as well as the GPT-2 baseline and the standard\ndLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128\nNFEs). These performance gains are accomplished with a negligible entropy loss\nof about 1% and 20x less additional training wall-clock time. We further\nvalidate the robustness and effectiveness of DiDi-Instruct through extensive\nablation studies, model scaling, and the generation of discrete protein\nsequences. In conclusion, DiDi-Instruct is an efficient yet effective\ndistillation method, enabling language generation in the blink of an eye. We\nwill release both code and models at github.com/haoyangzheng-ai/didi-instruct.", "AI": {"tldr": "DiDi-Instruct is a training-based method that accelerates language generation by initializing from pre-trained discrete diffusion language models, achieving 64x speedup while outperforming GPT-2 and standard dLLMs.", "motivation": "Fast generation of language texts is the holy grail that people pursue in the AI era, aiming to achieve efficient language generation with minimal computational overhead.", "method": "DiDi-Instruct builds on integral KL-divergence minimization framework with practical training algorithms, using techniques like grouped reward normalization, intermediate-state matching, and reward-guided ancestral sampler (RGAS) to improve training stability and inference performance.", "result": "On OpenWebText, DiDi-Instruct achieves sample perplexities from 62.2 (8 NFEs) to 18.4 (128 NFEs), outperforming all accelerated language generation models with only 1% entropy loss and 20x less training time compared to standard methods.", "conclusion": "DiDi-Instruct is an efficient yet effective distillation method that enables fast language generation with significant performance gains and minimal computational overhead."}}
{"id": "2509.24067", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24067", "abs": "https://arxiv.org/abs/2509.24067", "authors": ["Qiushui Xu", "Yuhao Huang", "Yushu Jiang", "Lei Song", "Jinyu Wang", "Wenliang Zheng", "Jiang Bian"], "title": "In-Context Compositional Q-Learning for Offline Reinforcement Learning", "comment": null, "summary": "Accurately estimating the Q-function is a central challenge in offline\nreinforcement learning. However, existing approaches often rely on a single\nglobal Q-function, which struggles to capture the compositional nature of tasks\ninvolving diverse subtasks. We propose In-context Compositional Q-Learning\n(\\texttt{ICQL}), the first offline RL framework that formulates Q-learning as a\ncontextual inference problem, using linear Transformers to adaptively infer\nlocal Q-functions from retrieved transitions without explicit subtask labels.\nTheoretically, we show that under two assumptions--linear approximability of\nthe local Q-function and accurate weight inference from retrieved\ncontext--\\texttt{ICQL} achieves bounded Q-function approximation error, and\nsupports near-optimal policy extraction. Empirically, \\texttt{ICQL}\nsubstantially improves performance in offline settings: improving performance\nin kitchen tasks by up to 16.4\\%, and in Gym and Adroit tasks by up to 8.6\\%\nand 6.3\\%. These results highlight the underexplored potential of in-context\nlearning for robust and compositional value estimation, positioning\n\\texttt{ICQL} as a principled and effective framework for offline RL.", "AI": {"tldr": "ICQL is an offline RL framework that formulates Q-learning as contextual inference using linear Transformers to adaptively infer local Q-functions from retrieved transitions, achieving improved performance in compositional tasks.", "motivation": "Existing approaches rely on single global Q-functions that struggle to capture compositional nature of tasks with diverse subtasks, limiting performance in offline RL settings.", "method": "Uses linear Transformers to formulate Q-learning as contextual inference problem, adaptively inferring local Q-functions from retrieved transitions without explicit subtask labels.", "result": "Improves performance in kitchen tasks by up to 16.4%, Gym tasks by 8.6%, and Adroit tasks by 6.3% compared to existing methods.", "conclusion": "ICQL demonstrates the potential of in-context learning for robust and compositional value estimation, providing a principled and effective framework for offline RL."}}
{"id": "2509.25037", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25037", "abs": "https://arxiv.org/abs/2509.25037", "authors": ["Adamu Lawan", "Haruna Yunusa"], "title": "GateMABSA: Aspect-Image Gated Fusion for Multimodal Aspect-based Sentiment Analysis", "comment": "6 pages, 2 tables", "summary": "Aspect-based Sentiment Analysis (ABSA) has recently advanced into the\nmultimodal domain, where user-generated content often combines text and images.\nHowever, existing multimodal ABSA (MABSA) models struggle to filter noisy\nvisual signals, and effectively align aspects with opinion-bearing content\nacross modalities. To address these challenges, we propose GateMABSA, a novel\ngated multimodal architecture that integrates syntactic, semantic, and\nfusion-aware mLSTM. Specifically, GateMABSA introduces three specialized\nmLSTMs: Syn-mLSTM to incorporate syntactic structure, Sem-mLSTM to emphasize\naspect--semantic relevance, and Fuse-mLSTM to perform selective multimodal\nfusion. Extensive experiments on two benchmark Twitter datasets demonstrate\nthat GateMABSA outperforms several baselines.", "AI": {"tldr": "GateMABSA is a novel gated multimodal architecture for aspect-based sentiment analysis that uses three specialized mLSTMs to handle syntactic structure, aspect-semantic relevance, and selective multimodal fusion, outperforming existing methods on Twitter datasets.", "motivation": "Existing multimodal ABSA models struggle with filtering noisy visual signals and effectively aligning aspects with opinion-bearing content across text and image modalities.", "method": "Proposes GateMABSA with three specialized mLSTMs: Syn-mLSTM for syntactic structure, Sem-mLSTM for aspect-semantic relevance, and Fuse-mLSTM for selective multimodal fusion.", "result": "Extensive experiments on two benchmark Twitter datasets demonstrate that GateMABSA outperforms several baselines.", "conclusion": "GateMABSA effectively addresses the challenges of noisy visual signals and cross-modal alignment in multimodal aspect-based sentiment analysis."}}
{"id": "2509.24068", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24068", "abs": "https://arxiv.org/abs/2509.24068", "authors": ["Roussel Rahman", "Jeff Shrager"], "title": "A Small Math Model: Recasting Strategy Choice Theory in an LLM-Inspired Architecture", "comment": null, "summary": "Strategy Choice Theory (SCT)\\footnote{``Strategy Choice Theory'',\n``Distributions of Associations'', and ``Overlapping Wave Theory'' have been\nused to refer to this line of work, emphasizing different\naspects.}\\citep[e.g.,][]{siegler1984strategychoices, siegler2000rebirth}\nexplains important aspects of children's arithmetic learning based upon\nprinciples including learning from developmentally naturalistic data,\nprobabilistic representation, confidence-based retrieval, and the phase-like\nimportance of scaffolding strategies, such as finger-counting. Here we recast\nSCT as a ``Small Math Model'' (SMM), employing a neural-network-based\narchitecture analogous to LLMs. The SMM extends SCT to include counting\npractice\\footnote{The original SCT model was pre-biased in accordance with the\nsupposed experience of counting.}, symbol (number) embedding, and gated\nattention. Similar to earlier work, the SMM demonstrates constructive and\ndestructive interference between counting and addition, and the ``wave-like''\nuse of finger-counting as sum recall improves. We plan to extend the SMM to\nlater aspects of the decades-long SCT program, including adaptive strategy\nchoice and eventually strategy discovery, providing a unified platform to\ninvestigate the understanding of numerical characteristics and relationships\nessential for mathematical reasoning -- as it can emerge in LLM-based agents.", "AI": {"tldr": "This paper recasts Strategy Choice Theory (SCT) as a \"Small Math Model\" (SMM) using neural-network architecture similar to LLMs, extending SCT with counting practice, number embedding, and gated attention.", "motivation": "To create a unified platform for investigating numerical understanding and mathematical reasoning in LLM-based agents by extending the decades-long SCT program with modern neural network approaches.", "method": "Developed a neural-network-based Small Math Model (SMM) that incorporates counting practice, symbol embedding, and gated attention mechanisms, building upon Strategy Choice Theory principles.", "result": "The SMM demonstrates constructive and destructive interference between counting and addition, and shows wave-like use of finger-counting as sum recall improves, similar to earlier SCT findings.", "conclusion": "The SMM provides a foundation for extending to later aspects of SCT including adaptive strategy choice and strategy discovery, offering a unified approach to study mathematical reasoning emergence in LLM-based systems."}}
{"id": "2509.25045", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25045", "abs": "https://arxiv.org/abs/2509.25045", "authors": ["Marco Bronzini", "Carlo Nicolini", "Bruno Lepri", "Jacopo Staiano", "Andrea Passerini"], "title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures", "comment": null, "summary": "Despite their capabilities, Large Language Models (LLMs) remain opaque with\nlimited understanding of their internal representations. Current\ninterpretability methods, such as direct logit attribution (DLA) and sparse\nautoencoders (SAEs), provide restricted insight due to limitations such as the\nmodel's output vocabulary or unclear feature names. This work introduces\nHyperdimensional Probe, a novel paradigm for decoding information from the LLM\nvector space. It combines ideas from symbolic representations and neural\nprobing to project the model's residual stream into interpretable concepts via\nVector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs\nand conventional probes while overcoming their key limitations. We validate our\ndecoding paradigm with controlled input-completion tasks, probing the model's\nfinal state before next-token prediction on inputs spanning syntactic pattern\nrecognition, key-value associations, and abstract inference. We further assess\nit in a question-answering setting, examining the state of the model both\nbefore and after text generation. Our experiments show that our probe reliably\nextracts meaningful concepts across varied LLMs, embedding sizes, and input\ndomains, also helping identify LLM failures. Our work advances information\ndecoding in LLM vector space, enabling extracting more informative,\ninterpretable, and structured features from neural representations.", "AI": {"tldr": "This paper introduces Hyperdimensional Probe, a novel method for decoding interpretable concepts from LLM vector spaces using Vector Symbolic Architectures, overcoming limitations of existing interpretability methods.", "motivation": "Current LLM interpretability methods like DLA and SAEs provide limited insight due to vocabulary constraints and unclear feature names, creating a need for better ways to understand internal representations.", "method": "The method projects LLM's residual stream into interpretable concepts using Vector Symbolic Architectures, combining strengths of SAEs and conventional probes while overcoming their limitations.", "result": "Experiments show the probe reliably extracts meaningful concepts across varied LLMs, embedding sizes, and input domains, and helps identify LLM failures in controlled tasks and question-answering settings.", "conclusion": "The work advances information decoding in LLM vector space, enabling extraction of more informative, interpretable, and structured features from neural representations."}}
{"id": "2509.24069", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.AP", "62M10, 68T45, 62P35, 92C40, 65C20, 60G35, 92C42, 92C35, 93E10", "I.2.6; C.2.4; H.3.4; I.2.4; H.3.5; C.2.4; C.3; I.4.8; I.5.1; J.3;\n  K.6.1; H.2.8"], "pdf": "https://arxiv.org/pdf/2509.24069", "abs": "https://arxiv.org/abs/2509.24069", "authors": ["Youssef Sabiri", "Walid Houmaidi", "Ouail El Maadi", "Yousra Chtouki"], "title": "AQUAIR: A High-Resolution Indoor Environmental Quality Dataset for Smart Aquaculture Monitoring", "comment": "6 pages, 6 figures, 3 tables. Accepted at the 9th IEEE Global\n  Conference on Artificial Intelligence & Internet of Things (IEEE GCAIoT)\n  2025. Final camera-ready manuscript. Math expressions in this field are\n  rendered via MathJax", "summary": "Smart aquaculture systems depend on rich environmental data streams to\nprotect fish welfare, optimize feeding, and reduce energy use. Yet public\ndatasets that describe the air surrounding indoor tanks remain scarce, limiting\nthe development of forecasting and anomaly-detection tools that couple\nhead-space conditions with water-quality dynamics. We therefore introduce\nAQUAIR, an open-access public dataset that logs six Indoor Environmental\nQuality (IEQ) variables--air temperature, relative humidity, carbon dioxide,\ntotal volatile organic compounds, PM2.5 and PM10--inside a fish aquaculture\nfacility in Amghass, Azrou, Morocco. A single Awair HOME monitor sampled every\nfive minutes from 14 October 2024 to 9 January 2025, producing more than 23,000\ntime-stamped observations that are fully quality-controlled and publicly\narchived on Figshare. We describe the sensor placement, ISO-compliant mounting\nheight, calibration checks against reference instruments, and an open-source\nprocessing pipeline that normalizes timestamps, interpolates short gaps, and\nexports analysis-ready tables. Exploratory statistics show stable conditions\n(median CO2 = 758 ppm; PM2.5 = 12 micrograms/m3) with pronounced feeding-time\npeaks, offering rich structure for short-horizon forecasting, event detection,\nand sensor drift studies. AQUAIR thus fills a critical gap in smart aquaculture\ninformatics and provides a reproducible benchmark for data-centric machine\nlearning curricula and environmental sensing research focused on head-space\ndynamics in recirculating aquaculture systems.", "AI": {"tldr": "AQUAIR is an open-access public dataset of indoor environmental quality variables from a fish aquaculture facility in Morocco, providing over 23,000 quality-controlled observations for smart aquaculture research.", "motivation": "Public datasets describing air conditions in indoor aquaculture tanks are scarce, limiting development of forecasting and anomaly-detection tools that connect head-space conditions with water-quality dynamics.", "method": "Used a single Awair HOME monitor to sample six IEQ variables (air temperature, humidity, CO2, TVOCs, PM2.5, PM10) every 5 minutes from October 2024 to January 2025, with ISO-compliant mounting, calibration checks, and an open-source processing pipeline.", "result": "Dataset contains over 23,000 time-stamped observations showing stable conditions with feeding-time peaks, suitable for short-horizon forecasting, event detection, and sensor drift studies.", "conclusion": "AQUAIR fills a critical gap in smart aquaculture informatics and provides a reproducible benchmark for machine learning curricula and environmental sensing research focused on head-space dynamics in recirculating aquaculture systems."}}
{"id": "2509.25048", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25048", "abs": "https://arxiv.org/abs/2509.25048", "authors": ["Abner Hernandez", "Tom\u00e1s Arias Vergara", "Andreas Maier", "Paula Andrea P\u00e9rez-Toro"], "title": "Confidence-Guided Error Correction for Disordered Speech Recognition", "comment": "Preprint submitted to ICASSP", "summary": "We investigate the use of large language models (LLMs) as post-processing\nmodules for automatic speech recognition (ASR), focusing on their ability to\nperform error correction for disordered speech. In particular, we propose\nconfidence-informed prompting, where word-level uncertainty estimates are\nembedded directly into LLM training to improve robustness and generalization\nacross speakers and datasets. This approach directs the model to uncertain ASR\nregions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare\nour approach to both transcript-only fine-tuning and post hoc confidence-based\nfiltering. Evaluations show that our method achieves a 10% relative WER\nreduction compared to naive LLM correction on the Speech Accessibility Project\nspontaneous speech and a 47% reduction on TORGO, demonstrating the\neffectiveness of confidence-aware fine-tuning for impaired speech.", "AI": {"tldr": "The paper proposes confidence-informed prompting for LLMs to correct ASR errors in disordered speech, achieving significant WER reductions.", "motivation": "To improve ASR error correction for disordered speech by leveraging LLMs with word-level uncertainty estimates to enhance robustness and reduce overcorrection.", "method": "Fine-tune LLaMA 3.1 with confidence-informed prompting that embeds word-level uncertainty estimates directly into LLM training, comparing against transcript-only fine-tuning and post hoc confidence-based filtering.", "result": "Achieved 10% relative WER reduction on Speech Accessibility Project spontaneous speech and 47% reduction on TORGO dataset compared to naive LLM correction.", "conclusion": "Confidence-aware fine-tuning is effective for impaired speech ASR correction, with confidence-informed prompting outperforming baseline methods across different datasets."}}
{"id": "2509.24076", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24076", "abs": "https://arxiv.org/abs/2509.24076", "authors": ["Bo Hu", "Jos\u00e9 C. Pr\u00edncipe"], "title": "A Family of Kernelized Matrix Costs for Multiple-Output Mixture Neural Networks", "comment": null, "summary": "Pairwise distance-based costs are crucial for self-supervised and contrastive\nfeature learning. Mixture Density Networks (MDNs) are a widely used approach\nfor generative models and density approximation, using neural networks to\nproduce multiple centers that define a Gaussian mixture. By combining MDNs with\ncontrastive costs, this paper proposes data density approximation using four\ntypes of kernelized matrix costs: the scalar cost, the vector-matrix cost, the\nmatrix-matrix cost (the trace of Schur complement), and the SVD cost (the\nnuclear norm), for learning multiple centers required to define a mixture\ndensity.", "AI": {"tldr": "This paper proposes combining Mixture Density Networks (MDNs) with contrastive costs using four kernelized matrix costs for data density approximation and multi-center learning.", "motivation": "To enhance self-supervised and contrastive feature learning by integrating pairwise distance-based costs with MDNs for better density approximation.", "method": "Combines MDNs with contrastive costs using four types of kernelized matrix costs: scalar cost, vector-matrix cost, matrix-matrix cost (trace of Schur complement), and SVD cost (nuclear norm).", "result": "The approach enables learning multiple centers required to define mixture densities through various kernelized cost formulations.", "conclusion": "The proposed integration of MDNs with contrastive costs using kernelized matrix formulations provides an effective framework for data density approximation in self-supervised learning."}}
{"id": "2509.25073", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25073", "abs": "https://arxiv.org/abs/2509.25073", "authors": ["Simeng Sun"], "title": "An empirical study on the limitation of Transformers in program trace generation", "comment": "two-page extended abstract", "summary": "We study Transformers on the task \\emph{program trace generation} (PTG),\nwhere models produce step-by-step execution traces for synthetic programs.\nUnlike existing algorithmic problems, PTG externalizes reasoning through long\ntraces where each step is trivial. We train small Transformers with diverse\nmodifications, including alternative position encodings, softmax replacements,\nhybrid model, and short convolutions. While these models achieve strong\nin-distribution accuracy, they exhibit systematic failures when generalizing to\nvarious factors (e.g., program length, trace steps), though some designs\nsignificantly improve generalization.", "AI": {"tldr": "Small Transformers trained on program trace generation show strong in-distribution accuracy but systematic generalization failures across program length and trace steps, with some architectural modifications improving generalization.", "motivation": "To study Transformers' reasoning capabilities through program trace generation, which externalizes reasoning via long traces with trivial individual steps.", "method": "Train small Transformers with diverse modifications including alternative position encodings, softmax replacements, hybrid models, and short convolutions on the program trace generation task.", "result": "Models achieve strong in-distribution accuracy but exhibit systematic generalization failures when tested on factors like program length and trace steps. Some architectural designs significantly improve generalization.", "conclusion": "While Transformers perform well on in-distribution program trace generation, they struggle with systematic generalization, though certain architectural modifications can enhance their generalization capabilities."}}
{"id": "2509.24077", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24077", "abs": "https://arxiv.org/abs/2509.24077", "authors": ["Zhongteng Cai", "Mohammad Mahdi Khalili", "Xueru Zhang"], "title": "Demographic-Agnostic Fairness without Harm", "comment": null, "summary": "As machine learning (ML) algorithms are increasingly used in social domains\nto make predictions about humans, there is a growing concern that these\nalgorithms may exhibit biases against certain social groups. Numerous notions\nof fairness have been proposed in the literature to measure the unfairness of\nML. Among them, one class that receives the most attention is\n\\textit{parity-based}, i.e., achieving fairness by equalizing treatment or\noutcomes for different social groups. However, achieving parity-based fairness\noften comes at the cost of lowering model accuracy and is undesirable for many\nhigh-stakes domains like healthcare. To avoid inferior accuracy, a line of\nresearch focuses on \\textit{preference-based} fairness, under which any group\nof individuals would experience the highest accuracy and collectively prefer\nthe ML outcomes assigned to them if they were given the choice between various\nsets of outcomes. However, these works assume individual demographic\ninformation is known and fully accessible during training. In this paper, we\nrelax this requirement and propose a novel \\textit{demographic-agnostic\nfairness without harm (DAFH)} optimization algorithm, which jointly learns a\ngroup classifier that partitions the population into multiple groups and a set\nof decoupled classifiers associated with these groups. Theoretically, we\nconduct sample complexity analysis and show that our method can outperform the\nbaselines when demographic information is known and used to train decoupled\nclassifiers. Experiments on both synthetic and real data validate the proposed\nmethod.", "AI": {"tldr": "The paper proposes a demographic-agnostic fairness optimization algorithm that jointly learns group classifiers and decoupled classifiers to achieve preference-based fairness without requiring demographic information during training.", "motivation": "Current fairness approaches either sacrifice accuracy (parity-based) or require demographic information (preference-based), which is problematic for high-stakes domains like healthcare where both accuracy and fairness are crucial.", "method": "Proposed DAFH algorithm that jointly learns a group classifier to partition population into multiple groups and a set of decoupled classifiers associated with these groups, without requiring demographic information.", "result": "Theoretical analysis shows the method outperforms baselines when demographic information is known, and experiments on synthetic and real data validate the proposed approach.", "conclusion": "The demographic-agnostic approach successfully achieves preference-based fairness without requiring demographic information, maintaining accuracy while ensuring fairness in ML predictions."}}
{"id": "2509.25084", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25084", "abs": "https://arxiv.org/abs/2509.25084", "authors": ["Shuofei Qiao", "Yanqiu Zhao", "Zhisong Qiu", "Xiaobin Wang", "Jintian Zhang", "Zhao Bin", "Ningyu Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "title": "Scaling Generalist Data-Analytic Agents", "comment": "Work in progress", "summary": "Data-analytic agents are emerging as a key catalyst for automated scientific\ndiscovery and for the vision of Innovating AI. Current approaches, however,\nrely heavily on prompt engineering over proprietary models, while open-source\nmodels struggle to face diverse-format, large-scale data files and\nlong-horizon, multi-step reasoning that real-world analytics demands. This\npaper introduces DataMind, a scalable data synthesis and agent training recipe\ndesigned to build generalist data-analytic agents. DataMind tackles three key\nchallenges in building open-source data-analytic agents, including insufficient\ndata resources, improper training strategy, and unstable code-based multi-turn\nrollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a\nrecursive easy-to-hard task composition mechanism to increase the diversity and\ndifficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling\nstrategy followed by model-based and rule-based filtering; 3) a dynamically\nadjustable training objective combining both SFT and RL losses; 4) a\nmemory-frugal and stable code-based multi-turn rollout framework. Built on\nDataMind, we curate DataMind-12K, a high-quality trajectory set spanning\ndiverse domains, task categories, and data file formats for data-analytic\ntasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with\nan average score of 71.16% on multiple data analysis benchmarks, outperforming\nthe strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B\nalso performs best among all open-source models with a score of 68.10%. We also\nincorporate some empirical insights gained from our exploratory trials into the\nanalysis experiments, aiming to provide actionable insights about agentic\ntraining for the community. We will release DataMind-12K and DataMind-7B,14B\nfor the community's future research.", "AI": {"tldr": "DataMind introduces a scalable data synthesis and agent training recipe to build generalist data-analytic agents, addressing challenges in open-source models through diverse task synthesis, knowledge-augmented sampling, dynamic training objectives, and stable multi-turn rollout.", "motivation": "Current data-analytic agents heavily rely on proprietary models with prompt engineering, while open-source models struggle with diverse-format large-scale data and multi-step reasoning required for real-world analytics.", "method": "1) Fine-grained task taxonomy with recursive easy-to-hard composition 2) Knowledge-augmented trajectory sampling with filtering 3) Dynamic training combining SFT and RL losses 4) Memory-frugal stable code-based multi-turn rollout framework", "result": "DataMind-14B achieves state-of-the-art 71.16% average score on data analysis benchmarks, outperforming proprietary models like DeepSeek-V3.1 and GPT-5. DataMind-7B performs best among open-source models with 68.10% score.", "conclusion": "DataMind successfully addresses key challenges in building open-source data-analytic agents and provides a scalable solution that outperforms proprietary models, with released datasets and models for community research."}}
{"id": "2509.24085", "categories": ["cs.LG", "cs.AI", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.24085", "abs": "https://arxiv.org/abs/2509.24085", "authors": ["Ju-Hyung Lee", "Yanqing Lu", "Klaus Doppler"], "title": "PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM", "comment": null, "summary": "We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a\nframework for cooperative cross-layer optimization in device-to-device (D2D)\ncommunication. Building on our previous work on single-device on-device LLMs,\nPEARL extends the paradigm by leveraging both publisher and subscriber states\nto guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which\nnormalizes latency by application tolerances and modulates energy by device\nbattery states, provides richer supervision for KL-based finetuning. We study\ntwo lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves\nthe best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms\ninference at near-identical objective scores. Across synthetic scenarios\ngrounded in real measurements, PEARL improves objective scores over heuristic\nand compact model baselines and reduces energy by up to 16% in cooperative\nlow-battery cases. These results demonstrate that peer-aware context,\nreward-aligned training, and head-based efficiency make LLMs practical for\nalways-on, on-device cross-layer control.", "AI": {"tldr": "PEARL is a framework using on-device LLMs for cooperative D2D communication optimization, improving Wi-Fi Aware parameter selection through peer-aware context and achieving up to 16% energy reduction.", "motivation": "To extend single-device on-device LLMs to cooperative scenarios by leveraging both publisher and subscriber states for better cross-layer optimization in D2D communication.", "method": "Uses context-aware reward (normalizing latency by application tolerances and modulating energy by battery states) for KL-based finetuning. Two variants: PEARL (Head + LoRA) and PEARL-Lite (Head-only).", "result": "PEARL improves objective scores over heuristic and compact model baselines, reduces energy by up to 16% in cooperative low-battery cases, with PEARL-Lite achieving sub-20 ms inference.", "conclusion": "Peer-aware context, reward-aligned training, and head-based efficiency make LLMs practical for always-on, on-device cross-layer control in D2D communication."}}
{"id": "2509.25085", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.25085", "abs": "https://arxiv.org/abs/2509.25085", "authors": ["Feng Wang", "Yuqing Li", "Han Xiao"], "title": "jina-reranker-v3: Last but Not Late Interaction for Document Reranking", "comment": "early draft, CoIR table needs to be updated", "summary": "jina-reranker-v3 is a 0.6B parameter multilingual document reranker that\nintroduces a novel last but not late interaction. Unlike late interaction\nmodels such as ColBERT that perform separate encoding followed by multi-vector\nmatching, our approach conducts causal self-attention between query and\ndocuments within the same context window, enabling rich cross-document\ninteractions before extracting contextual embeddings from the last token of\neach document. This compact architecture achieves state-of-the-art BEIR\nperformance with 61.94 nDCG@10 while being ten times smaller than generative\nlistwise rerankers.", "AI": {"tldr": "jina-reranker-v3 is a 0.6B parameter multilingual document reranker that uses a novel 'last but not late interaction' approach, achieving state-of-the-art BEIR performance with 61.94 nDCG@10 while being 10x smaller than generative listwise rerankers.", "motivation": "To develop a more efficient reranker that enables rich cross-document interactions while maintaining compact architecture, addressing limitations of late interaction models like ColBERT.", "method": "Introduces 'last but not late interaction' - conducts causal self-attention between query and documents within the same context window, then extracts contextual embeddings from the last token of each document.", "result": "Achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while being ten times smaller than generative listwise rerankers.", "conclusion": "The proposed compact architecture with novel interaction mechanism successfully balances performance and efficiency, demonstrating superior reranking capabilities with significantly reduced model size."}}
{"id": "2509.24093", "categories": ["cs.LG", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24093", "abs": "https://arxiv.org/abs/2509.24093", "authors": ["Owen Lewis Howell", "Linfeng Zhao", "Xupeng Zhu", "Yaoyao Qian", "Haojie Huang", "Lingfeng Sun", "Wil Thomason", "Robert Platt", "Robin Walters"], "title": "Clebsch-Gordan Transformer: Fast and Global Equivariant Attention", "comment": null, "summary": "The global attention mechanism is one of the keys to the success of\ntransformer architecture, but it incurs quadratic computational costs in\nrelation to the number of tokens. On the other hand, equivariant models, which\nleverage the underlying geometric structures of problem instance, often achieve\nsuperior accuracy in physical, biochemical, computer vision, and robotic tasks,\nat the cost of additional compute requirements. As a result, existing\nequivariant transformers only support low-order equivariant features and local\ncontext windows, limiting their expressiveness and performance. This work\nproposes Clebsch-Gordan Transformer, achieving efficient global attention by a\nnovel Clebsch-Gordon Convolution on $\\SO(3)$ irreducible representations. Our\nmethod enables equivariant modeling of features at all orders while achieving\n${O}(N \\log N)$ input token complexity. Additionally, the proposed method\nscales well with high-order irreducible features, by exploiting the sparsity of\nthe Clebsch-Gordon matrix. Lastly, we also incorporate optional token\npermutation equivariance through either weight sharing or data augmentation. We\nbenchmark our method on a diverse set of benchmarks including n-body\nsimulation, QM9, ModelNet point cloud classification and a robotic grasping\ndataset, showing clear gains over existing equivariant transformers in GPU\nmemory size, speed, and accuracy.", "AI": {"tldr": "The paper proposes Clebsch-Gordan Transformer, an efficient equivariant transformer that achieves global attention with O(N log N) complexity using Clebsch-Gordon Convolution on SO(3) irreducible representations.", "motivation": "Existing equivariant transformers suffer from quadratic computational costs and limited support for high-order equivariant features, restricting their expressiveness and performance in geometric tasks.", "method": "Uses Clebsch-Gordon Convolution on SO(3) irreducible representations to enable efficient global attention, exploits sparsity of Clebsch-Gordon matrix for scalability with high-order features, and incorporates optional token permutation equivariance.", "result": "Achieves O(N log N) complexity, scales well with high-order irreducible features, and shows clear gains in GPU memory size, speed, and accuracy across n-body simulation, QM9, ModelNet, and robotic grasping benchmarks.", "conclusion": "The Clebsch-Gordan Transformer provides an efficient solution for equivariant modeling with global attention, overcoming limitations of existing methods while maintaining superior performance across diverse geometric tasks."}}
{"id": "2509.25086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25086", "abs": "https://arxiv.org/abs/2509.25086", "authors": ["Akio Hayakawa", "Stefan Bott", "Horacio Saggion"], "title": "Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small LLMs", "comment": null, "summary": "Despite their strong performance, large language models (LLMs) face\nchallenges in real-world application of lexical simplification (LS),\nparticularly in privacy-sensitive and resource-constrained environments.\nMoreover, since vulnerable user groups (e.g., people with disabilities) are one\nof the key target groups of this technology, it is crucial to ensure the safety\nand correctness of the output of LS systems. To address these issues, we\npropose an efficient framework for LS systems that utilizes small LLMs\ndeployable in local environments. Within this framework, we explore knowledge\ndistillation with synthesized data and in-context learning as baselines. Our\nexperiments in five languages evaluate model outputs both automatically and\nmanually. Our manual analysis reveals that while knowledge distillation boosts\nautomatic metric scores, it also introduces a safety trade-off by increasing\nharmful simplifications. Importantly, we find that the model's output\nprobability is a useful signal for detecting harmful simplifications.\nLeveraging this, we propose a filtering strategy that suppresses harmful\nsimplifications while largely preserving beneficial ones. This work establishes\na benchmark for efficient and safe LS with small LLMs. It highlights the key\ntrade-offs between performance, efficiency, and safety, and demonstrates a\npromising approach for safe real-world deployment.", "AI": {"tldr": "Proposes efficient lexical simplification framework using small LLMs for privacy-sensitive environments, identifies safety trade-offs in knowledge distillation, and develops filtering strategy using model probabilities to detect harmful simplifications.", "motivation": "Address challenges of LLMs in real-world lexical simplification applications, especially for vulnerable user groups in privacy-sensitive and resource-constrained environments where safety and correctness are crucial.", "method": "Utilizes small LLMs deployable locally, explores knowledge distillation with synthesized data and in-context learning as baselines, and proposes filtering strategy using model output probabilities to detect harmful simplifications.", "result": "Knowledge distillation boosts automatic metric scores but introduces safety trade-off by increasing harmful simplifications. Model's output probability is effective for detecting harmful simplifications. Filtering strategy suppresses harmful simplifications while preserving beneficial ones.", "conclusion": "Establishes benchmark for efficient and safe lexical simplification with small LLMs, highlighting key trade-offs between performance, efficiency, and safety, and demonstrates promising approach for safe real-world deployment."}}
{"id": "2509.24115", "categories": ["cs.LG", "cond-mat.mtrl-sci", "math.OC", "68Q32 (Primary), 68T07 (Secondary)"], "pdf": "https://arxiv.org/pdf/2509.24115", "abs": "https://arxiv.org/abs/2509.24115", "authors": ["Evan Dramko", "Yihuang Xiong", "Yizhi Zhu", "Geoffroy Hautier", "Thomas Reps", "Christopher Jermaine", "Anastasios Kyrillidis"], "title": "ADAPT: Lightweight, Long-Range Machine Learning Force Fields Without Graphs", "comment": "14 total pages of main content, 4 of references, 3 in Appendix", "summary": "Point defects play a central role in driving the properties of materials.\nFirst-principles methods are widely used to compute defect energetics and\nstructures, including at scale for high-throughput defect databases. However,\nthese methods are computationally expensive, making machine-learning force\nfields (MLFFs) an attractive alternative for accelerating structural\nrelaxations. Most existing MLFFs are based on graph neural networks (GNNs),\nwhich can suffer from oversmoothing and poor representation of long-range\ninteractions. Both of these issues are especially of concern when modeling\npoint defects. To address these challenges, we introduce the Accelerated Deep\nAtomic Potential Transformer (ADAPT), an MLFF that replaces graph\nrepresentations with a direct coordinates-in-space formulation and explicitly\nconsiders all pairwise atomic interactions. Atoms are treated as tokens, with a\nTransformer encoder modeling their interactions. Applied to a dataset of\nsilicon point defects, ADAPT achieves a roughly 33 percent reduction in both\nforce and energy prediction errors relative to a state-of-the-art GNN-based\nmodel, while requiring only a fraction of the computational cost.", "AI": {"tldr": "ADAPT is a Transformer-based MLFF that replaces graph representations with direct coordinates and explicit pairwise interactions, achieving 33% lower errors than GNN models for silicon point defects.", "motivation": "Traditional first-principles methods are computationally expensive for defect studies, and existing GNN-based MLFFs suffer from oversmoothing and poor long-range interaction modeling, which are critical for point defects.", "method": "ADAPT uses a Transformer encoder architecture where atoms are treated as tokens, employs direct coordinates-in-space formulation instead of graphs, and explicitly considers all pairwise atomic interactions.", "result": "ADAPT achieves approximately 33% reduction in both force and energy prediction errors compared to state-of-the-art GNN models, while requiring significantly less computational cost.", "conclusion": "The ADAPT framework provides a more effective and efficient alternative to GNN-based MLFFs for modeling point defects, addressing key limitations in oversmoothing and long-range interaction representation."}}
{"id": "2509.25106", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.25106", "abs": "https://arxiv.org/abs/2509.25106", "authors": ["Yuan Liang", "Jiaxian Li", "Yuqing Wang", "Piaohong Wang", "Motong Tian", "Pai Liu", "Shuofei Qiao", "Runnan Fang", "He Zhu", "Ge Zhang", "Minghao Liu", "Yuchen Eleanor Jiang", "Ningyu Zhang", "Wangchunshu Zhou"], "title": "Towards Personalized Deep Research: Benchmarks and Evaluations", "comment": null, "summary": "Deep Research Agents (DRAs) can autonomously conduct complex investigations\nand generate comprehensive reports, demonstrating strong real-world potential.\nHowever, existing evaluations mostly rely on close-ended benchmarks, while\nopen-ended deep research benchmarks remain scarce and typically neglect\npersonalized scenarios. To bridge this gap, we introduce Personalized Deep\nResearch Bench, the first benchmark for evaluating personalization in DRAs. It\npairs 50 diverse research tasks across 10 domains with 25 authentic user\nprofiles that combine structured persona attributes with dynamic real-world\ncontexts, yielding 250 realistic user-task queries. To assess system\nperformance, we propose the PQR Evaluation Framework, which jointly measures\n(P) Personalization Alignment, (Q) Content Quality, and (R) Factual\nReliability. Our experiments on a range of systems highlight current\ncapabilities and limitations in handling personalized deep research. This work\nestablishes a rigorous foundation for developing and evaluating the next\ngeneration of truly personalized AI research assistants.", "AI": {"tldr": "The paper introduces Personalized Deep Research Bench, the first benchmark for evaluating personalization in Deep Research Agents (DRAs), featuring 50 research tasks across 10 domains paired with 25 user profiles, and proposes the PQR Evaluation Framework to measure Personalization Alignment, Content Quality, and Factual Reliability.", "motivation": "Existing evaluations for Deep Research Agents mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios, creating a gap in assessing truly personalized AI research assistants.", "method": "The authors introduce Personalized Deep Research Bench with 50 diverse research tasks across 10 domains paired with 25 authentic user profiles combining structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. They propose the PQR Evaluation Framework to jointly measure Personalization Alignment, Content Quality, and Factual Reliability.", "result": "Experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research, demonstrating the benchmark's effectiveness in assessing system performance.", "conclusion": "This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants by providing the first comprehensive benchmark for personalization in Deep Research Agents."}}
{"id": "2509.24117", "categories": ["cs.LG", "physics.comp-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24117", "abs": "https://arxiv.org/abs/2509.24117", "authors": ["Sifan Wang", "Zhikai Wu", "David van Dijk", "Lu Lu"], "title": "GeoFunFlow: Geometric Function Flow Matching for Inverse Operator Learning over Complex Geometries", "comment": "26 pages, 13 figures, 9 tables", "summary": "Inverse problems governed by partial differential equations (PDEs) are\ncrucial in science and engineering. They are particularly challenging due to\nill-posedness, data sparsity, and the added complexity of irregular geometries.\nClassical PDE-constrained optimization methods are computationally expensive,\nespecially when repeated posterior sampling is required. Learning-based\napproaches improve efficiency and scalability, yet most are designed for\nregular domains or focus on forward modeling. Here, we introduce {\\em\nGeoFunFlow}, a geometric diffusion model framework for inverse problems on\ncomplex geometries. GeoFunFlow combines a novel geometric function autoencoder\n(GeoFAE) and a latent diffusion model trained via rectified flow. GeoFAE\nemploys a Perceiver module to process unstructured meshes of varying sizes and\nproduces continuous reconstructions of physical fields, while the diffusion\nmodel enables posterior sampling from sparse and noisy data. Across five\nbenchmarks, GeoFunFlow achieves state-of-the-art reconstruction accuracy over\ncomplex geometries, provides calibrated uncertainty quantification, and\ndelivers efficient inference compared to operator-learning and diffusion model\nbaselines.", "AI": {"tldr": "GeoFunFlow is a geometric diffusion model framework for solving inverse problems on complex geometries, combining a geometric function autoencoder with latent diffusion to achieve state-of-the-art reconstruction accuracy and uncertainty quantification.", "motivation": "Inverse problems governed by PDEs are challenging due to ill-posedness, data sparsity, and irregular geometries. Classical methods are computationally expensive, while existing learning-based approaches are limited to regular domains or forward modeling.", "method": "GeoFunFlow combines a geometric function autoencoder (GeoFAE) using Perceiver modules to process unstructured meshes with a latent diffusion model trained via rectified flow for posterior sampling from sparse noisy data.", "result": "Across five benchmarks, GeoFunFlow achieves state-of-the-art reconstruction accuracy over complex geometries, provides calibrated uncertainty quantification, and delivers efficient inference compared to operator-learning and diffusion model baselines.", "conclusion": "The proposed framework successfully addresses inverse problems on complex geometries with improved accuracy, uncertainty quantification, and computational efficiency over existing methods."}}
{"id": "2509.25107", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25107", "abs": "https://arxiv.org/abs/2509.25107", "authors": ["Kai Sun", "Yin Huang", "Srishti Mehra", "Mohammad Kachuee", "Xilun Chen", "Renjie Tao", "Zhaojiang Lin", "Andrea Jessee", "Nirav Shah", "Alex Betty", "Yue Liu", "Anuj Kumar", "Wen-tau Yih", "Xin Luna Dong"], "title": "Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant for Question Answering in the Era of LLMs?", "comment": null, "summary": "The advent of Large Language Models (LLMs) has significantly advanced\nweb-based Question Answering (QA) systems over semi-structured content, raising\nquestions about the continued utility of knowledge extraction for question\nanswering. This paper investigates the value of triple extraction in this new\nparadigm by extending an existing benchmark with knowledge extraction\nannotations and evaluating commercial and open-source LLMs of varying sizes.\nOur results show that web-scale knowledge extraction remains a challenging task\nfor LLMs. Despite achieving high QA accuracy, LLMs can still benefit from\nknowledge extraction, through augmentation with extracted triples and\nmulti-task learning. These findings provide insights into the evolving role of\nknowledge triple extraction in web-based QA and highlight strategies for\nmaximizing LLM effectiveness across different model sizes and resource\nsettings.", "AI": {"tldr": "This paper investigates whether knowledge triple extraction remains useful for web-based QA in the era of LLMs, finding that despite high QA accuracy, LLMs still benefit from knowledge extraction through augmentation and multi-task learning.", "motivation": "With LLMs advancing web-based QA systems, the paper questions the continued utility of knowledge extraction for QA and aims to determine its value in this new paradigm.", "method": "Extends an existing benchmark with knowledge extraction annotations and evaluates commercial and open-source LLMs of varying sizes, testing augmentation with extracted triples and multi-task learning approaches.", "result": "Web-scale knowledge extraction remains challenging for LLMs, but LLMs can benefit from knowledge extraction through triple augmentation and multi-task learning, even when achieving high QA accuracy.", "conclusion": "Knowledge triple extraction still has value in web-based QA systems, and the findings provide strategies for maximizing LLM effectiveness across different model sizes and resource constraints."}}
{"id": "2509.24118", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24118", "abs": "https://arxiv.org/abs/2509.24118", "authors": ["Md Mozaharul Mottalib", "Thao-Ly T. Phan", "Rahmatollah Beheshti"], "title": "HyMaTE: A Hybrid Mamba and Transformer Model for EHR Representation Learning", "comment": null, "summary": "Electronic health Records (EHRs) have become a cornerstone in modern-day\nhealthcare. They are a crucial part for analyzing the progression of patient\nhealth; however, their complexity, characterized by long, multivariate\nsequences, sparsity, and missing values poses significant challenges in\ntraditional deep learning modeling. While Transformer-based models have\ndemonstrated success in modeling EHR data and predicting clinical outcomes,\ntheir quadratic computational complexity and limited context length hinder\ntheir efficiency and practical applications. On the other hand, State Space\nModels (SSMs) like Mamba present a promising alternative offering linear-time\nsequence modeling and improved efficiency for handling long sequences, but\nfocus mostly on mixing sequence-level information rather than channel-level\ndata. To overcome these challenges, we propose HyMaTE (A Hybrid Mamba and\nTransformer Model for EHR Representation Learning), a novel hybrid model\ntailored for representing longitudinal data, combining the strengths of SSMs\nwith advanced attention mechanisms. By testing the model on predictive tasks on\nmultiple clinical datasets, we demonstrate HyMaTE's ability to capture an\neffective, richer, and more nuanced unified representation of EHR data.\nAdditionally, the interpretability of the outcomes achieved by self-attention\nillustrates the effectiveness of our model as a scalable and generalizable\nsolution for real-world healthcare applications. Codes are available at:\nhttps://github.com/healthylaife/HyMaTE.", "AI": {"tldr": "HyMaTE is a hybrid model combining State Space Models (Mamba) and Transformers for EHR representation learning, addressing limitations of both approaches to handle long, multivariate EHR sequences more efficiently while capturing richer representations.", "motivation": "EHR data presents challenges with long sequences, sparsity, and missing values. Transformers have quadratic complexity limitations, while SSMs like Mamba focus mainly on sequence-level mixing rather than channel-level data. A hybrid approach is needed to overcome these limitations.", "method": "Proposed HyMaTE - a hybrid model combining State Space Models (specifically Mamba) with attention mechanisms from Transformers, designed specifically for longitudinal EHR data representation learning.", "result": "The model was tested on multiple clinical datasets for predictive tasks and demonstrated ability to capture effective, richer, and more nuanced unified representations of EHR data. The self-attention mechanism also provided interpretable outcomes.", "conclusion": "HyMaTE provides a scalable and generalizable solution for real-world healthcare applications, effectively combining the linear-time efficiency of SSMs with the representational power of attention mechanisms."}}
{"id": "2509.25138", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25138", "abs": "https://arxiv.org/abs/2509.25138", "authors": ["Ivan Vykopal", "Antonia Karamolegkou", "Jaroslav Kop\u010dan", "Qiwei Peng", "Tom\u00e1\u0161 Jav\u016frek", "Michal Gregor", "Mari\u00e1n \u0160imko"], "title": "Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection", "comment": null, "summary": "Multilingual Large Language Models (LLMs) offer powerful capabilities for\ncross-lingual fact-checking. However, these models often exhibit language bias,\nperforming disproportionately better on high-resource languages such as English\nthan on low-resource counterparts. We also present and inspect a novel concept\n- retrieval bias, when information retrieval systems tend to favor certain\ninformation over others, leaving the retrieval process skewed. In this paper,\nwe study language and retrieval bias in the context of Previously Fact-Checked\nClaim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20\nlanguages using a fully multilingual prompting strategy, leveraging the AMC-16K\ndataset. By translating task prompts into each language, we uncover disparities\nin monolingual and cross-lingual performance and identify key trends based on\nmodel family, size, and prompting strategy. Our findings highlight persistent\nbias in LLM behavior and offer recommendations for improving equity in\nmultilingual fact-checking. To investigate retrieval bias, we employed\nmultilingual embedding models and look into the frequency of retrieved claims.\nOur analysis reveals that certain claims are retrieved disproportionately\nacross different posts, leading to inflated retrieval performance for popular\nclaims while under-representing less common ones.", "AI": {"tldr": "This paper studies language bias and retrieval bias in multilingual LLMs for cross-lingual fact-checking, evaluating 6 models across 20 languages using AMC-16K dataset and multilingual prompting.", "motivation": "Multilingual LLMs show language bias, performing better on high-resource languages like English than low-resource languages, and retrieval systems exhibit retrieval bias by favoring certain information over others.", "method": "Evaluated 6 open-source multilingual LLMs across 20 languages using fully multilingual prompting strategy with AMC-16K dataset, translated prompts into each language, and used multilingual embedding models to analyze retrieval bias.", "result": "Found persistent bias in LLM behavior with disparities in monolingual and cross-lingual performance, identified trends by model family, size, and prompting strategy, and revealed retrieval bias where certain claims are retrieved disproportionately.", "conclusion": "Highlights persistent biases in multilingual fact-checking systems and provides recommendations for improving equity in multilingual fact-checking through better understanding of language and retrieval biases."}}
{"id": "2509.24122", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24122", "abs": "https://arxiv.org/abs/2509.24122", "authors": ["Hongbo Liu", "Jia Xu"], "title": "Echo Flow Networks", "comment": "Under Review", "summary": "At the heart of time-series forecasting (TSF) lies a fundamental challenge:\nhow can models efficiently and effectively capture long-range temporal\ndependencies across ever-growing sequences? While deep learning has brought\nnotable progress, conventional architectures often face a trade-off between\ncomputational complexity and their ability to retain accumulative information\nover extended horizons.\n  Echo State Networks (ESNs), a class of reservoir computing models, have\nrecently regained attention for their exceptional efficiency, offering constant\nmemory usage and per-step training complexity regardless of input length. This\nmakes them particularly attractive for modeling extremely long-term event\nhistory in TSF. However, traditional ESNs fall short of state-of-the-art\nperformance due to their limited nonlinear capacity, which constrains both\ntheir expressiveness and stability.\n  We introduce Echo Flow Networks (EFNs), a framework composed of a group of\nextended Echo State Networks (X-ESNs) with MLP readouts, enhanced by our novel\nMatrix-Gated Composite Random Activation (MCRA), which enables complex,\nneuron-specific temporal dynamics, significantly expanding the network's\nrepresentational capacity without compromising computational efficiency. In\naddition, we propose a dual-stream architecture in which recent input history\ndynamically selects signature reservoir features from an infinite-horizon\nmemory, leading to improved prediction accuracy and long-term stability.\n  Extensive evaluations on five benchmarks demonstrate that EFNs achieve up to\n4x faster training and 3x smaller model size compared to leading methods like\nPatchTST, reducing forecasting error from 43% to 35%, a 20% relative\nimprovement. One instantiation of our framework, EchoFormer, consistently\nachieves new state-of-the-art performance across five benchmark datasets: ETTh,\nETTm, DMV, Weather, and Air Quality.", "AI": {"tldr": "Echo Flow Networks (EFNs) introduce a novel reservoir computing framework that combines extended Echo State Networks with MLP readouts and Matrix-Gated Composite Random Activation, achieving superior time-series forecasting performance with significantly improved efficiency and accuracy.", "motivation": "To address the fundamental challenge in time-series forecasting of capturing long-range temporal dependencies efficiently, overcoming the limitations of conventional architectures and traditional Echo State Networks which face computational complexity vs. information retention trade-offs.", "method": "Proposed Echo Flow Networks framework with extended Echo State Networks (X-ESNs) using MLP readouts, enhanced by Matrix-Gated Composite Random Activation (MCRA) for complex neuron-specific temporal dynamics, and a dual-stream architecture that dynamically selects reservoir features from infinite-horizon memory.", "result": "EFNs achieve up to 4x faster training and 3x smaller model size compared to leading methods like PatchTST, reducing forecasting error from 43% to 35% (20% relative improvement). EchoFormer, an instantiation of EFNs, achieves state-of-the-art performance across five benchmark datasets.", "conclusion": "EFNs provide an efficient and effective solution for long-range time-series forecasting, significantly expanding representational capacity while maintaining computational efficiency, demonstrating superior performance across multiple benchmarks."}}
{"id": "2509.25144", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25144", "abs": "https://arxiv.org/abs/2509.25144", "authors": ["Yen-Ju Lu", "Thomas Thebaud", "Laureano Moro-Velazquez", "Najim Dehak", "Jesus Villalba"], "title": "Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs for Low-Resource Text Generation", "comment": "Accepted at EMNLP 2025 (Main Conference)", "summary": "We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline\nthat synthesizes accurate input-output pairs without human labels or parallel\ndata. In many low-resource natural language generation (NLG) scenarios,\npractitioners may have only raw outputs, like highlights, recaps, or questions,\nor only raw inputs, such as articles, dialogues, or paragraphs, but seldom\nboth. This mismatch forces small models to learn from very few examples or rely\non costly, broad-scope synthetic examples produced by large LLMs. PbT addresses\nthis by asking a teacher LLM to compress each unpaired example into a concise\nintermediate representation (IR), and training a student to reconstruct inputs\nfrom IRs. This enables outputs to be paired with student-generated inputs,\nyielding high-quality synthetic data. We evaluate PbT on five\nbenchmarks-document summarization (XSum, CNNDM), dialogue summarization\n(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired\nsetting on SwitchBoard (paired with DialogSum summaries). An 8B student trained\nonly on PbT data outperforms models trained on 70 B teacher-generated corpora\nand other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated\npairs and closing 82% of the oracle gap at one-third the annotation cost of\ndirect synthesis. Human evaluation on SwitchBoard further confirms that only\nPbT produces concise, faithful summaries aligned with the target style,\nhighlighting its advantage of generating in-domain sources that avoid the\nmismatch, limiting direct synthesis.", "AI": {"tldr": "PbT is a teacher-student pipeline that synthesizes accurate input-output pairs without human labels or parallel data by using teacher LLMs to compress unpaired examples into intermediate representations, then training students to reconstruct inputs from these representations.", "motivation": "Address the problem in low-resource NLG scenarios where practitioners often have only raw outputs (highlights, recaps, questions) or only raw inputs (articles, dialogues, paragraphs) but seldom both, forcing small models to learn from few examples or rely on costly synthetic data from large LLMs.", "method": "Two-stage teacher-student pipeline: 1) Teacher LLM compresses each unpaired example into concise intermediate representation (IR), 2) Student model is trained to reconstruct inputs from IRs, enabling outputs to be paired with student-generated inputs to create high-quality synthetic data.", "result": "8B student trained only on PbT data outperforms models trained on 70B teacher-generated corpora and other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated pairs and closing 82% of the oracle gap at one-third the annotation cost of direct synthesis. Human evaluation confirms PbT produces concise, faithful summaries aligned with target style.", "conclusion": "PbT effectively addresses the data scarcity problem in low-resource NLG by generating in-domain sources that avoid the mismatch limitations of direct synthesis, providing high-quality synthetic data at reduced cost."}}
{"id": "2509.24125", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24125", "abs": "https://arxiv.org/abs/2509.24125", "authors": ["Rohan Alur", "Chris Hays", "Manish Raghavan", "Devavrat Shah"], "title": "The Impossibility of Inverse Permutation Learning in Transformer Models", "comment": null, "summary": "In this technical note, we study the problem of inverse permutation learning\nin decoder-only transformers. Given a permutation and a string to which that\npermutation has been applied, the model is tasked with producing the original\n(``canonical'') string. We argue that this task models a natural robustness\nproperty across a variety of reasoning tasks, including long-context retrieval,\nmultiple choice QA and in-context learning. Our primary contribution is an\nimpossibility result: we show that an arbitrary depth, decoder-only transformer\ncannot learn this task. This result concerns the expressive capacity of\ndecoder-only transformer models and is agnostic to training dynamics or sample\ncomplexity. We give a pair of alternative constructions under which inverse\npermutation learning is feasible. The first of these highlights the fundamental\nrole of the causal attention mask, and reveals a gap between the expressivity\nof encoder-decoder transformers and the more popular decoder-only architecture.\nThe latter result is more surprising: we show that simply padding the input\nwith ``scratch tokens\" yields a construction under which inverse permutation\nlearning is possible. We conjecture that this may suggest an alternative\nmechanism by which chain-of-thought prompting or, more generally, intermediate\n``thinking'' tokens can enable reasoning in large language models, even when\nthese tokens encode no meaningful semantic information (e.g., the results of\nintermediate computations).", "AI": {"tldr": "Decoder-only transformers cannot learn inverse permutation tasks, but become capable with causal attention masks or scratch token padding, suggesting alternative reasoning mechanisms in LLMs.", "motivation": "To study the robustness property of transformer models across reasoning tasks like long-context retrieval and multiple choice QA through inverse permutation learning.", "method": "Analyze the expressive capacity of decoder-only transformers for inverse permutation learning, examine alternative constructions using causal attention masks and scratch token padding.", "result": "Proved impossibility of inverse permutation learning in arbitrary depth decoder-only transformers, but showed feasibility with causal attention masks or input padding with scratch tokens.", "conclusion": "There's an expressivity gap between encoder-decoder and decoder-only transformers, and scratch tokens may enable reasoning in LLMs even without semantic meaning, similar to chain-of-thought prompting."}}
{"id": "2509.25149", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25149", "abs": "https://arxiv.org/abs/2509.25149", "authors": ["NVIDIA", "Felix Abecassis", "Anjulie Agrusa", "Dong Ahn", "Jonah Alben", "Stefania Alborghetti", "Michael Andersch", "Sivakumar Arayandi", "Alexis Bjorlin", "Aaron Blakeman", "Evan Briones", "Ian Buck", "Bryan Catanzaro", "Jinhang Choi", "Mike Chrzanowski", "Eric Chung", "Victor Cui", "Steve Dai", "Bita Darvish Rouhani", "Carlo del Mundo", "Deena Donia", "Burc Eryilmaz", "Henry Estela", "Abhinav Goel", "Oleg Goncharov", "Yugi Guvvala", "Robert Hesse", "Russell Hewett", "Herbert Hum", "Ujval Kapasi", "Brucek Khailany", "Mikail Khona", "Nick Knight", "Alex Kondratenko", "Ronny Krashinsky", "Ben Lanir", "Simon Layton", "Michael Lightstone", "Daniel Lo", "Paulius Micikevicius", "Asit Mishra", "Tim Moon", "Deepak Narayanan", "Chao Ni", "Abhijit Paithankar", "Satish Pasumarthi", "Ankit Patel", "Mostofa Patwary", "Ashwin Poojary", "Gargi Prasad", "Sweta Priyadarshi", "Yigong Qin", "Xiaowei Ren", "Oleg Rybakov", "Charbel Sakr", "Sanjeev Satheesh", "Stas Sergienko", "Pasha Shamis", "Kirthi Shankar", "Nishant Sharma", "Mohammad Shoeybi", "Michael Siu", "Misha Smelyanskiy", "Darko Stosic", "Dusan Stosic", "Bor-Yiing Su", "Frank Sun", "Nima Tajbakhsh", "Shelby Thomas", "Przemek Tredak", "Evgeny Tsykunov", "Gandhi Vaithilingam", "Aditya Vavre", "Rangharajan Venkatesan", "Roger Waleffe", "Qiyu Wan", "Hexin Wang", "Mengdi Wang", "Lizzie Wei", "Hao Wu", "Evan Wu", "Keith Wyss", "Ning Xu", "Jinze Xue", "Charlene Yang", "Yujia Zhai", "Ruoxi Zhang", "Jingyang Zhu", "Zhongbo Zhu"], "title": "Pretraining Large Language Models with NVFP4", "comment": null, "summary": "Large Language Models (LLMs) today are powerful problem solvers across many\ndomains, and they continue to get stronger as they scale in model size,\ntraining set size, and training set quality, as shown by extensive research and\nexperimentation across the industry. Training a frontier model today requires\non the order of tens to hundreds of yottaflops, which is a massive investment\nof time, compute, and energy. Improving pretraining efficiency is therefore\nessential to enable the next generation of even more capable LLMs. While 8-bit\nfloating point (FP8) training is now widely adopted, transitioning to even\nnarrower precision, such as 4-bit floating point (FP4), could unlock additional\nimprovements in computational speed and resource utilization. However,\nquantization at this level poses challenges to training stability, convergence,\nand implementation, notably for large-scale models trained on long token\nhorizons.\n  In this study, we introduce a novel approach for stable and accurate training\nof large language models (LLMs) using the NVFP4 format. Our method integrates\nRandom Hadamard transforms (RHT) to bound block-level outliers, employs a\ntwo-dimensional quantization scheme for consistent representations across both\nthe forward and backward passes, utilizes stochastic rounding for unbiased\ngradient estimation, and incorporates selective high-precision layers. We\nvalidate our approach by training a 12-billion-parameter model on 10 trillion\ntokens -- the longest publicly documented training run in 4-bit precision to\ndate. Our results show that the model trained with our NVFP4-based pretraining\ntechnique achieves training loss and downstream task accuracies comparable to\nan FP8 baseline. These findings highlight that NVFP4, when combined with our\ntraining approach, represents a major step forward in narrow-precision LLM\ntraining algorithms.", "AI": {"tldr": "Introduces a novel approach for stable 4-bit floating point (FP4) training of large language models using NVFP4 format with Random Hadamard transforms, two-dimensional quantization, stochastic rounding, and selective high-precision layers, achieving comparable performance to FP8 baseline on a 12B parameter model trained on 10T tokens.", "motivation": "To improve pretraining efficiency for next-generation LLMs by transitioning from 8-bit to 4-bit floating point training, which could unlock additional computational speed and resource utilization benefits, while addressing challenges of training stability and convergence at such narrow precision.", "method": "Uses NVFP4 format with Random Hadamard transforms to bound block-level outliers, two-dimensional quantization scheme for consistent forward/backward pass representations, stochastic rounding for unbiased gradient estimation, and selective high-precision layers.", "result": "Successfully trained a 12-billion-parameter model on 10 trillion tokens - the longest publicly documented 4-bit training run - achieving training loss and downstream task accuracies comparable to FP8 baseline.", "conclusion": "NVFP4 combined with the proposed training approach represents a major step forward in narrow-precision LLM training algorithms, enabling efficient 4-bit training while maintaining model quality."}}
{"id": "2509.24140", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24140", "abs": "https://arxiv.org/abs/2509.24140", "authors": ["H. N. Mhaskar", "Ryan O'Dowd"], "title": "A signal separation view of classification", "comment": null, "summary": "The problem of classification in machine learning has often been approached\nin terms of function approximation. In this paper, we propose an alternative\napproach for classification in arbitrary compact metric spaces which, in\ntheory, yields both the number of classes, and a perfect classification using a\nminimal number of queried labels. Our approach uses localized trigonometric\npolynomial kernels initially developed for the point source signal separation\nproblem in signal processing. Rather than point sources, we argue that the\nvarious classes come from different probability distributions. The localized\nkernel technique developed for separating point sources is then shown to\nseparate the supports of these distributions. This is done in a hierarchical\nmanner in our MASC algorithm to accommodate touching/overlapping class\nboundaries. We illustrate our theory on several simulated and real life\ndatasets, including the Salinas and Indian Pines hyperspectral datasets and a\ndocument dataset.", "AI": {"tldr": "A novel classification approach using localized trigonometric polynomial kernels to separate class distributions in compact metric spaces, achieving perfect classification with minimal labels.", "motivation": "Traditional classification methods rely on function approximation, but this paper proposes an alternative that can determine the number of classes and achieve perfect classification using minimal labeled data.", "method": "Uses localized trigonometric polynomial kernels originally developed for signal processing to separate class distributions. Implements hierarchical MASC algorithm to handle touching/overlapping class boundaries.", "result": "Successfully demonstrated on simulated and real datasets including Salinas and Indian Pines hyperspectral datasets and document datasets, showing effective class separation.", "conclusion": "The proposed kernel-based approach provides an effective alternative to function approximation methods for classification, capable of determining class numbers and achieving perfect classification with minimal labeled data."}}
{"id": "2509.25175", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25175", "abs": "https://arxiv.org/abs/2509.25175", "authors": ["Haolei Xu", "Xinyu Mei", "Yuchen Yan", "Rui Zhou", "Wenqi Zhang", "Weiming Lu", "Yueting Zhuang", "Yongliang Shen"], "title": "EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering", "comment": "project: https://github.com/ZJU-REAL/EasySteer", "summary": "Large language model (LLM) steering has emerged as a promising paradigm for\ncontrolling model behavior at inference time through targeted manipulation of\nhidden states, offering a lightweight alternative to expensive retraining.\nHowever, existing steering frameworks suffer from critical limitations:\ncomputational inefficiency, limited extensibility, and restricted functionality\nthat hinder both research progress and practical deployment. We present\nEasySteer, a unified framework for high-performance, extensible LLM steering\nbuilt on vLLM. Our system features modular architecture with pluggable\ninterfaces for both analysis-based and learning-based methods, fine-grained\nparameter control, pre-computed steering vectors for eight application domains,\nand an interactive demonstration system. Through deep integration with vLLM's\noptimized inference engine, EasySteer achieves 5.5-11.4$\\times$ speedup over\nexisting frameworks. Extensive experiments demonstrate its effectiveness in\noverthinking mitigation, hallucination reduction, and other key applications.\nEasySteer transforms steering from research technique to production-ready\ncapability, establishing critical infrastructure for deployable, controllable\nlanguage models.", "AI": {"tldr": "EasySteer is a unified framework for efficient LLM steering that achieves 5.5-11.4x speedup over existing methods through vLLM integration, offering modular architecture and pre-computed steering vectors for various applications.", "motivation": "Existing LLM steering frameworks suffer from computational inefficiency, limited extensibility, and restricted functionality, hindering both research progress and practical deployment.", "method": "Built on vLLM with modular architecture featuring pluggable interfaces for analysis-based and learning-based methods, fine-grained parameter control, pre-computed steering vectors for eight domains, and interactive demonstration system.", "result": "Achieves 5.5-11.4x speedup over existing frameworks and demonstrates effectiveness in overthinking mitigation, hallucination reduction, and other key applications.", "conclusion": "EasySteer transforms steering from research technique to production-ready capability, establishing critical infrastructure for deployable, controllable language models."}}
{"id": "2509.24146", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24146", "abs": "https://arxiv.org/abs/2509.24146", "authors": ["Ethan Zachary Lo", "Dan Chie-Tien Lo"], "title": "Evaluation of Machine and Deep Learning Techniques for Cyclone Trajectory Regression and Status Classification by Time Series Data", "comment": null, "summary": "Accurate cyclone forecasting is essential for minimizing loss of life,\ninfrastructure damage, and economic disruption. Traditional numerical weather\nprediction models, though effective, are computationally intensive and prone to\nerror due to the chaotic nature of atmospheric systems. This study proposes a\nmachine learning (ML) approach to forecasting tropical cyclone trajectory and\nstatus using time series data from the National Hurricane Center, including\nrecently added best track wind radii. A two-stage ML pipeline is developed: a\nregression model first predicts cyclone features maximum wind speed, minimum\npressure, trajectory length, and directional change using a sliding window of\nhistorical data. These outputs are then input into classification models to\npredict the cyclone's categorical status. Gradient boosting regression and\nthree classifiers random forest (RF), support vector machine (SVM), and\nmultilayer perceptron (MLP) are evaluated. After hyperparameter tuning and\nsynthetic minority oversampling (SMOTE), the RF classifier achieves the highest\nperformance with 93% accuracy, outperforming SVM and MLP across precision,\nrecall, and F1 score. The RF model is particularly robust in identifying\nminority cyclone statuses and minimizing false negatives. Regression results\nyield low mean absolute errors, with pressure and wind predictions within about\n2.2 mb and 2.4 kt, respectively. These findings demonstrate that ML models,\nespecially ensemble-based classifiers, offer an effective, scalable alternative\nto traditional forecasting methods, with potential for real-time cyclone\nprediction and integration into decision support systems.", "AI": {"tldr": "This study proposes a machine learning approach for tropical cyclone forecasting using a two-stage pipeline: regression for cyclone features and classification for status prediction, achieving 93% accuracy with random forest classifier.", "motivation": "Traditional numerical weather prediction models are computationally intensive and error-prone due to atmospheric chaos. ML offers a more efficient and accurate alternative for cyclone forecasting to minimize life and economic losses.", "method": "Two-stage ML pipeline: 1) Gradient boosting regression predicts cyclone features (wind speed, pressure, trajectory) from historical time series data; 2) Classification models (RF, SVM, MLP) predict cyclone status using regression outputs, with SMOTE for handling imbalanced data.", "result": "Random forest classifier achieved 93% accuracy, outperforming SVM and MLP. Regression predictions had low errors: pressure within 2.2 mb and wind speed within 2.4 kt. RF was particularly effective for minority classes and minimizing false negatives.", "conclusion": "ML models, especially ensemble-based classifiers like random forest, provide an effective and scalable alternative to traditional cyclone forecasting methods, with potential for real-time prediction and integration into decision support systems."}}
{"id": "2509.25179", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25179", "abs": "https://arxiv.org/abs/2509.25179", "authors": ["Penghai Zhao", "Jinyu Tian", "Qinghua Xing", "Xin Zhang", "Zheng Li", "Jianjun Qian", "Ming-Ming Cheng", "Xiang Li"], "title": "NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation", "comment": "NAIPv2 complements our earlier work NAIPv1 (arXiv:2408.03934).\n  Whereas NAIPv1 addressed citation count-based impact prediction, NAIPv2\n  estimates research quality using peer review data", "summary": "The ability to estimate the quality of scientific papers is central to how\nboth humans and AI systems will advance scientific knowledge in the future.\nHowever, existing LLM-based estimation methods suffer from high inference cost,\nwhereas the faster direct score regression approach is limited by scale\ninconsistencies. We present NAIPv2, a debiased and efficient framework for\npaper quality estimation. NAIPv2 employs pairwise learning within domain-year\ngroups to reduce inconsistencies in reviewer ratings and introduces the Review\nTendency Signal (RTS) as a probabilistic integration of reviewer scores and\nconfidences. To support training and evaluation, we further construct NAIDv2, a\nlarge-scale dataset of 24,276 ICLR submissions enriched with metadata and\ndetailed structured content. Trained on pairwise comparisons but enabling\nefficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art\nperformance (78.2% AUC, 0.432 Spearman), while maintaining scalable,\nlinear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it\nfurther demonstrates strong generalization, with predicted scores increasing\nconsistently across decision categories from Rejected to Oral. These findings\nestablish NAIPv2 as a debiased and scalable framework for automated paper\nquality estimation, marking a step toward future scientific intelligence\nsystems. Code and dataset are released at\nhttps://sway.cloud.microsoft/Pr42npP80MfPhvj8.", "AI": {"tldr": "NAIPv2 is a debiased and efficient framework for scientific paper quality estimation that uses pairwise learning within domain-year groups to address scale inconsistencies in reviewer ratings, achieving state-of-the-art performance with linear-time inference efficiency.", "motivation": "Existing LLM-based estimation methods have high inference costs, while faster direct score regression suffers from scale inconsistencies in reviewer ratings, creating a need for a debiased and efficient paper quality estimation framework.", "method": "NAIPv2 employs pairwise learning within domain-year groups to reduce rating inconsistencies and introduces Review Tendency Signal (RTS) as probabilistic integration of reviewer scores and confidences. It's trained on pairwise comparisons but enables efficient pointwise prediction at deployment.", "result": "Achieves state-of-the-art performance (78.2% AUC, 0.432 Spearman) on ICLR submissions and demonstrates strong generalization on unseen NeurIPS submissions, with predicted scores consistently increasing across decision categories from Rejected to Oral.", "conclusion": "NAIPv2 establishes a debiased and scalable framework for automated paper quality estimation, representing progress toward future scientific intelligence systems."}}
{"id": "2509.24166", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24166", "abs": "https://arxiv.org/abs/2509.24166", "authors": ["Arpit Garg", "Hemanth Saratchandran", "Ravi Garg", "Simon Lucey"], "title": "Stable Forgetting: Bounded Parameter-Efficient Unlearning in LLMs", "comment": "In Submission", "summary": "Machine unlearning in large language models (LLMs) is essential for privacy\nand safety; however, existing approaches remain unstable and unreliable. A\nwidely used strategy, the gradient difference method, applies gradient descent\non retained data while performing gradient ascent on forget data, the data\nwhose influence should be removed. However, when combined with cross-entropy\nloss, this procedure causes unbounded growth of weights and gradients, leading\nto training instability and degrading both forgetting and retention. We provide\na theoretical framework that explains this failure, explicitly showing how\nascent on the forget set destabilizes optimization in the feedforward MLP\nlayers of LLMs. Guided by this insight, we propose Bounded Parameter-Efficient\nUnlearning, a parameter-efficient approach that stabilizes LoRA-based\nfine-tuning by applying bounded functions to MLP adapters. This simple\nmodification controls the weight dynamics during ascent, enabling the gradient\ndifference method to converge reliably. Across the TOFU, TDEC, and MUSE\nbenchmarks, and across architectures and scales from 125M to 8B parameters, our\nmethod achieves substantial improvements in forgetting while preserving\nretention, establishing a novel theoretically grounded and practically scalable\nframework for unlearning in LLMs.", "AI": {"tldr": "Proposes Bounded Parameter-Efficient Unlearning, a stable method for machine unlearning in LLMs that addresses gradient instability in existing gradient difference approaches by applying bounded functions to MLP adapters.", "motivation": "Existing machine unlearning methods in LLMs are unstable and unreliable, particularly the gradient difference method which causes unbounded weight growth and training instability when combined with cross-entropy loss.", "method": "A parameter-efficient approach that stabilizes LoRA-based fine-tuning by applying bounded functions to MLP adapters, controlling weight dynamics during gradient ascent on forget data.", "result": "Achieves substantial improvements in forgetting while preserving retention across TOFU, TDEC, and MUSE benchmarks, working reliably across architectures and scales from 125M to 8B parameters.", "conclusion": "Establishes a theoretically grounded and practically scalable framework for stable machine unlearning in large language models."}}
{"id": "2509.25184", "categories": ["cs.CL", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.25184", "abs": "https://arxiv.org/abs/2509.25184", "authors": ["Yanchen Jiang", "Zhe Feng", "Aranyak Mehta"], "title": "Incentive-Aligned Multi-Source LLM Summaries", "comment": null, "summary": "Large language models (LLMs) are increasingly used in modern search and\nanswer systems to synthesize multiple, sometimes conflicting, texts into a\nsingle response, yet current pipelines offer weak incentives for sources to be\naccurate and are vulnerable to adversarial content. We introduce Truthful Text\nSummarization (TTS), an incentive-aligned framework that improves factual\nrobustness without ground-truth labels. TTS (i) decomposes a draft synthesis\ninto atomic claims, (ii) elicits each source's stance on every claim, (iii)\nscores sources with an adapted multi-task peer-prediction mechanism that\nrewards informative agreement, and (iv) filters unreliable sources before\nre-summarizing. We establish formal guarantees that align a source's incentives\nwith informative honesty, making truthful reporting the utility-maximizing\nstrategy. Experiments show that TTS improves factual accuracy and robustness\nwhile preserving fluency, aligning exposure with informative corroboration and\ndisincentivizing manipulation.", "AI": {"tldr": "TTS is an incentive-aligned framework for truthful text summarization that decomposes draft summaries into claims, elicits source stances, scores sources using peer-prediction, and filters unreliable sources before re-summarizing.", "motivation": "Current LLM-based summarization pipelines have weak incentives for source accuracy and are vulnerable to adversarial content, lacking mechanisms to ensure factual robustness.", "method": "Four-step framework: (1) decompose draft synthesis into atomic claims, (2) elicit each source's stance on every claim, (3) score sources using multi-task peer-prediction mechanism rewarding informative agreement, (4) filter unreliable sources before re-summarizing.", "result": "TTS improves factual accuracy and robustness while preserving fluency, aligns exposure with informative corroboration, and disincentivizes manipulation.", "conclusion": "The framework establishes formal guarantees that align source incentives with informative honesty, making truthful reporting the utility-maximizing strategy without requiring ground-truth labels."}}
{"id": "2509.24168", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24168", "abs": "https://arxiv.org/abs/2509.24168", "authors": ["Qipeng Zhan", "Zhuoping Zhou", "Zexuan Wang", "Li Shen"], "title": "Multi-Scale Geometric Autoencoder", "comment": null, "summary": "Autoencoders have emerged as powerful models for visualization and\ndimensionality reduction based on the fundamental assumption that\nhigh-dimensional data is generated from a low-dimensional manifold. A critical\nchallenge in autoencoder design is to preserve the geometric structure of data\nin the latent space, with existing approaches typically focusing on either\nglobal or local geometric properties separately. Global approaches often\nencounter errors in distance approximation that accumulate, while local methods\nfrequently converge to suboptimal solutions that distort large-scale\nrelationships. We propose Multi-Scale Geometric Autoencoder (MAE), which\nintroduces an asymmetric architecture that simultaneously preserves both scales\nof the geometric structure by applying global distance constraints to the\nencoder and local geometric constraints to the decoder. Through theoretical\nanalysis, we establish that this asymmetric design aligns naturally with the\ndistinct roles of the encoder and decoder components. Our comprehensive\nexperiments on both synthetic manifolds and real-world datasets demonstrate\nthat MAE consistently outperforms existing methods across various evaluation\nmetrics.", "AI": {"tldr": "MAE is an asymmetric autoencoder that preserves both global and local geometric structures by applying global distance constraints to encoder and local constraints to decoder, outperforming existing methods.", "motivation": "Existing autoencoders struggle to preserve both global and local geometric structures simultaneously - global methods accumulate distance errors while local methods distort large-scale relationships.", "method": "Proposed Multi-Scale Geometric Autoencoder (MAE) with asymmetric architecture: global distance constraints on encoder and local geometric constraints on decoder.", "result": "MAE consistently outperforms existing methods across various evaluation metrics on both synthetic manifolds and real-world datasets.", "conclusion": "The asymmetric design naturally aligns with encoder-decoder roles and effectively preserves multi-scale geometric structure in latent representations."}}
{"id": "2509.25188", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25188", "abs": "https://arxiv.org/abs/2509.25188", "authors": ["Wenrui Bao", "Zhiben Chen", "Dan Xu", "Yuzhang Shang"], "title": "Learning to Parallel: Accelerating Diffusion Large Language Models via Adaptive Parallel Decoding", "comment": null, "summary": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache.", "AI": {"tldr": "Learn2PD is a framework that trains a lightweight filter model to enable adaptive parallel decoding in diffusion-based LLMs, achieving significant speedup without performance loss.", "motivation": "Autoregressive decoding in LLMs limits inference throughput, and current parallel decoding strategies use fixed heuristics that don't adapt to input characteristics, leading to suboptimal speed-quality trade-offs.", "method": "Proposes Learn2PD framework with a lightweight filter model that predicts token correctness, and End-of-Text Prediction to detect decoding completion. The filter is learned post-training with minimal computation.", "result": "Achieves up to 22.58x speedup without performance drop on LLaDA benchmark, and up to 57.51x when combined with KV-Cache.", "conclusion": "Learn2PD provides a flexible and dynamic approach to parallel decoding that significantly improves inference throughput while maintaining quality."}}
{"id": "2509.24171", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24171", "abs": "https://arxiv.org/abs/2509.24171", "authors": ["Ruibo Chen", "Sheng Zhang", "Yihan Wu", "Tong Zheng", "Peihua Mai", "Heng Huang"], "title": "Model Correlation Detection via Random Selection Probing", "comment": null, "summary": "The growing prevalence of large language models (LLMs) and vision-language\nmodels (VLMs) has heightened the need for reliable techniques to determine\nwhether a model has been fine-tuned from or is even identical to another.\nExisting similarity-based methods often require access to model parameters or\nproduce heuristic scores without principled thresholds, limiting their\napplicability. We introduce Random Selection Probing (RSP), a\nhypothesis-testing framework that formulates model correlation detection as a\nstatistical test. RSP optimizes textual or visual prefixes on a reference model\nfor a random selection task and evaluates their transferability to a target\nmodel, producing rigorous p-values that quantify evidence of correlation. To\nmitigate false positives, RSP incorporates an unrelated baseline model to\nfilter out generic, transferable features. We evaluate RSP across both LLMs and\nVLMs under diverse access conditions for reference models and test models.\nExperiments on fine-tuned and open-source models show that RSP consistently\nyields small p-values for related models while maintaining high p-values for\nunrelated ones. Extensive ablation studies further demonstrate the robustness\nof RSP. These results establish RSP as the first principled and general\nstatistical framework for model correlation detection, enabling transparent and\ninterpretable decisions in modern machine learning ecosystems.", "AI": {"tldr": "RSP is a statistical framework that detects model correlations by testing transferability of optimized prefixes, providing principled p-values for determining if models are related through fine-tuning or identity.", "motivation": "Existing methods for detecting model correlations require parameter access or produce heuristic scores without statistical rigor, limiting reliable application in LLM/VLM ecosystems.", "method": "Random Selection Probing (RSP) formulates correlation detection as hypothesis testing, optimizing prefixes on reference models for random selection tasks and evaluating transferability to target models with p-values.", "result": "RSP consistently yields small p-values for related models and high p-values for unrelated ones across LLMs and VLMs, with robustness confirmed through ablation studies.", "conclusion": "RSP provides the first principled statistical framework for model correlation detection, enabling transparent and interpretable decisions in machine learning ecosystems."}}
{"id": "2509.25189", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25189", "abs": "https://arxiv.org/abs/2509.25189", "authors": ["Gongrui Zhang", "Jialiang Zhu", "Ruiqi Yang", "Kai Qiu", "Miaosen Zhang", "Zhirong Wu", "Qi Dai", "Bei Liu", "Chong Luo", "Zhengyuan Yang", "Linjie Li", "Lijuan Wang", "Weizhu Chen", "Yuan Zhang", "Xin Li", "Zhaoyi Liu", "Xin Geng", "Baining Guo"], "title": "InfoAgent: Advancing Autonomous Information-Seeking Agents", "comment": null, "summary": "Building Large Language Model agents that expand their capabilities by\ninteracting with external tools represents a new frontier in AI research and\napplications. In this paper, we introduce InfoAgent, a deep research agent\npowered by an innovative data synthesis pipeline and orchestrated web search\ntools. To construct challenging, hard-to-find queries,we build entity trees and\napply sub-tree sampling with entity fuzzification to systematically increase\nquestion difficulty. Unlike prior work that relies heavily on commercial search\ntools, we develop a dedicated self-hosted search infrastructure, enhancing\ntransparency of agent environments and facilitating further advancement of\nagent capacity. We evaluate the effectiveness of our data pipeline by measuring\nthe average number of tool calls required to correctly answer a question, and\nalso show that our agent yields better performance when equipped with our\ntools. Our \\mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage\nrecipe: cold-start supervised finetuning to instill long-horizon search\nbehaviors, followed by reinforcement learning which significantly improves\nreasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy\non BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming\nprior open-source deep research agents such as WebSailor-72B and DeepDive-32B.", "AI": {"tldr": "InfoAgent is a deep research agent that uses a data synthesis pipeline and self-hosted web search tools to handle challenging queries, outperforming prior open-source agents on multiple benchmarks.", "motivation": "To build Large Language Model agents that expand capabilities through external tool interaction, addressing limitations of commercial search tools and improving transparency in agent environments.", "method": "Uses entity trees with sub-tree sampling and entity fuzzification to create difficult queries, develops self-hosted search infrastructure, and employs two-stage training: supervised finetuning followed by reinforcement learning.", "result": "Achieves 15.3% accuracy on BrowseComp, 29.2% on BrowseComp-ZH, and 40.4% on Xbench-DS, outperforming WebSailor-72B and DeepDive-32B.", "conclusion": "InfoAgent demonstrates effective deep research capabilities through innovative data synthesis and self-hosted tools, showing significant improvements over existing open-source agents."}}
{"id": "2509.24176", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24176", "abs": "https://arxiv.org/abs/2509.24176", "authors": ["Chuntian Chi", "John Clapham", "Leslie Cloud", "Ingrid Pretzer-Aboff", "GinaMari Blackwell", "Huajie Shao", "Gang Zhou"], "title": "FM-FoG: A Real-Time Foundation Model-based Wearable System for Freezing-of-Gait Mitigation", "comment": "This is a preprint version, 12 pages, 7 figures, 8 tables", "summary": "Freezing-of-Gait (FoG) affects over 50% of mid-to-late stage Parkinson's\ndisease (PD) patients, significantly impairing patients' mobility independence\nand reducing quality of life. FoG is characterized by sudden episodes where\nwalking cannot start or is interrupted, occurring exclusively during standing\nor walking, and never while sitting or lying down. Current FoG detection\nsystems require extensive patient-specific training data and lack\ngeneralization, limiting clinical deployment. To address these issues, we\nintroduce FM-FoG, a real-time foundation model-based wearable system achieving\nFoG detection in unseen patients without patient-specific training. Our\napproach combines self-supervised pretraining on diverse Inertial Measurement\nUnit (IMU) datasets with sensor context integration. Since FoG occurs only\nduring ambulatory activities, a lightweight CNN-LSTM activity classifier\nselectively activates the foundation model only during walking or standing,\navoiding unnecessary computation. Evaluated on the VCU FoG-IMU dataset with 23\nPD patients, FM-FoG achieves a 98.5% F1-score when tested on previously unseen\npatients, substantially outperforming competitive baseline methods. Deployed on\na Google Pixel 8a smartphone, the system extends battery life by up to 72%\nwhile maintaining sub-20ms intervention latency. The results indicate that our\nFM-FoG can enable practical, energy-efficient healthcare applications that\ngeneralize across patients without individual training requirements.", "AI": {"tldr": "FM-FoG is a real-time foundation model-based wearable system that detects Freezing-of-Gait in Parkinson's patients without patient-specific training, achieving 98.5% F1-score on unseen patients and extending smartphone battery life by 72%.", "motivation": "Current FoG detection systems require extensive patient-specific training data and lack generalization, limiting clinical deployment. FoG affects over 50% of mid-to-late stage PD patients, significantly impairing mobility independence.", "method": "Combines self-supervised pretraining on diverse IMU datasets with sensor context integration. Uses lightweight CNN-LSTM activity classifier to selectively activate foundation model only during walking or standing to avoid unnecessary computation.", "result": "Achieves 98.5% F1-score when tested on previously unseen patients (VCU FoG-IMU dataset with 23 PD patients), substantially outperforming baseline methods. Deployed on Google Pixel 8a smartphone, extends battery life by up to 72% while maintaining sub-20ms intervention latency.", "conclusion": "FM-FoG can enable practical, energy-efficient healthcare applications that generalize across patients without individual training requirements, making it suitable for clinical deployment."}}
{"id": "2504.14051", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2504.14051", "abs": "https://arxiv.org/abs/2504.14051", "authors": ["Raghavv Goel", "Junyoung Park", "Mukul Gagrani", "Dalton Jones", "Matthew Morse", "Harper Langston", "Mingu Lee", "Chris Lott"], "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based Token Eviction", "comment": "15 pages, 3 figures, 13 tables", "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.", "AI": {"tldr": "CAOTE is a token eviction method that uses attention output contributions rather than just attention scores, improving accuracy when combined with existing methods.", "motivation": "Address limitations of attention score-based token eviction by incorporating information about tokens' contributions to attention outputs, especially important for resource-constrained devices.", "method": "Proposes CAOTE which integrates attention scores and value vectors in closed-form to optimize eviction error, can be used as meta-heuristic with any token eviction method.", "result": "CAOTE consistently improves accuracies on downstream tasks when combined with state-of-the-art attention score-based methods.", "conclusion": "Leveraging value information during token eviction is crucial for better performance, and CAOTE provides an effective closed-form solution."}}
{"id": "2509.24198", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24198", "abs": "https://arxiv.org/abs/2509.24198", "authors": ["Linghao Kong", "Angelina Ning", "Micah Adler", "Nir Shavit"], "title": "Negative Pre-activations Differentiate Syntax", "comment": "10 pages, 7 figures", "summary": "A recently discovered class of entangled neurons, known as Wasserstein\nneurons, is disproportionately critical in large language models despite\nconstituting only a very small fraction of the network: their targeted removal\ncollapses the model, consistent with their unique role in differentiating\nsimilar inputs. Interestingly, in Wasserstein neurons immediately preceding\nsmooth activation functions, such differentiation manifests in the negative\npre-activation space, especially in early layers. Pairs of similar inputs are\ndriven to highly distinct negative values, and these pairs involve syntactic\ntokens such as determiners and prepositions. We show that this negative region\nis functional rather than simply favorable for optimization. A minimal,\nsign-specific intervention that zeroes only the negative pre-activations of a\nsmall subset of entangled neurons significantly weakens overall model function\nand disrupts grammatical behavior, while both random and perplexity-matched\ncontrols leave grammatical performance largely unchanged. Part of speech\nanalysis localizes the excess surprisal to syntactic scaffolding tokens, and\nlayer-specific interventions reveal that small local degradations accumulate\nacross depth. Over training checkpoints, the same ablation impairs grammatical\nbehavior as Wasserstein neurons emerge and stabilize. Together, these results\nidentify negative differentiation in a sparse subset of entangled neurons as a\ncrucial mechanism that language models rely on for syntax.", "AI": {"tldr": "Wasserstein neurons are a sparse but critical component in LLMs that enable syntax processing through negative pre-activation differentiation of similar syntactic tokens.", "motivation": "To understand the functional role of recently discovered Wasserstein neurons in language models and investigate why their targeted removal collapses model performance.", "method": "Used targeted ablation studies by zeroing negative pre-activations of Wasserstein neurons, compared with random and perplexity-matched controls, analyzed part-of-speech effects and layer-specific interventions across training checkpoints.", "result": "Targeted removal of negative pre-activations in Wasserstein neurons significantly impairs grammatical behavior and model function, while controls show minimal effects. The disruption specifically affects syntactic scaffolding tokens and accumulates across network depth.", "conclusion": "Negative differentiation in a sparse subset of entangled Wasserstein neurons is a crucial mechanism that language models rely on for syntactic processing."}}
{"id": "2509.21923", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21923", "abs": "https://arxiv.org/abs/2509.21923", "authors": ["Fumin Wang"], "title": "Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects", "comment": null, "summary": "Interpretability is one of the considerations when applying machine learning\nto high-stakes fields such as healthcare that involve matters of life safety.\nGeneralized Additive Models (GAMs) enhance interpretability by visualizing\nshape functions. Nevertheless, to preserve interpretability, GAMs omit\nhigher-order interaction effects (beyond pairwise interactions), which imposes\nsignificant constraints on their predictive performance. We observe that Curve\nErgodic Set Regression (CESR), a multiplicative model, naturally enables the\nvisualization of its shape functions and simultaneously incorporates both\ninteractions among all features and individual feature effects. Nevertheless,\nCESR fails to demonstrate superior performance compared to GAMs. We introduce\nMultiplicative-Additive Constrained Models (MACMs), which augment CESR with an\nadditive part to disentangle the intertwined coefficients of its interactive\nand independent terms, thus effectively broadening the hypothesis space. The\nmodel is composed of a multiplicative part and an additive part, whose shape\nfunctions can both be naturally visualized, thereby assisting users in\ninterpreting how features participate in the decision-making process.\nConsequently, MACMs constitute an improvement over both CESR and GAMs. The\nexperimental results indicate that neural network-based MACMs significantly\noutperform both CESR and the current state-of-the-art GAMs in terms of\npredictive performance.", "AI": {"tldr": "MACMs improve interpretable models by combining multiplicative and additive components, outperforming both CESR and GAMs while maintaining visual interpretability.", "motivation": "To address the trade-off between interpretability and predictive performance in machine learning for high-stakes applications like healthcare, where GAMs sacrifice higher-order interactions for interpretability and CESR fails to outperform GAMs despite incorporating all feature interactions.", "method": "Proposed Multiplicative-Additive Constrained Models (MACMs) that augment CESR with an additive component to disentangle interactive and independent feature effects, effectively expanding the hypothesis space while maintaining visual interpretability of shape functions.", "result": "Neural network-based MACMs significantly outperform both CESR and state-of-the-art GAMs in predictive performance while preserving the ability to visualize shape functions for interpretation.", "conclusion": "MACMs successfully bridge the gap between interpretability and performance by combining multiplicative and additive components, offering a superior alternative to both CESR and GAMs for interpretable machine learning in high-stakes domains."}}
{"id": "2509.24203", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24203", "abs": "https://arxiv.org/abs/2509.24203", "authors": ["Chaorui Yao", "Yanxi Chen", "Yuchang Sun", "Yushuo Chen", "Wenhao Zhang", "Xuchen Pan", "Yaliang Li", "Bolin Ding"], "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends", "comment": null, "summary": "Off-policy reinforcement learning (RL) for large language models (LLMs) is\nattracting growing interest, driven by practical constraints in real-world\napplications, the complexity of LLM-RL infrastructure, and the need for further\ninnovations of RL methodologies. While classic REINFORCE and its modern\nvariants like Group Relative Policy Optimization (GRPO) are typically regarded\nas on-policy algorithms with limited tolerance of off-policyness, we present in\nthis work a first-principles derivation for group-relative REINFORCE without\nassuming a specific training data distribution, showing that it admits a native\noff-policy interpretation. This perspective yields two general principles for\nadapting REINFORCE to off-policy settings: regularizing policy updates, and\nactively shaping the data distribution. Our analysis demystifies some myths\nabout the roles of importance sampling and clipping in GRPO, unifies and\nreinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and\nAsymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss,\nand offers theoretical justification for seemingly heuristic data-weighting\nstrategies. Our findings lead to actionable insights that are validated with\nextensive empirical studies, and open up new opportunities for principled\nalgorithm design in off-policy RL for LLMs. Source code for this work is\navailable at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.", "AI": {"tldr": "This paper provides a first-principles derivation showing that group-relative REINFORCE can be interpreted as an off-policy algorithm, challenging the conventional view that REINFORCE variants are strictly on-policy. It establishes two principles for adapting REINFORCE to off-policy settings and unifies recent algorithms under this framework.", "motivation": "The motivation stems from practical constraints in real-world LLM applications, the complexity of LLM-RL infrastructure, and the need for innovations in RL methodologies. There's growing interest in off-policy RL for LLMs, but conventional wisdom limits REINFORCE variants to on-policy settings.", "method": "The authors present a first-principles derivation for group-relative REINFORCE without assuming specific training data distributions, revealing its native off-policy interpretation. They establish two principles: regularizing policy updates and actively shaping data distribution.", "result": "The analysis demystifies myths about importance sampling and clipping in GRPO, unifies OPMD and AsymRE as regularized REINFORCE forms, provides theoretical justification for heuristic data-weighting strategies, and offers actionable insights validated through empirical studies.", "conclusion": "This work opens up new opportunities for principled algorithm design in off-policy RL for LLMs by providing a unified theoretical framework that challenges conventional assumptions about REINFORCE variants and enables more flexible off-policy approaches."}}
{"id": "2509.24217", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.24217", "abs": "https://arxiv.org/abs/2509.24217", "authors": ["Yuyang Sha", "Hongxin Pan", "Gang Luo", "Caijuan Shi", "Jing Wang", "Kefeng Li"], "title": "MDD-Thinker: Towards Large Reasoning Models for Major Depressive Disorder Diagnosis", "comment": null, "summary": "Background Major depressive disorder (MDD) is a leading cause of global\ndisability, yet current diagnostic approaches often rely on subjective\nassessments and lack the ability to integrate multimodal clinical information.\nLarge language models (LLMs) hold promise for enhancing diagnostic accuracy\nthrough advanced reasoning but face challenges in interpretability,\nhallucination, and reliance on synthetic data.\n  Methods We developed MDD-Thinker, an LLM-based diagnostic framework that\nintegrates supervised fine-tuning (SFT) with reinforcement learning (RL) to\nstrengthen reasoning ability and interpretability. Using the UK Biobank\ndataset, we generated 40,000 reasoning samples, supplemented with 10,000\nsamples from publicly available mental health datasets. The model was\nfine-tuned on these reasoning corpora, and its diagnostic and reasoning\nperformance was evaluated against machine learning, deep learning, and\nstate-of-the-art LLM baselines.\n  Findings MDD-Thinker achieved an accuracy of 0.8268 and F1-score of 0.8081,\nsignificantly outperforming traditional baselines such as SVM and MLP, as well\nas general-purpose LLMs. Incorporating both SFT and RL yielded the greatest\nimprovements, with relative gains of 29.0% in accuracy, 38.1% in F1-score, and\n34.8% in AUC. Moreover, the model demonstrated comparable reasoning performance\ncompared to much larger LLMs, while maintaining computational efficiency.\n  Interpretation This study presents the first reasoning-enhanced LLM framework\nfor MDD diagnosis trained on large-scale real-world clinical data. By\nintegrating SFT and RL, MDD-Thinker balances accuracy, interpretability, and\nefficiency, offering a scalable approach for intelligent psychiatric\ndiagnostics. These findings suggest that reasoning-oriented LLMs can provide\nclinically reliable support for MDD detection and may inform broader\napplications in mental health care.", "AI": {"tldr": "MDD-Thinker is an LLM-based diagnostic framework that combines supervised fine-tuning and reinforcement learning to improve MDD diagnosis accuracy and interpretability, achieving superior performance over traditional methods and general-purpose LLMs.", "motivation": "Current MDD diagnostic approaches rely on subjective assessments and lack multimodal integration. LLMs offer potential for enhanced accuracy but face challenges with interpretability, hallucination, and synthetic data reliance.", "method": "Developed MDD-Thinker using UK Biobank data (40,000 reasoning samples) plus 10,000 samples from public mental health datasets. Combined supervised fine-tuning with reinforcement learning to enhance reasoning and interpretability.", "result": "Achieved accuracy of 0.8268 and F1-score of 0.8081, significantly outperforming SVM, MLP, and general-purpose LLMs. SFT+RL combination yielded 29.0% accuracy improvement, 38.1% F1-score gain, and 34.8% AUC increase.", "conclusion": "First reasoning-enhanced LLM framework for MDD diagnosis using real-world clinical data. Successfully balances accuracy, interpretability, and efficiency, offering scalable approach for intelligent psychiatric diagnostics."}}
{"id": "2509.24218", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24218", "abs": "https://arxiv.org/abs/2509.24218", "authors": ["Junjie Wang", "Pan Zhou", "Yiming Dong", "Huan Li", "Jia Li", "Xun Zhou", "Qicheng Lao", "Cong Fang", "Zhouchen Lin"], "title": "Conda: Column-Normalized Adam for Training Large Language Models Faster", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive generalization and\nemergent capabilities, yet their pre-training remains computationally expensive\nand sensitive to optimization dynamics. While Adam-based optimizers offer fast\nconvergence by adapting learning rates coordinate-wise, recent studies reveal\nthat their updates often suffer from poor spectral conditioning and low-rank\nstructures, hindering efficiency. Muon addresses this issue via global spectral\nnormalization but lacks the per-coordinate adaptivity of Adam. In this work, we\npropose \\textbf{Column-Normalized Adam (Conda)}, a novel optimizer that bridges\nthe strengths of both approaches. Conda projects updates into an orthogonal\nsubspace and applies column-wise second moment normalization based on the\nprojected gradients, thereby achieving both improved spectral conditioning and\nmaintaining coordinate-wise adaptivity. This design alleviates the spectral\npathologies of Adam while preserving its fast convergence behavior. Extensive\nexperiments on the LLaMA and GPT-2 series show that Conda consistently\noutperforms AdamW, Muon, and other baselines in pre-training. Remarkably, on\nthe LLaMA series, \\textbf{Conda achieves $2{\\sim}2.5\\times$ the convergence\nspeed of AdamW, measured in both training steps and training time.} Further\nablations demonstrate its robustness under diverse training setups. These\nresults collectively highlight Conda as an effective and broadly applicable\noptimizer for large-scale LLM training. The code is released on\nhttps://github.com/jie040109/Conda", "AI": {"tldr": "Conda is a novel optimizer that combines Adam's coordinate-wise adaptivity with global spectral normalization, achieving 2-2.5x faster convergence than AdamW in LLM pre-training.", "motivation": "Address the limitations of Adam optimizers which suffer from poor spectral conditioning and low-rank structures, while Muon lacks per-coordinate adaptivity.", "method": "Projects updates into orthogonal subspace and applies column-wise second moment normalization based on projected gradients, maintaining coordinate-wise adaptivity while improving spectral conditioning.", "result": "Consistently outperforms AdamW, Muon, and other baselines in pre-training LLaMA and GPT-2 series, achieving 2-2.5x faster convergence speed than AdamW.", "conclusion": "Conda is an effective and broadly applicable optimizer for large-scale LLM training, bridging the strengths of both Adam and spectral normalization approaches."}}
{"id": "2509.24223", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24223", "abs": "https://arxiv.org/abs/2509.24223", "authors": ["Jianxin Zhang", "Clayton Scott"], "title": "Semantic Editing with Coupled Stochastic Differential Equations", "comment": null, "summary": "Editing the content of an image with a pretrained text-to-image model remains\nchallenging. Existing methods often distort fine details or introduce\nunintended artifacts. We propose using coupled stochastic differential\nequations (coupled SDEs) to guide the sampling process of any pre-trained\ngenerative model that can be sampled by solving an SDE, including diffusion and\nrectified flow models. By driving both the source image and the edited image\nwith the same correlated noise, our approach steers new samples toward the\ndesired semantics while preserving visual similarity to the source. The method\nworks out-of-the-box-without retraining or auxiliary networks-and achieves high\nprompt fidelity along with near-pixel-level consistency. These results position\ncoupled SDEs as a simple yet powerful tool for controlled generative AI.", "AI": {"tldr": "Proposes coupled SDEs to guide pre-trained generative models for image editing, preserving source image details while achieving semantic changes.", "motivation": "Existing image editing methods often distort fine details or introduce artifacts when using pre-trained text-to-image models.", "method": "Uses coupled stochastic differential equations with correlated noise to drive both source and edited images, guiding sampling process of any SDE-based generative model.", "result": "Achieves high prompt fidelity with near-pixel-level consistency, works out-of-the-box without retraining or auxiliary networks.", "conclusion": "Coupled SDEs are a simple yet powerful tool for controlled generative AI image editing."}}
{"id": "2509.24224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24224", "abs": "https://arxiv.org/abs/2509.24224", "authors": ["Ashiqur Rahman", "Hamed Alhoori"], "title": "Proposing a Framework for Machine Learning Adoption on Legacy Systems", "comment": "Accepted at The First International Workshop on Resilient Artificial\n  Intelligence for Manufacturing (ICDM'25)", "summary": "The integration of machine learning (ML) is critical for industrial\ncompetitiveness, yet its adoption is frequently stalled by the prohibitive\ncosts and operational disruptions of upgrading legacy systems. The financial\nand logistical overhead required to support the full ML lifecycle presents a\nformidable barrier to widespread implementation, particularly for small and\nmedium-sized enterprises. This paper introduces a pragmatic, API-based\nframework designed to overcome these challenges by strategically decoupling the\nML model lifecycle from the production environment. Our solution delivers the\nanalytical power of ML to domain experts through a lightweight, browser-based\ninterface, eliminating the need for local hardware upgrades and ensuring model\nmaintenance can occur with zero production downtime. This human-in-the-loop\napproach empowers experts with interactive control over model parameters,\nfostering trust and facilitating seamless integration into existing workflows.\nBy mitigating the primary financial and operational risks, this framework\noffers a scalable and accessible pathway to enhance production quality and\nsafety, thereby strengthening the competitive advantage of the manufacturing\nsector.", "AI": {"tldr": "API-based framework decouples ML lifecycle from production to reduce adoption costs and operational disruptions in manufacturing.", "motivation": "High costs and operational disruptions hinder ML adoption in industry, especially for SMEs needing to upgrade legacy systems.", "method": "Lightweight browser-based interface with human-in-the-loop approach, allowing domain experts interactive control over model parameters without local hardware upgrades.", "result": "Enables ML integration with zero production downtime, fostering trust and seamless workflow integration while reducing financial risks.", "conclusion": "Provides scalable, accessible pathway to enhance production quality and safety, strengthening manufacturing competitiveness."}}
{"id": "2509.24228", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24228", "abs": "https://arxiv.org/abs/2509.24228", "authors": ["Wei Wang", "Dong-Dong Wu", "Ming Li", "Jingxiong Zhang", "Gang Niu", "Masashi Sugiyama"], "title": "Accessible, Realistic, and Fair Evaluation of Positive-Unlabeled Learning Algorithms", "comment": null, "summary": "Positive-unlabeled (PU) learning is a weakly supervised binary classification\nproblem, in which the goal is to learn a binary classifier from only positive\nand unlabeled data, without access to negative data. In recent years, many PU\nlearning algorithms have been developed to improve model performance. However,\nexperimental settings are highly inconsistent, making it difficult to identify\nwhich algorithm performs better. In this paper, we propose the first PU\nlearning benchmark to systematically compare PU learning algorithms. During our\nimplementation, we identify subtle yet critical factors that affect the\nrealistic and fair evaluation of PU learning algorithms. On the one hand, many\nPU learning algorithms rely on a validation set that includes negative data for\nmodel selection. This is unrealistic in traditional PU learning settings, where\nno negative data are available. To handle this problem, we systematically\ninvestigate model selection criteria for PU learning. On the other hand, the\nproblem settings and solutions of PU learning have different families, i.e.,\nthe one-sample and two-sample settings. However, existing evaluation protocols\nare heavily biased towards the one-sample setting and neglect the significant\ndifference between them. We identify the internal label shift problem of\nunlabeled training data for the one-sample setting and propose a simple yet\neffective calibration approach to ensure fair comparisons within and across\nfamilies. We hope our framework will provide an accessible, realistic, and fair\nenvironment for evaluating PU learning algorithms in the future.", "AI": {"tldr": "Proposes the first PU learning benchmark to systematically compare algorithms, addressing unrealistic validation requirements and fair comparisons across different PU learning families.", "motivation": "Experimental settings for PU learning are highly inconsistent, making it difficult to identify which algorithm performs better. There's a need for systematic comparison and fair evaluation.", "method": "Develops a PU learning benchmark framework, investigates model selection criteria without negative data, identifies internal label shift problem, and proposes calibration approach for fair comparisons.", "result": "Created the first systematic PU learning benchmark that addresses unrealistic validation requirements and ensures fair comparisons across different PU learning families (one-sample vs two-sample settings).", "conclusion": "The proposed framework provides an accessible, realistic, and fair environment for evaluating PU learning algorithms, addressing critical evaluation factors identified during implementation."}}
{"id": "2509.24239", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24239", "abs": "https://arxiv.org/abs/2509.24239", "authors": ["Jincheng Liu", "Sijun He", "Jingjing Wu", "Xiangsen Wang", "Yang Chen", "Zhaoqi Kuang", "Siqi Bao", "Yuan Yao"], "title": "ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models", "comment": null, "summary": "Recent large language models (LLMs) have shown strong reasoning capabilities.\nHowever, a critical question remains: do these models possess genuine reasoning\nskills particularly complex strategic reasoning or are they primarily excelling\nat sophisticated pattern recognition within their training data? To address\nthis question, this paper presents a chess testbed, ChessArena, to evaluate the\nstrategic reasoning capabilities of LLMs. Chess requires complex strategic\nreasoning capabilities including long-term planning, strict rule comprehension,\nand multi-turn conversation memorization. Specifically, ChessArena is a\ncompetitive framework where LLMs play against each other, under four different\nplay modes. The testbed is equipped with a ranking algorithm and a leaderboard.\nThe testbed can also evaluate fine-grained capabilities including basic\nunderstanding, move selection, and puzzle solving. Over 13 LLMs with different\nmodes are evaluated in ChessArena, playing over 800 games. The results reveal\nsignificant shortcomings in current LLMs: no model can beat Maia-1100 (a chess\nengine at human amateur level), while some even failed to defeat a random\nplayer that selects moves arbitrarily. We also present a strong baseline to the\ntestbed: our fine-tuned Qwen3-8B substantially improved performance,\napproaching much larger state-of-the-art reasoning models.", "AI": {"tldr": "This paper introduces ChessArena, a chess-based testbed to evaluate strategic reasoning in LLMs, revealing current models' limitations in complex strategic thinking despite strong pattern recognition abilities.", "motivation": "To determine whether LLMs possess genuine strategic reasoning capabilities or are mainly excelling at pattern recognition, using chess as a test domain that requires long-term planning, rule comprehension, and multi-turn memory.", "method": "Developed ChessArena - a competitive framework where LLMs play chess against each other under four different modes, equipped with ranking algorithm and leaderboard. Evaluated 13 LLMs playing over 800 games, with capabilities assessed across basic understanding, move selection, and puzzle solving.", "result": "Current LLMs show significant shortcomings: no model could beat Maia-1100 (human amateur level chess engine), some failed to defeat random players. However, fine-tuned Qwen3-8B showed substantial improvement, approaching performance of much larger state-of-the-art reasoning models.", "conclusion": "Current LLMs lack genuine strategic reasoning capabilities for complex tasks like chess, but targeted fine-tuning can significantly improve performance, suggesting potential for developing better reasoning models."}}
{"id": "2509.24256", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24256", "abs": "https://arxiv.org/abs/2509.24256", "authors": ["Yunhao Liang", "Pujun Zhang", "Yuan Qu", "Shaochong Lin", "Zuo-jun Max Shen"], "title": "Graph Foundation Models: Bridging Language Model Paradigms and Graph Optimization", "comment": null, "summary": "The pretrain-transfer paradigm, which underpins the success of large language\nmodels (LLMs), has demonstrated the immense power of creating foundation models\nthat learn generalizable representations from vast datasets. However, extending\nthis paradigm to Operations Research (OR) problems on graph structures remains\nchallenging due to the fundamental conflict between the statistical flexibility\nof language and the strict combinatorial constraints of graphs. To bridge this\ngap, we introduce the Graph Foundation Model (GFM), the first framework capable\nof solving all distance-based optimization problems on graph structures. By\nintroducing the LLM-like self-supervised pre-training paradigm on the paths\ngenerated from random walks in the graph, GFM is compelled to internalize the\ngraph's complex topological and combinatorial rules, where the connectivity of\nthe structure itself can be treated as the supervisory signal. Unlike existing\nneural methods that learn complex and task-specific solving policies, our\napproach leverages the pre-trained GFM as a foundational model of the graph's\nintrinsic structure, which in turn enables a simple generative heuristic to\ntackle a diverse range of optimization challenges effectively. Comprehensive\nexperiments on networks ranging from 20 to 893 nodes demonstrate that GFM\nachieves competitive performance against specialized solvers across a variety\nof distinct optimization task classes, while maintaining significantly faster\ninference times. Our work establishes a new paradigm of adapting the\npretrain-transfer framework to graph optimization, opening the door for\napplying foundation model innovations to OR.", "AI": {"tldr": "GFM is the first framework that adapts the pretrain-transfer paradigm to solve distance-based optimization problems on graphs, using self-supervised pre-training on random walk paths to learn graph topology.", "motivation": "To bridge the gap between LLM's statistical flexibility and graph combinatorial constraints, enabling foundation models for Operations Research problems on graph structures.", "method": "Self-supervised pre-training on paths from random walks, treating graph connectivity as supervisory signal; uses pre-trained GFM as foundational model with simple generative heuristic.", "result": "Achieves competitive performance against specialized solvers on networks (20-893 nodes) across various optimization tasks with significantly faster inference times.", "conclusion": "Establishes new paradigm for applying foundation model innovations to graph optimization in Operations Research."}}
{"id": "2509.24274", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24274", "abs": "https://arxiv.org/abs/2509.24274", "authors": ["Inkyu Park", "Jeong-Gwan Lee", "Taehwan Kwon", "Juheon Choi", "Seungku Kim", "Junsu Kim", "Kimin Lee"], "title": "Adversarial Reinforcement Learning Framework for ESP Cheater Simulation", "comment": null, "summary": "Extra-Sensory Perception (ESP) cheats, which reveal hidden in-game\ninformation such as enemy locations, are difficult to detect because their\neffects are not directly observable in player behavior. The lack of observable\nevidence makes it difficult to collect reliably labeled data, which is\nessential for training effective anti-cheat systems. Furthermore, cheaters\noften adapt their behavior by limiting or disguising their cheat usage, which\nfurther complicates detection and detector development. To address these\nchallenges, we propose a simulation framework for controlled modeling of ESP\ncheaters, non-cheaters, and trajectory-based detectors. We model cheaters and\nnon-cheaters as reinforcement learning agents with different levels of\nobservability, while detectors classify their behavioral trajectories. Next, we\nformulate the interaction between the cheater and the detector as an\nadversarial game, allowing both players to co-adapt over time. To reflect\nrealistic cheater strategies, we introduce a structured cheater model that\ndynamically switches between cheating and non-cheating behaviors based on\ndetection risk. Experiments demonstrate that our framework successfully\nsimulates adaptive cheater behaviors that strategically balance reward\noptimization and detection evasion. This work provides a controllable and\nextensible platform for studying adaptive cheating behaviors and developing\neffective cheat detectors.", "AI": {"tldr": "A simulation framework for modeling ESP cheaters and detectors using reinforcement learning agents in an adversarial game setting, enabling study of adaptive cheating behaviors.", "motivation": "ESP cheats are hard to detect due to lack of observable evidence and cheaters' adaptive behaviors, making labeled data collection difficult for anti-cheat systems.", "method": "Model cheaters and non-cheaters as RL agents with different observability levels, formulate cheater-detector interaction as adversarial game, and introduce structured cheater model that dynamically switches behaviors based on detection risk.", "result": "Framework successfully simulates adaptive cheater behaviors that strategically balance reward optimization and detection evasion.", "conclusion": "Provides a controllable and extensible platform for studying adaptive cheating behaviors and developing effective cheat detectors."}}
{"id": "2509.24302", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24302", "abs": "https://arxiv.org/abs/2509.24302", "authors": ["Muyun Jiang", "Shuailei Zhang", "Zhenjie Yang", "Mengjun Wu", "Weibang Jiang", "Zhiwei Guo", "Wei Zhang", "Rui Liu", "Shangen Zhang", "Yong Li", "Yi Ding", "Cuntai Guan"], "title": "ELASTIQ: EEG-Language Alignment with Semantic Task Instruction and Querying", "comment": null, "summary": "Recent advances in electroencephalography (EEG) foundation models, which\ncapture transferable EEG representations, have greatly accelerated the\ndevelopment of brain-computer interfaces (BCI). However, existing approaches\nstill struggle to incorporate language instructions as prior constraints for\nEEG representation learning, limiting their ability to leverage the semantic\nknowledge inherent in language to unify different labels and tasks. To address\nthis challenge, we present ELASTIQ, a foundation model for EEG-Language\nAlignment with Semantic Task Instruction and Querying. ELASTIQ integrates\ntask-aware semantic guidance to produce structured and linguistically aligned\nEEG embeddings, thereby enhancing decoding robustness and transferability. In\nthe pretraining stage, we introduce a joint Spectral-Temporal Reconstruction\n(STR) module, which combines frequency masking as a global spectral\nperturbation with two complementary temporal objectives: random masking to\ncapture contextual dependencies and causal masking to model sequential\ndynamics. In the instruction tuning stage, we propose the\nInstruction-conditioned Q-Former (IQF), a query-based cross-attention\ntransformer that injects instruction embeddings into EEG tokens and aligns them\nwith textual label embeddings through learnable queries. We evaluate ELASTIQ on\n20 datasets spanning motor imagery, emotion recognition, steady-state visual\nevoked potentials, covert speech, and healthcare tasks. ELASTIQ achieves\nstate-of-the-art performance on 14 of the 20 datasets and obtains the best\naverage results across all five task categories. Importantly, our analyses\nreveal for the first time that explicit task instructions serve as semantic\npriors guiding EEG embeddings into coherent and linguistically grounded spaces.\nThe code and pre-trained weights will be released.", "AI": {"tldr": "ELASTIQ is an EEG-language foundation model that integrates task instructions to create linguistically aligned EEG embeddings, achieving state-of-the-art performance across multiple BCI tasks.", "motivation": "Existing EEG foundation models struggle to incorporate language instructions as prior constraints, limiting their ability to leverage semantic knowledge to unify different labels and tasks.", "method": "Combines joint Spectral-Temporal Reconstruction (STR) module for pretraining with frequency and temporal masking, and Instruction-conditioned Q-Former (IQF) for instruction tuning that aligns EEG tokens with textual label embeddings.", "result": "Achieves state-of-the-art performance on 14 out of 20 datasets across motor imagery, emotion recognition, SSVEP, covert speech, and healthcare tasks, with best average results across all five task categories.", "conclusion": "Task instructions serve as semantic priors that guide EEG embeddings into coherent and linguistically grounded spaces, enhancing decoding robustness and transferability."}}
{"id": "2509.24305", "categories": ["cs.LG", "cs.DC", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.24305", "abs": "https://arxiv.org/abs/2509.24305", "authors": ["Alexander Tyurin", "Andrei Spiridonov", "Varvara Rudenko"], "title": "Asynchronous Policy Gradient Aggregation for Efficient Distributed Reinforcement Learning", "comment": null, "summary": "We study distributed reinforcement learning (RL) with policy gradient methods\nunder asynchronous and parallel computations and communications. While\nnon-distributed methods are well understood theoretically and have achieved\nremarkable empirical success, their distributed counterparts remain less\nexplored, particularly in the presence of heterogeneous asynchronous\ncomputations and communication bottlenecks. We introduce two new algorithms,\nRennala NIGT and Malenia NIGT, which implement asynchronous policy gradient\naggregation and achieve state-of-the-art efficiency. In the homogeneous\nsetting, Rennala NIGT provably improves the total computational and\ncommunication complexity while supporting the AllReduce operation. In the\nheterogeneous setting, Malenia NIGT simultaneously handles asynchronous\ncomputations and heterogeneous environments with strictly better theoretical\nguarantees. Our results are further corroborated by experiments, showing that\nour methods significantly outperform prior approaches.", "AI": {"tldr": "The paper introduces two distributed RL algorithms (Rennala NIGT and Malenia NIGT) for asynchronous policy gradient aggregation, achieving state-of-the-art efficiency in both homogeneous and heterogeneous settings.", "motivation": "Distributed reinforcement learning methods remain less explored compared to non-distributed approaches, especially in the presence of heterogeneous asynchronous computations and communication bottlenecks.", "method": "Two new algorithms: Rennala NIGT for homogeneous settings with AllReduce operation support, and Malenia NIGT for heterogeneous settings handling asynchronous computations and heterogeneous environments.", "result": "The algorithms achieve state-of-the-art efficiency with improved computational and communication complexity, and experimental results show significant outperformance over prior approaches.", "conclusion": "The proposed distributed RL algorithms effectively address asynchronous computations and communication challenges, providing better theoretical guarantees and empirical performance in both homogeneous and heterogeneous settings."}}
{"id": "2509.24306", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24306", "abs": "https://arxiv.org/abs/2509.24306", "authors": ["Satyanarayana Raju G. V. V", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "A study of Universal ODE approaches to predicting soil organic carbon", "comment": null, "summary": "Soil Organic Carbon (SOC) is a foundation of soil health and global climate\nresilience, yet its prediction remains difficult because of intricate physical,\nchemical, and biological processes. In this study, we explore a Scientific\nMachine Learning (SciML) framework built on Universal Differential Equations\n(UDEs) to forecast SOC dynamics across soil depth and time. UDEs blend\nmechanistic physics, such as advection diffusion transport, with neural\nnetworks that learn nonlinear microbial production and respiration. Using\nsynthetic datasets, we systematically evaluated six experimental cases,\nprogressing from clean, noise free benchmarks to stress tests with high (35%)\nmultiplicative, spatially correlated noise. Our results highlight both the\npotential and limitations of the approach. In noise free and moderate noise\nsettings, the UDE accurately reconstructed SOC dynamics. In clean terminal\nprofile at 50 years (Case 4) achieved near perfect fidelity, with MSE = 1.6e-5,\nand R2 = 0.9999. Case 5, with 7% noise, remained robust (MSE = 3.4e-6, R2 =\n0.99998), capturing depth wise SOC trends while tolerating realistic\nmeasurement uncertainty. In contrast, Case 3 (35% noise at t = 0) showed clear\nevidence of overfitting: the model reproduced noisy inputs with high accuracy\nbut lost generalization against the clean truth (R2 = 0.94). Case 6 (35% noise\nat t = 50) collapsed toward overly smooth mean profiles, failing to capture\ndepth wise variability and yielding negative R2, underscoring the limits of\nstandard training under severe uncertainty. These findings suggest that UDEs\nare well suited for scalable, noise tolerant SOC forecasting, though advancing\ntoward field deployment will require noise aware loss functions, probabilistic\nmodelling, and tighter integration of microbial dynamics.", "AI": {"tldr": "This paper proposes a Scientific Machine Learning framework using Universal Differential Equations (UDEs) to predict Soil Organic Carbon dynamics across soil depth and time, combining mechanistic physics with neural networks for microbial processes.", "motivation": "Soil Organic Carbon prediction is challenging due to complex physical, chemical, and biological processes, requiring methods that can handle these intricate dynamics while being robust to measurement noise.", "method": "Used Universal Differential Equations that blend advection diffusion transport (mechanistic physics) with neural networks for learning nonlinear microbial production and respiration. Evaluated six experimental cases with synthetic datasets ranging from noise-free to high (35%) multiplicative, spatially correlated noise.", "result": "In noise-free and moderate noise settings (7%), UDEs achieved excellent performance (MSE = 1.6e-5, R\u00b2 = 0.9999 for clean case; MSE = 3.4e-6, R\u00b2 = 0.99998 for 7% noise). However, with 35% noise, models either overfitted (reproducing noisy inputs but losing generalization) or collapsed to overly smooth profiles with negative R\u00b2 values.", "conclusion": "UDEs show promise for scalable, noise-tolerant SOC forecasting but require noise-aware loss functions, probabilistic modeling, and better integration of microbial dynamics for field deployment."}}
{"id": "2509.24317", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24317", "abs": "https://arxiv.org/abs/2509.24317", "authors": ["Xianhang Li", "Chen Huang", "Chun-Liang Li", "Eran Malach", "Josh Susskind", "Vimal Thilak", "Etai Littwin"], "title": "Rethinking JEPA: Compute-Efficient Video SSL with Frozen Teachers", "comment": "Technical Report", "summary": "Video Joint Embedding Predictive Architectures (V-JEPA) learn generalizable\noff-the-shelf video representation by predicting masked regions in latent space\nwith an exponential moving average (EMA)-updated teacher. While EMA prevents\nrepresentation collapse, it complicates scalable model selection and couples\nteacher and student architectures. We revisit masked-latent prediction and show\nthat a frozen teacher suffices. Concretely, we (i) train a target encoder with\na simple pixel-reconstruction objective under V-JEPA masking, then (ii) freeze\nit and train a student to predict the teacher's latents on masked regions. This\nleads to a two-stage, unregularized scheme that we refer to as SALT\n(Static-teacher Asymmetric Latent Training). SALT decouples optimization into\npixel reconstruction (teacher) and masked latent prediction (student),\nincreasing transparency, efficiency, and scalability while preserving the\nability of representation to generalize under frozen evaluation. Empirically,\nour student models outperform recently proposed V-JEPA 2 encoders under frozen\nbackbone evaluation across diverse benchmarks. They are also more\ncompute-optimal: at matched pretraining FLOPs, our method achieves higher\nprobing accuracy, and its scaling curves dominate V-JEPA's accuracy-FLOPs\nPareto frontier. Finally, we find that student quality is remarkably robust to\nteacher quality: high-performing students emerge even with small, sub-optimal\nteachers. This points to a compute budget allocation that should overwhelmingly\nfavor the student. These results position SALT as a simple, scalable, and\ncompute-efficient alternative to EMA-based self-distillation for video\nrepresentation learning.", "AI": {"tldr": "SALT proposes a two-stage video representation learning method that uses a frozen teacher encoder trained with pixel reconstruction, followed by a student that predicts masked latents, outperforming V-JEPA 2 with better compute efficiency and scalability.", "motivation": "To address the limitations of EMA-based teacher-student frameworks in V-JEPA, which complicate model selection and couple architectures, by developing a simpler, more transparent alternative.", "method": "Two-stage approach: (1) train teacher encoder with pixel reconstruction under V-JEPA masking, (2) freeze teacher and train student to predict teacher's latents on masked regions.", "result": "SALT outperforms V-JEPA 2 encoders in frozen evaluation across benchmarks, achieves higher compute efficiency, and shows student quality is robust to teacher quality.", "conclusion": "SALT provides a simple, scalable, and compute-efficient alternative to EMA-based self-distillation for video representation learning."}}
{"id": "2509.24320", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24320", "abs": "https://arxiv.org/abs/2509.24320", "authors": ["Dipan Maity"], "title": "AuON: A Linear-time Alternative to Semi-Orthogonal Momentum Updates", "comment": null, "summary": "Orthogonal gradient updates have emerged as a promising direction in\noptimization for machine learning. However, traditional approaches such as\nSVD/QR decomposition incur prohibitive computational costs of O(n^3) and\nunderperform compared to well-tuned SGD with momentum, since momentum is\napplied only after strict orthogonalization. Recent advances, such as Muon,\nimprove efficiency by applying momentum before orthogonalization and producing\nsemi-orthogonal matrices via Newton-Schulz iterations, reducing complexity to\nO(n^2). Nevertheless, quadratic costs remain a bottleneck.\n  In this work, we study the semi-orthogonal properties of momentum-based\nupdates and develop a method to bound momentum updates under a spectral-norm\ntrust region, preserving directional information without requiring explicit\nsemi-orthogonalization.\n  We propose AuON (Alternative Unit-norm momentum updates by Normalized\nnonlinear scaling), a linear-time optimizer that achieves strong performance\nwithout constructing semi-orthogonal matrices, while preserving structural\nalignment and reconditioning ill-posed updates. Our approach combines\nhyperbolic-cosine RMS scaling transformations with normalization, demonstrating\nboth effectiveness and computational efficiency compared to Newton-Schulz\nmethods. We further introduce a hybrid variant (Hybrid-AuON) that applies a\nsingle Newton-Schulz iteration. Experiments across vision and language\nbenchmarks show that AuON and its hybrid variant achieve performance comparable\nto strong baselines such as AdamW and Muon.\n  Code is available at: https://github.com/ryyzn9/AuON", "AI": {"tldr": "AuON is a linear-time optimizer that achieves strong performance without constructing semi-orthogonal matrices, using hyperbolic-cosine RMS scaling with normalization to preserve structural alignment and recondition ill-posed updates.", "motivation": "Traditional orthogonal gradient methods like SVD/QR have O(n^3) computational costs and underperform compared to SGD with momentum. Recent methods like Muon reduce complexity to O(n^2) but quadratic costs remain a bottleneck.", "method": "AuON bounds momentum updates under a spectral-norm trust region, using hyperbolic-cosine RMS scaling transformations with normalization to preserve directional information without explicit semi-orthogonalization. Also introduces Hybrid-AuON with one Newton-Schulz iteration.", "result": "Experiments across vision and language benchmarks show AuON and its hybrid variant achieve performance comparable to strong baselines like AdamW and Muon.", "conclusion": "AuON provides an efficient linear-time alternative to traditional orthogonal optimization methods, achieving competitive performance while avoiding the computational bottlenecks of quadratic or cubic complexity approaches."}}
{"id": "2509.24330", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24330", "abs": "https://arxiv.org/abs/2509.24330", "authors": ["Shiyuan Zuo", "Rongfei Fan", "Cheng Zhan", "Jie Xu", "Puning Zhao", "Han Hu"], "title": "H+: An Efficient Similarity-Aware Aggregation for Byzantine Resilient Federated Learning", "comment": null, "summary": "Federated Learning (FL) enables decentralized model training without sharing\nraw data. However, it remains vulnerable to Byzantine attacks, which can\ncompromise the aggregation of locally updated parameters at the central server.\nSimilarity-aware aggregation has emerged as an effective strategy to mitigate\nsuch attacks by identifying and filtering out malicious clients based on\nsimilarity between client model parameters and those derived from clean data,\ni.e., data that is uncorrupted and trustworthy. However, existing methods adopt\nthis strategy only in FL systems with clean data, making them inapplicable to\nsettings where such data is unavailable. In this paper, we propose H+, a novel\nsimilarity-aware aggregation approach that not only outperforms existing\nmethods in scenarios with clean data, but also extends applicability to FL\nsystems without any clean data. Specifically, H+ randomly selects\n$r$-dimensional segments from the $p$-dimensional parameter vectors uploaded to\nthe server and applies a similarity check function $H$ to compare each segment\nagainst a reference vector, preserving the most similar client vectors for\naggregation. The reference vector is derived either from existing robust\nalgorithms when clean data is unavailable or directly from clean data.\nRepeating this process $K$ times enables effective identification of honest\nclients. Moreover, H+ maintains low computational complexity, with an\nanalytical time complexity of $\\mathcal{O}(KMr)$, where $M$ is the number of\nclients and $Kr \\ll p$. Comprehensive experiments validate H+ as a\nstate-of-the-art (SOTA) method, demonstrating substantial robustness\nimprovements over existing approaches under varying Byzantine attack ratios and\nmultiple types of traditional Byzantine attacks, across all evaluated scenarios\nand benchmark datasets.", "AI": {"tldr": "H+ is a novel similarity-aware aggregation method for Federated Learning that defends against Byzantine attacks without requiring clean data, using random parameter segment comparisons to identify honest clients.", "motivation": "Existing similarity-aware aggregation methods only work when clean data is available, making them inapplicable to FL systems without trustworthy data. There's a need for Byzantine-resilient methods that don't rely on clean data.", "method": "H+ randomly selects r-dimensional segments from p-dimensional parameter vectors and applies similarity check function H to compare each segment against a reference vector. The reference vector comes from robust algorithms (no clean data) or clean data. Process repeats K times to identify honest clients.", "result": "H+ outperforms existing methods in scenarios with clean data and extends to FL systems without clean data. It achieves state-of-the-art robustness under varying Byzantine attack ratios and multiple attack types across all evaluated scenarios and datasets.", "conclusion": "H+ provides an effective Byzantine-resilient aggregation approach with low computational complexity (O(KMr)), making it applicable to FL systems both with and without clean data availability."}}
{"id": "2509.24332", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24332", "abs": "https://arxiv.org/abs/2509.24332", "authors": ["Siyang Li", "Yize Chen", "Yan Guo", "Ming Huang", "Hui Xiong"], "title": "Towards Generalizable PDE Dynamics Forecasting via Physics-Guided Invariant Learning", "comment": "27 pages, 13 figues. In Submission", "summary": "Advanced deep learning-based approaches have been actively applied to\nforecast the spatiotemporal physical dynamics governed by partial differential\nequations (PDEs), which acts as a critical procedure in tackling many science\nand engineering problems. As real-world physical environments like PDE system\nparameters are always capricious, how to generalize across unseen\nout-of-distribution (OOD) forecasting scenarios using limited training data is\nof great importance. To bridge this barrier, existing methods focus on\ndiscovering domain-generalizable representations across various PDE dynamics\ntrajectories. However, their zero-shot OOD generalization capability remains\ndeficient, since extra test-time samples for domain-specific adaptation are\nstill required. This is because the fundamental physical invariance in PDE\ndynamical systems are yet to be investigated or integrated. To this end, we\nfirst explicitly define a two-fold PDE invariance principle, which points out\nthat ingredient operators and their composition relationships remain invariant\nacross different domains and PDE system evolution. Next, to capture this\ntwo-fold PDE invariance, we propose a physics-guided invariant learning method\ntermed iMOOE, featuring an Invariance-aligned Mixture Of Operator Expert\narchitecture and a frequency-enriched invariant learning objective. Extensive\nexperiments across simulated benchmarks and real-world applications validate\niMOOE's superior in-distribution performance and zero-shot generalization\ncapabilities on diverse OOD forecasting scenarios.", "AI": {"tldr": "The paper proposes iMOOE, a physics-guided invariant learning method for spatiotemporal PDE forecasting that achieves superior zero-shot OOD generalization by capturing fundamental PDE invariance principles.", "motivation": "Real-world PDE systems have varying parameters, making generalization across unseen out-of-distribution scenarios challenging. Existing methods require test-time adaptation samples and fail to capture fundamental physical invariances.", "method": "Defines a two-fold PDE invariance principle (ingredient operators and composition relationships remain invariant), then proposes iMOOE with Invariance-aligned Mixture Of Operator Expert architecture and frequency-enriched invariant learning objective.", "result": "Extensive experiments show iMOOE achieves superior in-distribution performance and zero-shot generalization capabilities across diverse OOD forecasting scenarios in both simulated benchmarks and real-world applications.", "conclusion": "The proposed physics-guided invariant learning approach effectively captures fundamental PDE invariances, enabling robust zero-shot generalization without requiring test-time adaptation samples."}}
{"id": "2509.24341", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24341", "abs": "https://arxiv.org/abs/2509.24341", "authors": ["Qingquan Zhang", "Ziqi Wang", "Yuchen Li", "Keyuan Zhang", "Bo Yuan", "Jialin Liu"], "title": "Expanding Horizons of Level Diversity via Multi-objective Evolutionary Learning", "comment": "12 pages,6 figures", "summary": "In recent years, the generation of diverse game levels has gained increasing\ninterest, contributing to a richer and more engaging gaming experience. A\nnumber of level diversity metrics have been proposed in literature, which are\nnaturally multi-dimensional, leading to conflicted, complementary, or both\nrelationships among these dimensions. However, existing level generation\napproaches often fail to comprehensively assess diversity across those\ndimensions. This paper aims to expand horizons of level diversity by\nconsidering multi-dimensional diversity when training generative models. We\nformulate the model training as a multi-objective learning problem, where each\ndiversity metric is treated as a distinct objective. Furthermore, a\nmulti-objective evolutionary learning framework that optimises multiple\ndiversity metrics simultaneously throughout the model training process is\nproposed. Our case study on the commonly used benchmark Super Mario Bros.\ndemonstrates that our proposed framework can enhance multi-dimensional\ndiversity and identify a Pareto front of generative models, which provides a\nrange of tradeoffs among playability and two representative diversity metrics,\nincluding a content-based one and a player-centered one. Such capability\nenables decision-makers to make informed choices when selecting generators\naccommodating a variety of scenarios and the diverse needs of players and\ndesigners.", "AI": {"tldr": "This paper proposes a multi-objective evolutionary learning framework to enhance multi-dimensional diversity in game level generation, treating each diversity metric as a distinct objective and optimizing them simultaneously during model training.", "motivation": "Existing level generation approaches fail to comprehensively assess diversity across multiple dimensions, as diversity metrics are naturally multi-dimensional with conflicted, complementary, or both relationships among dimensions.", "method": "Formulate model training as a multi-objective learning problem where each diversity metric is a distinct objective, and propose a multi-objective evolutionary learning framework that optimizes multiple diversity metrics simultaneously throughout training.", "result": "Case study on Super Mario Bros demonstrates enhanced multi-dimensional diversity and identification of a Pareto front of generative models providing tradeoffs among playability and two representative diversity metrics (content-based and player-centered).", "conclusion": "The framework enables decision-makers to make informed choices when selecting generators accommodating various scenarios and diverse needs of players and designers by providing a range of tradeoffs."}}
{"id": "2509.24372", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.24372", "abs": "https://arxiv.org/abs/2509.24372", "authors": ["Xin Qiu", "Yulu Gan", "Conor F. Hayes", "Qiyao Liang", "Elliot Meyerson", "Babak Hodjat", "Risto Miikkulainen"], "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning", "comment": "24 pages, including the appendix", "summary": "Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is\na critical step in the AI deployment pipeline. Reinforcement learning (RL) is\narguably the most prominent fine-tuning method, contributing to the birth of\nmany state-of-the-art LLMs. In contrast, evolution strategies (ES), which once\nshowed comparable performance to RL on models with a few million parameters,\nwas neglected due to the pessimistic perception of its scalability to larger\nmodels. In this work, we report the first successful attempt to scale up ES for\nfine-tuning the full parameters of LLMs, showing the surprising fact that ES\ncan search efficiently over billions of parameters and outperform existing RL\nfine-tuning methods in multiple respects, including sample efficiency,\ntolerance to long-horizon rewards, robustness to different base LLMs, less\ntendency to reward hacking, and more stable performance across runs. It\ntherefore serves as a basis to unlock a new direction in LLM fine-tuning beyond\nwhat current RL techniques provide. The source codes are provided at:\nhttps://github.com/VsonicV/es-fine-tuning-paper.", "AI": {"tldr": "This paper successfully scales up Evolution Strategies (ES) for fine-tuning large language models, showing ES outperforms RL methods in sample efficiency, reward tolerance, robustness, and stability.", "motivation": "Evolution strategies were neglected for LLM fine-tuning due to perceived scalability limitations, despite once showing comparable performance to RL on smaller models.", "method": "Scaling up evolution strategies to fine-tune the full parameters of large language models with billions of parameters.", "result": "ES can efficiently search over billions of parameters and outperforms RL methods in multiple aspects: better sample efficiency, tolerance to long-horizon rewards, robustness across different base LLMs, less reward hacking, and more stable performance.", "conclusion": "ES serves as a new direction for LLM fine-tuning beyond current RL techniques, unlocking previously overlooked potential in evolutionary approaches."}}
{"id": "2509.24378", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24378", "abs": "https://arxiv.org/abs/2509.24378", "authors": ["Tian Lan", "Hao Duong Le", "Jinbo Li", "Wenjun He", "Meng Wang", "Chenghao Liu", "Chen Zhang"], "title": "AXIS: Explainable Time Series Anomaly Detection with Large Language Models", "comment": null, "summary": "Time-series anomaly detection (TSAD) increasingly demands explanations that\narticulate not only if an anomaly occurred, but also what pattern it exhibits\nand why it is anomalous. Leveraging the impressive explanatory capabilities of\nLarge Language Models (LLMs), recent works have attempted to treat time series\nas text for explainable TSAD. However, this approach faces a fundamental\nchallenge: LLMs operate on discrete tokens and struggle to directly process\nlong, continuous signals. Consequently, naive time-to-text serialization\nsuffers from a lack of contextual grounding and representation alignment\nbetween the two modalities. To address this gap, we introduce AXIS, a framework\nthat conditions a frozen LLM for nuanced time-series understanding. Instead of\ndirect serialization, AXIS enriches the LLM's input with three complementary\nhints derived from the series: (i) a symbolic numeric hint for numerical\ngrounding, (ii) a context-integrated, step-aligned hint distilled from a\npretrained time-series encoder to capture fine-grained dynamics, and (iii) a\ntask-prior hint that encodes global anomaly characteristics. Furthermore, to\nfacilitate robust evaluation of explainability, we introduce a new benchmark\nfeaturing multi-format questions and rationales that supervise contextual\ngrounding and pattern-level semantics. Extensive experiments, including both\nLLM-based and human evaluations, demonstrate that AXIS yields explanations of\nsignificantly higher quality and achieves competitive detection accuracy\ncompared to general-purpose LLMs, specialized time-series LLMs, and time-series\nVision Language Models.", "AI": {"tldr": "AXIS is a framework that conditions frozen LLMs for explainable time-series anomaly detection by providing three complementary hints: symbolic numeric grounding, context-integrated step-aligned features, and task-prior anomaly characteristics, achieving superior explanation quality and competitive detection accuracy.", "motivation": "Current LLM-based approaches for explainable time-series anomaly detection struggle with processing continuous signals and lack contextual grounding between time series and text modalities.", "method": "AXIS conditions frozen LLMs with three hints: (1) symbolic numeric hint for numerical grounding, (2) context-integrated step-aligned hint from pretrained time-series encoder, and (3) task-prior hint encoding global anomaly characteristics.", "result": "AXIS yields significantly higher quality explanations and achieves competitive detection accuracy compared to general-purpose LLMs, specialized time-series LLMs, and time-series Vision Language Models.", "conclusion": "The AXIS framework effectively bridges the modality gap between time series and LLMs, enabling nuanced time-series understanding and high-quality explainable anomaly detection."}}
{"id": "2509.24406", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24406", "abs": "https://arxiv.org/abs/2509.24406", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Muon: Training and Trade-offs with Latent Attention and MoE", "comment": null, "summary": "We present a comprehensive theoretical and empirical study of the Muon\noptimizer for training transformers only with a small to medium decoder (30M -\n200M parameters), with an emphasis on its mathematical foundations, convergence\nproperties and synergistic interactions with modern architectural\noptimizations. Building on recent work showing Muon's scalability, we provide\nrigorous theoretical analysis including: (i)showing the convergence rate under\nstandard assumptions, (ii) spectral regularization properties that prevent\ngradient explosion, (iii) connection to natural gradient descent on the Stiefel\nmanifold, and (iv) equivalence to steepest gradient descent under the spectral\nnorm. Crucially, we demonstrate that Muon expands the Pareto frontier in the\ncompute-time trade-off by maintaining superior data efficiency at large batch\nsizes, a key finding of~\\cite{essentialai2025muon} that we validate across our\nmodel scales. Empirically, Muon reaches the target loss with 48-52\\% of the\ntraining calculated by AdamW while maintaining or improving the final\nperplexity, consistent with larger-scale results. When combined with Multi-Head\nLatent Attention (MLA) and Mixture-of-Experts (MoE), we observe multiplicative\nefficiency gains: MLA+MoE+Muon achieves 68\\% memory reduction and 3.2$\\times$\ninference speedup, while improving perplexity by 8-12\\%. We provide detailed\nprocedures on 15 architectural and optimizer components, stability analyzes\nacross 100+ training runs, and practical implementation guidelines including\nNewton-Schulz coefficients $(3.4445, -4.7750, 2.0315)$ optimized\nby~\\cite{su2024muonblog}. Our theoretical analysis and comprehensive\nexperiments establish Muon as a principled, robust alternative to AdamW that\nparticularly excels when combined with modern efficiency techniques and\nlarge-batch training regimes.", "AI": {"tldr": "Muon optimizer provides theoretical convergence guarantees and practical efficiency gains for training small-to-medium transformers, achieving 48-52% less compute than AdamW while maintaining or improving performance.", "motivation": "To establish Muon as a principled alternative to AdamW with rigorous theoretical foundations and demonstrate its synergistic benefits with modern architectural optimizations like MLA and MoE.", "method": "Theoretical analysis of convergence rates, spectral regularization, connections to natural gradient descent, and empirical validation across 100+ training runs with 15 architectural components.", "result": "Muon achieves target loss with 48-52% less training compute than AdamW, maintains or improves perplexity, and when combined with MLA+MoE achieves 68% memory reduction and 3.2x inference speedup with 8-12% perplexity improvement.", "conclusion": "Muon is a robust, principled optimizer that expands the Pareto frontier in compute-time trade-offs, particularly excelling with modern efficiency techniques and large-batch training."}}
{"id": "2509.24414", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24414", "abs": "https://arxiv.org/abs/2509.24414", "authors": ["Tao Yin", "Xiaohong Zhang", "Shaochen Fu", "Zhibin Zhang", "Li Huang", "Yiyuan Yang", "Kaixiang Yang", "Meng Yan"], "title": "ScatterAD: Temporal-Topological Scattering Mechanism for Time Series Anomaly Detection", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "One main challenge in time series anomaly detection for industrial IoT lies\nin the complex spatio-temporal couplings within multivariate data. However,\ntraditional anomaly detection methods focus on modeling spatial or temporal\ndependencies independently, resulting in suboptimal representation learning and\nlimited sensitivity to anomalous dispersion in high-dimensional spaces. In this\nwork, we conduct an empirical analysis showing that both normal and anomalous\nsamples tend to scatter in high-dimensional space, especially anomalous samples\nare markedly more dispersed. We formalize this dispersion phenomenon as\nscattering, quantified by the mean pairwise distance among sample\nrepresentations, and leverage it as an inductive signal to enhance\nspatio-temporal anomaly detection. Technically, we propose ScatterAD to model\nrepresentation scattering across temporal and topological dimensions. ScatterAD\nincorporates a topological encoder for capturing graph-structured scattering\nand a temporal encoder for constraining over-scattering through mean squared\nerror minimization between neighboring time steps. We introduce a contrastive\nfusion mechanism to ensure the complementarity of the learned temporal and\ntopological representations. Additionally, we theoretically show that\nmaximizing the conditional mutual information between temporal and topological\nviews improves cross-view consistency and enhances more discriminative\nrepresentations. Extensive experiments on multiple public benchmarks show that\nScatterAD achieves state-of-the-art performance on multivariate time series\nanomaly detection. Code is available at this repository:\nhttps://github.com/jk-sounds/ScatterAD.", "AI": {"tldr": "ScatterAD is a novel anomaly detection method for industrial IoT time series that models representation scattering across temporal and topological dimensions, leveraging dispersion patterns in high-dimensional space to improve detection sensitivity.", "motivation": "Traditional anomaly detection methods independently model spatial or temporal dependencies, leading to suboptimal representation learning and limited sensitivity to anomalous dispersion in high-dimensional spaces.", "method": "Proposes ScatterAD with topological encoder for graph-structured scattering, temporal encoder for constraining over-scattering, and contrastive fusion mechanism for complementary temporal-topological representations. Uses mean pairwise distance to quantify scattering.", "result": "Extensive experiments on multiple public benchmarks show ScatterAD achieves state-of-the-art performance on multivariate time series anomaly detection.", "conclusion": "Modeling representation scattering across temporal and topological dimensions effectively enhances spatio-temporal anomaly detection by leveraging dispersion patterns as inductive signals."}}
{"id": "2509.24425", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.24425", "abs": "https://arxiv.org/abs/2509.24425", "authors": ["Jingtao Zhang", "Yi Liu", "Qi Shen", "Changhong Wang"], "title": "BiHDTrans: binary hyperdimensional transformer for efficient multivariate time series classification", "comment": null, "summary": "The proliferation of Internet-of-Things (IoT) devices has led to an\nunprecedented volume of multivariate time series (MTS) data, requiring\nefficient and accurate processing for timely decision-making in\nresource-constrained edge environments. Hyperdimensional (HD) computing, with\nits inherent efficiency and parallelizability, has shown promise in\nclassification tasks but struggles to capture complex temporal patterns, while\nTransformers excel at sequence modeling but incur high computational and memory\noverhead. We introduce BiHDTrans, an efficient neurosymbolic binary\nhyperdimensional Transformer that integrates self-attention into the HD\ncomputing paradigm, unifying the representational efficiency of HD computing\nwith the temporal modeling power of Transformers. Empirically, BiHDTrans\noutperforms state-of-the-art (SOTA) HD computing models by at least 14.47% and\nachieves 6.67% higher accuracy on average than SOTA binary Transformers. With\nhardware acceleration on FPGA, our pipelined implementation leverages the\nindependent and identically distributed properties of high-dimensional\nrepresentations, delivering 39.4 times lower inference latency than SOTA binary\nTransformers. Theoretical analysis shows that binarizing in holographic\nhigh-dimensional space incurs significantly less information distortion than\ndirectly binarizing neural networks, explaining BiHDTrans's superior accuracy.\nFurthermore, dimensionality experiments confirm that BiHDTrans remains\ncompetitive even with a 64% reduction in hyperspace dimensionality, surpassing\nSOTA binary Transformers by 1-2% in accuracy with 4.4 times less model size, as\nwell as further reducing the latency by 49.8% compare to the full-dimensional\nbaseline. Together, these contributions bridge the gap between the\nexpressiveness of Transformers and the efficiency of HD computing, enabling\naccurate, scalable, and low-latency MTS classification.", "AI": {"tldr": "BiHDTrans is a neurosymbolic binary hyperdimensional Transformer that combines HD computing efficiency with Transformer temporal modeling, achieving superior accuracy and 39.4x lower latency than SOTA binary Transformers.", "motivation": "IoT devices generate massive multivariate time series data requiring efficient processing in resource-constrained edge environments. HD computing is efficient but struggles with temporal patterns, while Transformers excel at sequence modeling but have high computational overhead.", "method": "Integrates self-attention into HD computing paradigm, creating a neurosymbolic binary hyperdimensional Transformer that unifies HD computing representational efficiency with Transformer temporal modeling power.", "result": "Outperforms SOTA HD computing models by at least 14.47% and achieves 6.67% higher accuracy than SOTA binary Transformers. With FPGA acceleration, delivers 39.4x lower inference latency than SOTA binary Transformers.", "conclusion": "BiHDTrans bridges the gap between Transformer expressiveness and HD computing efficiency, enabling accurate, scalable, and low-latency MTS classification for edge environments."}}
{"id": "2509.24431", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24431", "abs": "https://arxiv.org/abs/2509.24431", "authors": ["Eleonora Grassucci", "Giordano Cicchetti", "Aurelio Uncini", "Danilo Comminiello"], "title": "Semantic Compression via Multimodal Representation Learning", "comment": null, "summary": "Multimodal representation learning produces high-dimensional embeddings that\nalign diverse modalities in a shared latent space. While this enables strong\ngeneralization, it also introduces scalability challenges, both in terms of\nstorage and downstream processing. A key open problem is how to achieve\nsemantic compression, reducing the memory footprint of multimodal embeddings\nwhile preserving their ability to represent shared semantic content across\nmodalities. In this paper, we prove a strong connection between reducing the\nmodality gap, which is the residual separation of embeddings from different\nmodalities, and the feasibility of post-training semantic compression. When the\ngap is sufficiently reduced, embeddings from different modalities but\nexpressing the same semantics share a common portion of the space. Therefore,\ntheir centroid is a faithful representation of such a semantic concept. This\nenables replacing multiple embeddings with a single centroid, yielding\nsignificant memory savings. We propose a novel approach for semantic\ncompression grounded on the latter intuition, operating directly on pretrained\nencoders. We demonstrate its effectiveness across diverse large-scale\nmultimodal downstream tasks. Our results highlight that modality alignment is a\nkey enabler for semantic compression, showing that the proposed approach\nachieves significant compression without sacrificing performance.", "AI": {"tldr": "This paper presents a semantic compression method for multimodal embeddings that reduces memory footprint by leveraging modality alignment, showing that when modality gap is minimized, embeddings from different modalities sharing the same semantics can be replaced with centroids without performance loss.", "motivation": "Multimodal representation learning creates high-dimensional embeddings that align different modalities but introduces scalability challenges in storage and processing. The key problem is achieving semantic compression while preserving the ability to represent shared semantic content across modalities.", "method": "The approach proves that reducing modality gap enables post-training semantic compression. When embeddings from different modalities expressing the same semantics share a common space portion, their centroid becomes a faithful semantic representation. The method operates directly on pretrained encoders, replacing multiple embeddings with single centroids.", "result": "The proposed semantic compression approach demonstrates effectiveness across diverse large-scale multimodal downstream tasks, achieving significant compression without sacrificing performance.", "conclusion": "Modality alignment is a key enabler for semantic compression, and the proposed centroid-based approach successfully compresses multimodal embeddings while maintaining their semantic representation capabilities."}}
{"id": "2509.24436", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.24436", "abs": "https://arxiv.org/abs/2509.24436", "authors": ["Yingshi Chen"], "title": "EOE: Evolutionary Optimization of Experts for Training Language Models", "comment": "6 pages, 2 figures", "summary": "This paper presents an evolutionary framework for the training of large\nlanguage models(LLM). The models are divided into several\nexperts(sub-networks), which have the same structure but different parameter\nvalues. Only one expert is trained at each step. After the classical AdamW\noptimization, some evolutionary operators(crossover, PSO, and mutation) act on\nthe tensor weights between the current expert and the best expert. So current\nexpert would learn the experience of best expert. The direction of best expert\nwould help current expert's loss decrease faster. Finally, only save the weight\nof the best expert. Experiments show that best expert would achieve nearly the\nsame accuracy as the full model. This would greatly reduce the size of the\nmodel for inference. Since only one expert is trained at each step, the\ntraining needs much less memory and has much higher throughput. Experiments\nshow that the throughput would accelerate more than ten times! Our source code\nis available. It's a pure c++/cu framework, which is suitable for easy\ndeployment on PCs and edge computing devices.", "AI": {"tldr": "An evolutionary framework for training LLMs using expert sub-networks with evolutionary operators to reduce model size and accelerate training throughput by 10x.", "motivation": "To reduce the memory requirements and computational cost of training large language models while maintaining accuracy, making them more suitable for deployment on PCs and edge devices.", "method": "Divide LLM into multiple expert sub-networks with same structure but different parameters. Train only one expert per step using AdamW optimization, then apply evolutionary operators (crossover, PSO, mutation) between current and best expert to transfer knowledge.", "result": "Best expert achieves nearly the same accuracy as full model while greatly reducing inference size. Training throughput accelerates more than 10 times due to memory efficiency.", "conclusion": "The evolutionary framework successfully enables efficient LLM training with significant memory reduction and throughput improvement while maintaining model accuracy, suitable for edge deployment."}}
{"id": "2509.24462", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.24462", "abs": "https://arxiv.org/abs/2509.24462", "authors": ["Zifan Wang", "Xinlei Yi", "Xenia Konti", "Michael M. Zavlanos", "Karl H. Johansson"], "title": "Distributionally Robust Federated Learning with Outlier Resilience", "comment": null, "summary": "Federated learning (FL) enables collaborative model training without direct\ndata sharing, but its performance can degrade significantly in the presence of\ndata distribution perturbations. Distributionally robust optimization (DRO)\nprovides a principled framework for handling this by optimizing performance\nagainst the worst-case distributions within a prescribed ambiguity set.\nHowever, existing DRO-based FL methods often overlook the detrimental impact of\noutliers in local datasets, which can disproportionately bias the learned\nmodels. In this work, we study distributionally robust federated learning with\nexplicit outlier resilience. We introduce a novel ambiguity set based on the\nunbalanced Wasserstein distance, which jointly captures geometric\ndistributional shifts and incorporates a non-geometric Kullback--Leibler\npenalization to mitigate the influence of outliers. This formulation naturally\nleads to a challenging min--max--max optimization problem. To enable\ndecentralized training, we reformulate the problem as a tractable Lagrangian\npenalty optimization, which admits robustness certificates. Building on this\nreformulation, we propose the distributionally outlier-robust federated\nlearning algorithm and establish its convergence guarantees. Extensive\nexperiments on both synthetic and real-world datasets demonstrate the\neffectiveness of our approach.", "AI": {"tldr": "This paper proposes a distributionally robust federated learning method with explicit outlier resilience using unbalanced Wasserstein distance and KL penalization to handle data distribution shifts and outliers.", "motivation": "Federated learning performance degrades with data distribution perturbations, and existing DRO-based FL methods overlook the detrimental impact of outliers in local datasets that disproportionately bias learned models.", "method": "Introduces a novel ambiguity set based on unbalanced Wasserstein distance that captures geometric distributional shifts and incorporates KL penalization to mitigate outlier influence. Reformulates the min-max-max optimization as tractable Lagrangian penalty optimization for decentralized training.", "result": "Proposes the distributionally outlier-robust federated learning algorithm with established convergence guarantees. Extensive experiments on synthetic and real-world datasets demonstrate effectiveness.", "conclusion": "The proposed approach effectively handles both distribution shifts and outliers in federated learning through a principled DRO framework with explicit outlier resilience mechanisms."}}
{"id": "2509.24467", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24467", "abs": "https://arxiv.org/abs/2509.24467", "authors": ["Maedeh Zarvandi", "Michael Timothy", "Theresa Wasserer", "Debarghya Ghoshdastidar"], "title": "Interpretable Kernel Representation Learning at Scale: A Unified Framework Utilizing Nystr\u00f6m Approximation", "comment": "19 Pages, 3 figures", "summary": "Kernel methods provide a theoretically grounded framework for non-linear and\nnon-parametric learning, with strong analytic foundations and statistical\nguarantees. Yet, their scalability has long been limited by prohibitive time\nand memory costs. While progress has been made in scaling kernel regression, no\nframework exists for scalable kernel-based representation learning, restricting\ntheir use in the era of foundation models where representations are learned\nfrom massive unlabeled data. We introduce KREPES -- a unified, scalable\nframework for kernel-based representation learning via Nystr\\\"om approximation.\nKREPES accommodates a wide range of unsupervised and self-supervised losses,\nand experiments on large image and tabular datasets demonstrate its efficiency.\nCrucially, KREPES enables principled interpretability of the learned\nrepresentations, an immediate benefit over deep models, which we substantiate\nthrough dedicated analysis.", "AI": {"tldr": "KREPES is a scalable framework for kernel-based representation learning using Nystr\u00f6m approximation, enabling efficient learning on large datasets while maintaining interpretability.", "motivation": "Kernel methods have strong theoretical foundations but suffer from scalability issues, especially for representation learning with massive unlabeled data in the foundation model era.", "method": "Uses Nystr\u00f6m approximation to create a unified framework for kernel-based representation learning that supports various unsupervised and self-supervised losses.", "result": "Demonstrated efficiency on large image and tabular datasets, with principled interpretability of learned representations compared to deep models.", "conclusion": "KREPES successfully addresses the scalability limitations of kernel methods for representation learning while providing interpretability benefits over deep learning approaches."}}
{"id": "2509.24472", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24472", "abs": "https://arxiv.org/abs/2509.24472", "authors": ["Ran Elbaz", "Guy Bar-Shalom", "Yam Eitan", "Fabrizio Frasca", "Haggai Maron"], "title": "FS-KAN: Permutation Equivariant Kolmogorov-Arnold Networks via Function Sharing", "comment": null, "summary": "Permutation equivariant neural networks employing parameter-sharing schemes\nhave emerged as powerful models for leveraging a wide range of data symmetries,\nsignificantly enhancing the generalization and computational efficiency of the\nresulting models. Recently, Kolmogorov-Arnold Networks (KANs) have demonstrated\npromise through their improved interpretability and expressivity compared to\ntraditional architectures based on MLPs. While equivariant KANs have been\nexplored in recent literature for a few specific data types, a principled\nframework for applying them to data with permutation symmetries in a general\ncontext remains absent. This paper introduces Function Sharing KAN (FS-KAN), a\nprincipled approach to constructing equivariant and invariant KA layers for\narbitrary permutation symmetry groups, unifying and significantly extending\nprevious work in this domain. We derive the basic construction of these FS-KAN\nlayers by generalizing parameter-sharing schemes to the Kolmogorov-Arnold setup\nand provide a theoretical analysis demonstrating that FS-KANs have the same\nexpressive power as networks that use standard parameter-sharing layers,\nallowing us to transfer well-known and important expressivity results from\nparameter-sharing networks to FS-KANs. Empirical evaluations on multiple data\ntypes and symmetry groups show that FS-KANs exhibit superior data efficiency\ncompared to standard parameter-sharing layers, by a wide margin in certain\ncases, while preserving the interpretability and adaptability of KANs, making\nthem an excellent architecture choice in low-data regimes.", "AI": {"tldr": "FS-KAN is a principled framework for building permutation equivariant Kolmogorov-Arnold Networks that unifies and extends previous work, achieving superior data efficiency while maintaining KANs' interpretability.", "motivation": "To develop a general framework for applying equivariant KANs to data with permutation symmetries, addressing the lack of principled approaches in existing literature.", "method": "Generalizes parameter-sharing schemes to the Kolmogorov-Arnold setup, constructing equivariant and invariant KA layers for arbitrary permutation symmetry groups through function sharing.", "result": "FS-KANs demonstrate superior data efficiency compared to standard parameter-sharing layers, with significant margins in some cases, while preserving interpretability and adaptability.", "conclusion": "FS-KANs provide an excellent architecture choice for low-data regimes, combining the expressive power of parameter-sharing networks with KANs' interpretability and data efficiency advantages."}}
{"id": "2509.24483", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24483", "abs": "https://arxiv.org/abs/2509.24483", "authors": ["Minh Le", "Bao-Ngoc Dao", "Huy Nguyen", "Quyen Tran", "Anh Nguyen", "Nhat Ho"], "title": "One-Prompt Strikes Back: Sparse Mixture of Experts for Prompt-based Continual Learning", "comment": "40 pages, 9 figures", "summary": "Prompt-based methods have recently gained prominence in Continual Learning\n(CL) due to their strong performance and memory efficiency. A prevalent\nstrategy in this paradigm assigns a dedicated subset of prompts to each task,\nwhich, while effective, incurs substantial computational overhead and causes\nmemory requirements to scale linearly with the number of tasks. Conversely,\napproaches employing a single shared prompt across tasks offer greater\nefficiency but often suffer from degraded performance due to knowledge\ninterference. To reconcile this trade-off, we propose SMoPE, a novel framework\nthat integrates the benefits of both task-specific and shared prompt\nstrategies. Inspired by recent findings on the relationship between Prefix\nTuning and Mixture of Experts (MoE), SMoPE organizes a shared prompt into\nmultiple \"prompt experts\" within a sparse MoE architecture. For each input,\nonly a select subset of relevant experts is activated, effectively mitigating\ninterference. To facilitate expert selection, we introduce a prompt-attention\nscore aggregation mechanism that computes a unified proxy score for each\nexpert, enabling dynamic and sparse activation. Additionally, we propose an\nadaptive noise mechanism to encourage balanced expert utilization while\npreserving knowledge from prior tasks. To further enhance expert\nspecialization, we design a prototype-based loss function that leverages prefix\nkeys as implicit memory representations. Extensive experiments across multiple\nCL benchmarks demonstrate that SMoPE consistently outperforms task-specific\nprompt methods and achieves performance competitive with state-of-the-art\napproaches, all while significantly reducing parameter counts and computational\ncosts.", "AI": {"tldr": "SMoPE is a novel continual learning framework that uses a sparse mixture of experts architecture with shared prompts to balance performance and efficiency, outperforming task-specific prompt methods while reducing computational costs.", "motivation": "To address the trade-off between task-specific prompts (high performance but computational overhead) and shared prompts (efficient but suffer from knowledge interference) in continual learning.", "method": "Organizes shared prompts into multiple \"prompt experts\" in a sparse MoE architecture, uses prompt-attention score aggregation for dynamic expert selection, adaptive noise mechanism for balanced utilization, and prototype-based loss for expert specialization.", "result": "Consistently outperforms task-specific prompt methods and achieves competitive performance with state-of-the-art approaches while significantly reducing parameter counts and computational costs across multiple CL benchmarks.", "conclusion": "SMoPE successfully reconciles the efficiency-performance trade-off in prompt-based continual learning through its sparse expert architecture and dynamic activation mechanisms."}}
{"id": "2509.24492", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24492", "abs": "https://arxiv.org/abs/2509.24492", "authors": ["Charmaine Barker", "Daniel Bethell", "Simos Gerasimou"], "title": "Guided Uncertainty Learning Using a Post-Hoc Evidential Meta-Model", "comment": null, "summary": "Reliable uncertainty quantification remains a major obstacle to the\ndeployment of deep learning models under distributional shift. Existing\npost-hoc approaches that retrofit pretrained models either inherit misplaced\nconfidence or merely reshape predictions, without teaching the model when to be\nuncertain. We introduce GUIDE, a lightweight evidential learning meta-model\napproach that attaches to a frozen deep learning model and explicitly learns\nhow and when to be uncertain. GUIDE identifies salient internal features via a\ncalibration stage, and then employs these features to construct a noise-driven\ncurriculum that teaches the model how and when to express uncertainty. GUIDE\nrequires no retraining, no architectural modifications, and no manual\nintermediate-layer selection to the base deep learning model, thus ensuring\nbroad applicability and minimal user intervention. The resulting model avoids\ndistilling overconfidence from the base model, improves out-of-distribution\ndetection by ~77% and adversarial attack detection by ~80%, while preserving\nin-distribution performance. Across diverse benchmarks, GUIDE consistently\noutperforms state-of-the-art approaches, evidencing the need for actively\nguiding uncertainty to close the gap between predictive confidence and\nreliability.", "AI": {"tldr": "GUIDE is a lightweight evidential learning meta-model that attaches to frozen deep learning models to teach them how and when to express uncertainty without retraining or architectural changes, improving OOD detection by ~77% and adversarial detection by ~80%.", "motivation": "Existing post-hoc uncertainty quantification methods either inherit misplaced confidence or merely reshape predictions without teaching models when to be uncertain, creating a reliability gap.", "method": "GUIDE identifies salient internal features via calibration, then uses these features to construct a noise-driven curriculum that explicitly teaches the model how and when to express uncertainty.", "result": "GUIDE improves out-of-distribution detection by ~77% and adversarial attack detection by ~80% while preserving in-distribution performance, consistently outperforming state-of-the-art approaches.", "conclusion": "GUIDE demonstrates the need for actively guiding uncertainty to close the gap between predictive confidence and reliability, providing broad applicability with minimal user intervention."}}
{"id": "2509.24496", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24496", "abs": "https://arxiv.org/abs/2509.24496", "authors": ["Zhaomin Wu", "Haodong Zhao", "Ziyang Wang", "Jizhou Guo", "Qian Wang", "Bingsheng He"], "title": "LLM DNA: Tracing Model Evolution via Functional Representations", "comment": null, "summary": "The explosive growth of large language models (LLMs) has created a vast but\nopaque landscape: millions of models exist, yet their evolutionary\nrelationships through fine-tuning, distillation, or adaptation are often\nundocumented or unclear, complicating LLM management. Existing methods are\nlimited by task specificity, fixed model sets, or strict assumptions about\ntokenizers or architectures. Inspired by biological DNA, we address these\nlimitations by mathematically defining LLM DNA as a low-dimensional,\nbi-Lipschitz representation of functional behavior. We prove that LLM DNA\nsatisfies inheritance and genetic determinism properties and establish the\nexistence of DNA. Building on this theory, we derive a general, scalable,\ntraining-free pipeline for DNA extraction. In experiments across 305 LLMs, DNA\naligns with prior studies on limited subsets and achieves superior or\ncompetitive performance on specific tasks. Beyond these tasks, DNA comparisons\nuncover previously undocumented relationships among LLMs. We further construct\nthe evolutionary tree of LLMs using phylogenetic algorithms, which align with\nshifts from encoder-decoder to decoder-only architectures, reflect temporal\nprogression, and reveal distinct evolutionary speeds across LLM families.", "AI": {"tldr": "This paper introduces LLM DNA, a mathematical representation of large language models' functional behavior that enables tracking evolutionary relationships through fine-tuning and adaptation processes.", "motivation": "The rapid growth of LLMs has created millions of models with undocumented evolutionary relationships, making LLM management difficult. Existing methods are limited by task specificity, fixed model sets, or strict assumptions.", "method": "The authors mathematically define LLM DNA as a low-dimensional, bi-Lipschitz representation of functional behavior, prove its inheritance properties, and develop a training-free pipeline for DNA extraction. They apply phylogenetic algorithms to construct evolutionary trees.", "result": "Experiments across 305 LLMs show DNA aligns with prior studies and achieves competitive performance on specific tasks. DNA comparisons reveal undocumented relationships and evolutionary trees show architectural shifts from encoder-decoder to decoder-only models, temporal progression, and varying evolutionary speeds.", "conclusion": "LLM DNA provides a general, scalable framework for understanding and tracking the evolutionary relationships among language models, overcoming limitations of existing methods and revealing previously undocumented connections."}}
{"id": "2509.24510", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24510", "abs": "https://arxiv.org/abs/2509.24510", "authors": ["Jonas H\u00fcbotter", "Patrik Wolf", "Alexander Shevchenko", "Dennis J\u00fcni", "Andreas Krause", "Gil Kur"], "title": "Specialization after Generalization: Towards Understanding Test-Time Training in Foundation Models", "comment": null, "summary": "Recent empirical studies have explored the idea of continuing to train a\nmodel at test-time for a given task, known as test-time training (TTT), and\nhave found it to yield significant performance improvements. However, there is\nlimited understanding of why and when TTT is effective. Earlier explanations\nmostly focused on the observation that TTT may help when applied to\nout-of-distribution adaptation or used with privileged data. However, the\ngrowing scale of foundation models with most test data being in-distribution\nquestions these explanations. We instead posit that foundation models remain\nglobally underparameterized, with TTT providing a mechanism for specialization\nafter generalization, focusing capacity on concepts relevant to the test task.\nSpecifically, under the linear representation hypothesis, we propose a model in\nwhich TTT achieves a substantially smaller in-distribution test error than\nglobal training. We empirically validate our model's key assumptions by\ntraining a sparse autoencoder on ImageNet, showing that semantically related\ndata points are explained by only a few shared concepts. Finally, we perform\nscaling studies across image and language tasks that confirm the practical\nimplications of our model, identifying the regimes where specialization is most\neffective.", "AI": {"tldr": "Test-time training (TTT) improves performance by allowing foundation models to specialize on test task concepts after initial generalization, addressing global underparameterization.", "motivation": "To understand why and when test-time training is effective, challenging previous explanations focused on out-of-distribution adaptation and proposing that foundation models remain globally underparameterized.", "method": "Proposed a theoretical model under linear representation hypothesis, empirically validated assumptions using sparse autoencoder on ImageNet, and performed scaling studies across image and language tasks.", "result": "TTT achieves substantially smaller in-distribution test error than global training, with semantic data points explained by few shared concepts, and identified regimes where specialization is most effective.", "conclusion": "TTT provides a mechanism for specialization after generalization, focusing model capacity on test-relevant concepts, with practical effectiveness confirmed across multiple domains."}}
{"id": "2509.24517", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24517", "abs": "https://arxiv.org/abs/2509.24517", "authors": ["Sophia N. Wilson", "Jens Hesselbjerg Christensen", "Raghavendra Selvan"], "title": "Trading Carbon for Physics: On the Resource Efficiency of Machine Learning for Spatio-Temporal Forecasting", "comment": "Source code available at\n  https://github.com/sophiawilson18/FlowMatching", "summary": "Development of modern deep learning methods has been driven primarily by the\npush for improving model efficacy (accuracy metrics). This sole focus on\nefficacy has steered development of large-scale models that require massive\nresources, and results in considerable carbon footprint across the model\nlife-cycle. In this work, we explore how physics inductive biases can offer\nuseful trade-offs between model efficacy and model efficiency (compute, energy,\nand carbon). We study a variety of models for spatio-temporal forecasting, a\ntask governed by physical laws and well-suited for exploring different levels\nof physics inductive bias. We show that embedding physics inductive biases into\nthe model design can yield substantial efficiency gains while retaining or even\nimproving efficacy for the tasks under consideration. In addition to using\nstandard physics-informed spatio-temporal models, we demonstrate the usefulness\nof more recent models like flow matching as a general purpose method for\nspatio-temporal forecasting. Our experiments show that incorporating physics\ninductive biases offer a principled way to improve the efficiency and reduce\nthe carbon footprint of machine learning models. We argue that model\nefficiency, along with model efficacy, should become a core consideration\ndriving machine learning model development and deployment.", "AI": {"tldr": "Physics inductive biases can improve model efficiency while maintaining or enhancing efficacy in spatio-temporal forecasting, offering a principled approach to reduce ML carbon footprint.", "motivation": "Current deep learning focuses solely on efficacy, leading to large models with high resource consumption and carbon footprint. The paper explores physics inductive biases as a way to balance efficacy with efficiency.", "method": "Study various spatio-temporal forecasting models with different levels of physics inductive bias, including standard physics-informed models and newer flow matching methods.", "result": "Physics inductive biases yield substantial efficiency gains while retaining or improving task efficacy. Flow matching shows promise as a general purpose method for spatio-temporal forecasting.", "conclusion": "Model efficiency should become a core consideration alongside efficacy in ML development to reduce carbon footprint and resource consumption."}}
{"id": "2509.24547", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24547", "abs": "https://arxiv.org/abs/2509.24547", "authors": ["Bao-Ngoc Dao", "Quang Nguyen", "Luyen Ngo Dinh", "Minh Le", "Linh Ngo Van"], "title": "LEAF: A Robust Expert-Based Framework for Few-Shot Continual Event Detection", "comment": null, "summary": "Few-shot Continual Event Detection (FCED) poses the dual challenges of\nlearning from limited data and mitigating catastrophic forgetting across\nsequential tasks. Existing approaches often suffer from severe forgetting due\nto the full fine-tuning of a shared base model, which leads to knowledge\ninterference between tasks. Moreover, they frequently rely on data augmentation\nstrategies that can introduce unnatural or semantically distorted inputs. To\naddress these limitations, we propose LEAF, a novel and robust expert-based\nframework for FCED. LEAF integrates a specialized mixture of experts\narchitecture into the base model, where each expert is parameterized with\nlow-rank adaptation (LoRA) matrices. A semantic-aware expert selection\nmechanism dynamically routes instances to the most relevant experts, enabling\nexpert specialization and reducing knowledge interference. To improve\ngeneralization in limited-data settings, LEAF incorporates a contrastive\nlearning objective guided by label descriptions, which capture high-level\nsemantic information about event types. Furthermore, to prevent overfitting on\nthe memory buffer, our framework employs a knowledge distillation strategy that\ntransfers knowledge from previous models to the current one. Extensive\nexperiments on multiple FCED benchmarks demonstrate that LEAF consistently\nachieves state-of-the-art performance.", "AI": {"tldr": "LEAF is a novel expert-based framework for Few-shot Continual Event Detection that uses mixture of experts with LoRA parameterization, semantic-aware expert selection, contrastive learning with label descriptions, and knowledge distillation to address catastrophic forgetting and data scarcity.", "motivation": "Existing FCED approaches suffer from severe forgetting due to full fine-tuning of shared base models causing knowledge interference, and rely on data augmentation that introduces unnatural inputs.", "method": "Integrates mixture of experts with LoRA matrices, semantic-aware expert selection for dynamic routing, contrastive learning guided by label descriptions, and knowledge distillation from previous models.", "result": "Extensive experiments on multiple FCED benchmarks show LEAF consistently achieves state-of-the-art performance.", "conclusion": "LEAF effectively addresses catastrophic forgetting and data scarcity in FCED through its expert-based architecture and specialized learning strategies."}}
{"id": "2509.24550", "categories": ["cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.24550", "abs": "https://arxiv.org/abs/2509.24550", "authors": ["Eleonora Grassucci", "Giuliano Galadini", "Giordano Cicchetti", "Aurelio Uncini", "Fabio Antonacci", "Danilo Comminiello"], "title": "Training-Free Multimodal Guidance for Video to Audio Generation", "comment": null, "summary": "Video-to-audio (V2A) generation aims to synthesize realistic and semantically\naligned audio from silent videos, with potential applications in video editing,\nFoley sound design, and assistive multimedia. Although the excellent results,\nexisting approaches either require costly joint training on large-scale paired\ndatasets or rely on pairwise similarities that may fail to capture global\nmultimodal coherence. In this work, we propose a novel training-free multimodal\nguidance mechanism for V2A diffusion that leverages the volume spanned by the\nmodality embeddings to enforce unified alignment across video, audio, and text.\nThe proposed multimodal diffusion guidance (MDG) provides a lightweight,\nplug-and-play control signal that can be applied on top of any pretrained audio\ndiffusion model without retraining. Experiments on VGGSound and AudioCaps\ndemonstrate that our MDG consistently improves perceptual quality and\nmultimodal alignment compared to baselines, proving the effectiveness of a\njoint multimodal guidance for V2A.", "AI": {"tldr": "A training-free multimodal guidance mechanism for video-to-audio generation that uses modality embeddings to enforce unified alignment across video, audio, and text without requiring retraining.", "motivation": "Existing V2A approaches require costly joint training on large datasets or rely on pairwise similarities that may fail to capture global multimodal coherence.", "method": "Proposed multimodal diffusion guidance (MDG) leverages the volume spanned by modality embeddings to enforce unified alignment across video, audio, and text as a plug-and-play control signal for pretrained audio diffusion models.", "result": "Experiments on VGGSound and AudioCaps show MDG consistently improves perceptual quality and multimodal alignment compared to baselines.", "conclusion": "The joint multimodal guidance approach proves effective for V2A generation, providing lightweight control without retraining requirements."}}
{"id": "2509.24552", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24552", "abs": "https://arxiv.org/abs/2509.24552", "authors": ["Lo\u00efc Cabannes", "Maximilian Beck", "Gergely Szilvasy", "Matthijs Douze", "Maria Lomeli", "Jade Copet", "Pierre-Emmanuel Mazar\u00e9", "Gabriel Synnaeve", "Herv\u00e9 J\u00e9gou"], "title": "Short window attention enables long-term memorization", "comment": null, "summary": "Recent works show that hybrid architectures combining sliding window softmax\nattention layers with linear recurrent neural network (RNN) layers outperform\nboth of these architectures taken separately. However, the impact of the window\nlength and the interplay between softmax attention and linear RNN layers remain\nunder-studied. In this work, we introduce SWAX, a hybrid architecture\nconsisting of sliding-window attention and xLSTM linear RNN layers.\n  A counter-intuitive finding with SWAX is that larger sliding windows do not\nimprove the long-context performance. In fact, short window attention\nencourages the model to better train the long-term memory of the xLSTM, by\nrelying less on the softmax attention mechanism for long context-retrieval.\n  The issue with small sliding windows is that they are detrimental for\nshort-context tasks, which could be solved with information from moderately\nlarger sliding windows otherwise. Therefore, we train SWAX by stochastically\nchanging the sliding window size, forcing the model to leverage both a longer\ncontext window and the xLSTM memory. SWAX trained with stochastic window sizes\nsignificantly outperforms regular window attention both on short and\nlong-context problems.", "AI": {"tldr": "SWAX is a hybrid architecture combining sliding-window attention and xLSTM linear RNN layers. Counter-intuitively, larger sliding windows don't improve long-context performance - short windows force better xLSTM memory training. Stochastic window size training solves short-context issues and outperforms regular window attention.", "motivation": "Hybrid architectures combining sliding window attention with linear RNN layers outperform individual architectures, but the impact of window length and interplay between attention and RNN layers remains under-studied.", "method": "Introduce SWAX architecture with sliding-window attention and xLSTM linear RNN layers. Use stochastic window size training to force model to leverage both longer context windows and xLSTM memory.", "result": "Larger sliding windows don't improve long-context performance. Short window attention encourages better xLSTM long-term memory training. Stochastic window size training significantly outperforms regular window attention on both short and long-context problems.", "conclusion": "SWAX with stochastic window sizes effectively combines the strengths of both attention mechanisms and xLSTM memory, achieving superior performance across different context lengths."}}
{"id": "2509.24556", "categories": ["cs.LG", "cs.AI", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2509.24556", "abs": "https://arxiv.org/abs/2509.24556", "authors": ["Hussam Sababha", "Bernat Font", "Mohammed Daqaq"], "title": "Deep Reinforcement Learning in Action: Real-Time Control of Vortex-Induced Vibrations", "comment": null, "summary": "This study showcases an experimental deployment of deep reinforcement\nlearning (DRL) for active flow control (AFC) of vortex-induced vibrations (VIV)\nin a circular cylinder at a high Reynolds number (Re = 3000) using rotary\nactuation. Departing from prior work that relied on low-Reynolds-number\nnumerical simulations, this research demonstrates real-time control in a\nchallenging experimental setting, successfully addressing practical constraints\nsuch as actuator delay. When the learning algorithm is provided with state\nfeedback alone (displacement and velocity of the oscillating cylinder), the DRL\nagent learns a low-frequency rotary control strategy that achieves up to 80%\nvibration suppression which leverages the traditional lock-on phenomenon. While\nthis level of suppression is significant, it remains below the performance\nachieved using high-frequency rotary actuation. The reduction in performance is\nattributed to actuation delays and can be mitigated by augmenting the learning\nalgorithm with past control actions. This enables the agent to learn a\nhigh-frequency rotary control strategy that effectively modifies vortex\nshedding and achieves over 95% vibration attenuation. These results demonstrate\nthe adaptability of DRL for AFC in real-world experiments and its ability to\novercome instrumental limitations such as actuation lag.", "AI": {"tldr": "Experimental deployment of deep reinforcement learning for active flow control of vortex-induced vibrations in a circular cylinder at Re=3000 using rotary actuation, achieving up to 95% vibration suppression.", "motivation": "To demonstrate real-time control in challenging experimental settings at high Reynolds numbers, addressing practical constraints like actuator delay that were not fully explored in prior low-Reynolds-number numerical simulations.", "method": "Used deep reinforcement learning with rotary actuation control, starting with state feedback (displacement and velocity) alone, then augmented with past control actions to address actuation delays.", "result": "With state feedback alone: achieved 80% vibration suppression using low-frequency rotary control. With augmented feedback: achieved over 95% vibration attenuation using high-frequency rotary control that modifies vortex shedding.", "conclusion": "DRL is adaptable for active flow control in real-world experiments and can overcome instrumental limitations like actuation lag, with performance improvements achieved by incorporating past control actions into the learning algorithm."}}
{"id": "2509.24559", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24559", "abs": "https://arxiv.org/abs/2509.24559", "authors": ["Marco Molinari", "Leonardo Nevali", "Saharsha Navani", "Omar G. Younis"], "title": "Emergent World Representations in OpenVLA", "comment": null, "summary": "Vision Language Action models (VLAs) trained with policy-based reinforcement\nlearning (RL) encode complex behaviors without explicitly modeling\nenvironmental dynamics. However, it remains unclear whether VLAs implicitly\nlearn world models, a hallmark of model-based RL. We propose an experimental\nmethodology using embedding arithmetic on state representations to probe\nwhether OpenVLA, the current state of the art in VLAs, contains latent\nknowledge of state transitions. Specifically, we measure the difference between\nembeddings of sequential environment states and test whether this transition\nvector is recoverable from intermediate model activations. Using linear and non\nlinear probes trained on activations across layers, we find statistically\nsignificant predictive ability on state transitions exceeding baselines\n(embeddings), indicating that OpenVLA encodes an internal world model (as\nopposed to the probes learning the state transitions). We investigate the\npredictive ability of an earlier checkpoint of OpenVLA, and uncover hints that\nthe world model emerges as training progresses. Finally, we outline a pipeline\nleveraging Sparse Autoencoders (SAEs) to analyze OpenVLA's world model.", "AI": {"tldr": "The paper investigates whether Vision Language Action models (VLAs) implicitly learn world models through embedding arithmetic analysis of state transitions in OpenVLA.", "motivation": "To determine if VLAs trained with policy-based RL encode internal world models, which is a key characteristic of model-based reinforcement learning.", "method": "Uses embedding arithmetic on state representations and trains linear/non-linear probes on model activations to test recoverability of state transition vectors. Also analyzes earlier checkpoints and proposes SAE pipeline.", "result": "Found statistically significant predictive ability for state transitions exceeding baseline embeddings, indicating OpenVLA encodes an internal world model. World model appears to emerge during training progression.", "conclusion": "OpenVLA implicitly learns world models despite being trained with policy-based RL, and the world model capability develops over the training process."}}
{"id": "2509.24573", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.24573", "abs": "https://arxiv.org/abs/2509.24573", "authors": ["Yusuf Guven", "Vincenzo Di Vito", "Ferdinando Fioretto"], "title": "Learning to Solve Optimization Problems Constrained with Partial Differential Equations", "comment": null, "summary": "Partial differential equation (PDE)-constrained optimization arises in many\nscientific and engineering domains, such as energy systems, fluid dynamics and\nmaterial design. In these problems, the decision variables (e.g., control\ninputs or design parameters) are tightly coupled with the PDE state variables,\nand the feasible set is implicitly defined by the governing PDE constraints.\nThis coupling makes the problems computationally demanding, as it requires\nhandling high dimensional discretization and dynamic constraints. To address\nthese challenges, this paper introduces a learning-based framework that\nintegrates a dynamic predictor with an optimization surrogate. The dynamic\npredictor, a novel time-discrete Neural Operator (Lu et al.), efficiently\napproximate system trajectories governed by PDE dynamics, while the\noptimization surrogate leverages proxy optimizer techniques (Kotary et al.) to\napproximate the associated optimal decisions. This dual-network design enables\nreal-time approximation of optimal strategies while explicitly capturing the\ncoupling between decisions and PDE dynamics. We validate the proposed approach\non benchmark PDE-constrained optimization tasks inlacing Burgers' equation,\nheat equation and voltage regulation, and demonstrate that it achieves solution\nquality comparable to classical control-based algorithms, such as the Direct\nMethod and Model Predictive Control (MPC), while providing up to four orders of\nmagnitude improvement in computational speed.", "AI": {"tldr": "A learning-based framework combining dynamic predictor and optimization surrogate for PDE-constrained optimization, achieving comparable solution quality to classical methods with 4 orders of magnitude speed improvement.", "motivation": "PDE-constrained optimization is computationally demanding due to tight coupling between decision variables and PDE state variables, requiring handling high dimensional discretization and dynamic constraints.", "method": "Dual-network design: dynamic predictor (time-discrete Neural Operator) approximates system trajectories, and optimization surrogate (proxy optimizer techniques) approximates optimal decisions, explicitly capturing decision-PDE dynamics coupling.", "result": "Validated on Burgers' equation, heat equation and voltage regulation benchmarks; achieves solution quality comparable to Direct Method and MPC with up to 10,000x computational speed improvement.", "conclusion": "The proposed framework enables real-time approximation of optimal strategies for PDE-constrained optimization problems while maintaining solution quality and dramatically reducing computational cost."}}
{"id": "2509.24580", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24580", "abs": "https://arxiv.org/abs/2509.24580", "authors": ["Lingyu Wang", "Xiangming Meng"], "title": "SAIP: A Plug-and-Play Scale-adaptive Module in Diffusion-based Inverse Problems", "comment": null, "summary": "Solving inverse problems with diffusion models has shown promise in tasks\nsuch as image restoration. A common approach is to formulate the problem in a\nBayesian framework and sample from the posterior by combining the prior score\nwith the likelihood score. Since the likelihood term is often intractable,\nestimators like DPS, DMPS, and $\\pi$GDM are widely adopted. However, these\nmethods rely on a fixed, manually tuned scale to balance prior and likelihood\ncontributions. Such a static design is suboptimal, as the ideal balance varies\nacross timesteps and tasks, limiting performance and generalization. To address\nthis issue, we propose SAIP, a plug-and-play module that adaptively refines the\nscale at each timestep without retraining or altering the diffusion backbone.\nSAIP integrates seamlessly into existing samplers and consistently improves\nreconstruction quality across diverse image restoration tasks, including\nchallenging scenarios.", "AI": {"tldr": "SAIP is a plug-and-play module that adaptively refines the scale parameter in diffusion-based inverse problem solvers, improving reconstruction quality across diverse image restoration tasks without retraining the diffusion backbone.", "motivation": "Existing diffusion-based inverse problem solvers use fixed, manually tuned scales to balance prior and likelihood contributions, which is suboptimal as the ideal balance varies across timesteps and tasks, limiting performance and generalization.", "method": "Propose SAIP, a plug-and-play module that adaptively refines the scale at each timestep without retraining or altering the diffusion backbone. It integrates seamlessly into existing samplers like DPS, DMPS, and \u03c0GDM.", "result": "SAIP consistently improves reconstruction quality across diverse image restoration tasks, including challenging scenarios.", "conclusion": "The adaptive scale refinement provided by SAIP addresses the limitations of static scale designs in diffusion-based inverse problem solvers, enhancing performance and generalization without requiring modifications to the diffusion backbone."}}
{"id": "2509.24601", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.24601", "abs": "https://arxiv.org/abs/2509.24601", "authors": ["Jae-Bum Seo", "Muhammad Salman", "Lismer Andres Caceres-Najarro"], "title": "CURA: Size Isnt All You Need -- A Compact Universal Architecture for On-Device Intelligence", "comment": "14 pages, 3 figures, 8 tables", "summary": "Existing on-device AI architectures for resource-constrained environments\nface two critical limitations: they lack compactness, with parameter\nrequirements scaling proportionally to task complexity, and they exhibit poor\ngeneralizability, performing effectively only on specific application domains\n(e.g., models designed for regression tasks cannot adapt to natural language\nprocessing (NLP) applications). In this paper, we propose CURA, an architecture\ninspired by analog audio signal processing circuits that provides a compact and\nlightweight solution for diverse machine learning tasks across multiple\ndomains. Our architecture offers three key advantages over existing approaches:\n(1) Compactness: it requires significantly fewer parameters regardless of task\ncomplexity; (2) Generalizability: it adapts seamlessly across regression,\nclassification, complex NLP, and computer vision tasks; and (3) Complex pattern\nrecognition: it can capture intricate data patterns while maintaining extremely\nlow model complexity. We evaluated CURA across diverse datasets and domains.\nFor compactness, it achieved equivalent accuracy using up to 2,500 times fewer\nparameters compared to baseline models. For generalizability, it demonstrated\nconsistent performance across four NLP benchmarks and one computer vision\ndataset, nearly matching specialized existing models (achieving F1-scores up to\n90%). Lastly, it delivers superior forecasting accuracy for complex patterns,\nachieving 1.6 times lower mean absolute error and 2.1 times lower mean squared\nerror than competing models.", "AI": {"tldr": "CURA is a compact, generalizable AI architecture inspired by analog audio circuits that achieves equivalent performance with up to 2,500x fewer parameters across diverse ML tasks including regression, classification, NLP, and computer vision.", "motivation": "Existing on-device AI architectures lack compactness (parameters scale with task complexity) and generalizability (models are domain-specific, e.g., regression models can't adapt to NLP).", "method": "Proposed CURA architecture inspired by analog audio signal processing circuits, providing a compact and lightweight solution for diverse machine learning tasks.", "result": "Achieved equivalent accuracy with up to 2,500x fewer parameters; demonstrated consistent performance across NLP benchmarks and computer vision datasets (F1-scores up to 90%); delivered superior forecasting accuracy with 1.6x lower MAE and 2.1x lower MSE than competing models.", "conclusion": "CURA provides a compact, generalizable solution that overcomes limitations of existing on-device AI architectures, enabling efficient deployment across diverse ML domains with minimal parameter requirements."}}
{"id": "2509.24608", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24608", "abs": "https://arxiv.org/abs/2509.24608", "authors": ["Louise AC Millard", "Peter A Flach"], "title": "Evaluating classification performance across operating contexts: A comparison of decision curve analysis and cost curves", "comment": null, "summary": "Classification models typically predict a score and use a decision threshold\nto produce a classification. Appropriate model evaluation should carefully\nconsider the context in which a model will be used, including the relative\nvalue of correct classifications of positive versus negative examples, which\naffects the threshold that should be used. Decision curve analysis (DCA) and\ncost curves are model evaluation approaches that assess the expected utility\nand expected loss of prediction models, respectively, across decision\nthresholds. We compared DCA and cost curves to determine how they are related,\nand their strengths and limitations. We demonstrate that decision curves are\nclosely related to a specific type of cost curve called a Brier curve. Both\ncurves are derived assuming model scores are calibrated and setting the\nclassification threshold using the relative value of correct positive and\nnegative classifications, and the x-axis of both curves are equivalent. Net\nbenefit (used for DCA) and Brier loss (used for Brier curves) will always\nchoose the same model as optimal at any given threshold. Across thresholds,\ndifferences in Brier loss are comparable whereas differences in net benefit\ncannot be compared. Brier curves are more generally applicable (when a wider\nrange of thresholds are plausible), and the area under the Brier curve is the\nBrier score. We demonstrate that reference lines common in each space can be\nincluded in either and suggest the upper envelope decision curve as a useful\ncomparison for DCA showing the possible gain in net benefit that could be\nachieved through recalibration alone.", "AI": {"tldr": "Decision curve analysis (DCA) and cost curves are compared, showing they are closely related through Brier curves. Both methods assess model utility across decision thresholds, with Brier curves being more generally applicable.", "motivation": "To understand the relationship between decision curve analysis and cost curves for model evaluation, and determine their respective strengths and limitations in assessing prediction models across different decision thresholds.", "method": "Theoretical comparison of decision curve analysis (DCA) and cost curves, specifically Brier curves, by analyzing their mathematical relationships and assumptions about model calibration and threshold setting.", "result": "Decision curves are closely related to Brier curves, with both sharing equivalent x-axes and always selecting the same optimal model at any given threshold. Brier curves are more generally applicable across thresholds, and the area under Brier curve equals the Brier score.", "conclusion": "Brier curves provide broader applicability for model evaluation across thresholds, while DCA and Brier curves are fundamentally related and consistent in model selection. The upper envelope decision curve is suggested as a useful comparison tool in DCA."}}
{"id": "2509.24610", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24610", "abs": "https://arxiv.org/abs/2509.24610", "authors": ["Liang Lin", "Zhihao Xu", "Junhao Dong", "Jian Zhao", "Yuchen Yuan", "Guibin Zhang", "Miao Yu", "Yiming Zhang", "Zhengtao Yao", "Huahui Yi", "Dongrui Liu", "Xinfeng Li", "Kun Wang"], "title": "OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment", "comment": null, "summary": "Large language model (LLM) alignment faces a critical dilemma when addressing\nmultiple human preferences: improvements in one dimension frequently come at\nthe expense of others, creating unavoidable trade-offs between competing\nobjectives like helpfulness and harmlessness. While prior work mainly focuses\non constraint-based optimization algorithms and data selection strategies to\nmitigate conflicts, these approaches overlook the fundamental issue of\nresolving conflicts directly at the parameter level. In this paper, we present\nOrthAlign, an innovative approach that pioneers a new paradigm by leveraging\northogonal subspace decomposition to fundamentally resolve gradient-level\nconflicts in multi-objective preference alignment. OrthAlign strategically\ndecomposes parameter update spaces into orthogonal subspaces, ensuring that\noptimization toward different preferences occurs in mathematically\nnon-interfering directions. Building upon this, we provide theoretical\nguarantees demonstrating that when parameter increments satisfy both orthogonal\nsubspace constraints and spectral norm bounds, the resulting updates exhibit\nlinear Lipschitz growth rather than exponential instability, ensuring stable\nconvergence across all preference dimensions. Extensive experiments show that:\nI. OrthAlign achieves maximum single-preference improvements ranging from\n34.61% to 50.89% after multiple-objective alignment across helpful, harmless,\nand truthful dimensions. II. With an average overall reward improvement of\n13.96%.", "AI": {"tldr": "OrthAlign uses orthogonal subspace decomposition to resolve gradient conflicts in multi-objective LLM alignment, achieving stable improvements across competing preferences like helpfulness and harmlessness.", "motivation": "Current LLM alignment approaches create trade-offs between competing objectives, with improvements in one dimension often degrading others, due to unresolved parameter-level conflicts.", "method": "OrthAlign decomposes parameter update spaces into orthogonal subspaces, ensuring optimization toward different preferences occurs in mathematically non-interfering directions with theoretical guarantees for stable convergence.", "result": "Achieves maximum single-preference improvements of 34.61% to 50.89% across helpful, harmless, and truthful dimensions, with 13.96% average overall reward improvement.", "conclusion": "OrthAlign provides a fundamental solution to multi-objective preference alignment by resolving gradient conflicts at the parameter level through orthogonal decomposition, enabling stable simultaneous improvements across competing objectives."}}
{"id": "2509.24627", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24627", "abs": "https://arxiv.org/abs/2509.24627", "authors": ["Katharina Friedl", "No\u00e9mie Jaquier", "Mika Liao", "Danica Kragic"], "title": "Learning Hamiltonian Dynamics at Scale: A Differential-Geometric Approach", "comment": "28 pages, 15 figures", "summary": "By embedding physical intuition, network architectures enforce fundamental\nproperties, such as energy conservation laws, leading to plausible predictions.\nYet, scaling these models to intrinsically high-dimensional systems remains a\nsignificant challenge. This paper introduces Geometric Reduced-order\nHamiltonian Neural Network (RO-HNN), a novel physics-inspired neural network\nthat combines the conservation laws of Hamiltonian mechanics with the\nscalability of model order reduction. RO-HNN is built on two core components: a\nnovel geometrically-constrained symplectic autoencoder that learns a\nlow-dimensional, structure-preserving symplectic submanifold, and a geometric\nHamiltonian neural network that models the dynamics on the submanifold. Our\nexperiments demonstrate that RO-HNN provides physically-consistent, stable, and\ngeneralizable predictions of complex high-dimensional dynamics, thereby\neffectively extending the scope of Hamiltonian neural networks to\nhigh-dimensional physical systems.", "AI": {"tldr": "RO-HNN combines Hamiltonian mechanics with model order reduction to scale physics-inspired neural networks to high-dimensional systems through a symplectic autoencoder and geometric Hamiltonian neural network.", "motivation": "Existing physics-inspired neural networks enforce conservation laws but struggle with scaling to high-dimensional systems, creating a need for methods that maintain physical consistency while handling complex dynamics.", "method": "Two core components: 1) geometrically-constrained symplectic autoencoder for learning low-dimensional symplectic submanifold, 2) geometric Hamiltonian neural network for modeling dynamics on the submanifold.", "result": "RO-HNN provides physically-consistent, stable, and generalizable predictions of complex high-dimensional dynamics, effectively extending Hamiltonian neural networks' scope.", "conclusion": "The proposed RO-HNN successfully combines conservation laws with scalability, enabling accurate modeling of high-dimensional physical systems while maintaining physical consistency."}}
{"id": "2509.24653", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24653", "abs": "https://arxiv.org/abs/2509.24653", "authors": ["Pengxiao Lin", "Zheng-An Chen", "Zhi-Qin John Xu"], "title": "Identity Bridge: Enabling Implicit Reasoning via Shared Latent Memory", "comment": null, "summary": "Despite remarkable advances, large language models often fail at\ncompositional reasoning tasks, a phenomenon exemplified by the ``curse of\ntwo-hop reasoning''. This paper introduces the Identity Bridge, a simple yet\npowerful mechanism that resolves this compositionality gap by supervising the\nmodel on a zero-hop identity task. We demonstrate empirically that this\naddition enables models to successfully perform out-of-distribution two-hop\nreasoning, a task they otherwise completely fail. To explain this phenomenon,\nwe provide a theoretical analysis using a simplified Emb-MLP model, proving\nthat identity supervision reshapes the model's latent geometry. We show this\nalignment is induced by an implicit nuclear-norm regularization during\noptimization, which favors low-rank solutions that share structure across\ntasks. For complex tasks, we use small initialization or weight decay to\nenhance the regularization effect, which enhances the latent space alignment\neffect and slows down the generalization decay. Finally, we extend our\ninvestigation to large-scale models, observing that they still achieve two-hop\nreasoning through the latent memory, which provides crucial inspiration for\nenhancing their implicit reasoning abilities.", "AI": {"tldr": "The Identity Bridge mechanism enables LLMs to perform out-of-distribution two-hop reasoning by supervising on zero-hop identity tasks, reshaping latent geometry through implicit nuclear-norm regularization.", "motivation": "Large language models often fail at compositional reasoning tasks, particularly suffering from the \"curse of two-hop reasoning\" where they cannot combine multiple reasoning steps effectively.", "method": "Introduces Identity Bridge - supervising models on zero-hop identity tasks, uses theoretical analysis with Emb-MLP model, employs small initialization or weight decay for complex tasks, and extends to large-scale models.", "result": "Models successfully perform out-of-distribution two-hop reasoning that they otherwise completely fail, with identity supervision reshaping latent geometry and inducing implicit nuclear-norm regularization favoring low-rank solutions.", "conclusion": "Identity Bridge resolves compositionality gap, latent space alignment through regularization enhances reasoning abilities, and large-scale models can achieve two-hop reasoning through latent memory, providing inspiration for improving implicit reasoning."}}
{"id": "2509.24655", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2509.24655", "abs": "https://arxiv.org/abs/2509.24655", "authors": ["Max van Spengler", "Artem Moskalev", "Tommaso Mansi", "Mangal Prakash", "Rui Liao"], "title": "HyperHELM: Hyperbolic Hierarchy Encoding for mRNA Language Modeling", "comment": null, "summary": "Language models are increasingly applied to biological sequences like\nproteins and mRNA, yet their default Euclidean geometry may mismatch the\nhierarchical structures inherent to biological data. While hyperbolic geometry\nprovides a better alternative for accommodating hierarchical data, it has yet\nto find a way into language modeling for mRNA sequences. In this work, we\nintroduce HyperHELM, a framework that implements masked language model\npre-training in hyperbolic space for mRNA sequences. Using a hybrid design with\nhyperbolic layers atop Euclidean backbone, HyperHELM aligns learned\nrepresentations with the biological hierarchy defined by the relationship\nbetween mRNA and amino acids. Across multiple multi-species datasets, it\noutperforms Euclidean baselines on 9 out of 10 tasks involving property\nprediction, with 10% improvement on average, and excels in out-of-distribution\ngeneralization to long and low-GC content sequences; for antibody region\nannotation, it surpasses hierarchy-aware Euclidean models by 3% in annotation\naccuracy. Our results highlight hyperbolic geometry as an effective inductive\nbias for hierarchical language modeling of mRNA sequences.", "AI": {"tldr": "HyperHELM introduces hyperbolic geometry for mRNA language modeling, outperforming Euclidean models on property prediction and generalization tasks.", "motivation": "Euclidean geometry may not align well with hierarchical biological structures, while hyperbolic geometry better accommodates hierarchical data but hasn't been applied to mRNA language modeling.", "method": "HyperHELM implements masked language model pre-training in hyperbolic space using a hybrid design with hyperbolic layers atop Euclidean backbone, aligning representations with mRNA-amino acid biological hierarchy.", "result": "Outperforms Euclidean baselines on 9/10 property prediction tasks (10% average improvement), excels in out-of-distribution generalization, and surpasses hierarchy-aware Euclidean models by 3% in antibody region annotation accuracy.", "conclusion": "Hyperbolic geometry serves as an effective inductive bias for hierarchical language modeling of mRNA sequences."}}
{"id": "2509.24696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24696", "abs": "https://arxiv.org/abs/2509.24696", "authors": ["Zikun Qu", "Min Zhang", "Mingze Kong", "Xiang Li", "Zhiwei Shang", "Zhiyong Wang", "Yikun Ban", "Shuang Qiu", "Yao Shu", "Zhongxiang Dai"], "title": "T-POP: Test-Time Personalization with Online Preference Feedback", "comment": "Preprint", "summary": "Personalizing large language models (LLMs) to individual user preferences is\na critical step beyond generating generically helpful responses. However,\ncurrent personalization methods are ill-suited for new users, as they typically\nrequire either slow, resource-intensive fine-tuning or a substantial amount of\npre-existing user data, creating a significant cold-start problem. To address\nthis challenge, we introduce a new paradigm for real-time personalization by\nlearning from online pairwise preference feedback collected during text\ngeneration. We propose T-POP (Test-Time Personalization with Online Preference\nFeedback}), a novel algorithm that synergistically combines test-time alignment\nwith dueling bandits. Without updating the LLM parameters, T-POP steers the\ndecoding process of a frozen LLM by learning a reward function online that\ncaptures user preferences. By leveraging dueling bandits, T-POP intelligently\nqueries the user to efficiently balance between exploring their preferences and\nexploiting the learned knowledge to generate personalized text. Extensive\nexperiments demonstrate that T-POP achieves rapid and data-efficient\npersonalization, significantly outperforming existing baselines and showing\nconsistent improvement with more user interactions.", "AI": {"tldr": "T-POP is a real-time personalization method for LLMs that uses online pairwise preference feedback during text generation, combining test-time alignment with dueling bandits to steer decoding without updating model parameters.", "motivation": "Current LLM personalization methods require fine-tuning or large user data, creating cold-start problems for new users. There's a need for real-time personalization that works immediately with minimal data.", "method": "T-POP combines test-time alignment with dueling bandits to learn user preferences online. It steers frozen LLM decoding by learning a reward function from pairwise preference feedback, balancing exploration and exploitation through intelligent querying.", "result": "Extensive experiments show T-POP achieves rapid and data-efficient personalization, significantly outperforming existing baselines and improving consistently with more user interactions.", "conclusion": "T-POP provides an effective solution to the cold-start problem in LLM personalization by enabling real-time adaptation through online preference learning without model parameter updates."}}
{"id": "2509.24701", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24701", "abs": "https://arxiv.org/abs/2509.24701", "authors": ["Pingchen Lu", "Zhi Hong", "Zhiwei Shang", "Zhiyong Wang", "Yikun Ban", "Yao Shu", "Min Zhang", "Shuang Qiu", "Zhongxiang Dai"], "title": "FedPOB: Sample-Efficient Federated Prompt Optimization via Bandits", "comment": "Preprint", "summary": "The performance of large language models (LLMs) is highly sensitive to the\ninput prompt, making prompt optimization a critical task. However, real-world\napplication is hindered by three major challenges: (1) the black-box nature of\npowerful proprietary LLMs, (2) the need for high sample efficiency due to query\ncosts, and (3) the desire for privacy-preserving collaboration among multiple\nusers. To address these challenges simultaneously, we introduce a novel\nframework for sample-efficient federated prompt optimization based on\nmulti-armed bandits (MABs). The MAB framework is uniquely suited for this\nproblem as it is (1) inherently a black-box optimization method, (2)\npractically sample-efficient, and (3) enables collaborative learning with\ntheoretically guaranteed benefit from more participating agents. We first\npropose the Federated Prompt Optimization via Bandits (FedPOB) algorithm, a\nfederated variant of the Linear UCB algorithm, where agents collaborate by\nsharing model parameters instead of raw data. We then extend our approach to\nthe practical setting of comparative user feedback by introducing FedPOB with\nPreference Feedback (FedPOB-Pref), an efficient algorithm based on federated\ndueling bandits. Extensive experiments demonstrate that both FedPOB and\nFedPOB-Pref significantly outperform existing baselines and that their\nperformance consistently improves as more agents participate in the\ncollaboration, validating the effectiveness of our federated approach.", "AI": {"tldr": "A federated prompt optimization framework using multi-armed bandits that addresses black-box LLM optimization, sample efficiency, and privacy-preserving collaboration among multiple users.", "motivation": "Real-world LLM applications face three challenges: black-box nature of proprietary LLMs, high query costs requiring sample efficiency, and need for privacy-preserving collaboration among users.", "method": "Proposed FedPOB (federated variant of Linear UCB) and FedPOB-Pref (for comparative feedback) algorithms where agents share model parameters instead of raw data, based on multi-armed bandit framework.", "result": "Extensive experiments show both FedPOB and FedPOB-Pref significantly outperform existing baselines, with performance consistently improving as more agents participate in collaboration.", "conclusion": "The federated multi-armed bandit approach effectively addresses the three key challenges in prompt optimization while enabling collaborative learning with proven benefits from more participating agents."}}
{"id": "2509.24713", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24713", "abs": "https://arxiv.org/abs/2509.24713", "authors": ["Jing Liu"], "title": "Circuit-Aware Reward Training: A Mechanistic Framework for Longtail Robustness in RLHF", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) reward models exhibit\nsystematic failures on longtail distributions, leading to reward hacking and\nmisalignment. We propose a mechanistic interpretability framework that\nidentifies specialized neural circuits responsible for rare-event processing in\nreward models. Drawing from recent advances showing distributed specialization\nfor rare tokens in language models\\citep{liu2025no, liu2025emergent}, we\nhypothesize that reward models also develop functionally distinct circuits for\nlongtail scenarios. Our theoretical framework establishes formal connections\nbetween circuit specialization, reward generalization bounds, and longtail\nperformance. We introduce \\textbf{Circuit-Aware Reward Training (CART)}, which\nuses circuit analysis to guide data augmentation, regularization, and ensemble\nstrategies. This approach provides both theoretical insights into reward model\nfailures and practical interventions for improving longtail robustness.", "AI": {"tldr": "The paper proposes a mechanistic interpretability framework to address systematic failures in RLHF reward models on longtail distributions, introducing Circuit-Aware Reward Training (CART) for improved robustness.", "motivation": "RLHF reward models exhibit systematic failures on longtail distributions, leading to reward hacking and misalignment, which motivates the need for better understanding and addressing these vulnerabilities.", "method": "The paper introduces a mechanistic interpretability framework that identifies specialized neural circuits for rare-event processing, and proposes Circuit-Aware Reward Training (CART) which uses circuit analysis to guide data augmentation, regularization, and ensemble strategies.", "result": "The approach provides theoretical insights into reward model failures and practical interventions for improving longtail robustness through circuit specialization analysis.", "conclusion": "The framework establishes formal connections between circuit specialization, reward generalization bounds, and longtail performance, offering both theoretical understanding and practical solutions for RLHF reward model vulnerabilities."}}
{"id": "2509.24923", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24923", "abs": "https://arxiv.org/abs/2509.24923", "authors": ["Sanxing Chen", "Xiaoyin Chen", "Yukun Huang", "Roy Xie", "Bhuwan Dhingra"], "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training", "comment": null, "summary": "While Large Language Models (LLMs) hold promise to become autonomous agents,\nthey often explore suboptimally in sequential decision-making. Recent work has\nsought to enhance this capability via supervised fine-tuning (SFT) or\nreinforcement learning (RL), improving regret on the classic multi-armed bandit\ntask. However, it remains unclear how these learning methods shape exploration\nstrategies and how well they generalize. We investigate both paradigms by\ntraining LLMs with SFT on expert trajectories and RL with a range of tailored\nreward signals including a strategic, regret-shaped reward to reduce variance,\nand an algorithmic reward that enables oracle imitation. The resulting agents\noutperform pre-trained models and achieve performance comparable to Upper\nConfidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x\nlonger horizons and across bandit families. Behavioral analysis reveals that\ngains often stem from more sophisticated but greedier exploitation: RL/SFT\nagents are more prone to early catastrophic failure than pre-trained models,\nprematurely abandoning exploration. Furthermore, agents trained to imitate UCB\nlearn to outperform their teacher by adopting more exploitative variants. Our\nfindings clarify when each training paradigm is preferable and advocate\ntailored reward design and evaluation beyond average regret to promote robust\nexploratory behavior.", "AI": {"tldr": "LLMs trained with SFT and RL outperform pre-trained models on multi-armed bandit tasks, achieving performance comparable to UCB and Thompson Sampling with robust generalization to longer horizons and different bandit families.", "motivation": "LLMs often explore suboptimally in sequential decision-making, and it's unclear how SFT and RL shape exploration strategies and their generalization capabilities.", "method": "Train LLMs with SFT on expert trajectories and RL with tailored reward signals including regret-shaped reward and algorithmic reward for oracle imitation.", "result": "Trained agents outperform pre-trained models and match UCB/Thompson Sampling performance, with robust generalization to 6x longer horizons and across bandit families.", "conclusion": "RL/SFT agents tend to be greedier in exploitation but more prone to early failure; tailored reward design and evaluation beyond average regret are needed for robust exploratory behavior."}}
{"id": "2509.24716", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.24716", "abs": "https://arxiv.org/abs/2509.24716", "authors": ["Michael Drolet", "Firas Al-Hafez", "Aditya Bhatt", "Jan Peters", "Oleg Arenz"], "title": "Discrete Variational Autoencoding via Policy Search", "comment": null, "summary": "Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit\nefficiency and can be modeled with autoregressive discrete distributions,\nenabling parameter-efficient multimodal search with transformers. However,\ndiscrete random variables do not allow for exact differentiable\nparameterization; therefore, discrete VAEs typically rely on approximations,\nsuch as Gumbel-Softmax reparameterization or straight-through gradient\nestimates, or employ high-variance gradient-free methods such as REINFORCE that\nhave had limited success on high-dimensional tasks such as image\nreconstruction. Inspired by popular techniques in policy search, we propose a\ntraining framework for discrete VAEs that leverages the natural gradient of a\nnon-parametric encoder to update the parametric encoder without requiring\nreparameterization. Our method, combined with automatic step size adaptation\nand a transformer-based encoder, scales to challenging datasets such as\nImageNet and outperforms both approximate reparameterization methods and\nquantization-based discrete autoencoders in reconstructing high-dimensional\ndata from compact latent spaces, achieving a 20% improvement on FID Score for\nImageNet 256.", "AI": {"tldr": "Proposes a training framework for discrete VAEs using natural gradient of non-parametric encoder to update parametric encoder without reparameterization, achieving better performance on high-dimensional data reconstruction.", "motivation": "Discrete VAEs offer high bit efficiency but face challenges with exact differentiable parameterization, relying on approximations that have limited success on high-dimensional tasks like image reconstruction.", "method": "Leverages natural gradient of non-parametric encoder to update parametric encoder without requiring reparameterization, combined with automatic step size adaptation and transformer-based encoder.", "result": "Scales to challenging datasets like ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders, achieving 20% improvement on FID Score for ImageNet 256.", "conclusion": "The proposed framework provides an effective training method for discrete VAEs that overcomes limitations of existing approaches and achieves superior performance in high-dimensional data reconstruction."}}
{"id": "2509.25087", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25087", "abs": "https://arxiv.org/abs/2509.25087", "authors": ["Shane Bergsma", "Bin Claire Zhang", "Nolan Dey", "Shaheer Muhammad", "Gurpreet Gosal", "Joel Hestness"], "title": "Scaling with Collapse: Efficient and Predictable Training of LLM Families", "comment": null, "summary": "Effective LLM training relies on *consistency*, meaning that key quantities\n-- such as final losses and optimal hyperparameters -- scale predictably across\nmodel sizes. Qiu et al. (2025) recently showed that this consistency extends\nbeyond scalars: whole training loss curves can *collapse* onto a universal\ntrajectory after a simple normalization. What remains unclear is whether this\nphenomenon holds for LLM families trained under *practical scaling recipes*,\nwhere width, depth, learning rate, batch size, and weight decay are scaled\njointly. We show that it does: loss curves collapse across scales precisely\nwhen optimization hyperparameters are set optimally for the given data budget,\nin accordance with recent empirical scaling laws. Collapse thus emerges as a\nsignature of compute-efficient training. We demonstrate two applications at\nscale: (1) deviation-from-collapse provides a sensitive, early diagnostic of\ntraining pathologies, and (2) the predictability of collapsed curves enables\nearly stopping in large-scale hyperparameter tuning. Finally, we train a\ncompetitive LLM family, *Celerity*, using these insights, highlighting collapse\nas an effective tool for developing efficient LLMs.", "AI": {"tldr": "Training loss curves of LLMs collapse onto a universal trajectory when optimization hyperparameters are optimally set according to scaling laws, serving as a signature of compute-efficient training.", "motivation": "To verify if the consistency phenomenon in LLM training extends to practical scaling recipes where multiple parameters are scaled jointly, and to explore applications of this collapse phenomenon.", "method": "Analyze training loss curves across different model scales under optimal hyperparameter settings based on empirical scaling laws, and demonstrate applications using deviation-from-collapse diagnostics and early stopping strategies.", "result": "Loss curves indeed collapse across scales when hyperparameters are optimally set for given data budgets, and this collapse enables effective training diagnostics and hyperparameter tuning efficiency.", "conclusion": "Collapse of training loss curves serves as an effective tool for developing efficient LLMs, as demonstrated by the successful training of the Celerity LLM family using these insights."}}
{"id": "2509.24725", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24725", "abs": "https://arxiv.org/abs/2509.24725", "authors": ["Ting Gao", "Elvin Isufi", "Winnie Daamen", "Erik-Sander Smits", "Serge Hoogendoorn"], "title": "Q-Net: Transferable Queue Length Estimation via Kalman-based Neural Networks", "comment": null, "summary": "Estimating queue lengths at signalized intersections remains a challenge in\ntraffic management, especially under partially observed conditions where\nvehicle flows are not fully captured. This paper introduces Q-Net, a\ndata-efficient and interpretable framework for queue length estimation that\nperforms robustly even when traffic conservation assumptions are violated.\nQ-Net integrates two widely available and privacy-friendly data sources: (i)\nvehicle counts from loop detectors near stop lines, and (ii) aggregated\nfloating car data (aFCD), which divides each road section into segments and\nprovides segment-wise average speed measurements. These data sources often\ndiffer in spatial and temporal resolution, creating fusion challenges. Q-Net\naddresses this by employing a tailored state-space model and an AI-augmented\nKalman filter, KalmanNet, which learns the Kalman gain from data without\nrequiring prior knowledge of noise covariances or full system dynamics. We\nbuild on the vanilla KalmanNet pipeline to decouple measurement dimensionality\nfrom section length, enabling spatial transferability across road segments.\nUnlike black-box models, Q-Net maintains physical interpretability, with\ninternal variables linked to real-world traffic dynamics. Evaluations on main\nroads in Rotterdam, the Netherlands, demonstrate that Q-Net outperforms\nbaseline methods by over 60\\% in Root Mean Square Error (RMSE), accurately\ntracking queue formation and dissipation while correcting aFCD-induced delays.\nQ-Net also demonstrates strong spatial and temporal transferability, enabling\ndeployment without costly sensing infrastructure like cameras or radar.\nAdditionally, we propose a real-time variant of Q-Net, highlighting its\npotential for integration into dynamic, queue-based traffic control systems.", "AI": {"tldr": "Q-Net is a data-efficient and interpretable framework for estimating queue lengths at signalized intersections using loop detector counts and aggregated floating car data, achieving over 60% RMSE improvement over baselines.", "motivation": "Estimating queue lengths at signalized intersections is challenging under partially observed conditions, especially when traffic conservation assumptions are violated and existing data sources have different spatial-temporal resolutions.", "method": "Q-Net integrates loop detector counts and aggregated floating car data using a tailored state-space model and AI-augmented Kalman filter (KalmanNet) that learns Kalman gain from data without requiring noise covariance knowledge.", "result": "Evaluations on Rotterdam roads show Q-Net outperforms baseline methods by over 60% in RMSE, accurately tracks queue formation/dissipation, corrects aFCD-induced delays, and demonstrates strong spatial-temporal transferability.", "conclusion": "Q-Net provides a robust, interpretable solution for queue length estimation that works with privacy-friendly data sources and enables deployment without costly sensing infrastructure, with potential for real-time traffic control systems."}}
{"id": "2509.25100", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25100", "abs": "https://arxiv.org/abs/2509.25100", "authors": ["Aasheesh Singh", "Vishal Vaddina", "Dagnachew Birru"], "title": "ORPO-Distill: Mixed-Policy Preference Optimization for Cross-Architecture LLM Distillation", "comment": "Accepted at NeurIPS 2025, Efficient Reasoning Workshop", "summary": "We introduce ORPO-Distill, a general-purpose method for cross-architecture\nLLM distillation that formulates the problem as a preference optimization task.\nUnlike standard CoT distillation, the approach transfers knowledge through\ndiverse reasoning traces. It employs an Odds-Ratio Preference Optimization\nobjective that contrasts teacher and student traces for more effective\nlearning, and adopts a mixed-policy strategy for utilizing student-generated\noutputs, outperforming both off- and on-policy alternatives. Experiments on\nfive datasets and multiple student models show consistent improvements over\nconventional black-box KD baselines.", "AI": {"tldr": "ORPO-Distill is a cross-architecture LLM distillation method that treats distillation as preference optimization, using diverse reasoning traces and an odds-ratio objective to contrast teacher and student outputs, achieving better performance than standard knowledge distillation baselines.", "motivation": "Standard chain-of-thought (CoT) distillation methods are limited in transferring knowledge across different model architectures. The authors aim to develop a more effective distillation approach that can work across various architectures by formulating it as a preference optimization problem.", "method": "ORPO-Distill uses diverse reasoning traces from teachers and employs an Odds-Ratio Preference Optimization objective to contrast teacher and student traces. It adopts a mixed-policy strategy for utilizing student-generated outputs, combining benefits of both off-policy and on-policy approaches.", "result": "Experiments conducted on five datasets with multiple student models show consistent improvements over conventional black-box knowledge distillation baselines, demonstrating the effectiveness of the approach across different architectures.", "conclusion": "ORPO-Distill provides a general-purpose solution for cross-architecture LLM distillation that outperforms traditional methods by treating distillation as preference optimization and using diverse reasoning traces with contrastive learning objectives."}}
{"id": "2509.24728", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24728", "abs": "https://arxiv.org/abs/2509.24728", "authors": ["Alessandro Manenti", "Cesare Alippi"], "title": "Beyond Softmax: A Natural Parameterization for Categorical Random Variables", "comment": null, "summary": "Latent categorical variables are frequently found in deep learning\narchitectures. They can model actions in discrete reinforcement-learning\nenvironments, represent categories in latent-variable models, or express\nrelations in graph neural networks. Despite their widespread use, their\ndiscrete nature poses significant challenges to gradient-descent learning\nalgorithms. While a substantial body of work has offered improved gradient\nestimation techniques, we take a complementary approach. Specifically, we: 1)\nrevisit the ubiquitous $\\textit{softmax}$ function and demonstrate its\nlimitations from an information-geometric perspective; 2) replace the\n$\\textit{softmax}$ with the $\\textit{catnat}$ function, a function composed of\na sequence of hierarchical binary splits; we prove that this choice offers\nsignificant advantages to gradient descent due to the resulting diagonal Fisher\nInformation Matrix. A rich set of experiments - including graph structure\nlearning, variational autoencoders, and reinforcement learning - empirically\nshow that the proposed function improves the learning efficiency and yields\nmodels characterized by consistently higher test performance. $\\textit{Catnat}$\nis simple to implement and seamlessly integrates into existing codebases.\nMoreover, it remains compatible with standard training stabilization techniques\nand, as such, offers a better alternative to the $\\textit{softmax}$ function.", "AI": {"tldr": "The paper proposes replacing softmax with catnat function for latent categorical variables, showing improved gradient descent efficiency and better test performance across various applications.", "motivation": "Latent categorical variables are widely used but pose challenges for gradient-based learning due to their discrete nature. Existing gradient estimation techniques have limitations, and softmax has information-geometric drawbacks that hinder efficient learning.", "method": "Replace softmax with catnat function - a hierarchical binary splits function that creates a diagonal Fisher Information Matrix, improving gradient descent efficiency. The method is simple to implement and compatible with existing training techniques.", "result": "Experiments in graph structure learning, variational autoencoders, and reinforcement learning show catnat improves learning efficiency and consistently achieves higher test performance compared to softmax.", "conclusion": "Catnat provides a better alternative to softmax for latent categorical variables, offering improved gradient descent properties and better model performance while maintaining compatibility with standard training methods."}}
{"id": "2509.24732", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.24732", "abs": "https://arxiv.org/abs/2509.24732", "authors": ["Juergen Schmidhuber"], "title": "Who invented deep residual learning?", "comment": "12 pages, 2 illustrations, circa 100 partially annotated references", "summary": "Modern AI is based on deep artificial neural networks (NNs). As of 2025, the\nmost cited scientific article of the 21st century is an NN paper on deep\nresidual learning with residual connections. Who invented this? We present a\ntimeline of the evolution of deep residual learning.", "AI": {"tldr": "The paper presents a timeline of the evolution of deep residual learning, noting that the most cited scientific article of the 21st century as of 2025 is a neural network paper on deep residual learning with residual connections.", "motivation": "To trace the origins and development of deep residual learning in neural networks, particularly focusing on identifying who invented this influential approach.", "method": "The authors present a historical timeline analysis of the evolution of deep residual learning.", "result": "The analysis reveals that the most cited scientific article of the 21st century (as of 2025) is a neural network paper on deep residual learning with residual connections.", "conclusion": "The paper provides a comprehensive timeline documenting the evolution of deep residual learning, highlighting its significance in modern AI through the citation impact of the foundational work."}}
{"id": "2509.25133", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25133", "abs": "https://arxiv.org/abs/2509.25133", "authors": ["Yuxian Jiang", "Yafu Li", "Guanxu Chen", "Dongrui Liu", "Yu Cheng", "Jing Shao"], "title": "Rethinking Entropy Regularization in Large Reasoning Models", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown great promise\nin enhancing the reasoning abilities of large reasoning models (LRMs). However,\nit suffers from a critical issue: entropy collapse and premature convergence.\nNaive entropy regularization, a common approach for encouraging exploration in\nthe traditional RL literature, fails to address this problem in the context of\nLRM. Our analysis reveals that this failure stems from the vast action space\nand long trajectories in LRMs, which easily trigger a global entropy explosion\nas the model indiscriminately explores all possible actions and states. To\naddress this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method\nthat confines exploration to a meaningful subset of actions and states. SIREN\nachieves this through a two-step entropy masking mechanism, consisting of a\ntop-p mask and a peak-entropy mask. In addition, regularization is transformed\ninto a self-anchored form to stabilize training. Across five mathematical\nbenchmarks, SIREN attains superior average performance over previous\nentropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on\nAIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes\ngreater response diversity and maintains entropy at an appropriate level, which\nhelps to preserve the validation pass@k throughout training. This effectively\nmitigates the premature convergence problem common in RLVR for LRM.", "AI": {"tldr": "SIREN addresses entropy collapse in RLVR for large reasoning models through selective entropy regularization with masking mechanisms and self-anchored stabilization.", "motivation": "RLVR suffers from entropy collapse and premature convergence in large reasoning models, where naive entropy regularization fails due to global entropy explosion from vast action spaces.", "method": "Proposed SIREN with two-step entropy masking (top-p mask and peak-entropy mask) to confine exploration to meaningful actions/states, plus self-anchored regularization for training stability.", "result": "Achieved superior performance across 5 math benchmarks, including +6.6 maj@k improvement on AIME24/25 with Qwen2.5-Math-7B, with greater response diversity and maintained appropriate entropy levels.", "conclusion": "SIREN effectively mitigates premature convergence in RLVR for LRMs by promoting diversity and preserving validation pass@k throughout training."}}
{"id": "2509.24734", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.24734", "abs": "https://arxiv.org/abs/2509.24734", "authors": ["Giordano Cicchetti", "Eleonora Grassucci", "Danilo Comminiello"], "title": "A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity", "comment": "NeurIPS 2025", "summary": "Multimodal learning plays a pivotal role in advancing artificial intelligence\nsystems by incorporating information from multiple modalities to build a more\ncomprehensive representation. Despite its importance, current state-of-the-art\nmodels still suffer from severe limitations that prevent the successful\ndevelopment of a fully multimodal model. Such methods may not provide\nindicators that all the involved modalities are effectively aligned. As a\nresult, some modalities may not be aligned, undermining the effectiveness of\nthe model in downstream tasks where multiple modalities should provide\nadditional information that the model fails to exploit. In this paper, we\npresent TRIANGLE: TRI-modAl Neural Geometric LEarning, the novel proposed\nsimilarity measure that is directly computed in the higher-dimensional space\nspanned by the modality embeddings. TRIANGLE improves the joint alignment of\nthree modalities via a triangle-area similarity, avoiding additional fusion\nlayers or pairwise similarities. When incorporated in contrastive losses\nreplacing cosine similarity, TRIANGLE significantly boosts the performance of\nmultimodal modeling, while yielding interpretable alignment rationales.\nExtensive evaluation in three-modal tasks such as video-text and audio-text\nretrieval or audio-video classification, demonstrates that TRIANGLE achieves\nstate-of-the-art results across different datasets improving the performance of\ncosine-based methods up to 9 points of Recall@1.", "AI": {"tldr": "TRIANGLE is a novel similarity measure for multimodal learning that uses triangle-area similarity in high-dimensional embedding space to improve joint alignment of three modalities without additional fusion layers.", "motivation": "Current multimodal models suffer from ineffective modality alignment, where some modalities may not be properly aligned, limiting the model's ability to exploit complementary information from multiple modalities in downstream tasks.", "method": "TRIANGLE computes similarity directly in the higher-dimensional space spanned by modality embeddings using triangle-area similarity, replacing cosine similarity in contrastive losses to improve joint alignment of three modalities.", "result": "TRIANGLE achieves state-of-the-art results in three-modal tasks including video-text and audio-text retrieval or audio-video classification, improving cosine-based methods by up to 9 points of Recall@1.", "conclusion": "TRIANGLE significantly boosts multimodal modeling performance while providing interpretable alignment rationales, demonstrating effectiveness in joint alignment of three modalities across various tasks."}}
{"id": "2509.24748", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24748", "abs": "https://arxiv.org/abs/2509.24748", "authors": ["Longxiang He", "Deheng Ye", "Junbo Tan", "Xueqian Wang", "Li Shen"], "title": "Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption", "comment": "39th Conference on Neural Information Processing Systems", "summary": "Pretraining a policy on offline data followed by fine-tuning through online\ninteractions, known as Offline-to-Online Reinforcement Learning (O2O RL), has\nemerged as a promising paradigm for real-world RL deployment. However, both\noffline datasets and online interactions in practical environments are often\nnoisy or even maliciously corrupted, severely degrading the performance of O2O\nRL. Existing works primarily focus on mitigating the conservatism of offline\npolicies via online exploration, while the robustness of O2O RL under data\ncorruption, including states, actions, rewards, and dynamics, is still\nunexplored. In this work, we observe that data corruption induces heavy-tailed\nbehavior in the policy, thereby substantially degrading the efficiency of\nonline exploration. To address this issue, we incorporate Inverse Probability\nWeighted (IPW) into the online exploration policy to alleviate\nheavy-tailedness, and propose a novel, simple yet effective method termed\n$\\textbf{RPEX}$: $\\textbf{R}$obust $\\textbf{P}$olicy $\\textbf{EX}$pansion.\nExtensive experimental results on D4RL datasets demonstrate that RPEX achieves\nSOTA O2O performance across a wide range of data corruption scenarios. Code is\navailable at\n$\\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$.", "AI": {"tldr": "RPEX is a robust offline-to-online RL method that addresses data corruption issues by incorporating Inverse Probability Weighted (IPW) into online exploration to mitigate heavy-tailed policy behavior caused by corrupted data.", "motivation": "Existing O2O RL methods focus on reducing offline policy conservatism but overlook robustness against data corruption (states, actions, rewards, dynamics), which severely degrades performance in practical environments with noisy or malicious data.", "method": "Proposed RPEX method incorporates Inverse Probability Weighted (IPW) into the online exploration policy to alleviate heavy-tailed behavior induced by data corruption, enabling robust policy expansion from offline pretraining to online fine-tuning.", "result": "Extensive experiments on D4RL datasets show RPEX achieves state-of-the-art O2O performance across various data corruption scenarios, demonstrating superior robustness compared to existing methods.", "conclusion": "RPEX provides an effective solution for robust offline-to-online RL under data corruption, addressing the critical but unexplored challenge of handling corrupted data in practical RL deployment scenarios."}}
{"id": "2509.24762", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24762", "abs": "https://arxiv.org/abs/2509.24762", "authors": ["David Berghaus", "Patrick Seifner", "Kostadin Cvejoski", "C\u00e9sar Ojeda", "Rams\u00e9s J. S\u00e1nchez"], "title": "In-Context Learning of Temporal Point Processes with Foundation Inference Models", "comment": null, "summary": "Modeling event sequences of multiple event types with marked temporal point\nprocesses (MTPPs) provides a principled way to uncover governing dynamical\nrules and predict future events. Current neural network approaches to MTPP\ninference rely on training separate, specialized models for each target system.\nWe pursue a radically different approach: drawing on amortized inference and\nin-context learning, we pretrain a deep neural network to infer, in-context,\nthe conditional intensity functions of event histories from a context defined\nby sets of event sequences. Pretraining is performed on a large synthetic\ndataset of MTPPs sampled from a broad distribution of Hawkes processes. Once\npretrained, our Foundation Inference Model for Point Processes (FIM-PP) can\nestimate MTPPs from real-world data without any additional training, or be\nrapidly finetuned to target systems. Experiments show that this amortized\napproach matches the performance of specialized models on next-event prediction\nacross common benchmark datasets.\n  Our pretrained model, repository and tutorials will soon be available online", "AI": {"tldr": "The paper introduces FIM-PP, a foundation model for temporal point processes that uses in-context learning to infer conditional intensity functions from event sequences, achieving performance comparable to specialized models without retraining.", "motivation": "Current neural MTPP approaches require training separate specialized models for each system, which is inefficient. The authors aim to develop a general-purpose model that can infer MTPPs across different systems without retraining.", "method": "Pretrain a deep neural network on a large synthetic dataset of Hawkes processes using amortized inference and in-context learning. The model learns to infer conditional intensity functions from context sets of event sequences.", "result": "FIM-PP matches the performance of specialized models on next-event prediction across common benchmark datasets without additional training, and can be rapidly fine-tuned when needed.", "conclusion": "The amortized inference approach provides an efficient alternative to specialized MTPP models, enabling general-purpose temporal point process inference with competitive performance."}}
{"id": "2509.25176", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25176", "abs": "https://arxiv.org/abs/2509.25176", "authors": ["Haoming Wen", "Yushi Bai", "Juanzi Li", "Jie Tang"], "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression", "comment": "In submission", "summary": "We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved\nCompression, a simple yet effective RL approach for Large Reasoning Models\n(LRMs) that enables more efficient and accurate reasoning. Existing studies\nhave observed repetitive thinking patterns in LRMs, and attempts to reduce them\noften come at the cost of performance. In this paper, we show that this\ntrade-off can be overcome through a training regime that iteratively alternates\nbetween compressing and expanding the reasoning budget, by dynamically\nadjusting the maximum rollout length during training. The compression phase\ncuts the rollout length, forcing the model to make precise and valuable\ndecisions within a limited context, which effectively reduces redundant tokens\nand increases reasoning density. The expansion phase then relaxes the length\nlimit, providing space for the model to explore and plan in long-horizon\nsettings. Remarkably, we find that after each compression-expansion cycle, the\nmodel's performance improves even as its output length decreases, steadily\npushing it closer to the Pareto frontier in the performance-efficiency\ntrade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves\nperformance on AIME24 by 43.2% while reducing token usage by 46.9% after three\niterations, and SIRI-high achieves the highest accuracy compared to all other\nmethods (Figure 1). Our findings shed light on the potential of periodically\noscillating the LRM's output truncation length during training to dynamically\nbalance exploration and efficiency in reasoning, converging towards an optimal\n\"sweet spot\" between the two. Our models are publicly available.", "AI": {"tldr": "SIRI is a reinforcement learning approach that alternates between compressing and expanding reasoning length during training, improving both performance and efficiency in Large Reasoning Models.", "motivation": "Existing methods struggle with repetitive thinking patterns in LRMs, creating a trade-off between reducing redundancy and maintaining performance.", "method": "Iterative training regime that dynamically adjusts maximum rollout length - compression phase reduces length to force precise decisions, expansion phase increases length for exploration.", "result": "After 3 iterations, SIRI-low improved AIME24 performance by 43.2% while reducing token usage by 46.9%; SIRI-high achieved highest accuracy among all methods.", "conclusion": "Periodically oscillating output truncation length during training dynamically balances exploration and efficiency, converging to an optimal sweet spot between performance and efficiency."}}
{"id": "2509.24770", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24770", "abs": "https://arxiv.org/abs/2509.24770", "authors": ["Fabrizio Frasca", "Guy Bar-Shalom", "Yftah Ziser", "Haggai Maron"], "title": "Neural Message-Passing on Attention Graphs for Hallucination Detection", "comment": "Preprint. 25 pages, 2 figures", "summary": "Large Language Models (LLMs) often generate incorrect or unsupported content,\nknown as hallucinations. Existing detection methods rely on heuristics or\nsimple models over isolated computational traces such as activations, or\nattention maps. We unify these signals by representing them as attributed\ngraphs, where tokens are nodes, edges follow attentional flows, and both carry\nfeatures from attention scores and activations. Our approach, CHARM, casts\nhallucination detection as a graph learning task and tackles it by applying\nGNNs over the above attributed graphs. We show that CHARM provably subsumes\nprior attention-based heuristics and, experimentally, it consistently\noutperforms other leading approaches across diverse benchmarks. Our results\nshed light on the relevant role played by the graph structure and on the\nbenefits of combining computational traces, whilst showing CHARM exhibits\npromising zero-shot performance on cross-dataset transfer.", "AI": {"tldr": "CHARM is a graph learning approach for detecting LLM hallucinations by representing computational traces as attributed graphs and applying GNNs, outperforming existing methods across benchmarks.", "motivation": "Existing hallucination detection methods rely on heuristics or simple models over isolated computational traces like activations or attention maps, lacking unified analysis.", "method": "Represent computational traces as attributed graphs with tokens as nodes, edges following attentional flows, and features from attention scores and activations. Apply GNNs for hallucination detection.", "result": "CHARM consistently outperforms other leading approaches across diverse benchmarks, shows promising zero-shot performance on cross-dataset transfer, and provably subsumes prior attention-based heuristics.", "conclusion": "Graph structure plays a relevant role in hallucination detection, and combining computational traces provides benefits. CHARM demonstrates effective unified analysis of LLM hallucinations."}}
{"id": "2509.24779", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.24779", "abs": "https://arxiv.org/abs/2509.24779", "authors": ["Kacper Kapu\u015bniak", "Cristian Gabellini", "Michael Bronstein", "Prudencio Tossou", "Francesco Di Giovanni"], "title": "MarS-FM: Generative Modeling of Molecular Dynamics via Markov State Models", "comment": null, "summary": "Molecular Dynamics (MD) is a powerful computational microscope for probing\nprotein functions. However, the need for fine-grained integration and the long\ntimescales of biomolecular events make MD computationally expensive. To address\nthis, several generative models have been proposed to generate surrogate\ntrajectories at lower cost. Yet, these models typically learn a fixed-lag\ntransition density, causing the training signal to be dominated by frequent but\nuninformative transitions. We introduce a new class of generative models, MSM\nEmulators, which instead learn to sample transitions across discrete states\ndefined by an underlying Markov State Model (MSM). We instantiate this class\nwith Markov Space Flow Matching (MarS-FM), whose sampling offers more than two\norders of magnitude speedup compared to implicit- or explicit-solvent MD\nsimulations. We benchmark Mars-FM ability to reproduce MD statistics through\nstructural observables such as RMSD, radius of gyration, and secondary\nstructure content. Our evaluation spans protein domains (up to 500 residues)\nwith significant chemical and structural diversity, including unfolding events,\nand enforces strict sequence dissimilarity between training and test sets to\nassess generalization. Across all metrics, MarS-FM outperforms existing\nmethods, often by a substantial margin.", "AI": {"tldr": "MSM Emulators learn to sample transitions across discrete states defined by an underlying Markov State Model, offering significant speedup over traditional MD simulations while maintaining accuracy.", "motivation": "Molecular Dynamics is computationally expensive due to fine-grained integration and long timescales, and existing generative models are dominated by uninformative transitions.", "method": "Introduce MSM Emulators class with Markov Space Flow Matching (MarS-FM) that samples transitions across discrete MSM states instead of learning fixed-lag transition density.", "result": "MarS-FM provides >100x speedup vs implicit/explicit-solvent MD, outperforms existing methods across structural metrics (RMSD, radius of gyration, secondary structure) on diverse protein domains up to 500 residues.", "conclusion": "MSM Emulators with MarS-FM offer efficient and accurate trajectory generation, substantially outperforming existing methods while maintaining generalization across chemically diverse proteins."}}
{"id": "2509.24784", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24784", "abs": "https://arxiv.org/abs/2509.24784", "authors": ["Nathan Gavenski", "Odinaldo Rodrigues"], "title": "Quantifying Generalisation in Imitation Learning", "comment": "NeurIPS 2025 Datasets and Benchmarks Track poster", "summary": "Imitation learning benchmarks often lack sufficient variation between\ntraining and evaluation, limiting meaningful generalisation assessment. We\nintroduce Labyrinth, a benchmarking environment designed to test generalisation\nwith precise control over structure, start and goal positions, and task\ncomplexity. It enables verifiably distinct training, evaluation, and test\nsettings. Labyrinth provides a discrete, fully observable state space and known\noptimal actions, supporting interpretability and fine-grained evaluation. Its\nflexible setup allows targeted testing of generalisation factors and includes\nvariants like partial observability, key-and-door tasks, and ice-floor hazards.\nBy enabling controlled, reproducible experiments, Labyrinth advances the\nevaluation of generalisation in imitation learning and provides a valuable tool\nfor developing more robust agents.", "AI": {"tldr": "Labyrinth is a benchmarking environment for imitation learning that enables controlled testing of generalization with distinct training/evaluation settings, flexible task variations, and interpretable evaluation.", "motivation": "Existing imitation learning benchmarks lack sufficient variation between training and evaluation, limiting meaningful assessment of generalization capabilities.", "method": "Developed Labyrinth environment with precise control over structure, start/goal positions, and task complexity. Features discrete, fully observable state space, known optimal actions, and variants like partial observability, key-and-door tasks, and ice-floor hazards.", "result": "Labyrinth enables verifiably distinct training, evaluation, and test settings, supporting interpretability and fine-grained evaluation of generalization factors.", "conclusion": "Labyrinth advances generalization evaluation in imitation learning through controlled, reproducible experiments and provides a valuable tool for developing more robust agents."}}
{"id": "2509.24788", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2509.24788", "abs": "https://arxiv.org/abs/2509.24788", "authors": ["Felix Strnad", "Jonathan Schmidt", "Fabian Mockert", "Philipp Hennig", "Nicole Ludwig"], "title": "Assessing the risk of future Dunkelflaute events for Germany using generative deep learning", "comment": null, "summary": "The European electricity power grid is transitioning towards renewable energy\nsources, characterized by an increasing share of off- and onshore wind and\nsolar power. However, the weather dependency of these energy sources poses a\nchallenge to grid stability, with so-called Dunkelflaute events -- periods of\nlow wind and solar power generation -- being of particular concern due to their\npotential to cause electricity supply shortages. In this study, we investigate\nthe impact of these events on the German electricity production in the years\nand decades to come. For this purpose, we adapt a recently developed generative\ndeep learning framework to downscale climate simulations from the CMIP6\nensemble. We first compare their statistics to the historical record taken from\nERA5 data. Next, we use these downscaled simulations to assess plausible future\noccurrences of Dunkelflaute events in Germany under the optimistic low\n(SSP2-4.5) and high (SSP5-8.5) emission scenarios. Our analysis indicates that\nboth the frequency and duration of Dunkelflaute events in Germany in the\nensemble mean are projected to remain largely unchanged compared to the\nhistorical period. This suggests that, under the considered climate scenarios,\nthe associated risk is expected to remain stable throughout the century.", "AI": {"tldr": "Study analyzes impact of Dunkelflaute events (low wind/solar periods) on German electricity production using climate simulations, finding frequency and duration remain largely unchanged under future climate scenarios.", "motivation": "European grid's transition to weather-dependent renewables creates vulnerability to Dunkelflaute events that could cause electricity shortages, requiring assessment of future risks.", "method": "Adapted generative deep learning framework to downscale CMIP6 climate simulations, compared with ERA5 historical data, analyzed under SSP2-4.5 and SSP5-8.5 emission scenarios.", "result": "Both frequency and duration of Dunkelflaute events in Germany projected to remain largely unchanged compared to historical period in ensemble mean.", "conclusion": "Risk from Dunkelflaute events expected to remain stable throughout the century under considered climate scenarios, suggesting no significant increase in grid vulnerability from climate change."}}
{"id": "2509.24789", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24789", "abs": "https://arxiv.org/abs/2509.24789", "authors": ["Zhijian Xu", "Wanxu Cai", "Xilin Dai", "Zhaorong Deng", "Qiang Xu"], "title": "Fidel-TS: A High-Fidelity Benchmark for Multimodal Time Series Forecasting", "comment": null, "summary": "The evaluation of time series forecasting models is hindered by a critical\nlack of high-quality benchmarks, leading to a potential illusion of progress.\nExisting datasets suffer from issues ranging from pre-training data\ncontamination in the age of LLMs to the causal and description leakage\nprevalent in early multimodal designs. To address this, we formalize the core\nprinciples of high-fidelity benchmarking, focusing on data sourcing integrity,\nstrict causal soundness, and structural clarity. We introduce Fidel-TS, a new\nlarge-scale benchmark built from the ground up on these principles by sourcing\ndata from live APIs. Our extensive experiments validate this approach by\nexposing the critical biases and design limitations of prior benchmarks.\nFurthermore, we conclusively demonstrate that the causal relevance of textual\ninformation is the key factor in unlocking genuine performance gains in\nmultimodal forecasting.", "AI": {"tldr": "Fidel-TS is a new large-scale time series forecasting benchmark built from live APIs that addresses data contamination and leakage issues in existing benchmarks, demonstrating that causal relevance of textual information is key for genuine performance gains.", "motivation": "Current time series forecasting benchmarks suffer from critical issues including pre-training data contamination in LLMs and causal/description leakage in multimodal designs, creating an illusion of progress in the field.", "method": "The authors formalize core principles of high-fidelity benchmarking (data sourcing integrity, strict causal soundness, structural clarity) and build Fidel-TS from ground up using live API data sources.", "result": "Extensive experiments validate the approach by exposing critical biases and design limitations of prior benchmarks, and demonstrate that causal relevance of textual information is the key factor for genuine performance improvements.", "conclusion": "Fidel-TS provides a reliable benchmark for time series forecasting that addresses fundamental issues in existing datasets, establishing causal relevance of textual information as crucial for meaningful multimodal forecasting advances."}}
{"id": "2509.24800", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24800", "abs": "https://arxiv.org/abs/2509.24800", "authors": ["Zixu Wang", "Hongbin Dong", "Xiaoping Zhang"], "title": "DSAT-HD: Dual-Stream Adaptive Transformer with Hybrid Decomposition for Multivariate Time Series Forecasting", "comment": "10 pages, 5 figures", "summary": "Time series forecasting is crucial for various applications, such as weather,\ntraffic, electricity, and energy predictions. Currently, common time series\nforecasting methods are based on Transformers. However, existing approaches\nprimarily model limited time series or fixed scales, making it more challenging\nto capture diverse features cross different ranges. Additionally, traditional\nmethods like STL for complex seasonality-trend decomposition require\npre-specified seasonal periods and typically handle only single, fixed\nseasonality. We propose the Hybrid Decomposition Dual-Stream Adaptive\nTransformer (DSAT-HD), which integrates three key innovations to address the\nlimitations of existing methods: 1) A hybrid decomposition mechanism combining\nEMA and Fourier decomposition with RevIN normalization, dynamically balancing\nseasonal and trend components through noise Top-k gating; 2) A multi-scale\nadaptive pathway leveraging a sparse allocator to route features to four\nparallel Transformer layers, followed by feature merging via a sparse combiner,\nenhanced by hybrid attention combining local CNNs and global interactions; 3) A\ndual-stream residual learning framework where CNN and MLP branches separately\nprocess seasonal and trend components, coordinated by a balanced loss function\nminimizing expert collaboration variance. Extensive experiments on nine\ndatasets demonstrate that DSAT-HD outperforms existing methods overall and\nachieves state-of-the-art performance on some datasets. Notably, it also\nexhibits stronger generalization capabilities across various transfer\nscenarios.", "AI": {"tldr": "DSAT-HD is a novel time series forecasting model that combines hybrid decomposition, multi-scale adaptive pathways, and dual-stream residual learning to overcome limitations of existing Transformer-based methods in handling diverse temporal features and complex seasonality.", "motivation": "Existing time series forecasting methods based on Transformers primarily model limited time series or fixed scales, making it challenging to capture diverse features across different ranges. Traditional decomposition methods like STL require pre-specified seasonal periods and typically handle only single, fixed seasonality.", "method": "1) Hybrid decomposition combining EMA and Fourier decomposition with RevIN normalization and noise Top-k gating; 2) Multi-scale adaptive pathway with sparse allocator routing features to four parallel Transformer layers and sparse combiner for feature merging; 3) Dual-stream residual learning framework with CNN and MLP branches processing seasonal/trend components separately, coordinated by balanced loss function.", "result": "Extensive experiments on nine datasets show DSAT-HD outperforms existing methods overall and achieves state-of-the-art performance on some datasets. It also exhibits stronger generalization capabilities across various transfer scenarios.", "conclusion": "DSAT-HD successfully addresses the limitations of existing time series forecasting methods by integrating hybrid decomposition, multi-scale adaptive pathways, and dual-stream learning, demonstrating superior performance and generalization capabilities."}}
{"id": "2509.24801", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.24801", "abs": "https://arxiv.org/abs/2509.24801", "authors": ["Anna Scampicchio", "Leonardo F. Toso", "Rahel Rickenbach", "James Anderson", "Melanie N. Zeilinger"], "title": "Physics-informed learning under mixing: How physical knowledge speeds up learning", "comment": null, "summary": "A major challenge in physics-informed machine learning is to understand how\nthe incorporation of prior domain knowledge affects learning rates when data\nare dependent. Focusing on empirical risk minimization with physics-informed\nregularization, we derive complexity-dependent bounds on the excess risk in\nprobability and in expectation. We prove that, when the physical prior\ninformation is aligned, the learning rate improves from the (slow) Sobolev\nminimax rate to the (fast) optimal i.i.d. one without any sample-size deflation\ndue to data dependence.", "AI": {"tldr": "Physics-informed regularization improves learning rates from slow Sobolev minimax rate to fast optimal i.i.d. rate when physical prior is aligned, eliminating sample-size deflation from data dependence.", "motivation": "To understand how domain knowledge incorporation affects learning rates with dependent data in physics-informed machine learning.", "method": "Empirical risk minimization with physics-informed regularization, deriving complexity-dependent bounds on excess risk.", "result": "When physical prior information is aligned, learning rate improves significantly without sample-size deflation due to data dependence.", "conclusion": "Properly aligned physics-informed regularization can overcome the slow learning rates typically associated with dependent data."}}
{"id": "2509.24804", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24804", "abs": "https://arxiv.org/abs/2509.24804", "authors": ["Boxuan Zhang", "Runqing Wang", "Wei Xiao", "Weipu Zhang", "Jian Sun", "Gao Huang", "Jie Chen", "Gang Wang"], "title": "DyMoDreamer: World Modeling with Dynamic Modulation", "comment": null, "summary": "A critical bottleneck in deep reinforcement learning (DRL) is sample\ninefficiency, as training high-performance agents often demands extensive\nenvironmental interactions. Model-based reinforcement learning (MBRL) mitigates\nthis by building world models that simulate environmental dynamics and generate\nsynthetic experience, improving sample efficiency. However, conventional world\nmodels process observations holistically, failing to decouple dynamic objects\nand temporal features from static backgrounds. This approach is computationally\ninefficient, especially for visual tasks where dynamic objects significantly\ninfluence rewards and decision-making performance. To address this, we\nintroduce DyMoDreamer, a novel MBRL algorithm that incorporates a dynamic\nmodulation mechanism to improve the extraction of dynamic features and enrich\nthe temporal information. DyMoDreamer employs differential observations derived\nfrom a novel inter-frame differencing mask, explicitly encoding object-level\nmotion cues and temporal dynamics. Dynamic modulation is modeled as stochastic\ncategorical distributions and integrated into a recurrent state-space model\n(RSSM), enhancing the model's focus on reward-relevant dynamics. Experiments\ndemonstrate that DyMoDreamer sets a new state-of-the-art on the Atari $100$k\nbenchmark with a $156.6$\\% mean human-normalized score, establishes a new\nrecord of $832$ on the DeepMind Visual Control Suite, and gains a $9.5$\\%\nperformance improvement after $1$M steps on the Crafter benchmark. Our code is\nreleased at https://github.com/Ultraman-Tiga1/DyMoDreamer.", "AI": {"tldr": "DyMoDreamer is a novel model-based reinforcement learning algorithm that uses dynamic modulation to improve sample efficiency by focusing on dynamic features and temporal information, achieving state-of-the-art performance on multiple benchmarks.", "motivation": "Address sample inefficiency in deep reinforcement learning by improving world models to better handle dynamic objects and temporal features, which are crucial for visual tasks but poorly handled by conventional holistic observation processing.", "method": "Introduces dynamic modulation mechanism using differential observations from inter-frame differencing mask, models dynamic modulation as stochastic categorical distributions, and integrates into recurrent state-space model (RSSM) to enhance focus on reward-relevant dynamics.", "result": "Sets new state-of-the-art on Atari 100k benchmark (156.6% mean human-normalized score), establishes new record of 832 on DeepMind Visual Control Suite, and achieves 9.5% performance improvement after 1M steps on Crafter benchmark.", "conclusion": "DyMoDreamer effectively addresses sample inefficiency in DRL by explicitly modeling dynamic features and temporal dynamics, demonstrating superior performance across multiple challenging benchmarks."}}
{"id": "2509.24827", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24827", "abs": "https://arxiv.org/abs/2509.24827", "authors": ["Bartosz Bieganowski", "Daniel Strzelecki", "Robert Skiba", "Mateusz Topolewski"], "title": "Putnam-like dataset summary: LLMs as mathematical competition contestants", "comment": "11 pages, 11 figures", "summary": "In this paper we summarize the results of the Putnam-like benchmark published\nby Google DeepMind. This dataset consists of 96 original problems in the spirit\nof the Putnam Competition and 576 solutions of LLMs. We analyse the performance\nof models on this set of problems to verify their ability to solve problems\nfrom mathematical contests.", "AI": {"tldr": "Analysis of LLM performance on 96 Putnam-like math problems from Google DeepMind's benchmark dataset with 576 solutions.", "motivation": "To evaluate LLMs' capability in solving mathematical contest problems similar to those in the Putnam Competition.", "method": "Analyzed performance of LLMs on 96 original Putnam-like problems and their 576 solutions from Google DeepMind's benchmark.", "result": "The paper presents performance analysis results showing how well LLMs can handle mathematical contest problems.", "conclusion": "Provides insights into LLMs' mathematical problem-solving abilities through systematic evaluation on Putnam-style problems."}}
{"id": "2509.24840", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.24840", "abs": "https://arxiv.org/abs/2509.24840", "authors": ["Oussama Kharouiche", "Aris Markogiannakis", "Xiao Fei", "Michail Chatzianastasis", "Michalis Vazirgiannis"], "title": "Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from RNA-Seq Data", "comment": null, "summary": "Single-cell RNA sequencing has transformed biology by enabling the\nmeasurement of gene expression at cellular resolution, providing information\nfor cell types, states, and disease contexts. Recently, single-cell foundation\nmodels have emerged as powerful tools for learning transferable representations\ndirectly from expression profiles, improving performance on classification and\nclustering tasks. However, these models are limited to discrete prediction\nheads, which collapse cellular complexity into predefined labels that fail to\ncapture the richer, contextual explanations biologists need. We introduce\nCell2Text, a multimodal generative framework that translates scRNA-seq profiles\ninto structured natural language descriptions. By integrating gene-level\nembeddings from single-cell foundation models with pretrained large language\nmodels, Cell2Text generates coherent summaries that capture cellular identity,\ntissue origin, disease associations, and pathway activity, generalizing to\nunseen cells. Empirically, Cell2Text outperforms baselines on classification\naccuracy, demonstrates strong ontological consistency using PageRank-based\nsimilarity metrics, and achieves high semantic fidelity in text generation.\nThese results demonstrate that coupling expression data with natural language\noffers both stronger predictive performance and inherently interpretable\noutputs, pointing to a scalable path for label-efficient characterization of\nunseen cells.", "AI": {"tldr": "Cell2Text is a multimodal framework that translates single-cell RNA sequencing profiles into structured natural language descriptions, outperforming traditional discrete prediction methods and providing interpretable outputs.", "motivation": "Current single-cell foundation models use discrete prediction heads that collapse cellular complexity into predefined labels, failing to capture the richer contextual explanations needed by biologists.", "method": "Integrates gene-level embeddings from single-cell foundation models with pretrained large language models to generate coherent natural language summaries of cellular identity, tissue origin, disease associations, and pathway activity.", "result": "Outperforms baselines on classification accuracy, demonstrates strong ontological consistency using PageRank-based similarity metrics, and achieves high semantic fidelity in text generation.", "conclusion": "Coupling expression data with natural language offers both stronger predictive performance and inherently interpretable outputs, providing a scalable path for label-efficient characterization of unseen cells."}}
{"id": "2509.24856", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24856", "abs": "https://arxiv.org/abs/2509.24856", "authors": ["Christos Mountzouris"], "title": "Beyond the Hook: Predicting Billboard Hot 100 Chart Inclusion with Machine Learning from Streaming, Audio Signals, and Perceptual Features", "comment": "17 pages, 6 figures, 3 tables", "summary": "The advent of digital streaming platforms have recently revolutionized the\nlandscape of music industry, with the ensuing digitalization providing\nstructured data collections that open new research avenues for investigating\npopularity dynamics and mainstream success. The present work explored which\ndeterminants hold the strongest predictive influence for a track's inclusion in\nthe Billboard Hot 100 charts, including streaming popularity, measurable audio\nsignal attributes, and probabilistic indicators of human listening. The\nanalysis revealed that popularity was by far the most decisive predictor of\nBillboard Hot 100 inclusion, with considerable contribution from\ninstrumentalness, valence, duration and speechiness. Logistic Regression\nachieved 90.0% accuracy, with very high recall for charting singles (0.986) but\nlower recall for non-charting ones (0.813), yielding balanced F1-scores around\n0.90. Random Forest slightly improved performance to 90.4% accuracy,\nmaintaining near-perfect precision for non-charting singles (0.990) and high\nrecall for charting ones (0.992), with F1-scores up to 0.91. Gradient Boosting\n(XGBoost) reached 90.3% accuracy, delivering a more balanced trade-off by\nimproving recall for non-charting singles (0.837) while sustaining high recall\nfor charting ones (0.969), resulting in F1-scores comparable to the other\nmodels.", "AI": {"tldr": "This study identifies key predictors for Billboard Hot 100 chart inclusion, finding popularity as the strongest factor, with audio attributes like instrumentalness, valence, duration, and speechiness also contributing significantly.", "motivation": "To investigate which determinants most strongly predict a track's inclusion in Billboard Hot 100 charts using digital streaming data, including streaming popularity, audio signal attributes, and human listening indicators.", "method": "Used machine learning models (Logistic Regression, Random Forest, XGBoost) to analyze predictors including streaming popularity, audio attributes (instrumentalness, valence, duration, speechiness), and human listening indicators.", "result": "Popularity was the most decisive predictor. Logistic Regression achieved 90.0% accuracy, Random Forest 90.4%, and XGBoost 90.3%. All models showed high performance with F1-scores around 0.90-0.91.", "conclusion": "Streaming popularity is the strongest predictor of Billboard Hot 100 inclusion, with audio attributes providing additional predictive power. Machine learning models can accurately predict chart success with over 90% accuracy."}}
{"id": "2509.24868", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.24868", "abs": "https://arxiv.org/abs/2509.24868", "authors": ["Jiayi Li", "Flora D. Salim"], "title": "DRIFT-Net: A Spectral--Coupled Neural Operator for PDEs Learning", "comment": null, "summary": "Learning PDE dynamics with neural solvers can significantly improve\nwall-clock efficiency and accuracy compared with classical numerical solvers.\nIn recent years, foundation models for PDEs have largely adopted multi-scale\nwindowed self-attention, with the scOT backbone in \\textsc{Poseidon} serving as\na representative example.\n  However, because of their locality, truly globally consistent spectral\ncoupling can only be propagated gradually through deep stacking and window\nshifting. This weakens global coupling and leads to error accumulation and\ndrift during closed-loop rollouts. To address this, we propose\n\\textbf{DRIFT-Net}. It employs a dual-branch design comprising a spectral\nbranch and an image branch. The spectral branch is responsible for capturing\nglobal, large-scale low-frequency information, whereas the image branch focuses\non local details and nonstationary structures. Specifically, we first perform\ncontrolled, lightweight mixing within the low-frequency range. Then we fuse the\nspectral and image paths at each layer via bandwise weighting, which avoids the\nwidth inflation and training instability caused by naive concatenation. The\nfused result is transformed back into the spatial domain and added to the image\nbranch, thereby preserving both global structure and high-frequency details\nacross scales. Compared with strong attention-based baselines, DRIFT-Net\nachieves lower error and higher throughput with fewer parameters under\nidentical training settings and budget. On Navier--Stokes benchmarks, the\nrelative $L_{1}$ error is reduced by 7\\%--54\\%, the parameter count decreases\nby about 15\\%, and the throughput remains higher than scOT. Ablation studies\nand theoretical analyses further demonstrate the stability and effectiveness of\nthis design. The code is available at\nhttps://github.com/cruiseresearchgroup/DRIFT-Net.", "AI": {"tldr": "DRIFT-Net proposes a dual-branch neural network architecture for PDE learning that combines spectral and image branches to capture both global low-frequency information and local details, addressing limitations of attention-based methods in maintaining global consistency during long-term rollouts.", "motivation": "Existing foundation models for PDEs using multi-scale windowed self-attention suffer from weak global coupling due to locality, leading to error accumulation and drift during closed-loop rollouts. The motivation is to develop a method that maintains strong global spectral coupling while preserving local details.", "method": "DRIFT-Net employs a dual-branch design with spectral and image branches. The spectral branch captures global low-frequency information, while the image branch handles local details. Controlled lightweight mixing within low-frequency range, followed by bandwise weighting fusion of both branches at each layer, and transformation back to spatial domain while preserving both global structure and high-frequency details.", "result": "Compared to attention-based baselines, DRIFT-Net achieves lower error and higher throughput with fewer parameters. On Navier-Stokes benchmarks: relative L1 error reduced by 7%-54%, parameter count decreased by ~15%, throughput remains higher than scOT. Ablation studies and theoretical analyses demonstrate stability and effectiveness.", "conclusion": "DRIFT-Net's dual-branch architecture effectively addresses global coupling limitations in PDE foundation models, providing improved accuracy, efficiency, and stability for learning PDE dynamics compared to existing attention-based approaches."}}
{"id": "2509.24873", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24873", "abs": "https://arxiv.org/abs/2509.24873", "authors": ["Teodor Chiaburu", "Vipin Singh", "Frank Hau\u00dfer", "Felix Bie\u00dfmann"], "title": "Uncertainty-Guided Expert-AI Collaboration for Efficient Soil Horizon Annotation", "comment": "11 pages, 7 figures, presented at ECAI 2025, CLEAR-AI Workshop,\n  Bologna", "summary": "Uncertainty quantification is essential in human-machine collaboration, as\nhuman agents tend to adjust their decisions based on the confidence of the\nmachine counterpart. Reliably calibrated model uncertainties, hence, enable\nmore effective collaboration, targeted expert intervention and more responsible\nusage of Machine Learning (ML) systems. Conformal prediction has become a well\nestablished model-agnostic framework for uncertainty calibration of ML models,\noffering statistically valid confidence estimates for both regression and\nclassification tasks. In this work, we apply conformal prediction to\n$\\textit{SoilNet}$, a multimodal multitask model for describing soil profiles.\nWe design a simulated human-in-the-loop (HIL) annotation pipeline, where a\nlimited budget for obtaining ground truth annotations from domain experts is\navailable when model uncertainty is high. Our experiments show that\nconformalizing SoilNet leads to more efficient annotation in regression tasks\nand comparable performance scores in classification tasks under the same\nannotation budget when tested against its non-conformal counterpart. All code\nand experiments can be found in our repository:\nhttps://github.com/calgo-lab/BGR", "AI": {"tldr": "This paper applies conformal prediction to SoilNet, a multimodal multitask soil profile model, to improve uncertainty quantification and enable more efficient human-in-the-loop annotation with limited expert budget.", "motivation": "Uncertainty quantification is essential for human-machine collaboration, as humans adjust decisions based on machine confidence. Reliable uncertainty calibration enables more effective collaboration, targeted expert intervention, and responsible ML usage.", "method": "Apply conformal prediction framework to SoilNet model, design simulated human-in-the-loop annotation pipeline where domain experts provide ground truth annotations only when model uncertainty is high, within limited budget constraints.", "result": "Conformalizing SoilNet leads to more efficient annotation in regression tasks and comparable performance scores in classification tasks under the same annotation budget compared to non-conformal counterpart.", "conclusion": "Conformal prediction effectively improves uncertainty quantification for SoilNet, enabling more efficient human-in-the-loop annotation workflows while maintaining performance, particularly beneficial for regression tasks."}}
{"id": "2509.24882", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24882", "abs": "https://arxiv.org/abs/2509.24882", "authors": ["Leonardo Defilippis", "Yizhou Xu", "Julius Girardin", "Emanuele Troiani", "Vittorio Erba", "Lenka Zdeborov\u00e1", "Bruno Loureiro", "Florent Krzakala"], "title": "Scaling Laws and Spectra of Shallow Neural Networks in the Feature Learning Regime", "comment": null, "summary": "Neural scaling laws underlie many of the recent advances in deep learning,\nyet their theoretical understanding remains largely confined to linear models.\nIn this work, we present a systematic analysis of scaling laws for quadratic\nand diagonal neural networks in the feature learning regime. Leveraging\nconnections with matrix compressed sensing and LASSO, we derive a detailed\nphase diagram for the scaling exponents of the excess risk as a function of\nsample complexity and weight decay. This analysis uncovers crossovers between\ndistinct scaling regimes and plateau behaviors, mirroring phenomena widely\nreported in the empirical neural scaling literature. Furthermore, we establish\na precise link between these regimes and the spectral properties of the trained\nnetwork weights, which we characterize in detail. As a consequence, we provide\na theoretical validation of recent empirical observations connecting the\nemergence of power-law tails in the weight spectrum with network generalization\nperformance, yielding an interpretation from first principles.", "AI": {"tldr": "This paper provides a theoretical analysis of neural scaling laws for quadratic and diagonal neural networks in the feature learning regime, connecting them to matrix compressed sensing and LASSO to derive phase diagrams and explain empirical observations about weight spectrum properties.", "motivation": "While neural scaling laws drive recent deep learning advances, their theoretical understanding remains limited to linear models. The authors aim to extend this understanding to more complex neural networks in the feature learning regime.", "method": "The authors analyze scaling laws for quadratic and diagonal neural networks using connections with matrix compressed sensing and LASSO, deriving phase diagrams for scaling exponents as functions of sample complexity and weight decay.", "result": "The analysis reveals crossovers between distinct scaling regimes and plateau behaviors, mirroring empirical observations. It establishes a precise link between these regimes and the spectral properties of trained network weights, particularly the emergence of power-law tails in the weight spectrum.", "conclusion": "The work provides theoretical validation for empirical observations connecting power-law tails in weight spectra with network generalization performance, offering an interpretation from first principles for neural scaling phenomena."}}
{"id": "2509.24886", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24886", "abs": "https://arxiv.org/abs/2509.24886", "authors": ["Ya-Wei Eileen Lin", "Ron Levie"], "title": "Adaptive Canonicalization with Application to Invariant Anisotropic Geometric Networks", "comment": null, "summary": "Canonicalization is a widely used strategy in equivariant machine learning,\nenforcing symmetry in neural networks by mapping each input to a standard form.\nYet, it often introduces discontinuities that can affect stability during\ntraining, limit generalization, and complicate universal approximation\ntheorems. In this paper, we address this by introducing \\emph{adaptive\ncanonicalization}, a general framework in which the canonicalization depends\nboth on the input and the network. Specifically, we present the adaptive\ncanonicalization based on prior maximization, where the standard form of the\ninput is chosen to maximize the predictive confidence of the network. We prove\nthat this construction yields continuous and symmetry-respecting models that\nadmit universal approximation properties.\n  We propose two applications of our setting: (i) resolving eigenbasis\nambiguities in spectral graph neural networks, and (ii) handling rotational\nsymmetries in point clouds. We empirically validate our methods on molecular\nand protein classification, as well as point cloud classification tasks. Our\nadaptive canonicalization outperforms the three other common solutions to\nequivariant machine learning: data augmentation, standard canonicalization, and\nequivariant architectures.", "AI": {"tldr": "The paper introduces adaptive canonicalization, a framework where canonicalization depends on both input and network, addressing discontinuities in traditional canonicalization methods while maintaining symmetry and enabling universal approximation.", "motivation": "Traditional canonicalization in equivariant ML introduces discontinuities that affect training stability, limit generalization, and complicate universal approximation theorems.", "method": "Proposes adaptive canonicalization based on prior maximization, where the input's standard form is chosen to maximize the network's predictive confidence, ensuring continuity and symmetry.", "result": "The method yields continuous and symmetry-respecting models with universal approximation properties, outperforming data augmentation, standard canonicalization, and equivariant architectures on molecular, protein, and point cloud classification tasks.", "conclusion": "Adaptive canonicalization effectively resolves discontinuities in canonicalization while maintaining symmetry and enabling universal approximation, demonstrating superior performance across various applications."}}
{"id": "2509.24895", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24895", "abs": "https://arxiv.org/abs/2509.24895", "authors": ["Kosio Beshkov", "Anders Malthe-S\u00f8renssen"], "title": "Towards Understanding the Shape of Representations in Protein Language Models", "comment": null, "summary": "While protein language models (PLMs) are one of the most promising avenues of\nresearch for future de novo protein design, the way in which they transform\nsequences to hidden representations, as well as the information encoded in such\nrepresentations is yet to be fully understood. Several works have attempted to\npropose interpretability tools for PLMs, but they have focused on understanding\nhow individual sequences are transformed by such models. Therefore, the way in\nwhich PLMs transform the whole space of sequences along with their relations is\nstill unknown. In this work we attempt to understand this transformed space of\nsequences by identifying protein structure and representation with square-root\nvelocity (SRV) representations and graph filtrations. Both approaches naturally\nlead to a metric space in which pairs of proteins or protein representations\ncan be compared with each other.\n  We analyze different types of proteins from the SCOP dataset and show that\nthe Karcher mean and effective dimension of the SRV shape space follow a\nnon-linear pattern as a function of the layers in ESM2 models of different\nsizes. Furthermore, we use graph filtrations as a tool to study the context\nlengths at which models encode the structural features of proteins. We find\nthat PLMs preferentially encode immediate as well as local relations between\nresidues, but start to degrade for larger context lengths. The most\nstructurally faithful encoding tends to occur close to, but before the last\nlayer of the models, indicating that training a folding model ontop of these\nlayers might lead to improved folding performance.", "AI": {"tldr": "This paper analyzes how protein language models (PLMs) transform sequence spaces and encode structural information using SRV representations and graph filtrations, revealing that PLMs preferentially encode local residue relations and that optimal structural encoding occurs before the final layers.", "motivation": "To understand how PLMs transform the entire sequence space and encode structural relationships, as current interpretability tools only focus on individual sequences.", "method": "Used square-root velocity (SRV) representations and graph filtrations to create metric spaces for comparing protein structures and representations, analyzing SCOP dataset proteins across ESM2 model layers.", "result": "Found non-linear patterns in Karcher mean and effective dimension across layers, with PLMs preferentially encoding immediate and local residue relations but degrading for larger contexts. Optimal structural encoding occurs before the last layers.", "conclusion": "Training folding models on layers just before the final layer might improve protein folding performance, as these layers provide the most structurally faithful encodings."}}
{"id": "2509.24933", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.24933", "abs": "https://arxiv.org/abs/2509.24933", "authors": ["Sebastian W. Ober", "Calvin McCarter", "Aniruddh Raghu", "Yucen Lily Li", "Alan N. Amin", "Andrew Gordon Wilson", "Hunter Elliott"], "title": "Is Sequence Information All You Need for Bayesian Optimization of Antibodies?", "comment": "Accepted into the AI for Science Workshop, NeurIPS 2025", "summary": "Bayesian optimization is a natural candidate for the engineering of antibody\ntherapeutic properties, which is often iterative and expensive. However,\nfinding the optimal choice of surrogate model for optimization over the highly\nstructured antibody space is difficult, and may differ depending on the\nproperty being optimized. Moreover, to the best of our knowledge, no prior\nworks have attempted to incorporate structural information into antibody\nBayesian optimization. In this work, we explore different approaches to\nincorporating structural information into Bayesian optimization, and compare\nthem to a variety of sequence-only approaches on two different antibody\nproperties, binding affinity and stability. In addition, we propose the use of\na protein language model-based ``soft constraint,'' which helps guide the\noptimization to promising regions of the space. We find that certain types of\nstructural information improve data efficiency in early optimization rounds for\nstability, but have equivalent peak performance. Moreover, when incorporating\nthe protein language model soft constraint we find that the data efficiency gap\nis diminished for affinity and eliminated for stability, resulting in\nsequence-only methods that match the performance of structure-based methods,\nraising questions about the necessity of structure in Bayesian optimization for\nantibodies.", "AI": {"tldr": "This paper explores Bayesian optimization for antibody engineering, comparing structure-based and sequence-only approaches with a novel protein language model soft constraint.", "motivation": "Antibody therapeutic property engineering is iterative and expensive, but existing Bayesian optimization methods lack structural information incorporation and optimal surrogate model selection varies by property.", "method": "Compared different structural information incorporation approaches with sequence-only methods, and proposed a protein language model-based soft constraint to guide optimization.", "result": "Structural information improved early data efficiency for stability but had equivalent peak performance. The protein language model soft constraint eliminated the data efficiency gap, making sequence-only methods match structure-based performance.", "conclusion": "The necessity of structural information in Bayesian optimization for antibodies is questioned, as sequence-only methods with language model constraints can achieve equivalent performance."}}
{"id": "2509.24936", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24936", "abs": "https://arxiv.org/abs/2509.24936", "authors": ["Angxiao Yue", "Anqi Dong", "Hongteng Xu"], "title": "OAT-FM: Optimal Acceleration Transport for Improved Flow Matching", "comment": null, "summary": "As a powerful technique in generative modeling, Flow Matching (FM) aims to\nlearn velocity fields from noise to data, which is often explained and\nimplemented as solving Optimal Transport (OT) problems. In this study, we\nbridge FM and the recent theory of Optimal Acceleration Transport (OAT),\ndeveloping an improved FM method called OAT-FM and exploring its benefits in\nboth theory and practice. In particular, we demonstrate that the straightening\nobjective hidden in existing OT-based FM methods is mathematically equivalent\nto minimizing the physical action associated with acceleration defined by OAT.\nAccordingly, instead of enforcing constant velocity, OAT-FM optimizes the\nacceleration transport in the product space of sample and velocity, whose\nobjective corresponds to a necessary and sufficient condition of flow\nstraightness. An efficient algorithm is designed to achieve OAT-FM with low\ncomplexity. OAT-FM motivates a new two-phase FM paradigm: Given a generative\nmodel trained by an arbitrary FM method, whose velocity information has been\nrelatively reliable, we can fine-tune and improve it via OAT-FM. This paradigm\neliminates the risk of data distribution drift and the need to generate a large\nnumber of noise data pairs, which consistently improves model performance in\nvarious generative tasks. Code is available at:\nhttps://github.com/AngxiaoYue/OAT-FM", "AI": {"tldr": "OAT-FM bridges Flow Matching with Optimal Acceleration Transport theory, developing an improved method that optimizes acceleration transport instead of constant velocity, leading to straighter flows and better performance in generative tasks.", "motivation": "Existing Flow Matching methods are explained as solving Optimal Transport problems, but this study aims to connect FM with the newer theory of Optimal Acceleration Transport to develop improved methods.", "method": "OAT-FM optimizes acceleration transport in the product space of sample and velocity, using an efficient algorithm with low complexity. It enables a two-phase FM paradigm where pretrained models can be fine-tuned via OAT-FM.", "result": "OAT-FM consistently improves model performance in various generative tasks, eliminates data distribution drift risk, and avoids the need to generate large noise data pairs.", "conclusion": "OAT-FM provides theoretical and practical improvements over traditional FM methods by leveraging Optimal Acceleration Transport theory, offering a more effective approach to generative modeling."}}
{"id": "2509.24947", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24947", "abs": "https://arxiv.org/abs/2509.24947", "authors": ["Sooraj Sathish", "Keshav Goyal", "Raghuram Bharadwaj Diddigi"], "title": "Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer", "comment": null, "summary": "Deep Reinforcement Learning (RL) has demonstrated success in solving complex\nsequential decision-making problems by integrating neural networks with the RL\nframework. However, training deep RL models poses several challenges, such as\nthe need for extensive hyperparameter tuning and high computational costs.\nTransfer learning has emerged as a promising strategy to address these\nchallenges by enabling the reuse of knowledge from previously learned tasks for\nnew, related tasks. This avoids the need for retraining models entirely from\nscratch. A commonly used approach for transfer learning in RL is to leverage\nthe internal representations learned by the neural network during training.\nSpecifically, the activations from the last hidden layer can be viewed as\nrefined state representations that encapsulate the essential features of the\ninput. In this work, we investigate whether these representations can be used\nas input for training simpler models, such as linear function approximators, on\nnew tasks. We observe that the representations learned by standard deep RL\nmodels can be highly correlated, which limits their effectiveness when used\nwith linear function approximation. To mitigate this problem, we propose a\nnovel deep Q-learning approach that introduces a regularization term to reduce\npositive correlations between feature representation of states. By leveraging\nthese reduced correlated features, we enable more effective use of linear\nfunction approximation in transfer learning. Through experiments and ablation\nstudies on standard RL benchmarks and MinAtar games, we demonstrate the\nefficacy of our approach in improving transfer learning performance and thereby\nreducing computational overhead.", "AI": {"tldr": "This paper proposes a novel deep Q-learning method with regularization to reduce correlation in feature representations, enabling more effective transfer learning using linear function approximation.", "motivation": "Deep RL training faces challenges like extensive hyperparameter tuning and high computational costs. Transfer learning can address these by reusing knowledge from previous tasks, but standard deep RL representations are often highly correlated, limiting their effectiveness with linear function approximation.", "method": "The authors propose a deep Q-learning approach with a regularization term that reduces positive correlations between state feature representations. This creates less correlated features that work better with linear function approximation for transfer learning.", "result": "Experiments on standard RL benchmarks and MinAtar games show that the proposed approach improves transfer learning performance and reduces computational overhead compared to standard methods.", "conclusion": "By reducing correlation in learned representations, the proposed method enables more effective use of linear function approximation in transfer learning, addressing key challenges in deep RL training while maintaining performance."}}
{"id": "2509.24957", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24957", "abs": "https://arxiv.org/abs/2509.24957", "authors": ["Weifan Jiang", "Rana Shahout", "Yilun Du", "Michael Mitzenmacher", "Minlan Yu"], "title": "Intra-request branch orchestration for efficient LLM reasoning", "comment": "15 pages, 6 figures", "summary": "Large Language Models (LLMs) increasingly rely on inference-time reasoning\nalgorithms such as chain-of-thought and multi-branch reasoning to improve\naccuracy on complex tasks. These methods, however, substantially increase token\nusage and per-request latency. Prior work has largely focused on reducing token\nusage, often at the expense of accuracy, while overlooking other latency\nfactors. We present DUCHESS, an LLM serving system that reduces cost and\nlatency without sacrificing accuracy through intra-request branch orchestration\nguided by predictions. DUCHESS employs a lightweight linear probing model over\nLLM layer activations to estimate branch correctness, and its orchestration\npolicy decides whether to terminate, duplicate, or continue a branch. When\nhandling multiple requests, DUCHESS further reduces latency by prioritizing\neasier reasoning tasks when complexity can be estimated from the prompt.\nExperiments on three reasoning benchmarks show that DUCHESS consistently\nimproves the token-accuracy Pareto frontier, reducing token usage by 42-63% at\nmatched accuracy compared to self-consistency. In serving with vLLM, DUCHESS\nreduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% with\nFirst-Come-First-Served scheduling, and achieves additional gains under\ndifficulty-aware scheduling at higher request rates.", "AI": {"tldr": "DUCHESS is an LLM serving system that reduces inference latency and token usage for reasoning tasks without sacrificing accuracy, using branch orchestration guided by correctness predictions.", "motivation": "Current inference-time reasoning methods like chain-of-thought and multi-branch reasoning significantly increase token usage and latency, with prior work focusing mainly on token reduction at the expense of accuracy.", "method": "DUCHESS uses lightweight linear probing on LLM layer activations to predict branch correctness, and implements orchestration policies to terminate, duplicate, or continue branches. It also prioritizes easier reasoning tasks when complexity can be estimated.", "result": "On three reasoning benchmarks, DUCHESS reduces token usage by 42-63% at matched accuracy compared to self-consistency. In serving with vLLM, it reduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% respectively.", "conclusion": "DUCHESS effectively improves the token-accuracy Pareto frontier and significantly reduces latency in LLM serving for reasoning tasks while maintaining accuracy."}}
{"id": "2509.24962", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24962", "abs": "https://arxiv.org/abs/2509.24962", "authors": ["Valentyn Melnychuk", "Dennis Frauen", "Jonas Schweisthal", "Stefan Feuerriegel"], "title": "Overlap-Adaptive Regularization for Conditional Average Treatment Effect Estimation", "comment": null, "summary": "The conditional average treatment effect (CATE) is widely used in\npersonalized medicine to inform therapeutic decisions. However,\nstate-of-the-art methods for CATE estimation (so-called meta-learners) often\nperform poorly in the presence of low overlap. In this work, we introduce a new\napproach to tackle this issue and improve the performance of existing\nmeta-learners in the low-overlap regions. Specifically, we introduce\nOverlap-Adaptive Regularization (OAR) that regularizes target models\nproportionally to overlap weights so that, informally, the regularization is\nhigher in regions with low overlap. To the best of our knowledge, our OAR is\nthe first approach to leverage overlap weights in the regularization terms of\nthe meta-learners. Our OAR approach is flexible and works with any existing\nCATE meta-learner: we demonstrate how OAR can be applied to both parametric and\nnon-parametric second-stage models. Furthermore, we propose debiased versions\nof our OAR that preserve the Neyman-orthogonality of existing meta-learners and\nthus ensure more robust inference. Through a series of (semi-)synthetic\nexperiments, we demonstrate that our OAR significantly improves CATE estimation\nin low-overlap settings in comparison to constant regularization.", "AI": {"tldr": "This paper introduces Overlap-Adaptive Regularization (OAR) to improve CATE estimation in low-overlap regions by regularizing models proportionally to overlap weights, with higher regularization in low-overlap areas.", "motivation": "State-of-the-art CATE estimation methods (meta-learners) perform poorly in low-overlap regions, limiting their effectiveness in personalized medicine for therapeutic decision-making.", "method": "Proposed Overlap-Adaptive Regularization (OAR) that applies regularization proportional to overlap weights, with debiased versions preserving Neyman-orthogonality. Works with both parametric and non-parametric meta-learners.", "result": "Through (semi-)synthetic experiments, OAR significantly improves CATE estimation in low-overlap settings compared to constant regularization approaches.", "conclusion": "OAR is the first approach to leverage overlap weights in regularization terms of meta-learners, providing flexible and effective improvement for CATE estimation in challenging low-overlap scenarios."}}
{"id": "2509.24974", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24974", "abs": "https://arxiv.org/abs/2509.24974", "authors": ["Ahmad Fraij", "Sam Dauncey"], "title": "Double Descent as a Lens for Sample Efficiency in Autoregressive vs. Discrete Diffusion Models", "comment": null, "summary": "Data scarcity drives the need for more sample-efficient large language\nmodels. In this work, we use the double descent phenomenon to holistically\ncompare the sample efficiency of discrete diffusion and autoregressive models.\nWe show that discrete diffusion models require larger capacity and more\ntraining epochs to escape their underparameterized regime and reach the\ninterpolation threshold. In the strongly overparameterized regime, both models\nexhibit similar behavior, with neither exhibiting a pronounced second descent\nin test loss across a large range of model sizes. Overall, our results indicate\nthat autoregressive models are more sample-efficient on small-scale datasets,\nwhile discrete diffusion models only become competitive when given sufficient\ncapacity and compute.", "AI": {"tldr": "This paper compares sample efficiency between discrete diffusion and autoregressive models using double descent phenomenon, finding autoregressive models are more sample-efficient on small datasets while discrete diffusion models need more capacity and compute to become competitive.", "motivation": "Data scarcity drives the need for more sample-efficient large language models, motivating comparison of discrete diffusion vs autoregressive models.", "method": "Using double descent phenomenon to holistically compare sample efficiency of discrete diffusion and autoregressive models across different capacity regimes.", "result": "Discrete diffusion models require larger capacity and more training epochs to reach interpolation threshold. In overparameterized regime, both show similar behavior without pronounced second descent. Autoregressive models are more sample-efficient on small datasets.", "conclusion": "Autoregressive models are more sample-efficient on small-scale datasets, while discrete diffusion models only become competitive when given sufficient capacity and compute."}}
{"id": "2509.24981", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24981", "abs": "https://arxiv.org/abs/2509.24981", "authors": ["Haoran He", "Yuxiao Ye", "Qingpeng Cai", "Chen Hu", "Binxing Jiao", "Daxin Jiang", "Ling Pan"], "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards", "comment": "32 pages", "summary": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for\nimproving the reasoning abilities of large language models (LLMs). Current\nmethods rely primarily on policy optimization frameworks like PPO and GRPO,\nwhich follow generalized policy iteration that alternates between evaluating\nthe current policy's value and improving the policy based on evaluation. While\neffective, they often suffer from training instability and diversity collapse,\nrequiring complex heuristic tricks and careful tuning. We observe that standard\nRLVR in math reasoning can be formalized as a specialized finite-horizon Markov\nDecision Process with deterministic state transitions, tree-structured\ndynamics, and binary terminal rewards. Though large in scale, the underlying\nstructure is simpler than general-purpose control settings for which popular RL\nalgorithms (e.g., PPO) were developed, suggesting that several sophisticated\ntechniques in existing methods may be reduced or even omitted. Based on this\ninsight, we prove a surprising result: the optimal action can be recovered from\nthe Q-function of a fixed uniformly random policy, thereby bypassing the\ngeneralized policy iteration loop and its associated heuristics. We introduce\nRandom Policy Valuation for Diverse Reasoning (ROVER) to translate this\nprinciple into a practical and scalable algorithm for LLM math reasoning, a\nminimalist yet highly effective RL method that samples actions from a softmax\nover these uniform-policy Q-values. ROVER preserves diversity throughout\ntraining, allowing sustained exploration of multiple valid pathways. Across\nmultiple base models and standard math reasoning benchmarks, ROVER demonstrates\nsuperior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1,\n\\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite\nits radical simplification compared to strong, complicated existing methods.", "AI": {"tldr": "ROVER is a minimalist RL method for math reasoning that uses Q-values from a uniformly random policy instead of complex policy optimization, achieving better performance and diversity with simpler implementation.", "motivation": "Current RLVR methods using PPO/GRPO suffer from training instability and diversity collapse, requiring complex heuristics. The authors observe that math reasoning has simpler structure than general RL problems, suggesting existing sophisticated techniques may be unnecessary.", "method": "Prove that optimal actions can be recovered from Q-function of fixed uniformly random policy, bypassing policy iteration loop. Introduce ROVER algorithm that samples actions from softmax over these uniform-policy Q-values.", "result": "ROVER achieves superior performance: +8.2 on pass@1, +16.8 on pass@256, and +17.6% diversity across multiple base models and math reasoning benchmarks.", "conclusion": "ROVER demonstrates that minimalist RL approaches can outperform complex existing methods in math reasoning, preserving diversity throughout training while being simpler to implement."}}
{"id": "2509.24991", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24991", "abs": "https://arxiv.org/abs/2509.24991", "authors": ["Lu Zou", "Wendi Ren", "Weizhong Zhang", "Liang Ding", "Shuang Li"], "title": "Sampling Complexity of TD and PPO in RKHS", "comment": null, "summary": "We revisit Proximal Policy Optimization (PPO) from a function-space\nperspective. Our analysis decouples policy evaluation and improvement in a\nreproducing kernel Hilbert space (RKHS): (i) A kernelized temporal-difference\n(TD) critic performs efficient RKHS-gradient updates using only one-step\nstate-action transition samples; (ii) a KL-regularized, natural-gradient policy\nstep exponentiates the evaluated action-value, recovering a PPO/TRPO-style\nproximal update in continuous state-action spaces. We provide non-asymptotic,\ninstance-adaptive guarantees whose rates depend on RKHS entropy, unifying\ntabular, linear, Sobolev, Gaussian, and Neural Tangent Kernel (NTK) regimes,\nand we derive a sampling rule for the proximal update that ensures the optimal\n$k^{-1/2}$ convergence rate for stochastic optimization. Empirically, the\ntheory-aligned schedule improves stability and sample efficiency on common\ncontrol tasks (e.g., CartPole, Acrobot), while our TD-based critic attains\nfavorable throughput versus a GAE baseline. Altogether, our results place PPO\non a firmer theoretical footing beyond finite-dimensional assumptions and\nclarify when RKHS-proximal updates with kernel-TD critics yield global policy\nimprovement with practical efficiency.", "AI": {"tldr": "This paper provides a function-space analysis of PPO using reproducing kernel Hilbert spaces, decoupling policy evaluation (via kernelized TD critic) and improvement (via KL-regularized natural gradient), with theoretical guarantees and empirical improvements.", "motivation": "To place PPO on firmer theoretical footing beyond finite-dimensional assumptions and understand when RKHS-proximal updates with kernel-TD critics yield global policy improvement with practical efficiency.", "method": "Decouples policy evaluation and improvement in RKHS: (i) kernelized TD critic performs efficient RKHS-gradient updates, (ii) KL-regularized natural-gradient policy step exponentiates action-value, recovering PPO/TRPO-style proximal updates in continuous spaces.", "result": "Provides non-asymptotic, instance-adaptive guarantees with rates depending on RKHS entropy, unifying various kernel regimes. Derives optimal k^{-1/2} convergence rate sampling rule. Empirically improves stability and sample efficiency on control tasks, with favorable throughput versus GAE baseline.", "conclusion": "The analysis places PPO on firmer theoretical foundation and clarifies conditions for global policy improvement with practical efficiency using RKHS-proximal updates and kernel-TD critics."}}
{"id": "2509.25003", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25003", "abs": "https://arxiv.org/abs/2509.25003", "authors": ["Mingxing Rao", "Bowen Qu", "Daniel Moyer"], "title": "Score-based Membership Inference on Diffusion Models", "comment": null, "summary": "Membership inference attacks (MIAs) against diffusion models have emerged as\na pressing privacy concern, as these models may inadvertently reveal whether a\ngiven sample was part of their training set. We present a theoretical and\nempirical study of score-based MIAs, focusing on the predicted noise vectors\nthat diffusion models learn to approximate. We show that the expected denoiser\noutput points toward a kernel-weighted local mean of nearby training samples,\nsuch that its norm encodes proximity to the training set and thereby reveals\nmembership. Building on this observation, we propose SimA, a single-query\nattack that provides a principled, efficient alternative to existing\nmulti-query methods. SimA achieves consistently strong performance across\nvariants of DDPM, Latent Diffusion Model (LDM). Notably, we find that Latent\nDiffusion Models are surprisingly less vulnerable than pixel-space models, due\nto the strong information bottleneck imposed by their latent auto-encoder. We\nfurther investigate this by differing the regularization hyperparameters\n($\\beta$ in $\\beta$-VAE) in latent channel and suggest a strategy to make LDM\ntraining more robust to MIA. Our results solidify the theory of score-based\nMIAs, while highlighting that Latent Diffusion class of methods requires better\nunderstanding of inversion for VAE, and not simply inversion of the Diffusion\nprocess", "AI": {"tldr": "The paper presents SimA, a single-query membership inference attack for diffusion models that uses predicted noise vector norms to detect training data membership, showing latent diffusion models are less vulnerable due to VAE bottlenecks.", "motivation": "Membership inference attacks pose serious privacy risks for diffusion models by revealing whether specific samples were in training data, requiring efficient detection methods.", "method": "Proposed SimA attack analyzes predicted noise vectors from diffusion models, showing their norms encode proximity to training set; tested on DDPM and Latent Diffusion Model variants.", "result": "SimA achieves strong performance across diffusion model variants; latent diffusion models are less vulnerable due to VAE bottlenecks; varying VAE regularization affects vulnerability.", "conclusion": "Score-based MIAs are theoretically grounded; latent diffusion methods need better VAE inversion understanding, not just diffusion process inversion; suggested strategies to improve LDM robustness."}}
{"id": "2509.25017", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25017", "abs": "https://arxiv.org/abs/2509.25017", "authors": ["Spyros Kondylatos", "Gustau Camps-Valls", "Ioannis Papoutsis"], "title": "Uncertainty-Aware Deep Learning for Wildfire Danger Forecasting", "comment": null, "summary": "Wildfires are among the most severe natural hazards, posing a significant\nthreat to both humans and natural ecosystems. The growing risk of wildfires\nincreases the demand for forecasting models that are not only accurate but also\nreliable. Deep Learning (DL) has shown promise in predicting wildfire danger;\nhowever, its adoption is hindered by concerns over the reliability of its\npredictions, some of which stem from the lack of uncertainty quantification. To\naddress this challenge, we present an uncertainty-aware DL framework that\njointly captures epistemic (model) and aleatoric (data) uncertainty to enhance\nshort-term wildfire danger forecasting. In the next-day forecasting, our\nbest-performing model improves the F1 Score by 2.3% and reduces the Expected\nCalibration Error by 2.1% compared to a deterministic baseline, enhancing both\npredictive skill and calibration. Our experiments confirm the reliability of\nthe uncertainty estimates and illustrate their practical utility for decision\nsupport, including the identification of uncertainty thresholds for rejecting\nlow-confidence predictions and the generation of well-calibrated wildfire\ndanger maps with accompanying uncertainty layers. Extending the forecast\nhorizon up to ten days, we observe that aleatoric uncertainty increases with\ntime, showing greater variability in environmental conditions, while epistemic\nuncertainty remains stable. Finally, we show that although the two uncertainty\ntypes may be redundant in low-uncertainty cases, they provide complementary\ninsights under more challenging conditions, underscoring the value of their\njoint modeling for robust wildfire danger prediction. In summary, our approach\nsignificantly improves the accuracy and reliability of wildfire danger\nforecasting, advancing the development of trustworthy wildfire DL systems.", "AI": {"tldr": "An uncertainty-aware deep learning framework that jointly models epistemic and aleatoric uncertainty for short-term wildfire danger forecasting, improving both predictive accuracy and reliability.", "motivation": "Wildfires pose severe threats, but deep learning adoption is hindered by reliability concerns due to lack of uncertainty quantification in predictions.", "method": "Developed an uncertainty-aware DL framework that captures both epistemic (model) and aleatoric (data) uncertainty for wildfire danger forecasting.", "result": "In next-day forecasting, improved F1 Score by 2.3% and reduced Expected Calibration Error by 2.1% compared to deterministic baseline. Uncertainty estimates proved reliable for decision support.", "conclusion": "Joint modeling of both uncertainty types provides complementary insights, significantly improving accuracy and reliability of wildfire danger forecasting for trustworthy DL systems."}}
{"id": "2509.25020", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25020", "abs": "https://arxiv.org/abs/2509.25020", "authors": ["Jiayu Liu", "Zhenya Huang", "Anya Sims", "Enhong Chen", "Yee Whye Teh", "Ning Miao"], "title": "MARCOS: Deep Thinking by Markov Chain of Continuous Thoughts", "comment": null, "summary": "The current paradigm for reasoning in large language models (LLMs) involves\nmodels \"thinking out loud\" via a sequence of tokens, known as chain-of-thought\n(CoT). This approach, while effective, has several significant drawbacks.\nFirstly, inference requires autoregressive generation of often thousands of CoT\ntokens, which is slow and computationally expensive. Secondly, it constrains\nreasoning to the discrete space of tokens, creating an information bottleneck\nacross reasoning steps. Thirdly, it fundamentally entangles reasoning with\ntoken generation, forcing LLMs to \"think while speaking,\" which causes\npotentially short-sighted reasoning. In light of these limitations, we\nre-imagine reasoning in LLMs and present a new paradigm: MARCOS. In our\napproach, rather than autoregressively generating tokens, we model reasoning as\na hidden Markov chain of continuous, high-dimensional \"thoughts\". Each\nreasoning step involves a transition of the internal thoughts, where explicit\nreasoning steps (which may consist of hundreds of tokens) serve as observable\nvariables, which are windows to peek into the implicit thoughts. Since this\nlatent process is incompatible with the standard supervised learning, we\nfurther propose a two-phase variational training scheme. Our experiments on\nthree benchmarks demonstrate that MARCOS outperforms existing continuous\nreasoning methods and, for the first time, achieves performance comparable to\ntoken-based CoT, even surpassing it by 4.7% on GSM8K with up to 15.7x speedup\nin inference. Beyond this, MARCOS offers additional advantages, such as\nstep-level instead of token-level control over randomness, opening significant\nopportunities for reinforcement learning and reasoning in LLMs.", "AI": {"tldr": "MARCOS proposes a new reasoning paradigm for LLMs that models reasoning as a hidden Markov chain of continuous thoughts instead of discrete token generation, achieving comparable performance to chain-of-thought with significant speedup.", "motivation": "Current chain-of-thought reasoning has three main drawbacks: slow and expensive inference due to autoregressive token generation, constrained reasoning in discrete token space creating information bottlenecks, and entangled reasoning with token generation causing potentially short-sighted reasoning.", "method": "Models reasoning as a hidden Markov chain of continuous, high-dimensional thoughts. Each reasoning step involves transitions of internal thoughts, with explicit reasoning steps serving as observable variables. Uses a two-phase variational training scheme since the latent process is incompatible with standard supervised learning.", "result": "Outperforms existing continuous reasoning methods and achieves performance comparable to token-based CoT, surpassing it by 4.7% on GSM8K with up to 15.7x speedup in inference. Also enables step-level control over randomness.", "conclusion": "MARCOS presents a viable alternative to chain-of-thought reasoning that addresses its fundamental limitations while maintaining performance and offering significant computational benefits and additional advantages for reinforcement learning and reasoning control."}}
{"id": "2509.25031", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25031", "abs": "https://arxiv.org/abs/2509.25031", "authors": ["Sophia V. Kuhn", "Rafael Bischof", "Marius Weber", "Antoine Binggeli", "Michael A. Kraus", "Walter Kaufmann", "Fernando P\u00e9rez-Cruz"], "title": "Bayesian Surrogates for Risk-Aware Pre-Assessment of Aging Bridge Portfolios", "comment": "Accepted at the NeurIPS 2025 Workshop on MLxOR: Mathematical\n  Foundations and Operational Integration of Machine Learning for\n  Uncertainty-Aware Decision-Making", "summary": "Aging infrastructure portfolios pose a critical resource allocation\nchallenge: deciding which structures require intervention and which can safely\nremain in service. Structural assessments must balance the trade-off between\ncheaper, conservative analysis methods and accurate but costly simulations that\ndo not scale portfolio-wide. We propose Bayesian neural network (BNN)\nsurrogates for rapid structural pre-assessment of worldwide common bridge\ntypes, such as reinforced concrete frame bridges. Trained on a large-scale\ndatabase of non-linear finite element analyses generated via a parametric\npipeline and developed based on the Swiss Federal Railway's bridge portfolio,\nthe models accurately and efficiently estimate high-fidelity structural\nanalysis results by predicting code compliance factors with calibrated\nepistemic uncertainty. Our BNN surrogate enables fast, uncertainty-aware\ntriage: flagging likely critical structures and providing guidance where\nrefined analysis is pertinent. We demonstrate the framework's effectiveness in\na real-world case study of a railway underpass, showing its potential to\nsignificantly reduce costs and emissions by avoiding unnecessary analyses and\nphysical interventions across entire infrastructure portfolios.", "AI": {"tldr": "BNN surrogates enable rapid structural pre-assessment of bridges by predicting code compliance factors with calibrated uncertainty, facilitating cost-effective portfolio-wide infrastructure management.", "motivation": "Aging infrastructure requires efficient resource allocation, balancing cheap conservative analysis vs accurate but costly simulations that don't scale portfolio-wide.", "method": "Bayesian neural network surrogates trained on large-scale nonlinear finite element analysis database from parametric pipeline, based on Swiss Federal Railway's bridge portfolio.", "result": "Models accurately estimate high-fidelity structural analysis results by predicting code compliance factors with calibrated epistemic uncertainty, enabling fast uncertainty-aware triage.", "conclusion": "The framework significantly reduces costs and emissions by avoiding unnecessary analyses and physical interventions across entire infrastructure portfolios, demonstrated in real-world railway underpass case study."}}
{"id": "2509.25040", "categories": ["cs.LG", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25040", "abs": "https://arxiv.org/abs/2509.25040", "authors": ["Giuseppe Bruno", "Federico Pasqualotto", "Andrea Agazzi"], "title": "A multiscale analysis of mean-field transformers in the moderate interaction regime", "comment": "30 pages, 4 figures", "summary": "In this paper, we study the evolution of tokens through the depth of\nencoder-only transformer models at inference time by modeling them as a system\nof particles interacting in a mean-field way and studying the corresponding\ndynamics. More specifically, we consider this problem in the moderate\ninteraction regime, where the number $N$ of tokens is large and the inverse\ntemperature parameter $\\beta$ of the model scales together with $N$. In this\nregime, the dynamics of the system displays a multiscale behavior: a fast\nphase, where the token empirical measure collapses on a low-dimensional space,\nan intermediate phase, where the measure further collapses into clusters, and a\nslow one, where such clusters sequentially merge into a single one. We provide\na rigorous characterization of the limiting dynamics in each of these phases\nand prove convergence in the above mentioned limit, exemplifying our results\nwith some simulations.", "AI": {"tldr": "This paper models token evolution in encoder-only transformers as interacting particles in a mean-field system, analyzing dynamics in the moderate interaction regime where token count and inverse temperature scale together.", "motivation": "To understand how tokens evolve through transformer encoder depth at inference time by applying statistical physics concepts to analyze token interactions and clustering behavior.", "method": "Model tokens as particles in a mean-field system, study dynamics in moderate interaction regime (large N tokens with \u03b2 scaling with N), and analyze multiscale behavior through rigorous mathematical characterization.", "result": "Identified three-phase multiscale dynamics: fast phase (token empirical measure collapses to low-dimensional space), intermediate phase (further collapse into clusters), and slow phase (sequential cluster merging into single cluster).", "conclusion": "The paper provides rigorous characterization of limiting dynamics in each phase and proves convergence in the moderate interaction limit, demonstrating the approach with simulations."}}
{"id": "2509.25049", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25049", "abs": "https://arxiv.org/abs/2509.25049", "authors": ["Bingrui Li", "Jiaxin Wen", "Zhanpeng Zhou", "Jun Zhu", "Jianfei Chen"], "title": "Efficient Hyperparameter Tuning via Trajectory Invariance Principle", "comment": null, "summary": "As hyperparameter tuning becomes increasingly costly at scale, efficient\ntuning methods are essential. Yet principles for guiding hyperparameter tuning\nremain limited. In this work, we seek to establish such principles by\nconsidering a broad range of hyperparameters, including batch size, learning\nrate, and weight decay. We identify a phenomenon we call trajectory invariance,\nwhere pre-training loss curves, gradient noise, and gradient norm exhibit\ninvariance--closely overlapping--with respect to a quantity that combines\nlearning rate and weight decay. This phenomenon effectively reduces the\noriginal two-dimensional hyperparameter space to one dimension, yielding an\nefficient tuning rule: follow the salient direction revealed by trajectory\ninvariance. Furthermore, we refine previous scaling laws and challenge several\nexisting viewpoints. Overall, our work proposes new principles for efficient\ntuning and inspires future research on scaling laws.", "AI": {"tldr": "The paper identifies trajectory invariance phenomenon that reduces 2D hyperparameter space to 1D, enabling efficient tuning by following the salient direction revealed by this invariance.", "motivation": "As hyperparameter tuning becomes increasingly costly at scale, there is a need for efficient tuning methods and principles to guide hyperparameter selection.", "method": "The authors identify trajectory invariance phenomenon where pre-training loss curves, gradient noise, and gradient norm exhibit invariance with respect to a combination of learning rate and weight decay.", "result": "Trajectory invariance effectively reduces the original two-dimensional hyperparameter space to one dimension, yielding an efficient tuning rule.", "conclusion": "The work proposes new principles for efficient hyperparameter tuning, refines previous scaling laws, challenges existing viewpoints, and inspires future research on scaling laws."}}
{"id": "2509.25050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25050", "abs": "https://arxiv.org/abs/2509.25050", "authors": ["Shuchen Xue", "Chongjian Ge", "Shilong Zhang", "Yichen Li", "Zhi-Ming Ma"], "title": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion Models", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing\nLarge Language Models (LLMs), where pre-training and RL post-training share the\nsame log-likelihood formulation. In contrast, recent RL approaches for\ndiffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),\noptimize an objective different from the pretraining objectives--score/flow\nmatching loss. In this work, we establish a novel theoretical analysis: DDPO is\nan implicit form of score/flow matching with noisy targets, which increases\nvariance and slows convergence. Building on this analysis, we introduce\n\\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for\ndiffusion. It uses the same score/flow-matching loss as pretraining to obtain a\nlower-variance objective and reweights each sample by its advantage. In effect,\nAWM raises the influence of high-reward samples and suppresses low-reward ones\nwhile keeping the modeling objective identical to pretraining. This unifies\npretraining and RL conceptually and practically, is consistent with\npolicy-gradient theory, reduces variance, and yields faster convergence. This\nsimple yet effective design yields substantial benefits: on GenEval, OCR, and\nPickScore benchmarks, AWM delivers up to a $24\\times$ speedup over Flow-GRPO\n(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,\nwithout compromising generation quality. Code is available at\nhttps://github.com/scxue/advantage_weighted_matching.", "AI": {"tldr": "AWM is a new RL method for diffusion models that uses the same score/flow-matching loss as pretraining, reducing variance and speeding up convergence compared to DDPO-based approaches.", "motivation": "Current RL methods for diffusion models like DDPO optimize different objectives than pretraining, causing increased variance and slower convergence. The authors aim to unify pretraining and RL objectives.", "method": "Advantage Weighted Matching (AWM) uses score/flow-matching loss identical to pretraining, reweighting samples by their advantage scores to emphasize high-reward samples.", "result": "AWM achieves up to 24\u00d7 speedup over Flow-GRPO on GenEval, OCR, and PickScore benchmarks with Stable Diffusion 3.5 Medium and FLUX, without quality loss.", "conclusion": "AWM successfully unifies pretraining and RL for diffusion models, providing faster convergence and lower variance while maintaining generation quality."}}
{"id": "2509.25080", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25080", "abs": "https://arxiv.org/abs/2509.25080", "authors": ["Bogdan Raoni\u0107", "Siddhartha Mishra", "Samuel Lanthaler"], "title": "Towards a Certificate of Trust: Task-Aware OOD Detection for Scientific AI", "comment": null, "summary": "Data-driven models are increasingly adopted in critical scientific fields\nlike weather forecasting and fluid dynamics. These methods can fail on\nout-of-distribution (OOD) data, but detecting such failures in regression tasks\nis an open challenge. We propose a new OOD detection method based on estimating\njoint likelihoods using a score-based diffusion model. This approach considers\nnot just the input but also the regression model's prediction, providing a\ntask-aware reliability score. Across numerous scientific datasets, including\nPDE datasets, satellite imagery and brain tumor segmentation, we show that this\nlikelihood strongly correlates with prediction error. Our work provides a\nfoundational step towards building a verifiable 'certificate of trust', thereby\noffering a practical tool for assessing the trustworthiness of AI-based\nscientific predictions. Our code is publicly available at\nhttps://github.com/bogdanraonic3/OOD_Detection_ScientificML", "AI": {"tldr": "Proposes a new OOD detection method using score-based diffusion models to estimate joint likelihoods of inputs and predictions, providing task-aware reliability scores for regression tasks in scientific ML.", "motivation": "Data-driven models in critical scientific fields like weather forecasting and fluid dynamics can fail on out-of-distribution data, but detecting such failures in regression tasks remains challenging.", "method": "Uses score-based diffusion models to estimate joint likelihoods considering both input data and regression model predictions, creating task-aware reliability scores for OOD detection.", "result": "Across multiple scientific datasets (PDE datasets, satellite imagery, brain tumor segmentation), the estimated likelihood strongly correlates with prediction error, demonstrating effective OOD detection.", "conclusion": "Provides a foundational step towards building verifiable 'certificates of trust' for AI-based scientific predictions, offering practical tools for assessing trustworthiness in critical applications."}}
{"id": "2509.25104", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25104", "abs": "https://arxiv.org/abs/2509.25104", "authors": ["Albert Vong", "Steven Henke", "Oliver Hoidn", "Hanna Ruth", "Junjing Deng", "Alexander Hexemer", "Apurva Mehta", "Arianna Gleason", "Levi Hancock", "Nicholas Schwarz"], "title": "Towards generalizable deep ptychography neural networks", "comment": "Submitted to scientific journal for peer review", "summary": "X-ray ptychography is a data-intensive imaging technique expected to become\nubiquitous at next-generation light sources delivering many-fold increases in\ncoherent flux. The need for real-time feedback under accelerated acquisition\nrates motivates surrogate reconstruction models like deep neural networks,\nwhich offer orders-of-magnitude speedup over conventional methods. However,\nexisting deep learning approaches lack robustness across diverse experimental\nconditions. We propose an unsupervised training workflow emphasizing probe\nlearning by combining experimentally-measured probes with synthetic,\nprocedurally generated objects. This probe-centric approach enables a single\nphysics-informed neural network to reconstruct unseen experiments across\nmultiple beamlines; among the first demonstrations of multi-probe\ngeneralization. We find probe learning is equally important as in-distribution\nlearning; models trained using this synthetic workflow achieve reconstruction\nfidelity comparable to those trained exclusively on experimental data, even\nwhen changing the type of synthetic training object. The proposed approach\nenables training of experiment-steering models that provide real-time feedback\nunder dynamic experimental conditions.", "AI": {"tldr": "Proposed an unsupervised training workflow for X-ray ptychography using probe learning with synthetic objects, enabling multi-probe generalization and real-time reconstruction across different beamlines.", "motivation": "X-ray ptychography needs real-time feedback at next-gen light sources, but existing deep learning approaches lack robustness across diverse experimental conditions.", "method": "Unsupervised training combining experimentally-measured probes with procedurally generated synthetic objects, emphasizing probe learning for physics-informed neural networks.", "result": "Achieved reconstruction fidelity comparable to experimental data training, demonstrated multi-probe generalization across multiple beamlines, with probe learning being equally important as in-distribution learning.", "conclusion": "The probe-centric synthetic workflow enables training of experiment-steering models that provide real-time feedback under dynamic experimental conditions."}}
{"id": "2509.25135", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25135", "abs": "https://arxiv.org/abs/2509.25135", "authors": ["Daniil Dmitriev", "Harald Eskelund Franck", "Carolin Heinzler", "Amartya Sanyal"], "title": "Learning in an Echo Chamber: Online Learning with Replay Adversary", "comment": null, "summary": "As machine learning systems increasingly train on self-annotated data, they\nrisk reinforcing errors and becoming echo chambers of their own beliefs. We\nmodel this phenomenon by introducing a learning-theoretic framework: Online\nLearning in the Replay Setting. In round $t$, the learner outputs a hypothesis\n$\\hat{h}_t$; the adversary then reveals either the true label $f^\\ast(x_t)$ or\na replayed label $\\hat{h}_i(x_t)$ from an earlier round $i < t$. A mistake is\ncounted only when the true label is shown, yet classical algorithms such as the\nSOA or the halving algorithm are easily misled by the replayed errors.\n  We introduce the Extended Threshold dimension, $\\mathrm{ExThD}(\\mathcal{H})$,\nand prove matching upper and lower bounds that make\n$\\mathrm{ExThD}(\\mathcal{H})$ the exact measure of learnability in this model.\nA closure-based learner makes at most $\\mathrm{ExThD}(\\mathcal{H})$ mistakes\nagainst any adaptive adversary, and no algorithm can perform better. For\nstochastic adversaries, we prove a similar bound for every intersection-closed\nclass. The replay setting is provably harder than the classical mistake bound\nsetting: some classes have constant Littlestone dimension but arbitrarily large\n$\\mathrm{ExThD}(\\mathcal{H})$. Proper learning exhibits an even sharper\nseparation: a class is properly learnable under replay if and only if it is\n(almost) intersection-closed. Otherwise, every proper learner suffers\n$\\Omega(T)$ errors, whereas our improper algorithm still achieves the\n$\\mathrm{ExThD}(\\mathcal{H})$ bound. These results give the first tight\nanalysis of learning against replay adversaries, based on new results for\nclosure-type algorithms.", "AI": {"tldr": "This paper introduces a learning-theoretic framework for Online Learning in the Replay Setting, where learners risk reinforcing their own errors when training on self-annotated data. The authors define the Extended Threshold dimension as the exact measure of learnability and prove matching upper and lower bounds.", "motivation": "As machine learning systems increasingly train on self-annotated data, they risk creating echo chambers that reinforce their own errors. This motivates studying learning in settings where models can be misled by their own previous predictions.", "method": "The authors introduce the Online Learning in the Replay Setting framework, where in each round the learner outputs a hypothesis and the adversary reveals either the true label or a replayed label from an earlier round. They define the Extended Threshold dimension and develop closure-based learners.", "result": "The paper proves that Extended Threshold dimension is the exact measure of learnability in this model, with matching upper and lower bounds. They show the replay setting is provably harder than classical mistake-bound learning, and proper learning requires intersection-closed classes.", "conclusion": "The replay setting presents fundamental challenges for machine learning systems using self-annotated data, with Extended Threshold dimension characterizing learnability. Proper learning is only possible for intersection-closed classes, while improper learning can achieve the Extended Threshold dimension bound."}}
{"id": "2509.25136", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25136", "abs": "https://arxiv.org/abs/2509.25136", "authors": ["David Gonz\u00e1lez Mart\u00ednez"], "title": "BALF: Budgeted Activation-Aware Low-Rank Factorization for Fine-Tuning-Free Model Compression", "comment": null, "summary": "Neural network compression techniques typically require expensive fine-tuning\nor search procedures, rendering them impractical on commodity hardware.\nInspired by recent LLM compression research, we present a general\nactivation-aware factorization framework that can be applied to a broad range\nof layers. Moreover, we introduce a scalable budgeted rank allocator that\nallows flexible control over compression targets (e.g., retaining 50% of\nparameters) with no overhead. Together, these components form BALF, an\nefficient pipeline for compressing models without fine-tuning. We demonstrate\nits effectiveness across multiple scales and architectures, from ResNet-20 on\nCIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it\nachieves excellent results in the fine-tuning-free regime. For instance, BALF\nreduces FLOPs on ResNeXt-101 by 45% with only a 1-percentage-point top-1\naccuracy drop.", "AI": {"tldr": "BALF is a fine-tuning-free neural network compression framework that uses activation-aware factorization and a scalable budgeted rank allocator to achieve efficient compression across various models.", "motivation": "Traditional neural network compression methods require expensive fine-tuning or search procedures, making them impractical on commodity hardware. The goal is to develop a compression approach that works without fine-tuning.", "method": "The method combines an activation-aware factorization framework applicable to various layers with a scalable budgeted rank allocator that enables flexible control over compression targets without overhead.", "result": "BALF achieves excellent results without fine-tuning, reducing FLOPs on ResNeXt-101 by 45% with only 1% top-1 accuracy drop. It works effectively across multiple scales and architectures including ResNet-20, ResNeXt-101, and vision transformers on CIFAR-10 and ImageNet datasets.", "conclusion": "BALF provides an efficient pipeline for model compression that eliminates the need for fine-tuning while maintaining competitive performance across diverse neural network architectures and scales."}}
{"id": "2509.25153", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25153", "abs": "https://arxiv.org/abs/2509.25153", "authors": ["Nicholas Barnfield", "Hugo Cui", "Yue M. Lu"], "title": "High-Dimensional Analysis of Single-Layer Attention for Sparse-Token Classification", "comment": null, "summary": "When and how can an attention mechanism learn to selectively attend to\ninformative tokens, thereby enabling detection of weak, rare, and sparsely\nlocated features? We address these questions theoretically in a sparse-token\nclassification model in which positive samples embed a weak signal vector in a\nrandomly chosen subset of tokens, whereas negative samples are pure noise. In\nthe long-sequence limit, we show that a simple single-layer attention\nclassifier can in principle achieve vanishing test error when the signal\nstrength grows only logarithmically in the sequence length $L$, whereas linear\nclassifiers require $\\sqrt{L}$ scaling. Moving from representational power to\nlearnability, we study training at finite $L$ in a high-dimensional regime,\nwhere sample size and embedding dimension grow proportionally. We prove that\njust two gradient updates suffice for the query weight vector of the attention\nclassifier to acquire a nontrivial alignment with the hidden signal, inducing\nan attention map that selectively amplifies informative tokens. We further\nderive an exact asymptotic expression for the test error and training loss of\nthe trained attention-based classifier, and quantify its capacity -- the\nlargest dataset size that is typically perfectly separable -- thereby\nexplaining the advantage of adaptive token selection over nonadaptive linear\nbaselines.", "AI": {"tldr": "Attention mechanisms can learn to selectively attend to informative tokens with weak signals, achieving better performance than linear classifiers in sparse-token classification tasks.", "motivation": "To understand when and how attention mechanisms can learn to selectively attend to informative tokens, especially for detecting weak, rare, and sparsely located features in classification tasks.", "method": "Theoretical analysis of a sparse-token classification model using a simple single-layer attention classifier, studying training in a high-dimensional regime with proportional growth of sample size and embedding dimension.", "result": "Attention classifiers can achieve vanishing test error with logarithmic signal strength scaling, outperform linear classifiers that require square root scaling, and can acquire nontrivial alignment with hidden signals in just two gradient updates.", "conclusion": "Attention mechanisms provide significant advantages over non-adaptive linear baselines through adaptive token selection, explaining their superior performance in detecting sparse weak features."}}
{"id": "2509.25157", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25157", "abs": "https://arxiv.org/abs/2509.25157", "authors": ["Jinhao Liang", "Yixuan Sun", "Anirban Samaddar", "Sandeep Madireddy", "Ferdinando Fioretto"], "title": "Chance-constrained Flow Matching for High-Fidelity Constraint-aware Generation", "comment": null, "summary": "Generative models excel at synthesizing high-fidelity samples from complex\ndata distributions, but they often violate hard constraints arising from\nphysical laws or task specifications. A common remedy is to project\nintermediate samples onto the feasible set; however, repeated projection can\ndistort the learned distribution and induce a mismatch with the data manifold.\nThus, recent multi-stage procedures attempt to defer projection to clean\nsamples during sampling, but they increase algorithmic complexity and\naccumulate errors across steps. This paper addresses these challenges by\nproposing a novel training-free method, Chance-constrained Flow Matching\n(CCFM), that integrates stochastic optimization into the sampling process,\nenabling effective enforcement of hard constraints while maintaining\nhigh-fidelity sample generation. Importantly, CCFM guarantees feasibility in\nthe same manner as conventional repeated projection, yet, despite operating\ndirectly on noisy intermediate samples, it is theoretically equivalent to\nprojecting onto the feasible set defined by clean samples. This yields a\nsampler that mitigates distributional distortion. Empirical experiments show\nthat CCFM outperforms current state-of-the-art constrained generative models in\nmodeling complex physical systems governed by partial differential equations\nand molecular docking problems, delivering higher feasibility and fidelity.", "AI": {"tldr": "CCFM is a training-free method that integrates stochastic optimization into sampling to enforce hard constraints while maintaining high-fidelity generation, outperforming current constrained generative models.", "motivation": "Generative models often violate hard constraints from physical laws or specifications, and existing projection methods can distort distributions or increase complexity.", "method": "Proposes Chance-constrained Flow Matching (CCFM) that uses stochastic optimization during sampling to enforce constraints while operating on noisy intermediate samples.", "result": "CCFM guarantees feasibility like conventional projection but avoids distributional distortion, achieving higher feasibility and fidelity in physical systems and molecular docking.", "conclusion": "CCFM effectively enforces hard constraints while maintaining sample quality, providing a superior approach for constrained generative modeling."}}
{"id": "2509.25158", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.25158", "abs": "https://arxiv.org/abs/2509.25158", "authors": ["Ehimare Okoyomon", "Arbel Yaniv", "Christoph Goebel"], "title": "Physics-Informed Inductive Biases for Voltage Prediction in Distribution Grids", "comment": null, "summary": "Voltage prediction in distribution grids is a critical yet difficult task for\nmaintaining power system stability. Machine learning approaches, particularly\nGraph Neural Networks (GNNs), offer significant speedups but suffer from poor\ngeneralization when trained on limited or incomplete data. In this work, we\nsystematically investigate the role of inductive biases in improving a model's\nability to reliably learn power flow. Specifically, we evaluate three\nphysics-informed strategies: (i) power-flow-constrained loss functions, (ii)\ncomplex-valued neural networks, and (iii) residual-based task reformulation.\nUsing the ENGAGE dataset, which spans multiple low- and medium-voltage grid\nconfigurations, we conduct controlled experiments to isolate the effect of each\ninductive bias and assess both standard predictive performance and\nout-of-distribution generalization. Our study provides practical insights into\nwhich model assumptions most effectively guide learning for reliable and\nefficient voltage prediction in modern distribution networks.", "AI": {"tldr": "This paper systematically investigates three physics-informed inductive biases (power-flow-constrained loss functions, complex-valued neural networks, and residual-based task reformulation) to improve Graph Neural Networks' generalization for voltage prediction in distribution grids.", "motivation": "Voltage prediction in distribution grids is critical for power system stability, but machine learning approaches like GNNs suffer from poor generalization when trained on limited or incomplete data.", "method": "The authors evaluate three physics-informed strategies using the ENGAGE dataset across multiple grid configurations: (i) power-flow-constrained loss functions, (ii) complex-valued neural networks, and (iii) residual-based task reformulation, with controlled experiments to isolate each bias's effect.", "result": "The study assesses both standard predictive performance and out-of-distribution generalization, providing practical insights into which model assumptions most effectively guide learning.", "conclusion": "The research offers insights into which physics-informed inductive biases most effectively improve reliable and efficient voltage prediction in modern distribution networks."}}
{"id": "2509.25170", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25170", "abs": "https://arxiv.org/abs/2509.25170", "authors": ["Peter Holderrieth", "Uriel Singer", "Tommi Jaakkola", "Ricky T. Q. Chen", "Yaron Lipman", "Brian Karrer"], "title": "GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion Models", "comment": null, "summary": "The performance of flow matching and diffusion models can be greatly improved\nat inference time using reward alignment algorithms, yet efficiency remains a\nmajor limitation. While several algorithms were proposed, we demonstrate that a\ncommon bottleneck is the sampling method these algorithms rely on: many\nalgorithms require to sample Markov transitions via SDE sampling, which is\nsignificantly less efficient and often less performant than ODE sampling. To\nremove this bottleneck, we introduce GLASS Flows, a new sampling paradigm that\nsimulates a \"flow matching model within a flow matching model\" to sample Markov\ntransitions. As we show in this work, this \"inner\" flow matching model can be\nretrieved from a pre-trained model without any re-training, combining the\nefficiency of ODEs with the stochastic evolution of SDEs. On large-scale\ntext-to-image models, we show that GLASS Flows eliminate the trade-off between\nstochastic evolution and efficiency. Combined with Feynman-Kac Steering, GLASS\nFlows improve state-of-the-art performance in text-to-image generation, making\nit a simple, drop-in solution for inference-time scaling of flow and diffusion\nmodels.", "AI": {"tldr": "GLASS Flows is a new sampling method that enables efficient Markov transitions in flow matching models by simulating a \"flow matching model within a flow matching model\", combining ODE efficiency with SDE stochasticity.", "motivation": "Current reward alignment algorithms for flow matching and diffusion models suffer from efficiency limitations due to their reliance on SDE sampling, which is slower and often less performant than ODE sampling.", "method": "GLASS Flows introduces a novel sampling paradigm that retrieves an \"inner\" flow matching model from pre-trained models without retraining, enabling efficient Markov transitions while maintaining stochastic evolution.", "result": "GLASS Flows eliminate the trade-off between stochastic evolution and efficiency in text-to-image generation, improving state-of-the-art performance when combined with Feynman-Kac Steering.", "conclusion": "GLASS Flows provides a simple, drop-in solution for efficient inference-time scaling of flow and diffusion models, combining the benefits of both ODE and SDE sampling approaches."}}
{"id": "2509.25171", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.25171", "abs": "https://arxiv.org/abs/2509.25171", "authors": ["Sophia Tang", "Yuchen Zhu", "Molei Tao", "Pranam Chatterjee"], "title": "TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion", "comment": null, "summary": "Reinforcement learning with stochastic optimal control offers a promising\nframework for diffusion fine-tuning, where a pre-trained diffusion model is\noptimized to generate paths that lead to a reward-tilted distribution. While\nthese approaches enable optimization without access to explicit samples from\nthe optimal distribution, they require training on rollouts under the current\nfine-tuned model, making them susceptible to reinforcing sub-optimal\ntrajectories that yield poor rewards. To overcome this challenge, we introduce\nTRee Search Guided TRajectory-Aware Fine-Tuning for Discrete Diffusion\n(TR2-D2), a novel framework that optimizes reward-guided discrete diffusion\ntrajectories with tree search to construct replay buffers for trajectory-aware\nfine-tuning. These buffers are generated using Monte Carlo Tree Search (MCTS)\nand subsequently used to fine-tune a pre-trained discrete diffusion model under\na stochastic optimal control objective. We validate our framework on single-\nand multi-objective fine-tuning of biological sequence diffusion models,\nhighlighting the overall effectiveness of TR2-D2 for reliable reward-guided\nfine-tuning in discrete sequence generation.", "AI": {"tldr": "TR2-D2 is a novel framework that combines tree search with trajectory-aware fine-tuning for discrete diffusion models, using MCTS to generate replay buffers for more reliable reward-guided optimization.", "motivation": "Existing reinforcement learning approaches for diffusion fine-tuning are susceptible to reinforcing sub-optimal trajectories that yield poor rewards, as they require training on rollouts under the current fine-tuned model.", "method": "The framework uses Monte Carlo Tree Search (MCTS) to construct replay buffers for trajectory-aware fine-tuning, then fine-tunes a pre-trained discrete diffusion model under a stochastic optimal control objective.", "result": "The method is validated on single- and multi-objective fine-tuning of biological sequence diffusion models, demonstrating overall effectiveness for reliable reward-guided fine-tuning.", "conclusion": "TR2-D2 provides an effective solution for optimizing reward-guided discrete diffusion trajectories by leveraging tree search to overcome limitations of traditional reinforcement learning approaches."}}
{"id": "2509.25174", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25174", "abs": "https://arxiv.org/abs/2509.25174", "authors": ["Daniel Palenicek", "Florian Vogt", "Joe Watson", "Ingmar Posner", "Jan Peters"], "title": "XQC: Well-conditioned Optimization Accelerates Deep Reinforcement Learning", "comment": null, "summary": "Sample efficiency is a central property of effective deep reinforcement\nlearning algorithms. Recent work has improved this through added complexity,\nsuch as larger models, exotic network architectures, and more complex\nalgorithms, which are typically motivated purely by empirical performance. We\ntake a more principled approach by focusing on the optimization landscape of\nthe critic network. Using the eigenspectrum and condition number of the\ncritic's Hessian, we systematically investigate the impact of common\narchitectural design decisions on training dynamics. Our analysis reveals that\na novel combination of batch normalization (BN), weight normalization (WN), and\na distributional cross-entropy (CE) loss produces condition numbers orders of\nmagnitude smaller than baselines. This combination also naturally bounds\ngradient norms, a property critical for maintaining a stable effective learning\nrate under non-stationary targets and bootstrapping. Based on these insights,\nwe introduce XQC: a well-motivated, sample-efficient deep actor-critic\nalgorithm built upon soft actor-critic that embodies these optimization-aware\nprinciples. We achieve state-of-the-art sample efficiency across 55\nproprioception and 15 vision-based continuous control tasks, all while using\nsignificantly fewer parameters than competing methods.", "AI": {"tldr": "XQC is a sample-efficient deep actor-critic algorithm that improves training dynamics by optimizing the critic network's Hessian condition number through batch normalization, weight normalization, and distributional cross-entropy loss.", "motivation": "To improve sample efficiency in deep reinforcement learning through principled optimization landscape analysis rather than empirical complexity additions, focusing on the critic network's training dynamics.", "method": "Systematic analysis of critic network optimization using Hessian eigenspectrum and condition number; combination of batch normalization, weight normalization, and distributional cross-entropy loss; built upon soft actor-critic framework.", "result": "Achieved orders of magnitude smaller condition numbers than baselines; state-of-the-art sample efficiency on 55 proprioception and 15 vision-based continuous control tasks; used significantly fewer parameters than competing methods.", "conclusion": "Optimization-aware architectural design decisions can dramatically improve sample efficiency in deep reinforcement learning while reducing model complexity."}}
{"id": "2501.05601", "categories": ["cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2501.05601", "abs": "https://arxiv.org/abs/2501.05601", "authors": ["Adrian Marius Dumitran", "Adrian-Catalin Badea", "Stefan-Gabriel Muscalu", "Angela-Liliana Dumitran", "Stefan-Cosmin Dascalescu", "Radu-Sebastian Amarie"], "title": "Exploring Large Language Models for Translating Romanian Computational Problems into English", "comment": "12 pages", "summary": "Recent studies have suggested that large language models (LLMs) underperform\non mathematical and computer science tasks when these problems are translated\nfrom Romanian into English, compared to their original Romanian format.\nAccurate translation is critical for applications ranging from automatic\ntranslations in programming competitions to the creation of high-quality\neducational materials, as well as minimizing errors or fraud in human\ntranslations. This study shows that robust large language models (LLMs) can\nmaintain or even enhance their performance in translating less common languages\nwhen given well-structured prompts. Our findings suggest that LLMs, with\nappropriate supervision, can be reliably used for the automatic translation of\nIOI (International Olympiad in Informatics)-style tasks. We evaluate several\ntranslation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B,\nLlama 3.2 3B and GPT-4o, assessing their translation accuracy and performance\nstability through repeated runs. Additionally, we augment the OJI (Romanian\nCounty-Level Informatics Olympiad) Romanian dataset with accurate English\ntranslations, enhancing its utility for future LLM training and evaluation.\nThrough detailed syntactic and semantic analyses, we confirm that with human\noversight, LLMs can serve as a viable solution for multilingual\nproblem-solving. We also compare the translation quality of LLMs against human\ntranslators, as evaluated by a certified expert, underscoring the potential of\nLLMs in realworld scenarios.", "AI": {"tldr": "LLMs can maintain or enhance performance in translating less common languages like Romanian to English for IOI-style tasks when given structured prompts and human oversight.", "motivation": "Address the performance gap of LLMs on mathematical/CS tasks when translated from Romanian to English, and explore reliable automatic translation for programming competitions and educational materials.", "method": "Evaluated multiple LLMs (OpenRoLLM, Llama 3.1 8B, Llama 3.2 3B, GPT-4o) using various translation methods, assessed accuracy and stability through repeated runs, and conducted syntactic/semantic analyses with human oversight.", "result": "LLMs with appropriate supervision can reliably translate IOI-style tasks, maintaining or enhancing performance. Human-LLM translations were comparable to human-only translations as evaluated by certified experts.", "conclusion": "LLMs with human oversight serve as viable solutions for multilingual problem-solving and can be reliably used for automatic translation of technical content in real-world scenarios."}}
{"id": "2506.05991", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.05991", "abs": "https://arxiv.org/abs/2506.05991", "authors": ["Alexandru-Gabriel Ganea", "Antonia-Adelina Popovici", "Adrian-Marius Dumitran"], "title": "A Culturally-Rich Romanian NLP Dataset from \"Who Wants to Be a Millionaire?\" Videos", "comment": "10 pages", "summary": "Large Language Models (LLMs) demonstrate varying performance across languages\nand cultural contexts. This study introduces a novel, culturally-rich,\nmultilingual dataset derived from video recordings of the Romanian game show\n\"Who Wants to Be a Millionaire?\" (Vrei s\\u{a} fii Milionar?). We employed an\ninnovative process combining optical character recognition (OCR), automated\ntext extraction, and manual verification to collect question-answer pairs,\nenriching them with metadata including question domain (e.g., biology,\nhistory), cultural relevance (Romanian-specific vs. international), and\ndifficulty. Benchmarking state-of-the-art LLMs, including Romanian-adapted\nmodels, on this dataset revealed significant performance disparities: models\nconsistently achieve higher accuracy (80-95%) on international questions\ncompared to Romanian-specific cultural questions (50-75%). We further\ninvestigate these differences through experiments involving machine translation\nof Romanian questions into English and cross-lingual tests using a comparable\ndataset in French. Our findings underscore the impact of cultural context and\ndata source on LLM performance and offer practical insights for building\nrobust, culturally-aware multilingual NLP systems, especially in educational\ndomains. The dataset is publicly available at Hugging Face.", "AI": {"tldr": "This paper introduces a culturally-rich multilingual dataset from Romanian game show \"Who Wants to Be a Millionaire?\" and benchmarks LLMs, revealing significant performance gaps between international (80-95% accuracy) and Romanian-specific cultural questions (50-75% accuracy).", "motivation": "To address the varying performance of LLMs across languages and cultural contexts, particularly for culturally-specific content that current models struggle with.", "method": "Used OCR, automated text extraction, and manual verification to collect question-answer pairs from Romanian game show, enriched with metadata (domain, cultural relevance, difficulty). Tested state-of-the-art LLMs including Romanian-adapted models, and conducted cross-lingual experiments with machine translation and French dataset comparisons.", "result": "LLMs show significant performance disparities: 80-95% accuracy on international questions vs 50-75% on Romanian-specific cultural questions. Cross-lingual experiments confirmed cultural context impact persists even with translation.", "conclusion": "Cultural context and data source significantly impact LLM performance, highlighting the need for building robust, culturally-aware multilingual NLP systems, especially in educational domains."}}
{"id": "2506.22694", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22694", "abs": "https://arxiv.org/abs/2506.22694", "authors": ["Raghavv Goel", "Sudhanshu Agrawal", "Mukul Gagrani", "Junyoung Park", "Yifan Zao", "He Zhang", "Tian Liu", "Yiping Yang", "Xin Yuan", "Jiuyan Lu", "Chris Lott", "Mingu Lee"], "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs", "comment": "8 pages, 4 figures, 5 tables, accepted at ICML 2025 workshop on\n  Efficient Systems for Foundational Models", "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.", "AI": {"tldr": "VocabTrim is a training-free technique that improves speculative decoding speed by reducing drafter vocabulary size, achieving 16% speed-up for Llama-3.2-3B-Instruct.", "motivation": "Current speculative decoding methods have unnecessary inference overhead due to large vocabulary sizes in drafters, especially problematic for memory-bound environments like edge devices.", "method": "Reconstruct drafter LM head to contain only frequently sampled tokens from target model vocabulary, reducing drafting latency while slightly impacting acceptance rate.", "result": "16% memory-bound speed-up improvement for Llama-3.2-3B-Instruct on Spec-Bench, with reduced drafting latency outweighing minor acceptance rate degradation.", "conclusion": "VocabTrim effectively improves speculative decoding speed in memory-bound scenarios by strategically limiting drafter vocabulary size."}}
{"id": "2508.14279", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14279", "abs": "https://arxiv.org/abs/2508.14279", "authors": ["Adrian-Marius Dumitran", "Alexandra-Mihaela Danila", "Angela-Liliana Dumitran"], "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs", "comment": "Accepted as long paper @RANLP2025", "summary": "LLMs (Large language models) have revolutionized NLP (Natural Language\nProcessing), yet their pedagogical value for low-resource languages remains\nunclear. We present GRILE (Grammar Romanian Inference and Language\nExplanations) , the first open benchmark of 1,151 multiple-choice questions\nharvested from Romanian high-stakes exams (National Evaluation, Baccalaureate,\nuniversity admissions). GRILE enables us to probe two complementary abilities\nof seven state-of-the-art multilingual and Romanian-specific LLMs: (i)\nselecting the correct answer, and (ii) producing linguistically accurate\nexplanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight\nmodels stay below 65%, and 48% of their explanations contain factual or\npedagogical flaws according to expert review. A detailed error analysis\npinpoints systematic weaknesses in morphology and in applying the latest DOOM3\northographic norms. All data, code and a public web demo are released to\ncatalyze future research. Our findings expose open challenges for trustworthy\neducational NLP in low-resource settings and establish GRILE as a new test-bed\nfor controllable explanation generation and evaluation.", "AI": {"tldr": "GRILE is the first open benchmark for Romanian language testing, showing LLMs struggle with accuracy and explanations in low-resource settings.", "motivation": "To evaluate the pedagogical value of LLMs for low-resource languages like Romanian, where their capabilities remain unclear.", "method": "Created GRILE benchmark with 1,151 multiple-choice questions from Romanian exams, tested 7 multilingual and Romanian-specific LLMs on answer selection and explanation generation.", "result": "Gemini 2.5 Pro achieved 83% accuracy, but most open-weight models stayed below 65%, with 48% of explanations containing factual or pedagogical flaws.", "conclusion": "LLMs face significant challenges in trustworthy educational NLP for low-resource languages, and GRILE serves as a valuable test-bed for future research."}}
