<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 105]
- [cs.CR](#cs.CR) [Total: 23]
- [cs.LG](#cs.LG) [Total: 101]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [OpenStaxQA: A multilingual dataset based on open-source college textbooks](https://arxiv.org/abs/2510.06239)
*Pranav Gupta*

Main category: cs.CL

TL;DR: OpenStaxQA is a college-level educational benchmark using 43 open-source textbooks in English, Spanish, and Polish. LLMs with ~7B parameters are fine-tuned using QLoRa and evaluated, with additional zero-shot testing on AI2 reasoning challenge to assess transfer learning.


<details>
  <summary>Details</summary>
Motivation: To create a specialized evaluation benchmark for college-level educational applications using open-source textbooks, addressing the need for domain-specific educational AI evaluation.

Method: Developed OpenStaxQA benchmark from 43 Creative Commons licensed textbooks in three languages. Fine-tuned ~7B parameter LLMs using quantized low rank adapters (QLoRa), and performed zero-shot evaluation on AI2 reasoning challenge dataset.

Result: The paper presents the OpenStaxQA benchmark and demonstrates fine-tuning of LLMs on this educational dataset, with additional evaluation showing potential for improved performance on other reasoning tasks.

Conclusion: OpenStaxQA provides a valuable educational benchmark for evaluating AI systems, and the authors discuss broader impacts of such educational datasets for AI development and deployment.

Abstract: We present OpenStaxQA, an evaluation benchmark specific to college-level
educational applications based on 43 open-source college textbooks in English,
Spanish, and Polish, available under a permissive Creative Commons license. We
finetune and evaluate large language models (LLMs) with approximately 7 billion
parameters on this dataset using quantized low rank adapters (QLoRa).
Additionally we also perform a zero-shot evaluation on the AI2 reasoning
challenge dev dataset in order to check if OpenStaxQA can lead to an improved
performance on other tasks. We also discuss broader impacts relevant to
datasets such as OpenStaxQA.

</details>


### [2] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan,Zhenke Duan,Jiani Tu,Anzhi Cheng,Yanqing Wang*

Main category: cs.CL

TL;DR: KG-MASD is a distillation method that transfers multi-agent reasoning capabilities to lightweight models using knowledge graphs for verification, improving industrial QA accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Industrial QA systems need higher safety than general dialogue models, but multi-agent LLMs have uncontrolled iterations and unverifiable outputs, while conventional distillation fails to transfer collaborative reasoning to deployable models.

Method: Formulates distillation as Markov Decision Process, incorporates knowledge graph as verifiable structured prior to enrich state representation and ensure convergence, integrates collaborative reasoning with knowledge grounding.

Result: Improves accuracy by 2.4% to 20.1% over baselines on industrial QA dataset, significantly enhances reliability for safety-critical deployment.

Conclusion: KG-MASD enables trustworthy AI deployment in safety-critical industrial scenarios by distilling reasoning depth and verifiability into compact student models.

Abstract: Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.

</details>


### [3] [Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses](https://arxiv.org/abs/2510.06242)
*Subin An,Yugyeong Ji,Junyoung Kim,Heejin Kook,Yang Lu,Josh Seltzer*

Main category: cs.CL

TL;DR: Proposes a two-stage evaluation framework for human survey responses with gibberish filtering and three quality dimensions (effort, relevance, completeness) using LLMs, outperforming existing metrics.


<details>
  <summary>Details</summary>
Motivation: Open-ended survey responses provide valuable insights but low-quality responses burden researchers and risk misleading conclusions. Existing automatic evaluation methods target LLM-generated text and inadequately assess human-written responses.

Method: Two-stage evaluation framework: 1) gibberish filtering removes nonsensical responses, 2) three dimensions (effort, relevance, completeness) evaluated using LLM capabilities, grounded in empirical analysis of real-world survey data.

Result: Validation on English and Korean datasets shows the framework outperforms existing metrics and demonstrates high practical applicability for response quality prediction and response rejection, with strong correlations with expert assessment.

Conclusion: The proposed framework effectively addresses the distinct characteristics of human survey responses and provides reliable automatic evaluation for real-world marketing research applications.

Abstract: Open-ended survey responses provide valuable insights in marketing research,
but low-quality responses not only burden researchers with manual filtering but
also risk leading to misleading conclusions, underscoring the need for
effective evaluation. Existing automatic evaluation methods target
LLM-generated text and inadequately assess human-written responses with their
distinct characteristics. To address such characteristics, we propose a
two-stage evaluation framework specifically designed for human survey
responses. First, gibberish filtering removes nonsensical responses. Then,
three dimensions-effort, relevance, and completeness-are evaluated using LLM
capabilities, grounded in empirical analysis of real-world survey data.
Validation on English and Korean datasets shows that our framework not only
outperforms existing metrics but also demonstrates high practical applicability
for real-world applications such as response quality prediction and response
rejection, showing strong correlations with expert assessment.

</details>


### [4] [CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning](https://arxiv.org/abs/2510.06243)
*Qihua Dong,Luis Figueroa,Handong Zhao,Kushal Kafle,Jason Kuen,Zhihong Ding,Scott Cohen,Yun Fu*

Main category: cs.CL

TL;DR: CoT Referring enhances MLLMs by using chain-of-thought training to parse textual structures into sequential referring steps, improving accuracy in complex queries through better relationship identification and reference alignment.


<details>
  <summary>Details</summary>
Motivation: To improve multimodal language understanding by addressing challenges in Referring Expression Comprehension and Segmentation, which serve as benchmarks for MLLM capabilities.

Method: Restructure training data with chain-of-thought annotations, integrate detection and segmentation into unified MLLM framework, and use adaptive weighted loss for optimization.

Result: Achieved 2.5%+ improvement over baseline models on curated benchmark and RefCOCO/+/g datasets.

Conclusion: CoT Referring strategy effectively enhances multimodal reasoning through structured chain-of-thought training and unified framework integration.

Abstract: Referring Expression Comprehension and Segmentation are critical tasks for
assessing the integration of language understanding and image comprehension,
serving as benchmarks for Multimodal Large Language Models (MLLMs)
capabilities. To address these challenges, we propose a new strategy, CoT
Referring, which enhances model reasoning across modalities through a
structured, chain-of-thought training data structure. Our approach
systematically parses textual structures to a sequential referring step, where
in each step it identifies relationships and ensures consistent reference
alignment, thereby improving accuracy in complex query scenarios. We
restructure the training data to enforce a new output form, providing new
annotations for existing datasets and compiling an evaluation benchmark from
existing resources. This benchmark is designed explicitly for complex referring
cases. We also integrate detection and segmentation capabilities into a unified
MLLM framework, training it with a novel adaptive weighted loss to optimize
performance. Experimental results on our curated benchmark and RefCOCO/+/g
demonstrate the effectiveness of our approach, with a notable increase of 2.5%+
over baseline models.

</details>


### [5] [Evaluating Embedding Frameworks for Scientific Domain](https://arxiv.org/abs/2510.06244)
*Nouman Ahmed,Ronin Wu,Victor Botev*

Main category: cs.CL

TL;DR: This paper focuses on finding optimal word representation algorithms and tokenization methods for scientific domain NLP, and building a comprehensive evaluation suite to test various algorithms.


<details>
  <summary>Details</summary>
Motivation: The same word can have different meanings in different domains, and while transformer models generate good contextual embeddings, they are computationally expensive to pre-train from scratch, especially for scientific domains.

Method: Built an evaluation suite with multiple downstream tasks and relevant datasets, then used this suite to test various word representation and tokenization algorithms.

Result: The paper developed a comprehensive evaluation framework for scientific domain NLP and tested multiple word representation and tokenization methods using this framework.

Conclusion: The research provides both optimal word representation/tokenization methods for scientific NLP tasks and a reusable evaluation suite for future algorithm comparisons in the scientific domain.

Abstract: Finding an optimal word representation algorithm is particularly important in
terms of domain specific data, as the same word can have different meanings and
hence, different representations depending on the domain and context. While
Generative AI and transformer architecture does a great job at generating
contextualized embeddings for any given work, they are quite time and compute
extensive, especially if we were to pre-train such a model from scratch. In
this work, we focus on the scientific domain and finding the optimal word
representation algorithm along with the tokenization method that could be used
to represent words in the scientific domain. The goal of this research is two
fold: 1) finding the optimal word representation and tokenization methods that
can be used in downstream scientific domain NLP tasks, and 2) building a
comprehensive evaluation suite that could be used to evaluate various word
representation and tokenization algorithms (even as new ones are introduced) in
the scientific domain. To this end, we build an evaluation suite consisting of
several downstream tasks and relevant datasets for each task. Furthermore, we
use the constructed evaluation suite to test various word representation and
tokenization algorithms.

</details>


### [6] [TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B](https://arxiv.org/abs/2510.06249)
*Toshiki Nakai,Ravi Kiran Chikkala,Lena Sophie Oberkircher,Nicholas Jennings,Natalia Skachkova,Tatiana Anikina,Jesujoba Oluwadara Alabi*

Main category: cs.CL

TL;DR: TRepLiNa method combines CKA and REPINA to improve low-resource language translation by enforcing cross-lingual similarity in mid-level layers of multilingual LLMs.


<details>
  <summary>Details</summary>
Motivation: Address the linguistic gap for India's diverse low-resource languages (LRLs) by improving translation quality from LRL to high-resource languages.

Method: Combine Centered Kernel Alignment (CKA) with REPINA regularization into TRepLiNa, applied to Aya-23 8B with QLoRA in zero-shot, few-shot, and fine-tuning settings across Mundari, Santali, Bhili language pairs.

Result: Aligning mid-level layers using TRepLiNa improves LRL translation quality, especially in data-scarce settings, demonstrating a low-cost practical approach.

Conclusion: TRepLiNa (CKA+REPINA) is an effective method for enhancing low-resource language translation by enforcing cross-lingual similarity in specific model layers.

Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact
(MMLoSo) Language Challenge addresses one of India's most pressing linguistic
gaps: the lack of resources for its diverse low-resource languages (LRLs). In
this study, we investigate whether enforcing cross-lingual similarity in
specific internal layers of a decoder-only multilingual large language model
(LLM) can improve translation quality from LRL to high-resource language (HRL).
Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric
that encourages representations of different languages to align, with REPINA, a
regularization method that constrains parameter updates to remain close to the
pretrained model, into a joint method we call TRepLiNa. In this research
project, we experiment with zero-shot, few-shot, and fine-tuning settings using
Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,
Santali, Bhili) with Hindi/English pivots. Our results show that aligning
mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach
to improving LRL translation, especially in data-scarce settings.

</details>


### [7] [Scalable multilingual PII annotation for responsible AI in LLMs](https://arxiv.org/abs/2510.06250)
*Bharti Meena,Joanna Skubisz,Harshit Rajgarhia,Nand Dave,Kiran Ganesh,Shivali Dalmia,Abhishek Mukherji,Vasudevan Sundarababu,Olga Pospelova*

Main category: cs.CL

TL;DR: A scalable multilingual data curation framework for high-quality PII annotation across 13 underrepresented locales, covering 336 locale-specific PII types, using human-in-the-loop methodology to improve recall and reduce false positives.


<details>
  <summary>Details</summary>
Motivation: As LLMs gain wider adoption, ensuring reliable handling of Personally Identifiable Information (PII) across diverse regulatory contexts has become essential.

Method: Phased human-in-the-loop annotation methodology combining linguistic expertise with rigorous quality assurance, leveraging inter-annotator agreement metrics and root-cause analysis to resolve annotation inconsistencies.

Result: Substantial improvements in recall and false positive rates from pilot, training, and production phases, resulting in high-fidelity datasets suitable for supervised LLM fine-tuning.

Conclusion: Iterative, analytics-driven pipelines can enhance both annotation quality and downstream model reliability, addressing common annotator challenges in multilingual PII labeling.

Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable
handling of Personally Identifiable Information (PII) across diverse regulatory
contexts has become essential. This work introduces a scalable multilingual
data curation framework designed for high-quality PII annotation across 13
underrepresented locales, covering approximately 336 locale-specific PII types.
Our phased, human-in-the-loop annotation methodology combines linguistic
expertise with rigorous quality assurance, leading to substantial improvements
in recall and false positive rates from pilot, training, and production phases.
By leveraging inter-annotator agreement metrics and root-cause analysis, the
framework systematically uncovers and resolves annotation inconsistencies,
resulting in high-fidelity datasets suitable for supervised LLM fine-tuning.
Beyond reporting empirical gains, we highlight common annotator challenges in
multilingual PII labeling and demonstrate how iterative, analytics-driven
pipelines can enhance both annotation quality and downstream model reliability.

</details>


### [8] [Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments](https://arxiv.org/abs/2510.06262)
*Aryan Kumar Singh,Janvi Singh*

Main category: cs.CL

TL;DR: A bilingual English-Hindi Prakriti Assessment Questionnaire dataset with 24 items evaluating Ayurvedic physical, physiological, and psychological traits, following AYUSH/CCRAS guidelines with automated dosha scoring.


<details>
  <summary>Details</summary>
Motivation: To create a standardized dataset for computational intelligence research in Ayurvedic studies and personalized health analytics, supporting trait analysis and predictive modeling.

Method: Developed 24 mandatory multiple-choice questions following AYUSH/CCRAS guidelines, collected via Google Forms with hidden dosha labels and automated response scoring.

Result: Created a structured dataset enabling analysis of trait distributions, correlations, and mapping individual characteristics to dosha-specific scores (Vata, Pitta, Kapha).

Conclusion: The dataset serves as a valuable platform for computational Ayurvedic research, personalized health applications, and future Prakriti-based studies.

Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi)
Prakriti Assessment Questionnaire designed to evaluate the physical,
physiological, and psychological characteristics of individuals according to
classical Ayurvedic principles. The questionnaire consists of 24
multiple-choice items covering body features, appetite, sleep patterns, energy
levels, and temperament. It was developed following AYUSH/CCRAS guidelines to
ensure comprehensive and accurate data collection. All questions are mandatory
and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)
are hidden from participants. Data were collected via a Google Forms
deployment, enabling automated scoring of responses to map individual traits to
dosha-specific scores. The resulting dataset provides a structured platform for
research in computational intelligence, Ayurvedic studies, and personalized
health analytics, supporting analysis of trait distributions, correlations, and
predictive modeling. It can also serve as a reference for future Prakriti-based
studies and the development of intelligent health applications.

</details>


### [9] [Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians](https://arxiv.org/abs/2510.06263)
*Jiajun Wu,Swaleh Zaidi,Braden Teitge,Henry Leung,Jiayu Zhou,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: A two-stage offline EHR summarization system using dual Jetson Nano devices for privacy-preserving clinical data processing, achieving useful summaries in under 30 seconds.


<details>
  <summary>Details</summary>
Motivation: Emergency physicians are overwhelmed by extensive unstructured clinical data in EHRs, needing quick access to critical information while maintaining patient privacy.

Method: Dual-device architecture: Jetson Nano-R retrieves relevant EHR sections, Jetson Nano-S generates structured summaries using small language models, with LLM-as-Judge evaluation for quality assessment.

Result: Preliminary results on MIMIC-IV and real EHRs show the system effectively produces useful clinical summaries in under 30 seconds while operating fully offline.

Conclusion: The proposed embedded system enables privacy-preserving, efficient clinical summarization that can assist emergency physicians in quickly identifying critical patient information.

Abstract: Electronic health records (EHRs) contain extensive unstructured clinical data
that can overwhelm emergency physicians trying to identify critical
information. We present a two-stage summarization system that runs entirely on
embedded devices, enabling offline clinical summarization while preserving
patient privacy. In our approach, a dual-device architecture first retrieves
relevant patient record sections using the Jetson Nano-R (Retrieve), then
generates a structured summary on another Jetson Nano-S (Summarize),
communicating via a lightweight socket link. The summarization output is
two-fold: (1) a fixed-format list of critical findings, and (2) a
context-specific narrative focused on the clinician's query. The retrieval
stage uses locally stored EHRs, splits long notes into semantically coherent
sections, and searches for the most relevant sections per query. The generation
stage uses a locally hosted small language model (SLM) to produce the summary
from the retrieved text, operating within the constraints of two NVIDIA Jetson
devices. We first benchmarked six open-source SLMs under 7B parameters to
identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to
assess summary quality in terms of factual accuracy, completeness, and clarity.
Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that
our fully offline system can effectively produce useful summaries in under 30
seconds.

</details>


### [10] [A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation](https://arxiv.org/abs/2510.06265)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

TL;DR: This survey provides a comprehensive review of hallucination in large language models (LLMs), covering causes, detection methods, and mitigation strategies for the generation of factually inaccurate content.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce fluent but factually incorrect information (hallucinations), which undermines their reliability and trustworthiness, especially in domains requiring factual accuracy.

Method: The survey presents taxonomies of hallucination types and root causes across the LLM development lifecycle, examines detection approaches and mitigation strategies, and reviews evaluation benchmarks and metrics.

Result: The paper provides a structured framework for understanding, detecting, and mitigating hallucinations in LLMs, analyzing the strengths and limitations of current approaches.

Conclusion: The survey outlines key open challenges and promising directions for future research to develop more truthful and trustworthy LLMs.

Abstract: Large language models (LLMs) have transformed natural language processing,
achieving remarkable performance across diverse tasks. However, their
impressive fluency often comes at the cost of producing false or fabricated
information, a phenomenon known as hallucination. Hallucination refers to the
generation of content by an LLM that is fluent and syntactically correct but
factually inaccurate or unsupported by external evidence. Hallucinations
undermine the reliability and trustworthiness of LLMs, especially in domains
requiring factual accuracy. This survey provides a comprehensive review of
research on hallucination in LLMs, with a focus on causes, detection, and
mitigation. We first present a taxonomy of hallucination types and analyze
their root causes across the entire LLM development lifecycle, from data
collection and architecture design to inference. We further examine how
hallucinations emerge in key natural language generation tasks. Building on
this foundation, we introduce a structured taxonomy of detection approaches and
another taxonomy of mitigation strategies. We also analyze the strengths and
limitations of current detection and mitigation approaches and review existing
evaluation benchmarks and metrics used to quantify LLMs hallucinations.
Finally, we outline key open challenges and promising directions for future
research, providing a foundation for the development of more truthful and
trustworthy LLMs.

</details>


### [11] [Language models for longitudinal analysis of abusive content in Billboard Music Charts](https://arxiv.org/abs/2510.06266)
*Rohitash Chandra,Yathin Suresh,Divyansh Raj Sinha,Sanchit Jindal*

Main category: cs.CL

TL;DR: Deep learning analysis of Billboard songs from 1950s-2020s shows significant increase in explicit content (profanity, sexual themes) since 1990, reflecting changing societal norms.


<details>
  <summary>Details</summary>
Motivation: Lack of validated studies on increasing abusive/sexually explicit content in popular music despite its harmful effects on children and youth, hindering effective policy development.

Method: Used deep learning and language models to analyze lyrics from Billboard Charts over 7 decades, employing sentiment analysis and abuse detection for sexually explicit content.

Result: Significant rise in explicit content in popular music from 1990 onwards, with increasing prevalence of profane, sexually explicit, and inappropriate language in lyrics.

Conclusion: Language models effectively capture nuanced patterns in lyrical content evolution, reflecting shifts in societal norms and language use over time, providing evidence for policy considerations.

Abstract: There is no doubt that there has been a drastic increase in abusive and
sexually explicit content in music, particularly in Billboard Music Charts.
However, there is a lack of studies that validate the trend for effective
policy development, as such content has harmful behavioural changes in children
and youths. In this study, we utilise deep learning methods to analyse songs
(lyrics) from Billboard Charts of the United States in the last seven decades.
We provide a longitudinal study using deep learning and language models and
review the evolution of content using sentiment analysis and abuse detection,
including sexually explicit content. Our results show a significant rise in
explicit content in popular music from 1990 onwards. Furthermore, we find an
increasing prevalence of songs with lyrics containing profane, sexually
explicit, and otherwise inappropriate language. The longitudinal analysis of
the ability of language models to capture nuanced patterns in lyrical content,
reflecting shifts in societal norms and language use over time.

</details>


### [12] [Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"](https://arxiv.org/abs/2510.06275)
*Ranjan Mishra,Julian I. Bibo,Quinten van Engelen,Henk Schaapman*

Main category: cs.CL

TL;DR: This paper reproduces XRec framework for explainable recommendations using Llama 3 instead of GPT-3.5-turbo, finding that XRec effectively generates personalized explanations but doesn't consistently outperform all baselines. The study also analyzes the role of Mixture of Experts embeddings.


<details>
  <summary>Details</summary>
Motivation: To replicate and extend the original XRec paper by testing with different LLM (Llama 3) and analyzing the impact of Mixture of Experts module embeddings on explanation generation.

Method: Built on original XRec source code, used Llama 3 for evaluation, modified input embeddings and deleted output embeddings of the Mixture of Experts module to study their effects.

Result: XRec effectively generates personalized explanations and stability improves with collaborative information, but doesn't consistently outperform all baseline models. Mixture of Experts embeddings significantly shape explanation structures.

Conclusion: The reproduction study validates XRec's effectiveness while providing insights into how collaborative signals interact with language modeling through Mixture of Experts embeddings, with open-source implementation for broader accessibility.

Abstract: In this study, we reproduced the work done in the paper "XRec: Large Language
Models for Explainable Recommendation" by Ma et al. (2024). The original
authors introduced XRec, a model-agnostic collaborative instruction-tuning
framework that enables large language models (LLMs) to provide users with
comprehensive explanations of generated recommendations. Our objective was to
replicate the results of the original paper, albeit using Llama 3 as the LLM
for evaluation instead of GPT-3.5-turbo. We built on the source code provided
by Ma et al. (2024) to achieve our goal. Our work extends the original paper by
modifying the input embeddings or deleting the output embeddings of XRec's
Mixture of Experts module. Based on our results, XRec effectively generates
personalized explanations and its stability is improved by incorporating
collaborative information. However, XRec did not consistently outperform all
baseline models in every metric. Our extended analysis further highlights the
importance of the Mixture of Experts embeddings in shaping the explanation
structures, showcasing how collaborative signals interact with language
modeling. Through our work, we provide an open-source evaluation implementation
that enhances accessibility for researchers and practitioners alike. Our
complete code repository can be found at
https://github.com/julianbibo/xrec-reproducibility.

</details>


### [13] [Type and Complexity Signals in Multilingual Question Representations](https://arxiv.org/abs/2510.06304)
*Robin Kokot,Wessel Poelman*

Main category: cs.CL

TL;DR: This paper investigates how multilingual transformer models represent morphosyntactic properties of questions using a new QTC dataset across 7 languages, comparing neural probes against statistical baselines to understand when contextual representations outperform traditional methods.


<details>
  <summary>Details</summary>
Motivation: To understand how multilingual transformer models encode morphosyntactic properties of questions and determine when contextual representations provide advantages over statistical baselines for capturing linguistic complexity.

Method: Created QTC dataset with question sentences in 7 languages annotated with type and complexity metrics; used layer-wise probing on frozen Glot500-m representations with regression labels and selectivity controls; compared against subword TF-IDF baselines and fine-tuned models.

Result: Statistical features effectively classify questions in languages with explicit marking, while neural probes better capture fine-grained structural complexity patterns; contextual representations outperform statistical baselines for capturing structural complexity.

Conclusion: The study provides insights into when contextual representations excel over statistical methods and evaluates whether parameter updates reduce the availability of pre-trained linguistic information in transformer models.

Abstract: This work investigates how a multilingual transformer model represents
morphosyntactic properties of questions. We introduce the Question Type and
Complexity (QTC) dataset with sentences across seven languages, annotated with
type information and complexity metrics including dependency length, tree
depth, and lexical density. Our evaluation extends probing methods to
regression labels with selectivity controls to quantify gains in
generalizability. We compare layer-wise probes on frozen Glot500-m (Imani et
al., 2023) representations against subword TF-IDF baselines, and a fine-tuned
model. Results show that statistical features classify questions effectively in
languages with explicit marking, while neural probes capture fine-grained
structural complexity patterns better. We use these results to evaluate when
contextual representations outperform statistical baselines and whether
parameter updates reduce the availability of pre-trained linguistic
information.

</details>


### [14] [LLM Bias Detection and Mitigation through the Lens of Desired Distributions](https://arxiv.org/abs/2510.06354)
*Ingroj Shrestha,Padmini Srinivasan*

Main category: cs.CL

TL;DR: This paper proposes a weighted adaptive loss fine-tuning method to align LLM gender-profession outputs with desired distributions (equal or real-world), achieving significant bias reduction while preserving language modeling capability.


<details>
  <summary>Details</summary>
Motivation: Prior bias mitigation work focused on social equality, but less attention was given to aligning LLM outputs with desired distributions like real-world data for factual grounding. The paper defines bias as deviation from a desired distribution.

Method: Proposed a weighted adaptive loss based fine-tuning method that aligns LLM's gender-profession output distribution with desired distributions. Used 3 profession sets (male/female-dominated, gender-balanced) from US labor statistics (2024) to assess both adaptive (reality) and non-adaptive (equality) variants.

Result: Across three masked language models, bias was observed under both distributions. Achieved near-complete mitigation under equality and 30-75% reduction under real-world settings. Autoregressive LLMs showed no bias under equality but notable bias under real-world settings, with Llama Instruct models achieving 50-62% reduction.

Conclusion: The proposed weighted adaptive loss fine-tuning effectively reduces bias in LLM gender-profession outputs for both equality and real-world distribution goals while maintaining language modeling performance.

Abstract: Although prior work on bias mitigation has focused on promoting social
equality and demographic parity, less attention has been given to aligning
LLM's outputs to desired distributions. For example, we might want to align a
model with real-world distributions to support factual grounding. Thus, we
define bias as deviation from a desired distribution, which may be an equal or
real-world distribution, depending on application goals. We propose a weighted
adaptive loss based fine-tuning method that aligns LLM's gender-profession
output distribution with the desired distribution, while preserving language
modeling capability. Using 3 profession sets -- male-dominated,
female-dominated, and gender-balanced -- derived from U.S. labor statistics
(2024), we assess both our adaptive method for reflecting reality and a
non-adaptive variant for equality. Across three masked language models, bias is
observed under both distributions. We achieve near-complete mitigation under
equality and 30-75% reduction under real-world settings. Autoregressive LLMs
show no bias under equality but notable bias under real-world settings, with
the Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50-62% reduction.

</details>


### [15] [EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference](https://arxiv.org/abs/2510.06370)
*Kshitish Ghate,Andy Liu,Devansh Jain,Taylor Sorensen,Atoosa Kasirzadeh,Aylin Caliskan,Mona T. Diab,Maarten Sap*

Main category: cs.CL

TL;DR: EVALUESTEER is a benchmark for evaluating LLMs' and reward models' ability to align with diverse user value and style preferences, showing current models struggle with complex preference profiles.


<details>
  <summary>Details</summary>
Motivation: To address the need for pluralistic AI systems that can accommodate diverse global user preferences and values, and fill the gap in existing datasets for controlled evaluation of reward model steering.

Method: Created 165,888 synthetic preference pairs systematically varying 4 value dimensions (traditional, secular-rational, survival, self-expression) and 4 style dimensions (verbosity, readability, confidence, warmth), then evaluated 6 LLMs/RMs under 16 prompting conditions and 6 preference scenarios.

Result: Best models achieved <75% accuracy when given full user profiles, compared to >99% accuracy when only relevant preferences were provided, highlighting limitations in identifying and adapting to relevant user information.

Conclusion: EVALUESTEER reveals current RMs' limitations in steering towards diverse human values and preferences, providing a challenging testbed for developing more adaptable pluralistic AI systems.

Abstract: As large language models (LLMs) are deployed globally, creating pluralistic
systems that can accommodate the diverse preferences and values of users
worldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure
LLMs' and reward models' (RMs) steerability towards users' value and stylistic
preference profiles grounded in psychology and human-LLM interaction
literature. To address the gap in existing datasets that do not support
controlled evaluations of RM steering, we synthetically generated 165,888
preference pairs -- systematically varying pairs along 4 value dimensions
(traditional, secular-rational, survival, and self-expression) and 4 style
dimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER
to evaluate whether, given a user profile and a pair of candidate value-laden
and style-laden responses, LLMs and RMs are able to select the output that
aligns with the user's preferences. We evaluate six open-source and proprietary
LLMs and RMs under sixteen systematic prompting conditions and six preference
comparison scenarios. Notably, our results show that, when given the user's
full profile of values and stylistic preferences, the best models achieve <75%
accuracy at choosing the correct response, in contrast to >99% accuracy when
only relevant style and value preferences are provided. EVALUESTEER thus
highlights the limitations of current RMs at identifying and adapting to
relevant user profile information, and provides a challenging testbed for
developing RMs that can be steered towards diverse human values and
preferences.

</details>


### [16] [EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA](https://arxiv.org/abs/2510.06371)
*Firoj Alam,Ali Ezzat Shahroor,Md. Arid Hasan,Zien Sheikh Ali,Hunzalah Hassan Bhatti,Mohamed Bayan Kmainasi,Shammur Absar Chowdhury,Basel Mousi,Fahim Dalvi,Nadir Durrani,Natasa Milic-Frayling*

Main category: cs.CL

TL;DR: The paper introduces EverydayMMQA framework and OASIS dataset to address cultural and linguistic gaps in multimodal AI, focusing on everyday knowledge in underrepresented languages like Arabic varieties.


<details>
  <summary>Details</summary>
Motivation: Large multimodal models fail on culturally grounded everyday knowledge tasks, especially in low-resource languages, creating a need for culturally-aware benchmarks.

Method: Developed EverydayMMQA framework to create culturally-grounded datasets, resulting in OASIS dataset with 0.92M images, 14.8M QA pairs, and 3.7M spoken questions across speech, text, and image modalities.

Result: Created comprehensive multimodal dataset covering 18 countries with English and Arabic varieties, enabling testing of pragmatic, commonsense, and cultural reasoning beyond object recognition.

Conclusion: EverydayMMQA and OASIS provide essential benchmarks and training data for developing culturally-aware multimodal LLMs, addressing gaps in underrepresented languages and everyday knowledge.

Abstract: Large-scale multimodal models achieve strong results on tasks like Visual
Question Answering (VQA), but they often fail when queries require culturally
grounded, everyday knowledge, particularly in low-resource and underrepresented
languages. To bridge this gap, we introduce Everyday Multimodal and
Multilingual QA (EverydayMMQA), a framework for creating large-scale,
culturally-grounded datasets for spoken and visual question answering (SVQA).
Using this framework, we developed OASIS, a multimodal dataset integrating
speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS
contains 3.7M spoken questions, enabling four unique input combinations:
speech-only, text-only, speech+image, and text+image. Focused on English and
Arabic varieties, 18 countries, the dataset content is curated to reflect
diverse, real-world situations. OASIS tests models on tasks beyond object
recognition that involve pragmatic, commonsense, and culturally aware
reasoning. We benchmarked four closed-source models, three open-source models,
and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark
and training dataset for building multimodal LLMs for a comprehensive set of
everyday tasks within cultural contexts. The framework and dataset will be made
publicly available to the community.

</details>


### [17] [Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language](https://arxiv.org/abs/2510.06378)
*Angie Boggust,Donghao Ren,Yannick Assogba,Dominik Moritz,Arvind Satyanarayan,Fred Hohman*

Main category: cs.CL

TL;DR: The paper introduces semantic regexes as structured language descriptions for LLM features, addressing the vagueness and inconsistency of natural language descriptions in automated interpretability.


<details>
  <summary>Details</summary>
Motivation: Natural language descriptions of LLM features are often vague, inconsistent, and require manual relabeling, which limits the effectiveness of automated interpretability methods.

Method: Semantic regexes combine linguistic and semantic feature pattern primitives with modifiers for contextualization, composition, and quantification to create precise and expressive feature descriptions.

Result: Semantic regexes match natural language accuracy while providing more concise and consistent descriptions. They enable new analyses like quantifying feature complexity across layers and scaling interpretability from individual features to model-wide patterns.

Conclusion: Semantic regexes help people build accurate mental models of LLM feature activations and provide a more structured approach to automated interpretability that overcomes limitations of natural language descriptions.

Abstract: Automated interpretability aims to translate large language model (LLM)
features into human understandable descriptions. However, these natural
language feature descriptions are often vague, inconsistent, and require manual
relabeling. In response, we introduce semantic regexes, structured language
descriptions of LLM features. By combining primitives that capture linguistic
and semantic feature patterns with modifiers for contextualization,
composition, and quantification, semantic regexes produce precise and
expressive feature descriptions. Across quantitative benchmarks and qualitative
analyses, we find that semantic regexes match the accuracy of natural language
while yielding more concise and consistent feature descriptions. Moreover,
their inherent structure affords new types of analyses, including quantifying
feature complexity across layers, scaling automated interpretability from
insights into individual features to model-wide patterns. Finally, in user
studies, we find that semantic regex descriptions help people build accurate
mental models of LLM feature activations.

</details>


### [18] [Protecting De-identified Documents from Search-based Linkage Attacks](https://arxiv.org/abs/2510.06383)
*Pierre Lison,Mark Anderson*

Main category: cs.CL

TL;DR: This paper presents a method to prevent search-based linkage attacks on de-identified documents by identifying rare N-grams and using LLM-based rewriting to eliminate linkage risks while preserving semantic integrity.


<details>
  <summary>Details</summary>
Motivation: Current de-identification models fail to address linkage risks, where de-identified text can be mapped back to its original source through search-based attacks using extracted phrases.

Method: Two-step approach: 1) Build inverted index of N-grams to identify those appearing in fewer than k documents, 2) Use iterative LLM-based rewriting to reformulate these rare spans until linkage is impossible.

Result: Experimental evaluation on court case documents shows the method effectively prevents search-based linkages while maintaining faithfulness to original content.

Conclusion: The proposed approach successfully counters search-based linkage attacks by systematically identifying and rewriting rare N-grams, providing enhanced privacy protection beyond basic de-identification.

Abstract: While de-identification models can help conceal the identity of the
individual(s) mentioned in a document, they fail to address linkage risks,
defined as the potential to map the de-identified text back to its source. One
straightforward way to perform such linkages is to extract phrases from the
de-identified document and then check their presence in the original dataset.
This paper presents a method to counter search-based linkage attacks while
preserving the semantic integrity of the text. The method proceeds in two
steps. We first construct an inverted index of the N-grams occurring in the
document collection, making it possible to efficiently determine which N-grams
appear in less than $k$ documents (either alone or in combination with other
N-grams). An LLM-based rewriter is then iteratively queried to reformulate
those spans until linkage is no longer possible. Experimental results on a
collection of court cases show that the method is able to effectively prevent
search-based linkages while remaining faithful to the original content.

</details>


### [19] [Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion](https://arxiv.org/abs/2510.06386)
*Fan Zhou,Chang Tian,Tim Van de Cruys*

Main category: cs.CL

TL;DR: RegDiff is a regularized diffusion framework for controllable text generation that uses attribute features without needing pretrained classifiers during sampling, achieving better attribute control with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion methods for text generation have limitations: classifier-free guidance preserves semantics but lacks effective attribute control, while classifier guidance enables better attribute alignment but has high computational costs and classifier generalization issues.

Method: RegDiff employs a VAE-based encoder-decoder architecture for reconstruction fidelity and a latent diffusion model trained with attribute supervision. Attribute information is injected only during training, eliminating the need for classifiers during sampling.

Result: Experiments on five datasets across multiple stylistic attributes show that RegDiff outperforms strong baselines in generating stylistic texts.

Conclusion: RegDiff provides an efficient solution for attribute-controllable text diffusion by leveraging attribute features during training without requiring classifiers during sampling, achieving both computational efficiency and effective attribute control.

Abstract: Generating stylistic text with specific attributes is a key problem in
controllable text generation. Recently, diffusion models have emerged as a
powerful paradigm for both visual and textual generation. Existing approaches
can be broadly categorized into classifier-free guidance (CFG) and classifier
guidance (CG) methods. While CFG effectively preserves semantic content, it
often fails to provide effective attribute control. In contrast, CG modifies
the denoising trajectory using classifier gradients, enabling better attribute
alignment but incurring high computational costs during sampling and suffering
from classifier generalization issues. In this work, we propose RegDiff, a
regularized diffusion framework that leverages attribute features without
requiring a pretrained classifier during sampling, thereby achieving
controllable generation with reduced computational costs. Specifically, RegDiff
employs a VAE-based encoder--decoder architecture to ensure reconstruction
fidelity and a latent diffusion model trained with attribute supervision to
enable controllable text generation. Attribute information is injected only
during training. Experiments on five datasets spanning multiple stylistic
attributes demonstrate that RegDiff outperforms strong baselines in generating
stylistic texts. These results validate the effectiveness of RegDiff as an
efficient solution for attribute-controllable text diffusion. Our code,
datasets, and resources will be released upon publication at
https://github.com/xxxx.

</details>


### [20] [Reward Model Perspectives: Whose Opinions Do Reward Models Reward?](https://arxiv.org/abs/2510.06391)
*Elle*

Main category: cs.CL

TL;DR: This paper analyzes reward models (RMs) in language model alignment, finding they exhibit sociodemographic biases, poorly align with various demographic groups, and systematically reward harmful stereotypes. Steering techniques alone are insufficient to overcome these limitations.


<details>
  <summary>Details</summary>
Motivation: To understand reward model behavior and investigate the extent to which RMs demonstrate sociodemographic biases, as RMs serve as proxies for human preferences but our understanding of their behavior is limited.

Method: The authors formalize a framework for measuring RM opinion alignment, investigate sociodemographic biases in RMs, and explore the effects of prompting to steer rewards toward target group preferences, focusing on controversial topics to quantify RM perspectives.

Result: RMs are poorly aligned with several demographic groups, systematically reward harmful stereotypes, and steering alone is insufficient to overcome these limitations.

Conclusion: More careful consideration of RM behavior in model alignment during preference learning is needed to prevent propagation of unwanted social biases in language technologies.

Abstract: Reward models (RMs) are central to the alignment of language models (LMs). An
RM often serves as a proxy for human preferences to guide downstream LM
behavior. However, our understanding of RM behavior is limited. Our work (i)
formalizes a framework for measuring the alignment of opinions captured by RMs,
(ii) investigates the extent to which RMs demonstrate sociodemographic biases,
and (iii) explores the effects of prompting to steer rewards towards the
preferences of a target group. We study the subjective and diverse perspectives
on controversial topics, which allows us to quantify RM perspectives in terms
of their opinions, attitudes, and values. We show that RMs are poorly aligned
with several demographic groups and can systematically reward harmful
stereotypes, and steering alone is not enough to overcome these limitations.
Our findings underscore the need for more careful consideration of RM behavior
in model alignment during preference learning to prevent the propagation of
unwanted social biases in the language technologies that we use.

</details>


### [21] [Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?](https://arxiv.org/abs/2510.06411)
*R. Alexander Knipper,Indrani Dey,Souvika Sarkar,Hari Narayanan,Sadhana Puntambekar,Santu Karmaker*

Main category: cs.CL

TL;DR: A framework using LLMs to help teachers generate instructional goal-aligned questions for virtual labs through natural language interaction, improving question quality and format adherence.


<details>
  <summary>Details</summary>
Motivation: Teachers struggle to adapt virtual labs to their instructional goals, and developing custom resources is time-consuming. LLMs offer a promising solution to address these limitations.

Method: An alignment framework with four components: instructional goal understanding via teacher-LLM dialogue, lab understanding via knowledge analysis, question taxonomy for structuring intent, and TELeR taxonomy for prompt control.

Result: The framework improved cognitive demand (0.29-0.39 quality points increase), achieved 80% parsability and >90% format adherence. Larger models showed strongest gains: +37.1% parsability, +25.7% adherence, and +0.8 quality points.

Conclusion: The proposed framework successfully enables teachers to leverage LLMs for generating pedagogically meaningful, simulation-aligned questions through natural language interaction, with larger models yielding the best performance.

Abstract: Virtual Labs offer valuable opportunities for hands-on, inquiry-based science
learning, yet teachers often struggle to adapt them to fit their instructional
goals. Third-party materials may not align with classroom needs, and developing
custom resources can be time-consuming and difficult to scale. Recent advances
in Large Language Models (LLMs) offer a promising avenue for addressing these
limitations. In this paper, we introduce a novel alignment framework for
instructional goal-aligned question generation, enabling teachers to leverage
LLMs to produce simulation-aligned, pedagogically meaningful questions through
natural language interaction. The framework integrates four components:
instructional goal understanding via teacher-LLM dialogue, lab understanding
via knowledge unit and relationship analysis, a question taxonomy for
structuring cognitive and pedagogical intent, and the TELeR taxonomy for
controlling prompt detail. Early design choices were informed by a small
teacher-assisted case study, while our final evaluation analyzed over 1,100
questions from 19 open-source LLMs. With goal and lab understanding grounding
questions in teacher intent and simulation context, the question taxonomy
elevates cognitive demand (open-ended formats and relational types raise
quality by 0.29-0.39 points), and optimized TELeR prompts enhance format
adherence (80% parsability, >90% adherence). Larger models yield the strongest
gains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert
points.

</details>


### [22] [FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering](https://arxiv.org/abs/2510.06426)
*Yitao Long,Tiansheng Hu,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.CL

TL;DR: FinLFQA is a benchmark for evaluating LLMs' ability to generate long-form financial answers with reliable attribution, assessing evidence extraction, numerical reasoning, and domain knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on simple attribution with textual evidence, but real-world financial applications require more nuanced attribution including numerical reasoning and domain knowledge.

Method: Introduces FinLFQA benchmark with human annotations for three attribution aspects: supporting evidence from financial reports, intermediate numerical reasoning steps, and domain-specific financial knowledge. Provides automatic evaluation framework for answer and attribution quality.

Result: Experiments on eight LLMs show fine-grained metrics distinguish model capabilities, end-to-end generation matches post-hoc approaches, and iterative refinement only helps with external feedback.

Conclusion: Financial attribution requires comprehensive evaluation beyond simple reference retrieval, and the proposed benchmark enables better assessment of LLMs' attribution capabilities in complex domains.

Abstract: Large Language Models (LLMs) frequently hallucinate to long-form questions,
producing plausible yet factually incorrect answers. A common mitigation
strategy is to provide attribution to LLM outputs. However, existing benchmarks
primarily focus on simple attribution that retrieves supporting textual
evidence as references. We argue that in real-world scenarios such as financial
applications, attribution goes beyond reference retrieval. We introduce
FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate
long-form answers to complex financial questions with reliable and nuanced
attributions. FinLFQA evaluates three critical aspects of attribution through
human annotations: (1) supporting evidence extracted from financial reports,
(2) intermediate numerical reasoning steps, and (3) domain-specific financial
knowledge that informs the reasoning process. We further provide an automatic
evaluation framework covering both answer quality and attribution quality.
Through extensive experiments on eight LLMs across multiple
attribution-generation paradigms, we find that fine-grained metrics are
important to distinguish model capabilities, that end-to-end generation
achieves comparable performance to post-hoc approaches, and that iterative
refinement only helps when guided by external feedback.

</details>


### [23] [Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser](https://arxiv.org/abs/2510.06427)
*Elena Chistova*

Main category: cs.CL

TL;DR: UniRST is the first unified RST-style discourse parser that handles 18 treebanks across 11 languages without modifying relation inventories, using parameter-efficient training strategies that outperform most mono-treebank baselines.


<details>
  <summary>Details</summary>
Motivation: To overcome inventory incompatibilities across different RST treebanks and enable unified multilingual discourse parsing with a single model.

Method: Proposed two training strategies: Multi-Head (separate relation classification per inventory) and Masked-Union (shared parameter training with selective label masking), plus augmentation for low-resource settings.

Result: Masked-Union approach is both parameter-efficient and strongest performer; UniRST outperforms 16 of 18 mono-treebank baselines.

Conclusion: A single unified model for multilingual end-to-end discourse parsing is advantageous across diverse resources, with the Masked-Union strategy being particularly effective.

Abstract: We introduce UniRST, the first unified RST-style discourse parser capable of
handling 18 treebanks in 11 languages without modifying their relation
inventories. To overcome inventory incompatibilities, we propose and evaluate
two training strategies: Multi-Head, which assigns separate relation
classification layer per inventory, and Masked-Union, which enables shared
parameter training through selective label masking. We first benchmark
monotreebank parsing with a simple yet effective augmentation technique for
low-resource settings. We then train a unified model and show that (1) the
parameter efficient Masked-Union approach is also the strongest, and (2) UniRST
outperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a
single-model, multilingual end-to-end discourse parsing across diverse
resources.

</details>


### [24] [MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning](https://arxiv.org/abs/2510.06430)
*Neeraja Kirtane,Yuvraj Khanna,Peter Relan*

Main category: cs.CL

TL;DR: The paper introduces MathRobust-LV, a test set for evaluating LLMs' math reasoning robustness to linguistic variations in high school-level problems, showing accuracy declines when problems are rephrased while keeping numerical structure constant.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored robustness of LLMs' math reasoning to linguistic variations in real educational settings, where instructors frequently rephrase problems while maintaining difficulty.

Method: Created MathRobust-LV test set by changing surface details (names, contexts, variables) while preserving numerical structure and answers, then evaluated 34 models on baseline vs. variant problems.

Result: Accuracy declines when moving from baseline to variants, with severe drops for smaller models (9-11%) and measurable degradation for stronger models, though frontier models like GPT-5 and Gemini-2.5pro remain relatively stable.

Conclusion: Robustness to linguistic variation is a fundamental challenge that exposes reasoning vulnerabilities in LLMs, even when benchmark performance appears saturated.

Abstract: Large language models excel on math benchmarks, but their math reasoning
robustness to linguistic variation is underexplored. While recent work
increasingly treats high-difficulty competitions like the IMO as the gold
standard for evaluating reasoning, we believe in comprehensive benchmarking of
high school-level math problems in real educational settings. We introduce
MathRobust-LV, a test set and evaluation methodology that mirrors how
instructors rephrase problems across assessments while keeping difficulty
constant: we change surface details (names, contexts, variables) while
preserving numerical structure and answers. In contrast to prior efforts that
alter problem content or emphasize IMO-level tasks, we focus on
high-school-level dataset problems at the difficulty level where models are
currently deployed in educational settings: tutoring and assessment systems. In
these applications, instructors rephrase identical concepts in varied ways,
making linguistic robustness essential for reliable deployment. Although MATH
data benchmarking is often regarded as saturated, our experiment on 34 models
reveals that accuracy declines when moving from the baseline to the variants.
These drops are severe for smaller models (9-11%) while stronger models also
show measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain
comparatively stable. Our results highlight that robustness to linguistic
variation is a fundamental challenge, exposing reasoning vulnerabilities in
models.

</details>


### [25] [A Survey on Agentic Security: Applications, Threats and Defenses](https://arxiv.org/abs/2510.06445)
*Asif Shahriar,Md Nafiu Rahman,Sadif Ahmed,Farig Sadeque,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: This paper presents the first comprehensive survey of security risks in autonomous LLM-agents, categorizing the field into Applications, Threats, and Defenses with analysis of over 150 papers.


<details>
  <summary>Details</summary>
Motivation: The transition from passive LLMs to autonomous LLM-agents introduces new security vulnerabilities in the agentic context that need systematic analysis.

Method: The authors conduct a holistic survey and provide a comprehensive taxonomy of over 150 papers, structuring the analysis around three pillars: Applications, Threats, and Defenses.

Result: The survey reveals emerging trends in agent architecture and identifies critical research gaps in model and modality coverage through detailed cross-cutting analysis.

Conclusion: This work establishes a foundational framework for understanding the security landscape of autonomous LLM-agents and highlights areas requiring further research attention.

Abstract: The rapid shift from passive LLMs to autonomous LLM-agents marks a new
paradigm in cybersecurity. While these agents can act as powerful tools for
both offensive and defensive operations, the very agentic context introduces a
new class of inherent security risks. In this work we present the first
holistic survey of the agentic security landscape, structuring the field around
three interdependent pillars: Applications, Threats, and Defenses. We provide a
comprehensive taxonomy of over 150 papers, explaining how agents are used, the
vulnerabilities they possess, and the countermeasures designed to protect them.
A detailed cross-cutting analysis shows emerging trends in agent architecture
while revealing critical research gaps in model and modality coverage.

</details>


### [26] [Linguistically Informed Tokenization Improves ASR for Underresourced Languages](https://arxiv.org/abs/2510.06461)
*Massimo Daul,Alessio Tosolini,Claire Bowern*

Main category: cs.CL

TL;DR: Fine-tuning wav2vec2 ASR model on Yan-nhangu language shows phonemic tokenization outperforms orthographic tokenization, and ASR-assisted transcription is faster than manual transcription for underresourced languages.


<details>
  <summary>Details</summary>
Motivation: Modern ASR systems require large datasets, making them unsuitable for underresourced languages. This research explores ASR's viability for language documentation of dormant Indigenous languages.

Method: Fine-tuned wav2vec2 ASR model on Yan-nhangu language, comparing phonemic vs orthographic tokenization strategies, and evaluated ASR as a tool in language documentation pipeline.

Result: Phonemic tokenization significantly improved Word Error Rate (WER) and Character Error Rate (CER) compared to orthographic tokenization. Hand-correcting ASR output was much faster than manual transcription.

Conclusion: ASR can be effectively used for underresourced languages when employing linguistically informed tokenization strategies, providing efficient tools for language documentation.

Abstract: Automatic speech recognition (ASR) is a crucial tool for linguists aiming to
perform a variety of language documentation tasks. However, modern ASR systems
use data-hungry transformer architectures, rendering them generally unusable
for underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu,
a dormant Indigenous Australian language, comparing the effects of phonemic and
orthographic tokenization strategies on performance. In parallel, we explore
ASR's viability as a tool in a language documentation pipeline. We find that a
linguistically informed phonemic tokenization system substantially improves WER
and CER compared to a baseline orthographic tokenization scheme. Finally, we
show that hand-correcting the output of an ASR model is much faster than
hand-transcribing audio from scratch, demonstrating that ASR can work for
underresourced languages.

</details>


### [27] [Test-Time Scaling of Reasoning Models for Machine Translation](https://arxiv.org/abs/2510.06471)
*Zihao Li,Shaoxiong Ji,Jörg Tiedemann*

Main category: cs.CL

TL;DR: Test-time scaling (TTS) shows limited benefits for direct machine translation with general-purpose models but becomes effective when combined with domain-specific fine-tuning or used in post-editing workflows.


<details>
  <summary>Details</summary>
Motivation: To investigate whether increased inference-time computation through test-time scaling improves translation quality in machine translation, given its proven effectiveness in other reasoning tasks.

Method: Evaluated 12 reasoning models across diverse MT benchmarks using three scenarios: direct translation, forced-reasoning extrapolation, and post-editing, analyzing the effects of TTS under different conditions.

Result: TTS provides limited and inconsistent benefits for direct translation with general models, but domain-specific fine-tuning unlocks consistent improvements up to optimal reasoning depth. Forced reasoning beyond natural stopping point degrades quality, while post-editing with TTS reliably improves translations.

Conclusion: The value of inference-time computation in MT lies in targeted applications like multi-step self-correction workflows and task-specialized models, rather than enhancing single-pass translation with general models.

Abstract: Test-time scaling (TTS) has enhanced the performance of Reasoning Models
(RMs) on various tasks such as math and coding, yet its efficacy in machine
translation (MT) remains underexplored. This paper investigates whether
increased inference-time computation improves translation quality. We evaluate
12 RMs across a diverse suite of MT benchmarks spanning multiple domains,
examining three scenarios: direct translation, forced-reasoning extrapolation,
and post-editing. Our findings show that for general-purpose RMs, TTS provides
limited and inconsistent benefits for direct translation, with performance
quickly plateauing. However, the effectiveness of TTS is unlocked by
domain-specific fine-tuning, which aligns a model's reasoning process with task
requirements, leading to consistent improvements up to an optimal,
self-determined reasoning depth. We also find that forcing a model to reason
beyond its natural stopping point consistently degrades translation quality. In
contrast, TTS proves highly effective in a post-editing context, reliably
turning self-correction into a beneficial process. These results indicate that
the value of inference-time computation in MT lies not in enhancing single-pass
translation with general models, but in targeted applications like multi-step,
self-correction workflows and in conjunction with task-specialized models.

</details>


### [28] [Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels](https://arxiv.org/abs/2510.06499)
*Zhepeng Cen,Haolin Chen,Shiyu Wang,Zuxin Liu,Zhiwei Liu,Ding Zhao,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: Webscale-RL pipeline converts large-scale pre-training documents into millions of diverse, verifiable QA pairs for RL, enabling more efficient training than continual pre-training.


<details>
  <summary>Details</summary>
Motivation: Address the training-generation gap in LLMs and overcome the data bottleneck in RL applications by creating web-scale RL datasets.

Method: Developed Webscale-RL pipeline to systematically convert pre-training documents into 1.2M diverse QA pairs across 9+ domains for RL training.

Result: Models trained on Webscale-RL dataset significantly outperform continual pre-training and data refinement baselines, achieving same performance with up to 100× fewer tokens.

Conclusion: Webscale-RL presents a viable path to scale RL to pre-training levels, enabling more capable and efficient language models.

Abstract: Large Language Models (LLMs) have achieved remarkable success through
imitation learning on vast text corpora, but this paradigm creates a
training-generation gap and limits robust reasoning. Reinforcement learning
(RL) offers a more data-efficient solution capable of bridging this gap, yet
its application has been constrained by a critical data bottleneck: existing RL
datasets are orders of magnitude smaller and less diverse than web-scale
pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a
scalable data engine that systematically converts large-scale pre-training
documents into millions of diverse, verifiable question-answer pairs for RL.
Using this pipeline, we construct the Webscale-RL dataset, containing 1.2
million examples across more than 9 domains. Our experiments show that the
model trained on this dataset significantly outperforms continual pretraining
and strong data refinement baselines across a suite of benchmarks. Notably, RL
training with our dataset proves substantially more efficient, achieving the
performance of continual pre-training with up to 100$\times$ fewer tokens. Our
work presents a viable path toward scaling RL to pre-training levels, enabling
more capable and efficient language models.

</details>


### [29] [From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining](https://arxiv.org/abs/2510.06548)
*Seng Pei Liew,Takuya Kato*

Main category: cs.CL

TL;DR: Bootstrapped pretraining's effectiveness diminishes predictably as base models are overtrained, following a scaling law where benefits decrease logarithmically with more base pretraining tokens.


<details>
  <summary>Details</summary>
Motivation: To understand the effectiveness of bootstrapped pretraining (reusing pretrained models for further training) and how it scales, especially when applied to overtrained base models.

Method: Empirical study of bootstrapped pretraining scaling behavior, analyzing how scaling exponent decreases with base model pretraining tokens and developing a simple scaling law model.

Result: Found that bootstrapped pretraining scaling efficiency diminishes predictably - scaling exponent decreases logarithmically with base model pretraining tokens, revealing a saturation effect.

Conclusion: There's a fundamental trade-off in multi-stage pretraining: more extensive base model pretraining reduces bootstrapping benefits, providing practical insights for efficient LM training and reuse of overtrained models.

Abstract: Bootstrapped pretraining, i.e., the reuse of a pretrained base model for
further pretraining, such as continual pretraining or model growth, is
promising at reducing the cost of training language models from scratch.
However, its effectiveness remains unclear, especially when applied to
overtrained base models. In this work, we empirically study the scaling
behavior of bootstrapped pretraining and find that its scaling efficiency
diminishes in a predictable manner: The scaling exponent with respect to
second-stage pretraining tokens decreases logarithmically with the number of
tokens used to pretrain the base model. The joint dependence on first- and
second-stage tokens is accurately modeled by a simple scaling law. Such
saturation effect reveals a fundamental trade-off in multi-stage pretraining
strategies: the more extensively a model is pretrained, the less additional
benefit bootstrapping provides. Our findings provide practical insights for
efficient language model training and raise important considerations for the
reuse of overtrained models.

</details>


### [30] [Flipping the Dialogue: Training and Evaluating User Language Models](https://arxiv.org/abs/2510.06552)
*Tarek Naous,Philippe Laban,Wei Xu,Jennifer Neville*

Main category: cs.CL

TL;DR: Assistant LMs make poor user simulators - better assistants yield worse simulators. Purpose-built User LMs better simulate human users and reveal assistant struggles in realistic multi-turn conversations.


<details>
  <summary>Details</summary>
Motivation: To evaluate LM performance in realistic settings, need better user simulators since assistant LMs are poor at simulating human users despite being trained as helpful assistants.

Method: Introduce purpose-built User Language Models (User LMs) - models post-trained specifically to simulate human users in multi-turn conversations.

Result: User LMs align better with human behavior and achieve better simulation robustness. When used to simulate coding/math conversations, GPT-4o's performance drops from 74.6% to 57.4%.

Conclusion: More realistic simulation environments using User LMs reveal that assistants struggle to cope with nuances of real users in multi-turn conversations.

Abstract: Conversations with LMs involve two participants: a human user leading the
conversation, and an LM assistant responding to the user's request. To satisfy
this specific role, LMs are post-trained to be helpful assistants -- optimized
to produce exhaustive and well-structured responses, free of ambiguity and
grammar errors. User utterances, on the other hand, are rarely perfected, with
each user phrasing requests in unique ways, sometimes putting in partial effort
at each turn and refining on the fly. To evaluate LM performance in realistic
settings, prior work simulated users in multi-turn conversations, often
prompting an LLM originally trained to be a helpful assistant to act as a user.
However, we show that assistant LMs make for poor user simulators, with the
surprising finding that better assistants yield worse simulators. Instead, we
introduce purpose-built User Language Models (User LMs) - models post-trained
to simulate human users in multi-turn conversations. Through various
evaluations, we show how User LMs align better with human behavior and achieve
better simulation robustness than existing simulation methods. When leveraging
User LMs to simulate coding and math conversations, the performance of a strong
assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic
simulation environments lead to assistant struggles as they fail to cope with
the nuances of users in multi-turn setups.

</details>


### [31] [The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law](https://arxiv.org/abs/2510.06559)
*Cheonkam Jeong,Sungdo Kim,Jewoo Park*

Main category: cs.CL

TL;DR: The paper presents Savassan, a neuro-symbolic system that treats language model alignment as a parsing problem, compiling utterances into typed logical forms with deontic operators to address hallucination and compliance issues across jurisdictions.


<details>
  <summary>Details</summary>
Motivation: Current language models mis-handle semantic types in their outputs, leading to hallucination, brittle moderation, and opaque compliance outcomes. The authors argue these are type-theoretic semantic problems rather than data or scale limitations.

Method: Savassan architecture: neural components extract candidate structures from inputs; symbolic components perform type checking, constraint reasoning, and cross-jurisdiction mapping using Montague-style logical forms and typed ontologies with deontic operators.

Result: The system can parse once and project results into multiple legal ontologies (e.g., defamation risk in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into explainable decisions rather than binary censorship.

Conclusion: Trustworthy autonomy requires compositional typing of meaning, enabling systems to reason about descriptive, normative, and legal dimensions within a unified algebra of meaning, addressing hallucination as a type error.

Abstract: Contemporary language models are fluent yet routinely mis-handle the types of
meaning their outputs entail. We argue that hallucination, brittle moderation,
and opaque compliance outcomes are symptoms of missing type-theoretic semantics
rather than data or scale limitations. Building on Montague's view of language
as typed, compositional algebra, we recast alignment as a parsing problem:
natural-language inputs must be compiled into structures that make explicit
their descriptive, normative, and legal dimensions under context.
  We present Savassan, a neuro-symbolic architecture that compiles utterances
into Montague-style logical forms and maps them to typed ontologies extended
with deontic operators and jurisdictional contexts. Neural components extract
candidate structures from unstructured inputs; symbolic components perform type
checking, constraint reasoning, and cross-jurisdiction mapping to produce
compliance-aware guidance rather than binary censorship. In cross-border
scenarios, the system "parses once" (e.g., defect claim(product x, company y))
and projects the result into multiple legal ontologies (e.g., defamation risk
in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into
a single, explainable decision.
  This paper contributes: (i) a diagnosis of hallucination as a type error;
(ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii)
a production-oriented design that embeds typed interfaces across the pipeline.
We outline an evaluation plan using legal reasoning benchmarks and synthetic
multi-jurisdiction suites. Our position is that trustworthy autonomy requires
compositional typing of meaning, enabling systems to reason about what is
described, what is prescribed, and what incurs liability within a unified
algebra of meaning.

</details>


### [32] [TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents](https://arxiv.org/abs/2510.06579)
*Haofei Yu,Keyang Xuan,Fenghai Li,Kunlun Zhu,Zijie Lei,Jiaxun Zhang,Ziheng Qi,Kyle Richardson,Jiaxuan You*

Main category: cs.CL

TL;DR: TinyScientist is an interactive, extensible framework that simplifies building and maintaining complex automatic research workflows using LLMs, addressing the growing complexity in multi-agent systems and research automation.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of automatic research workflows involving multi-agent systems, planning, tool usage, and human-agent interaction has made extending and maintaining these systems challenging as algorithms advance.

Method: Identifies essential components of automatic research workflow and proposes an interactive, extensible, and controllable framework that adapts to new tools and supports iterative growth.

Result: Developed an open-source codebase, interactive web demonstration, and PyPI Python package to make state-of-the-art auto-research pipelines accessible to researchers and developers.

Conclusion: TinyScientist provides a practical solution to manage the complexity of modern automatic research workflows, making advanced research automation tools broadly accessible through an extensible framework.

Abstract: Automatic research with Large Language Models (LLMs) is rapidly gaining
importance, driving the development of increasingly complex workflows involving
multi-agent systems, planning, tool usage, code execution, and human-agent
interaction to accelerate research processes. However, as more researchers and
developers begin to use and build upon these tools and platforms, the
complexity and difficulty of extending and maintaining such agentic workflows
have become a significant challenge, particularly as algorithms and
architectures continue to advance. To address this growing complexity,
TinyScientist identifies the essential components of the automatic research
workflow and proposes an interactive, extensible, and controllable framework
that easily adapts to new tools and supports iterative growth. We provide an
open-source codebase, an interactive web demonstration, and a PyPI Python
package to make state-of-the-art auto-research pipelines broadly accessible to
every researcher and developer.

</details>


### [33] [Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?](https://arxiv.org/abs/2510.06594)
*Sri Durga Sai Sowmya Kadali,Evangelos E. Papalexakis*

Main category: cs.CL

TL;DR: This paper investigates jailbreaking in large language models by analyzing internal representations, focusing on how hidden layers respond differently to jailbreak vs benign prompts in GPT-J and Mamba2 models.


<details>
  <summary>Details</summary>
Motivation: Jailbreaking LLMs is a critical security concern as adversarial users exploit carefully engineered prompts to elicit restricted outputs, and existing defenses are insufficient against evolving attack techniques.

Method: Examine internal representations of LLMs by analyzing how hidden layers respond to jailbreak versus benign prompts, specifically studying GPT-J and Mamba2 models.

Result: Preliminary findings show distinct layer-wise behaviors in how models process jailbreak prompts compared to benign ones, revealing differences in internal model dynamics.

Conclusion: The results suggest promising directions for future research on leveraging internal model dynamics for robust jailbreak detection and defense mechanisms.

Abstract: Jailbreaking large language models (LLMs) has emerged as a pressing concern
with the increasing prevalence and accessibility of conversational LLMs.
Adversarial users often exploit these models through carefully engineered
prompts to elicit restricted or sensitive outputs, a strategy widely referred
to as jailbreaking. While numerous defense mechanisms have been proposed,
attackers continuously develop novel prompting techniques, and no existing
model can be considered fully resistant. In this study, we investigate the
jailbreak phenomenon by examining the internal representations of LLMs, with a
focus on how hidden layers respond to jailbreak versus benign prompts.
Specifically, we analyze the open-source LLM GPT-J and the state-space model
Mamba2, presenting preliminary findings that highlight distinct layer-wise
behaviors. Our results suggest promising directions for further research on
leveraging internal model dynamics for robust jailbreak detection and defense.

</details>


### [34] [A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures](https://arxiv.org/abs/2510.06640)
*Nhat M. Hoang,Do Xuan Long,Cong-Duy Nguyen,Min-Yen Kan,Luu Anh Tuan*

Main category: cs.CL

TL;DR: This paper provides the first unified analysis of representation propagation in State Space Models (SSMs) vs Transformer-Based Models (TBMs), revealing key differences in how they handle token diversity across layers.


<details>
  <summary>Details</summary>
Motivation: While SSMs have emerged as efficient alternatives to TBMs for long-sequence processing, there's limited understanding of how contextual information flows across layers and tokens in these architectures.

Method: Used centered kernel alignment, stability metrics, and probing to analyze representation evolution within and across layers, plus theoretical analysis and parameter randomization.

Result: Found TBMs rapidly homogenize token representations early (with diversity reemerging later), while SSMs preserve token uniqueness early but converge to homogenization deeper. Oversmoothing in TBMs stems from architectural design, while in SSMs it comes from training dynamics.

Conclusion: These insights clarify the inductive biases of both architectures and inform future model and training designs for long-context reasoning.

Abstract: State Space Models (SSMs) have recently emerged as efficient alternatives to
Transformer-Based Models (TBMs) for long-sequence processing, offering linear
scaling and lower memory use. Yet, how contextual information flows across
layers and tokens in these architectures remains understudied. We present the
first unified, token- and layer-level analysis of representation propagation in
SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,
we characterize how representations evolve within and across layers. We find a
key divergence: TBMs rapidly homogenize token representations, with diversity
reemerging only in later layers, while SSMs preserve token uniqueness early but
converge to homogenization deeper. Theoretical analysis and parameter
randomization further reveal that oversmoothing in TBMs stems from
architectural design, whereas in SSMs it arises mainly from training dynamics.
These insights clarify the inductive biases of both architectures and inform
future model and training designs for long-context reasoning.

</details>


### [35] [Aligning Large Language Models via Fully Self-Synthetic Data](https://arxiv.org/abs/2510.06652)
*Shangjian Yin,Zhepei Wei,Xinyu Zhu,Wei-Lin Chen,Yu Meng*

Main category: cs.CL

TL;DR: SAO is a fully self-synthetic framework for LLM alignment that generates all training data (prompts, responses, preferences) internally without human or external model annotation, improving chat capabilities while maintaining performance on objective tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional RLHF requires expensive human-annotated datasets, and RLAIF also incurs significant costs from collecting diverse prompts/responses and using external reward models. There's a need for a more cost-effective self-alignment approach.

Method: SAO instructs LLMs to engage in persona role-play to generate diverse prompts and responses, then self-evaluates these for preference optimization - creating a completely self-contained training loop.

Result: Extensive experiments show SAO effectively enhances model chat capabilities on benchmarks like AlpacaEval 2.0 while maintaining strong performance on downstream objective tasks (question-answering, math reasoning).

Conclusion: SAO provides a practical solution for self-improvement in aligning LLMs, offering a cost-effective alternative to traditional RLHF and RLAIF approaches.

Abstract: Traditional reinforcement learning from human feedback (RLHF) for large
language models (LLMs) relies on expensive human-annotated datasets, while
Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,
requiring the collection of diverse prompts and corresponding responses, often
necessitating external reward models or proprietary models like GPT-4 to
annotate preference pairs. In this work, we introduce Self-Alignment
Optimization (SAO), a fully self-synthetic framework for LLM alignment, where
all training data, including prompts (i.e., user queries), responses, and
preferences, are generated by the model itself. Specifically, SAO first
instructs the LLM to engage in persona role-play and generate diverse prompts
and responses, which are then self-evaluated for preference optimization.
Extensive experiments demonstrate that SAO effectively enhances the model's
chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining
strong performance on downstream objective tasks (e.g., question-answering,
math reasoning). Our work provides a practical solution for self-improvement in
aligning LLMs, and the code for reproducing our results is available at:
https://github.com/SJY8460/SAO.

</details>


### [36] [ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory](https://arxiv.org/abs/2510.06664)
*Yunzhong Xiao,Yangmin Li,Hewei Wang,Yunlong Tang,Zora Zhiruo Wang*

Main category: cs.CL

TL;DR: ToolMem enables AI agents to develop memory of tool capabilities from previous interactions, improving tool selection and performance prediction by 14.8-28.7% compared to agents without memory.


<details>
  <summary>Details</summary>
Motivation: Current AI agents rely on fixed tools despite neural tools performing uncertainly across different scenarios, while humans adaptively select optimal tools based on learned capabilities.

Method: ToolMem allows agents to summarize tool strengths/weaknesses from interactions and store them in memory, then retrieve relevant entries to select the best tool for individual tasks.

Result: ToolMem-augmented agents predict tool performance 14.8% and 28.7% more accurately in text and multimodal generation, and improve optimal tool selection by 21% and 24% absolute increases.

Conclusion: ToolMem effectively enables agents to learn and leverage tool capabilities through memory, significantly improving tool selection and performance prediction across diverse tasks.

Abstract: Agents utilizing tools powered by large language models (LLMs) or
vision-language models (VLMs) have demonstrated remarkable progress in diverse
tasks across text and visual modalities. Unlike traditional tools such as
calculators, which give deterministic outputs, neural tools perform uncertainly
across task scenarios. While different tools for a task may excel in varied
scenarios, existing agents typically rely on fixed tools, thus limiting the
flexibility in selecting the most suitable tool for specific tasks. In
contrast, humans snowball their understanding of the capabilities of different
tools by interacting with them, and apply this knowledge to select the optimal
tool when solving a future task. To build agents that similarly benefit from
this process, we propose ToolMem that enables agents to develop memories of
tool capabilities from previous interactions, by summarizing their strengths
and weaknesses and storing them in memory; at inference, the agent can retrieve
relevant entries from ToolMem, and select the best tool to solve individual
tasks more accurately. We evaluate ToolMem on learning varied text generation
and text-to-image generation neural tools. Compared to no-memory, generic
agents, we find ToolMem-augmented agents predict tool performance 14.8% and
28.7% more accurately across text and multimodal generation scenarios.
Moreover, ToolMem facilitates optimal tool selection among multiple choices by
21% and 24% absolute increases in respective scenarios.

</details>


### [37] [PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch](https://arxiv.org/abs/2510.06670)
*Shangjian Yin,Shining Liang,Wenbiao Ding,Yuli Qian,Zhouxing Shi,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: PiKa introduces a data-efficient family of expert-level alignment datasets that achieve superior performance with only 30k SFT examples, outperforming models trained on much larger datasets and even surpassing proprietary models trained on 10M+ examples.


<details>
  <summary>Details</summary>
Motivation: Existing alignment datasets are either private or require costly human annotation, limiting reproducibility and scalability. Current approaches use over 300k examples but still underperform proprietary models, creating barriers for academic and resource-limited communities.

Method: Developed PiKa, a data-efficient family of expert-level alignment datasets, with PiKa-SFT using only 30k supervised fine-tuning examples. Evaluated by fine-tuning Llama-3-8B-Base and Qwen2.5 series models on PiKa and other public datasets.

Result: PiKa-SFT outperforms models trained on much larger data on AlpacaEval 2.0 and Arena-Hard benchmarks. It even surpasses the official Llama-3-8B-Instruct model trained on over 10 million proprietary examples. Consistent gains achieved across Qwen2.5 series (0.5B to 7B).

Conclusion: High-quality alignment can be achieved with significantly less data, offering a scalable path for open-source LLM alignment. PiKa demonstrates that data efficiency is key to democratizing LLM alignment.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone
for aligning large language models (LLMs). However, its effectiveness depends
on high-quality instruction data. Most existing alignment datasets are either
private or require costly human annotation, which limits reproducibility and
scalability. Even with Reinforcement Learning from AI Feedback (RLAIF),
concerns about data quality remain. Moreover, it is unclear how much data is
actually required to fine-tune a base model into a strong instruction-following
model. Current approaches often rely on over 300k examples even at the
supervised fine-tuning (SFT) stage, yet they still underperform compared to
proprietary models, creating barriers for academic and resource-limited
communities. To address this gap, we introduce PiKa, a data-efficient family of
expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only
30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through
evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,
we show that PiKa-SFT outperforms models trained on much larger data. On
AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses
the official Llama-3-8B-Instruct model trained on over 10 million proprietary
examples. We further extend our study by training the Qwen2.5 series (0.5B to
7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that
high-quality alignment can be achieved with significantly less data, offering a
scalable path for open-source LLM alignment. Code and data:
https://github.com/SJY8460/PiKa.

</details>


### [38] [Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback](https://arxiv.org/abs/2510.06677)
*Yisha Wu,Cen,Zhao,Yuanpei Cao,Xiaoqing Su,Yashar Mehdad,Mindy Ji,Claire Na Cheng*

Main category: cs.CL

TL;DR: An incremental summarization system for customer support agents that generates concise bullet notes during conversations, reducing context-switching and redundant review.


<details>
  <summary>Details</summary>
Motivation: To reduce customer support agents' context-switching effort and redundant review during conversations by providing timely, concise summaries.

Method: Combines a fine-tuned Mixtral-8x7B model for continuous note generation with a DeBERTa-based classifier to filter trivial content, and uses agent edits to refine online notes and inform offline model retraining.

Result: Deployed in production, achieved 3% reduction in case handling time (up to 9% in highly complex cases) and high agent satisfaction ratings.

Conclusion: Incremental summarization with continuous feedback effectively enhances summary quality and agent productivity at scale.

Abstract: We introduce an incremental summarization system for customer support agents
that intelligently determines when to generate concise bullet notes during
conversations, reducing agents' context-switching effort and redundant review.
Our approach combines a fine-tuned Mixtral-8x7B model for continuous note
generation with a DeBERTa-based classifier to filter trivial content. Agent
edits refine the online notes generation and regularly inform offline model
retraining, closing the agent edits feedback loop. Deployed in production, our
system achieved a 3% reduction in case handling time compared to bulk
summarization (with reductions of up to 9% in highly complex cases), alongside
high agent satisfaction ratings from surveys. These results demonstrate that
incremental summarization with continuous feedback effectively enhances summary
quality and agent productivity at scale.

</details>


### [39] [Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks](https://arxiv.org/abs/2510.06695)
*Qinhao Zhou,Xiang Xiang,Kun He,John E. Hopcroft*

Main category: cs.CL

TL;DR: This paper introduces a novel prompt optimization method specifically designed for machine translation tasks, using a small-parameter model trained with back-translation strategy to reduce training overhead while maintaining high effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing prompt engineering methods focus mainly on optimizing instruction components for general tasks and require large-parameter LLMs as auxiliary tools, but they have limited applicability for machine translation where the input component plays a more critical role.

Method: The proposed approach employs a small-parameter model trained using a back-translation-based strategy, which significantly reduces training overhead for single-task optimization.

Result: The method delivers highly effective performance for machine translation tasks.

Conclusion: With certain adaptations, this method can also be extended to other downstream tasks beyond machine translation.

Abstract: In recent years, the growing interest in Large Language Models (LLMs) has
significantly advanced prompt engineering, transitioning from manual design to
model-based optimization. Prompts for LLMs generally comprise two components:
the \textit{instruction}, which defines the task or objective, and the
\textit{input}, which is tailored to the instruction type. In natural language
generation (NLG) tasks such as machine translation, the \textit{input}
component is particularly critical, while the \textit{instruction} component
tends to be concise. Existing prompt engineering methods primarily focus on
optimizing the \textit{instruction} component for general tasks, often
requiring large-parameter LLMs as auxiliary tools. However, these approaches
exhibit limited applicability for tasks like machine translation, where the
\textit{input} component plays a more pivotal role. To address this limitation,
this paper introduces a novel prompt optimization method specifically designed
for machine translation tasks. The proposed approach employs a small-parameter
model trained using a back-translation-based strategy, significantly reducing
training overhead for single-task optimization while delivering highly
effective performance. With certain adaptations, this method can also be
extended to other downstream tasks.

</details>


### [40] [How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects](https://arxiv.org/abs/2510.06700)
*Leonardo Bertolazzi,Sandro Pezzelle,Raffaelle Bernardi*

Main category: cs.CL

TL;DR: LLMs exhibit content effects where semantic plausibility biases logical validity judgments, similar to humans. The study shows validity and plausibility are linearly represented and aligned in LLMs' internal representations, causing conflation between them.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind content effects in LLMs - why semantic plausibility influences logical validity judgments, similar to human dual-process reasoning.

Method: Investigated how LLMs encode validity and plausibility concepts in internal representations, used steering vectors to test causal relationships, and constructed debiasing vectors to disentangle concepts.

Result: Found both validity and plausibility are linearly represented and strongly aligned in representational geometry. Plausibility vectors causally bias validity judgments and vice versa. Degree of alignment predicts behavioral content effects across models.

Conclusion: Representational alignment between validity and plausibility causes content effects in LLMs. Debiasing vectors can disentangle these concepts, reducing content effects and improving reasoning accuracy, offering a path toward more logical AI systems.

Abstract: Both humans and large language models (LLMs) exhibit content effects: biases
in which the plausibility of the semantic content of a reasoning problem
influences judgments regarding its logical validity. While this phenomenon in
humans is best explained by the dual-process theory of reasoning, the
mechanisms behind content effects in LLMs remain unclear. In this work, we
address this issue by investigating how LLMs encode the concepts of validity
and plausibility within their internal representations. We show that both
concepts are linearly represented and strongly aligned in representational
geometry, leading models to conflate plausibility with validity. Using steering
vectors, we demonstrate that plausibility vectors can causally bias validity
judgements, and vice versa, and that the degree of alignment between these two
concepts predicts the magnitude of behavioral content effects across models.
Finally, we construct debiasing vectors that disentangle these concepts,
reducing content effects and improving reasoning accuracy. Our findings advance
understanding of how abstract logical concepts are represented in LLMs and
highlight representational interventions as a path toward more logical systems.

</details>


### [41] [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)
*Miao Lu,Weiwei Sun,Weihua Du,Zhan Ling,Xuesong Yao,Kang Liu,Jiecao Chen*

Main category: cs.CL

TL;DR: SUPO introduces summarization-based context management to enable RL fine-tuning of LLM agents for long-horizon multi-turn tool use, overcoming context length bottlenecks by compressing tool history with LLM-generated summaries.


<details>
  <summary>Details</summary>
Motivation: Existing RL pipelines for LLM agents face degraded instruction following, excessive rollout costs, and strict context limits that hinder long-horizon multi-turn tool use.

Method: Proposes SUPO (Summarization augmented Policy Optimization) - an RL algorithm that periodically compresses tool-use history using LLM-generated summaries to maintain compact context, with end-to-end optimization of both tool-use behaviors and summarization strategies.

Result: Experiments on interactive function calling and searching tasks show SUPO significantly improves success rates while maintaining same or lower context length compared to baselines, and scales well beyond training-time summarization limits.

Conclusion: Summarization-based context management provides a principled and scalable approach for training RL agents beyond fixed context length limits, enabling effective long-horizon tool use.

Abstract: We study reinforcement learning (RL) fine-tuning of large language model
(LLM) agents for long-horizon multi-turn tool use, where context length quickly
becomes a fundamental bottleneck. Existing RL pipelines can suffer from
degraded instruction following, excessive rollout costs, and most importantly,
strict context limits. To address these challenges, we introduce
summarization-based context management to training. In specific, it
periodically compresses the tool using history by LLM-generated summaries that
retain task-relevant information to keep a compact context while enabling the
agent to scale beyond the fixed context window. Building on this formulation,
we derive a policy gradient representation that seamlessly enables standard LLM
RL infrastructures to optimize both tool-use behaviors as well as summarization
strategies in an end-to-end fashion. We instantiate this framework with
\underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization
(\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond
a fixed context limit. Experiments on interactive function calling and
searching tasks demonstrate that \texttt{SUPO} significantly improves the
success rate while maintaining the same or even lower working context length
compared to baselines. We also demonstrate that for complex searching tasks,
\texttt{SUPO} can further improve the evaluation performance when scaling
test-time maximum round of summarization beyond that of training time. Our
results establish summarization-based context management as a principled and
scalable approach for training RL agents beyond a fixed context length limit.

</details>


### [42] [PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs](https://arxiv.org/abs/2510.06730)
*Manuel Frank,Haithem Afli*

Main category: cs.CL

TL;DR: PTEB is a dynamic evaluation protocol that generates paraphrases at test time to assess sentence embedding robustness, revealing sensitivity to token variations despite preserved semantics.


<details>
  <summary>Details</summary>
Motivation: Static benchmarks like MTEB can inflate performance through repeated tuning and obscure real-world robustness. Need for dynamic evaluation that tests model sensitivity to semantic-preserving variations.

Method: Uses LLM-based method to generate token-diverse but semantically preserving paraphrases at evaluation time, aggregating results across multiple runs. Applied to 7 MTEB tasks and 3 multilingual datasets across 10 languages.

Result: Sentence encoders are sensitive to token space changes even when semantics remain fixed. Smaller models are not disproportionately affected relative to larger ones. Results are statistically robust across multiple runs.

Conclusion: Proposes a shift from static benchmarks to dynamic, stochastic evaluation leveraging eval-time compute, providing more realistic assessment of sentence embedding robustness.

Abstract: Current evaluations of sentence embedding models typically rely on static
test beds such as the Massive Text Embedding Benchmark (MTEB). While
invaluable, repeated tuning on a fixed suite can inflate reported performance
and obscure real-world robustness. We introduce the Paraphrasing Text Embedding
Benchmark (PTEB), a dynamic protocol that stochastically generates
meaning-preserving paraphrases at evaluation time and aggregates results across
multiple runs. Using a cost-efficient LLM-based method grounded in semantic
textual similarity gold ratings, we show that LLMs generate token-diverse but
semantically preserving, paraphrases. Across 7 MTEB tasks, we validate our
hypothesis that the performance of sentence encoders is sensitive to changes in
token space even when semantics remain fixed. We also observe that smaller
models are not disproportionately affected relative to larger ones. Our results
are statistically robust over multiple runs and we extended our experiments to
3 multilingual datasets covering 10 languages. More generally, we aim to
propose a new evaluation paradigm in NLP that relies less on static,
pre-defined benchmarks but shifts towards dynamic, stochastic evaluation
leveraging eval-time compute.

</details>


### [43] [Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization](https://arxiv.org/abs/2510.06732)
*Tiancheng Xing,Jerry Li,Yixuan Du,Xiyang Hu*

Main category: cs.CL

TL;DR: RAF is a two-stage token optimization method that crafts natural-sounding prompts to manipulate LLM-based ranking systems, exposing their vulnerability to adversarial manipulation.


<details>
  <summary>Details</summary>
Motivation: To expose the vulnerability of LLM-based rerankers to manipulation by small, natural-sounding prompts that can steer ranking behavior.

Method: Two-stage token optimization: Stage 1 uses Greedy Coordinate Gradient to shortlist candidate tokens combining rank-target gradient and readability; Stage 2 evaluates candidates under ranking and readability losses with entropy-based dynamic weighting and temperature-controlled sampling.

Result: RAF significantly boosts target item ranks using naturalistic language across multiple LLMs, with greater robustness than existing methods in both promotion effectiveness and naturalness preservation.

Conclusion: LLM-based reranking is inherently susceptible to adversarial manipulation, raising critical security implications for the trustworthiness and robustness of modern retrieval systems.

Abstract: Large language models (LLMs) are increasingly used as rerankers in
information retrieval, yet their ranking behavior can be steered by small,
natural-sounding prompts. To expose this vulnerability, we present Rank
Anything First (RAF), a two-stage token optimization method that crafts concise
textual perturbations to consistently promote a target item in LLM-generated
rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate
Gradient to shortlist candidate tokens at the current position by combining the
gradient of the rank-target with a readability score; Stage 2 evaluates those
candidates under exact ranking and readability losses using an entropy-based
dynamic weighting scheme, and selects a token via temperature-controlled
sampling. RAF generates ranking-promoting prompts token-by-token, guided by
dual objectives: maximizing ranking effectiveness and preserving linguistic
naturalness. Experiments across multiple LLMs show that RAF significantly
boosts the rank of target items using naturalistic language, with greater
robustness than existing methods in both promoting target items and maintaining
naturalness. These findings underscore a critical security implication:
LLM-based reranking is inherently susceptible to adversarial manipulation,
raising new challenges for the trustworthiness and robustness of modern
retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.

</details>


### [44] [AWM: Accurate Weight-Matrix Fingerprint for Large Language Models](https://arxiv.org/abs/2510.06738)
*Boyi Zeng,Lin Chen,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: Proposes a training-free fingerprinting method using weight matrices and Linear Assignment Problem with Centered Kernel Alignment to identify if LLMs are derived from base models, achieving perfect classification metrics with high robustness against post-training modifications.


<details>
  <summary>Details</summary>
Motivation: Protecting intellectual property of LLMs is crucial due to substantial training resources. Need reliable methods to determine if suspect LLMs are trained from scratch or derived from existing models, especially challenging due to intensive post-training processes.

Method: Training-free fingerprinting based on weight matrices using Linear Assignment Problem (LAP) and unbiased Centered Kernel Alignment (CKA) similarity to neutralize parameter manipulation effects.

Result: Achieved perfect scores on all classification metrics with near-zero false positive risk on 60 positive and 90 negative model pairs. Highly robust against all six post-training categories. Computation completes within 30s on NVIDIA 3090 GPU.

Conclusion: The method establishes strong basis for reliable model lineage verification with exceptional robustness and high fidelity, providing effective IP protection for LLMs.

Abstract: Protecting the intellectual property of large language models (LLMs) is
crucial, given the substantial resources required for their training.
Consequently, there is an urgent need for both model owners and third parties
to determine whether a suspect LLM is trained from scratch or derived from an
existing base model. However, the intensive post-training processes that models
typically undergo-such as supervised fine-tuning, extensive continued
pretraining, reinforcement learning, multi-modal extension, pruning, and
upcycling-pose significant challenges to reliable identification. In this work,
we propose a training-free fingerprinting method based on weight matrices. We
leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel
Alignment (CKA) similarity to neutralize the effects of parameter
manipulations, yielding a highly robust and high-fidelity similarity metric. On
a comprehensive testbed of 60 positive and 90 negative model pairs, our method
demonstrates exceptional robustness against all six aforementioned
post-training categories while exhibiting a near-zero risk of false positives.
By achieving perfect scores on all classification metrics, our approach
establishes a strong basis for reliable model lineage verification. Moreover,
the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is
available at https://github.com/LUMIA-Group/AWM.

</details>


### [45] [TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs](https://arxiv.org/abs/2510.06747)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: A training-free, label-free short text clustering method using iterative vector updating with LLM guidance, achieving comparable or better results than contrastive learning methods without needing cluster or label information.


<details>
  <summary>Details</summary>
Motivation: Companies dealing with customer-facing chatbots need to cluster large amounts of user utterances by intent, but typically have no labeled data and unknown number of clusters in commercial settings.

Method: Iterative vector updating: constructs sparse vectors from representative texts and refines them through LLM guidance, working with any embedder and requiring no training or labels.

Result: Achieves comparable or superior results to state-of-the-art contrastive learning methods, works with diverse datasets and smaller LLMs, scales to large datasets with reduced computational cost.

Conclusion: The method is model-agnostic, scalable, and better aligned with real-world scenarios due to its low-resource requirements and adaptability to different clustering methods.

Abstract: In this paper, we propose a training-free and label-free method for short
text clustering that can be used on top of any existing embedder. In the
context of customer-facing chatbots, companies are dealing with large amounts
of user utterances that need to be clustered according to their intent. In
these commercial settings, no labeled data is typically available, and the
number of clusters is not known. Our method is based on iterative vector
updating: it constructs sparse vectors based on representative texts, and then
iteratively refines them through LLM guidance. Our method achieves comparable
or superior results to state-of-the-art methods that use contrastive learning,
but without assuming prior knowledge of clusters or labels. Experiments on
diverse datasets and smaller LLMs show that our method is model agnostic and
can be applied to any embedder, with relatively small LLMs, and different
clustering methods. We also show that our method scales to large datasets,
reducing the computational cost of the LLM. These low-resource, adaptable
settings and the scalability of our method make it more aligned with real-world
scenarios than existing clustering methods.

</details>


### [46] [A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction](https://arxiv.org/abs/2510.06749)
*Eitan Klinger,Zihao Huang,Tran Minh Nguyen,Emma Jayeon Park,Yige Chen,Yang Gu,Qingyu Gao,Siliang Liu,Mengyang Qiu,Jungyeul Park*

Main category: cs.CL

TL;DR: This paper introduces a fluency-based multi-reference evaluation framework for grammatical error correction that addresses limitations of existing edit-based metrics by using n-gram similarity and multiple aggregation strategies.


<details>
  <summary>Details</summary>
Motivation: Existing GEC evaluation metrics are edit-based, English-centric, and rely on rigid alignments, limiting their applicability in multilingual and generative settings. They fail to reflect the diversity of valid human corrections.

Method: Proposes a formal framework for fluency-based multi-reference evaluation, framing n-gram similarity as an aggregation problem. Instantiates GLEU metric through four aggregation strategies: select-best, simple-average, weighted-average, and merged-counts.

Result: Empirical evaluation on Czech, Estonian, Ukrainian, and Chinese corpora shows that the different aggregation strategies capture complementary aspects of fluency and coverage.

Conclusion: The framework provides a principled, fluency-oriented approach that incorporates linguistic diversity without penalizing legitimate variation, unifying multi-reference evaluation.

Abstract: Evaluating grammatical error correction requires metrics that reflect the
diversity of valid human corrections rather than privileging a single
reference. Existing frameworks, largely edit-based and English-centric, rely on
rigid alignments between system and reference edits, limiting their
applicability in multilingual and generative settings. This paper introduces a
formal framework for \textit{fluency-based multi-reference evaluation}, framing
$n$-gram similarity as an aggregation problem over multiple legitimate
corrections. Within this formulation, we instantiate GLEU through four
aggregation strategies--\textsc{select-best}, \textsc{simple-average},
\textsc{weighted-average}, and \textsc{merged-counts}--and analyze their
properties of boundedness, monotonicity, and sensitivity to reference
variation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora
show that these strategies capture complementary aspects of fluency and
coverage. The framework unifies multi-reference evaluation into a principled,
fluency-oriented approach that incorporates linguistic diversity without
penalizing legitimate variation.

</details>


### [47] [Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs](https://arxiv.org/abs/2510.06750)
*Jaeseong Lee,Dayoung Kwon,seung-won hwang*

Main category: cs.CL

TL;DR: Proposes a superposed deployment strategy with lightweight regulation to optimize LRM inference by selectively unlearning from LRM at inference, avoiding overthinking while preserving reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models (LRMs) often suffer from overthinking, which degrades performance and wastes computational resources. Traditional routing approaches require deploying multiple models, which can be costly or impractical.

Method: Uses a superposed deployment strategy with training-free regulation that switches one model on and off. Instead of routing, selectively unlearns from LRM at inference by analyzing cumulative energy of singular values to identify optimal low-rank projections.

Result: Enables scaling down computation while preserving reasoning capabilities, achieving optimal reasoning adjustment without the need for multiple model deployments.

Conclusion: The proposed method provides an efficient way to regulate LRM inference, preventing overthinking while maintaining reasoning performance through selective unlearning and low-rank projections.

Abstract: Large Reasoning Models (LRMs) excel in structured tasks by emulating
deliberate human reasoning but often suffer from overthinking, degrading
performance and wasting resources. One possible baseline is to deploy both LLM
and LRM, then route input by predicting whether it requires reasoning and may
cause overthinking. However, deploying multiple models can be costly or
impractical. We propose a superposed deployment strategy with a lightweight,
training-free regulation to optimize inference by switching one model on and
off. Instead of routing, we selectively unlearn from LRM at inference, scaling
down computation while preserving reasoning. By analyzing the cumulative energy
of singular values, we identify optimal low-rank projections to adjust
reasoning just right.

</details>


### [48] [Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition](https://arxiv.org/abs/2510.06774)
*Lei Xu,Pierre Beckmann,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: An adaptive neuro-symbolic framework that automatically identifies formal reasoning strategies from natural language problems and dynamically selects appropriate logical solvers, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Current neuro-symbolic NLP methods are static and predetermined, lacking flexibility to employ diverse formal inference strategies for different reasoning tasks.

Method: Developed an adaptive multi-paradigm neuro-symbolic inference framework that: (1) automatically identifies formal reasoning strategies from natural language problems, and (2) dynamically selects and applies specialized formal logical solvers through autoformalization interfaces.

Result: LLMs achieve over 90% accuracy in predicting necessary formal reasoning strategies. The framework outperforms GPT-4o by 27% and DeepSeek-V3.1 by 6%. Adaptive reasoning also improves pure LLM methods by 10%, 5%, and 6% on zero-shot, CoT, and symbolic CoT settings respectively.

Conclusion: This work establishes foundations for adaptive LLM-symbolic reasoning, providing a path to unify material and formal inferences for heterogeneous reasoning challenges, with post-training offering improvement for smaller models.

Abstract: Neuro-symbolic NLP methods aim to leverage the complementary strengths of
large language models and formal logical solvers. However, current approaches
are mostly static in nature, i.e., the integration of a target solver is
predetermined at design time, hindering the ability to employ diverse formal
inference strategies. To address this, we introduce an adaptive,
multi-paradigm, neuro-symbolic inference framework that: (1) automatically
identifies formal reasoning strategies from problems expressed in natural
language; and (2) dynamically selects and applies specialized formal logical
solvers via autoformalization interfaces. Extensive experiments on individual
and multi-paradigm reasoning tasks support the following conclusions: LLMs are
effective at predicting the necessary formal reasoning strategies with an
accuracy above 90 percent. This enables flexible integration with formal
logical solvers, resulting in our framework outperforming competing baselines
by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively.
Moreover, adaptive reasoning can even positively impact pure LLM methods,
yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT
settings with GPT-4o. Finally, although smaller models struggle with adaptive
neuro-symbolic reasoning, post-training offers a viable path to improvement.
Overall, this work establishes the foundations for adaptive LLM-symbolic
reasoning, offering a path forward for unifying material and formal inferences
on heterogeneous reasoning challenges.

</details>


### [49] [Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness](https://arxiv.org/abs/2510.06780)
*Luca Giordano,Simon Razniewski*

Main category: cs.CL

TL;DR: Systematic study of LLM knowledge extraction shows high termination rates but mixed reproducibility and varying robustness across different perturbation types.


<details>
  <summary>Details</summary>
Motivation: To measure and systematize the factual knowledge encoded in LLMs by converting it into structured format, addressing key questions about termination, reproducibility, and robustness of knowledge extraction.

Method: Used miniGPTKBs (domain-specific subcrawls) to analyze LLM knowledge materialization across three metric categories: yield, lexical similarity, and semantic similarity. Tested four variations (seed, language, randomness, model) in three domains (history, entertainment, finance).

Result: Found (i) high termination rates (model-dependent), (ii) mixed reproducibility, and (iii) robustness varying by perturbation type: high for seeds and temperature, lower for languages and models.

Conclusion: LLM knowledge materialization can reliably extract core knowledge but has important limitations in reproducibility and robustness across different conditions.

Abstract: Large Language Models (LLMs) encode substantial factual knowledge, yet
measuring and systematizing this knowledge remains challenging. Converting it
into structured format, for example through recursive extraction approaches
such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key
open questions include whether such extraction can terminate, whether its
outputs are reproducible, and how robust they are to variations. We
systematically study LLM knowledge materialization using miniGPTKBs
(domain-specific, tractable subcrawls), analyzing termination, reproducibility,
and robustness across three categories of metrics: yield, lexical similarity,
and semantic similarity. We experiment with four variations (seed, language,
randomness, model) and three illustrative domains (from history, entertainment,
and finance). Our findings show (i) high termination rates, though
model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies
by perturbation type: high for seeds and temperature, lower for languages and
models. These results suggest that LLM knowledge materialization can reliably
surface core knowledge, while also revealing important limitations.

</details>


### [50] [FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline](https://arxiv.org/abs/2510.06800)
*Haotian Wu,Shufan Jiang,Chios Chen,Yiyang Feng,Hehai Lin,Heqing Zou,Yao Shu,Yanran Li,Chengwei Qin*

Main category: cs.CL

TL;DR: FURINA-Builder is a multi-agent pipeline for automatically creating customizable role-playing benchmarks, addressing limitations of existing benchmarks. It enables evaluation of arbitrary characters across diverse scenarios and formats.


<details>
  <summary>Details</summary>
Motivation: Existing role-playing benchmarks become obsolete quickly due to narrow scope, outdated interaction paradigms, and limited adaptability across diverse application scenarios.

Method: FURINA-Builder uses multi-agent collaboration to simulate dialogues between test characters and other characters from a character-scene pool, with an LLM judge selecting evaluation dimensions and adjusting responses into final test utterances.

Result: Built FURINA-Bench with both established and synthesized characters. Found o3 and DeepSeek-R1 perform best on English and Chinese RP tasks respectively. Established characters consistently outperform synthesized ones, and reasoning capabilities amplify this disparity. Model scale doesn't monotonically reduce hallucinations. Reasoning LLMs show a trade-off: improved RP performance but increased hallucinations.

Conclusion: FURINA-Builder effectively addresses benchmark limitations and FURINA-Bench poses significant challenges, revealing a Pareto frontier between RP performance and reliability across all LLMs.

Abstract: As large language models (LLMs) advance in role-playing (RP) tasks, existing
benchmarks quickly become obsolete due to their narrow scope, outdated
interaction paradigms, and limited adaptability across diverse application
scenarios. To address this gap, we introduce FURINA-Builder, a novel
multi-agent collaboration pipeline that automatically constructs fully
customizable RP benchmarks at any scale. It enables evaluation of arbitrary
characters across diverse scenarios and prompt formats, as the first benchmark
builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues
between a test character and other characters drawn from a well-constructed
character-scene pool, while an LLM judge selects fine-grained evaluation
dimensions and adjusts the test character's responses into final test
utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive
role-playing benchmark featuring both established and synthesized test
characters, each assessed with dimension-specific evaluation criteria. Human
evaluation and preliminary separability analysis justify our pipeline and
benchmark design. We conduct extensive evaluations of cutting-edge LLMs and
find that o3 and DeepSeek-R1 achieve the best performance on English and
Chinese RP tasks, respectively. Across all models, established characters
consistently outperform synthesized ones, with reasoning capabilities further
amplifying this disparity. Interestingly, we observe that model scale does not
monotonically reduce hallucinations. More critically, for reasoning LLMs, we
uncover a novel trade-off: reasoning improves RP performance but simultaneously
increases RP hallucinations. This trade-off extends to a broader Pareto
frontier between RP performance and reliability for all LLMs. These findings
demonstrate the effectiveness of FURINA-Builder and the challenge posed by
FURINA-Bench.

</details>


### [51] [Overview of the Plagiarism Detection Task at PAN 2025](https://arxiv.org/abs/2510.06805)
*André Greiner-Petter,Maik Fröbe,Jan Philip Wahle,Terry Ruas,Bela Gipp,Akiko Aizawa,Martin Potthast*

Main category: cs.CL

TL;DR: The PAN 2025 generative plagiarism detection task focuses on identifying AI-generated plagiarism in scientific articles using a novel dataset created with Llama, DeepSeek-R1, and Mistral models. While semantic similarity approaches show promising results (0.8 recall, 0.5 precision) on the new dataset, they perform poorly on the 2015 dataset, revealing generalization issues.


<details>
  <summary>Details</summary>
Motivation: To address the emerging challenge of detecting automatically generated plagiarism in scientific articles using modern large language models, and to evaluate the robustness of detection approaches across different datasets.

Method: Created a large-scale dataset of automatically generated plagiarism using three LLMs (Llama, DeepSeek-R1, Mistral), compared participant results with four baselines, and evaluated approaches on the PAN 2015 dataset to assess generalizability.

Result: Semantic similarity approaches based on embedding vectors achieved up to 0.8 recall and 0.5 precision on the new dataset, but most approaches significantly underperformed on the 2015 dataset, showing poor generalization.

Conclusion: Current approaches lack diversity and generalizability, with naive semantic similarity methods performing well on the new dataset but failing on older datasets, indicating the need for more robust and generalized plagiarism detection methods.

Abstract: The generative plagiarism detection task at PAN 2025 aims at identifying
automatically generated textual plagiarism in scientific articles and aligning
them with their respective sources. We created a novel large-scale dataset of
automatically generated plagiarism using three large language models: Llama,
DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation
of this dataset, summarize and compare the results of all participants and four
baselines, and evaluate the results on the last plagiarism detection task from
PAN 2015 in order to interpret the robustness of the proposed approaches. We
found that the current iteration does not invite a large variety of approaches
as naive semantic similarity approaches based on embedding vectors provide
promising results of up to 0.8 recall and 0.5 precision. In contrast, most of
these approaches underperform significantly on the 2015 dataset, indicating a
lack in generalizability.

</details>


### [52] [BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods](https://arxiv.org/abs/2510.06811)
*Philipp Mondorf,Mingyang Wang,Sebastian Gerstner,Ahmad Dawar Hakimi,Yihong Liu,Leonor Veloso,Shijia Zhou,Hinrich Schütze,Barbara Plank*

Main category: cs.CL

TL;DR: This paper explores ensembling methods for circuit localization in LLMs, showing that both parallel and sequential ensembling improve performance on the MIB benchmark.


<details>
  <summary>Details</summary>
Motivation: To investigate whether combining multiple circuit localization methods can improve the accuracy of identifying subnetworks responsible for specific task behaviors in large language models.

Method: Two ensembling approaches: parallel ensembling (combining attribution scores via averaging, min, or max) and sequential ensembling (using EAP-IG as warm start for edge pruning).

Result: Both ensembling approaches yield notable gains on benchmark metrics, with parallel ensemble over various methods (including sequential ensemble) achieving the best results.

Conclusion: Ensembling circuit localization methods leads to more precise circuit identification, with the combination of multiple methods providing optimal performance on the MIB benchmark.

Abstract: The Circuit Localization track of the Mechanistic Interpretability Benchmark
(MIB) evaluates methods for localizing circuits within large language models
(LLMs), i.e., subnetworks responsible for specific task behaviors. In this
work, we investigate whether ensembling two or more circuit localization
methods can improve performance. We explore two variants: parallel and
sequential ensembling. In parallel ensembling, we combine attribution scores
assigned to each edge by different methods-e.g., by averaging or taking the
minimum or maximum value. In the sequential ensemble, we use edge attribution
scores obtained via EAP-IG as a warm start for a more expensive but more
precise circuit identification method, namely edge pruning. We observe that
both approaches yield notable gains on the benchmark metrics, leading to a more
precise circuit identification approach. Finally, we find that taking a
parallel ensemble over various methods, including the sequential ensemble,
achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB
Shared Task, comparing ensemble scores to official baselines across multiple
model-task combinations.

</details>


### [53] [Adaptive Tool Generation with Models as Tools and Reinforcement Learning](https://arxiv.org/abs/2510.06825)
*Chenpeng Wang,Xiaojie Cheng,Chunye Wang,Linfeng Yang,Lei Zhang*

Main category: cs.CL

TL;DR: MTR is a simulation-first training framework for tool-augmented reasoning that learns from complete ReAct traces with simulated observations instead of live APIs, achieving competitive performance on multi-hop QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: Tool-augmented language models rely on live API access which creates scalability and reliability challenges during training and deployment.

Method: Uses multi-agent architecture with ToolMaker, AutoAgent, and ToolActor to generate simulated observations. Training involves Stage-1 SFT for 'trace grammar' and Stage-2 GRPO with composite trace reward balancing correctness and consistency.

Result: Attains competitive Exact Match scores to live-API systems across four multi-hop QA benchmarks (HotpotQA, MuSiQue, 2WikiMultiHopQA, Bamboogle) and excels on reasoning-intensive tasks.

Conclusion: Effective tool reasoning can be learned from structured traces without live interactions.

Abstract: Tool-augmented language models have demonstrated strong capabilities, but
their reliance on live API access creates scalability and reliability
challenges during training and deployment. We propose MTR, a simulation-first
training framework for tool-augmented reasoning. Instead of relying on live
APIs, MTR learns from complete ReAct traces with schema-validated, simulated
observations. Our approach operates through a multi-agent architecture where a
ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an
AutoAgent produces structured think-act-observe sequences, and a ToolActor
simulates realistic responses. Training proceeds in two stages: Stage-1
Supervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning
sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy
with a composite trace reward that balances answer correctness and internal
consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue,
2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to
live-API systems and excels on reasoning-intensive tasks, suggesting that
effective tool reasoning can be learned from structured traces without live
interactions.

</details>


### [54] [Mid-Training of Large Language Models: A Survey](https://arxiv.org/abs/2510.06826)
*Kaixiang Mo,Yuxin Shi,Weiwei Weng,Zhiqiang Zhou,Shuman Liu,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

TL;DR: This paper introduces the first comprehensive survey of mid-training as a unified paradigm in LLM development, providing taxonomy, practical insights, benchmarks, and identifying future research directions.


<details>
  <summary>Details</summary>
Motivation: Despite widespread use in state-of-the-art systems, there has been no prior survey of mid-training as a unified paradigm to understand its effectiveness and systematize approaches.

Method: The authors introduce the first taxonomy of LLM mid-training spanning data distribution, learning-rate scheduling, and long-context extension, and compile evaluation benchmarks for structured comparisons.

Result: The paper distills practical insights, reports gains from mid-training approaches, and enables structured comparisons across models through compiled benchmarks.

Conclusion: Mid-training is an important intermediate stage that mitigates diminishing returns, stabilizes convergence, and expands model capability, with identified open challenges and proposed future research avenues.

Abstract: Large language models (LLMs) are typically developed through large-scale
pre-training followed by task-specific fine-tuning. Recent advances highlight
the importance of an intermediate mid-training stage, where models undergo
multiple annealing-style phases that refine data quality, adapt optimization
schedules, and extend context length. This stage mitigates diminishing returns
from noisy tokens, stabilizes convergence, and expands model capability in late
training. Its effectiveness can be explained through gradient noise scale, the
information bottleneck, and curriculum learning, which together promote
generalization and abstraction. Despite widespread use in state-of-the-art
systems, there has been no prior survey of mid-training as a unified paradigm.
We introduce the first taxonomy of LLM mid-training spanning data distribution,
learning-rate scheduling, and long-context extension. We distill practical
insights, compile evaluation benchmarks, and report gains to enable structured
comparisons across models. We also identify open challenges and propose avenues
for future research and practice.

</details>


### [55] [GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics](https://arxiv.org/abs/2510.06841)
*Giorgos Filandrianos,Orfeas Menis Mastromichalakis,Wafaa Mohammed,Giuseppe Attanasio,Chrysoula Zerva*

Main category: cs.CL

TL;DR: This paper introduces a large-scale challenge set to analyze gender bias in automatic quality estimation (QE) metrics across 33 language pairs, focusing on gender-ambiguous occupational terms.


<details>
  <summary>Details</summary>
Motivation: Gender bias in machine translation has been well-documented, but bias in automatic quality estimation metrics remains underexplored, with existing studies limited by small datasets and narrow coverage.

Method: Built on the GAMBIT corpus, the authors extend coverage to three source languages (genderless or natural-gendered) and eleven target languages with grammatical gender, creating parallel texts where only the grammatical gender of occupational terms differs between masculine and feminine versions.

Result: The dataset enables fine-grained bias analysis by occupation and systematic comparisons across languages, with the expectation that unbiased QE metrics should assign equal scores to both gender versions.

Conclusion: The large-scale, fully parallel dataset provides a comprehensive framework for detecting and analyzing gender bias in quality estimation metrics across diverse language pairs and occupational contexts.

Abstract: Gender bias in machine translation (MT) systems has been extensively
documented, but bias in automatic quality estimation (QE) metrics remains
comparatively underexplored. Existing studies suggest that QE metrics can also
exhibit gender bias, yet most analyses are limited by small datasets, narrow
occupational coverage, and restricted language variety. To address this gap, we
introduce a large-scale challenge set specifically designed to probe the
behavior of QE metrics when evaluating translations containing gender-ambiguous
occupational terms. Building on the GAMBIT corpus of English texts with
gender-ambiguous occupations, we extend coverage to three source languages that
are genderless or natural-gendered, and eleven target languages with
grammatical gender, resulting in 33 source-target language pairs. Each source
text is paired with two target versions differing only in the grammatical
gender of the occupational term(s) (masculine vs. feminine), with all dependent
grammatical elements adjusted accordingly. An unbiased QE metric should assign
equal or near-equal scores to both versions. The dataset's scale, breadth, and
fully parallel design, where the same set of texts is aligned across all
languages, enables fine-grained bias analysis by occupation and systematic
comparisons across languages.

</details>


### [56] [SID: Multi-LLM Debate Driven by Self Signals](https://arxiv.org/abs/2510.06843)
*Xuhang Chen,Zhifan Song,Deyi Ji,Shuo Gao,Lanyun Zhu*

Main category: cs.CL

TL;DR: The paper introduces SID, a Self-Signals Driven Multi-LLM Debate method that uses model-level confidence and token-level semantic focus to optimize multi-agent debate systems, improving both accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing Multi-LLM Agent Debate methods focus on external structures and neglect self signals like token logits and attention, leading to redundant computation and potential performance degradation.

Method: SID leverages two types of self-signals: model-level confidence for early exit of high-confidence agents, and token-level semantic focus based on attention mechanism to compress redundant debate contents.

Result: Experimental results show SID outperforms existing MAD techniques in accuracy and reduces token consumption across various LLMs and Multimodal LLMs on multiple challenging benchmarks.

Conclusion: Utilizing self signals effectively enhances both performance and efficiency of multi-agent debate systems, demonstrating the value of internal model signals in optimizing collaborative AI systems.

Abstract: Large Language Models (LLMs) have exhibited impressive capabilities across
diverse application domains. Recent work has explored Multi-LLM Agent Debate
(MAD) as a way to enhance performance by enabling multiple LLMs to discuss and
refine responses iteratively. Nevertheless, existing MAD methods predominantly
focus on utilizing external structures, such as debate graphs, using
LLM-as-a-Judge, while neglecting the application of self signals, such as token
logits and attention, that arise during generation. This omission leads to
redundant computation and potential performance degradation. In this paper, we
shift the focus to the self signals of multi-LLM debate and introduce a
Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of
self-signals: model-level confidence and token-level semantic focus, to
adaptively guide the debate process. Our approach enables high-confidence
agents to exit early at the model level and compress the redundant debate
contents based on the attention mechanism. We evaluate our method on various
LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental
results demonstrate that our method not only outperforms existing MAD
techniques in accuracy but also reduces token consumption, highlighting the
effectiveness of utilizing self signals in enhancing both the performance and
efficiency of multi-agent debate systems. Our code will be available
at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.

</details>


### [57] [OpenJAI-v1.0: An Open Thai Large Language Model](https://arxiv.org/abs/2510.06847)
*Pontakorn Trakuekul,Attapol T. Rutherford,Jullajak Karnjanaekarin,Narongkorn Panitsrisit,Sumana Sumanakul*

Main category: cs.CL

TL;DR: OpenJAI-v1.0 is an open-source Thai-English LLM based on Qwen3-14B, enhanced for instruction following, long-context understanding, and tool use.


<details>
  <summary>Details</summary>
Motivation: To provide an improved open-source NLP resource for the Thai AI community by boosting performance on practical tasks.

Method: Developed from Qwen3-14B model using carefully curated data across three key use cases: instruction following, long-context understanding, and tool use.

Result: Outperforms other leading open-source Thai models on diverse benchmarks while avoiding catastrophic forgetting, improving on base model capabilities.

Conclusion: OpenJAI-v1.0 is publicly released as an alternative NLP resource for the Thai AI community.

Abstract: We introduce OpenJAI-v1.0, an open-source large language model for Thai and
English, developed from the Qwen3-14B model. Our work focuses on boosting
performance on practical tasks through carefully curated data across three key
use cases: instruction following, long-context understanding, and tool use.
Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its
base model and outperforms other leading open-source Thai models on a diverse
suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is
publicly released as another alternative NLP resource for the Thai AI
community.

</details>


### [58] [Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding](https://arxiv.org/abs/2510.06866)
*Wafaa Mohammed,Vlad Niculae,Chrysoula Zerva*

Main category: cs.CL

TL;DR: LLMs struggle with discourse phenomena in translation. This study investigates their performance, shows discourse knowledge is encoded in LLMs, and proposes Quality-Aware Decoding (QAD) to extract this knowledge effectively, improving translation quality and human alignment.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are strong in machine translation but struggle with discourse phenomena like pronoun resolution and lexical cohesion at document level.

Method: Thorough investigation of LLMs' discourse phenomena performance in context-aware translation, demonstrating discourse knowledge encoding within LLMs and proposing Quality-Aware Decoding (QAD) to extract this knowledge.

Result: QAD shows superiority over other decoding approaches, enhances semantic richness of translations, and aligns them more closely with human preferences.

Conclusion: Quality-Aware Decoding effectively extracts discourse knowledge from LLMs, improving translation quality and better matching human preferences.

Abstract: Large language models (LLMs) have emerged as strong contenders in machine
translation.Yet, they still struggle to adequately handle discourse phenomena,
such as pronoun resolution and lexical cohesion at the document level. In this
study, we thoroughly investigate the discourse phenomena performance of LLMs in
context-aware translation. We demonstrate that discourse knowledge is encoded
within LLMs and propose the use of quality-aware decoding (QAD) to effectively
extract this knowledge, showcasing its superiority over other decoding
approaches through comprehensive analysis. Furthermore, we illustrate that QAD
enhances the semantic richness of translations and aligns them more closely
with human preferences.

</details>


### [59] [$λ$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences](https://arxiv.org/abs/2510.06870)
*Yining Wang,Jinman Zhao,Chuangxin Zhao,Shuhao Guan,Gerald Penn,Shinan Liu*

Main category: cs.CL

TL;DR: The paper introduces λ-GRPO, a method that addresses length bias in RLHF by learning adaptive token-level preferences during optimization, achieving consistent improvements in mathematical reasoning benchmarks without additional computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing RLHF methods like GRPO suffer from length bias where longer responses disproportionately influence gradient updates, and current variants like DAPO and Dr. GRPO lack interpretability and remain heuristic in their token preference handling.

Method: The authors unify existing frameworks under a single formulation and introduce a learnable parameter λ that adaptively controls token-level weighting during optimization, allowing the model to learn its own token preferences.

Result: λ-GRPO achieves consistent improvements over vanilla GRPO and DAPO on multiple mathematical reasoning benchmarks, with average accuracy improvements of +1.9%, +1.0%, and +1.7% on Qwen2.5 models with 1.5B, 3B, and 7B parameters respectively.

Conclusion: The method demonstrates that learning token preferences during optimization is effective and practical, providing performance gains without modifications to training data or additional computational costs.

Abstract: Reinforcement Learning with Human Feedback (RLHF) has been the dominant
approach for improving the reasoning capabilities of Large Language Models
(LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has
simplified this paradigm by replacing the reward and value models with
rule-based verifiers. A prominent example is Group Relative Policy Optimization
(GRPO). However, GRPO inherently suffers from a length bias, since the same
advantage is uniformly assigned to all tokens of a response. As a result,
longer responses distribute the reward over more tokens and thus contribute
disproportionately to gradient updates. Several variants, such as DAPO and Dr.
GRPO, modify the token-level aggregation of the loss, yet these methods remain
heuristic and offer limited interpretability regarding their implicit token
preferences. In this work, we explore the possibility of allowing the model to
learn its own token preference during optimization. We unify existing
frameworks under a single formulation and introduce a learnable parameter
$\lambda$ that adaptively controls token-level weighting. We use $\lambda$-GRPO
to denote our method, and we find that $\lambda$-GRPO achieves consistent
improvements over vanilla GRPO and DAPO on multiple mathematical reasoning
benchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\lambda$-GRPO
improves average accuracy by $+1.9\%$, $+1.0\%$, and $+1.7\%$ compared to GRPO,
respectively. Importantly, these gains come without any modifications to the
training data or additional computational cost, highlighting the effectiveness
and practicality of learning token preferences.

</details>


### [60] [MeXtract: Light-Weight Metadata Extraction from Scientific Papers](https://arxiv.org/abs/2510.06889)
*Zaid Alyafeai,Maged S. Al-Shaibani,Bernard Ghanem*

Main category: cs.CL

TL;DR: MeXtract is a family of lightweight language models (0.5B-3B parameters) for metadata extraction from scientific papers, achieving SOTA performance on MOLE benchmark and showing strong transfer learning capabilities across schemas.


<details>
  <summary>Details</summary>
Motivation: Traditional metadata extraction methods struggle with generalization across domains and schema variations, creating a need for more robust and adaptable solutions.

Method: Fine-tuned Qwen 2.5 models to create MeXtract family, extended MOLE benchmark with model-specific metadata for out-of-domain evaluation.

Result: Achieved state-of-the-art performance on metadata extraction, demonstrated effective transfer to unseen schemas, showing robustness and adaptability.

Conclusion: Fine-tuning on specific schemas yields high accuracy and transfers well to new schemas, providing an effective solution for metadata extraction challenges.

Abstract: Metadata plays a critical role in indexing, documenting, and analyzing
scientific literature, yet extracting it accurately and efficiently remains a
challenging task. Traditional approaches often rely on rule-based or
task-specific models, which struggle to generalize across domains and schema
variations. In this paper, we present MeXtract, a family of lightweight
language models designed for metadata extraction from scientific papers. The
models, ranging from 0.5B to 3B parameters, are built by fine-tuning Qwen 2.5
counterparts. In their size family, MeXtract achieves state-of-the-art
performance on metadata extraction on the MOLE benchmark. To further support
evaluation, we extend the MOLE benchmark to incorporate model-specific
metadata, providing an out-of-domain challenging subset. Our experiments show
that fine-tuning on a given schema not only yields high accuracy but also
transfers effectively to unseen schemas, demonstrating the robustness and
adaptability of our approach. We release all the code, datasets, and models
openly for the research community.

</details>


### [61] [LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling](https://arxiv.org/abs/2510.06915)
*Zecheng Tang,Baibei Ji,Quantong Qiu,Haitian Wang,Xiaobo Liang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: Long-RewardBench is a benchmark for evaluating reward models on long-context scenarios, revealing that current models struggle with context-response consistency. The authors propose a multi-stage training strategy to create robust Long-context Reward Models (LongRMs) that outperform larger baselines.


<details>
  <summary>Details</summary>
Motivation: Current reward models are limited to short contexts and focus mainly on response-level attributes, neglecting long context-response consistency which is crucial for real-world applications like LLM agents.

Method: Proposed a multi-stage training strategy to scale arbitrary models into robust Long-context Reward Models (LongRMs), using the Long-RewardBench benchmark with Pairwise Comparison and Best-of-N tasks.

Result: The 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of proprietary Gemini 2.5 Pro model, while maintaining strong short-context capability.

Conclusion: The proposed multi-stage training approach effectively creates robust long-context reward models that address the critical gap in context-response consistency for long history trajectories.

Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM)
with human preferences. As real-world applications increasingly involve long
history trajectories, e.g., LLM agent, it becomes indispensable to evaluate
whether a model's responses are not only high-quality but also grounded in and
consistent with the provided context. Yet, current RMs remain confined to
short-context settings and primarily focus on response-level attributes (e.g.,
safety or helpfulness), while largely neglecting the critical dimension of long
context-response consistency. In this work, we introduce Long-RewardBench, a
benchmark specifically designed for long-context RM evaluation, featuring both
Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that
even state-of-the-art generative RMs exhibit significant fragility in
long-context scenarios, failing to maintain context-aware preference judgments.
Motivated by the analysis of failure patterns observed in model outputs, we
propose a general multi-stage training strategy that effectively scales
arbitrary models into robust Long-context RMs (LongRMs). Experiments show that
our approach not only substantially improves performance on long-context
evaluation but also preserves strong short-context capability. Notably, our 8B
LongRM outperforms much larger 70B-scale baselines and matches the performance
of the proprietary Gemini 2.5 Pro model.

</details>


### [62] [SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models](https://arxiv.org/abs/2510.06917)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: SHANKS is a framework that enables spoken language models to generate unspoken reasoning while listening to user speech, allowing real-time interruption and tool calls during conversation rather than waiting for turn completion.


<details>
  <summary>Details</summary>
Motivation: Current LLMs/SLMs only start thinking after users finish speaking, causing high latency that's unsuitable for speech-to-speech interaction where real-time exchange is crucial. Humans naturally "think while listening," which inspired this approach.

Method: SHANKS streams input speech in fixed chunks and generates unspoken chain-of-thought reasoning as each chunk arrives, using all previous speech and reasoning. This enables real-time interruption decisions and tool calls while the user continues speaking.

Result: In math problem scenarios, SHANKS achieved 37.1% higher interruption accuracy than baseline. In tool-augmented dialogues, it completed 56.9% of tool calls before users finished speaking. The framework enables thinking throughout conversation rather than only after turns.

Conclusion: SHANKS successfully moves toward models that think continuously during conversations, enabling real-time interaction by generating reasoning while listening, which is essential for low-latency speech-to-speech applications.

Abstract: Current large language models (LLMs) and spoken language models (SLMs) begin
thinking and taking actions only after the user has finished their turn. This
prevents the model from interacting during the user's turn and can lead to high
response latency while it waits to think. Consequently, thinking after
receiving the full input is not suitable for speech-to-speech interaction,
where real-time, low-latency exchange is important. We address this by noting
that humans naturally "think while listening." In this paper, we propose
SHANKS, a general inference framework that enables SLMs to generate unspoken
chain-of-thought reasoning while listening to the user input. SHANKS streams
the input speech in fixed-duration chunks and, as soon as a chunk is received,
generates unspoken reasoning based on all previous speech and reasoning, while
the user continues speaking. SHANKS uses this unspoken reasoning to decide
whether to interrupt the user and to make tool calls to complete the task. We
demonstrate that SHANKS enhances real-time user-SLM interaction in two
scenarios: (1) when the user is presenting a step-by-step solution to a math
problem, SHANKS can listen, reason, and interrupt when the user makes a
mistake, achieving 37.1% higher interruption accuracy than a baseline that
interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can
complete 56.9% of the tool calls before the user finishes their turn. Overall,
SHANKS moves toward models that keep thinking throughout the conversation, not
only after a turn ends. Animated illustrations of Shanks can be found at
https://d223302.github.io/SHANKS/

</details>


### [63] [Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation](https://arxiv.org/abs/2510.06961)
*Vaibhav Srivastav,Steven Zheng,Eric Bezzam,Eustache Le Bihan,Nithin Koluguri,Piotr Żelasko,Somshubra Majumdar,Adel Moumen,Sanchit Gandhi*

Main category: cs.CL

TL;DR: The Open ASR Leaderboard is a reproducible benchmark comparing 60+ ASR systems across 11 datasets, featuring multilingual and long-form tracks with standardized evaluation metrics including WER and RTFx.


<details>
  <summary>Details</summary>
Motivation: To address the saturation of ASR evaluation with short-form English and lack of efficiency reporting, providing fair accuracy-efficiency comparisons.

Method: Created a fully reproducible benchmark with standardized text normalization, evaluating systems across 11 datasets including dedicated multilingual and long-form tracks, reporting both WER and inverse real-time factor.

Result: Conformer encoders with LLM decoders achieve best average WER for English but are slower, while CTC and TDT decoders offer better RTFx. Whisper-derived encoders fine-tuned for English improve accuracy but reduce multilingual coverage.

Conclusion: The benchmark enables transparent ASR evaluation, revealing trade-offs between accuracy and efficiency, with all code and dataset loaders open-sourced for extensible evaluation.

Abstract: Despite rapid progress, ASR evaluation remains saturated with short-form
English, and efficiency is rarely reported. We present the Open ASR
Leaderboard, a fully reproducible benchmark and interactive leaderboard
comparing 60+ open-source and proprietary systems across 11 datasets, including
dedicated multilingual and long-form tracks. We standardize text normalization
and report both word error rate (WER) and inverse real-time factor (RTFx),
enabling fair accuracy-efficiency comparisons. For English transcription,
Conformer encoders paired with LLM decoders achieve the best average WER but
are slower, while CTC and TDT decoders deliver much better RTFx, making them
attractive for long-form and offline use. Whisper-derived encoders fine-tuned
for English improve accuracy but often trade off multilingual coverage. All
code and dataset loaders are open-sourced to support transparent, extensible
evaluation.

</details>


### [64] [EDUMATH: Generating Standards-aligned Educational Math Word Problems](https://arxiv.org/abs/2510.06965)
*Bryan R. Christ,Penelope Molitz,Jonathan Kropko,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: LLMs can generate customized math word problems (MWPs) that match student interests and education standards, with teacher-annotated data enabling smaller models to outperform larger ones and students preferring customized MWPs.


<details>
  <summary>Details</summary>
Motivation: Teachers lack time to customize MWPs for individual students despite the educational benefits, and LLMs could help automate this personalization process.

Method: Used joint human expert-LLM evaluation of 11,000+ MWPs, created teacher-annotated dataset, trained 12B and 30B open models, and conducted student study comparing customized vs human-written MWPs.

Result: 12B model matched larger models' performance, 30B model outperformed closed baselines without training, generated MWPs were more human-like, and students performed similarly but preferred customized MWPs.

Conclusion: LLMs can effectively generate standards-aligned, customized MWPs that students prefer, providing scalable support for personalized math education.

Abstract: Math word problems (MWPs) are critical K-12 educational tools, and
customizing them to students' interests and ability levels can increase
learning outcomes. However, teachers struggle to find time to customize MWPs
for each student given large class sizes and increasing burnout. We propose
that LLMs can support math education by generating MWPs customized to student
interests and math education standards. To this end, we use a joint human
expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and
closed LLMs and develop the first teacher-annotated dataset for
standards-aligned educational MWP generation. We show the value of our data by
using it to train a 12B open model that matches the performance of larger and
more capable open models. We also use our teacher-annotated data to train a
text classifier that enables a 30B open LLM to outperform existing closed
baselines without any training. Next, we show our models' MWPs are more similar
to human-written MWPs than those from existing models. We conclude by
conducting the first study of customized LLM-generated MWPs with grade school
students, finding they perform similarly on our models' MWPs relative to
human-written MWPs but consistently prefer our customized MWPs.

</details>


### [65] [Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups](https://arxiv.org/abs/2510.06974)
*Geng Liu,Feng Li,Junjie Mu,Mengxiao Zhu,Francesco Pierri*

Main category: cs.CL

TL;DR: The study investigates social identity biases in Chinese LLMs, finding systematic ingroup-positive and outgroup-negative tendencies across 10 models using Mandarin prompts and real conversation data.


<details>
  <summary>Details</summary>
Motivation: To address concerns about LLMs reflecting and amplifying social biases in user-facing applications, particularly in Chinese language contexts.

Method: Used Mandarin-specific prompts across 10 Chinese LLMs, evaluated responses to ingroup ("We") and outgroup ("They") framings across 240 social groups, and analyzed real chatbot conversations.

Result: Found systematic ingroup-positive and outgroup-negative tendencies across all models, with biases appearing in both controlled experiments and naturalistic dialogue, intensifying in real interactions.

Conclusion: Social identity biases documented in English LLMs generalize cross-linguistically to Chinese models and strengthen in user-facing contexts, providing a language-aware evaluation framework for Chinese LLMs.

Abstract: Large language models (LLMs) are increasingly deployed in user-facing
applications, raising concerns about their potential to reflect and amplify
social biases. We investigate social identity framing in Chinese LLMs using
Mandarin-specific prompts across ten representative Chinese LLMs, evaluating
responses to ingroup ("We") and outgroup ("They") framings, and extending the
setting to 240 social groups salient in the Chinese context. To complement
controlled experiments, we further analyze Chinese-language conversations from
a corpus of real interactions between users and chatbots. Across models, we
observe systematic ingroup-positive and outgroup-negative tendencies, which are
not confined to synthetic prompts but also appear in naturalistic dialogue,
indicating that bias dynamics might strengthen in real interactions. Our study
provides a language-aware evaluation framework for Chinese LLMs, demonstrating
that social identity biases documented in English generalize
cross-linguistically and intensify in user-facing contexts.

</details>


### [66] [Towards Reliable Retrieval in RAG Systems for Large Legal Datasets](https://arxiv.org/abs/2510.06999)
*Markus Reuter,Tobias Lingenberg,Rūta Liepiņa,Francesca Lagioia,Marco Lippi,Giovanni Sartor,Andrea Passerini,Burcu Sayin*

Main category: cs.CL

TL;DR: This paper introduces Summary-Augmented Chunking (SAC), a simple method that adds document-level summaries to text chunks to reduce Document-Level Retrieval Mismatch in legal RAG systems, improving retrieval accuracy without requiring legal domain expertise.


<details>
  <summary>Details</summary>
Motivation: Retrieval-Augmented Generation (RAG) for legal applications suffers from Document-Level Retrieval Mismatch (DRM) where retrievers select information from incorrect source documents due to structurally similar legal documents in large databases.

Method: The authors propose Summary-Augmented Chunking (SAC), which enhances each text chunk with a document-level synthetic summary to provide global context that standard chunking processes lose. They compare generic summarization with legal expert-targeted approaches.

Result: SAC significantly reduces DRM and improves both text-level retrieval precision and recall. Surprisingly, generic summarization outperformed legal expert domain knowledge approaches.

Conclusion: SAC is a practical, scalable, and easily integrable technique that enhances RAG system reliability for large-scale legal document datasets, with generic summarization being more effective than domain-specific approaches.

Abstract: Retrieval-Augmented Generation (RAG) is a promising approach to mitigate
hallucinations in Large Language Models (LLMs) for legal applications, but its
reliability is critically dependent on the accuracy of the retrieval step. This
is particularly challenging in the legal domain, where large databases of
structurally similar documents often cause retrieval systems to fail. In this
paper, we address this challenge by first identifying and quantifying a
critical failure mode we term Document-Level Retrieval Mismatch (DRM), where
the retriever selects information from entirely incorrect source documents. To
mitigate DRM, we investigate a simple and computationally efficient technique
which we refer to as Summary-Augmented Chunking (SAC). This method enhances
each text chunk with a document-level synthetic summary, thereby injecting
crucial global context that would otherwise be lost during a standard chunking
process. Our experiments on a diverse set of legal information retrieval tasks
show that SAC greatly reduces DRM and, consequently, also improves text-level
retrieval precision and recall. Interestingly, we find that a generic
summarization strategy outperforms an approach that incorporates legal expert
domain knowledge to target specific legal elements. Our work provides evidence
that this practical, scalable, and easily integrable technique enhances the
reliability of RAG systems when applied to large-scale legal document datasets.

</details>


### [67] [Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages](https://arxiv.org/abs/2510.07000)
*Neel Prabhanjan Rachamalla,Aravind Konakalla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: A human-in-the-loop pipeline combining translations with synthetic expansion creates reliable and diverse Indic post-training datasets (Pragyaan-IT and Pragyaan-Align) across 10 Indian languages, addressing gaps in multilingual coverage, cultural grounding, and task diversity.


<details>
  <summary>Details</summary>
Motivation: Existing open-source datasets lack multilingual coverage, cultural grounding, and suffer from task diversity gaps, especially for Indian languages, limiting the effectiveness of LLMs in these contexts.

Method: Human-in-the-loop pipeline that combines translations with synthetic expansion to produce reliable and diverse Indic post-training data, curating two datasets across 10 Indian languages covering 13 broad and 56 sub-categories from 57 diverse datasets.

Result: Created Pragyaan-IT (22.5K) and Pragyaan-Align (100K) datasets across 10 Indian languages with enhanced task diversity, multi-turn dialogue, instruction fidelity, safety alignment, and preservation of cultural nuance.

Conclusion: This approach provides a foundation for more inclusive and effective multilingual LLMs by addressing critical gaps in existing datasets and emphasizing often-overlooked dimensions like cultural preservation and task diversity.

Abstract: The effectiveness of Large Language Models (LLMs) depends heavily on the
availability of high-quality post-training data, particularly
instruction-tuning and preference-based examples. Existing open-source
datasets, however, often lack multilingual coverage, cultural grounding, and
suffer from task diversity gaps that are especially pronounced for Indian
languages. We introduce a human-in-the-loop pipeline that combines translations
with synthetic expansion to produce reliable and diverse Indic post-training
data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and
Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56
sub-categories, leveraging 57 diverse datasets. Our dataset protocol
incorporates several often-overlooked dimensions and emphasize task diversity,
multi-turn dialogue, instruction fidelity, safety alignment, and preservation
of cultural nuance, providing a foundation for more inclusive and effective
multilingual LLMs.

</details>


### [68] [Native Hybrid Attention for Efficient Sequence Modeling](https://arxiv.org/abs/2510.07019)
*Jusen Du,Jiaxi Hu,Tao Zhang,Weigao Sun,Yu Cheng*

Main category: cs.CL

TL;DR: Native Hybrid Attention (NHA) is a hybrid architecture combining linear and full attention with intra & inter-layer hybridization, using linear RNN for long-term context and sliding window for short-term tokens, achieving better efficiency and accuracy than Transformers.


<details>
  <summary>Details</summary>
Motivation: Transformers have quadratic complexity issues, while linear attention improves efficiency but sacrifices recall accuracy over long contexts. There's a need for a solution that balances efficiency and accuracy.

Method: NHA integrates linear RNN for long-term context in key-value slots with short-term tokens from sliding window, applying single softmax attention over all keys/values without extra fusion parameters. Uses sliding window size hyperparameter to control inter-layer behavior.

Result: NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Pretrained LLMs hybridized with NHA achieve competitive accuracy with significant efficiency gains.

Conclusion: NHA provides an effective hybrid attention architecture that balances efficiency and accuracy, enabling smooth adjustment between linear and full attention while maintaining structural uniformity across layers.

Abstract: Transformers excel at sequence modeling but face quadratic complexity, while
linear attention offers improved efficiency but often compromises recall
accuracy over long contexts. In this work, we introduce Native Hybrid Attention
(NHA), a novel hybrid architecture of linear and full attention that integrates
both intra \& inter-layer hybridization into a unified layer design. NHA
maintains long-term context in key-value slots updated by a linear RNN, and
augments them with short-term tokens from a sliding window. A single
\texttt{softmax attention} operation is then applied over all keys and values,
enabling per-token and per-head context-dependent weighting without requiring
additional fusion parameters. The inter-layer behavior is controlled through a
single hyperparameter, the sliding window size, which allows smooth adjustment
between purely linear and full attention while keeping all layers structurally
uniform. Experimental results show that NHA surpasses Transformers and other
hybrid baselines on recall-intensive and commonsense reasoning tasks.
Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving
competitive accuracy while delivering significant efficiency gains. Code is
available at https://github.com/JusenD/NHA.

</details>


### [69] [Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge](https://arxiv.org/abs/2510.07024)
*Shrestha Ghosh,Luca Giordano,Yujia Hu,Tuan-Phong Nguyen,Simon Razniewski*

Main category: cs.CL

TL;DR: Analysis of GPT-4.1's factual knowledge reveals significant differences from established knowledge bases, lower accuracy than benchmarked, and major issues with inconsistency, ambiguity, and hallucinations.


<details>
  <summary>Details</summary>
Motivation: To deeply understand the factual knowledge of frontier LLMs, which remains poorly understood and is usually analyzed from biased samples.

Method: Analyzed GPTKB v1.5, a recursively elicited set of 100 million beliefs from GPT-4.1, one of the strongest currently available frontier LLMs.

Result: Models' factual knowledge differs significantly from established knowledge bases; accuracy is significantly lower than indicated by previous benchmarks; inconsistency, ambiguity and hallucinations are major issues.

Conclusion: The findings shed light on future research opportunities concerning factual LLM knowledge, highlighting the need for better understanding and improvement of factual accuracy in LLMs.

Abstract: LLMs are remarkable artifacts that have revolutionized a range of NLP and AI
tasks. A significant contributor is their factual knowledge, which, to date,
remains poorly understood, and is usually analyzed from biased samples. In this
paper, we take a deep tour into the factual knowledge (or beliefs) of a
frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited
set of 100 million beliefs of one of the strongest currently available frontier
LLMs, GPT-4.1. We find that the models' factual knowledge differs quite
significantly from established knowledge bases, and that its accuracy is
significantly lower than indicated by previous benchmarks. We also find that
inconsistency, ambiguity and hallucinations are major issues, shedding light on
future research opportunities concerning factual LLM knowledge.

</details>


### [70] [Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models](https://arxiv.org/abs/2510.07037)
*Rajvee Sheth,Samridhi Raj Sinha,Mahavir Patil,Himanshu Beniwal,Mayank Singh*

Main category: cs.CL

TL;DR: This survey provides the first comprehensive analysis of code-switching (CSW) in large language models (LLMs), covering over 80 studies across 5 research areas, 12 NLP tasks, 30+ datasets, and 80+ languages. It classifies advances by architecture, training strategy, and evaluation methodology, highlighting persistent challenges and providing a roadmap for future research.


<details>
  <summary>Details</summary>
Motivation: Code-switching remains a fundamental challenge for multilingual NLP despite advances in LLMs. Most LLMs struggle with mixed-language inputs, limited CSW datasets, and evaluation biases, hindering deployment in multilingual societies where code-switching is common.

Method: The survey conducts comprehensive analysis of CSW-aware LLM research by reviewing over 80 studies, classifying recent advances by architecture, training strategy, and evaluation methodology across multiple research areas, NLP tasks, datasets, and languages.

Result: The survey provides a systematic classification of CSW research in LLMs, outlining how LLMs have reshaped CSW modeling while identifying persistent challenges in handling mixed-language inputs and evaluation biases.

Conclusion: The paper concludes with a roadmap emphasizing the need for inclusive datasets, fair evaluation, and linguistically grounded models to achieve truly multilingual intelligence. A curated collection of resources is maintained at the provided GitHub repository.

Abstract: Code-switching (CSW), the alternation of languages and scripts within a
single utterance, remains a fundamental challenge for multiling ual NLP, even
amidst the rapid advances of large language models (LLMs). Most LLMs still
struggle with mixed-language inputs, limited CSW datasets, and evaluation
biases, hindering deployment in multilingual societies. This survey provides
the first comprehensive analysis of CSW-aware LLM research, reviewing
\total{unique_references} studies spanning five research areas, 12 NLP tasks,
30+ datasets, and 80+ languages. We classify recent advances by architecture,
training strategy, and evaluation methodology, outlining how LLMs have reshaped
CSW modeling and what challenges persist. The paper concludes with a roadmap
emphasizing the need for inclusive datasets, fair evaluation, and
linguistically grounded models to achieve truly multilingual intelligence. A
curated collection of all resources is maintained at
https://github.com/lingo-iitgn/awesome-code-mixing/.

</details>


### [71] [Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models](https://arxiv.org/abs/2510.07048)
*Yuntao Gui,James Cheng*

Main category: cs.CL

TL;DR: Search-R3 is a novel framework that adapts LLMs to generate search embeddings directly from their reasoning process, combining supervised learning, reinforcement learning, and specialized training environments to outperform prior methods on complex retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models have strong natural language understanding but are underutilized for retrieval tasks. The authors aim to leverage LLMs' chain-of-thought reasoning capabilities to produce more effective search embeddings.

Method: Three-stage approach: (1) supervised learning for quality embeddings, (2) reinforcement learning to optimize embedding generation with reasoning, (3) specialized RL environment that handles evolving embeddings without full corpus re-encoding.

Result: Extensive evaluations on diverse benchmarks show Search-R3 significantly outperforms prior methods by unifying reasoning and embedding generation processes.

Conclusion: The integrated post-training approach represents substantial advancement for complex knowledge-intensive tasks requiring both sophisticated reasoning and effective information retrieval.

Abstract: Despite their remarkable natural language understanding capabilities, Large
Language Models (LLMs) have been underutilized for retrieval tasks. We present
Search-R3, a novel framework that addresses this limitation by adapting LLMs to
generate search embeddings as a direct output of their reasoning process. Our
approach exploits LLMs' chain-of-thought capabilities, allowing them to produce
more effective embeddings by reasoning step-by-step through complex semantic
analyses. We implement this through three complementary mechanisms. (1) a
supervised learning stage enables the model's ability to produce quality
embeddings, (2) a reinforcement learning (RL) methodology that optimizes
embedding generation alongside reasoning, and (3) a specialized RL environment
that efficiently handles evolving embedding representations without requiring
complete corpus re-encoding at each training iteration. Our extensive
evaluations on diverse benchmarks demonstrate that Search-R3 significantly
outperforms prior methods by unifying the reasoning and embedding generation
processes. This integrated post-training approach represents a substantial
advancement in handling complex knowledge-intensive tasks that require both
sophisticated reasoning and effective information retrieval. Project page:
https://github.com/ytgui/Search-R3

</details>


### [72] [Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations](https://arxiv.org/abs/2510.07060)
*Miriam Wanner,Sophia Hager,Anjalie Field*

Main category: cs.CL

TL;DR: Sinclair Broadcast Group's acquisition of local news stations leads to increased national news coverage and more polarizing topics, reducing local content.


<details>
  <summary>Details</summary>
Motivation: To investigate how Sinclair's acquisition affects local news coverage, specifically changes in content focus and politicization.

Method: Computational analysis of internet content from local news stations before and after acquisition, compared to national outlets.

Result: Clear evidence of increased national news reporting and more polarizing topics, with reduced local coverage.

Conclusion: Sinclair's acquisition shifts local news towards national, politicized content, undermining their traditional role as trusted local sources.

Abstract: Local news stations are often considered to be reliable sources of
non-politicized information, particularly local concerns that residents care
about. Because these stations are trusted news sources, viewers are
particularly susceptible to the information they report. The Sinclair Broadcast
group is a broadcasting company that has acquired many local news stations in
the last decade. We investigate the effects of local news stations being
acquired by Sinclair: how does coverage change? We use computational methods to
investigate changes in internet content put out by local news stations before
and after being acquired by Sinclair and in comparison to national news
outlets. We find that there is clear evidence that local news stations report
more frequently on national news at the expense of local topics, and that their
coverage of polarizing national topics increases.

</details>


### [73] [Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages](https://arxiv.org/abs/2510.07061)
*Amir Hossein Yari,Kalmit Kulkarni,Ahmad Raza Khan,Fajri Koto*

Main category: cs.CL

TL;DR: ITEM is a benchmark evaluating 26 automatic metrics' alignment with human judgments across 6 Indian languages, revealing LLM-based evaluators perform best, outliers significantly impact agreement, and metrics differ in content fidelity vs fluency capture between summarization and translation tasks.


<details>
  <summary>Details</summary>
Motivation: Current automatic metrics are developed and validated primarily for English and high-resource languages, leaving Indian languages (spoken by 1.5B+ people) overlooked and questioning the universality of evaluation practices.

Method: Created ITEM benchmark with fine-grained annotations to systematically evaluate 26 automatic metrics' alignment with human judgments across 6 major Indian languages, covering agreement, outlier sensitivity, language-specific reliability, inter-metric correlations, and perturbation resilience.

Result: Four key findings: (1) LLM-based evaluators show strongest human alignment; (2) outliers significantly impact metric-human agreement; (3) TS metrics better capture content fidelity while MT metrics better reflect fluency; (4) metrics differ in robustness to perturbations.

Conclusion: The findings provide critical guidance for advancing metric design and evaluation specifically for Indian languages, addressing the current gap in multilingual evaluation practices.

Abstract: While automatic metrics drive progress in Machine Translation (MT) and Text
Summarization (TS), existing metrics have been developed and validated almost
exclusively for English and other high-resource languages. This narrow focus
leaves Indian languages, spoken by over 1.5 billion people, largely overlooked,
casting doubt on the universality of current evaluation practices. To address
this gap, we introduce ITEM, a large-scale benchmark that systematically
evaluates the alignment of 26 automatic metrics with human judgments across six
major Indian languages, enriched with fine-grained annotations. Our extensive
evaluation, covering agreement with human judgments, sensitivity to outliers,
language-specific reliability, inter-metric correlations, and resilience to
controlled perturbations, reveals four central findings: (1) LLM-based
evaluators show the strongest alignment with human judgments at both segment
and system levels; (2) outliers exert a significant impact on metric-human
agreement; (3) in TS, metrics are more effective at capturing content fidelity,
whereas in MT, they better reflect fluency; and (4) metrics differ in their
robustness and sensitivity when subjected to diverse perturbations.
Collectively, these findings offer critical guidance for advancing metric
design and evaluation in Indian languages.

</details>


### [74] [LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish](https://arxiv.org/abs/2510.07074)
*Fred Philippy,Laura Bernardy,Siwen Guo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: Cross-lingual instruction tuning for Luxembourgish using aligned English, French, and German data instead of machine translation, improving model performance while preserving linguistic and cultural nuances.


<details>
  <summary>Details</summary>
Motivation: Low-resource languages like Luxembourgish lack high-quality instruction datasets, and traditional machine translation approaches introduce semantic misalignment and cultural inaccuracies.

Method: Created cross-lingual instruction tuning dataset by leveraging aligned data from English, French, and German, avoiding machine-generated translations into Luxembourgish.

Result: Cross-lingual instruction tuning improves representational alignment across languages and enhances the model's generative capabilities in Luxembourgish.

Conclusion: Cross-lingual data curation avoids pitfalls of machine-translated data and directly benefits low-resource language development.

Abstract: Instruction tuning has become a key technique for enhancing the performance
of large language models, enabling them to better follow human prompts.
However, low-resource languages such as Luxembourgish face severe limitations
due to the lack of high-quality instruction datasets. Traditional reliance on
machine translation often introduces semantic misalignment and cultural
inaccuracies. In this work, we address these challenges by creating a
cross-lingual instruction tuning dataset for Luxembourgish, without resorting
to machine-generated translations into it. Instead, by leveraging aligned data
from English, French, and German, we build a high-quality dataset that
preserves linguistic and cultural nuances. We provide evidence that
cross-lingual instruction tuning not only improves representational alignment
across languages but also the model's generative capabilities in Luxembourgish.
This highlights how cross-lingual data curation can avoid the common pitfalls
of machine-translated data and directly benefit low-resource language
development.

</details>


### [75] [Accelerating Diffusion LLM Inference via Local Determinism Propagation](https://arxiv.org/abs/2510.07081)
*Fanheng Kong,Jingyuan Zhang,Yahui Liu,Zirui Wu,Yu Tian,Victoria W.,Guorui Zhou*

Main category: cs.CL

TL;DR: LocalLeap is a training-free adaptive parallel decoding strategy for diffusion LLMs that addresses delayed decoding inefficiencies by using local determinism propagation and progressive spatial consistency decay to achieve 6.94× throughput improvements with minimal performance impact.


<details>
  <summary>Details</summary>
Motivation: Existing open-source diffusion LLM implementations suffer from quality-speed trade-offs where conservative sampling strategies (greedy decoding) cause delayed decoding - repeated redundant refinement iterations that reduce inference efficiency.

Method: Proposes LocalLeap based on two principles: local determinism propagation around high-confidence anchors and progressive spatial consistency decay. Identifies anchors and performs localized relaxed parallel decoding within bounded neighborhoods to enable early commitment of determined tokens.

Result: Achieves 6.94× throughput improvements and reduces decoding steps to just 14.2% of original requirement across various benchmarks, with negligible performance impact on output quality.

Conclusion: LocalLeap effectively addresses delayed decoding in diffusion LLMs through adaptive parallel decoding, enabling substantial inference acceleration without compromising generation quality.

Abstract: Diffusion large language models (dLLMs) represent a significant advancement
in text generation, offering parallel token decoding capabilities. However,
existing open-source implementations suffer from quality-speed trade-offs that
impede their practical deployment. Conservative sampling strategies typically
decode only the most confident token per step to ensure quality (i.e., greedy
decoding), at the cost of inference efficiency due to repeated redundant
refinement iterations--a phenomenon we term delayed decoding. Through
systematic analysis of dLLM decoding dynamics, we characterize this delayed
decoding behavior and propose a training-free adaptive parallel decoding
strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built
on two fundamental empirical principles: local determinism propagation centered
on high-confidence anchors and progressive spatial consistency decay. By
applying these principles, LocalLeap identifies anchors and performs localized
relaxed parallel decoding within bounded neighborhoods, achieving substantial
inference step reduction through early commitment of already-determined tokens
without compromising output quality. Comprehensive evaluation on various
benchmarks demonstrates that LocalLeap achieves 6.94$\times$ throughput
improvements and reduces decoding steps to just 14.2\% of the original
requirement, achieving these gains with negligible performance impact. The
source codes are available at: https://github.com/friedrichor/LocalLeap.

</details>


### [76] [All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations](https://arxiv.org/abs/2510.07083)
*Miriam Wanner,Leif Azzopardi,Paul Thomas,Soham Dan,Benjamin Van Durme,Nick Craswell*

Main category: cs.CL

TL;DR: This paper introduces VITAL, a new set of metrics for evaluating LLM factuality that considers claim importance, addressing the limitation of existing methods that treat all claims equally.


<details>
  <summary>Details</summary>
Motivation: Existing factuality evaluation methods treat all claims as equally important, leading to misleading assessments when key information is missing or incorrect. Current approaches are insensitive to omitted or false key information.

Method: The authors constructed VITALERRORS benchmark with 6,733 queries containing minimally altered LLM responses that omit or falsify key information. They then introduced VITAL metrics that incorporate claim relevance and importance relative to the query.

Result: Analysis showed that existing evaluation metrics are insensitive to key information errors, while VITAL metrics more reliably detect errors in key information compared to previous methods.

Conclusion: The VITAL metrics, dataset, and analysis provide a foundation for more accurate and robust assessment of LLM factuality by properly weighting the importance of different claims.

Abstract: Existing methods for evaluating the factuality of large language model (LLM)
responses treat all claims as equally important. This results in misleading
evaluations when vital information is missing or incorrect as it receives the
same weight as peripheral details, raising the question: how can we reliably
detect such differences when there are errors in key information? Current
approaches that measure factuality tend to be insensitive to omitted or false
key information. To investigate this lack of sensitivity, we construct
VITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses
designed to omit or falsify key information. Using this dataset, we demonstrate
the insensitivities of existing evaluation metrics to key information errors.
To address this gap, we introduce VITAL, a set of metrics that provide greater
sensitivity in measuring the factuality of responses by incorporating the
relevance and importance of claims with respect to the query. Our analysis
demonstrates that VITAL metrics more reliably detect errors in key information
than previous methods. Our dataset, metrics, and analysis provide a foundation
for more accurate and robust assessment of LLM factuality.

</details>


### [77] [Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis](https://arxiv.org/abs/2510.07096)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: LLM-enhanced RAG framework for sarcasm-aware speech synthesis using semantic embeddings from fine-tuned LLaMA 3 and prosodic exemplars from RAG, integrated with VITS backbone.


<details>
  <summary>Details</summary>
Motivation: Sarcasm poses challenges for speech synthesis due to its nuanced semantic, contextual, and prosodic cues, and remains largely unexplored compared to broad emotional categories.

Method: Dual conditioning approach combining semantic embeddings from LoRA-fine-tuned LLaMA 3 (capturing pragmatic incongruity) and prosodic exemplars from RAG module, integrated within VITS backbone.

Result: Outperforms baselines in objective measures and subjective evaluations, with improvements in speech naturalness, sarcastic expressivity, and downstream sarcasm detection.

Conclusion: The proposed framework enables more natural and contextually appropriate sarcastic speech synthesis by effectively capturing both semantic and prosodic aspects of sarcasm.

Abstract: Sarcasm is a subtle form of non-literal language that poses significant
challenges for speech synthesis due to its reliance on nuanced semantic,
contextual, and prosodic cues. While existing speech synthesis research has
focused primarily on broad emotional categories, sarcasm remains largely
unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced
Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach
combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture
pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic
exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which
provide expressive reference patterns of sarcastic delivery. Integrated within
a VITS backbone, this dual conditioning enables more natural and contextually
appropriate sarcastic speech. Experiments demonstrate that our method
outperforms baselines in both objective measures and subjective evaluations,
yielding improvements in speech naturalness, sarcastic expressivity, and
downstream sarcasm detection.

</details>


### [78] [TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription](https://arxiv.org/abs/2510.07098)
*Guo Yutong,Wanying Wang,Yue Wu,Zichen Miao,Haoyu Wang*

Main category: cs.CL

TL;DR: TALENT is a lightweight framework that uses dual representations (OCR text + natural language narration) from a small VLM, combined with LLM reasoning, to achieve Table VQA performance comparable to large VLMs at much lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Large VLMs for Table VQA are computationally prohibitive for mobile deployment and miss fine-grained details, while existing OCR+LLM approaches using structured outputs like Markdown tables still introduce substantial errors.

Method: TALENT prompts a small VLM to produce both OCR text and natural language narration of tables, then combines these dual representations with the question for reasoning by an LLM, reframing Table VQA as an LLM-centric multimodal task.

Result: TALENT enables small VLM-LLM combinations to match or surpass single large VLMs at significantly lower computational cost on both public datasets and the newly constructed ReTabVQA dataset requiring multi-step quantitative reasoning.

Conclusion: The proposed dual-representation approach effectively separates perception from reasoning, making Table VQA more efficient and accurate while being suitable for mobile deployment.

Abstract: Table Visual Question Answering (Table VQA) is typically addressed by large
vision-language models (VLMs). While such models can answer directly from
images, they often miss fine-grained details unless scaled to very large sizes,
which are computationally prohibitive, especially for mobile deployment. A
lighter alternative is to have a small VLM perform OCR and then use a large
language model (LLM) to reason over structured outputs such as Markdown tables.
However, these representations are not naturally optimized for LLMs and still
introduce substantial errors. We propose TALENT (Table VQA via Augmented
Language-Enhanced Natural-text Transcription), a lightweight framework that
leverages dual representations of tables. TALENT prompts a small VLM to produce
both OCR text and natural language narration, then combines them with the
question for reasoning by an LLM. This reframes Table VQA as an LLM-centric
multimodal reasoning task, where the VLM serves as a perception-narration
module rather than a monolithic solver. Additionally, we construct ReTabVQA, a
more challenging Table VQA dataset requiring multi-step quantitative reasoning
over table images. Experiments show that TALENT enables a small VLM-LLM
combination to match or surpass a single large VLM at significantly lower
computational cost on both public datasets and ReTabVQA.

</details>


### [79] [Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning](https://arxiv.org/abs/2510.07105)
*Taylor Sorensen,Yejin Choi*

Main category: cs.CL

TL;DR: A system for modeling human variation in NLP tasks using LLMs' in-context learning with two-step meta-learning, winning the LeWiDi competition and showing importance of rater examples and model scale.


<details>
  <summary>Details</summary>
Motivation: Many NLP tasks involve subjectivity, ambiguity, or legitimate disagreement between annotators, requiring methods to model human variation.

Method: Leverages LLMs' in-context learning with two-step meta-learning: 1) post-training on many in-context datasets, 2) specializing via in-context meta-learning to specific data distributions.

Result: Won overall winner on both tasks in Learning With Disagreements (LeWiDi) competition. Ablation study showed rater examples in-context are crucial, dataset-specific fine-tuning helps on larger datasets, post-training helps on one competition dataset, and performance improves with model scale.

Conclusion: The proposed system effectively models human variation in NLP tasks through in-context learning and meta-learning, demonstrating strong performance in handling annotator disagreements.

Abstract: Many natural language processing (NLP) tasks involve subjectivity, ambiguity,
or legitimate disagreement between annotators. In this paper, we outline our
system for modeling human variation. Our system leverages language models'
(LLMs) in-context learning abilities, along with a two-step meta-learning
training procedure for 1) post-training on many datasets requiring in-context
learning and 2) specializing the model via in-context meta-learning to the
particular data distribution of interest. We also evaluate the performance of
our system submission to the Learning With Disagreements (LeWiDi) competition,
where it was the overall winner on both tasks. Additionally, we perform an
ablation study to measure the importance of each system component. We find that
including rater examples in-context is crucial for our system's performance,
dataset-specific fine-tuning is helpful on the larger datasets, post-training
on other in-context datasets is helpful on one of the competition datasets, and
that performance improves with model scale.

</details>


### [80] [TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning](https://arxiv.org/abs/2510.07118)
*Manish Nagaraj,Sakshi Choudhary,Utkarsh Saxena,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CL

TL;DR: TRIM is a forward-only, token-centric framework that uses attention-based fingerprints instead of gradients to select high-quality coresets for instruction tuning, achieving better performance than full-data fine-tuning in some cases with much lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing coreset selection methods for instruction tuning rely on computationally expensive gradient-based approaches that overlook fine-grained token-level features, making efficient high-quality dataset curation challenging.

Method: TRIM uses attention-based "fingerprints" from target samples to identify representational patterns, operating in a forward-only manner without requiring backward passes, making it computationally efficient and sensitive to structural task features.

Result: Coresets selected by TRIM outperform state-of-the-art baselines by up to 9% on downstream tasks and sometimes surpass full-data fine-tuning performance, while achieving this at a fraction of the computational cost.

Conclusion: TRIM establishes itself as a scalable and efficient alternative for building high-quality instruction-tuning datasets by avoiding expensive gradient computations while maintaining or improving performance.

Abstract: Instruction tuning is essential for aligning large language models (LLMs) to
downstream tasks and commonly relies on large, diverse corpora. However, small,
high-quality subsets, known as coresets, can deliver comparable or superior
results, though curating them remains challenging. Existing methods often rely
on coarse, sample-level signals like gradients, an approach that is
computationally expensive and overlooks fine-grained features. To address this,
we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a
forward-only, token-centric framework. Instead of using gradients, TRIM
operates by matching underlying representational patterns identified via
attention-based "fingerprints" from a handful of target samples. Such an
approach makes TRIM highly efficient and uniquely sensitive to the structural
features that define a task. Coresets selected by our method consistently
outperform state-of-the-art baselines by up to 9% on downstream tasks and even
surpass the performance of full-data fine-tuning in some settings. By avoiding
expensive backward passes, TRIM achieves this at a fraction of the
computational cost. These findings establish TRIM as a scalable and efficient
alternative for building high-quality instruction-tuning datasets.

</details>


### [81] [Comparing human and language models sentence processing difficulties on complex structures](https://arxiv.org/abs/2510.07141)
*Samuel Joseph Amouyal,Aya Meltzer-Asscher,Jonathan Berant*

Main category: cs.CL

TL;DR: LLMs struggle with garden path sentences similar to humans, showing convergence in sentence comprehension difficulties but divergence in overall performance patterns.


<details>
  <summary>Details</summary>
Motivation: To systematically compare human and LLM sentence comprehension across challenging linguistic structures, particularly examining whether LLMs experience human-like processing difficulties.

Method: Collected sentence comprehension data from humans and five families of state-of-the-art LLMs in a unified experimental framework, testing seven challenging linguistic structures including garden path sentences and their matched baselines.

Result: LLMs overall struggle on target structures, especially garden path sentences (46.8% accuracy for GPT-5 vs 93.7% on non-GP structures). Rank correlation between humans and models increases with parameter count. Performance gap patterns between target and baseline sentences show convergence for mid-range models but divergence for very weak or strong models.

Conclusion: The study reveals both convergence and divergence in human and LLM sentence comprehension, offering new insights into the similarity between humans and LLMs in processing challenging linguistic structures.

Abstract: Large language models (LLMs) that fluently converse with humans are a reality
- but do LLMs experience human-like processing difficulties? We systematically
compare human and LLM sentence comprehension across seven challenging
linguistic structures. We collect sentence comprehension data from humans and
five families of state-of-the-art LLMs, varying in size and training procedure
in a unified experimental framework. Our results show LLMs overall struggle on
the target structures, but especially on garden path (GP) sentences. Indeed,
while the strongest models achieve near perfect accuracy on non-GP structures
(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).
Additionally, when ranking structures based on average performance, rank
correlation between humans and models increases with parameter count. For each
target structure, we also collect data for their matched baseline without the
difficult structure. Comparing performance on the target vs. baseline
sentences, the performance gap observed in humans holds for LLMs, with two
exceptions: for models that are too weak performance is uniformly low across
both sentence types, and for models that are too strong the performance is
uniformly high. Together, these reveal convergence and divergence in human and
LLM sentence comprehension, offering new insights into the similarity of humans
and LLMs.

</details>


### [82] [Reasoning for Hierarchical Text Classification: The Case of Patents](https://arxiv.org/abs/2510.07167)
*Lekang Jiang,Wenjun Sun,Stephan Goetz*

Main category: cs.CL

TL;DR: RHC reformulates hierarchical text classification as step-by-step reasoning, using LLMs trained with CoT alignment and RL to achieve superior performance, explainability, scalability, and broad applicability.


<details>
  <summary>Details</summary>
Motivation: Automated patent classification is challenging due to domain complexity and large label sets, and existing methods lack explainability by only outputting flat labels without reasoning insights.

Method: Two-stage training: cold-start stage aligns LLM outputs with chain-of-thought reasoning format, followed by reinforcement learning stage to enhance multi-step reasoning ability for hierarchical label deduction.

Result: RHC outperforms baselines by ~3% in accuracy and macro F1, provides natural-language justifications, scales better with model size than standard fine-tuning, and achieves SOTA on multiple HTC benchmarks.

Conclusion: RHC effectively addresses hierarchical text classification challenges through reasoning-based approach, offering improved performance, explainability, and broad applicability beyond patent classification.

Abstract: Hierarchical text classification (HTC) assigns documents to multiple levels
of a pre-defined taxonomy. Automated patent subject classification represents
one of the hardest HTC scenarios because of domain knowledge difficulty and a
huge number of labels. Prior approaches only output a flat label set, which
offers little insight into the reason behind predictions. Therefore, we propose
Reasoning for Hierarchical Classification (RHC), a novel framework that
reformulates HTC as a step-by-step reasoning task to sequentially deduce
hierarchical labels. RHC trains large language models (LLMs) in two stages: a
cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning
format and a reinforcement learning (RL) stage to enhance multi-step reasoning
ability. RHC demonstrates four advantages in our experiments. (1)
Effectiveness: RHC surpasses previous baselines and outperforms the supervised
fine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)
Explainability: RHC produces natural-language justifications before prediction
to facilitate human inspection. (3) Scalability: RHC scales favorably with
model size with larger gains compared to standard fine-tuning. (4)
Applicability: Beyond patents, we further demonstrate that RHC achieves
state-of-the-art performance on other widely used HTC benchmarks, which
highlights its broad applicability.

</details>


### [83] [More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning](https://arxiv.org/abs/2510.07169)
*Yike Zhao,Simin Guo,Ziqing Yang,Shifan Han,Dahua Lin,Fei Tan*

Main category: cs.CL

TL;DR: Comprehensive analysis of mathematical reasoning datasets and synthesis methods, showing that better data quality and structure outweighs simply scaling data volume.


<details>
  <summary>Details</summary>
Motivation: LLM reasoning capabilities depend heavily on training data quality, but practical utility of various data construction methods in real-world pipelines remains underexplored.

Method: Conducted unified analysis of open-source datasets and data synthesis techniques for mathematical reasoning, evaluating them under a pipeline mirroring training and deployment scenarios.

Result: Found that structuring data in interpretable formats and distilling from stronger models often provides better results than simply scaling up data volume.

Conclusion: Provides actionable guidance for integrating training data to enhance LLM capabilities, supporting cost-effective data curation and scalable model enhancement.

Abstract: The reasoning capabilities of Large Language Models (LLMs) play a critical
role in many downstream tasks, yet depend strongly on the quality of training
data. Despite various proposed data construction methods, their practical
utility in real-world pipelines remains underexplored. In this work, we conduct
a comprehensive analysis of open-source datasets and data synthesis techniques
for mathematical reasoning, evaluating them under a unified pipeline designed
to mirror training and deployment scenarios. We further distill effective data
selection strategies and identify practical methods suitable for industrial
applications. Our findings highlight that structuring data in more
interpretable formats, or distilling from stronger models often outweighs
simply scaling up data volume. This study provides actionable guidance for
integrating training data to enhance LLM capabilities, supporting both
cost-effective data curation and scalable model enhancement. We hope this work
will inspire further research on how to balance "more data" versus "better
data" for real-world reasoning tasks.

</details>


### [84] [NurseLLM: The First Specialized Language Model for Nursing](https://arxiv.org/abs/2510.07173)
*Md Tawkat Islam Khondaker,Julia Harrington,Shady Shehata*

Main category: cs.CL

TL;DR: NurseLLM is the first nursing-specialized large language model designed for multiple choice question-answering tasks, outperforming comparable general-purpose and medical-specialized LLMs through a multi-stage data generation pipeline and specialized training.


<details>
  <summary>Details</summary>
Motivation: Large language models have transformed medical systems but their potential in specialized domains like nursing remains underexplored, creating a need for domain-specific models.

Method: Developed a multi-stage data generation pipeline to build the first large-scale nursing MCQ dataset, trained NurseLLM on broad nursing topics, and introduced multiple nursing benchmarks for evaluation.

Result: NurseLLM outperforms state-of-the-art general-purpose and medical-specialized LLMs of comparable size across different benchmarks, demonstrating the importance of specialized models for nursing.

Conclusion: Specialized LLMs are crucial for the nursing domain, and reasoning and multi-agent collaboration systems show promise for future nursing research and applications.

Abstract: Recent advancements in large language models (LLMs) have significantly
transformed medical systems. However, their potential within specialized
domains such as nursing remains largely underexplored. In this work, we
introduce NurseLLM, the first nursing-specialized LLM tailored for multiple
choice question-answering (MCQ) tasks. We develop a multi-stage data generation
pipeline to build the first large scale nursing MCQ dataset to train LLMs on a
broad spectrum of nursing topics. We further introduce multiple nursing
benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate
that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of
comparable size on different benchmarks, underscoring the importance of a
specialized LLM for the nursing domain. Finally, we explore the role of
reasoning and multi-agent collaboration systems in nursing, highlighting their
promise for future research and applications.

</details>


### [85] [Quantifying Data Contamination in Psychometric Evaluations of LLMs](https://arxiv.org/abs/2510.07175)
*Jongwook Han,Woojung Song,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: This paper proposes a framework to systematically measure data contamination in psychometric evaluations of LLMs, finding that popular inventories like BFI-44 and PVQ-40 show strong contamination where models memorize items and can adjust responses to achieve specific scores.


<details>
  <summary>Details</summary>
Motivation: Prior work raised concerns about data contamination from psychometric inventories in LLM evaluations, but there was no systematic attempt to quantify this contamination, which threatens the reliability of psychological assessments of LLMs.

Method: Proposed a framework to measure three aspects of data contamination: (1) item memorization, (2) evaluation memorization, and (3) target score matching. Applied this framework to 21 models from major families and four widely used psychometric inventories.

Result: Found that popular inventories such as Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40) exhibit strong contamination, where models not only memorize items but can also adjust their responses to achieve specific target scores.

Conclusion: The study provides evidence of significant data contamination in psychometric evaluations of LLMs, highlighting the need for more reliable assessment methods that account for this contamination issue.

Abstract: Recent studies apply psychometric questionnaires to Large Language Models
(LLMs) to assess high-level psychological constructs such as values,
personality, moral foundations, and dark traits. Although prior work has raised
concerns about possible data contamination from psychometric inventories, which
may threaten the reliability of such evaluations, there has been no systematic
attempt to quantify the extent of this contamination. To address this gap, we
propose a framework to systematically measure data contamination in
psychometric evaluations of LLMs, evaluating three aspects: (1) item
memorization, (2) evaluation memorization, and (3) target score matching.
Applying this framework to 21 models from major families and four widely used
psychometric inventories, we provide evidence that popular inventories such as
the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)
exhibit strong contamination, where models not only memorize items but can also
adjust their responses to achieve specific target scores.

</details>


### [86] [CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models](https://arxiv.org/abs/2510.07177)
*Yong-En Tian,Yu-Chien Tang,An-Zi Yen,Wen-Chih Peng*

Main category: cs.CL

TL;DR: This paper introduces CARPAS, a new task for content-aware refinement of provided aspects in aspect-based summarization, where LLMs dynamically adjust input aspects based on document context before generating summaries.


<details>
  <summary>Details</summary>
Motivation: Real-world scenarios often present challenges where predefined aspects for summarization may be incomplete, irrelevant, or missing from documents, requiring systems to adaptively refine aspects based on actual content.

Method: Proposed CARPAS task with three new datasets; used LLMs with four prompting strategies; introduced preliminary subtask to predict number of relevant aspects to guide LLMs and reduce inference difficulty.

Result: LLMs tend to predict overly comprehensive aspect sets leading to excessively long and misaligned summaries; using predicted number of aspects as guidance significantly improves performance across all datasets.

Conclusion: The proposed approach effectively addresses aspect refinement challenges in summarization, and analysis reveals LLMs' compliance when requested aspect numbers differ from their estimations, providing crucial insights for real-world LLM deployment.

Abstract: Aspect-based summarization has attracted significant attention for its
ability to generate more fine-grained and user-aligned summaries. While most
existing approaches assume a set of predefined aspects as input, real-world
scenarios often present challenges where these given aspects may be incomplete,
irrelevant, or entirely missing from the document. Users frequently expect
systems to adaptively refine or filter the provided aspects based on the actual
content. In this paper, we initiate this novel task setting, termed
Content-Aware Refinement of Provided Aspects for Summarization (CARPAS), with
the aim of dynamically adjusting the provided aspects based on the document
context before summarizing. We construct three new datasets to facilitate our
pilot experiments, and by using LLMs with four representative prompting
strategies in this task, we find that LLMs tend to predict an overly
comprehensive set of aspects, which often results in excessively long and
misaligned summaries. Building on this observation, we propose a preliminary
subtask to predict the number of relevant aspects, and demonstrate that the
predicted number can serve as effective guidance for the LLMs, reducing the
inference difficulty, and enabling them to focus on the most pertinent aspects.
Our extensive experiments show that the proposed approach significantly
improves performance across all datasets. Moreover, our deeper analyses uncover
LLMs' compliance when the requested number of aspects differs from their own
estimations, establishing a crucial insight for the deployment of LLMs in
similar real-world applications.

</details>


### [87] [Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible](https://arxiv.org/abs/2510.07178)
*Imry Ziv,Nur Lan,Emmanuel Chemla,Roni Katzir*

Main category: cs.CL

TL;DR: LLMs like GPT-2 don't distinguish between humanly possible and impossible languages, showing they lack human innate linguistic biases.


<details>
  <summary>Details</summary>
Motivation: To test whether LLMs share human innate learning biases by examining if they can distinguish between possible and impossible human languages.

Method: Compared GPT-2 learning curves on natural languages and impossible counterparts created via perturbations, analyzed cross-linguistic variance in perplexity metrics.

Result: GPT-2 learns natural and impossible languages equally easily, showing no systematic separation between possible and impossible language sets.

Conclusion: LLMs do not possess the same innate linguistic biases that shape human language acquisition and typology.

Abstract: Are large language models (LLMs) sensitive to the distinction between humanly
possible languages and humanly impossible languages? This question is taken by
many to bear on whether LLMs and humans share the same innate learning biases.
Previous work has attempted to answer it in the positive by comparing LLM
learning curves on existing language datasets and on "impossible" datasets
derived from them via various perturbation functions. Using the same
methodology, we examine this claim on a wider set of languages and impossible
perturbations. We find that in most cases, GPT-2 learns each language and its
impossible counterpart equally easily, in contrast to previous claims. We also
apply a more lenient condition by testing whether GPT-2 provides any kind of
separation between the whole set of natural languages and the whole set of
impossible languages. By considering cross-linguistic variance in various
metrics computed on the perplexity curves, we show that GPT-2 provides no
systematic separation between the possible and the impossible. Taken together,
these perspectives show that LLMs do not share the human innate biases that
shape linguistic typology.

</details>


### [88] [Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models](https://arxiv.org/abs/2510.07203)
*Benjamin Akera,Evelyn Nafula Ouma,Gilbert Yiga,Patrick Walukagga,Phionah Natukunda,Trevor Saaka,Solomon Nsumba,Lilian Teddy Nabukeera,Joel Muhanguzi,Imran Sekalala,Nimpamya Janat Namara,Engineer Bainomugisha,Ernest Mwebaze,John Quinn*

Main category: cs.CL

TL;DR: Sunflower models (14B and 32B) provide state-of-the-art language comprehension for Ugandan languages, addressing the gap in language technology support for Africa's diverse linguistic landscape through a regionally focused approach.


<details>
  <summary>Details</summary>
Motivation: Most African languages (over 2000) lack adequate language technology support, with current LLMs prioritizing only the most common languages like Swahili and Yoruba, leaving many languages underserved.

Method: Developed Sunflower 14B and 32B models based on Qwen 3, adopting a regionally focused approach specifically for Uganda's high linguistic diversity rather than piecemeal language support.

Result: Created open source models with state-of-the-art comprehension capabilities for the majority of Ugandan languages.

Conclusion: Regionally focused language models like Sunflower can effectively reduce language barriers and provide practical applications for linguistically diverse regions in Africa.

Abstract: There are more than 2000 living languages in Africa, most of which have been
bypassed by advances in language technology. Current leading LLMs exhibit
strong performance on a number of the most common languages (e.g. Swahili or
Yoruba), but prioritise support for the languages with the most speakers first,
resulting in piecemeal ability across disparate languages. We contend that a
regionally focussed approach is more efficient, and present a case study for
Uganda, a country with high linguistic diversity. We describe the development
of Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the
art comprehension in the majority of all Ugandan languages. These models are
open source and can be used to reduce language barriers in a number of
important practical applications.

</details>


### [89] [Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models](https://arxiv.org/abs/2510.07213)
*Chengzhi Zhong,Fei Cheng,Qianying Liu,Yugo Murawaki,Chenhui Chu,Sadao Kurohashi*

Main category: cs.CL

TL;DR: The paper identifies sparse cross-lingual dimensions in LLMs that control language switching, and introduces a training-free method to manipulate them for multilingual generation control.


<details>
  <summary>Details</summary>
Motivation: Large language models show strong multilingual capabilities despite limited non-English training data, suggesting they use English-aligned representations as an intermediate step for cross-lingual processing.

Method: A simple training-free method that identifies sparse cross-lingual dimensions in intermediate-to-final layers using only 50 sentences of parallel or monolingual data, then manipulates these dimensions to control output language.

Result: The method successfully switches output language while preserving semantic content, outperforms prior neuron-based approaches, and operates at substantially lower computational cost.

Conclusion: Cross-lingual transitions in LLMs are governed by sparse, interpretable dimensions that can be efficiently manipulated for controlled multilingual generation without additional training.

Abstract: Large language models exhibit strong multilingual capabilities despite
limited exposure to non-English data. Prior studies show that English-centric
large language models map multilingual content into English-aligned
representations at intermediate layers and then project them back into
target-language token spaces in the final layer. From this observation, we
hypothesize that this cross-lingual transition is governed by a small and
sparse set of dimensions, which occur at consistent indices across the
intermediate to final layers. Building on this insight, we introduce a simple,
training-free method to identify and manipulate these dimensions, requiring
only as few as 50 sentences of either parallel or monolingual data. Experiments
on a multilingual generation control task reveal the interpretability of these
dimensions, demonstrating that the interventions in these dimensions can switch
the output language while preserving semantic content, and that it surpasses
the performance of prior neuron-based approaches at a substantially lower cost.

</details>


### [90] [How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu](https://arxiv.org/abs/2510.07221)
*Benjamin Akera,Evelyn Nafula,Patrick Walukagga,Gilbert Yiga,John Quinn,Ernest Mwebaze*

Main category: cs.CL

TL;DR: This paper provides practical guidelines for deploying Whisper ASR systems in low-resource African languages, finding that 50 hours of training data achieves viable performance (WER < 13%) and 200 hours reaches <10% WER, with data quality issues being a major failure mode.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of developing ASR systems for low-resource African languages by determining minimum data requirements and identifying key failure modes for practical deployment.

Method: Comprehensive experiments on two Bantu languages: systematic data scaling analysis on Kinyarwanda (1-1,400 hours training data) and detailed error characterization on Kikuyu (270 hours training data).

Result: Practical ASR performance (WER < 13%) achieved with 50 hours of data, with continued improvement to <10% WER at 200 hours. Error analysis showed 38.6% of high-error cases due to noisy ground truth transcriptions.

Conclusion: Data curation is as critical as data volume for robust ASR performance in low-resource contexts, providing actionable benchmarks for similar language deployments.

Abstract: The development of Automatic Speech Recognition (ASR) systems for
low-resource African languages remains challenging due to limited transcribed
speech data. While recent advances in large multilingual models like OpenAI's
Whisper offer promising pathways for low-resource ASR development, critical
questions persist regarding practical deployment requirements. This paper
addresses two fundamental concerns for practitioners: determining the minimum
data volumes needed for viable performance and characterizing the primary
failure modes that emerge in production systems. We evaluate Whisper's
performance through comprehensive experiments on two Bantu languages:
systematic data scaling analysis on Kinyarwanda using training sets from 1 to
1,400 hours, and detailed error characterization on Kikuyu using 270 hours of
training data. Our scaling experiments demonstrate that practical ASR
performance (WER < 13\%) becomes achievable with as little as 50 hours of
training data, with substantial improvements continuing through 200 hours (WER
< 10\%). Complementing these volume-focused findings, our error analysis
reveals that data quality issues, particularly noisy ground truth
transcriptions, account for 38.6\% of high-error cases, indicating that careful
data curation is as critical as data volume for robust system performance.
These results provide actionable benchmarks and deployment guidance for teams
developing ASR systems across similar low-resource language contexts. We
release accompanying and models see
https://github.com/SunbirdAI/kinyarwanda-whisper-eval

</details>


### [91] [Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation](https://arxiv.org/abs/2510.07227)
*Arjun Krishnakumar,Rhea Sanjay Sukthanker,Hannan Javed Mahadik,Gabriela Kadlecová,Vladyslav Moroshan,Timur Carstensen,Frank Hutter,Aaron Klein*

Main category: cs.CL

TL;DR: A framework for efficient pretraining of Small Language Models (SLMs) using sparse sub-network initialization, evolutionary search, and knowledge distillation, achieving 9.2x fewer pretraining tokens while matching performance.


<details>
  <summary>Details</summary>
Motivation: To make SLMs more efficient and accessible as alternatives to LLMs by reducing resource requirements while maintaining strong performance.

Method: Three-component framework: 1) Structurally sparse sub-network initialization, 2) Evolutionary search for optimal initializations, 3) Knowledge distillation from larger teacher models.

Result: Best model matches validation perplexity of comparable Pythia SLM with 9.2x fewer pretraining tokens.

Conclusion: Provides a practical and reproducible path for cost-efficient SLM development at scale, with all code and models publicly available.

Abstract: Small Language models (SLMs) offer an efficient and accessible alternative to
Large Language Models (LLMs), delivering strong performance while using far
fewer resources. We introduce a simple and effective framework for pretraining
SLMs that brings together three complementary ideas. First, we identify
structurally sparse sub-network initializations that consistently outperform
randomly initialized models of similar size under the same compute budget.
Second, we use evolutionary search to automatically discover high-quality
sub-network initializations, providing better starting points for pretraining.
Third, we apply knowledge distillation from larger teacher models to speed up
training and improve generalization. Together, these components make SLM
pretraining substantially more efficient: our best model, discovered using
evolutionary search and initialized with LLM weights, matches the validation
perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining
tokens. We release all code and models at
https://github.com/whittle-org/whittle/, offering a practical and reproducible
path toward cost-efficient small language model development at scale.

</details>


### [92] [Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping](https://arxiv.org/abs/2510.07230)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Dakuo Wang*

Main category: cs.CL

TL;DR: Customer-R1 is an RL-based method that enables personalized user behavior simulation in online shopping by conditioning on explicit user personas and optimizing next-step rationale and action generation.


<details>
  <summary>Details</summary>
Motivation: Prior methods for simulating step-wise human behavior with LLMs learn population-level policies without user persona conditioning, resulting in generic rather than personalized simulations.

Method: Customer-R1 uses reinforcement learning with action correctness reward signals to optimize next-step rationale and action generation, conditioned on explicit user personas.

Result: Experiments on OPeRA dataset show Customer-R1 significantly outperforms prompting and SFT baselines in next-action prediction and better matches users' action distribution.

Conclusion: Customer-R1 enables higher fidelity personalized behavior simulation by incorporating explicit persona conditioning and RL-based optimization.

Abstract: Simulating step-wise human behavior with Large Language Models (LLMs) has
become an emerging research direction, enabling applications in various
practical domains. While prior methods, including prompting, supervised
fine-tuning (SFT), and reinforcement learning (RL), have shown promise in
modeling step-wise behavior, they primarily learn a population-level policy
without conditioning on a user's persona, yielding generic rather than
personalized simulations. In this work, we pose a critical question: how can
LLM agents better simulate personalized user behavior? We introduce
Customer-R1, an RL-based method for personalized, step-wise user behavior
simulation in online shopping environments. Our policy is conditioned on an
explicit persona, and we optimize next-step rationale and action generation via
action correctness reward signals. Experiments on the OPeRA dataset emonstrate
that Customer-R1 not only significantly outperforms prompting and SFT-based
baselines in next-action prediction tasks, but also better matches users'
action distribution, indicating higher fidelity in personalized behavior
simulation.

</details>


### [93] [Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships](https://arxiv.org/abs/2510.07231)
*Donggyu Lee,Sungwon Park,Yerin Hwang,Hyunwoo Oh,Hyoshin Kim,Jungwon Kim,Meeyoung Cha,Sangyoon Park,Jihee Kim*

Main category: cs.CL

TL;DR: A new causal reasoning benchmark for LLMs using real-world economic/finance research data, revealing significant performance gaps (best model: 57.6% accuracy) despite model scaling.


<details>
  <summary>Details</summary>
Motivation: Existing causal reasoning benchmarks have limitations like synthetic data and narrow domains, failing to assess LLMs' genuine understanding of cause-effect relationships needed for high-stakes applications.

Method: Constructed benchmark from causal relationships in top economics/finance journals using rigorous methodologies (instrumental variables, difference-in-differences, regression discontinuity), covering 40,379 items across 5 task types in diverse domains.

Result: Tested 8 state-of-the-art LLMs showing substantial limitations - best model achieved only 57.6% accuracy. Model scale doesn't consistently improve performance, and advanced reasoning models struggle with basic causal identification.

Conclusion: There's a critical gap between current LLM capabilities and reliable causal reasoning demands for high-stakes applications, highlighting the need for improved causal understanding beyond pattern matching.

Abstract: Causal reasoning is fundamental for Large Language Models (LLMs) to
understand genuine cause-and-effect relationships beyond pattern matching.
Existing benchmarks suffer from critical limitations such as reliance on
synthetic data and narrow domain coverage. We introduce a novel benchmark
constructed from casually identified relationships extracted from top-tier
economics and finance journals, drawing on rigorous methodologies including
instrumental variables, difference-in-differences, and regression discontinuity
designs. Our benchmark comprises 40,379 evaluation items covering five task
types across domains such as health, environment, technology, law, and culture.
Experimental results on eight state-of-the-art LLMs reveal substantial
limitations, with the best model achieving only 57.6\% accuracy. Moreover,
model scale does not consistently translate to superior performance, and even
advanced reasoning models struggle with fundamental causal relationship
identification. These findings underscore a critical gap between current LLM
capabilities and demands of reliable causal reasoning in high-stakes
applications.

</details>


### [94] [LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding](https://arxiv.org/abs/2510.07233)
*Zhivar Sourati,Zheng Wang,Marianne Menglin Liu,Yazhe Hu,Mengqing Guo,Sujeeth Bharadwaj,Kyu Han,Tao Sheng,Sujith Ravi,Morteza Dehghani,Dan Roth*

Main category: cs.CL

TL;DR: LAD-RAG is a Layout-Aware Dynamic RAG framework that improves question answering over visually rich documents by capturing structural layout and cross-page dependencies through symbolic document graphs, enabling adaptive evidence retrieval.


<details>
  <summary>Details</summary>
Motivation: Conventional RAG methods lose structural and cross-page dependencies by encoding content in isolated chunks, leading to incomplete evidence retrieval and degraded answer quality for multi-page reasoning tasks.

Method: Constructs symbolic document graphs during ingestion to capture layout structure and cross-page dependencies, and uses an LLM agent during inference to dynamically interact with neural and symbolic indices for adaptive evidence retrieval.

Result: Achieves over 90% perfect recall on average without top-k tuning, outperforms baseline retrievers by up to 20% in recall at comparable noise levels, and yields higher QA accuracy with minimal latency.

Conclusion: LAD-RAG effectively addresses structural dependency loss in conventional RAG methods and significantly improves retrieval performance and answer quality for multi-page document reasoning tasks.

Abstract: Question answering over visually rich documents (VRDs) requires reasoning not
only over isolated content but also over documents' structural organization and
cross-page dependencies. However, conventional retrieval-augmented generation
(RAG) methods encode content in isolated chunks during ingestion, losing
structural and cross-page dependencies, and retrieve a fixed number of pages at
inference, regardless of the specific demands of the question or context. This
often results in incomplete evidence retrieval and degraded answer quality for
multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a
novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs
a symbolic document graph that captures layout structure and cross-page
dependencies, adding it alongside standard neural embeddings to yield a more
holistic representation of the document. During inference, an LLM agent
dynamically interacts with the neural and symbolic indices to adaptively
retrieve the necessary evidence based on the query. Experiments on
MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG
improves retrieval, achieving over 90% perfect recall on average without any
top-k tuning, and outperforming baseline retrievers by up to 20% in recall at
comparable noise levels, yielding higher QA accuracy with minimal latency.

</details>


### [95] [When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation](https://arxiv.org/abs/2510.07238)
*Xunyi Jiang,Dingyi Chang,Julian McAuley,Xin Xu*

Main category: cs.CL

TL;DR: This paper investigates the aging problem of popular factuality benchmarks for evaluating large language models (LLMs), finding that outdated samples lead to unreliable assessments of LLM factuality.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of LLMs and real-world facts has outpaced static evaluation benchmarks, raising concerns about their reliability for assessing LLM factuality, yet this temporal misalignment remains underexplored.

Method: Systematic investigation using five popular factuality benchmarks and eight LLMs across different years, employing an up-to-date fact retrieval pipeline and three tailored metrics to quantify benchmark aging and its impact.

Result: Experimental results show that a considerable portion of samples in widely used factuality benchmarks are outdated, leading to unreliable assessments of LLM factuality.

Conclusion: The work provides a testbed to assess benchmark reliability for LLM factuality evaluation and aims to inspire more research on the benchmark aging issue.

Abstract: The rapid evolution of large language models (LLMs) and the real world has
outpaced the static nature of widely used evaluation benchmarks, raising
concerns about their reliability for evaluating LLM factuality. While
substantial works continue to rely on the popular but old benchmarks, their
temporal misalignment with real-world facts and modern LLMs, and their effects
on LLM factuality evaluation remain underexplored. Therefore, in this work, we
present a systematic investigation of this issue by examining five popular
factuality benchmarks and eight LLMs released across different years. An
up-to-date fact retrieval pipeline and three metrics are tailored to quantify
benchmark aging and its impact on LLM factuality evaluation. Experimental
results and analysis illustrate that a considerable portion of samples in the
widely used factuality benchmarks are outdated, leading to unreliable
assessments of LLM factuality. We hope our work can provide a testbed to assess
the reliability of a benchmark for LLM factuality evaluation and inspire more
research on the benchmark aging issue. Codes are available in
https://github.com/JiangXunyi/BenchAge.

</details>


### [96] [Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts](https://arxiv.org/abs/2510.07239)
*Christos Ziakas,Nicholas Loo,Nishita Jain,Alessandra Russo*

Main category: cs.CL

TL;DR: Red-Bandit is a red-teaming framework that adapts online to identify model vulnerabilities using specialized LoRA experts for different attack styles, with a bandit policy for dynamic selection.


<details>
  <summary>Details</summary>
Motivation: Existing automated red-teaming approaches lack mechanisms to efficiently adapt to model-specific vulnerabilities at inference time.

Method: Post-trains parameter-efficient LoRA experts specialized for different attack styles using RL, then uses a multi-armed bandit policy to dynamically select experts based on target model responses.

Result: Achieves state-of-the-art results on AdvBench (ASR@10) with more human-readable prompts (lower perplexity), and serves as a diagnostic tool for identifying model vulnerabilities.

Conclusion: Red-Bandit provides an effective adaptive red-teaming framework that balances exploration and exploitation while uncovering model-specific failure modes.

Abstract: Automated red-teaming has emerged as a scalable approach for auditing Large
Language Models (LLMs) prior to deployment, yet existing approaches lack
mechanisms to efficiently adapt to model-specific vulnerabilities at inference.
We introduce Red-Bandit, a red-teaming framework that adapts online to identify
and exploit model failure modes under distinct attack styles (e.g.,
manipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA
experts, each specialized for a particular attack style, using reinforcement
learning that rewards the generation of unsafe prompts via a rule-based safety
model. At inference, a multi-armed bandit policy dynamically selects among
these attack-style experts based on the target model's response safety,
balancing exploration and exploitation. Red-Bandit achieves state-of-the-art
results on AdvBench under sufficient exploration (ASR@10), while producing more
human-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy
serves as a diagnostic tool for uncovering model-specific vulnerabilities by
indicating which attack styles most effectively elicit unsafe behaviors.

</details>


### [97] [Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense](https://arxiv.org/abs/2510.07242)
*Leitian Tao,Ilia Kulikov,Swarnadeep Saha,Tianlu Wang,Jing Xu,Yixuan Li,Jason E Weston,Ping Yu*

Main category: cs.CL

TL;DR: HERO is a reinforcement learning framework that combines binary verifier signals with continuous reward model scores through stratified normalization and variance-aware weighting to improve reasoning in LLMs.


<details>
  <summary>Details</summary>
Motivation: Current post-training for LLM reasoning relies on binary verifiers that provide 0-1 correctness signals, which are brittle and under-credit partially correct or alternative answers, limiting learning potential.

Method: HERO integrates verifier signals with reward-model scores using stratified normalization to bound reward-model scores within verifier-defined groups, and variance-aware weighting to emphasize challenging prompts where dense signals matter most.

Result: Across diverse mathematical reasoning benchmarks, HERO consistently outperforms RM-only and verifier-only baselines, with strong gains on both verifiable and hard-to-verify tasks.

Conclusion: Hybrid reward design retains the stability of verifiers while leveraging the nuance of reward models to advance reasoning capabilities in LLMs.

Abstract: Post-training for reasoning of large language models (LLMs) increasingly
relies on verifiable rewards: deterministic checkers that provide 0-1
correctness signals. While reliable, such binary feedback is brittle--many
tasks admit partially correct or alternative answers that verifiers
under-credit, and the resulting all-or-nothing supervision limits learning.
Reward models offer richer, continuous feedback, which can serve as a
complementary supervisory signal to verifiers. We introduce HERO (Hybrid
Ensemble Reward Optimization), a reinforcement learning framework that
integrates verifier signals with reward-model scores in a structured way. HERO
employs stratified normalization to bound reward-model scores within
verifier-defined groups, preserving correctness while refining quality
distinctions, and variance-aware weighting to emphasize challenging prompts
where dense signals matter most. Across diverse mathematical reasoning
benchmarks, HERO consistently outperforms RM-only and verifier-only baselines,
with strong gains on both verifiable and hard-to-verify tasks. Our results show
that hybrid reward design retains the stability of verifiers while leveraging
the nuance of reward models to advance reasoning.

</details>


### [98] [LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation](https://arxiv.org/abs/2510.07243)
*Joseph Enguehard,Morgane Van Ermengem,Kate Atkinson,Sujeong Cha,Arijit Ghosh Chowdhury,Prashanth Kallur Ramaswamy,Jeremy Roghair,Hannah R Marlowe,Carina Suzana Negreanu,Kitty Boxall,Diana Mincu*

Main category: cs.CL

TL;DR: A novel reference-free evaluation method for LLM outputs in legal domain using Legal Data Points (LDPs) that outperforms baselines and correlates better with human expert evaluations.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluation methods in legal domain are limited - they either require costly reference data or use standardized assessments that don't reflect how lawyers actually evaluate legal answers, leading to unreliable and variable results.

Method: Break down lengthy legal responses into 'Legal Data Points' (LDPs) - self-contained information units, and introduce a reference-free evaluation methodology that mimics how lawyers evaluate legal answers.

Result: The method outperforms various baselines on proprietary and LegalBench datasets, correlates more closely with human expert evaluations, and improves inter-annotator agreement.

Conclusion: The proposed Legal Data Points approach provides a more reliable and effective way to evaluate LLM outputs in legal contexts, bridging the gap between automated evaluation and human expert judgment in legal question-answering.

Abstract: Evaluating large language model (LLM) outputs in the legal domain presents
unique challenges due to the complex and nuanced nature of legal analysis.
Current evaluation approaches either depend on reference data, which is costly
to produce, or use standardized assessment methods, both of which have
significant limitations for legal applications.
  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its
reliability and effectiveness in legal contexts depend heavily on evaluation
processes unique to the legal industry and how trustworthy the evaluation
appears to the human legal expert. This is where existing evaluation methods
currently fail and exhibit considerable variability.
  This paper aims to close the gap: a) we break down lengthy responses into
'Legal Data Points' (LDPs), self-contained units of information, and introduce
a novel, reference-free evaluation methodology that reflects how lawyers
evaluate legal answers; b) we demonstrate that our method outperforms a variety
of baselines on both our proprietary dataset and an open-source dataset
(LegalBench); c) we show how our method correlates more closely with human
expert evaluations and helps improve inter-annotator agreement; and finally d)
we open source our Legal Data Points for a subset of LegalBench used in our
experiments, allowing the research community to replicate our results and
advance research in this vital area of LLM evaluation on legal
question-answering.

</details>


### [99] [Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models](https://arxiv.org/abs/2510.07248)
*Jonggeun Lee,Woojung Song,Jongwook Han,Haesung Pyun,Yohan Jo*

Main category: cs.CL

TL;DR: PA-Tool is a training-free method that adapts tool schemas to align with small language models' pretrained knowledge, reducing schema misalignment errors by 80% and improving performance by up to 17 percentage points.


<details>
  <summary>Details</summary>
Motivation: Small language models struggle with tool-use tasks due to schema misalignment, where they hallucinate plausible but non-existent tool names based on pretraining knowledge rather than adapting to arbitrary schemas.

Method: PA-Tool leverages peakedness (a signal from contamination detection) to automatically rename tool components by generating multiple candidates and selecting those with highest output concentration across samples.

Result: Experiments on MetaTool and RoTBench show improvements of up to 17% points, with schema misalignment errors reduced by 80%, enabling small models to approach state-of-the-art performance.

Conclusion: Schema-level interventions can unlock the tool-use potential of resource-efficient models by adapting schemas to models rather than models to schemas, maintaining computational efficiency for adaptation to new tools without retraining.

Abstract: Small language models (SLMs) offer significant computational advantages for
tool-augmented AI systems, yet they struggle with tool-use tasks, particularly
in selecting appropriate tools and identifying correct parameters. A common
failure mode is schema misalignment: models hallucinate plausible but
non-existent tool names that reflect naming conventions internalized during
pretraining but absent from the provided tool schema. Rather than forcing
models to adapt to arbitrary schemas, we propose adapting schemas to align with
models' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool
Schema Generation), a training-free method that leverages peakedness-a signal
from contamination detection indicating pretraining familiarity-to
automatically rename tool components. By generating multiple candidates and
selecting those with highest output concentration across samples, PA-Tool
identifies pretrain-aligned naming patterns. Experiments on MetaTool and
RoTBench show improvements of up to 17% points, with schema misalignment errors
reduced by 80%. PA-Tool enables small models to approach state-of-the-art
performance while maintaining computational efficiency for adaptation to new
tools without retraining. Our work demonstrates that schema-level interventions
can unlock the tool-use potential of resource-efficient models by adapting
schemas to models rather than models to schemas.

</details>


### [100] [Online Rubrics Elicitation from Pairwise Comparisons](https://arxiv.org/abs/2510.07284)
*MohammadHossein Rezaei,Robert Vacareanu,Zihao Wang,Clinton Wang,Yunzhong He,Afra Feyza Akyürek*

Main category: cs.CL

TL;DR: OnlineRubrics is a method that dynamically updates evaluation criteria during LLM training through pairwise comparisons, preventing reward hacking and capturing emergent requirements, achieving up to 8% improvement over static rubrics.


<details>
  <summary>Details</summary>
Motivation: Static rubrics in LLM training are vulnerable to reward hacking and fail to capture emergent desiderata that arise during training, limiting their effectiveness.

Method: Online Rubrics Elicitation (OnlineRubrics) - dynamically curates evaluation criteria through pairwise comparisons of responses from current and reference policies in an online manner during training.

Result: Consistent improvements of up to 8% over training with static rubrics across multiple benchmarks including AlpacaEval, GPQA, ArenaHard, and expert question validation sets.

Conclusion: Online dynamic rubric elicitation enables continuous error identification and mitigation during training, yielding significant performance gains and capturing important evaluation themes like transparency, practicality, organization, and reasoning.

Abstract: Rubrics provide a flexible way to train LLMs on open-ended long-form answers
where verifiable rewards are not applicable and human preferences provide
coarse signals. Prior work shows that reinforcement learning with rubric-based
rewards leads to consistent gains in LLM post-training. Most existing
approaches rely on rubrics that remain static over the course of training. Such
static rubrics, however, are vulnerable to reward-hacking type behaviors and
fail to capture emergent desiderata that arise during training. We introduce
Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates
evaluation criteria in an online manner through pairwise comparisons of
responses from current and reference policies. This online process enables
continuous identification and mitigation of errors as training proceeds.
Empirically, this approach yields consistent improvements of up to 8% over
training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as
well as the validation sets of expert questions and rubrics. We qualitatively
analyze the elicited criteria and identify prominent themes such as
transparency, practicality, organization, and reasoning.

</details>


### [101] [On the Convergence of Moral Self-Correction in Large Language Models](https://arxiv.org/abs/2510.07290)
*Guangliang Liu,Haitao Mao,Bochuan Cao,Zhiyu Xue,Xitong Zhang,Rongrong Wang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: This paper investigates how LLMs perform intrinsic self-correction in moral contexts, revealing that multi-round interactions lead to performance convergence through stabilized moral concept activation.


<details>
  <summary>Details</summary>
Motivation: While empirical evidence shows LLMs can improve responses through self-correction, the underlying mechanisms of intrinsic self-correction (without specific error details) remain unknown, particularly in moral contexts.

Method: The authors conducted mechanistic analysis of moral self-correction in LLMs, examining multi-round interactions and how self-correction instructions activate moral concepts that reduce model uncertainty.

Result: The study found that intrinsic self-correction exhibits performance convergence through successive rounds, where consistently applied instructions activate and stabilize moral concepts, reducing uncertainty in LLM responses.

Conclusion: Moral self-correction demonstrates strong potential with its converged performance property, providing insights into how LLMs internally improve responses without external error specifications.

Abstract: Large Language Models (LLMs) are able to improve their responses when
instructed to do so, a capability known as self-correction. When instructions
provide only a general and abstract goal without specific details about
potential issues in the response, LLMs must rely on their internal knowledge to
improve response quality, a process referred to as intrinsic self-correction.
The empirical success of intrinsic self-correction is evident in various
applications, but how and why it is effective remains unknown. Focusing on
moral self-correction in LLMs, we reveal a key characteristic of intrinsic
self-correction: performance convergence through multi-round interactions; and
provide a mechanistic analysis of this convergence behavior. Based on our
experimental results and analysis, we uncover the underlying mechanism of
convergence: consistently injected self-correction instructions activate moral
concepts that reduce model uncertainty, leading to converged performance as the
activated moral concepts stabilize over successive rounds. This paper
demonstrates the strong potential of moral self-correction by showing that it
exhibits a desirable property of converged performance.

</details>


### [102] [Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning](https://arxiv.org/abs/2510.07300)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Kaiyu Huang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: M-Thinker addresses language inconsistency and poor reasoning in non-English LRMs using GRPO with Language Consistency and Cross-lingual Thinking Alignment rewards, achieving near-perfect consistency and superior multilingual performance.


<details>
  <summary>Details</summary>
Motivation: Current Large Reasoning Models struggle with language consistency and reasoning quality in non-English languages, degrading user experience and hindering global deployment.

Method: Proposed M-Thinker trained with GRPO algorithm featuring Language Consistency reward for input-thought-answer consistency and Cross-lingual Thinking Alignment reward to transfer English reasoning capability to other languages.

Result: M-Thinker-1.5B/7B models achieve nearly 100% language consistency, superior performance on MMATH and PolyMath benchmarks, and excellent generalization to out-of-domain languages.

Conclusion: M-Thinker effectively solves language inconsistency and reasoning degradation in non-English LRMs through innovative reward mechanisms, enabling better global deployment of reasoning models.

Abstract: Large Reasoning Models (LRMs) have achieved remarkable performance on complex
reasoning tasks by adopting the "think-then-answer" paradigm, which enhances
both accuracy and interpretability. However, current LRMs exhibit two critical
limitations when processing non-English languages: (1) They often struggle to
maintain input-output language consistency; (2) They generally perform poorly
with wrong reasoning paths and lower answer accuracy compared to English. These
limitations significantly degrade the user experience for non-English speakers
and hinder the global deployment of LRMs. To address these limitations, we
propose M-Thinker, which is trained by the GRPO algorithm that involves a
Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment
(CTA) reward. Specifically, the LC reward defines a strict constraint on the
language consistency between the input, thought, and answer. Besides, the CTA
reward compares the model's non-English reasoning paths with its English
reasoning path to transfer its own reasoning capability from English to
non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B
models not only achieve nearly 100% language consistency and superior
performance on two multilingual benchmarks (MMATH and PolyMath), but also
exhibit excellent generalization on out-of-domain languages.

</details>


### [103] [Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain](https://arxiv.org/abs/2510.07309)
*Yue Li,Ran Tao,Derek Hommel,Yusuf Denizay Dönder,Sungyong Chang,David Mimno,Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: CORGI is a new text-to-SQL benchmark designed for real-world business contexts, featuring synthetic databases from companies like Doordash and Airbnb, with questions across four complexity levels that require causal reasoning and strategic planning.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-SQL benchmarks focus on factual retrieval of past records, but real business intelligence requires more complex reasoning like predictions and recommendations.

Method: Created CORGI benchmark with synthetic enterprise databases and questions across four categories: descriptive, explanatory, predictive, and recommendational queries.

Result: LLM performance drops significantly on high-level questions, with CORGI being 21% more difficult than BIRD benchmark, showing gaps in prediction accuracy and actionable planning.

Conclusion: There's a significant gap between current LLM capabilities and real-world business intelligence needs, highlighting the need for benchmarks that test multi-level agentic intelligence.

Abstract: In the business domain, where data-driven decision making is crucial,
text-to-SQL is fundamental for easy natural language access to structured data.
While recent LLMs have achieved strong performance in code generation, existing
text-to-SQL benchmarks remain focused on factual retrieval of past records. We
introduce CORGI, a new benchmark specifically designed for real-world business
contexts. CORGI is composed of synthetic databases inspired by enterprises such
as Doordash, Airbnb, and Lululemon. It provides questions across four
increasingly complex categories of business queries: descriptive, explanatory,
predictive, and recommendational. This challenge calls for causal reasoning,
temporal forecasting, and strategic recommendation, reflecting multi-level and
multi-step agentic intelligence. We find that LLM performance drops on
high-level questions, struggling to make accurate predictions and offer
actionable plans. Based on execution success rate, the CORGI benchmark is about
21\% more difficult than the BIRD benchmark. This highlights the gap between
popular LLMs and the need for real-world business intelligence. We release a
public dataset and evaluation framework, and a website for public submissions.

</details>


### [104] [Vibe Checker: Aligning Code Evaluation with Human Preference](https://arxiv.org/abs/2510.07315)
*Ming Zhong,Xiang Zhou,Ting-Yun Chang,Qingze Wang,Nan Xu,Xiance Si,Dan Garrette,Shyam Upadhyay,Jeremiah Liu,Jiawei Han,Benoit Schillings,Jiao Sun*

Main category: cs.CL

TL;DR: The paper introduces VeriCode, a taxonomy of 30 verifiable code instructions with deterministic verifiers, and Vibe Checker testbed to evaluate LLMs' code instruction following capabilities beyond functional correctness.


<details>
  <summary>Details</summary>
Motivation: Current code evaluation focuses only on functional correctness (pass@k) but overlooks non-functional instructions that users apply during vibe coding - where code should feel right, read cleanly, preserve intent, and remain correct.

Method: Developed VeriCode taxonomy of 30 verifiable code instructions with deterministic verifiers, and created Vibe Checker testbed to assess both code instruction following and functional correctness. Evaluated 31 leading LLMs.

Result: Even the strongest models struggle to comply with multiple instructions and exhibit functional regression. A composite score of functional correctness and instruction following correlates best with human preference, with instruction following being the primary differentiator on real-world tasks.

Conclusion: Instruction following is the missing piece underlying vibe check that represents human preference in coding. The work identifies core factors of vibe check and provides a path for benchmarking models that better align with user preferences.

Abstract: Large Language Models (LLMs) have catalyzed vibe coding, where users leverage
LLMs to generate and iteratively refine code through natural language
interactions until it passes their vibe check. Vibe check is tied to real-world
human preference and goes beyond functionality: the solution should feel right,
read cleanly, preserve intent, and remain correct. However, current code
evaluation remains anchored to pass@k and captures only functional correctness,
overlooking the non-functional instructions that users routinely apply. In this
paper, we hypothesize that instruction following is the missing piece
underlying vibe check that represents human preference in coding besides
functional correctness. To quantify models' code instruction following
capabilities with measurable signals, we present VeriCode, a taxonomy of 30
verifiable code instructions together with corresponding deterministic
verifiers. We use the taxonomy to augment established evaluation suites,
resulting in Vibe Checker, a testbed to assess both code instruction following
and functional correctness. Upon evaluating 31 leading LLMs, we show that even
the strongest models struggle to comply with multiple instructions and exhibit
clear functional regression. Most importantly, a composite score of functional
correctness and instruction following correlates the best with human
preference, with the latter emerging as the primary differentiator on
real-world programming tasks. Our work identifies core factors of the vibe
check, providing a concrete path for benchmarking and developing models that
better align with user preferences in coding.

</details>


### [105] [Artificial Hippocampus Networks for Efficient Long-Context Modeling](https://arxiv.org/abs/2510.07318)
*Yunhao Fang,Weihao Yu,Shu Zhong,Qinghao Ye,Xuehan Xiong,Lai Wei*

Main category: cs.CL

TL;DR: AHN framework combines sliding window KV cache with RNN-like compression for efficient long-sequence modeling, achieving near full-attention performance with significantly reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between efficient fixed-size memory in RNNs and lossless growing memory in Transformers for long-sequence modeling, inspired by cognitive science's Multi-Store Model.

Method: Maintain sliding window KV cache as short-term memory, use Artificial Hippocampus Network (AHN) to compress out-of-window information into fixed-size long-term memory, implemented with modern RNN architectures like Mamba2, DeltaNet, and Gated DeltaNet.

Result: AHN-augmented models outperform sliding window baselines and achieve comparable/superior performance to full-attention models while reducing FLOPs by 40.5% and memory cache by 74.0% for Qwen2.5-3B-Instruct on LV-Eval.

Conclusion: The AHN framework successfully bridges the gap between efficient RNN-like compression and high-fidelity Transformer modeling, enabling scalable long-context processing with substantial computational savings.

Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency
of compressive fixed-size memory in RNN-like models and the fidelity of
lossless growing memory in attention-based Transformers. Inspired by the
Multi-Store Model in cognitive science, we introduce a memory framework of
artificial neural networks. Our method maintains a sliding window of the
Transformer's KV cache as lossless short-term memory, while a learnable module
termed Artificial Hippocampus Network (AHN) recurrently compresses
out-of-window information into a fixed-size compact long-term memory. To
validate this framework, we instantiate AHNs using modern RNN-like
architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive
experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate
that AHN-augmented models consistently outperform sliding window baselines and
achieve performance comparable or even superior to full-attention models, while
substantially reducing computational and memory requirements. For instance,
augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%
and memory cache by 74.0%, while improving its average score on LV-Eval (128k
sequence length) from 4.41 to 5.88. Code is available at:
https://github.com/ByteDance-Seed/AHN.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [106] [Automated Repeatable Adversary Threat Emulation with Effects Language (EL)](https://arxiv.org/abs/2510.06420)
*Suresh K. Damodaran,Paul D. Rowe*

Main category: cs.CR

TL;DR: The paper introduces Effects Language (EL), a visual programming language with graph-based operational semantics, to automate multi-step attack emulation for training defenders and evaluating defense tools.


<details>
  <summary>Details</summary>
Motivation: To address challenges in automating multi-step attacks attributed to advanced persistent threats for training defenders and evaluating defense tools.

Method: Formally define the execution semantics of EL, prove important execution properties, and apply EL to codify attacks using publicly available attack scenarios.

Result: Demonstrates how EL can be utilized to provide proof-of-attack of complex multi-step attacks and highlights improvements in time and resource efficiency for repeatable automation.

Conclusion: EL effectively addresses challenges in multi-step attack automation, providing a formal and efficient solution for attack emulation and defense evaluation.

Abstract: The emulation of multi-step attacks attributed to advanced persistent threats
is valuable for training defenders and evaluating defense tools. In this paper,
we discuss the numerous challenges and desired attributes associated with such
automation. Additionally, we introduce the use of Effects Language (EL), a
visual programming language with graph-based operational semantics, as a
solution to address many of these challenges and requirements. We formally
define the execution semantics of EL, and prove important execution properties.
Furthermore, we showcase the application of EL to codify attacks using an
example from one of the publicly available attack scenarios. We also
demonstrate how EL can be utilized to provide proof-of-attack of complex
multi-step attacks. Our results highlight the improvements in time and resource
efficiency achieved through the use of EL for repeatable automation.

</details>


### [107] [Breaking Precision Time: OS Vulnerability Exploits Against IEEE 1588](https://arxiv.org/abs/2510.06421)
*Muhammad Abdullah Soomro,Fatima Muhammad Anwar*

Main category: cs.CR

TL;DR: This paper identifies kernel-level attacks on PTP time synchronization systems, showing how compromised hosts can manipulate system time without altering network traffic, bypassing existing security measures.


<details>
  <summary>Details</summary>
Motivation: Current PTP security defenses focus on network-based attacks but assume uncompromised hosts, creating a critical blind spot for kernel-level adversaries.

Method: Implemented three attack primitives (constant offset, progressive skew, random jitter) using in-kernel payloads targeting ptp4l and phc2sys daemons.

Result: Attacks can silently destabilize clock synchronization and bypass existing PTP security extensions.

Conclusion: Host-level trust assumptions need reconsideration and kernel integrity must be integrated into secure time synchronization system design.

Abstract: The Precision Time Protocol (PTP), standardized as IEEE 1588, provides
sub-microsecond synchronization across distributed systems and underpins
critical infrastructure in telecommunications, finance, power systems, and
industrial automation. While prior work has extensively analyzed PTP's
vulnerability to network-based attacks, prompting the development of
cryptographic protections and anomaly detectors, these defenses presume an
uncompromised host. In this paper, we identify and exploit a critical blind
spot in current threat models: kernel-level adversaries operating from within
the host running the PTP stack. We present the first systematic study of
kernel-rooted attacks on PTP, demonstrating how privileged attackers can
manipulate system time by corrupting key interfaces without altering PTP
network traffic. We implement three attack primitives, constant offset,
progressive skew, and random jitter, using in-kernel payloads, and evaluate
their impact on the widely used ptp4l and phc2sys daemons. Our experiments
reveal that these attacks can silently destabilize clock synchronization,
bypassing existing PTP security extensions. These findings highlight the urgent
need to reconsider host-level trust assumptions and integrate kernel integrity
into the design of secure time synchronization systems.

</details>


### [108] [Proofs of No Intrusion](https://arxiv.org/abs/2510.06432)
*Vipul Goyal,Justin Raizes*

Main category: cs.CR

TL;DR: Proofs of No Intrusion enable classical clients to remotely test if quantum servers have been hacked without destroying the data, using non-destructive testing of coset states and applying to various unclonable primitives.


<details>
  <summary>Details</summary>
Motivation: Classical data security cannot detect perfect copies, but quantum mechanics forbids duplication, creating new opportunities for intrusion detection without data destruction.

Method: Introduces Proofs of No Intrusion using non-destructive testing of coset states with classical communication, assuming fully homomorphic encryption and applying to unclonable primitives.

Result: Successfully constructs proofs of no intrusion for ciphertexts and equips unclonable decryption keys and signature tokens with non-intrusion proofs.

Conclusion: Proofs of non-intrusion can be defined for essentially any unclonable primitive, providing a new quantum-based approach to detect data theft without data destruction.

Abstract: A central challenge in data security is not just preventing theft, but
detecting whether it has occurred. Classically, this is impossible because a
perfect copy leaves no evidence. Quantum mechanics, on the other hand, forbids
general duplication, opening up new possibilities.
  We introduce Proofs of No Intrusion, which enable a classical client to
remotely test whether a quantum server has been hacked and the client's data
stolen. Crucially, the test does not destroy the data being tested, avoiding
the need to store a backup elsewhere. We define and construct proofs of no
intrusion for ciphertexts assuming fully homomorphic encryption. Additionally,
we show how to equip several constructions of unclonable primitives with proofs
of non-intrusion, such as unclonable decryption keys and signature tokens.
Conceptually, proofs of non-intrusion can be defined for essentially any
unclonable primitive.
  At the heart of our techniques is a new method for non-destructively testing
coset states with classical communication. It can be viewed as a
non-destructive proof of knowledge of a measurement result of the coset state.

</details>


### [109] [BATTLE for Bitcoin: Capital-Efficient Optimistic Bridges with Large Committees](https://arxiv.org/abs/2510.06468)
*Sergio Demian Lerner,Ariel Futoransky*

Main category: cs.CR

TL;DR: BATTLE for Bitcoin is a DoS-resilient dispute layer that secures optimistic bridges between Bitcoin and rollups/sidechains using BitVM-style components and garbled circuits with on-demand L1 security bonds.


<details>
  <summary>Details</summary>
Motivation: To create a secure and efficient dispute resolution mechanism for optimistic bridges connecting Bitcoin with rollups or sidechains, addressing the need for DoS-resilience and maintaining constant capital requirements for honest asserters.

Method: Adapts the BATTLE tournament protocol to Bitcoin's UTXO model using BitVM-style FLEX components and garbled circuits with on-demand L1 security bonds. Uses standard timelocks and pre-signed transaction DAGs without requiring new opcodes.

Result: The protocol resolves disputes in logarithmic rounds while recycling rewards, keeping the honest asserter's minimum initial capital constant even with many permissionless challengers. It requires O(N²) pre-signed transactions, signatures, and message exchanges but remains practical for N ≳ 10³ operators.

Conclusion: BATTLE for Bitcoin provides a fully contestable, practical dispute layer that enables high decentralization for optimistic bridges between Bitcoin and other systems, using only existing Bitcoin features without requiring protocol changes.

Abstract: We present BATTLE for Bitcoin, a DoS-resilient dispute layer that secures
optimistic bridges between Bitcoin and rollups or sidechains. Our design adapts
the BATTLE tournament protocol to Bitcoin's UTXO model using BitVM-style FLEX
components and garbled circuits with on-demand L1 security bonds. Disputes are
resolved in logarithmic rounds while recycling rewards, keeping the honest
asserter's minimum initial capital constant even under many permissionless
challengers. The construction is fully contestable (challengers can supply
higher-work counter-proofs) and relies only on standard timelocks and
pre-signed transaction DAGs, without new opcodes.
  For $N$ operators, the protocol requires $O(N^2)$ pre-signed transactions,
signatures, and message exchanges, yet remains practical at $N\!\gtrsim\!10^3$,
enabling high decentralization.

</details>


### [110] [From Description to Detection: LLM based Extendable O-RAN Compliant Blind DoS Detection in 5G and Beyond](https://arxiv.org/abs/2510.06530)
*Thusitha Dayaratne,Ngoc Duy Pham,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.CR

TL;DR: Proposes a novel LLM-based anomaly detection framework for 5G control-plane security that works in zero-shot mode using short natural language attack descriptions, demonstrating superior performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods for 5G control-plane protocols (RRC/NAS) have limitations including need for extensive training data, predefined rules, and limited explainability, while vulnerabilities in these protocols pose significant security threats like Blind DoS attacks.

Method: Leverages Large Language Models in zero-shot mode with unordered data and short natural language attack descriptions within O-RAN architecture, analyzing robustness to prompt variation and automating attack descriptions.

Result: Demonstrates superior performance in attack detection compared to traditional methods, shows detection quality relies on semantic completeness rather than phrasing/length, and validates practicality within O-RAN's real-time constraints.

Conclusion: The framework shows potential for detecting other Layer-3 attacks and provides an effective solution for 5G control-plane security with better explainability and reduced data requirements.

Abstract: The quality and experience of mobile communication have significantly
improved with the introduction of 5G, and these improvements are expected to
continue beyond the 5G era. However, vulnerabilities in control-plane
protocols, such as Radio Resource Control (RRC) and Non-Access Stratum (NAS),
pose significant security threats, such as Blind Denial of Service (DoS)
attacks. Despite the availability of existing anomaly detection methods that
leverage rule-based systems or traditional machine learning methods, these
methods have several limitations, including the need for extensive training
data, predefined rules, and limited explainability. Addressing these
challenges, we propose a novel anomaly detection framework that leverages the
capabilities of Large Language Models (LLMs) in zero-shot mode with unordered
data and short natural language attack descriptions within the Open Radio
Access Network (O-RAN) architecture. We analyse robustness to prompt variation,
demonstrate the practicality of automating the attack descriptions and show
that detection quality relies on the semantic completeness of the description
rather than its phrasing or length. We utilise an RRC/NAS dataset to evaluate
the solution and provide an extensive comparison of open-source and proprietary
LLM implementations to demonstrate superior performance in attack detection. We
further validate the practicality of our framework within O-RAN's real-time
constraints, illustrating its potential for detecting other Layer-3 attacks.

</details>


### [111] [SpyChain: Multi-Vector Supply Chain Attacks on Small Satellite Systems](https://arxiv.org/abs/2510.06535)
*Jack Vanlyssel,Enrique Sobrados,Ramsha Anwar,Gruia-Catalin Roman,Afsah Anwar*

Main category: cs.CR

TL;DR: SpyChain is the first end-to-end hardware supply chain attack framework targeting small satellites, demonstrating how COTS components can evade detection, exfiltrate telemetry, disrupt operations, and launch DoS attacks through covert channels.


<details>
  <summary>Details</summary>
Motivation: Small satellites rely heavily on COTS hardware, creating supply chain vulnerabilities that are largely unexplored in space systems. While flight software has strong security oversight, auxiliary COTS components have similar privileged access but lack robust security assurance.

Method: Developed SpyChain framework with independent and colluding hardware supply chain threats. Used NASA's NOS3 satellite simulation to demonstrate attacks across five stealth scenarios, from simple solo components to dynamic coordinating malware.

Result: SpyChain successfully evaded testing, exfiltrated telemetry, disrupted operations, and launched DoS attacks through covert channels that bypass ground monitoring. The framework introduced novel multi-component execution techniques now included in SPARTA matrix.

Conclusion: Implicit trust in auxiliary COTS components enables covert persistence and novel attack vectors. Lightweight onboard defenses including runtime monitoring can mitigate such threats. Findings were acknowledged and affirmed by NASA's NOS3 team.

Abstract: Small satellites are integral to scientific, commercial, and defense
missions, but reliance on commercial off-the-shelf (COTS) hardware broadens
their attack surface. Although supply chain threats are well studied in other
cyber-physical domains, their feasibility and stealth in space systems remain
largely unexplored. Prior work has focused on flight software, which benefits
from strict security practices and oversight. In contrast, auxiliary COTS
components often lack robust assurance yet enjoy comparable access to critical
on-board resources, including telemetry, system calls, and the software bus.
Despite this privileged access, the insider threat within COTS hardware supply
chains has received little attention. In this work, we present SpyChain, the
first end-to-end design and implementation of independent and colluding
hardware supply chain threats targeting small satellites. Using NASA's
satellite simulation (NOS3), we demonstrate that SpyChain can evade testing,
exfiltrate telemetry, disrupt operations, and launch Denial of Service (DoS)
attacks through covert channels that bypass ground monitoring. Our study traces
an escalation from a simple solo component to dynamic, coordinating malware,
introducing a taxonomy of stealth across five scenarios. We showcase how
implicit trust in auxiliary components enables covert persistence and reveal
novel attack vectors, highlighting a new multi-component execution technique
that is now incorporated into the SPARTA matrix. Our findings are reinforced by
acknowledgment and affirmation from NASA's NOS3 team. Finally, we implement
lightweight onboard defenses, including runtime monitoring, to mitigate threats
like SpyChain.

</details>


### [112] [Auto-Stega: An Agent-Driven System for Lifelong Strategy Evolution in LLM-Based Text Steganography](https://arxiv.org/abs/2510.06565)
*Jiuan Zhou,Yu Cheng,Yuan Xie,Zhaoxia Yin*

Main category: cs.CR

TL;DR: Auto-Stega is a self-evolving text steganography framework that automatically discovers and adapts steganographic strategies using LLMs, achieving better performance at high embedding rates compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current text steganography methods struggle to balance efficiency, imperceptibility, and security at high embedding rates, relying on hand-crafted strategies that lack adaptability.

Method: Proposes Auto-Stega framework with agent-driven closed-loop system for strategy discovery, composition, and adaptation; introduces PC-DNTE algorithm for maintaining distribution alignment at high embedding rates; uses decoding LLM for information recovery.

Result: At high embedding rates, achieves 42.2% improvement in perplexity and 1.6% improvement in anti-steganalysis performance over state-of-the-art methods.

Conclusion: Auto-Stega successfully addresses the limitations of existing steganography methods by enabling self-evolving strategies and maintaining imperceptibility at high embedding rates, demonstrating superior performance in text steganography.

Abstract: With the rapid progress of LLMs, high quality generative text has become
widely available as a cover for text steganography. However, prevailing methods
rely on hand-crafted or pre-specified strategies and struggle to balance
efficiency, imperceptibility, and security, particularly at high embedding
rates. Accordingly, we propose Auto-Stega, an agent-driven self-evolving
framework that is the first to realize self-evolving steganographic strategies
by automatically discovering, composing, and adapting strategies at inference
time; the framework operates as a closed loop of generating, evaluating,
summarizing, and updating that continually curates a structured strategy
library and adapts across corpora, styles, and task constraints. A decoding LLM
recovers the information under the shared strategy. To handle high embedding
rates, we introduce PC-DNTE, a plug-and-play algorithm that maintains alignment
with the base model's conditional distribution at high embedding rates,
preserving imperceptibility while enhancing security. Experimental results
demonstrate that at higher embedding rates Auto-Stega achieves superior
performance with gains of 42.2\% in perplexity and 1.6\% in anti-steganalysis
performance over SOTA methods.

</details>


### [113] [Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation](https://arxiv.org/abs/2510.06605)
*Shuo Shao,Yiming Li,Hongwei Yao,Yifei Chen,Yuchen Yang,Zhan Qin*

Main category: cs.CR

TL;DR: ZeroPrint is a novel black-box fingerprinting method that uses zeroth-order estimation to approximate gradients through semantic-preserving word substitutions, achieving state-of-the-art performance in identifying illicit LLM copies.


<details>
  <summary>Details</summary>
Motivation: Existing black-box fingerprinting methods fail to generate distinctive LLM fingerprints because they rely on model outputs that lose critical information about unique parameters due to non-linear functions.

Method: Leverages Fisher Information Theory to show gradients are more informative than outputs, then uses zeroth-order estimation with semantic-preserving word substitutions to approximate the model's Jacobian matrix as a fingerprint.

Result: Experiments on standard benchmarks show ZeroPrint achieves state-of-the-art effectiveness and robustness, significantly outperforming existing black-box methods.

Conclusion: Gradient-based fingerprinting via zeroth-order estimation provides a more effective approach for LLM copyright protection compared to output-based methods.

Abstract: The substantial investment required to develop Large Language Models (LLMs)
makes them valuable intellectual property, raising significant concerns about
copyright protection. LLM fingerprinting has emerged as a key technique to
address this, which aims to verify a model's origin by extracting an intrinsic,
unique signature (a "fingerprint") and comparing it to that of a source model
to identify illicit copies. However, existing black-box fingerprinting methods
often fail to generate distinctive LLM fingerprints. This ineffectiveness
arises because black-box methods typically rely on model outputs, which lose
critical information about the model's unique parameters due to the usage of
non-linear functions. To address this, we first leverage Fisher Information
Theory to formally demonstrate that the gradient of the model's input is a more
informative feature for fingerprinting than the output. Based on this insight,
we propose ZeroPrint, a novel method that approximates these information-rich
gradients in a black-box setting using zeroth-order estimation. ZeroPrint
overcomes the challenge of applying this to discrete text by simulating input
perturbations via semantic-preserving word substitutions. This operation allows
ZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint.
Experiments on the standard benchmark show ZeroPrint achieves a
state-of-the-art effectiveness and robustness, significantly outperforming
existing black-box methods.

</details>


### [114] [Code Agent can be an End-to-end System Hacker: Benchmarking Real-world Threats of Computer-use Agent](https://arxiv.org/abs/2510.06607)
*Weidi Luo,Qiming Zhang,Tianyu Lu,Xiaogeng Liu,Bin Hu,Hung-Chun Chiu,Siyuan Ma,Yizhe Zhang,Xusheng Xiao,Yinzhi Cao,Zhen Xiang,Chaowei Xiao*

Main category: cs.CR

TL;DR: AdvCUA is the first benchmark aligned with real-world MITRE ATT&CK TTPs to evaluate computer-use agents' security risks in enterprise OS environments, finding current CUAs enable complex intrusions even by inexperienced attackers.


<details>
  <summary>Details</summary>
Motivation: As computer-use agents become embedded in daily OS operations, there's an urgent need to examine their real-world security implications and whether they can be misused for security attacks, given limitations in existing evaluation methods.

Method: Proposed AdvCUA benchmark with 140 tasks (40 direct malicious, 74 TTP-based malicious, 26 end-to-end kill chains) systematically evaluating CUAs under realistic enterprise OS security threats in multi-host environment sandbox using hard-coded evaluation.

Result: Evaluation of 5 mainstream CUAs (ReAct, AutoGPT, Gemini CLI, Cursor CLI, Cursor IDE) based on 8 foundation LLMs shows current frontier CUAs don't adequately cover OS security threats and enable complex enterprise intrusions.

Conclusion: CUAs reduce dependence on custom malware and deep expertise, enabling inexperienced attackers to mount complex intrusions, raising social concerns about CUA responsibility and security.

Abstract: Computer-use agent (CUA) frameworks, powered by large language models (LLMs)
or multimodal LLMs (MLLMs), are rapidly maturing as assistants that can
perceive context, reason, and act directly within software environments. Among
their most critical applications is operating system (OS) control. As CUAs in
the OS domain become increasingly embedded in daily operations, it is
imperative to examine their real-world security implications, specifically
whether CUAs can be misused to perform realistic, security-relevant attacks.
Existing works exhibit four major limitations: Missing attacker-knowledge model
on tactics, techniques, and procedures (TTP), Incomplete coverage for
end-to-end kill chains, unrealistic environment without multi-host and
encrypted user credentials, and unreliable judgment dependent on
LLM-as-a-Judge. To address these gaps, we propose AdvCUA, the first benchmark
aligned with real-world TTPs in MITRE ATT&CK Enterprise Matrix, which comprises
140 tasks, including 40 direct malicious tasks, 74 TTP-based malicious tasks,
and 26 end-to-end kill chains, systematically evaluates CUAs under a realistic
enterprise OS security threat in a multi-host environment sandbox by hard-coded
evaluation. We evaluate the existing five mainstream CUAs, including ReAct,
AutoGPT, Gemini CLI, Cursor CLI, and Cursor IDE based on 8 foundation LLMs. The
results demonstrate that current frontier CUAs do not adequately cover OS
security-centric threats. These capabilities of CUAs reduce dependence on
custom malware and deep domain expertise, enabling even inexperienced attackers
to mount complex enterprise intrusions, which raises social concern about the
responsibility and security of CUAs.

</details>


### [115] [Unsupervised Backdoor Detection and Mitigation for Spiking Neural Networks](https://arxiv.org/abs/2510.06629)
*Jiachen Li,Bang Wu,Xiaoyu Xia,Xiaoning Liu,Xun Yi,Xiuzhen Zhang*

Main category: cs.CR

TL;DR: Proposes TMPBD for detecting backdoor attacks in SNNs using temporal membrane potential statistics, and NDSBM for mitigating attacks by suppressing malicious neurons while preserving clean accuracy.


<details>
  <summary>Details</summary>
Motivation: SNNs have superior energy efficiency but their security under backdoor attacks is understudied. Traditional ANN defenses fail in SNNs due to event-driven and temporal dependencies.

Method: TMPBD uses maximum margin statistics of temporal membrane potential in final spiking layer for unsupervised detection. NDSBM clamps dendritic connections between early convolutional layers to suppress malicious neurons using TMP from clean unlabeled data.

Result: TMPBD achieves 100% detection accuracy. NDSBM reduces attack success rate from 100% to 8.44%, and to 2.81% when combined with detection, without degrading clean accuracy.

Conclusion: The proposed framework effectively addresses backdoor attacks in SNNs through temporal membrane potential analysis and neural dendrite suppression, achieving high detection and mitigation performance.

Abstract: Spiking Neural Networks (SNNs) have gained increasing attention for their
superior energy efficiency compared to Artificial Neural Networks (ANNs).
However, their security aspects, particularly under backdoor attacks, have
received limited attention. Existing defense methods developed for ANNs perform
poorly or can be easily bypassed in SNNs due to their event-driven and temporal
dependencies. This paper identifies the key blockers that hinder traditional
backdoor defenses in SNNs and proposes an unsupervised post-training detection
framework, Temporal Membrane Potential Backdoor Detection (TMPBD), to overcome
these challenges. TMPBD leverages the maximum margin statistics of temporal
membrane potential (TMP) in the final spiking layer to detect target labels
without any attack knowledge or data access. We further introduce a robust
mitigation mechanism, Neural Dendrites Suppression Backdoor Mitigation (NDSBM),
which clamps dendritic connections between early convolutional layers to
suppress malicious neurons while preserving benign behaviors, guided by TMP
extracted from a small, clean, unlabeled dataset. Extensive experiments on
multiple neuromorphic benchmarks and state-of-the-art input-aware dynamic
trigger attacks demonstrate that TMPBD achieves 100% detection accuracy, while
NDSBM reduces the attack success rate from 100% to 8.44%, and to 2.81% when
combined with detection, without degrading clean accuracy.

</details>


### [116] [Distilling Lightweight Language Models for C/C++ Vulnerabilities](https://arxiv.org/abs/2510.06645)
*Zhiyuan Wei,Xiaoxuan Yang,Jing Sun,Zijian Zhang*

Main category: cs.CR

TL;DR: FineSec is a framework that uses knowledge distillation from large language models to efficiently detect vulnerabilities in C/C++ code with high accuracy and low computational cost.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of software systems leads to more security vulnerabilities, requiring robust detection methods. LLMs show promise but their potential for automated code vulnerability detection is underexplored.

Method: Uses knowledge distillation to transfer expertise from large teacher models to compact student models. Integrates data preparation, training, evaluation, and continuous learning into a unified single-task workflow.

Result: Extensive evaluations on C/C++ codebases show superiority over both base models and larger LLMs in identifying complex vulnerabilities and logical flaws.

Conclusion: FineSec establishes itself as a practical and scalable solution for real-world software security, with datasets, source code, and results publicly available for reproducibility.

Abstract: The increasing complexity of modern software systems exacerbates the
prevalence of security vulnerabilities, posing risks of severe breaches and
substantial economic loss. Consequently, robust code vulnerability detection is
essential for software security. While Large Language Models (LLMs) have
demonstrated remarkable capabilities in natural language processing, their
potential for automated code vulnerability detection remains underexplored.
This paper presents FineSec, a novel framework that harnesses LLMs through
knowledge distillation to enable efficient and precise vulnerability
identification in C/C++ codebases. FineSec utilizes knowledge distillation to
transfer expertise from large teacher models to compact student models,
achieving high accuracy with minimal computational cost. By integrating data
preparation, training, evaluation, and continuous learning into a unified,
single-task workflow, FineSec offers a streamlined approach. Extensive
evaluations on C/C++ codebases demonstrate its superiority over both base
models and larger LLMs in identifying complex vulnerabilities and logical
flaws, establishing FineSec as a practical and scalable solution for real-world
software security. To facilitate reproducibility, the datasets, source code,
and experimental results are made publicly available at:
https://github.com/yangxiaoxuan123/FineSec_detect.

</details>


### [117] [Differentially Private Synthetic Text Generation for Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2510.06719)
*Junki Mori,Kazuya Kakizaki,Taiki Miyagawa,Jun Sakuma*

Main category: cs.CR

TL;DR: DP-SynRAG is a privacy-preserving RAG framework that generates differentially private synthetic databases using LLMs, avoiding repeated noise injection and enabling reusable synthetic text with fixed privacy budget.


<details>
  <summary>Details</summary>
Motivation: Existing private RAG methods rely on query-time differential privacy, which requires repeated noise injection and leads to accumulated privacy loss, limiting their application in sensitive domains.

Method: DP-SynRAG extends private prediction by using LLMs to generate differentially private synthetic RAG databases that mimic subsampled database records, allowing the synthetic text to be reused without additional privacy costs.

Result: Experiments show DP-SynRAG achieves superior performance compared to state-of-the-art private RAG systems while maintaining a fixed privacy budget.

Conclusion: DP-SynRAG offers a scalable solution for privacy-preserving RAG by generating reusable differentially private synthetic databases, avoiding the privacy accumulation issues of query-time DP methods.

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
grounding them in external knowledge. However, its application in sensitive
domains is limited by privacy risks. Existing private RAG methods typically
rely on query-time differential privacy (DP), which requires repeated noise
injection and leads to accumulated privacy loss. To address this issue, we
propose DP-SynRAG, a framework that uses LLMs to generate differentially
private synthetic RAG databases. Unlike prior methods, the synthetic text can
be reused once created, thereby avoiding repeated noise injection and
additional privacy costs. To preserve essential information for downstream RAG
tasks, DP-SynRAG extends private prediction, which instructs LLMs to generate
text that mimics subsampled database records in a DP manner. Experiments show
that DP-SynRAG achieves superior performanec to the state-of-the-art private
RAG systems while maintaining a fixed privacy budget, offering a scalable
solution for privacy-preserving RAG.

</details>


### [118] [Bionetta: Efficient Client-Side Zero-Knowledge Machine Learning Proving](https://arxiv.org/abs/2510.06784)
*Dmytro Zakharov,Oleksandr Kurbatov,Artem Sdobnov,Lev Soukhanov,Yevhenii Sekhin,Vitalii Volovyk,Mykhailo Velykodnyi,Mark Cherepovskyi,Kyrylo Baibula,Lasha Antadze,Pavlo Kravchenko,Volodymyr Dubinin,Yaroslav Panasenko*

Main category: cs.CR

TL;DR: Bionetta framework outperforms similar tools like EZKL, Lagrange's deep-prove, and zkml in proving time for neural networks, enabling mobile device deployment with EVM compatibility.


<details>
  <summary>Details</summary>
Motivation: To enable client-side proving applications on mobile devices and native EVM smart contracts without overwhelming proof size and verification overheads.

Method: Uses UltraGroth-based zero-knowledge machine learning framework with custom-crafted neural networks, accepting increased preprocessing costs for better deployment capabilities.

Result: Significant boost in proving time for neural networks, making them provable on mobile devices while maintaining EVM compatibility with manageable proof size.

Conclusion: Bionetta is the only currently deployable solution for native EVM smart contracts with practical proof size and verification costs, despite higher preprocessing requirements.

Abstract: In this report, we compare the performance of our UltraGroth-based
zero-knowledge machine learning framework Bionetta to other tools of similar
purpose such as EZKL, Lagrange's deep-prove, or zkml. The results show a
significant boost in the proving time for custom-crafted neural networks: they
can be proven even on mobile devices, enabling numerous client-side proving
applications. While our scheme increases the cost of one-time preprocessing
steps, such as circuit compilation and generating trusted setup, our approach
is, to the best of our knowledge, the only one that is deployable on the native
EVM smart contracts without overwhelming proof size and verification overheads.

</details>


### [119] [Exposing Citation Vulnerabilities in Generative Engines](https://arxiv.org/abs/2510.06823)
*Riku Mochizuki,Shusuke Komatsu,Souta Noguchi,Kazuto Ataka*

Main category: cs.CR

TL;DR: This paper analyzes generative engines' vulnerability to poisoning attacks through web citations, introduces evaluation criteria to assess poisoning threats, and shows US political answers have higher poisoning risk than Japan.


<details>
  <summary>Details</summary>
Motivation: Generative engines are vulnerable to poisoning attacks since anyone can publish web content, but existing citation evaluation focuses on faithfulness rather than source selection for defense.

Method: Introduce evaluation criteria to classify citation publisher attributes and estimate content-injection barrier, then conduct experiments in political domains in Japan and US.

Result: Citations from official party websites are 25%-45% in US vs 60%-65% in Japan, indicating higher poisoning risk in US; low-barrier sources are frequently cited but poorly reflected in answers.

Conclusion: Primary source publishers need better exposure strategies, but existing techniques are limited by language differences; poisoning threats exist in current generative engines.

Abstract: We analyze answers generated by generative engines (GEs) from the
perspectives of citation publishers and the content-injection barrier, defined
as the difficulty for attackers to manipulate answers to user prompts by
placing malicious content on the web. GEs integrate two functions: web search
and answer generation that cites web pages using large language models. Because
anyone can publish information on the web, GEs are vulnerable to poisoning
attacks. Existing studies of citation evaluation focus on how faithfully answer
content reflects cited sources, leaving unexamined which web sources should be
selected as citations to defend against poisoning attacks. To fill this gap, we
introduce evaluation criteria that assess poisoning threats using the citation
information contained in answers. Our criteria classify the publisher
attributes of citations to estimate the content-injection barrier thereby
revealing the threat of poisoning attacks in current GEs. We conduct
experiments in political domains in Japan and the United States (U.S.) using
our criteria and show that citations from official party websites (primary
sources) are approximately \(25\%\)--\(45\%\) in the U.S. and
\(60\%\)--\(65\%\) in Japan, indicating that U.S. political answers are at
higher risk of poisoning attacks. We also find that sources with low
content-injection barriers are frequently cited yet are poorly reflected in
answer content. To mitigate this threat, we discuss how publishers of primary
sources can increase exposure of their web content in answers and show that
well-known techniques are limited by language differences.

</details>


### [120] [I Can't Patch My OT Systems! A Look at CISA's KEVC Workarounds & Mitigations for OT](https://arxiv.org/abs/2510.06951)
*Philip Huff,Nishka Gandu,Pavel Novák*

Main category: cs.CR

TL;DR: Analysis of CISA's Known Exploitable Vulnerabilities Catalog reveals that while most vulnerabilities affect OT environments, only 13% provide vendor workarounds or mitigations as alternatives to patching, highlighting a significant gap in remediation options for operational technology.


<details>
  <summary>Details</summary>
Motivation: To assess whether current publicly available information about known exploitable vulnerabilities is sufficient for effective and reliable remediation in operational technology (OT) environments, given the unique constraints and requirements of OT systems.

Method: Analyzed all entries in CISA's Known Exploitable Vulnerabilities Catalog (KEVC) through July 2025 to determine the extent to which OT environments can rely on existing remediation recommendations and examined the feasibility of developing workarounds based on vulnerability characteristics.

Result: Found that although most KEVC entries could affect OT environments, only 13% include vendor workarounds or mitigations as alternatives to patching, indicating a significant deficiency in available remediation options for OT settings.

Conclusion: There is a critical need for more comprehensive remediation guidance specifically tailored for OT environments, and early evidence suggests that developing alternative mitigation strategies based on vulnerability characteristics is a feasible approach to address this gap.

Abstract: We examine the state of publicly available information about known
exploitable vulnerabilities applicable to operational technology (OT)
environments. Specifically, we analyze the Known Exploitable Vulnerabilities
Catalog (KEVC) maintained by the US Department of Homeland Security
Cybersecurity and Infrastructure Security Agency (CISA) to assess whether
currently available data is sufficient for effective and reliable remediation
in OT settings. Our team analyzed all KEVC entries through July 2025 to
determine the extent to which OT environments can rely on existing remediation
recommendations. We found that although most entries in the KEVC could affect
OT environments, only 13% include vendor workarounds or mitigations as
alternatives to patching. This paper also examines the feasibility of
developing such alternatives based on vulnerability and exploit
characteristics, and we present early evidence of success with this approach.

</details>


### [121] [VelLMes: A high-interaction AI-based deception framework](https://arxiv.org/abs/2510.06975)
*Muris Sladić,Veronica Valeros,Carlos Catania,Sebastian Garcia*

Main category: cs.CR

TL;DR: VelLMes is an AI-based deception framework that simulates multiple protocols/services (SSH, MySQL, POP3, HTTP) as honeypots, evaluated with human attackers showing 30% deception success rate for SSH shells.


<details>
  <summary>Details</summary>
Motivation: There are few LLM-based deception systems, existing ones are limited to single services like SSH, and lack human attacker evaluation. Generative AI can enhance cyber-deception by creating realistic honeytokens and simulated systems.

Method: Developed VelLMes framework using LLMs to simulate multiple protocols/services. Evaluated generative capabilities with unit tests and deception capabilities with 89 human attackers for SSH shells. Also deployed 10 SSH honeypot instances on the Internet.

Result: LLMs achieved 100% passing rate in unit tests with careful prompting. 30% of human attackers were deceived into thinking SSH honeypot was real. Internet deployment showed LLM honeypots perform well against unstructured attacks, responding correctly to most commands.

Conclusion: LLM-based honeypots like VelLMes can effectively simulate multiple services and deceive human attackers, demonstrating practical value for cyber-deception with realistic interactive capabilities.

Abstract: There are very few SotA deception systems based on Large Language Models. The
existing ones are limited only to simulating one type of service, mainly SSH
shells. These systems - but also the deception technologies not based on LLMs -
lack an extensive evaluation that includes human attackers. Generative AI has
recently become a valuable asset for cybersecurity researchers and
practitioners, and the field of cyber-deception is no exception. Researchers
have demonstrated how LLMs can be leveraged to create realistic-looking
honeytokens, fake users, and even simulated systems that can be used as
honeypots. This paper presents an AI-based deception framework called VelLMes,
which can simulate multiple protocols and services such as SSH Linux shell,
MySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus
VelLMes offers a variety of choices for deception design based on the users'
needs. VelLMes is designed to be attacked by humans, so interactivity and
realism are key for its performance. We evaluate the generative capabilities
and the deception capabilities. Generative capabilities were evaluated using
unit tests for LLMs. The results of the unit tests show that, with careful
prompting, LLMs can produce realistic-looking responses, with some LLMs having
a 100% passing rate. In the case of the SSH Linux shell, we evaluated deception
capabilities with 89 human attackers. The results showed that about 30% of the
attackers thought that they were interacting with a real system when they were
assigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH
Linux shell honeypot on the Internet to capture real-life attacks. Analysis of
these attacks showed us that LLM honeypots simulating Linux shells can perform
well against unstructured and unexpected attacks on the Internet, responding
correctly to most of the issued commands.

</details>


### [122] [RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning](https://arxiv.org/abs/2510.06994)
*Artur Horal,Daniel Pina,Henrique Paz,Iago Paulo,João Soares,Rafael Ferreira,Diogo Tavares,Diogo Glória-Silva,João Magalhães,David Semedo*

Main category: cs.CR

TL;DR: RedTWIZ is an adaptive multi-turn red teaming framework for auditing LLM robustness in AI-assisted software development, featuring systematic assessment, diverse attack generation, and hierarchical planning.


<details>
  <summary>Details</summary>
Motivation: To comprehensively evaluate and expose weaknesses in LLMs' robustness through systematic red teaming, addressing the need for robust assessment of conversational jailbreaks.

Method: Combines three components: systematic assessment of LLM conversational jailbreaks, diverse generative multi-turn attack suite with compositional strategies, and hierarchical attack planner that adaptively targets specific LLM vulnerabilities.

Result: Experimental results show the multi-turn adversarial attack strategies can successfully make state-of-the-art LLMs produce unsafe generations, demonstrating framework effectiveness.

Conclusion: Highlights the pressing need for more research into enhancing LLM robustness, as current models remain vulnerable to sophisticated multi-turn attacks.

Abstract: This paper presents the vision, scientific contributions, and technical
details of RedTWIZ: an adaptive and diverse multi-turn red teaming framework,
to audit the robustness of Large Language Models (LLMs) in AI-assisted software
development. Our work is driven by three major research streams: (1) robust and
systematic assessment of LLM conversational jailbreaks; (2) a diverse
generative multi-turn attack suite, supporting compositional, realistic and
goal-oriented jailbreak conversational strategies; and (3) a hierarchical
attack planner, which adaptively plans, serializes, and triggers attacks
tailored to specific LLM's vulnerabilities. Together, these contributions form
a unified framework -- combining assessment, attack generation, and strategic
planning -- to comprehensively evaluate and expose weaknesses in LLMs'
robustness. Extensive evaluation is conducted to systematically assess and
analyze the performance of the overall system and each component. Experimental
results demonstrate that our multi-turn adversarial attack strategies can
successfully lead state-of-the-art LLMs to produce unsafe generations,
highlighting the pressing need for more research into enhancing LLM's
robustness.

</details>


### [123] [Pseudo-MDPs: A Novel Framework for Efficiently Optimizing Last Revealer Seed Manipulations in Blockchains](https://arxiv.org/abs/2510.07080)
*Maxime Reynouard*

Main category: cs.CR

TL;DR: This paper introduces pseudo-MDPs (pMDPs) to solve computational challenges in MDPs, specifically targeting the Last Revealer Attack in PoS blockchains like Ethereum. It proposes two problem reductions that dramatically improve computational efficiency from O(2^κ κ^(2κ+4)) to O(κ^4) per iteration.


<details>
  <summary>Details</summary>
Motivation: Address computational challenges in solving MDPs for restricted problem classes, particularly motivated by the Last Revealer Attack that undermines fairness in Proof-of-Stake blockchains like Ethereum ($400B market cap).

Method: Introduces pseudo-MDPs (pMDPs) framework and proposes two distinct problem reductions to standard MDPs. Combines these reductions to significantly improve dynamic programming algorithms like value iteration.

Result: Achieves dramatic computational complexity reduction from O(2^κ κ^(2κ+4)) to O(κ^4) per iteration for the Last Revealer Attack (where κ=32 in Ethereum). Provides exponentially fast convergence to optimal solution and simplifies policy extraction for resource-constrained agents.

Conclusion: The pMDP framework effectively solves large-scale MDP problems, advances MDP study, and contributes to understanding blockchain security vulnerabilities. Validated through case studies including Ethereum's random seed consensus protocol.

Abstract: This study tackles the computational challenges of solving Markov Decision
Processes (MDPs) for a restricted class of problems. It is motivated by the
Last Revealer Attack (LRA), which undermines fairness in some Proof-of-Stake
(PoS) blockchains such as Ethereum (\$400B market capitalization). We introduce
pseudo-MDPs (pMDPs) a framework that naturally models such problems and propose
two distinct problem reductions to standard MDPs. One problem reduction
provides a novel, counter-intuitive perspective, and combining the two problem
reductions enables significant improvements in dynamic programming algorithms
such as value iteration. In the case of the LRA which size is parameterized by
$\kappa$ (in Ethereum's case $\kappa$ = 32), we reduce the computational
complexity from O(2^$\kappa$ $\kappa$^2^($\kappa$+2)) to O($\kappa$^4) (per
iteration). This solution also provide the usual benefits from Dynamic
Programming solutions: exponentially fast convergence toward the optimal
solution is guaranteed. The dual perspective also simplifies policy extraction,
making the approach well-suited for resource-constrained agents who can operate
with very limited memory and computation once the problem has been solved.
Furthermore, we generalize those results to a broader class of MDPs, enhancing
their applicability. The framework is validated through two case studies: a
fictional card game and the LRA on the Ethereum random seed consensus protocol.
These applications demonstrate the framework's ability to solve large-scale
problems effectively while offering actionable insights into optimal
strategies. This work advances the study of MDPs and contributes to
understanding security vulnerabilities in blockchain systems.

</details>


### [124] [GNN-enhanced Traffic Anomaly Detection for Next-Generation SDN-Enabled Consumer Electronics](https://arxiv.org/abs/2510.07109)
*Guan-Yan Yang,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: Proposes GNN-NAD framework combining SDN and CFN for IoT security, using GSAGE+RF model to detect anomalies with superior performance.


<details>
  <summary>Details</summary>
Motivation: Consumer IoT devices are vulnerable to attacks like DDoS and web threats, while existing deep learning detection systems are too complex and require manual configuration.

Method: Integrates SDN and CFN in scalable network model, uses GNN-based framework (GNN-NAD) that fuses static vulnerability-aware attack graphs with dynamic traffic features, employing GSAGE for graph learning and Random Forest classifier.

Result: Achieves superior accuracy, recall, precision, and F1 score metrics in CE environments, outperforming current methods even with small sample sizes.

Conclusion: Advances security and efficiency of next-generation intelligent consumer electronics networks through the proposed scalable framework.

Abstract: Consumer electronics (CE) connected to the Internet of Things are susceptible
to various attacks, including DDoS and web-based threats, which can compromise
their functionality and facilitate remote hijacking. These vulnerabilities
allow attackers to exploit CE for broader system attacks while enabling the
propagation of malicious code across the CE network, resulting in device
failures. Existing deep learning-based traffic anomaly detection systems
exhibit high accuracy in traditional network environments but are often overly
complex and reliant on static infrastructure, necessitating manual
configuration and management. To address these limitations, we propose a
scalable network model that integrates Software-defined Networking (SDN) and
Compute First Networking (CFN) for next-generation CE networks. In this network
model, we propose a Graph Neural Networks-based Network Anomaly Detection
framework (GNN-NAD) that integrates SDN-based CE networks and enables the CFN
architecture. GNN-NAD uniquely fuses a static, vulnerability-aware attack graph
with dynamic traffic features, providing a holistic view of network security.
The core of the framework is a GNN model (GSAGE) for graph representation
learning, followed by a Random Forest (RF) classifier. This design (GSAGE+RF)
demonstrates superior performance compared to existing feature selection
methods. Experimental evaluations on CE environment reveal that GNN-NAD
achieves superior metrics in accuracy, recall, precision, and F1 score, even
with small sample sizes, exceeding the performance of current network anomaly
detection methods. This work advances the security and efficiency of
next-generation intelligent CE networks.

</details>


### [125] [A multi-layered embedded intrusion detection framework for programmable logic controllers](https://arxiv.org/abs/2510.07171)
*Rishabh Das. Aaron Werth,Tommy Morris*

Main category: cs.CR

TL;DR: An embedded intrusion detection system for industrial control systems that runs inside PLCs, using header-level telemetry to detect network attacks with minimal latency impact.


<details>
  <summary>Details</summary>
Motivation: Industrial control systems lack layered defenses, making them vulnerable when trusted endpoints are compromised, which can lead to unsafe actuator commands and safety risks.

Method: Combines semi-supervised anomaly detector and supervised attack classifier using header-level telemetry, evaluated on oil-terminal testbed with tanker-truck loading datasets.

Result: Anomaly detector: 0 missed attacks, 0.998 Matthews correlation; Supervised stage: 97.37% hold-out accuracy, 97.03% external accuracy; adds only 2,031 microseconds median latency without impacting PLC cycle time.

Conclusion: The embedded architecture provides multi-layer security that meets real-time industrial system requirements while effectively detecting network attacks.

Abstract: Industrial control system (ICS) operations use trusted endpoints like human
machine interfaces (HMIs) and workstations to relay commands to programmable
logic controllers (PLCs). Because most PLCs lack layered defenses, compromise
of a trusted endpoint can drive unsafe actuator commands and risk
safety-critical operation. This research presents an embedded intrusion
detection system that runs inside the controller and uses header-level
telemetry to detect and respond to network attacks. The system combines a
semi-supervised anomaly detector and a supervised attack classifier. We
evaluate the approach on a midstream oil-terminal testbed using three datasets
collected during tanker-truck loading. The anomaly detector achieves zero
missed attacks, corresponding to 0.998 Matthews correlation. The supervised
stage attains 97.37 percent hold-out accuracy and 97.03 percent external
accuracy. The embedded design adds a median of 2,031 microseconds of end-to-end
latency and does not impact PLC's cycle time. The proposed architecture
provides a multi-layer embedded security that meets the real-time requirements
of an industrial system.

</details>


### [126] [Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of Privacy Risks in LLM Agent Interactions](https://arxiv.org/abs/2510.07176)
*Yixiang Zhang,Xinhao Deng,Zhongyi Gu,Yihao Chen,Ke Xu,Qi Li,Jianping Wu*

Main category: cs.CR

TL;DR: LLM agents' interactive behaviors create identifiable traffic patterns that can reveal agent activities, specific agents, and sensitive user attributes, posing privacy risks.


<details>
  <summary>Details</summary>
Motivation: To highlight the privacy risks associated with LLM agents' interactive behaviors that leave distinctive fingerprints in encrypted traffic, enabling adversaries to infer sensitive information.

Method: Developed AgentPrint to analyze traffic patterns associated with agent workflows and tool invocations for agent identification and user attribute inference.

Result: AgentPrint achieves F1-score of 0.866 in agent identification and 73.9%/69.1% top-3 accuracy in user attribute inference for simulated/real-user settings.

Conclusion: The interactivity that empowers LLM agents also exposes user privacy, requiring urgent technical countermeasures and regulatory safeguards.

Abstract: Large Language Models (LLMs) are increasingly deployed as agents that
orchestrate tasks and integrate external tools to execute complex workflows. We
demonstrate that these interactive behaviors leave distinctive fingerprints in
encrypted traffic exchanged between users and LLM agents. By analyzing traffic
patterns associated with agent workflows and tool invocations, adversaries can
infer agent activities, distinguish specific agents, and even profile sensitive
user attributes. To highlight this risk, we develop AgentPrint, which achieves
an F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3
accuracy in user attribute inference for simulated- and real-user settings,
respectively. These results uncover an overlooked risk: the very interactivity
that empowers LLM agents also exposes user privacy, underscoring the urgent
need for technical countermeasures alongside regulatory and policy safeguards.

</details>


### [127] [Security-Robustness Trade-offs in Diffusion Steganography: A Comparative Analysis of Pixel-Space and VAE-Based Architectures](https://arxiv.org/abs/2510.07219)
*Yuhua Xu,Wei Sun,Chengpei Tang,Jiaxing Lu,Jingying Zhou,Chen Gu*

Main category: cs.CR

TL;DR: This paper introduces an efficient generative steganography framework using approximate Gaussian mapping and reveals a security-robustness trade-off between pixel-space models (high security but fragile) and VAE-based systems (robust but vulnerable).


<details>
  <summary>Details</summary>
Motivation: Current generative steganography research focuses on computationally expensive perfect Gaussian mappings within single architectures, lacking systematic comparison between different model types.

Method: Developed an efficient framework based on approximate Gaussian mapping with scale factor calibration through capacity-aware adaptive optimization, used as a unified tool to analyze pixel-space vs VAE-based latent-space systems.

Result: Found architecture-dependent trade-off: pixel-space models achieve high security against steganalysis but are fragile to channel distortions, while VAE-based systems offer robustness at security cost. VAE encoder provides robustness via manifold regularization, while decoder introduces vulnerabilities.

Conclusion: The study characterizes conflicting architectural roles in generative steganography and establishes foundation for future research on balancing security and robustness.

Abstract: Current generative steganography research mainly pursues computationally
expensive mappings to perfect Gaussian priors within single diffusion model
architectures. This work introduces an efficient framework based on approximate
Gaussian mapping governed by a scale factor calibrated through capacity-aware
adaptive optimization. Using this framework as a unified analytical tool,
systematic comparative analysis of steganography in pixel-space models versus
VAE-based latent-space systems is conducted. The investigation reveals a
pronounced architecture dependent security-robustness trade-off: pixel-space
models achieve high security against steganalysis but exhibit fragility to
channel distortions, while VAE-based systems like Stable Diffusion offer
substantial robustness at the cost of security vulnerabilities. Further
analysis indicates that the VAE component drives this behavior through opposing
mechanisms where the encoder confers robustness via manifold regularization
while the decoder introduces vulnerabilities by amplifying latent perturbations
into detectable artifacts. These findings characterize the conflicting
architectural roles in generative steganography and establish a foundation for
future research.

</details>


### [128] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: This paper proposes a Graph Structure Learning (GSL)-based framework to protect Internet of Energy (IoE) networks against sophisticated cyber threats by jointly optimizing graph topology and node representations for inherent adversarial resistance.


<details>
  <summary>Details</summary>
Motivation: The interconnectivity of IoE systems exposes critical energy infrastructure to sophisticated cyber threats with heightened public safety consequences, demanding resilient solutions beyond traditional safeguards.

Method: Proposes a Graph Structure Learning (GSL)-based safeguards framework that jointly optimizes graph topology and node representations to inherently resist adversarial network model manipulation.

Result: Through conceptual overview, architectural discussion, and case study on a security dataset, the paper demonstrates GSL's superior robustness over representative methods.

Conclusion: GSL offers a viable path to secure IoE networks against evolving attacks and enhances resilience for critical infrastructure management, with identified open challenges and future research directions in this novel area.

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [129] [RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases](https://arxiv.org/abs/2510.06267)
*Khartik Uppalapati,Shakeel Abdulkareem,Bora Yimenicioglu*

Main category: cs.LG

TL;DR: RareGraph-Synth is a knowledge-guided diffusion framework that generates privacy-preserving synthetic EHR trajectories for ultra-rare diseases using a heterogeneous knowledge graph with 8M edges.


<details>
  <summary>Details</summary>
Motivation: To enable safer data sharing for rare-disease research by generating realistic yet privacy-preserving synthetic EHR data that maintains biological plausibility while protecting patient privacy.

Method: Unifies five biomedical resources into a heterogeneous KG, uses meta-path scores to modulate noise schedule in forward SDE, and employs reverse denoiser to generate timestamped sequences of lab-medication-adverse-event triples without PHI.

Result: 40% lower categorical MMD vs unguided diffusion, >60% lower vs GANs; AUROC of 0.53 in membership inference attack (below 0.55 safe threshold vs 0.61±0.03 for baselines).

Conclusion: Integrating biomedical knowledge graphs directly into diffusion noise schedules can simultaneously enhance fidelity and privacy for rare-disease data sharing.

Abstract: We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion
framework that generates realistic yet privacy-preserving synthetic
electronic-health-record (EHR) trajectories for ultra-rare diseases.
RareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human
Phenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA
Adverse Event Reporting System (FAERS) into a heterogeneous knowledge graph
comprising approximately 8 M typed edges. Meta-path scores extracted from this
8-million-edge KG modulate the per-token noise schedule in the forward
stochastic differential equation, steering generation toward biologically
plausible lab-medication-adverse-event co-occurrences while retaining
score-based diffusion model stability. The reverse denoiser then produces
timestamped sequences of lab-code, medication-code, and adverse-event-flag
triples that contain no protected health information. On simulated
ultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean
Discrepancy by 40 percent relative to an unguided diffusion baseline and by
greater than 60 percent versus GAN counterparts, without sacrificing downstream
predictive utility. A black-box membership-inference evaluation using the
DOMIAS attacker yields AUROC approximately 0.53, well below the 0.55
safe-release threshold and substantially better than the approximately 0.61
plus or minus 0.03 observed for non-KG baselines, demonstrating strong
resistance to re-identification. These results suggest that integrating
biomedical knowledge graphs directly into diffusion noise schedules can
simultaneously enhance fidelity and privacy, enabling safer data sharing for
rare-disease research.

</details>


### [130] [MCCE: A Framework for Multi-LLM Collaborative Co-Evolution](https://arxiv.org/abs/2510.06270)
*Nian Ran,Zhongzheng Li,Yue Wang,Qingsong Ran,Xiaoyuan Zhang,Shikun Feng,Richard Allmendinger,Xiaoguang Zhao*

Main category: cs.LG

TL;DR: MCCE is a hybrid framework combining frozen closed-source LLMs with trainable small models for multi-objective optimization, achieving state-of-the-art performance in drug design through collaborative co-evolution.


<details>
  <summary>Details</summary>
Motivation: Multi-objective discrete optimization problems like molecular design face challenges in vast combinatorial spaces. Traditional methods get stuck in local optima, while LLMs offer strong priors but closed-source models cannot learn from experience and small models lack reasoning capability.

Method: Multi-LLM Collaborative Co-evolution (MCCE) framework that unites a frozen closed-source LLM with a lightweight trainable model. Uses trajectory memory and reinforcement learning to progressively refine the small model, with both models supporting each other in global exploration.

Result: Experiments on multi-objective drug design benchmarks show MCCE achieves state-of-the-art Pareto front quality and consistently outperforms baselines.

Conclusion: MCCE presents a new paradigm for continual evolution in hybrid LLM systems, effectively combining knowledge-driven exploration with experience-driven learning for superior optimization performance.

Abstract: Multi-objective discrete optimization problems, such as molecular design,
pose significant challenges due to their vast and unstructured combinatorial
spaces. Traditional evolutionary algorithms often get trapped in local optima,
while expert knowledge can provide crucial guidance for accelerating
convergence. Large language models (LLMs) offer powerful priors and reasoning
ability, making them natural optimizers when expert knowledge matters. However,
closed-source LLMs, though strong in exploration, cannot update their
parameters and thus cannot internalize experience. Conversely, smaller open
models can be continually fine-tuned but lack broad knowledge and reasoning
strength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid
framework that unites a frozen closed-source LLM with a lightweight trainable
model. The system maintains a trajectory memory of past search processes; the
small model is progressively refined via reinforcement learning, with the two
models jointly supporting and complementing each other in global exploration.
Unlike model distillation, this process enhances the capabilities of both
models through mutual inspiration. Experiments on multi-objective drug design
benchmarks show that MCCE achieves state-of-the-art Pareto front quality and
consistently outperforms baselines. These results highlight a new paradigm for
enabling continual evolution in hybrid LLM systems, combining knowledge-driven
exploration with experience-driven learning.

</details>


### [131] [RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets](https://arxiv.org/abs/2510.06278)
*M. Sajid,Mushir Akhtar,A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: Proposed RVFL-X, a complex-valued extension of RVFL networks, with two methods for transforming real-valued tabular data to complex representations, achieving superior performance over original RVFL and SOTA RNN variants on 80 UCI datasets.


<details>
  <summary>Details</summary>
Motivation: Complex numbers have superior representational power in neural networks, but their adoption in randomized neural networks (RNNs) has been limited due to lack of effective methods for transforming real-valued tabular datasets into complex-valued representations.

Method: Proposed two methods for generating complex-valued representations from real-valued datasets: natural transformation and autoencoder-driven method. Developed RVFL-X as complex-valued extension of RVFL network, integrating complex transformations while maintaining RVFL's simplicity and efficiency.

Result: Comprehensive evaluations on 80 real-valued UCI datasets demonstrate RVFL-X consistently outperforms both original RVFL and state-of-the-art RNN variants, showing robustness and effectiveness across diverse application domains.

Conclusion: RVFL-X successfully bridges the gap between complex number representational power and practical application in randomized neural networks, providing an effective framework for complex-valued processing of real-world tabular data.

Abstract: Recent advancements in neural networks, supported by foundational theoretical
insights, emphasize the superior representational power of complex numbers.
However, their adoption in randomized neural networks (RNNs) has been limited
due to the lack of effective methods for transforming real-valued tabular
datasets into complex-valued representations. To address this limitation, we
propose two methods for generating complex-valued representations from
real-valued datasets: a natural transformation and an autoencoder-driven
method. Building on these mechanisms, we propose RVFL-X, a complex-valued
extension of the random vector functional link (RVFL) network. RVFL-X
integrates complex transformations into real-valued datasets while maintaining
the simplicity and efficiency of the original RVFL architecture. By leveraging
complex components such as input, weights, and activation functions, RVFL-X
processes complex representations and produces real-valued outputs.
Comprehensive evaluations on 80 real-valued UCI datasets demonstrate that
RVFL-X consistently outperforms both the original RVFL and state-of-the-art
(SOTA) RNN variants, showcasing its robustness and effectiveness across diverse
application domains.

</details>


### [132] [On knot detection via picture recognition](https://arxiv.org/abs/2510.06284)
*Anne Dranowski,Yura Kabkov,Daniel Tubbenhauer*

Main category: cs.LG

TL;DR: Using machine learning (CNNs and transformers) combined with traditional knot invariants to recognize knots from photos, with initial success in predicting crossing numbers from images.


<details>
  <summary>Details</summary>
Motivation: To develop a system that can automatically recognize knots from photos, bridging visual perception with topological analysis.

Method: Two-stage approach: 1) Machine learning (CNNs and transformers) to extract structural information from images and predict crossing numbers; 2) Symbolic reconstruction into planar diagram codes for computing invariants like Jones polynomial.

Result: Lightweight CNN and transformer architectures can successfully recover meaningful structural information and predict crossing numbers directly from knot images.

Conclusion: Combining machine learning for visual perception with traditional knot invariants provides a robust approach for knot classification, leveraging the strengths of both methods.

Abstract: Our goal is to one day take a photo of a knot and have a phone automatically
recognize it. In this expository work, we explain a strategy to approximate
this goal, using a mixture of modern machine learning methods (in particular
convolutional neural networks and transformers for image recognition) and
traditional algorithms (to compute quantum invariants like the Jones
polynomial). We present simple baselines that predict crossing number directly
from images, showing that even lightweight CNN and transformer architectures
can recover meaningful structural information. The longer-term aim is to
combine these perception modules with symbolic reconstruction into planar
diagram (PD) codes, enabling downstream invariant computation for robust knot
classification. This two-stage approach highlights the complementarity between
machine learning, which handles noisy visual data, and invariants, which
enforce rigorous topological distinctions.

</details>


### [133] [Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation](https://arxiv.org/abs/2510.06291)
*Zhiyang Zhang,Ningcong Chen,Xin Zhang,Yanhua Li,Shen Su,Hui Lu,Jun Luo*

Main category: cs.LG

TL;DR: Proposes Trajectory Transformer, a transformer-based model for GPS trajectory generation that outperforms convolution-based diffusion models by reducing deviations and preserving street-level details.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for trajectory generation use convolution-based architectures (like UNet) which suffer from notable deviations and loss of fine-grained street-level details due to limited model capacity.

Method: Uses a transformer backbone for both conditional information embedding and noise prediction, exploring two GPS coordinate embedding strategies: location embedding and longitude-latitude embedding, with analysis at different scales.

Result: Experiments on two real-world datasets show Trajectory Transformer significantly enhances generation quality and effectively alleviates the deviation issues observed in prior approaches.

Conclusion: Transformer-based architecture is superior to convolution-based methods for trajectory generation, providing better quality results and addressing the deviation problems of existing approaches.

Abstract: The widespread use of GPS devices has driven advances in spatiotemporal data
mining, enabling machine learning models to simulate human decision making and
generate realistic trajectories, addressing both data collection costs and
privacy concerns. Recent studies have shown the promise of diffusion models for
high-quality trajectory generation. However, most existing methods rely on
convolution based architectures (e.g. UNet) to predict noise during the
diffusion process, which often results in notable deviations and the loss of
fine-grained street-level details due to limited model capacity. In this paper,
we propose Trajectory Transformer, a novel model that employs a transformer
backbone for both conditional information embedding and noise prediction. We
explore two GPS coordinate embedding strategies, location embedding and
longitude-latitude embedding, and analyze model performance at different
scales. Experiments on two real-world datasets demonstrate that Trajectory
Transformer significantly enhances generation quality and effectively
alleviates the deviation issues observed in prior approaches.

</details>


### [134] [BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression](https://arxiv.org/abs/2510.06293)
*Cristian Meo,Varun Sarathchandran,Avijit Majhi,Shao Hung,Carlo Saccardi,Ruben Imhoff,Roberto Deidda,Remko Uijlenhoet,Justin Dauwels*

Main category: cs.LG

TL;DR: BlockGPT is a generative autoregressive transformer for precipitation nowcasting that uses batched tokenization to predict full 2D fields per time step, achieving superior accuracy and 31x faster inference than baselines.


<details>
  <summary>Details</summary>
Motivation: Current precipitation nowcasting methods have limitations: token-based autoregressive models suffer from flawed inductive biases and slow inference, while diffusion models are computationally intensive. There's a need for accurate and efficient real-time forecasting.

Method: BlockGPT uses a model-agnostic paradigm with batched tokenization (Block method) that factorizes space-time using self-attention within frames and causal attention across frames. It predicts complete 2D precipitation fields at each time step.

Result: On KNMI (Netherlands) and SEVIR (U.S.) datasets, BlockGPT outperforms state-of-the-art baselines including NowcastingGPT and DiffCast+Phydnet in accuracy, event localization, and achieves inference speeds up to 31x faster.

Conclusion: BlockGPT provides an effective solution for precipitation nowcasting by combining the benefits of generative modeling with computational efficiency, making it suitable for real-time weather forecasting applications.

Abstract: Predicting precipitation maps is a highly complex spatiotemporal modeling
task, critical for mitigating the impacts of extreme weather events. Short-term
precipitation forecasting, or nowcasting, requires models that are not only
accurate but also computationally efficient for real-time applications. Current
methods, such as token-based autoregressive models, often suffer from flawed
inductive biases and slow inference, while diffusion models can be
computationally intensive. To address these limitations, we introduce BlockGPT,
a generative autoregressive transformer using batched tokenization (Block)
method that predicts full two-dimensional fields (frames) at each time step.
Conceived as a model-agnostic paradigm for video prediction, BlockGPT
factorizes space-time by using self-attention within each frame and causal
attention across frames; in this work, we instantiate it for precipitation
nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI
(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines
including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)
models. The results show that BlockGPT achieves superior accuracy, event
localization as measured by categorical metrics, and inference speeds up to 31x
faster than comparable baselines.

</details>


### [135] [SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation](https://arxiv.org/abs/2510.06303)
*Shuang Cheng,Yihan Bian,Dawei Liu,Yuhua Jiang,Yihao Liu,Linfeng Zhang,Wenhai Wang,Qipeng Guo,Kai Chen,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: SDAR is a synergistic paradigm that converts autoregressive models into blockwise diffusion models through lightweight adaptation, enabling parallel inference while maintaining training efficiency.


<details>
  <summary>Details</summary>
Motivation: To combine the training efficiency of autoregressive models with the parallel inference capability of diffusion models, avoiding costly end-to-end diffusion training.

Method: Performs lightweight paradigm conversion from well-trained AR models to blockwise diffusion models via brief, data-efficient adaptation. During inference, generates sequences autoregressively across blocks for coherence while decoding tokens within each block in parallel via discrete diffusion.

Result: SDAR achieves efficient AR-to-diffusion conversion with minimal cost, preserving AR-level performance while enabling parallel generation. Larger models show stronger robustness to block size and decoding thresholds, yielding greater speedups without accuracy loss. The 30B MoE model surpasses AR counterpart on scientific reasoning benchmarks like GPQA and ChemBench.

Conclusion: SDAR establishes a practical paradigm that combines strengths of autoregression and diffusion for scalable, high-throughput reasoning, demonstrating enhanced reasoning and domain adaptability.

Abstract: We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies
the training efficiency of autoregressive models with the parallel inference
capability of diffusion. Instead of costly end-to-end diffusion training, SDAR
performs a lightweight paradigm conversion that transforms a well-trained
autoregressive (AR) model into a blockwise diffusion model through brief,
data-efficient adaptation. During inference, SDAR generates sequences
autoregressively across blocks for global coherence while decoding all tokens
within each block in parallel via a discrete diffusion process. Extensive
experiments show that AR models remain substantially more compute-efficient
than masked diffusion models, providing a strong foundation for adaptation.
Building on this insight, SDAR achieves efficient AR-to-diffusion conversion
with minimal cost, preserving AR-level performance while enabling parallel
generation. Scaling studies across dense and Mixture-of-Experts architectures
confirm that SDAR scales without compromise: larger models exhibit stronger
robustness to block size and decoding thresholds, yielding greater speedups
without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning
and domain adaptability. Our 30B MoE model surpasses its AR counterpart on
challenging scientific reasoning benchmarks such as GPQA and ChemBench, and
gains further improvements under test-time scaling methods like majority voting
and pass@k. Together, these results establish SDAR as a practical paradigm that
combines the strengths of autoregression and diffusion for scalable,
high-throughput reasoning.

</details>


### [136] [Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks](https://arxiv.org/abs/2510.06349)
*Moein E. Samadi,Andreas Schuppert*

Main category: cs.LG

TL;DR: Foundation models show limited gains in dynamic real-world domains like intensive care. The paper proposes decentralized small agent networks (SANs) as superior to monolithic models for self-adaptive decision-making in complex systems, though with reduced reproducibility.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adapting complex systems to dynamic environments where foundation models show modest gains, particularly in critical domains like medical diagnosis and treatment that require reliable self-adaptive modeling.

Method: Proposes a decentralized architecture of interacting small agent networks (SANs), where each agent represents specialized substructures covering only subsets of system functions, enabling swarm-learning in diverse swarms.

Result: SANs can overcome the curse of dimensionality barrier and deliver superior decision-making in dynamic environments compared to monolithic foundation models, but with reduced reproducibility in detail.

Conclusion: Decentralized SAN architectures offer a promising alternative to monolithic foundation models for self-adaptive AI in complex dynamic systems, though AI should demonstrate clear superiority in these settings before assuming broader decision-making roles.

Abstract: Foundation models have rapidly advanced AI, raising the question of whether
their decisions will ultimately surpass human strategies in real-world domains.
The exponential, and possibly super-exponential, pace of AI development makes
such analysis elusive. Nevertheless, many application areas that matter for
daily life and society show only modest gains so far; a prominent case is
diagnosing and treating dynamically evolving disease in intensive care.
  The common challenge is adapting complex systems to dynamic environments.
Effective strategies must optimize outcomes in systems composed of strongly
interacting functions while avoiding shared side effects; this requires
reliable, self-adaptive modeling. These tasks align with building digital twins
of highly complex systems whose mechanisms are not fully or quantitatively
understood. It is therefore essential to develop methods for self-adapting AI
models with minimal data and limited mechanistic knowledge. As this challenge
extends beyond medicine, AI should demonstrate clear superiority in these
settings before assuming broader decision-making roles.
  We identify the curse of dimensionality as a fundamental barrier to efficient
self-adaptation and argue that monolithic foundation models face conceptual
limits in overcoming it. As an alternative, we propose a decentralized
architecture of interacting small agent networks (SANs). We focus on agents
representing the specialized substructure of the system, where each agent
covers only a subset of the full system functions. Drawing on mathematical
results on the learning behavior of SANs and evidence from existing
applications, we argue that swarm-learning in diverse swarms can enable
self-adaptive SANs to deliver superior decision-making in dynamic environments
compared with monolithic foundation models, though at the cost of reduced
reproducibility in detail.

</details>


### [137] [PIKAN: Physics-Inspired Kolmogorov-Arnold Networks for Explainable UAV Channel Modelling](https://arxiv.org/abs/2510.06355)
*Kürşat Tekbıyık,Güneş Karabulut Kurt,Antoine Lesage-Landry*

Main category: cs.LG

TL;DR: Proposed PIKAN model embeds physical principles into neural networks for UAV channel modeling, achieving comparable accuracy to DL models with only 232 parameters while providing explainable expressions.


<details>
  <summary>Details</summary>
Motivation: UAV communications need accurate yet interpretable A2G channel models that can adapt to nonstationary environments. Current deterministic models are rigid while DL models lack explainability.

Method: Physics-Inspired Kolmogorov-Arnold Network (PIKAN) that embeds physical principles (free-space path loss, two-ray reflections) as flexible inductive biases in the learning process, unlike rigid PINNs.

Result: PIKAN achieves comparable accuracy to DL models with only 232 parameters (37x lighter than MLP baselines), provides symbolic explainable expressions aligned with propagation laws, and maintains correlation with measurements.

Conclusion: PIKAN is an efficient, interpretable, and scalable solution for UAV channel modeling in beyond-5G and 6G networks, bridging the gap between accuracy and explainability.

Abstract: Unmanned aerial vehicle (UAV) communications demand accurate yet
interpretable air-to-ground (A2G) channel models that can adapt to
nonstationary propagation environments. While deterministic models offer
interpretability and deep learning (DL) models provide accuracy, both
approaches suffer from either rigidity or a lack of explainability. To bridge
this gap, we propose the Physics-Inspired Kolmogorov-Arnold Network (PIKAN)
that embeds physical principles (e.g., free-space path loss, two-ray
reflections) into the learning process. Unlike physics-informed neural networks
(PINNs), PIKAN is more flexible for applying physical information because it
introduces them as flexible inductive biases. Thus, it enables a more flexible
training process. Experiments on UAV A2G measurement data show that PIKAN
achieves comparable accuracy to DL models while providing symbolic and
explainable expressions aligned with propagation laws. Remarkably, PIKAN
achieves this performance with only 232 parameters, making it up to 37 times
lighter than multilayer perceptron (MLP) baselines with thousands of
parameters, without sacrificing correlation with measurements and also
providing symbolic expressions. These results highlight PIKAN as an efficient,
interpretable, and scalable solution for UAV channel modelling in beyond-5G and
6G networks.

</details>


### [138] [Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics](https://arxiv.org/abs/2510.06367)
*Luca Wolf,Tobias Buck,Bjoern Malte Schaefer*

Main category: cs.LG

TL;DR: The paper introduces Helmholtz metrics to quantify how well neural ODEs resemble physical Euler-Lagrange equations, and develops Lagrangian neural ODEs that can learn Euler-Lagrange equations directly with no extra inference cost.


<details>
  <summary>Details</summary>
Motivation: Neural ODEs are powerful for physics applications, but many solutions aren't physical Euler-Lagrange equations, limiting their physical interpretability and accuracy.

Method: Developed Helmholtz metrics to measure ODE resemblance to Euler-Lagrange equations, and created Lagrangian neural ODEs using second-order neural ODEs to directly learn Euler-Lagrange equations.

Result: The approach can distinguish Lagrangian from non-Lagrangian systems using only positional data, and improves neural ODE solution quality.

Conclusion: Lagrangian neural ODEs provide a direct way to learn physically meaningful Euler-Lagrange equations without additional inference cost, enhancing both interpretability and performance.

Abstract: Neural ODEs are a widely used, powerful machine learning technique in
particular for physics. However, not every solution is physical in that it is
an Euler-Lagrange equation. We present Helmholtz metrics to quantify this
resemblance for a given ODE and demonstrate their capabilities on several
fundamental systems with noise. We combine them with a second order neural ODE
to form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations
in a direct fashion and with zero additional inference cost. We demonstrate
that, using only positional data, they can distinguish Lagrangian and
non-Lagrangian systems and improve the neural ODE solutions.

</details>


### [139] [Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data](https://arxiv.org/abs/2510.06377)
*Rishabh Ranjan,Valter Hudovernik,Mark Znidar,Charilaos Kanatsoulis,Roshan Upendra,Mahmoud Mohammadi,Joe Meyer,Tom Palczewski,Carlos Guestrin,Jure Leskovec*

Main category: cs.LG

TL;DR: The Relational Transformer (RT) is a novel architecture that enables zero-shot transfer across diverse relational databases without task-specific fine-tuning, achieving strong performance through relational attention mechanisms and pretraining on heterogeneous schemas.


<details>
  <summary>Details</summary>
Motivation: Relational domains lack architectures that can transfer across datasets and tasks due to the diversity of relational data with varying schemas, graph structures, and functional dependencies.

Method: RT tokenizes cells with table/column metadata, uses masked token prediction pretraining, and employs a novel Relational Attention mechanism over columns, rows, and primary-foreign key links.

Result: Pretrained on RelBench datasets, RT achieves 94% of fully supervised AUROC on binary classification tasks with a 22M parameter model in zero-shot setting, outperforming a 27B LLM (84%). Fine-tuning yields state-of-the-art results with high sample efficiency.

Conclusion: RT provides a practical path toward foundation models for relational data by effectively harnessing task-table context, relational attention patterns, and schema semantics for zero-shot transfer.

Abstract: Pretrained transformers readily adapt to new sequence modeling tasks via
zero-shot prompting, but relational domains still lack architectures that
transfer across datasets and tasks. The core challenge is the diversity of
relational data, with varying heterogeneous schemas, graph structures and
functional dependencies. In this paper, we present the Relational Transformer
(RT) architecture, which can be pretrained on diverse relational databases and
directly applied to unseen datasets and tasks without task- or dataset-specific
fine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with
table/column metadata, (ii) is pretrained via masked token prediction, and
(iii) utilizes a novel \textit{Relational Attention} mechanism over columns,
rows, and primary-foreign key links. Pretrained on RelBench datasets spanning
tasks such as churn and sales forecasting, RT attains strong zero-shot
performance, averaging 94% of fully supervised AUROC on binary classification
tasks with a single forward pass of a 22M parameter model, as opposed to 84%
for a 27B LLM. Fine-tuning yields state-of-the-art results with high sample
efficiency. Our experiments show that RT's zero-shot transfer harnesses
task-table context, relational attention patterns and schema semantics.
Overall, RT provides a practical path toward foundation models for relational
data.

</details>


### [140] [Monte Carlo Permutation Search](https://arxiv.org/abs/2510.06381)
*Tristan Cazenave*

Main category: cs.LG

TL;DR: MCPS is an improved MCTS algorithm that enhances GRAVE by incorporating permutation statistics from all playouts containing moves along the path from root to node, achieving better performance in two-player games.


<details>
  <summary>Details</summary>
Motivation: To develop a better MCTS algorithm for scenarios where deep reinforcement learning isn't feasible or computing power is limited, such as in General Game Playing contexts.

Method: MCPS modifies the exploration term by including statistics from all playouts that contain all moves on the path from root to node, using abstract codes for moves and improved mathematical formulas without GRAVE's bias hyperparameter.

Result: MCPS outperforms GRAVE in all two-player games tested (board games, wargame, investment game, video game) and has equivalent performance in multi-player games due to inherent game balance.

Conclusion: MCPS is a significant improvement over GRAVE, particularly for two-player games, with reduced sensitivity to hyperparameters and better statistical weighting through abstract move coding.

Abstract: We propose Monte Carlo Permutation Search (MCPS), a general-purpose Monte
Carlo Tree Search (MCTS) algorithm that improves upon the GRAVE algorithm. MCPS
is relevant when deep reinforcement learning is not an option, or when the
computing power available before play is not substantial, such as in General
Game Playing, for example. The principle of MCPS is to include in the
exploration term of a node the statistics on all the playouts that contain all
the moves on the path from the root to the node. We extensively test MCPS on a
variety of games: board games, wargame, investment game, video game and
multi-player games. MCPS has better results than GRAVE in all the two-player
games. It has equivalent results for multi-player games because these games are
inherently balanced even when players have different strengths. We also show
that using abstract codes for moves instead of exact codes can be beneficial to
both MCPS and GRAVE, as they improve the permutation statistics and the AMAF
statistics. We also provide a mathematical derivation of the formulas used for
weighting the three sources of statistics. These formulas are an improvement on
the GRAVE formula since they no longer use the bias hyperparameter of GRAVE.
Moreover, MCPS is not sensitive to the ref hyperparameter.

</details>


### [141] [Making and Evaluating Calibrated Forecasts](https://arxiv.org/abs/2510.06388)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Lunjia Hu*

Main category: cs.LG

TL;DR: The paper introduces the first perfectly truthful calibration measure for multi-class prediction tasks, generalizing previous binary-only truthful measures and addressing robustness issues in existing calibration metrics.


<details>
  <summary>Details</summary>
Motivation: Existing calibration measures are non-truthful, meaning they incentivize predictors to lie to appear more calibrated. While recent work introduced truthful measures for binary prediction, there was a need to extend truthful calibration assessment to multi-class settings.

Method: The authors generalize truthful calibration measures from binary to multi-class prediction, studying which extension methods preserve truthfulness. They mathematically prove and empirically verify their measure's robustness properties.

Result: The paper successfully develops a perfectly truthful calibration measure for multi-class prediction that robustly preserves ordering between predictors regardless of hyperparameter choices, solving the non-robustness issue of binned ECE.

Conclusion: This work provides the first truthful calibration measure for multi-class prediction that is both perfectly truthful and robust, enabling reliable calibration assessment without incentivizing dishonest behavior from predictors.

Abstract: Calibrated predictions can be reliably interpreted as probabilities. An
important step towards achieving better calibration is to design an appropriate
calibration measure to meaningfully assess the miscalibration level of a
predictor. A recent line of work initiated by Haghtalab et al. [2024] studies
the design of truthful calibration measures: a truthful measure is minimized
when a predictor outputs the true probabilities, whereas a non-truthful measure
incentivizes the predictor to lie so as to appear more calibrated. All previous
calibration measures were non-truthful until Hartline et al. [2025] introduced
the first perfectly truthful calibration measures for binary prediction tasks
in the batch setting.
  We introduce a perfectly truthful calibration measure for multi-class
prediction tasks, generalizing the work of Hartline et al. [2025] beyond binary
prediction. We study common methods of extending calibration measures from
binary to multi-class prediction and identify ones that do or do not preserve
truthfulness. In addition to truthfulness, we mathematically prove and
empirically verify that our calibration measure exhibits superior robustness:
it robustly preserves the ordering between dominant and dominated predictors,
regardless of the choice of hyperparameters (bin sizes). This result addresses
the non-robustness issue of binned ECE, which has been observed repeatedly in
prior work.

</details>


### [142] [Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings](https://arxiv.org/abs/2510.06397)
*Ali Baheri*

Main category: cs.LG

TL;DR: Non-Euclidean models in curved spaces like hyperbolic geometry have boundary-driven asymmetry that makes them vulnerable to backdoor attacks, where small input changes near boundaries create large representation shifts that evade detection while maintaining attack success.


<details>
  <summary>Details</summary>
Motivation: To understand and formalize the security vulnerabilities in non-Euclidean foundation models, particularly how the curved geometry creates asymmetric effects that backdoor triggers can exploit near boundaries.

Method: Theoretical analysis of boundary-driven asymmetry in curved spaces, formalization of the geometric effects, and development of a geometry-adaptive trigger evaluated across various tasks and architectures.

Result: Attack success increases toward boundaries while conventional detectors weaken, confirming theoretical predictions. Defenses that pull points inward can suppress triggers but sacrifice useful model sensitivity.

Conclusion: Non-Euclidean models have geometry-specific vulnerabilities that require analysis-backed defenses, with boundary regions being particularly susceptible to backdoor attacks that exploit the asymmetric geometric effects.

Abstract: Non-Euclidean foundation models increasingly place representations in curved
spaces such as hyperbolic geometry. We show that this geometry creates a
boundary-driven asymmetry that backdoor triggers can exploit. Near the
boundary, small input changes appear subtle to standard input-space detectors
but produce disproportionately large shifts in the model's representation
space. Our analysis formalizes this effect and also reveals a limitation for
defenses: methods that act by pulling points inward along the radius can
suppress such triggers, but only by sacrificing useful model sensitivity in
that same direction. Building on these insights, we propose a simple
geometry-adaptive trigger and evaluate it across tasks and architectures.
Empirically, attack success increases toward the boundary, whereas conventional
detectors weaken, mirroring the theoretical trends. Together, these results
surface a geometry-specific vulnerability in non-Euclidean models and offer
analysis-backed guidance for designing and understanding the limits of
defenses.

</details>


### [143] [The Effect of Label Noise on the Information Content of Neural Representations](https://arxiv.org/abs/2510.06401)
*Ali Hussaini Umar,Franky Kevin Nando Tezoh,Jean Barbier,Santiago Acevedo,Alessandro Laio*

Main category: cs.LG

TL;DR: This paper studies how label noise affects neural network hidden representations using Information Imbalance analysis, revealing double descent behavior and showing overparameterized networks are robust to label noise.


<details>
  <summary>Details</summary>
Motivation: While label noise impact on model performance is well-studied, its effects on hidden representations remain poorly understood, creating a research gap.

Method: Systematically compare hidden representations using Information Imbalance, a computationally efficient proxy of conditional mutual information, across different parameterization regimes.

Result: Hidden representations show double descent behavior similar to test error. Underparameterized networks with noisy labels produce more informative representations than clean labels, while overparameterized networks show equal informativeness and robustness to label noise.

Conclusion: Overparameterized networks are robust to label noise, and the information imbalance between layers decreases with cross-entropy loss, offering new insights into generalization. Training on random labels drives networks beyond lazy learning as weights adapt to encode label information.

Abstract: In supervised classification tasks, models are trained to predict a label for
each data point. In real-world datasets, these labels are often noisy due to
annotation errors. While the impact of label noise on the performance of deep
learning models has been widely studied, its effects on the networks' hidden
representations remain poorly understood. We address this gap by systematically
comparing hidden representations using the Information Imbalance, a
computationally efficient proxy of conditional mutual information. Through this
analysis, we observe that the information content of the hidden representations
follows a double descent as a function of the number of network parameters,
akin to the behavior of the test error. We further demonstrate that in the
underparameterized regime, representations learned with noisy labels are more
informative than those learned with clean labels, while in the
overparameterized regime, these representations are equally informative. Our
results indicate that the representations of overparameterized networks are
robust to label noise. We also found that the information imbalance between the
penultimate and pre-softmax layers decreases with cross-entropy loss in the
overparameterized regime. This offers a new perspective on understanding
generalization in classification tasks. Extending our analysis to
representations learned from random labels, we show that these perform worse
than random features. This indicates that training on random labels drives
networks much beyond lazy learning, as weights adapt to encode labels
information.

</details>


### [144] [Test-Time Efficient Pretrained Model Portfolios for Time Series Forecasting](https://arxiv.org/abs/2510.06419)
*Mert Kayaalp,Caner Turkmen,Oleksandr Shchur,Pedro Mercado,Abdul Fatir Ansari,Michael Bohlke-Schneider,Bernie Wang*

Main category: cs.LG

TL;DR: Building portfolios of smaller pretrained forecasting models instead of single large models achieves competitive performance with fewer parameters through ensembling and model selection.


<details>
  <summary>Details</summary>
Motivation: To explore whether bigger models are always better for time series foundation models, and investigate alternatives to training large monolithic models.

Method: Create portfolios of smaller pretrained forecasting models, apply ensembling or model selection strategies, and design portfolios using specialist models created through post-training base models.

Result: Collections of specialist models consistently outperform portfolios of independently trained generalists, achieving competitive performance on large-scale benchmarks with fewer parameters.

Conclusion: Ensembling and model selection over portfolios of smaller specialist models are more compute-efficient than test-time fine-tuning and provide a viable alternative to large monolithic time series foundation models.

Abstract: Is bigger always better for time series foundation models? With the question
in mind, we explore an alternative to training a single, large monolithic
model: building a portfolio of smaller, pretrained forecasting models. By
applying ensembling or model selection over these portfolios, we achieve
competitive performance on large-scale benchmarks using much fewer parameters.
We explore strategies for designing such portfolios and find that collections
of specialist models consistently outperform portfolios of independently
trained generalists. Remarkably, we demonstrate that post-training a base model
is a compute-effective approach for creating sufficiently diverse specialists,
and provide evidences that ensembling and model selection are more
compute-efficient than test-time fine-tuning.

</details>


### [145] [Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization](https://arxiv.org/abs/2510.06434)
*Eliot Shekhtman,Yichen Zhou,Ingvar Ziemann,Nikolai Matni,Stephen Tu*

Main category: cs.LG

TL;DR: This paper presents a new framework for instance-optimal learning from multi-trajectory data using Hellinger localization, achieving near-optimal rates across diverse models without requiring mixing assumptions.


<details>
  <summary>Details</summary>
Motivation: Current understanding of sequential learning is incomplete, especially for multi-trajectory data where existing methods either reduce to i.i.d. learning with limited sample size scaling or require mixing assumptions. There's a need for broader instance-optimal guarantees.

Method: The Hellinger localization framework controls squared Hellinger distance at path-measure level via reduction to i.i.d. learning, followed by localization as a quadratic form in parameter space weighted by trajectory Fisher information.

Result: The framework yields instance-optimal bounds scaling with full data budget under broad conditions, demonstrated across four case studies: mixture of Markov chains, dependent linear regression, generalized linear models, and linear-attention sequence models.

Conclusion: The proposed method significantly broadens instance-optimal rates in multi-trajectory settings, nearly matching asymptotic normality rates and substantially improving over standard reductions across diverse models.

Abstract: Learning from temporally-correlated data is a core facet of modern machine
learning. Yet our understanding of sequential learning remains incomplete,
particularly in the multi-trajectory setting where data consists of many
independent realizations of a time-indexed stochastic process. This important
regime both reflects modern training pipelines such as for large foundation
models, and offers the potential for learning without the typical mixing
assumptions made in the single-trajectory case. However, instance-optimal
bounds are known only for least-squares regression with dependent covariates;
for more general models or loss functions, the only broadly applicable
guarantees result from a reduction to either i.i.d. learning, with effective
sample size scaling only in the number of trajectories, or an existing
single-trajectory result when each individual trajectory mixes, with effective
sample size scaling as the full data budget deflated by the mixing-time.
  In this work, we significantly broaden the scope of instance-optimal rates in
multi-trajectory settings via the Hellinger localization framework, a general
approach for maximum likelihood estimation. Our method proceeds by first
controlling the squared Hellinger distance at the path-measure level via a
reduction to i.i.d. learning, followed by localization as a quadratic form in
parameter space weighted by the trajectory Fisher information. This yields
instance-optimal bounds that scale with the full data budget under a broad set
of conditions. We instantiate our framework across four diverse case studies: a
simple mixture of Markov chains, dependent linear regression under non-Gaussian
noise, generalized linear models with non-monotonic activations, and
linear-attention sequence models. In all cases, our bounds nearly match the
instance-optimal rates from asymptotic normality, substantially improving over
standard reductions.

</details>


### [146] [Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models](https://arxiv.org/abs/2510.06439)
*Akash Yadav,Ruda Zhang*

Main category: cs.LG

TL;DR: A novel Bayesian optimization framework for hyperparameter tuning under uncertainty that reduces computational cost by 40x compared to conventional Monte Carlo methods.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter tuning is challenging under uncertainty due to noisy function evaluations, making optimization computationally expensive.

Method: Uses Bayesian optimization with statistical surrogate for random variables, enabling analytical evaluation of expectations and closed-form optimizer for acquisition functions.

Result: Requires 40 times fewer data points than conventional Monte Carlo methods, achieving up to 40-fold reduction in computational cost.

Conclusion: The proposed method is effective for hyperparameter tuning under uncertainty, as demonstrated through computational engineering examples.

Abstract: Hyperparameter tuning is a challenging problem especially when the system
itself involves uncertainty. Due to noisy function evaluations, optimization
under uncertainty can be computationally expensive. In this paper, we present a
novel Bayesian optimization framework tailored for hyperparameter tuning under
uncertainty, with a focus on optimizing a scale- or precision-type parameter in
stochastic models. The proposed method employs a statistical surrogate for the
underlying random variable, enabling analytical evaluation of the expectation
operator. Moreover, we derive a closed-form expression for the optimizer of the
random acquisition function, which significantly reduces computational cost per
iteration. Compared with a conventional one-dimensional Monte Carlo-based
optimization scheme, the proposed approach requires 40 times fewer data points,
resulting in up to a 40-fold reduction in computational cost. We demonstrate
the effectiveness of the proposed method through two numerical examples in
computational engineering.

</details>


### [147] [Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks](https://arxiv.org/abs/2510.06444)
*Joel Pfeffer,J. M. Diederik Kruijssen,Clément Gossart,Mélanie Chevance,Diego Campo Millan,Florian Stecker,Steven N. Longmore*

Main category: cs.LG

TL;DR: The paper proposes a performance forecasting model for decentralized learning networks that predicts model performance to enable proactive weight assignment, improving network inference accuracy compared to reactive linear pooling methods.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic prediction combination methods in decentralized learning are reactive and slow to adapt to changing circumstances due to their reliance on historical performance averaging. There's a need for more responsive, context-aware approaches.

Method: Developed a machine learning model that forecasts performance of individual models at each epoch, enabling proactive weight assignment. Used forecasting of regret or regret z-score to identify models likely to perform better at specific times.

Result: Performance forecasting models predicting regret or regret z-score showed greater improvement than loss-based models, which often didn't outperform naive network inference. The approach improved accuracy of network inferences in decentralized learning networks.

Conclusion: Performance forecasting enables predictive rather than reactive model weighting, providing context-awareness in decentralized learning. The method's effectiveness depends on feature set choices and training epochs, requiring domain-specific tailoring. This approach has broader applications beyond decentralized learning networks.

Abstract: In decentralized learning networks, predictions from many participants are
combined to generate a network inference. While many studies have demonstrated
performance benefits of combining multiple model predictions, existing
strategies using linear pooling methods (ranging from simple averaging to
dynamic weight updates) face a key limitation. Dynamic prediction combinations
that rely on historical performance to update weights are necessarily reactive.
Due to the need to average over a reasonable number of epochs (with moving
averages or exponential weighting), they tend to be slow to adjust to changing
circumstances (phase or regime changes). In this work, we develop a model that
uses machine learning to forecast the performance of predictions by models at
each epoch in a time series. This enables `context-awareness' by assigning
higher weight to models that are likely to be more accurate at a given time. We
show that adding a performance forecasting worker in a decentralized learning
network, following a design similar to the Allora network, can improve the
accuracy of network inferences. Specifically, we find forecasting models that
predict regret (performance relative to the network inference) or regret
z-score (performance relative to other workers) show greater improvement than
models predicting losses, which often do not outperform the naive network
inference (historically weighted average of all inferences). Through a series
of optimization tests, we show that the performance of the forecasting model
can be sensitive to choices in the feature set and number of training epochs.
These properties may depend on the exact problem and should be tailored to each
domain. Although initially designed for a decentralized learning network, using
performance forecasting for prediction combination may be useful in any
situation where predictive rather than reactive model weighting is needed.

</details>


### [148] [How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation](https://arxiv.org/abs/2510.06448)
*Prabhant Singh,Sibylle Hess,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: Current benchmarks for transferability estimation metrics are flawed, artificially inflating performance of existing methods where simple heuristics can outperform sophisticated approaches due to unrealistic model spaces and static hierarchies.


<details>
  <summary>Details</summary>
Motivation: To identify shortcomings in widely used benchmark setups for evaluating transferability estimation metrics, which are used to select pre-trained models without fine-tuning or source data access.

Method: Empirical analysis demonstrating that current benchmarks have unrealistic model spaces and static performance hierarchies that artificially boost perceived metric performance.

Result: Simple dataset-agnostic heuristics can outperform sophisticated transferability estimation methods under current flawed evaluation protocols, revealing a disconnect from real-world model selection complexities.

Conclusion: Provides concrete recommendations for constructing more robust and realistic benchmarks to guide future research in transferability estimation metrics.

Abstract: Transferability estimation metrics are used to find a high-performing
pre-trained model for a given target task without fine-tuning models and
without access to the source dataset. Despite the growing interest in
developing such metrics, the benchmarks used to measure their progress have
gone largely unexamined. In this work, we empirically show the shortcomings of
widely used benchmark setups to evaluate transferability estimation metrics. We
argue that the benchmarks on which these metrics are evaluated are
fundamentally flawed. We empirically demonstrate that their unrealistic model
spaces and static performance hierarchies artificially inflate the perceived
performance of existing metrics, to the point where simple, dataset-agnostic
heuristics can outperform sophisticated methods. Our analysis reveals a
critical disconnect between current evaluation protocols and the complexities
of real-world model selection. To address this, we provide concrete
recommendations for constructing more robust and realistic benchmarks to guide
future research in a more meaningful direction.

</details>


### [149] [Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin](https://arxiv.org/abs/2510.06477)
*Enrique Queipo-de-Llano,Álvaro Arroyo,Federico Barbero,Xiaowen Dong,Michael Bronstein,Yann LeCun,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: Attention sinks and compression valleys are connected through massive activations in residual streams, leading to a unified Mix-Compress-Refine theory of information flow in Transformers.


<details>
  <summary>Details</summary>
Motivation: To understand the connection between attention sinks and compression valleys, which have been studied separately, and explain how LLMs organize computation through massive activations.

Method: Theoretical analysis proving massive activations cause representational compression, experimental validation across models (410M-120B parameters), and targeted ablation studies.

Result: When beginning-of-sequence tokens develop extreme activation norms in middle layers, both compression valleys and attention sinks emerge simultaneously, confirming theoretical predictions.

Conclusion: Proposed Mix-Compress-Refine theory explains Transformer computation in three phases: early mixing, middle compression, and late refinement, clarifying task-dependent representation differences.

Abstract: Attention sinks and compression valleys have attracted significant attention
as two puzzling phenomena in large language models, but have been studied in
isolation. In this work, we present a surprising connection between attention
sinks and compression valleys, tracing both to the formation of massive
activations in the residual stream. We prove theoretically that massive
activations necessarily produce representational compression and establish
bounds on the resulting entropy reduction. Through experiments across several
models (410M-120B parameters), we confirm that when the beginning-of-sequence
token develops extreme activation norms in the middle layers, both compression
valleys and attention sinks emerge simultaneously. Targeted ablation studies
validate our theoretical predictions. This unified view motivates us to propose
the Mix-Compress-Refine theory of information flow, as an attempt to explain
how LLMs organize their computation in depth by controlling attention and
representational compression via massive activations. Specifically, we posit
that Transformer-based LLMs process tokens in three distinct phases: (1) broad
mixing in the early layers, (2) compressed computation with limited mixing in
the middle layers, and (3) selective refinement in the late layers. Our
framework helps explain why embedding tasks perform best at intermediate
layers, whereas generation tasks benefit from full-depth processing, clarifying
differences in task-dependent representations.

</details>


### [150] [Valid Stopping for LLM Generation via Empirical Dynamic Formal Lift](https://arxiv.org/abs/2510.06478)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.LG

TL;DR: Sequential-EDFL applies anytime-valid sequential testing to language model generation stopping, using information lift tracking with formal error control, reducing generation by 22-28% while maintaining delta-level control.


<details>
  <summary>Details</summary>
Motivation: To develop a method for early stopping of language model generation that provides formal error guarantees regardless of when stopping occurs, reducing computational costs while maintaining reliability.

Method: Uses self-normalized empirical-Bernstein e-processes to track information lift (log-likelihood ratio between full models and weakened "skeleton" baselines), with online mean estimation for unknown centering, mixture e-processes for multiple parameters, and adaptive resets for distributional drift.

Result: Reduces generation by 22-28% vs. sequential baselines while maintaining delta-level control with 12% computational overhead. When combined with correctness gate, improves end-task correctness while preserving anytime-valid guarantees.

Conclusion: EDFL serves as an effective first-stage filter reducing verification burden by 83%, but is not a standalone solution for safety-critical domains as it controls information sufficiency rather than factual correctness.

Abstract: We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), applying
anytime-valid sequential testing to language model generation stopping. Our
approach tracks information lift -- the log-likelihood ratio between full
models and deliberately weakened "skeleton" baselines -- using self-normalized
empirical-Bernstein e-processes that provide formal delta-level error control
regardless of stopping time. We handle unknown centering through online mean
estimation, combine multiple parameters via mixture e-processes, and support
adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL
reduces generation by 22-28% vs. sequential baselines while maintaining
delta-level control with 12% computational overhead. We introduce automated
skeletons (distilled submodels, randomized logits) and show robustness across
skeleton families. Composing EDFL with a lightweight correctness gate (sentence
boundaries + verifier) improves end-task correctness while preserving
anytime-valid guarantees by only delaying stopping. Our certificates control
information sufficiency, not factual correctness -- 10.9% of stopped sequences
remain incorrect even with the gate (13.2-22.7% without it). EDFL serves as a
first-stage filter reducing verification burden by 83%, not as a standalone
solution for safety-critical domains.

</details>


### [151] [GUIDE: Guided Initialization and Distillation of Embeddings](https://arxiv.org/abs/2510.06502)
*Khoa Trinh,Gaurav Menghani,Erik Vee*

Main category: cs.LG

TL;DR: GUIDE is a distillation technique that forces student models to match teacher models in parameter space, achieving 25-26% reduction in quality gap with no training or inference overhead.


<details>
  <summary>Details</summary>
Motivation: Standard distillation only makes students match teacher outputs, but given the high cost of training large teacher models, more useful information should be extracted from teachers.

Method: GUIDE (Guided Initialization and Distillation of Embeddings) forces student models to match teacher models in the parameter space rather than just output space.

Result: 25-26% reduction in teacher-student quality gap for large student models (400M-1B parameters) trained on ~20B tokens; GUIDE alone outperforms knowledge distillation alone.

Conclusion: GUIDE provides substantial quality improvements over standard distillation with no training or inference overhead, making the gains virtually free.

Abstract: Algorithmic efficiency techniques such as distillation
(\cite{hinton2015distillation}) are useful in improving model quality without
increasing serving costs, provided a larger teacher model is available for a
smaller student model to learn from during training. Standard distillation
methods are limited to only forcing the student to match the teacher's outputs.
Given the costs associated with training a large model, we believe we should be
extracting more useful information from a teacher model than by just making the
student match the teacher's outputs.
  In this paper, we introduce \guide (Guided Initialization and Distillation of
Embeddings). \guide can be considered a distillation technique that forces the
student to match the teacher in the parameter space. Using \guide we show
25-26\% reduction in the teacher-student quality gap when using large student
models (400M - 1B parameters) trained on $\approx$ 20B tokens. We also present
a thorough analysis demonstrating that \guide can be combined with knowledge
distillation with near additive improvements. Furthermore, we show that
applying \guide alone leads to substantially better model quality than applying
knowledge distillation by itself.
  Most importantly, \guide introduces no training or inference overhead and
hence any model quality gains from our method are virtually free.

</details>


### [152] [ATLO-ML: Adaptive Time-Length Optimizer for Machine Learning -- Insights from Air Quality Forecasting](https://arxiv.org/abs/2510.06503)
*I-Hsi Kao,Kanji Uchino*

Main category: cs.LG

TL;DR: ATLO-ML is an adaptive system that automatically optimizes input time length and sampling rate for time-series predictions based on user-defined output time length, improving ML model accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate time-series predictions in machine learning are heavily influenced by the selection of appropriate input time length and sampling rate, which are typically fixed and suboptimal.

Method: ATLO-ML adaptively determines optimal input time length and sampling rate based on user-defined output time length, providing flexible time-series data pre-processing with dynamic parameter adjustment.

Result: Validation using air quality datasets shows that optimized time length and sampling rate significantly improve ML model accuracy compared to fixed time lengths.

Conclusion: ATLO-ML demonstrates potential for generalization across various time-sensitive applications and offers a robust solution for optimizing temporal input parameters in ML workflows.

Abstract: Accurate time-series predictions in machine learning are heavily influenced
by the selection of appropriate input time length and sampling rate. This paper
introduces ATLO-ML, an adaptive time-length optimization system that
automatically determines the optimal input time length and sampling rate based
on user-defined output time length. The system provides a flexible approach to
time-series data pre-processing, dynamically adjusting these parameters to
enhance predictive performance. ATLO-ML is validated using air quality
datasets, including both GAMS-dataset and proprietary data collected from a
data center, both in time series format. Results demonstrate that utilizing the
optimized time length and sampling rate significantly improves the accuracy of
machine learning models compared to fixed time lengths. ATLO-ML shows potential
for generalization across various time-sensitive applications, offering a
robust solution for optimizing temporal input parameters in machine learning
workflows.

</details>


### [153] [A Median Perspective on Unlabeled Data for Out-of-Distribution Detection](https://arxiv.org/abs/2510.06505)
*Momin Abbas,Ali Falahati,Hossein Goli,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: Medix is a novel OOD detection framework that identifies outliers from unlabeled data using median operations, then trains a robust OOD classifier using these identified outliers and labeled InD data.


<details>
  <summary>Details</summary>
Motivation: Effectively utilizing unlabeled in-the-wild data for OOD detection is challenging due to mixed InD and OOD samples, and the lack of distinct OOD samples makes training optimal classifiers difficult.

Method: Uses median operation to identify potential outliers from unlabeled data due to its robustness against noise and outliers, then trains OOD classifier with identified outliers and labeled InD data.

Result: Empirical results show Medix outperforms existing methods across the board in open-world settings, and theoretical analysis demonstrates low error rate bounds.

Conclusion: Medix provides an effective framework for OOD detection using unlabeled data, with both theoretical guarantees and empirical superiority over existing approaches.

Abstract: Out-of-distribution (OOD) detection plays a crucial role in ensuring the
robustness and reliability of machine learning systems deployed in real-world
applications. Recent approaches have explored the use of unlabeled data,
showing potential for enhancing OOD detection capabilities. However,
effectively utilizing unlabeled in-the-wild data remains challenging due to the
mixed nature of both in-distribution (InD) and OOD samples. The lack of a
distinct set of OOD samples complicates the task of training an optimal OOD
classifier. In this work, we introduce Medix, a novel framework designed to
identify potential outliers from unlabeled data using the median operation. We
use the median because it provides a stable estimate of the central tendency,
as an OOD detection mechanism, due to its robustness against noise and
outliers. Using these identified outliers, along with labeled InD data, we
train a robust OOD classifier. From a theoretical perspective, we derive error
bounds that demonstrate Medix achieves a low error rate. Empirical results
further substantiate our claims, as Medix outperforms existing methods across
the board in open-world settings, confirming the validity of our theoretical
insights.

</details>


### [154] [Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security](https://arxiv.org/abs/2510.06525)
*Ali Naseh,Anshuman Suri,Yuefeng Peng,Harsh Chaudhari,Alina Oprea,Amir Houmansadr*

Main category: cs.LG

TL;DR: Text-to-image leaderboards are highly vulnerable to model deanonymization attacks using simple CLIP-based classification, enabling easy rank manipulation.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that text-to-image leaderboards are more vulnerable to deanonymization attacks than LLM leaderboards, making rank manipulation easier.

Method: Used 150,000+ images from 280 prompts and 19 diverse models, performing real-time classification in CLIP embedding space without prompt control or historical data.

Result: Simple CLIP-based classification achieves high accuracy in identifying generating models, with some prompts enabling near-perfect deanonymization.

Conclusion: Rank manipulation in text-to-image leaderboards is easier than previously recognized, highlighting the need for stronger defensive measures.

Abstract: Generative AI leaderboards are central to evaluating model capabilities, but
remain vulnerable to manipulation. Among key adversarial objectives is rank
manipulation, where an attacker must first deanonymize the models behind
displayed outputs -- a threat previously demonstrated and explored for large
language models (LLMs). We show that this problem can be even more severe for
text-to-image leaderboards, where deanonymization is markedly easier. Using
over 150,000 generated images from 280 prompts and 19 diverse models spanning
multiple organizations, architectures, and sizes, we demonstrate that simple
real-time classification in CLIP embedding space identifies the generating
model with high accuracy, even without prompt control or historical data. We
further introduce a prompt-level separability metric and identify prompts that
enable near-perfect deanonymization. Our results indicate that rank
manipulation in text-to-image leaderboards is easier than previously
recognized, underscoring the need for stronger defenses.

</details>


### [155] [Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture](https://arxiv.org/abs/2510.06527)
*John Dunbar,Scott Aaronson*

Main category: cs.LG

TL;DR: Randomly initialized wide neural networks with zero-mean activation functions (like shifted ReLU/GeLU, tanh) produce nearly independent outputs, making them suitable for testing interpretability limits.


<details>
  <summary>Details</summary>
Motivation: To understand when neural networks produce independent outputs and test the Alignment Research Center's computational no-coincidence conjecture about AI interpretability limits.

Method: Analyze randomly initialized neural networks with large width and zero-mean activation functions under Gaussian measure.

Result: Networks with activation functions satisfying E[σ(z)]=0 (e.g., shifted ReLU/GeLU, tanh) produce nearly independent outputs, while standard ReLU/GeLU do not.

Conclusion: Zero-mean activation functions enable nearly independent network outputs, making them promising for testing interpretability conjectures.

Abstract: We establish that randomly initialized neural networks, with large width and
a natural choice of hyperparameters, have nearly independent outputs exactly
when their activation function is nonlinear with zero mean under the Gaussian
measure: $\mathbb{E}_{z \sim \mathcal{N}(0,1)}[\sigma(z)]=0$. For example, this
includes ReLU and GeLU with an additive shift, as well as tanh, but not ReLU or
GeLU by themselves. Because of their nearly independent outputs, we propose
neural networks with zero-mean activation functions as a promising candidate
for the Alignment Research Center's computational no-coincidence conjecture --
a conjecture that aims to measure the limits of AI interpretability.

</details>


### [156] [Scalable Policy-Based RL Algorithms for POMDPs](https://arxiv.org/abs/2510.06540)
*Ameya Anjarlekar,Rasoul Etesami,R Srikant*

Main category: cs.LG

TL;DR: This paper proposes transforming POMDPs into finite-state Superstate MDPs using finite histories, enabling approximate solution via TD-learning and policy optimization with provable error bounds.


<details>
  <summary>Details</summary>
Motivation: To address computational challenges in solving POMDPs due to continuous belief states by approximating them as finite-state MDPs.

Method: Transform POMDP into Superstate MDP using finite histories, apply TD-learning with linear function approximation, followed by policy optimization.

Result: Derived improved theoretical guarantees showing approximation error decreases exponentially with history length, providing first finite-time bounds for TD-learning in non-Markovian settings.

Conclusion: POMDPs can be effectively approximated and solved using standard MDP techniques with finite histories, with provable performance guarantees.

Abstract: The continuous nature of belief states in POMDPs presents significant
computational challenges in learning the optimal policy. In this paper, we
consider an approach that solves a Partially Observable Reinforcement Learning
(PORL) problem by approximating the corresponding POMDP model into a
finite-state Markov Decision Process (MDP) (called Superstate MDP). We first
derive theoretical guarantees that improve upon prior work that relate the
optimal value function of the transformed Superstate MDP to the optimal value
function of the original POMDP. Next, we propose a policy-based learning
approach with linear function approximation to learn the optimal policy for the
Superstate MDP. Consequently, our approach shows that a POMDP can be
approximately solved using TD-learning followed by Policy Optimization by
treating it as an MDP, where the MDP state corresponds to a finite history. We
show that the approximation error decreases exponentially with the length of
this history. To the best of our knowledge, our finite-time bounds are the
first to explicitly quantify the error introduced when applying standard TD
learning to a setting where the true dynamics are not Markovian.

</details>


### [157] [Is the Hard-Label Cryptanalytic Model Extraction Really Polynomial?](https://arxiv.org/abs/2510.06692)
*Akira Ito,Takayuki Miura,Yosuke Todo*

Main category: cs.LG

TL;DR: This paper identifies limitations in existing hard-label model extraction attacks on ReLU-based DNNs, showing they require exponential queries for deep networks. The authors propose CrossLayer Extraction, a novel attack that exploits neuron interactions across layers to reduce query complexity.


<details>
  <summary>Details</summary>
Motivation: Existing model extraction attacks assume unrealistic conditions for deep networks, requiring exponential queries that make them impractical. There's a need for more efficient attacks that work under realistic hard-label settings.

Method: Proposed CrossLayer Extraction attack that exploits neuron interactions across layers instead of directly extracting individual neuron parameters. This approach avoids the exponential cost of traditional methods.

Result: The new attack significantly reduces query complexity compared to existing approaches, making model extraction more practical for deep neural networks under hard-label settings.

Conclusion: CrossLayer Extraction provides a more efficient and practical approach to model extraction from deep ReLU-based DNNs, overcoming the exponential query limitations of previous methods.

Abstract: Deep Neural Networks (DNNs) have attracted significant attention, and their
internal models are now considered valuable intellectual assets. Extracting
these internal models through access to a DNN is conceptually similar to
extracting a secret key via oracle access to a block cipher. Consequently,
cryptanalytic techniques, particularly differential-like attacks, have been
actively explored recently. ReLU-based DNNs are the most commonly and widely
deployed architectures. While early works (e.g., Crypto 2020, Eurocrypt 2024)
assume access to exact output logits, which are usually invisible, more recent
works (e.g., Asiacrypt 2024, Eurocrypt 2025) focus on the hard-label setting,
where only the final classification result (e.g., "dog" or "car") is available
to the attacker. Notably, Carlini et al. (Eurocrypt 2025) demonstrated that
model extraction is feasible in polynomial time even under this restricted
setting.
  In this paper, we first show that the assumptions underlying their attack
become increasingly unrealistic as the attack-target depth grows. In practice,
satisfying these assumptions requires an exponential number of queries with
respect to the attack depth, implying that the attack does not always run in
polynomial time. To address this critical limitation, we propose a novel attack
method called CrossLayer Extraction. Instead of directly extracting the secret
parameters (e.g., weights and biases) of a specific neuron, which incurs
exponential cost, we exploit neuron interactions across layers to extract this
information from deeper layers. This technique significantly reduces query
complexity and mitigates the limitations of existing model extraction
approaches.

</details>


### [158] [Incoherence in goal-conditioned autoregressive models](https://arxiv.org/abs/2510.06545)
*Jacek Karwowski,Raymond Douglas*

Main category: cs.LG

TL;DR: This paper mathematically analyzes incoherence in reinforcement learning policies from naive goal-conditioning of autoregressive models, showing that online RL fine-tuning reduces incoherence and improves performance, with connections to control-as-inference and soft Q learning.


<details>
  <summary>Details</summary>
Motivation: To understand the structural issue of incoherence in RL policies derived from naive goal-conditioning of autoregressive models, and to characterize how online RL fine-tuning addresses this problem.

Method: Mathematical investigation of incoherence, re-framing control-as-inference and soft Q learning concepts, establishing three-way correspondence between different interpretations of iterative re-training, and analyzing soft-conditioning generative models.

Result: Proved that online RL fine-tuning decreases incoherence and improves return, established correspondence between iterative re-training as folding posterior into reward and decreasing temperature parameter, and linked incoherence to effective horizon.

Conclusion: Online RL fine-tuning effectively addresses incoherence in goal-conditioned policies, with mathematical connections to established RL frameworks and implications for training-inference trade-offs and effective horizon considerations.

Abstract: We investigate mathematically the notion of incoherence: a structural issue
with reinforcement learning policies derived by naive goal-conditioning of
autoregressive models. We focus on the process of re-training models on their
own actions, that is, fine-tuning offline-learned policies with online RL. We
prove that it decreases incoherence and leads to an improvement in return, and
we aim to characterize the resulting trajectory of policies. By re-framing
standard notions of control-as-inference and soft Q learning, we establish a
three-way correspondence with two other ways of understanding the iterative
re-training process: as folding the posterior into the reward and, in the
deterministic case, as decreasing the temperature parameter; the correspondence
has computational content via the training-inference trade-off. Through
soft-conditioning generative models, we discuss the link between incoherence
and the effective horizon.

</details>


### [159] [The Markovian Thinker](https://arxiv.org/abs/2510.06557)
*Milad Aghajohari,Kamran Chitsaz,Amirhossein Kazemnejad,Sarath Chandar,Alessandro Sordoni,Aaron Courville,Siva Reddy*

Main category: cs.LG

TL;DR: Delethink enables efficient long-chain reasoning in LLMs by using Markovian Thinking with fixed-size chunks, achieving linear compute scaling instead of quadratic, while matching or surpassing traditional LongCoT-RL performance.


<details>
  <summary>Details</summary>
Motivation: Standard RL for reasoning LLMs suffers from quadratic compute overhead as thought chains lengthen due to unbounded state size, making long reasoning sequences computationally expensive.

Method: Proposes Markovian Thinking paradigm with Delethink RL environment that structures reasoning into fixed-size chunks, resetting context at boundaries with short carryover text, allowing policies to learn seamless continuation.

Result: 1.5B model achieves 24K token reasoning with 8K chunks, matching 24K LongCoT-RL performance. At 96K thinking length, Delethink costs 7 H100-months vs 27 for LongCoT-RL, with continued scaling where LongCoT plateaus.

Conclusion: Redesigning the thinking environment enables efficient long reasoning without quadratic overhead, providing a path toward scalable reasoning LLMs, with existing models showing Markovian capabilities zero-shot.

Abstract: Reinforcement learning (RL) has recently become a strong recipe for training
reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard
RL "thinking environment", where the state is the prompt plus all prior
reasoning tokens, makes the state unbounded and forces attention-based policies
to pay quadratic compute as thoughts lengthen. We revisit the environment
itself. We propose Markovian Thinking, a paradigm in which the policy advances
reasoning while conditioning on a constant-size state, decoupling thinking
length from context size. As an immediate consequence this yields linear
compute with constant memory. We instantiate this idea with Delethink, an RL
environment that structures reasoning into fixed-size chunks. Within each
chunk, the model thinks as usual; at the boundary, the environment resets the
context and reinitializes the prompt with a short carryover. Through RL, the
policy learns to write a textual state near the end of each chunk sufficient
for seamless continuation of reasoning after reset. Trained in this
environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up
to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.
With test-time scaling, Delethink continues to improve where LongCoT plateaus.
The effect of linear compute is substantial: we empirically estimate at 96K
average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.
Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)
often sample Markovian traces zero-shot across diverse benchmarks, providing
positive samples that make RL effective at scale. Our results show that
redesigning the thinking environment is a powerful lever: it enables very long
reasoning without quadratic overhead and opens a path toward efficient,
scalable reasoning LLMs.

</details>


### [160] [The Framework That Survives Bad Models: Human-AI Collaboration For Clinical Trials](https://arxiv.org/abs/2510.06567)
*Yao Chen,David Ohlssen,Aimee Readie,Gregory Ligozio,Ruvie Martin,Thibaud Coroller*

Main category: cs.LG

TL;DR: AI-SR (AI as supporting reader) is the most suitable AI framework for clinical trials as it maintains reliable disease estimation and preserves treatment effect estimates even with bad models, while meeting cost, accuracy, robustness, and generalization criteria.


<details>
  <summary>Details</summary>
Motivation: To address the risks of deploying AI in clinical trials without safeguards, particularly when AI evaluates patient endpoints that directly impact trial conclusions.

Method: Compared two AI frameworks against human-only assessment for medical image-based disease evaluation, measuring cost, accuracy, robustness, and generalization. Stress-tested by injecting bad models (random guesses to naive predictions) to ensure treatment effects remain valid under model degradation.

Result: AI-SR framework consistently provided reliable disease estimation, preserved clinical trial treatment effect estimates and conclusions, and retained advantages when applied to different populations, even with bad models.

Conclusion: Using AI as a supporting reader is the most suitable approach for clinical trials as it meets all criteria across various model types and maintains reliability under severe model degradation.

Abstract: Artificial intelligence (AI) holds great promise for supporting clinical
trials, from patient recruitment and endpoint assessment to treatment response
prediction. However, deploying AI without safeguards poses significant risks,
particularly when evaluating patient endpoints that directly impact trial
conclusions. We compared two AI frameworks against human-only assessment for
medical image-based disease evaluation, measuring cost, accuracy, robustness,
and generalization ability. To stress-test these frameworks, we injected bad
models, ranging from random guesses to naive predictions, to ensure that
observed treatment effects remain valid even under severe model degradation. We
evaluated the frameworks using two randomized controlled trials with endpoints
derived from spinal X-ray images. Our findings indicate that using AI as a
supporting reader (AI-SR) is the most suitable approach for clinical trials, as
it meets all criteria across various model types, even with bad models. This
method consistently provides reliable disease estimation, preserves clinical
trial treatment effect estimates and conclusions, and retains these advantages
when applied to different populations.

</details>


### [161] [DPA-Net: A Dual-Path Attention Neural Network for Inferring Glycemic Control Metrics from Self-Monitored Blood Glucose Data](https://arxiv.org/abs/2510.06623)
*Canyu Lei,Benjamin Lobo,Jianxin Xie*

Main category: cs.LG

TL;DR: DPA-Net estimates AGP metrics from sparse SMBG data using dual-path attention network with spatial-channel reconstruction and multi-scale ResNet, achieving robust accuracy without requiring expensive CGM.


<details>
  <summary>Details</summary>
Motivation: CGM provides accurate glucose monitoring but is expensive and inaccessible in low-income regions, while SMBG is widely available but produces sparse data that can't generate clinically meaningful AGP metrics.

Method: Dual-Path Attention Neural Network (DPA-Net) with spatial-channel attention path for CGM-like trajectory reconstruction and multi-scale ResNet path for direct AGP prediction, plus alignment mechanism and active point selector.

Result: DPA-Net achieves robust accuracy with low errors and reduced systematic bias on large real-world dataset, successfully estimating AGP metrics from SMBG data.

Conclusion: First supervised ML framework for estimating AGP metrics from SMBG data, providing practical decision-support tool where CGM is inaccessible.

Abstract: Continuous glucose monitoring (CGM) provides dense and dynamic glucose
profiles that enable reliable estimation of Ambulatory Glucose Profile (AGP)
metrics, such as Time in Range (TIR), Time Below Range (TBR), and Time Above
Range (TAR). However, the high cost and limited accessibility of CGM restrict
its widespread adoption, particularly in low- and middle-income regions. In
contrast, self-monitoring of blood glucose (SMBG) is inexpensive and widely
available but yields sparse and irregular data that are challenging to
translate into clinically meaningful glycemic metrics.
  In this work, we propose a Dual-Path Attention Neural Network (DPA-Net) to
estimate AGP metrics directly from SMBG data. DPA-Net integrates two
complementary paths: (1) a spatial-channel attention path that reconstructs a
CGM-like trajectory from sparse SMBG observations, and (2) a multi-scale ResNet
path that directly predicts AGP metrics. An alignment mechanism between the two
paths is introduced to reduce bias and mitigate overfitting. In addition, we
develop an active point selector to identify realistic and informative SMBG
sampling points that reflect patient behavioral patterns.
  Experimental results on a large, real-world dataset demonstrate that DPA-Net
achieves robust accuracy with low errors while reducing systematic bias. To the
best of our knowledge, this is the first supervised machine learning framework
for estimating AGP metrics from SMBG data, offering a practical and clinically
relevant decision-support tool in settings where CGM is not accessible.

</details>


### [162] [POME: Post Optimization Model Edit via Muon-style Projection](https://arxiv.org/abs/2510.06627)
*Yong Liu,Di Fu,Yang Luo,Zirui Zhu,Minhao Cheng,Cho-Jui Hsieh,Yang You*

Main category: cs.LG

TL;DR: POME is a post-optimization algorithm that improves fine-tuned LLMs by applying muon-style projection with truncated SVD to weight differences, requiring no extra data or training.


<details>
  <summary>Details</summary>
Motivation: To enhance fine-tuned language model performance without additional data or optimization overhead, leveraging only existing pretrained and fine-tuned checkpoints.

Method: Applies muon-style projection to ΔW (weight differences) using truncated SVD to equalize dominant update directions and prune noisy singular values as a post-processing step.

Result: Consistent performance gains: +2.5% on GSM8K and +1.0% on code generation, applicable to models from 7B to 72B parameters.

Conclusion: POME provides a practical, zero-cost enhancement compatible with any fine-tuning pipeline, requiring no modifications to existing training frameworks.

Abstract: We introduce Post-Optimization Model Edit (POME), a new algorithm that
enhances the performance of fine-tuned large language models using only their
pretrained and fine-tuned checkpoints, without requiring extra data or further
optimization. The core idea is to apply a muon-style projection to $\Delta W$,
the difference between the fine-tuned and pretrained weights. This projection
uses truncated singular value decomposition (SVD) to equalize the influence of
dominant update directions and prune small singular values, which often
represent noise. As a simple post-processing step, POME is completely decoupled
from the training pipeline. It requires zero modifications and imposes no
overhead, making it universally compatible with any optimizer or distributed
framework. POME delivers consistent gains, boosting average performance by
+2.5\% on GSM8K and +1.0\% on code generation. Its broad applicability -- from
7B foundation models to 72B RLHF-instructed models -- establishes it as a
practical, zero-cost enhancement for any fine-tuning pipeline. Code is
available at https://github.com/NUS-HPC-AI-Lab/POME.

</details>


### [163] [AI-Driven Forecasting and Monitoring of Urban Water System](https://arxiv.org/abs/2510.06631)
*Qiming Guo,Bishal Khatri,Hua Zhang,Wenlu Wang*

Main category: cs.LG

TL;DR: Proposes an AI and remote-sensor framework called HydroNet for detecting leaks in underground water pipelines using sparse sensor deployments and pipeline attributes in a directed graph model.


<details>
  <summary>Details</summary>
Motivation: Underground water pipelines suffer from leaks and infiltrations causing water loss and environmental damage, while conventional inspections are inefficient and dense sensor networks are too expensive.

Method: Deploy sparse remote sensors to capture real-time flow and depth data, combined with HydroNet model that uses pipeline attributes (material, diameter, slope) in a directed graph structure with edge-aware message passing and hydraulic simulations.

Result: Evaluation on real-world campus wastewater network shows the system collects effective spatio-temporal hydraulic data and HydroNet outperforms advanced baselines in leak detection accuracy.

Conclusion: The integration of edge-aware message passing with hydraulic simulations enables accurate network-wide predictions from limited sensor deployments, and this approach can be effectively extended to various underground water pipeline networks.

Abstract: Underground water and wastewater pipelines are vital for city operations but
plagued by anomalies like leaks and infiltrations, causing substantial water
loss, environmental damage, and high repair costs. Conventional manual
inspections lack efficiency, while dense sensor deployments are prohibitively
expensive. In recent years, artificial intelligence has advanced rapidly and is
increasingly applied to urban infrastructure. In this research, we propose an
integrated AI and remote-sensor framework to address the challenge of leak
detection in underground water pipelines, through deploying a sparse set of
remote sensors to capture real-time flow and depth data, paired with HydroNet -
a dedicated model utilizing pipeline attributes (e.g., material, diameter,
slope) in a directed graph for higher-precision modeling. Evaluations on a
real-world campus wastewater network dataset demonstrate that our system
collects effective spatio-temporal hydraulic data, enabling HydroNet to
outperform advanced baselines. This integration of edge-aware message passing
with hydraulic simulations enables accurate network-wide predictions from
limited sensor deployments. We envision that this approach can be effectively
extended to a wide range of underground water pipeline networks.

</details>


### [164] [Chem-NMF: Multi-layer $α$-divergence Non-Negative Matrix Factorization for Cardiorespiratory Disease Clustering, with Improved Convergence Inspired by Chemical Catalysts and Rigorous Asymptotic Analysis](https://arxiv.org/abs/2510.06632)
*Yasaman Torabi,Shahram Shirani,James P. Reilly*

Main category: cs.LG

TL;DR: Chem-NMF: A novel multi-layer NMF method inspired by chemical reaction energy barriers, with theoretical convergence analysis and improved clustering performance on biomedical signals and face images.


<details>
  <summary>Details</summary>
Motivation: Extending NMF with α-divergence to multi-layer architectures faces convergence challenges, requiring rigorous theoretical analysis.

Method: Proposed Chem-NMF with bounding factor inspired by Boltzmann probability of energy barriers in chemical reactions, providing theoretical convergence analysis.

Result: Improves clustering accuracy by 5.6% ± 2.7% on biomedical signals and 11.1% ± 7.2% on face images.

Conclusion: First study applying physical chemistry perspective to analyze NMF convergence, demonstrating practical improvements in real-world applications.

Abstract: Non-Negative Matrix Factorization (NMF) is an unsupervised learning method
offering low-rank representations across various domains such as audio
processing, biomedical signal analysis, and image recognition. The
incorporation of $\alpha$-divergence in NMF formulations enhances flexibility
in optimization, yet extending these methods to multi-layer architectures
presents challenges in ensuring convergence. To address this, we introduce a
novel approach inspired by the Boltzmann probability of the energy barriers in
chemical reactions to theoretically perform convergence analysis. We introduce
a novel method, called Chem-NMF, with a bounding factor which stabilizes
convergence. To our knowledge, this is the first study to apply a physical
chemistry perspective to rigorously analyze the convergence behaviour of the
NMF algorithm. We start from mathematically proven asymptotic convergence
results and then show how they apply to real data. Experimental results
demonstrate that the proposed algorithm improves clustering accuracy by 5.6%
$\pm$ 2.7% on biomedical signals and 11.1% $\pm$ 7.2% on face images (mean
$\pm$ std).

</details>


### [165] [Three Forms of Stochastic Injection for Improved Distribution-to-Distribution Generative Modeling](https://arxiv.org/abs/2510.06634)
*Shiye Su,Yuhui Zhang,Linqi Zhou,Rajesh Ranganath,Serena Yeung-Levy*

Main category: cs.LG

TL;DR: The paper addresses the challenge of modeling transformations between arbitrary data distributions, proposing a stochastic flow matching method that improves generation quality and reduces transport cost in distribution-to-distribution settings.


<details>
  <summary>Details</summary>
Motivation: Current flow matching methods primarily focus on noise-to-data transformations, but their application in general distribution-to-distribution settings is underexplored, especially when the source is a data distribution learned from limited samples where standard flow matching fails due to sparse supervision.

Method: The authors propose a simple and computationally efficient method that injects stochasticity into the training process by perturbing source samples and flow interpolants.

Result: On five diverse imaging tasks spanning biology, radiology, and astronomy, the method significantly improves generation quality, outperforming existing baselines by an average of 9 FID points, and reduces transport cost between input and generated samples.

Conclusion: The proposed approach makes flow matching a more practical tool for simulating diverse distribution transformations that arise in scientific applications like drug discovery and evolutionary simulation.

Abstract: Modeling transformations between arbitrary data distributions is a
fundamental scientific challenge, arising in applications like drug discovery
and evolutionary simulation. While flow matching offers a natural framework for
this task, its use has thus far primarily focused on the noise-to-data setting,
while its application in the general distribution-to-distribution setting is
underexplored. We find that in the latter case, where the source is also a data
distribution to be learned from limited samples, standard flow matching fails
due to sparse supervision. To address this, we propose a simple and
computationally efficient method that injects stochasticity into the training
process by perturbing source samples and flow interpolants. On five diverse
imaging tasks spanning biology, radiology, and astronomy, our method
significantly improves generation quality, outperforming existing baselines by
an average of 9 FID points. Our approach also reduces the transport cost
between input and generated samples to better highlight the true effect of the
transformation, making flow matching a more practical tool for simulating the
diverse distribution transformations that arise in science.

</details>


### [166] [StruSR: Structure-Aware Symbolic Regression with Physics-Informed Taylor Guidance](https://arxiv.org/abs/2510.06635)
*Yunpeng Gong,Sihan Lan,Can Yang,Kunpeng Xu,Min Jiang*

Main category: cs.LG

TL;DR: StruSR is a structure-aware symbolic regression framework that uses Physics-Informed Neural Networks (PINNs) to extract physical priors from time series data, guiding symbolic expression evolution through local Taylor expansions and genetic programming with physics-aware operations.


<details>
  <summary>Details</summary>
Motivation: Traditional symbolic regression methods lack mechanisms to extract structured physical priors from time series observations, making it difficult to capture symbolic expressions that reflect the system's global behavior governed by physical laws.

Method: Uses trained PINNs to extract locally structured physical priors via local Taylor expansions, introduces masking-based attribution to quantify subtree contributions, and employs genetic programming with physics-aware mutation/crossover operations guided by a hybrid fitness function minimizing physics residuals and Taylor coefficient mismatch.

Result: Experiments on benchmark PDE systems show StruSR improves convergence speed, structural fidelity, and expression interpretability compared to conventional baselines.

Conclusion: StruSR offers a principled paradigm for physics-grounded symbolic discovery by effectively leveraging physical priors and structural information from neural networks to guide symbolic regression.

Abstract: Symbolic regression aims to find interpretable analytical expressions by
searching over mathematical formula spaces to capture underlying system
behavior, particularly in scientific modeling governed by physical laws.
However, traditional methods lack mechanisms for extracting structured physical
priors from time series observations, making it difficult to capture symbolic
expressions that reflect the system's global behavior. In this work, we propose
a structure-aware symbolic regression framework, called StruSR, that leverages
trained Physics-Informed Neural Networks (PINNs) to extract locally structured
physical priors from time series data. By performing local Taylor expansions on
the outputs of the trained PINN, we obtain derivative-based structural
information to guide symbolic expression evolution. To assess the importance of
expression components, we introduce a masking-based attribution mechanism that
quantifies each subtree's contribution to structural alignment and physical
residual reduction. These sensitivity scores steer mutation and crossover
operations within genetic programming, preserving substructures with high
physical or structural significance while selectively modifying less
informative components. A hybrid fitness function jointly minimizes physics
residuals and Taylor coefficient mismatch, ensuring consistency with both the
governing equations and the local analytical behavior encoded by the PINN.
Experiments on benchmark PDE systems demonstrate that StruSR improves
convergence speed, structural fidelity, and expression interpretability
compared to conventional baselines, offering a principled paradigm for
physics-grounded symbolic discovery.

</details>


### [167] [Control-Augmented Autoregressive Diffusion for Data Assimilation](https://arxiv.org/abs/2510.06637)
*Prakhar Srivastava,Farrin Marouf Sofian,Francesco Immorlano,Kushagra Pandey,Stephan Mandt*

Main category: cs.LG

TL;DR: Proposes an amortized framework with a lightweight controller network for Auto-Regressive Diffusion Models (ARDMs) that enables efficient data assimilation for chaotic spatiotemporal PDEs through single forward rollouts with on-the-fly corrections.


<details>
  <summary>Details</summary>
Motivation: Guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored, and existing methods for data assimilation in chaotic spatiotemporal PDEs are computationally prohibitive and prone to forecast drift under sparse observations.

Method: Augments pretrained ARDMs with a lightweight controller network trained offline by previewing future ARDM rollouts and learning stepwise controls that anticipate upcoming observations under a terminal cost objective.

Result: Reduces data assimilation inference to a single forward rollout with on-the-fly corrections, avoiding expensive adjoint computations and optimizations during inference. Consistently outperforms four state-of-the-art baselines in stability, accuracy, and physical fidelity across two canonical PDEs and six observation regimes.

Conclusion: The proposed amortized framework provides an efficient and effective solution for data assimilation in chaotic spatiotemporal systems, demonstrating superior performance compared to existing methods while maintaining computational efficiency.

Abstract: Despite recent advances in test-time scaling and finetuning of diffusion
models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains
underexplored. We introduce an amortized framework that augments pretrained
ARDMs with a lightweight controller network, trained offline by previewing
future ARDM rollouts and learning stepwise controls that anticipate upcoming
observations under a terminal cost objective. We evaluate this framework in the
context of data assimilation (DA) for chaotic spatiotemporal partial
differential equations (PDEs), a setting where existing methods are often
computationally prohibitive and prone to forecast drift under sparse
observations. Our approach reduces DA inference to a single forward rollout
with on-the-fly corrections, avoiding expensive adjoint computations and/or
optimizations during inference. We demonstrate that our method consistently
outperforms four state-of-the-art baselines in stability, accuracy, and
physical fidelity across two canonical PDEs and six observation regimes. We
will release code and checkpoints publicly.

</details>


### [168] [The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators](https://arxiv.org/abs/2510.06646)
*Mansi Sakarvadia,Kareem Hegazy,Amin Totounferoush,Kyle Chard,Yaoqing Yang,Ian Foster,Michael W. Mahoney*

Main category: cs.LG

TL;DR: Machine-learned operators (MLOs) fail at zero-shot super-resolution and multi-resolution inference due to aliasing and inability to extrapolate/interpolate across resolutions. A simple multi-resolution training protocol is proposed to address these issues.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether MLOs can perform zero-shot super-resolution - inference at higher resolutions than training data - which is crucial for scientific machine learning of continuous phenomena.

Method: Comprehensive evaluation of MLOs' multi-resolution inference capabilities, decoupling extrapolation to varying frequencies and interpolation across resolutions. Proposed a data-driven multi-resolution training protocol.

Result: MLOs fail at zero-shot multi-resolution inference - they cannot extrapolate to different frequencies or interpolate across resolutions, and are brittle/susceptible to aliasing.

Conclusion: MLOs cannot perform accurate zero-shot multi-resolution inference without specialized training. The proposed multi-resolution training protocol overcomes aliasing and enables robust generalization across resolutions.

Abstract: A core challenge in scientific machine learning, and scientific computing
more generally, is modeling continuous phenomena which (in practice) are
represented discretely. Machine-learned operators (MLOs) have been introduced
as a means to achieve this modeling goal, as this class of architecture can
perform inference at arbitrary resolution. In this work, we evaluate whether
this architectural innovation is sufficient to perform "zero-shot
super-resolution," namely to enable a model to serve inference on
higher-resolution data than that on which it was originally trained. We
comprehensively evaluate both zero-shot sub-resolution and super-resolution
(i.e., multi-resolution) inference in MLOs. We decouple multi-resolution
inference into two key behaviors: 1) extrapolation to varying frequency
information; and 2) interpolating across varying resolutions. We empirically
demonstrate that MLOs fail to do both of these tasks in a zero-shot manner.
Consequently, we find MLOs are not able to perform accurate inference at
resolutions different from those on which they were trained, and instead they
are brittle and susceptible to aliasing. To address these failure modes, we
propose a simple, computationally-efficient, and data-driven multi-resolution
training protocol that overcomes aliasing and that provides robust
multi-resolution generalization.

</details>


### [169] [Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions](https://arxiv.org/abs/2510.06649)
*Frank Wu,Mengye Ren*

Main category: cs.LG

TL;DR: The paper introduces ARQ, a novel value estimation method that applies the Forward-Forward algorithm's goodness function to reinforcement learning, achieving state-of-the-art performance without backpropagation.


<details>
  <summary>Details</summary>
Motivation: The Forward-Forward algorithm has been largely confined to supervised learning, creating a gap in domains like RL where learning signals can be more naturally obtained. The authors aim to extend FF's principles to reinforcement learning.

Method: Proposes Action-conditioned Root mean squared Q-Functions (ARQ), which uses a goodness function and action conditioning for local RL with temporal difference learning, inspired by FF's layer activity statistics approach.

Result: ARQ achieves superior performance compared to state-of-the-art local backprop-free RL methods on MinAtar and DeepMind Control Suite benchmarks, and outperforms backpropagation-trained algorithms on most tasks.

Conclusion: The ARQ method successfully bridges the gap between Forward-Forward algorithms and reinforcement learning, demonstrating that biologically-inspired local learning approaches can achieve competitive performance without backpropagation.

Abstract: The Forward-Forward (FF) Algorithm is a recently proposed learning procedure
for neural networks that employs two forward passes instead of the traditional
forward and backward passes used in backpropagation. However, FF remains
largely confined to supervised settings, leaving a gap at domains where
learning signals can be yielded more naturally such as RL. In this work,
inspired by FF's goodness function using layer activity statistics, we
introduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value
estimation method that applies a goodness function and action conditioning for
local RL using temporal difference learning. Despite its simplicity and
biological grounding, our approach achieves superior performance compared to
state-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind
Control Suite benchmarks, while also outperforming algorithms trained with
backpropagation on most tasks. Code can be found at
https://github.com/agentic-learning-ai-lab/arq.

</details>


### [170] [Rethinking Nonlinearity: Trainable Gaussian Mixture Modules for Modern Neural Architectures](https://arxiv.org/abs/2510.06660)
*Weiguo Lu,Gangnan Yuan,Hong-kun Zhang,Shangyang Li*

Main category: cs.LG

TL;DR: The paper introduces Gaussian Mixture-Inspired Nonlinear Modules (GMNM), a new differentiable module that enhances neural network nonlinearity by leveraging Gaussian mixture models and distance properties, improving performance across various architectures.


<details>
  <summary>Details</summary>
Motivation: Conventional neural networks are limited in nonlinearity by standard activation functions like ReLU and Softmax. The authors aim to overcome this limitation by developing more flexible nonlinear modules.

Method: GMNM draws on Gaussian mixture models and Gaussian kernel distance properties. It relaxes probabilistic constraints and uses flexible Gaussian projections, allowing seamless integration into diverse neural architectures and end-to-end training with gradient methods.

Result: Experiments show that incorporating GMNM into MLPs, CNNs, attention mechanisms, and LSTMs consistently improves performance over standard baselines across various applications.

Conclusion: GMNM demonstrates strong potential as a powerful and flexible module for enhancing both efficiency and accuracy in a wide range of machine learning applications.

Abstract: Neural networks in general, from MLPs and CNNs to attention-based
Transformers, are constructed from layers of linear combinations followed by
nonlinear operations such as ReLU, Sigmoid, or Softmax. Despite their strength,
these conventional designs are often limited in introducing non-linearity by
the choice of activation functions. In this work, we introduce Gaussian
Mixture-Inspired Nonlinear Modules (GMNM), a new class of differentiable
modules that draw on the universal density approximation Gaussian mixture
models (GMMs) and distance properties (metric space) of Gaussian kernal. By
relaxing probabilistic constraints and adopting a flexible parameterization of
Gaussian projections, GMNM can be seamlessly integrated into diverse neural
architectures and trained end-to-end with gradient-based methods. Our
experiments demonstrate that incorporating GMNM into architectures such as
MLPs, CNNs, attention mechanisms, and LSTMs consistently improves performance
over standard baselines. These results highlight GMNM's potential as a powerful
and flexible module for enhancing efficiency and accuracy across a wide range
of machine learning applications.

</details>


### [171] [The Effect of Attention Head Count on Transformer Approximation](https://arxiv.org/abs/2510.06662)
*Penghao Yu,Haotian Jiang,Zeyu Bao,Ruoxi Yu,Qianxiao Li*

Main category: cs.LG

TL;DR: This paper analyzes how the number of attention heads in transformers affects their expressive power, establishing theoretical bounds on parameter complexity for approximation and showing practical implications.


<details>
  <summary>Details</summary>
Motivation: Despite transformers being dominant for sequence modeling, there's limited understanding of how structural parameters like attention heads influence expressive power.

Method: Introduced generalized D-retrieval task, established upper/lower bounds on parameter complexity for ε-approximation, analyzed single-head case, and validated with experiments on synthetic and real-world data.

Result: Transformers with many heads admit efficient approximation, while too few heads require parameter scaling of O(1/ε^{cT}). Single-head transformers with O(T) embedding dimension can achieve complete memorization.

Conclusion: The number of attention heads critically impacts transformer expressive power, with rigorous theoretical bounds established and practical relevance demonstrated.

Abstract: Transformer has become the dominant architecture for sequence modeling, yet a
detailed understanding of how its structural parameters influence expressive
power remains limited. In this work, we study the approximation properties of
transformers, with particular emphasis on the role of the number of attention
heads. Our analysis begins with the introduction of a generalized $D$-retrieval
task, which we prove to be dense in the space of continuous functions, thereby
providing the basis for our theoretical framework. We then establish both upper
and lower bounds on the parameter complexity required for
$\epsilon$-approximation. Specifically, we show that transformers with
sufficiently many heads admit efficient approximation, whereas with too few
heads, the number of parameters must scale at least as $O(1/\epsilon^{cT})$,
for some constant $c$ and sequence length $T$. To the best of our knowledge,
this constitutes the first rigorous lower bound of this type in a nonlinear and
practically relevant setting. We further examine the single-head case and
demonstrate that an embedding dimension of order $O(T)$ allows complete
memorization of the input, where approximation is entirely achieved by the
feed-forward block. Finally, we validate our theoretical findings with
experiments on both synthetic data and real-world tasks, illustrating the
practical relevance of our results.

</details>


### [172] [XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation](https://arxiv.org/abs/2510.06672)
*Udbhav Bamba,Minghao Fang,Yifan Yu,Haizhong Zheng,Fan Lai*

Main category: cs.LG

TL;DR: XRPO is a reinforcement learning framework that improves LLM reasoning by balancing exploration and exploitation through adaptive rollout allocation, in-context seeding, and advantage sharpening, achieving better performance and faster convergence than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing RL approaches for LLMs suffer from limited exploration on challenging prompts and underexploited feedback signals due to uniform rollout allocation and reliance on sparse rewards.

Method: XRPO introduces: 1) adaptive rollout allocator prioritizing prompts with high uncertainty reduction potential, 2) in-context seeding with curated exemplars for difficult reasoning trajectories, 3) group-relative novelty-aware advantage sharpening using sequence likelihoods to amplify correct low-probability responses.

Result: XRPO outperforms GRPO and GSPO by up to 4% pass@1 and 6% cons@32 across math and coding benchmarks, while accelerating training convergence by up to 2.7X.

Conclusion: XRPO provides a principled exploration-exploitation framework that significantly improves LLM reasoning performance and training efficiency through adaptive rollout allocation and enhanced feedback utilization.

Abstract: Reinforcement learning algorithms such as GRPO have driven recent advances in
large language model (LLM) reasoning. While scaling the number of rollouts
stabilizes training, existing approaches suffer from limited exploration on
challenging prompts and leave informative feedback signals underexploited, due
to context-independent rollout allocation across prompts (e.g., generating 16
rollouts per prompt) and relying heavily on sparse rewards. This paper presents
XRPO(eXplore - eXploit GRPO), a unified framework that recasts policy
optimization through the principled lens of rollout exploration-exploitation.
To enhance exploration, XRPO introduces a mathematically grounded rollout
allocator that adaptively prioritizes prompts with higher potential for
uncertainty reduction. It further addresses stagnation on zero-reward prompts
through an in-context seeding strategy that injects curated exemplars, steering
the model into more difficult reasoning trajectories. To strengthen
exploitation, XRPO develops a group-relative, novelty-aware advantage
sharpening mechanism that leverages sequence likelihoods to amplify
low-probability yet correct responses, thereby extending the policy's reach
beyond sparse rewards. Experiments across diverse math and coding benchmarks on
both reasoning and non-reasoning models demonstrate that XRPO outperforms
existing advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while
accelerating training convergence by up to 2.7X.

</details>


### [173] [TimeFormer: Transformer with Attention Modulation Empowered by Temporal Characteristics for Time Series Forecasting](https://arxiv.org/abs/2510.06680)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Baixin Li,Yongsheng Huang,Mingyang Geng,Changsheng Zhang,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: TimeFormer is a novel Transformer architecture for time series forecasting that incorporates temporal priors through a modulated self-attention mechanism and multi-scale analysis, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Transformers are effective in NLP but face challenges in time series forecasting due to insufficient consideration of differences between textual and temporal modalities, particularly unidirectional influence and decaying influence over time.

Method: Proposes TimeFormer with MoSA (modulated self-attention) mechanism that captures temporal priors under Hawkes process constraints and causal masking, plus multi-scale subsequence analysis framework for capturing semantic dependencies at different temporal scales.

Result: Significantly outperforms state-of-the-art methods on multiple real-world datasets, achieving up to 7.45% reduction in MSE compared to best baseline and setting new benchmarks on 94.04% of evaluation metrics.

Conclusion: TimeFormer effectively addresses temporal characteristics in time series forecasting and the MoSA mechanism can be broadly applied to enhance other Transformer-based models.

Abstract: Although Transformers excel in natural language processing, their extension
to time series forecasting remains challenging due to insufficient
consideration of the differences between textual and temporal modalities. In
this paper, we develop a novel Transformer architecture designed for time
series data, aiming to maximize its representational capacity. We identify two
key but often overlooked characteristics of time series: (1) unidirectional
influence from the past to the future, and (2) the phenomenon of decaying
influence over time. These characteristics are introduced to enhance the
attention mechanism of Transformers. We propose TimeFormer, whose core
innovation is a self-attention mechanism with two modulation terms (MoSA),
designed to capture these temporal priors of time series under the constraints
of the Hawkes process and causal masking. Additionally, TimeFormer introduces a
framework based on multi-scale and subsequence analysis to capture semantic
dependencies at different temporal scales, enriching the temporal dependencies.
Extensive experiments conducted on multiple real-world datasets show that
TimeFormer significantly outperforms state-of-the-art methods, achieving up to
a 7.45% reduction in MSE compared to the best baseline and setting new
benchmarks on 94.04\% of evaluation metrics. Moreover, we demonstrate that the
MoSA mechanism can be broadly applied to enhance the performance of other
Transformer-based models.

</details>


### [174] [Distributed Algorithms for Multi-Agent Multi-Armed Bandits with Collision](https://arxiv.org/abs/2510.06683)
*Daoyuan Zhou,Xuchuang Wang,Lin Yang,Yang Gao*

Main category: cs.LG

TL;DR: A distributed algorithm for multiplayer multi-armed bandits with adaptive communication achieves near-optimal regret with only O(log log T) communication cost, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: To solve the multiplayer multi-armed bandit problem in distributed settings without central coordination, where collisions occur when players select the same arm, and players can only observe their own actions and collision feedback.

Method: Proposed a distributed algorithm with an adaptive, efficient communication protocol that allows players to coordinate while minimizing communication overhead.

Result: Achieves near-optimal group and individual regret with communication cost of only O(log log T), and experiments show significant performance improvements over existing baselines with notable reduction in individual regret.

Conclusion: The approach effectively solves distributed multiplayer bandit problems with minimal communication, and can be extended to periodic asynchronous settings with logarithmic regret guarantees.

Abstract: We study the stochastic Multiplayer Multi-Armed Bandit (MMAB) problem, where
multiple players select arms to maximize their cumulative rewards. Collisions
occur when two or more players select the same arm, resulting in no reward, and
are observed by the players involved. We consider a distributed setting without
central coordination, where each player can only observe their own actions and
collision feedback. We propose a distributed algorithm with an adaptive,
efficient communication protocol. The algorithm achieves near-optimal group and
individual regret, with a communication cost of only $\mathcal{O}(\log\log T)$.
Our experiments demonstrate significant performance improvements over existing
baselines. Compared to state-of-the-art (SOTA) methods, our approach achieves a
notable reduction in individual regret. Finally, we extend our approach to a
periodic asynchronous setting, proving the lower bound for this problem and
presenting an algorithm that achieves logarithmic regret.

</details>


### [175] [AutoBalance: An Automatic Balancing Framework for Training Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06684)
*Kang An,Chenhao Si,Ming Yan,Shiqian Ma*

Main category: cs.LG

TL;DR: AutoBalance introduces a post-combine training paradigm for PINNs that assigns independent adaptive optimizers to each loss component, overcoming limitations of pre-combine gradient manipulation methods.


<details>
  <summary>Details</summary>
Motivation: Training PINNs is difficult due to conflicting objectives and different curvatures in multiple loss terms (PDE residuals, boundary conditions). Existing pre-combine gradient manipulation methods are fundamentally limited as they disrupt optimizer's internal preconditioning.

Method: AutoBalance uses a post-combine strategy where each loss component gets its own independent adaptive optimizer, and the resulting preconditioned updates are aggregated afterwards.

Result: Extensive experiments show AutoBalance consistently outperforms existing frameworks with significant reductions in solution error (MSE and L∞ norms). It also amplifies effectiveness of other PINN methodologies.

Conclusion: AutoBalance provides a superior training paradigm for PINNs that is orthogonal and complementary to existing methods, achieving better performance on challenging PDE benchmarks.

Abstract: Physics-Informed Neural Networks (PINNs) provide a powerful and general
framework for solving Partial Differential Equations (PDEs) by embedding
physical laws into loss functions. However, training PINNs is notoriously
difficult due to the need to balance multiple loss terms, such as PDE residuals
and boundary conditions, which often have conflicting objectives and vastly
different curvatures. Existing methods address this issue by manipulating
gradients before optimization (a "pre-combine" strategy). We argue that this
approach is fundamentally limited, as forcing a single optimizer to process
gradients from spectrally heterogeneous loss landscapes disrupts its internal
preconditioning. In this work, we introduce AutoBalance, a novel "post-combine"
training paradigm. AutoBalance assigns an independent adaptive optimizer to
each loss component and aggregates the resulting preconditioned updates
afterwards. Extensive experiments on challenging PDE benchmarks show that
AutoBalance consistently outperforms existing frameworks, achieving significant
reductions in solution error, as measured by both the MSE and $L^{\infty}$
norms. Moreover, AutoBalance is orthogonal to and complementary with other
popular PINN methodologies, amplifying their effectiveness on demanding
benchmarks.

</details>


### [176] [A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking](https://arxiv.org/abs/2510.06699)
*Gal Fadlon,Idan Arbiv,Nimrod Berman,Omri Azencot*

Main category: cs.LG

TL;DR: A novel two-step framework for generating realistic irregular time series data using completion and diffusion models, achieving 70% improvement in discriminative score and 85% reduction in computational cost.


<details>
  <summary>Details</summary>
Motivation: Irregular sampling and missing values in time series data pose significant challenges for generation tasks. Existing methods yield suboptimal results with high computational costs, and simple masking approaches create unnatural neighborhoods that disrupt learning.

Method: Two-step framework: 1) Time Series Transformer completes irregular sequences to create natural neighborhoods, 2) Vision-based diffusion model with masking minimizes dependence on completed values, leveraging strengths of both completion and masking.

Result: State-of-the-art performance with 70% relative improvement in discriminative score and 85% reduction in computational cost compared to previous methods.

Conclusion: The proposed approach effectively addresses irregular time series generation by combining completion and masking strategies, enabling robust and efficient generation of realistic time series data.

Abstract: Generating realistic time series data is critical for applications in
healthcare, finance, and science. However, irregular sampling and missing
values present significant challenges. While prior methods address these
irregularities, they often yield suboptimal results and incur high
computational costs. Recent advances in regular time series generation, such as
the diffusion-based ImagenTime model, demonstrate strong, fast, and scalable
generative capabilities by transforming time series into image representations,
making them a promising solution. However, extending ImagenTime to irregular
sequences using simple masking introduces "unnatural" neighborhoods, where
missing values replaced by zeros disrupt the learning process. To overcome
this, we propose a novel two-step framework: first, a Time Series Transformer
completes irregular sequences, creating natural neighborhoods; second, a
vision-based diffusion model with masking minimizes dependence on the completed
values. This approach leverages the strengths of both completion and masking,
enabling robust and efficient generation of realistic time series. Our method
achieves state-of-the-art performance, achieving a relative improvement in
discriminative score by $70\%$ and in computational cost by $85\%$. Code is at
https://github.com/azencot-group/ImagenI2R.

</details>


### [177] [Dual Goal Representations](https://arxiv.org/abs/2510.06714)
*Seohong Park,Deepinder Mann,Sergey Levine*

Main category: cs.LG

TL;DR: The paper introduces dual goal representations for goal-conditioned RL, which encode states through temporal distances to all other states, providing dynamics-invariant representations that improve goal-reaching performance.


<details>
  <summary>Details</summary>
Motivation: To create goal representations that are invariant to state representation and capture essential temporal relationships between states for more effective goal-conditioned reinforcement learning.

Method: Develop dual goal representations that characterize states by their temporal distances to all other states, and create a practical learning method that can be integrated with existing GCRL algorithms.

Result: Empirical evaluation on OGBench shows consistent performance improvements across 20 state- and pixel-based tasks in offline goal-reaching scenarios.

Conclusion: Dual goal representations provide theoretically sound and practically effective representations for GCRL that are dynamics-invariant and improve goal-reaching performance across diverse environments.

Abstract: In this work, we introduce dual goal representations for goal-conditioned
reinforcement learning (GCRL). A dual goal representation characterizes a state
by "the set of temporal distances from all other states"; in other words, it
encodes a state through its relations to every other state, measured by
temporal distance. This representation provides several appealing theoretical
properties. First, it depends only on the intrinsic dynamics of the environment
and is invariant to the original state representation. Second, it contains
provably sufficient information to recover an optimal goal-reaching policy,
while being able to filter out exogenous noise. Based on this concept, we
develop a practical goal representation learning method that can be combined
with any existing GCRL algorithm. Through diverse experiments on the OGBench
task suite, we empirically show that dual goal representations consistently
improve offline goal-reaching performance across 20 state- and pixel-based
tasks.

</details>


### [178] [Incorporating Expert Knowledge into Bayesian Causal Discovery of Mixtures of Directed Acyclic Graphs](https://arxiv.org/abs/2510.06735)
*Zachris Björkman,Jorge Loría,Sophie Wharrie,Samuel Kaski*

Main category: cs.LG

TL;DR: A Bayesian causal discovery method for heterogeneous domains that combines expert elicitation with variational mixture structure learning to infer multiple causal Bayesian networks.


<details>
  <summary>Details</summary>
Motivation: Existing prior elicitation approaches assume a single causal graph and are unsuitable for heterogeneous domains where multiple causal models may exist.

Method: Proposes causal elicitation strategy based on Bayesian experimental design (BED) and variational mixture structure learning (VaMSL) extending DiBS method to iteratively infer mixtures of causal Bayesian networks.

Result: Method successfully produces alternative causal models (mixture components), achieves improved structure learning on heterogeneous synthetic data with simulated expert feedback, and captures complex distributions in breast cancer database.

Conclusion: The approach effectively handles heterogeneous causal discovery by incorporating expert knowledge through Bayesian experimental design and variational mixture learning.

Abstract: Bayesian causal discovery benefits from prior information elicited from
domain experts, and in heterogeneous domains any prior knowledge would be badly
needed. However, so far prior elicitation approaches have assumed a single
causal graph and hence are not suited to heterogeneous domains. We propose a
causal elicitation strategy for heterogeneous settings, based on Bayesian
experimental design (BED) principles, and a variational mixture structure
learning (VaMSL) method -- extending the earlier differentiable Bayesian
structure learning (DiBS) method -- to iteratively infer mixtures of causal
Bayesian networks (CBNs). We construct an informative graph prior incorporating
elicited expert feedback in the inference of mixtures of CBNs. Our proposed
method successfully produces a set of alternative causal models (mixture
components or clusters), and achieves an improved structure learning
performance on heterogeneous synthetic data when informed by a simulated
expert. Finally, we demonstrate that our approach is capable of capturing
complex distributions in a breast cancer database.

</details>


### [179] [Function regression using the forward forward training and inferring paradigm](https://arxiv.org/abs/2510.06762)
*Shivam Padmani,Akshay Joshi*

Main category: cs.LG

TL;DR: This paper introduces a novel methodology for function regression using the Forward-Forward algorithm, extending its application beyond classification tasks to include univariate and multivariate function approximation, with preliminary studies on Kolmogorov Arnold Networks and Deep Physical Neural Networks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend the Forward-Forward learning algorithm, which is well-suited for neuromorphic computing and physical neural networks, from classification tasks to function regression/approximation tasks, as this fundamental machine learning application currently lacks Forward-Forward implementations.

Method: The paper develops a new methodology for function regression using the Forward-Forward algorithm, evaluating it on both univariate and multivariate functions, and conducts preliminary studies on extending this approach to Kolmogorov Arnold Networks and Deep Physical Neural Networks.

Result: The paper presents results from evaluating the developed Forward-Forward regression methodology on various function approximation tasks, though specific performance metrics are not detailed in the abstract.

Conclusion: The paper successfully demonstrates that the Forward-Forward algorithm can be extended to function regression tasks, opening up new possibilities for implementing regression in neuromorphic computing systems and physical neural networks that benefit from the Forward-Forward paradigm.

Abstract: Function regression/approximation is a fundamental application of machine
learning. Neural networks (NNs) can be easily trained for function regression
using a sufficient number of neurons and epochs. The forward-forward learning
algorithm is a novel approach for training neural networks without
backpropagation, and is well suited for implementation in neuromorphic
computing and physical analogs for neural networks. To the best of the authors'
knowledge, the Forward Forward paradigm of training and inferencing NNs is
currently only restricted to classification tasks. This paper introduces a new
methodology for approximating functions (function regression) using the
Forward-Forward algorithm. Furthermore, the paper evaluates the developed
methodology on univariate and multivariate functions, and provides preliminary
studies of extending the proposed Forward-Forward regression to Kolmogorov
Arnold Networks, and Deep Physical Neural Networks.

</details>


### [180] [Modeling COVID-19 Dynamics in German States Using Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06776)
*Phillip Rothenbeck,Sai Karthikeya Vemuri,Niklas Penzel,Joachim Denzler*

Main category: cs.LG

TL;DR: Using Physics-Informed Neural Networks (PINNs) to solve inverse SIR model problems with German COVID-19 data, enabling spatio-temporal analysis of transmission parameters across federal states over 3 years.


<details>
  <summary>Details</summary>
Motivation: Compartmental models like SIR have limitations in incorporating noisy observational data directly. Need for quantitative modeling to understand COVID-19 dynamics and evaluate public health interventions.

Method: Employ Physics-Informed Neural Networks (PINNs) to solve inverse SIR model problems using infection data from Robert Koch Institute (RKI) for German federal states.

Result: Estimated state-specific transmission and recovery parameters and time-varying reproduction number (R_t). Found strong regional variations in transmission behavior correlated with vaccination uptake and pandemic phases.

Conclusion: Demonstrates utility of PINNs for localized, long-term epidemiological modeling with fine-grained spatio-temporal analysis capabilities.

Abstract: The COVID-19 pandemic has highlighted the need for quantitative modeling and
analysis to understand real-world disease dynamics. In particular, post hoc
analyses using compartmental models offer valuable insights into the
effectiveness of public health interventions, such as vaccination strategies
and containment policies. However, such compartmental models like SIR
(Susceptible-Infectious-Recovered) often face limitations in directly
incorporating noisy observational data. In this work, we employ
Physics-Informed Neural Networks (PINNs) to solve the inverse problem of the
SIR model using infection data from the Robert Koch Institute (RKI). Our main
contribution is a fine-grained, spatio-temporal analysis of COVID-19 dynamics
across all German federal states over a three-year period. We estimate
state-specific transmission and recovery parameters and time-varying
reproduction number (R_t) to track the pandemic progression. The results
highlight strong variations in transmission behavior across regions, revealing
correlations with vaccination uptake and temporal patterns associated with
major pandemic phases. Our findings demonstrate the utility of PINNs in
localized, long-term epidemiological modeling.

</details>


### [181] [Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness](https://arxiv.org/abs/2510.06790)
*Tavish McDonald,Bo Lei,Stanislav Fort,Bhavya Kailkhura,Brian Bartoldson*

Main category: cs.LG

TL;DR: The paper proposes the Robustness from Inference Compute Hypothesis (RICH), arguing that inference-compute defenses work better when models can compositionally generalize from in-distribution components to understand out-of-distribution adversarial data, especially when base models are already robust.


<details>
  <summary>Details</summary>
Motivation: To address the limitation that test-time compute defenses lose effectiveness against gradient-based or multimodal attacks, and to show that inference-compute can still provide robustness benefits in these challenging scenarios.

Method: Empirical validation across vision language models and attack types, examining how compositional generalization enables adherence to defensive specifications on adversarially OOD inputs through inference-compute defenses.

Result: Robustness gains from test-time compute occur when compositional generalization enables specification following on OOD data, with inference-compute benefits correlating with base model robustness - creating a rich-get-richer dynamic.

Conclusion: Train-time and test-time defenses should be layered synergistically, as inference-compute defenses work best when base models are already robustified, enabling compositional generalization from ID components to understand OOD adversarial data.

Abstract: Models are susceptible to adversarially out-of-distribution (OOD) data
despite large training-compute investments into their robustification. Zaremba
et al. (2025) make progress on this problem at test time, showing LLM reasoning
improves satisfaction of model specifications designed to thwart attacks,
resulting in a correlation between reasoning effort and robustness to
jailbreaks. However, this benefit of test compute fades when attackers are
given access to gradients or multimodal inputs. We address this gap, clarifying
that inference-compute offers benefits even in such cases. Our approach argues
that compositional generalization, through which OOD data is understandable via
its in-distribution (ID) components, enables adherence to defensive
specifications on adversarially OOD inputs. Namely, we posit the Robustness
from Inference Compute Hypothesis (RICH): inference-compute defenses profit as
the model's training data better reflects the attacked data's components. We
empirically support this hypothesis across vision language model and attack
types, finding robustness gains from test-time compute if specification
following on OOD data is unlocked by compositional generalization, while RL
finetuning and protracted reasoning are not critical. For example, increasing
emphasis on defensive specifications via prompting lowers the success rate of
gradient-based multimodal attacks on VLMs robustified by adversarial
pretraining, but this same intervention provides no such benefit to
not-robustified models. This correlation of inference-compute's robustness
benefit with base model robustness is the rich-get-richer dynamic of the RICH:
attacked data components are more ID for robustified models, aiding
compositional generalization to OOD data. Accordingly, we advise layering
train-time and test-time defenses to obtain their synergistic benefit.

</details>


### [182] [The Unreasonable Effectiveness of Randomized Representations in Online Continual Graph Learning](https://arxiv.org/abs/2510.06819)
*Giovanni Donghi,Daniele Zambon,Luca Pasa,Cesare Alippi,Nicolò Navarin*

Main category: cs.LG

TL;DR: A simple yet effective approach for Online Continual Graph Learning (OCGL) that uses a fixed, randomly initialized encoder to generate stable node embeddings, training only a lightweight classifier online to minimize catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: To address catastrophic forgetting in OCGL where nodes arrive sequentially and distribution drifts occur, without requiring complex replay mechanisms or offline training.

Method: Freeze a randomly initialized graph encoder to generate stable node embeddings, and train only a lightweight classifier online to prevent representation parameter drifts.

Result: Achieves consistent gains over state-of-the-art methods, with up to 30% improvement, often approaching joint offline-training upper bound performance, without using memory buffers.

Conclusion: Catastrophic forgetting in OCGL can be effectively minimized through architectural simplicity and stability rather than complex replay or regularization strategies.

Abstract: Catastrophic forgetting is one of the main obstacles for Online Continual
Graph Learning (OCGL), where nodes arrive one by one, distribution drifts may
occur at any time and offline training on task-specific subgraphs is not
feasible. In this work, we explore a surprisingly simple yet highly effective
approach for OCGL: we use a fixed, randomly initialized encoder to generate
robust and expressive node embeddings by aggregating neighborhood information,
training online only a lightweight classifier. By freezing the encoder, we
eliminate drifts of the representation parameters, a key source of forgetting,
obtaining embeddings that are both expressive and stable. When evaluated across
several OCGL benchmarks, despite its simplicity and lack of memory buffer, this
approach yields consistent gains over state-of-the-art methods, with surprising
improvements of up to 30% and performance often approaching that of the joint
offline-training upper bound. These results suggest that in OCGL, catastrophic
forgetting can be minimized without complex replay or regularization by
embracing architectural simplicity and stability.

</details>


### [183] [Efficient numeracy in language models through single-token number embeddings](https://arxiv.org/abs/2510.06824)
*Linus Kreitner,Paul Hager,Jonathan Mengedoht,Georgios Kaissis,Daniel Rueckert,Martin J. Menten*

Main category: cs.LG

TL;DR: BitTokens: A novel tokenization method that encodes numbers as single tokens using IEEE 754 binary floating-point representation, enabling LLMs to process numerical data more efficiently and solve arithmetic operations with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with numerical data processing due to inefficient tokenization that splits numbers into multiple tokens, requiring excessive reasoning tokens for basic calculations and limiting the complexity of problems they can solve.

Method: Proposed BitTokens - a tokenization strategy that embeds any number into a single token using its IEEE 754 binary floating-point representation, fulfilling desiderata for efficient number encodings that existing approaches fail to meet.

Result: Extensive experiments show BitTokens enable even small language models to learn algorithms that solve basic arithmetic operations nearly perfectly, significantly improving numerical processing efficiency.

Conclusion: BitTokens' efficient single-token number encoding could expand the length and complexity of problems that language models can solve, addressing current limitations in numerical data processing.

Abstract: To drive progress in science and engineering, large language models (LLMs)
must be able to process large amounts of numerical data and solve long
calculations efficiently. This is currently only possible through the use of
external tools or extensive reasoning chains, either limiting the numerical
intuition of LLMs or limiting the length of problems they can solve. We show
that frontier LLMs require excessive amounts of reasoning tokens to solve even
basic calculations, which is exacerbated by their tokenization strategies that
split single numbers into multiple tokens. This motivates the need for
efficient and effective single-token number encodings. We introduce a set of
desiderata for such encodings and show that existing approaches fail to fulfill
them. To address these shortcomings, we propose BitTokens, a novel tokenization
strategy that embeds any number into a single token using its IEEE 754 binary
floating-point representation. Through extensive experiments we show that our
BitTokens allow even small language models to learn algorithms that solve basic
arithmetic operations nearly perfectly. This newly gained efficiency could
expand the length and complexity of problems language models can solve.

</details>


### [184] [Recurrence-Complete Frame-based Action Models](https://arxiv.org/abs/2510.06828)
*Michael Keiblinger*

Main category: cs.LG

TL;DR: The paper challenges the view that attention mechanisms alone are sufficient, arguing that recurrent architectures are necessary for long-running agentic tasks. It introduces a recurrence-complete architecture that shows improved performance with longer training sequences.


<details>
  <summary>Details</summary>
Motivation: To address limitations of non-recurrent architectures in handling long-running agentic tasks, particularly in scenarios where fully parallelizable models fail to correctly aggregate inputs over extended time periods.

Method: Introduces a recurrence-complete architecture and trains it on GitHub-derived action sequences, analyzing loss patterns and computational efficiency across different sequence lengths.

Result: Loss follows a power law with trained sequence length while maintaining fixed parameter count. Longer-sequence training amortizes its linearly increasing wall-time cost, yielding lower loss as a function of wall time.

Conclusion: Recurrent architectures are essential for certain classes of problems in agentic systems, and the proposed recurrence-complete model demonstrates scalable performance improvements with longer training sequences.

Abstract: In recent years, attention-like mechanisms have been used to great success in
the space of large language models, unlocking scaling potential to a previously
unthinkable extent. "Attention Is All You Need" famously claims RNN cells are
not needed in conjunction with attention. We challenge this view. In this
paper, we point to existing proofs that architectures with fully parallelizable
forward or backward passes cannot represent classes of problems specifically
interesting for long-running agentic tasks. We further conjecture a critical
time t beyond which non-recurrence-complete models fail to aggregate inputs
correctly, with concrete implications for agentic systems (e.g., software
engineering agents). To address this, we introduce a recurrence-complete
architecture and train it on GitHub-derived action sequences. Loss follows a
power law in the trained sequence length while the parameter count remains
fixed. Moreover, longer-sequence training always amortizes its linearly
increasing wall-time cost, yielding lower loss as a function of wall time.

</details>


### [185] [Early wind turbine alarm prediction based on machine learning: AlarmForecasting](https://arxiv.org/abs/2510.06831)
*Syed Shazaib Shah,Daoliang Tan*

Main category: cs.LG

TL;DR: The paper proposes an Alarm Forecasting and Classification (AFC) framework using LSTM for predicting wind turbine alarms 10-30 minutes in advance, achieving 82-41% accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches use alarm data only for diagnostics after faults occur, but this study aims to prevent alarms from triggering altogether to avoid impending failures.

Method: Two-stage framework: LSTM-based regression for time-series alarm forecasting, followed by classification module for alarm tagging on forecasted alarms.

Result: Experimental results on 14 Senvion MM82 turbines over 5 years showed 82%, 52%, and 41% accurate forecasts for 10, 20, and 30 minute alarm forecasts respectively.

Conclusion: The framework successfully anticipates and averts alarms, significantly reducing alarm frequency and enhancing operational efficiency through proactive intervention.

Abstract: Alarm data is pivotal in curbing fault behavior in Wind Turbines (WTs) and
forms the backbone for advancedpredictive monitoring systems. Traditionally,
research cohorts have been confined to utilizing alarm data solelyas a
diagnostic tool, merely indicative of unhealthy status. However, this study
aims to offer a transformativeleap towards preempting alarms, preventing alarms
from triggering altogether, and consequently avertingimpending failures. Our
proposed Alarm Forecasting and Classification (AFC) framework is designed on
twosuccessive modules: first, the regression module based on long short-term
memory (LSTM) for time-series alarmforecasting, and thereafter, the
classification module to implement alarm tagging on the forecasted alarm.
Thisway, the entire alarm taxonomy can be forecasted reliably rather than a few
specific alarms. 14 Senvion MM82turbines with an operational period of 5 years
are used as a case study; the results demonstrated 82%, 52%,and 41% accurate
forecasts for 10, 20, and 30 min alarm forecasts, respectively. The results
substantiateanticipating and averting alarms, which is significant in curbing
alarm frequency and enhancing operationalefficiency through proactive
intervention.

</details>


### [186] [Vectorized FlashAttention with Low-cost Exponential Computation in RISC-V Vector Processors](https://arxiv.org/abs/2510.06834)
*Vasileios Titopoulos,Kosmas Alexandridis,Giorgos Dimitrakopoulos*

Main category: cs.LG

TL;DR: This paper presents a vectorized implementation of FlashAttention for RISC-V processors, using low-cost exponential approximations and tiling strategies to improve performance.


<details>
  <summary>Details</summary>
Motivation: To accelerate attention kernels in machine learning models by vectorizing FlashAttention on RISC-V processors, reducing scalar code and computational complexity.

Method: Vectorized FlashAttention implementation using low-cost floating-point exponential approximations without custom ISA extensions, combined with memory locality optimization through tiling strategies.

Result: Experimental results show scalable approach with significant performance gains in processing attention layers for practical applications.

Conclusion: The vectorized FlashAttention implementation successfully accelerates attention kernels on RISC-V processors through efficient exponential approximations and memory optimization techniques.

Abstract: Attention is a core operation in numerous machine learning and artificial
intelligence models. This work focuses on the acceleration of attention kernel
using FlashAttention algorithm, in vector processors, particularly those based
on the RISC-V instruction set architecture (ISA). This work represents the
first effort to vectorize FlashAttention, minimizing scalar code and
simplifying the computational complexity of evaluating exponentials needed by
softmax used in attention. By utilizing a low-cost approximation for
exponentials in floating-point arithmetic, we reduce the cost of computing the
exponential function without the need to extend baseline vector ISA with new
custom instructions. Also, appropriate tiling strategies are explored with the
goal to improve memory locality. Experimental results highlight the scalability
of our approach, demonstrating significant performance gains with the
vectorized implementations when processing attention layers in practical
applications.

</details>


### [187] [CNN-TFT explained by SHAP with multi-head attention weights for time series forecasting](https://arxiv.org/abs/2510.06840)
*Stefano F. Stefenon,João P. Matos-Carvalho,Valderi R. Q. Leithardt,Kin-Choong Yow*

Main category: cs.LG

TL;DR: A hybrid CNN-Transformer architecture for multivariate time series forecasting that combines convolutional feature extraction with temporal fusion transformer, achieving 2.2% MAPE on hydroelectric flow data.


<details>
  <summary>Details</summary>
Motivation: To leverage complementary strengths of CNNs (local pattern capture) and transformers (long-range dependencies) for improved multivariate time series forecasting.

Method: Hierarchical 1D CNN for local pattern extraction and noise reduction, followed by temporal fusion transformer with multi-head attention for capturing dependencies and adaptive covariate weighting.

Result: Outperforms established deep learning models with 2.2% mean absolute percentage error on hydroelectric natural flow dataset.

Conclusion: The CNN-TFT-SHAP-MHAW architecture provides high-fidelity multivariate time series forecasting with explainability through SHAP-MHAW, promising for applications requiring accurate forecasts.

Abstract: Convolutional neural networks (CNNs) and transformer architectures offer
strengths for modeling temporal data: CNNs excel at capturing local patterns
and translational invariances, while transformers effectively model long-range
dependencies via self-attention. This paper proposes a hybrid architecture
integrating convolutional feature extraction with a temporal fusion transformer
(TFT) backbone to enhance multivariate time series forecasting. The CNN module
first applies a hierarchy of one-dimensional convolutional layers to distill
salient local patterns from raw input sequences, reducing noise and
dimensionality. The resulting feature maps are then fed into the TFT, which
applies multi-head attention to capture both short- and long-term dependencies
and to weigh relevant covariates adaptively. We evaluate the CNN-TFT on a
hydroelectric natural flow time series dataset. Experimental results
demonstrate that CNN-TFT outperforms well-established deep learning models,
with a mean absolute percentage error of up to 2.2%. The explainability of the
model is obtained by a proposed Shapley additive explanations with multi-head
attention weights (SHAP-MHAW). Our novel architecture, named CNN-TFT-SHAP-MHAW,
is promising for applications requiring high-fidelity, multivariate time series
forecasts, being available for future analysis at
https://github.com/SFStefenon/CNN-TFT-SHAP-MHAW .

</details>


### [188] [Enhancing Bankruptcy Prediction of Banks through Advanced Machine Learning Techniques: An Innovative Approach and Analysis](https://arxiv.org/abs/2510.06852)
*Zuherman Rustam,Sri Hartini,Sardar M. N. Islam,Fevi Novkaniza,Fiftitah R. Aszhari,Muhammad Rifqi*

Main category: cs.LG

TL;DR: Machine learning models (LR, RF, SVM) outperform statistical methods in predicting bank bankruptcy, achieving 90% accuracy with RF on commercial bank data and accurate predictions for rural banks.


<details>
  <summary>Details</summary>
Motivation: Traditional statistical methods like Altman's Z-Score have rigid assumptions leading to low forecast accuracy, necessitating more accurate machine learning approaches for financial system stability.

Method: Used logistic regression, random forest, and support vector machines on commercial bank data from Turkey (44 active, 21 bankrupt) and rural bank data from Indonesia (43 active, 43 bankrupt) with financial statement analysis.

Result: Random forest achieved 90% accuracy in predicting commercial bank bankruptcy, and all three ML methods accurately predicted rural bank bankruptcy likelihood.

Conclusion: Machine learning approaches provide superior bankruptcy prediction capabilities that can help implement policies to reduce bankruptcy costs and maintain financial system stability.

Abstract: Context: Financial system stability is determined by the condition of the
banking system. A bank failure can destroy the stability of the financial
system, as banks are subject to systemic risk, affecting not only individual
banks but also segments or the entire financial system. Calculating the
probability of a bank going bankrupt is one way to ensure the banking system is
safe and sound. Existing literature and limitations: Statistical models, such
as Altman's Z-Score, are one of the common techniques for developing a
bankruptcy prediction model. However, statistical methods rely on rigid and
sometimes irrelevant assumptions, which can result in low forecast accuracy.
New approaches are necessary. Objective of the research: Bankruptcy models are
developed using machine learning techniques, such as logistic regression (LR),
random forest (RF), and support vector machines (SVM). According to several
studies, machine learning is also more accurate and effective than statistical
methods for categorising and forecasting banking risk management. Present
Research: The commercial bank data are derived from the annual financial
statements of 44 active banks and 21 bankrupt banks in Turkey from 1994 to
2004, and the rural bank data are derived from the quarterly financial reports
of 43 active and 43 bankrupt rural banks in Indonesia between 2013 and 2019.
Five rural banks in Indonesia have also been selected to demonstrate the
feasibility of analysing bank bankruptcy trends. Findings and implications: The
results of the research experiments show that RF can forecast data from
commercial banks with a 90% accuracy rate. Furthermore, the three machine
learning methods proposed accurately predict the likelihood of rural bank
bankruptcy. Contribution and Conclusion: The proposed innovative machine
learning approach help to implement policies that reduce the costs of
bankruptcy.

</details>


### [189] [Towards Generalization of Graph Neural Networks for AC Optimal Power Flow](https://arxiv.org/abs/2510.06860)
*Olayiwola Arowolo,Jochen L. Cremer*

Main category: cs.LG

TL;DR: HH-MPNN achieves 1,000-10,000x speedup for ACOPF with <3% optimality gap across grid sizes and topologies without retraining.


<details>
  <summary>Details</summary>
Motivation: ACOPF is computationally expensive for large power systems, and existing ML methods lack scalability and topology adaptability without costly retraining.

Method: Hybrid Heterogeneous Message Passing Neural Network (HH-MPNN) models different grid components as distinct node/edge types with transformer for long-range dependencies.

Result: Achieves <1% optimality gap on default topologies (14-2,000 buses) and <3% gap on unseen topologies zero-shot; computational speedups of 1,000x-10,000x vs interior point solvers.

Conclusion: HH-MPNN enables practical, generalizable ML for real-time power system operations with scalability and topology adaptability.

Abstract: AC Optimal Power Flow (ACOPF) is computationally expensive for large-scale
power systems, with conventional solvers requiring prohibitive solution times.
Machine learning approaches offer computational speedups but struggle with
scalability and topology adaptability without expensive retraining. To enable
scalability across grid sizes and adaptability to topology changes, we propose
a Hybrid Heterogeneous Message Passing Neural Network (HH-MPNN). HH-MPNN models
buses, generators, loads, shunts, transmission lines and transformers as
distinct node or edge types, combined with a scalable transformer model for
handling long-range dependencies. On grids from 14 to 2,000 buses, HH-MPNN
achieves less than 1% optimality gap on default topologies. Applied zero-shot
to thousands of unseen topologies, HH-MPNN achieves less than 3% optimality gap
despite training only on default topologies. Pre-training on smaller grids also
improves results on a larger grid. Computational speedups reach 1,000x to
10,000x compared to interior point solvers. These results advance practical,
generalizable machine learning for real-time power system operations.

</details>


### [190] [SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models](https://arxiv.org/abs/2510.06871)
*Huahui Yi,Kun Wang,Qiankun Li,Miao Yu,Liang Lin,Gongli Xi,Hao Wu,Xuming Hu,Kang Li,Yang Liu*

Main category: cs.LG

TL;DR: SaFeR-VLM is a safety-aligned reinforcement learning framework that embeds safety directly into multimodal reasoning, addressing the "Reasoning Tax" where MLRMs amplify safety risks. It achieves superior safety performance without compromising helpfulness.


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal reasoning but often amplify safety risks under adversarial or unsafe prompts (Reasoning Tax). Existing defenses mainly act at output level and don't constrain reasoning process, leaving models exposed to implicit risks.

Method: Proposes SaFeR-VLM framework with four components: (I) QI-Safe-10K dataset for safety-critical cases; (II) safety-aware rollout with reflection and correction; (III) structured reward modeling with multi-dimensional criteria and penalties; (IV) GRPO optimization reinforcing safe and corrected trajectories.

Result: SaFeR-VLM-3B achieves average performance 70.13 and 78.97 on safety and helpfulness across six benchmarks, surpassing same-scale and >10× larger models. SaFeR-VLM-7B surpasses GPT-5-mini and Gemini-2.5-Flash by 6.47 and 16.76 points respectively on safety metrics without degradation in helpfulness.

Conclusion: SaFeR-VLM shifts safety from passive safeguard to active driver of reasoning, enabling scalable and generalizable safety-aware reasoning with robustness against both explicit and implicit risks, supporting dynamic and interpretable safety decisions.

Abstract: Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal
reasoning but often amplify safety risks under adversarial or unsafe prompts, a
phenomenon we call the \textit{Reasoning Tax}. Existing defenses mainly act at
the output level and do not constrain the reasoning process, leaving models
exposed to implicit risks. In this paper, we propose SaFeR-VLM, a
safety-aligned reinforcement learning framework that embeds safety directly
into multimodal reasoning. The framework integrates four components: (I)
QI-Safe-10K, a curated dataset emphasizing safety-critical and
reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations
undergo reflection and correction instead of being discarded; (III) structured
reward modeling with multi-dimensional weighted criteria and explicit penalties
for hallucinations and contradictions; and (IV) GRPO optimization, which
reinforces both safe and corrected trajectories. This unified design shifts
safety from a passive safeguard to an active driver of reasoning, enabling
scalable and generalizable safety-aware reasoning. SaFeR-VLM further
demonstrates robustness against both explicit and implicit risks, supporting
dynamic and interpretable safety decisions beyond surface-level filtering.
SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and
helpfulness across six benchmarks, surpassing both same-scale and $>10\times$
larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B.
Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass
GPT-5-mini and Gemini-2.5-Flash by \num{6.47} and \num{16.76} points
respectively on safety metrics, achieving this improvement without any
degradation in helpfulness performance. Our codes are available at
https://github.com/HarveyYi/SaFeR-VLM.

</details>


### [191] [MoRE-GNN: Multi-omics Data Integration with a Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2510.06880)
*Zhiyu Wang,Sonia Koszut,Pietro Liò,Francesco Ceccarelli*

Main category: cs.LG

TL;DR: MoRE-GNN is a heterogeneous graph autoencoder that uses graph convolution and attention to dynamically build relational graphs from multi-omics single-cell data, outperforming existing methods in capturing biological relationships.


<details>
  <summary>Details</summary>
Motivation: Multi-omics single-cell data integration is challenging due to high dimensionality and complex inter-modality relationships, requiring better methods to capture meaningful biological connections.

Method: MoRE-GNN combines graph convolution and attention mechanisms in a heterogeneous graph autoencoder to dynamically construct relational graphs directly from multi-omics data.

Result: Evaluations on six datasets show MoRE-GNN captures biologically meaningful relationships and outperforms existing methods, especially with strong inter-modality correlations, enabling accurate cross-modal predictions.

Conclusion: MoRE-GNN provides an adaptive, scalable and interpretable framework for multi-omics integration, though performance may vary with dataset complexity.

Abstract: The integration of multi-omics single-cell data remains challenging due to
high-dimensionality and complex inter-modality relationships. To address this,
we introduce MoRE-GNN (Multi-omics Relational Edge Graph Neural Network), a
heterogeneous graph autoencoder that combines graph convolution and attention
mechanisms to dynamically construct relational graphs directly from data.
Evaluations on six publicly available datasets demonstrate that MoRE-GNN
captures biologically meaningful relationships and outperforms existing
methods, particularly in settings with strong inter-modality correlations.
Furthermore, the learned representations allow for accurate downstream
cross-modal predictions. While performance may vary with dataset complexity,
MoRE-GNN offers an adaptive, scalable and interpretable framework for advancing
multi-omics integration.

</details>


### [192] [Angular Constraint Embedding via SpherePair Loss for Constrained Clustering](https://arxiv.org/abs/2510.06907)
*Shaojie Zhang,Ke Chen*

Main category: cs.LG

TL;DR: SpherePair is a novel deep constrained clustering method that uses angular constraint embedding to effectively separate representation learning from clustering, providing superior performance and scalability without requiring exact cluster numbers.


<details>
  <summary>Details</summary>
Motivation: Existing deep constrained clustering methods are limited by anchors in end-to-end modeling or struggle with learning discriminative Euclidean embedding, restricting their scalability and real-world applicability.

Method: Proposes SpherePair loss with geometric formulation for angular constraint embedding, which faithfully encodes pairwise constraints and creates clustering-friendly embeddings in angular space while separating representation learning from clustering.

Result: SpherePair preserves pairwise relations without conflict, removes the need to specify exact cluster numbers, generalizes to unseen data, enables rapid inference of cluster numbers, and shows superior performance and scalability in comparative evaluations.

Conclusion: SpherePair provides a theoretically-grounded approach for deep constrained clustering that overcomes limitations of existing methods and demonstrates superior real-world effectiveness.

Abstract: Constrained clustering integrates domain knowledge through pairwise
constraints. However, existing deep constrained clustering (DCC) methods are
either limited by anchors inherent in end-to-end modeling or struggle with
learning discriminative Euclidean embedding, restricting their scalability and
real-world applicability. To avoid their respective pitfalls, we propose a
novel angular constraint embedding approach for DCC, termed SpherePair. Using
the SpherePair loss with a geometric formulation, our method faithfully encodes
pairwise constraints and leads to embeddings that are clustering-friendly in
angular space, effectively separating representation learning from clustering.
SpherePair preserves pairwise relations without conflict, removes the need to
specify the exact number of clusters, generalizes to unseen data, enables rapid
inference of the number of clusters, and is supported by rigorous theoretical
guarantees. Comparative evaluations with state-of-the-art DCC methods on
diverse benchmarks, along with empirical validation of theoretical insights,
confirm its superior performance, scalability, and overall real-world
effectiveness. Code is available at
\href{https://github.com/spherepaircc/SpherePairCC/tree/main}{our repository}.

</details>


### [193] [Vacuum Spiker: A Spiking Neural Network-Based Model for Efficient Anomaly Detection in Time Series](https://arxiv.org/abs/2510.06910)
*Iago Xabier Vázquez,Javier Sedano,Muhammad Afzal,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: The paper proposes Vacuum Spiker algorithm, a novel Spiking Neural Network-based method for energy-efficient anomaly detection in time series, achieving competitive performance with significantly reduced energy consumption compared to deep learning baselines.


<details>
  <summary>Details</summary>
Motivation: Address the high energy consumption of deep learning models in anomaly detection, which limits deployment in resource-constrained environments like IoT devices, edge computing, and wearables.

Method: Vacuum Spiker algorithm uses Spiking Neural Networks with a new detection criterion based on global changes in neural activity, trained with Spike Time-Dependent Plasticity, and employs an efficient encoding scheme that discretizes input space into non-overlapping intervals with single spike per time step.

Result: Experimental results show competitive performance with significant energy reduction compared to deep learning and machine learning baselines. Successfully validated in real-world case study identifying power curtailment events in solar inverters.

Conclusion: The proposed algorithm demonstrates potential for sustainable and efficient anomaly detection, particularly suitable for resource-constrained environments.

Abstract: Anomaly detection is a key task across domains such as industry, healthcare,
and cybersecurity. Many real-world anomaly detection problems involve analyzing
multiple features over time, making time series analysis a natural approach for
such problems. While deep learning models have achieved strong performance in
this field, their trend to exhibit high energy consumption limits their
deployment in resource-constrained environments such as IoT devices, edge
computing platforms, and wearables. To address this challenge, this paper
introduces the \textit{Vacuum Spiker algorithm}, a novel Spiking Neural
Network-based method for anomaly detection in time series. It incorporates a
new detection criterion that relies on global changes in neural activity rather
than reconstruction or prediction error. It is trained using Spike
Time-Dependent Plasticity in a novel way, intended to induce changes in neural
activity when anomalies occur. A new efficient encoding scheme is also
proposed, which discretizes the input space into non-overlapping intervals,
assigning each to a single neuron. This strategy encodes information with a
single spike per time step, improving energy efficiency compared to
conventional encoding methods. Experimental results on publicly available
datasets show that the proposed algorithm achieves competitive performance
while significantly reducing energy consumption, compared to a wide set of deep
learning and machine learning baselines. Furthermore, its practical utility is
validated in a real-world case study, where the model successfully identifies
power curtailment events in a solar inverter. These results highlight its
potential for sustainable and efficient anomaly detection.

</details>


### [194] [Utilizing Large Language Models for Machine Learning Explainability](https://arxiv.org/abs/2510.06912)
*Alexandros Vassiliades,Nikolaos Polatidis,Stamatios Samaras,Sotiris Diplaris,Ignacio Cabrera Martin,Yannis Manolopoulos,Stefanos Vrochidis,Ioannis Kompatsiaris*

Main category: cs.LG

TL;DR: LLMs can autonomously generate effective and interpretable machine learning pipelines for classification tasks, achieving performance comparable to manually engineered baselines.


<details>
  <summary>Details</summary>
Motivation: To explore the explainability capabilities of LLMs when used to autonomously generate ML solutions and evaluate their potential as automated tools for interpretable ML pipeline generation.

Method: Used three state-of-the-art LLMs (OpenAI GPT, Anthropic Claude, DeepSeek) to design training pipelines for four classifiers (Random Forest, XGBoost, MLP, LSTM) on two classification tasks: binary driver alertness prediction and multilabel yeast dataset classification.

Result: LLMs produced effective models with high predictive performance (recall, precision, F1-score) and good explainability metrics (high SHAP fidelity and consistent sparsity), closely matching manually engineered baselines.

Conclusion: LLMs demonstrate strong potential as automated tools for generating interpretable ML pipelines, capable of producing both effective and explainable models.

Abstract: This study explores the explainability capabilities of large language models
(LLMs), when employed to autonomously generate machine learning (ML) solutions.
We examine two classification tasks: (i) a binary classification problem
focused on predicting driver alertness states, and (ii) a multilabel
classification problem based on the yeast dataset. Three state-of-the-art LLMs
(i.e. OpenAI GPT, Anthropic Claude, and DeepSeek) are prompted to design
training pipelines for four common classifiers: Random Forest, XGBoost,
Multilayer Perceptron, and Long Short-Term Memory networks. The generated
models are evaluated in terms of predictive performance (recall, precision, and
F1-score) and explainability using SHAP (SHapley Additive exPlanations).
Specifically, we measure Average SHAP Fidelity (Mean Squared Error between SHAP
approximations and model outputs) and Average SHAP Sparsity (number of features
deemed influential). The results reveal that LLMs are capable of producing
effective and interpretable models, achieving high fidelity and consistent
sparsity, highlighting their potential as automated tools for interpretable ML
pipeline generation. The results show that LLMs can produce effective,
interpretable pipelines with high fidelity and consistent sparsity, closely
matching manually engineered baselines.

</details>


### [195] [DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning](https://arxiv.org/abs/2510.06913)
*Ke Guo,Haochen Liu,Xiaojun Wu,Chen Lv*

Main category: cs.LG

TL;DR: Proposed DecompGAIL to address instability in multi-agent GAIL by decomposing realism into ego-map and ego-neighbor components, filtering irrelevant interactions, and adding social PPO with neighborhood rewards.


<details>
  <summary>Details</summary>
Motivation: Existing imitation learning approaches fail to model realistic traffic behaviors - behavior cloning suffers from covariate shift, while GAIL is unstable in multi-agent settings due to irrelevant interaction misguidance.

Method: Decomposed Multi-agent GAIL (DecompGAIL) that explicitly decomposes realism into ego-map and ego-neighbor components, filters out misleading neighbor-neighbor and neighbor-map interactions, and introduces social PPO objective with distance-weighted neighborhood rewards.

Result: Achieves state-of-the-art performance on the WOMD Sim Agents 2025 benchmark when integrated into a lightweight SMART-based backbone.

Conclusion: DecompGAIL effectively addresses the instability issues in multi-agent GAIL by decomposing realism components and incorporating social rewards, enabling more realistic traffic simulation.

Abstract: Realistic traffic simulation is critical for the development of autonomous
driving systems and urban mobility planning, yet existing imitation learning
approaches often fail to model realistic traffic behaviors. Behavior cloning
suffers from covariate shift, while Generative Adversarial Imitation Learning
(GAIL) is notoriously unstable in multi-agent settings. We identify a key
source of this instability: irrelevant interaction misguidance, where a
discriminator penalizes an ego vehicle's realistic behavior due to unrealistic
interactions among its neighbors. To address this, we propose Decomposed
Multi-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map
and ego-neighbor components, filtering out misleading neighbor: neighbor and
neighbor: map interactions. We further introduce a social PPO objective that
augments ego rewards with distance-weighted neighborhood rewards, encouraging
overall realism across agents. Integrated into a lightweight SMART-based
backbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim
Agents 2025 benchmark.

</details>


### [196] [Revisiting Node Affinity Prediction in Temporal Graphs](https://arxiv.org/abs/2510.06940)
*Krishna Sri Ipsit Mantri,Or Feldman,Moshe Eliasof,Chaim Baskin*

Main category: cs.LG

TL;DR: NAViS is a node affinity prediction model that addresses limitations of current temporal graph neural networks by using virtual states and a novel loss function, outperforming state-of-the-art methods and simple heuristics.


<details>
  <summary>Details</summary>
Motivation: Current temporal graph neural networks underperform simple heuristics like Persistent Forecast or Moving Average for node affinity prediction, indicating fundamental training challenges that need to be addressed.

Method: Developed NAViS by exploiting equivalence between heuristics and state space models, using virtual states and introducing a novel loss function specifically designed for node affinity prediction.

Result: NAViS outperforms state-of-the-art methods including heuristics on TGB benchmarks, demonstrating superior performance in node affinity prediction tasks.

Conclusion: The proposed NAViS model successfully addresses training challenges in temporal graph neural networks for node affinity prediction and establishes new state-of-the-art performance.

Abstract: Node affinity prediction is a common task that is widely used in temporal
graph learning with applications in social and financial networks, recommender
systems, and more. Recent works have addressed this task by adapting
state-of-the-art dynamic link property prediction models to node affinity
prediction. However, simple heuristics, such as Persistent Forecast or Moving
Average, outperform these models. In this work, we analyze the challenges in
training current Temporal Graph Neural Networks for node affinity prediction
and suggest appropriate solutions. Combining the solutions, we develop NAViS -
Node Affinity prediction model using Virtual State, by exploiting the
equivalence between heuristics and state space models. While promising,
training NAViS is non-trivial. Therefore, we further introduce a novel loss
function for node affinity prediction. We evaluate NAViS on TGB and show that
it outperforms the state-of-the-art, including heuristics. Our source code is
available at https://github.com/orfeld415/NAVIS

</details>


### [197] [Fisher Information, Training and Bias in Fourier Regression Models](https://arxiv.org/abs/2510.06945)
*Lorenzo Pastori,Veronika Eyring,Mierk Schwabe*

Main category: cs.LG

TL;DR: The paper studies how Fisher information matrix (FIM) metrics predict quantum neural network (QNN) performance, showing that higher effective dimension benefits unbiased models while lower effective dimension helps biased models.


<details>
  <summary>Details</summary>
Motivation: Growing interest in quantum machine learning and QNNs motivates studying FIM-based evaluation metrics for predicting training and prediction performance.

Method: Exploit equivalence between QNNs and Fourier models, derive analytical FIM expression, construct models with tunable effective dimension and bias, and introduce tensor network representation.

Result: For unbiased models, higher effective dimension improves trainability and performance; for biased models, lower effective dimension is beneficial during training.

Conclusion: Findings demonstrate interplay between geometrical properties, model-task alignment and training, relevant for broader machine learning community.

Abstract: Motivated by the growing interest in quantum machine learning, in particular
quantum neural networks (QNNs), we study how recently introduced evaluation
metrics based on the Fisher information matrix (FIM) are effective for
predicting their training and prediction performance. We exploit the
equivalence between a broad class of QNNs and Fourier models, and study the
interplay between the \emph{effective dimension} and the \emph{bias} of a model
towards a given task, investigating how these affect the model's training and
performance. We show that for a model that is completely agnostic, or unbiased,
towards the function to be learned, a higher effective dimension likely results
in a better trainability and performance. On the other hand, for models that
are biased towards the function to be learned a lower effective dimension is
likely beneficial during training. To obtain these results, we derive an
analytical expression of the FIM for Fourier models and identify the features
controlling a model's effective dimension. This allows us to construct models
with tunable effective dimension and bias, and to compare their training. We
furthermore introduce a tensor network representation of the considered Fourier
models, which could be a tool of independent interest for the analysis of QNN
models. Overall, these findings provide an explicit example of the interplay
between geometrical properties, model-task alignment and training, which are
relevant for the broader machine learning community.

</details>


### [198] [Grouped Differential Attention](https://arxiv.org/abs/2510.06949)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Wai Ting Cheung,Beomgyu Kim,Taehwan Kim,Haesol Lee,Junhyeok Lee,Dongpin Oh,Eunhwan Park*

Main category: cs.LG

TL;DR: Grouped Differential Attention (GDA) improves Transformer efficiency by using unbalanced head allocation between signal-preserving and noise-control groups, achieving better signal focus with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Self-attention mechanisms in Transformers often waste resources on redundant or noisy context. Previous Differential Attention approach had rigid constraints due to balanced head allocation.

Method: Proposed GDA with unbalanced head allocation: more heads for signal extraction, fewer for noise-control with controlled repetition. Extended to group-differentiated growth for scalable capacity expansion.

Result: Moderate imbalance ratios in GDA yield substantial improvements in generalization and stability compared to symmetric baselines in large-scale pretraining experiments.

Conclusion: Ratio-aware head allocation and selective expansion provide an effective path for designing scalable, computation-efficient Transformer architectures.

Abstract: The self-attention mechanism, while foundational to modern Transformer
architectures, suffers from a critical inefficiency: it frequently allocates
substantial attention to redundant or noisy context. Differential Attention
addressed this by using subtractive attention maps for signal and noise, but
its required balanced head allocation imposes rigid constraints on
representational flexibility and scalability.
  To overcome this, we propose Grouped Differential Attention (GDA), a novel
approach that introduces unbalanced head allocation between signal-preserving
and noise-control groups. GDA significantly enhances signal focus by
strategically assigning more heads to signal extraction and fewer to
noise-control, stabilizing the latter through controlled repetition (akin to
GQA). This design achieves stronger signal fidelity with minimal computational
overhead. We further extend this principle to group-differentiated growth, a
scalable strategy that selectively replicates only the signal-focused heads,
thereby ensuring efficient capacity expansion.
  Through large-scale pretraining and continual training experiments, we
demonstrate that moderate imbalance ratios in GDA yield substantial
improvements in generalization and stability compared to symmetric baselines.
Our results collectively establish that ratio-aware head allocation and
selective expansion offer an effective and practical path toward designing
scalable, computation-efficient Transformer architectures.

</details>


### [199] [From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics](https://arxiv.org/abs/2510.06954)
*Zheng-An Chen,Tao Luo*

Main category: cs.LG

TL;DR: The paper analyzes transformer training dynamics using gradient flow framework, revealing a two-stage process: asymmetric weight perturbations enable escape from small initialization, followed by key-query matrix condensation and rank collapse.


<details>
  <summary>Details</summary>
Motivation: To understand fundamental principles of transformer training dynamics beyond configuration-specific studies, inspired by improved reasoning capabilities under small initialization scales in language models.

Method: Employ gradient flow analytical framework from previous work to systematically investigate linearized Transformer training dynamics through theoretical analysis.

Result: Identified two distinct stages: 1) asymmetric weight perturbations sustain gradient dynamics for escaping small initialization, 2) key-query matrices become active and drive normalized matrices toward asymptotic rank collapse.

Conclusion: The two-stage framework generalizes classical directional convergence results and provides systematic understanding of transformer training dynamics.

Abstract: Although transformer-based models have shown exceptional empirical
performance, the fundamental principles governing their training dynamics are
inadequately characterized beyond configuration-specific studies. Inspired by
empirical evidence showing improved reasoning capabilities under small
initialization scales in language models, we employ the gradient flow
analytical framework established in [Zhou et al. NeurIPS 2022] to
systematically investigate linearized Transformer training dynamics. Our
theoretical analysis dissects the dynamics of attention modules into two
distinct stages. In the first stage, asymmetric weight perturbations from
random initialization sustain non-degenerate gradient dynamics in parameter
matrices, facilitating systematic escape from small initialization regimes.
Subsequently, these matrices undergo condensation, progressively aligning
toward the target orientation. In the second stage, the previously static
key-query matrices actively participate in training, driving the normalized
matrices toward asymptotic rank collapse. This two-stage framework generalizes
classical directional convergence results.

</details>


### [200] [High-Rate Mixout: Revisiting Mixout for Robust Domain Generalization](https://arxiv.org/abs/2510.06955)
*Masih Aminbeidokhti,Heitor Rapela Medeiros,Eric Granger,Marco Pedersoli*

Main category: cs.LG

TL;DR: Mixout is a lightweight alternative to model ensembling for domain generalization that uses high masking probabilities (0.9 for ViTs, 0.8 for ResNets) to swap fine-tuned weights with pre-trained counterparts, achieving comparable performance to ensembles with 45% less gradient computation and 90% less memory usage.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of model ensembling for domain generalization while avoiding the over-regularization issues of Dropout when applied to pre-trained models.

Method: Mixout regularization that probabilistically swaps fine-tuned weights with pre-trained weights during training, using high masking rates to maintain balance between adaptation and prior knowledge retention.

Result: Achieves comparable out-of-domain accuracy to ensemble methods across five benchmarks (PACS, VLCS, OfficeHome, TerraIncognita, DomainNet) with ResNet and ViT architectures, while reducing gradient computation by 45% and gradient memory usage by 90%.

Conclusion: High-rate Mixout provides an efficient and effective alternative to model ensembling for domain generalization, balancing performance with computational efficiency.

Abstract: Ensembling fine-tuned models initialized from powerful pre-trained weights is
a common strategy to improve robustness under distribution shifts, but it comes
with substantial computational costs due to the need to train and store
multiple models. Dropout offers a lightweight alternative by simulating
ensembles through random neuron deactivation; however, when applied to
pre-trained models, it tends to over-regularize and disrupt critical
representations necessary for generalization. In this work, we investigate
Mixout, a stochastic regularization technique that provides an alternative to
Dropout for domain generalization. Rather than deactivating neurons, Mixout
mitigates overfitting by probabilistically swapping a subset of fine-tuned
weights with their pre-trained counterparts during training, thereby
maintaining a balance between adaptation and retention of prior knowledge. Our
study reveals that achieving strong performance with Mixout on domain
generalization benchmarks requires a notably high masking probability of 0.9
for ViTs and 0.8 for ResNets. While this may seem like a simple adjustment, it
yields two key advantages for domain generalization: (1) higher masking rates
more strongly penalize deviations from the pre-trained parameters, promoting
better generalization to unseen domains; and (2) high-rate masking
substantially reduces computational overhead, cutting gradient computation by
up to 45% and gradient memory usage by up to 90%. Experiments across five
domain generalization benchmarks, PACS, VLCS, OfficeHome, TerraIncognita, and
DomainNet, using ResNet and ViT architectures, show that our approach,
High-rate Mixout, achieves out-of-domain accuracy comparable to ensemble-based
methods while significantly reducing training costs.

</details>


### [201] [Revisiting Mixout: An Overlooked Path to Robust Finetuning](https://arxiv.org/abs/2510.06982)
*Masih Aminbeidokhti,Heitor Rapela Medeiros,Eric Granger,Marco Pedersoli*

Main category: cs.LG

TL;DR: GMixout improves finetuning robustness by using adaptive weight averaging and controlled masking frequency, achieving better accuracy and distribution shift performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Finetuning vision models improves in-domain accuracy but reduces robustness under distribution shift. The paper aims to enhance robustness while maintaining accuracy.

Method: Introduces GMixout with adaptive exponential moving-average anchor and regulated masking frequency, implemented via sparse-kernel updates for efficiency.

Result: GMixout consistently improves in-domain accuracy beyond zero-shot performance and outperforms Model Soups and parameter-efficient baselines under distribution shift across multiple benchmarks.

Conclusion: GMixout effectively balances in-domain accuracy and robustness through adaptive weight regularization, providing a practical solution for finetuning vision foundation models.

Abstract: Finetuning vision foundation models often improves in-domain accuracy but
comes at the cost of robustness under distribution shift. We revisit Mixout, a
stochastic regularizer that intermittently replaces finetuned weights with
their pretrained reference, through the lens of a single-run, weight-sharing
implicit ensemble. This perspective reveals three key levers that govern
robustness: the \emph{masking anchor}, \emph{resampling frequency}, and
\emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i)
replaces the fixed anchor with an exponential moving-average snapshot that
adapts during training, and (ii) regulates masking period via an explicit
resampling-frequency hyperparameter. Our sparse-kernel implementation updates
only a small fraction of parameters with no inference-time overhead, enabling
training on consumer-grade GPUs. Experiments on benchmarks covering covariate
shift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet,
iWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy
beyond zero-shot performance while surpassing both Model Soups and strong
parameter-efficient finetuning baselines under distribution shift.

</details>


### [202] [Spiral Model Technique For Data Science & Machine Learning Lifecycle](https://arxiv.org/abs/2510.06987)
*Rohith Mahadevan*

Main category: cs.LG

TL;DR: This paper introduces a spiral technique for data science lifecycles in business contexts with clear end goals, emphasizing versatility, agility and iterative approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional data science lifecycles are often depicted as linear or cyclical models that can restart after completion, but businesses need more focused approaches for problems with clear end goals.

Method: The paper proposes a new spiral technique that incorporates data science lifecycle principles into business processes, focusing on iterative development and agile methodologies.

Result: The spiral technique provides a more versatile and agile approach to data science lifecycles in business contexts, allowing for iterative refinement while maintaining focus on clear end goals.

Conclusion: The spiral technique offers an improved framework for applying data science lifecycles to business problems with defined objectives, enhancing productivity and competitiveness through its iterative and agile nature.

Abstract: Analytics play an important role in modern business. Companies adapt data
science lifecycles to their culture to seek productivity and improve their
competitiveness among others. Data science lifecycles are fairly an important
contributing factor to start and end a project that are data dependent. Data
science and Machine learning life cycles comprises of series of steps that are
involved in a project. A typical life cycle states that it is a linear or
cyclical model that revolves around. It is mostly depicted that it is possible
in a traditional data science life cycle to start the process again after
reaching the end of cycle. This paper suggests a new technique to incorporate
data science life cycle to business problems that have a clear end goal. A new
technique called spiral technique is introduced to emphasize versatility,
agility and iterative approach to business processes.

</details>


### [203] [Sharpness-Aware Data Generation for Zero-shot Quantization](https://arxiv.org/abs/2510.07018)
*Dung Hoang-Anh,Cuong Pham Trung Le,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: Zero-shot quantization method that considers model sharpness during synthetic data generation to improve generalization, using gradient matching between synthetic and real data gradients.


<details>
  <summary>Details</summary>
Motivation: Previous zero-shot quantization approaches don't consider quantized model sharpness as a criterion for synthetic data generation, even though low sharpness is known to improve generalization.

Method: Proposes sharpness minimization through gradient matching between reconstruction loss gradients on synthetic and real validation data, approximated by matching gradients between generated samples and their neighbors when real data is unavailable.

Result: Experimental evaluations on CIFAR-100 and ImageNet show superior performance over state-of-the-art techniques in low-bit quantization settings.

Conclusion: Considering quantized model sharpness in synthetic data generation enhances generalization in zero-shot quantization.

Abstract: Zero-shot quantization aims to learn a quantized model from a pre-trained
full-precision model with no access to original real training data. The common
idea in zero-shot quantization approaches is to generate synthetic data for
quantizing the full-precision model. While it is well-known that deep neural
networks with low sharpness have better generalization ability, none of the
previous zero-shot quantization works considers the sharpness of the quantized
model as a criterion for generating training data. This paper introduces a
novel methodology that takes into account quantized model sharpness in
synthetic data generation to enhance generalization. Specifically, we first
demonstrate that sharpness minimization can be attained by maximizing gradient
matching between the reconstruction loss gradients computed on synthetic and
real validation data, under certain assumptions. We then circumvent the problem
of the gradient matching without real validation set by approximating it with
the gradient matching between each generated sample and its neighbors.
Experimental evaluations on CIFAR-100 and ImageNet datasets demonstrate the
superiority of the proposed method over the state-of-the-art techniques in
low-bit quantization settings.

</details>


### [204] [Federated Unlearning in the Wild: Rethinking Fairness and Data Discrepancy](https://arxiv.org/abs/2510.07022)
*ZiHeng Huang,Di Wu,Jun Bai,Jiale Zhang,Sicong Cao,Ji Zhang,Yingjie Hu*

Main category: cs.LG

TL;DR: This paper addresses fairness and realistic evaluation challenges in Federated Unlearning (FU), proposing FedCCCU as a fairness-aware solution that outperforms existing methods under realistic data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Federated Learning requires unlearning capabilities for data deletion rights enforcement, but current approaches overlook fairness issues and rely on unrealistic synthetic data assumptions that limit real-world applicability.

Method: Proposed Federated Cross-Client-Constrains Unlearning (FedCCCU), a fairness-aware approach that addresses both fairness and scalability challenges in FU through explicit constraints and realistic evaluation.

Result: Experimental results show existing FU methods perform poorly under realistic settings, while FedCCCU consistently outperforms them in both fairness and effectiveness metrics.

Conclusion: FedCCCU provides a practical and scalable solution for real-world Federated Unlearning that effectively addresses fairness concerns and works well under realistic data heterogeneity conditions.

Abstract: Machine unlearning is critical for enforcing data deletion rights like the
"right to be forgotten." As a decentralized paradigm, Federated Learning (FL)
also requires unlearning, but realistic implementations face two major
challenges. First, fairness in Federated Unlearning (FU) is often overlooked.
Exact unlearning methods typically force all clients into costly retraining,
even those uninvolved. Approximate approaches, using gradient ascent or
distillation, make coarse interventions that can unfairly degrade performance
for clients with only retained data. Second, most FU evaluations rely on
synthetic data assumptions (IID/non-IID) that ignore real-world heterogeneity.
These unrealistic benchmarks obscure the true impact of unlearning and limit
the applicability of current methods. We first conduct a comprehensive
benchmark of existing FU methods under realistic data heterogeneity and
fairness conditions. We then propose a novel, fairness-aware FU approach,
Federated Cross-Client-Constrains Unlearning (FedCCCU), to explicitly address
both challenges. FedCCCU offers a practical and scalable solution for
real-world FU. Experimental results show that existing methods perform poorly
in realistic settings, while our approach consistently outperforms them.

</details>


### [205] [Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration](https://arxiv.org/abs/2510.07035)
*Tengwei Song,Min Wu,Yuan Fang*

Main category: cs.LG

TL;DR: FlexMol is a flexible molecular pre-training framework that learns unified representations from 2D and 3D molecular data, supporting single-modality input when one modality is unavailable or expensive to generate.


<details>
  <summary>Details</summary>
Motivation: Existing methods require paired 2D and 3D molecular data for training, which limits applicability when certain modalities are unavailable or computationally expensive to generate.

Method: Uses separate models for 2D and 3D data with parameter sharing, employs a decoder to generate features for missing modalities, and implements multistage continuous learning where both modalities collaborate during training.

Result: Achieves superior performance across various molecular property prediction tasks and demonstrates effectiveness with incomplete data.

Conclusion: FlexMol provides a robust framework for molecular representation learning that works effectively even when only single modality is available during inference, overcoming limitations of existing paired-data approaches.

Abstract: Molecular representation learning plays a crucial role in advancing
applications such as drug discovery and material design. Existing work
leverages 2D and 3D modalities of molecular information for pre-training,
aiming to capture comprehensive structural and geometric insights. However,
these methods require paired 2D and 3D molecular data to train the model
effectively and prevent it from collapsing into a single modality, posing
limitations in scenarios where a certain modality is unavailable or
computationally expensive to generate. To overcome this limitation, we propose
FlexMol, a flexible molecule pre-training framework that learns unified
molecular representations while supporting single-modality input. Specifically,
inspired by the unified structure in vision-language models, our approach
employs separate models for 2D and 3D molecular data, leverages parameter
sharing to improve computational efficiency, and utilizes a decoder to generate
features for the missing modality. This enables a multistage continuous
learning process where both modalities contribute collaboratively during
training, while ensuring robustness when only one modality is available during
inference. Extensive experiments demonstrate that FlexMol achieves superior
performance across a wide range of molecular property prediction tasks, and we
also empirically demonstrate its effectiveness with incomplete data. Our code
and data are available at https://github.com/tewiSong/FlexMol.

</details>


### [206] [COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization](https://arxiv.org/abs/2510.07043)
*Tian Qin,Felix Bai,Ting-Yao Hu,Raviteja Vemulapalli,Hema Swetha Koppula,Zhiyang Xu,Bowen Jin,Mert Cemri,Jiarui Lu,Zirui Wang,Meng Cao*

Main category: cs.LG

TL;DR: COMPASS is a benchmark for evaluating LLM agents on realistic travel planning tasks, focusing on constrained preference optimization and multi-service coordination.


<details>
  <summary>Details</summary>
Motivation: Real-world LLM agents need to master strategic tool use and user preference optimization through multi-turn interactions for complex planning tasks like travel planning.

Method: Built a realistic travel database for 20 U.S. National Parks covering transportation, accommodation, and ticketing, with a comprehensive tool ecosystem mirroring commercial booking platforms.

Result: Identified two critical gaps: acceptable-optimal gap (agents meet constraints but fail to optimize preferences) and plan-coordination gap (performance collapses on multi-service coordination tasks, especially for open-source models).

Conclusion: COMPASS provides a benchmark that directly measures agents' ability to optimize user preferences in realistic tasks, bridging theoretical advances with real-world impact.

Abstract: Real-world large language model (LLM) agents must master strategic tool use
and user preference optimization through multi-turn interactions to assist
users with complex planning tasks. We introduce COMPASS (Constrained
Optimization through Multi-turn Planning and Strategic Solutions), a benchmark
that evaluates agents on realistic travel-planning scenarios. We cast travel
planning as a constrained preference optimization problem, where agents must
satisfy hard constraints while simultaneously optimizing soft user preferences.
To support this, we build a realistic travel database covering transportation,
accommodation, and ticketing for 20 U.S. National Parks, along with a
comprehensive tool ecosystem that mirrors commercial booking platforms.
Evaluating state-of-the-art models, we uncover two critical gaps: (i) an
acceptable-optimal gap, where agents reliably meet constraints but fail to
optimize preferences, and (ii) a plan-coordination gap, where performance
collapses on multi-service (flight and hotel) coordination tasks, especially
for open-source models. By grounding reasoning and planning in a practical,
user-facing domain, COMPASS provides a benchmark that directly measures an
agent's ability to optimize user preferences in realistic tasks, bridging
theoretical advances with real-world impact.

</details>


### [207] [Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models and Hyper-Parameter Optimisation](https://arxiv.org/abs/2510.07052)
*Aryan Golbaghi,Shuo Zhou*

Main category: cs.LG

TL;DR: A workflow combining pre-trained speech representations with automated hyperparameter optimization (HPO) for speech emotion recognition, achieving high accuracy on commodity CPUs.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient speech emotion recognition system that doesn't require expensive GPU resources and can achieve competitive performance through automated hyperparameter tuning.

Method: Used SpeechBrain wav2vec2-base model fine-tuned on IEMOCAP as encoder, compared Gaussian Process Bayesian Optimization (GP-BO) and Tree-structured Parzen Estimators (TPE) under 4D search space with 15-trial budget.

Result: GP-BO achieved 0.96 BCA in 11 minutes, TPE achieved 0.97 in 15 minutes, significantly outperforming grid search (143 trials, 1680 minutes) and AutoSpeech 2020 baseline (0.85 in 30 minutes on GPU). Cross-lingual generalization showed 0.25-0.26 accuracy improvements.

Conclusion: Efficient HPO with pre-trained encoders enables competitive speech emotion recognition performance on commodity CPUs, offering a practical alternative to GPU-based approaches.

Abstract: We propose a workflow for speech emotion recognition (SER) that combines
pre-trained representations with automated hyperparameter optimisation (HPO).
Using SpeechBrain wav2vec2-base model fine-tuned on IEMOCAP as the encoder, we
compare two HPO strategies, Gaussian Process Bayesian Optimisation (GP-BO) and
Tree-structured Parzen Estimators (TPE), under an identical four-dimensional
search space and 15-trial budget, with balanced class accuracy (BCA) on the
German EmoDB corpus as the objective. All experiments run on 8 CPU cores with
32 GB RAM. GP-BO achieves 0.96 BCA in 11 minutes, and TPE (Hyperopt
implementation) attains 0.97 in 15 minutes. In contrast, grid search requires
143 trials and 1,680 minutes to exceed 0.9 BCA, and the best AutoSpeech 2020
baseline reports only 0.85 in 30 minutes on GPU. For cross-lingual
generalisation, an EmoDB-trained HPO-tuned model improves zero-shot accuracy by
0.25 on CREMA-D and 0.26 on RAVDESS. Results show that efficient HPO with
pre-trained encoders delivers competitive SER on commodity CPUs. Source code to
this work is available at:
https://github.com/youngaryan/speechbrain-emotion-hpo.

</details>


### [208] [Introspection in Learned Semantic Scene Graph Localisation](https://arxiv.org/abs/2510.07053)
*Manshika Charvi Bissessur,Efimia Panagiotaki,Daniele De Martini*

Main category: cs.LG

TL;DR: This paper analyzes how semantics affect localization performance and robustness in self-supervised contrastive semantic localization. It examines whether models filter environmental noise and prioritize distinctive landmarks through interpretability methods.


<details>
  <summary>Details</summary>
Motivation: To understand how semantics influence localization performance and robustness in learned self-supervised frameworks, specifically investigating whether models filter noise and prioritize distinctive landmarks.

Method: Trained localization network on original and perturbed maps, conducted post-hoc introspection analysis using various interpretability methods (integrated gradients, attention weights), performed semantic class ablation study.

Result: Integrated gradients and attention weights were the most reliable interpretability methods. Semantic class ablation revealed implicit weighting where frequent objects are often down-weighted. Models learn noise-robust, semantically salient relations.

Conclusion: The model learns noise-robust, semantically salient relations for place definition, enabling explainable registration under challenging visual and structural variations.

Abstract: This work investigates how semantics influence localisation performance and
robustness in a learned self-supervised, contrastive semantic localisation
framework. After training a localisation network on both original and perturbed
maps, we conduct a thorough post-hoc introspection analysis to probe whether
the model filters environmental noise and prioritises distinctive landmarks
over routine clutter. We validate various interpretability methods and present
a comparative reliability analysis. Integrated gradients and Attention Weights
consistently emerge as the most reliable probes of learned behaviour. A
semantic class ablation further reveals an implicit weighting in which frequent
objects are often down-weighted. Overall, the results indicate that the model
learns noise-robust, semantically salient relations about place definition,
thereby enabling explainable registration under challenging visual and
structural variations.

</details>


### [209] [Blind Construction of Angular Power Maps in Massive MIMO Networks](https://arxiv.org/abs/2510.07071)
*Zheng Xing,Junting Chen*

Main category: cs.LG

TL;DR: This paper proposes an unsupervised method for constructing angular power maps in massive MIMO networks using hidden Markov models to estimate mobile trajectories from CSI data without location labels.


<details>
  <summary>Details</summary>
Motivation: Traditional radio map construction requires location-labeled CSI data, which is difficult to obtain in practice. The paper aims to solve this by developing an unsupervised approach that can work with unlabeled CSI data.

Method: Uses a hidden Markov model (HMM) to connect mobile trajectory with CSI evolution in massive MIMO channels, enabling location estimation and angular power map construction from timescale CSI data without location labels.

Result: Theoretical analysis shows localization error can vanish under certain conditions (uniform rectilinear mobility with Poisson-distributed BSs), but remains nonzero when BSs are confined. Real-world RSRP data testing achieved 18-meter average localization error using mainly single-cell measurements.

Conclusion: The proposed unsupervised HMM-based approach successfully constructs angular power maps from unlabeled CSI data, demonstrating practical feasibility with reasonable localization accuracy in real massive MIMO networks.

Abstract: Channel state information (CSI) acquisition is a challenging problem in
massive multiple-input multiple-output (MIMO) networks. Radio maps provide a
promising solution for radio resource management by reducing online CSI
acquisition. However, conventional approaches for radio map construction
require location-labeled CSI data, which is challenging in practice. This paper
investigates unsupervised angular power map construction based on large
timescale CSI data collected in a massive MIMO network without location labels.
A hidden Markov model (HMM) is built to connect the hidden trajectory of a
mobile with the CSI evolution of a massive MIMO channel. As a result, the
mobile location can be estimated, enabling the construction of an angular power
map. We show that under uniform rectilinear mobility with Poisson-distributed
base stations (BSs), the Cramer-Rao Lower Bound (CRLB) for localization error
can vanish at any signal-to-noise ratios (SNRs), whereas when BSs are confined
to a limited region, the error remains nonzero even with infinite independent
measurements. Based on reference signal received power (RSRP) data collected in
a real multi-cell massive MIMO network, an average localization error of 18
meters can be achieved although measurements are mainly obtained from a single
serving cell.

</details>


### [210] [HTMformer: Hybrid Time and Multivariate Transformer for Time Series Forecasting](https://arxiv.org/abs/2510.07084)
*Tan Wang,Yun Wei Dong,Tao Zhang,Qi Wang*

Main category: cs.LG

TL;DR: The paper proposes HTMformer, a lightweight Transformer-based time series forecasting model that uses Hybrid Temporal and Multivariate Embeddings (HTME) to address limitations in existing Transformers that overemphasize temporal dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing Transformers for time series forecasting tend to overemphasize temporal dependencies, incurring computational overhead without corresponding performance gains. The performance is highly dependent on embedding methods for effective representations.

Method: Introduces Hybrid Temporal and Multivariate Embeddings (HTME) that extract multivariate features to augment embedding representations. HTME integrates lightweight temporal feature extraction with carefully designed multivariate feature extraction to provide complementary features.

Result: Experiments on eight real-world datasets show that HTMformer outperforms existing baselines in both accuracy and efficiency.

Conclusion: HTMformer achieves better performance by leveraging enhanced feature extraction through HTME embeddings, balancing model complexity and performance while providing richer sequence representations.

Abstract: Transformer-based methods have achieved impressive results in time series
forecasting. However, existing Transformers still exhibit limitations in
sequence modeling as they tend to overemphasize temporal dependencies. This
incurs additional computational overhead without yielding corresponding
performance gains. We find that the performance of Transformers is highly
dependent on the embedding method used to learn effective representations. To
address this issue, we extract multivariate features to augment the effective
information captured in the embedding layer, yielding multidimensional
embeddings that convey richer and more meaningful sequence representations.
These representations enable Transformer-based forecasters to better understand
the series. Specifically, we introduce Hybrid Temporal and Multivariate
Embeddings (HTME). The HTME extractor integrates a lightweight temporal feature
extraction module with a carefully designed multivariate feature extraction
module to provide complementary features, thereby achieving a balance between
model complexity and performance. By combining HTME with the Transformer
architecture, we present HTMformer, leveraging the enhanced feature extraction
capability of the HTME extractor to build a lightweight forecaster. Experiments
conducted on eight real-world datasets demonstrate that our approach
outperforms existing baselines in both accuracy and efficiency.

</details>


### [211] [Non-Stationary Online Structured Prediction with Surrogate Losses](https://arxiv.org/abs/2510.07086)
*Shinsaku Sakaue,Han Bao,Yuzhou Cao*

Main category: cs.LG

TL;DR: This paper addresses online structured prediction in non-stationary environments by proving a novel bound on cumulative target loss that depends on surrogate loss and path length of comparator sequences, rather than time horizon T.


<details>
  <summary>Details</summary>
Motivation: Existing surrogate regret bounds for online prediction break down in non-stationary environments where fixed estimators suffer linear growth in surrogate loss with time T. This work aims to provide stronger guarantees that adapt to non-stationarity.

Method: The core approach synthesizes the dynamic regret bound of online gradient descent (OGD) with techniques for exploiting the surrogate gap. The paper also introduces a new Polyak-style learning rate for OGD and extends the method to broader problems using convolutional Fenchel-Young loss.

Result: The paper proves a bound of form F_T + C(1 + P_T) on cumulative target loss, where F_T is cumulative surrogate loss of comparator sequences, P_T is their path length, and C is constant. This provides much stronger guarantees in non-stationary environments.

Conclusion: The proposed approach successfully handles non-stationary online prediction by providing bounds that depend on comparator performance rather than time horizon. A lower bound confirms the tightness of the dependence on F_T and P_T.

Abstract: Online structured prediction, including online classification as a special
case, is the task of sequentially predicting labels from input features.
Therein the surrogate regret -- the cumulative excess of the target loss (e.g.,
0-1 loss) over the surrogate loss (e.g., logistic loss) of the fixed best
estimator -- has gained attention, particularly because it often admits a
finite bound independent of the time horizon $T$. However, such guarantees
break down in non-stationary environments, where every fixed estimator may
incur the surrogate loss growing linearly with $T$. We address this by proving
a bound of the form $F_T + C(1 + P_T)$ on the cumulative target loss, where
$F_T$ is the cumulative surrogate loss of any comparator sequence, $P_T$ is its
path length, and $C > 0$ is some constant. This bound depends on $T$ only
through $F_T$ and $P_T$, often yielding much stronger guarantees in
non-stationary environments. Our core idea is to synthesize the dynamic regret
bound of the online gradient descent (OGD) with the technique of exploiting the
surrogate gap. Our analysis also sheds light on a new Polyak-style learning
rate for OGD, which systematically offers target-loss guarantees and exhibits
promising empirical performance. We further extend our approach to a broader
class of problems via the convolutional Fenchel--Young loss. Finally, we prove
a lower bound showing that the dependence on $F_T$ and $P_T$ is tight.

</details>


### [212] [Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report](https://arxiv.org/abs/2510.07092)
*Riccardo Mereu,Aidan Scannell,Yuxin Hou,Yi Zhao,Aditya Jitta,Antonio Dominguez,Luigi Acerbi,Amos Storkey,Paul Chang*

Main category: cs.LG

TL;DR: The paper presents winning solutions for the 1X World Model Challenge, achieving 1st place in both sampling and compression tracks by adapting video generation models and training transformers for humanoid interaction tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of world modeling in real-world humanoid interaction, which requires forecasting future visual observations or compact latent states for AI and robotics applications.

Method: For sampling track: adapted Wan-2.2 TI2V-5B video generation model with robot state conditioning using AdaLN-Zero and LoRA post-training. For compression track: trained Spatio-Temporal Transformer from scratch.

Result: Achieved 23.0 dB PSNR in sampling task and Top-500 CE of 6.6386 in compression task, securing 1st place in both challenges.

Conclusion: The proposed approaches successfully demonstrate effective world modeling for real-world humanoid interaction, with adapted foundation models and custom transformers achieving state-of-the-art performance in both visual forecasting and latent state prediction tasks.

Abstract: World models are a powerful paradigm in AI and robotics, enabling agents to
reason about the future by predicting visual observations or compact latent
states. The 1X World Model Challenge introduces an open-source benchmark of
real-world humanoid interaction, with two complementary tracks: sampling,
focused on forecasting future image frames, and compression, focused on
predicting future discrete latent codes. For the sampling track, we adapt the
video generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned
future frame prediction. We condition the video generation on robot states
using AdaLN-Zero, and further post-train the model using LoRA. For the
compression track, we train a Spatio-Temporal Transformer model from scratch.
Our models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386
in the compression task, securing 1st place in both challenges.

</details>


### [213] [Non-Asymptotic Analysis of Efficiency in Conformalized Regression](https://arxiv.org/abs/2510.07093)
*Yunzhen Yao,Lie He,Michael Gastpar*

Main category: cs.LG

TL;DR: This paper establishes non-asymptotic bounds on prediction set length deviation for conformalized quantile and median regression, capturing joint dependence on training size, calibration size, and miscoverage level.


<details>
  <summary>Details</summary>
Motivation: Prior work treats miscoverage level as fixed constant, but this work aims to understand how efficiency depends jointly on dataset sizes and miscoverage level to guide data allocation.

Method: Analyze conformalized quantile and median regression trained via SGD, establish theoretical bounds under mild data distribution assumptions.

Result: Derived bounds of order O(1/√n + 1/(α²n) + 1/√m + exp(-α²m)) showing phase transitions in convergence rates across different α regimes.

Conclusion: Theoretical findings provide guidance for data allocation to control excess prediction set length, with empirical results supporting the analysis.

Abstract: Conformal prediction provides prediction sets with coverage guarantees. The
informativeness of conformal prediction depends on its efficiency, typically
quantified by the expected size of the prediction set. Prior work on the
efficiency of conformalized regression commonly treats the miscoverage level
$\alpha$ as a fixed constant. In this work, we establish non-asymptotic bounds
on the deviation of the prediction set length from the oracle interval length
for conformalized quantile and median regression trained via SGD, under mild
assumptions on the data distribution. Our bounds of order
$\mathcal{O}(1/\sqrt{n} + 1/(\alpha^2 n) + 1/\sqrt{m} + \exp(-\alpha^2 m))$
capture the joint dependence of efficiency on the proper training set size $n$,
the calibration set size $m$, and the miscoverage level $\alpha$. The results
identify phase transitions in convergence rates across different regimes of
$\alpha$, offering guidance for allocating data to control excess prediction
set length. Empirical results are consistent with our theoretical findings.

</details>


### [214] [DPMM-CFL: Clustered Federated Learning via Dirichlet Process Mixture Model Nonparametric Clustering](https://arxiv.org/abs/2510.07132)
*Mariona Jaramillo-Civill,Peng Wu,Pau Closas*

Main category: cs.LG

TL;DR: DPMM-CFL is a Clustered Federated Learning method that automatically determines the number of clusters using Dirichlet Process priors, eliminating the need to pre-specify cluster count.


<details>
  <summary>Details</summary>
Motivation: Existing CFL methods require fixing the number of clusters K beforehand, which is impractical when the latent cluster structure is unknown.

Method: Places Dirichlet Process prior over cluster parameters and uses nonparametric Bayesian inference to jointly infer cluster count and client assignments while optimizing federated objectives.

Result: Validated on benchmark datasets under Dirichlet and class-split non-IID partitions, showing effective automatic cluster discovery.

Conclusion: DPMM-CFL provides a practical CFL solution that automatically adapts to unknown cluster structures through coupled federated updates and cluster inference.

Abstract: Clustered Federated Learning (CFL) improves performance under non-IID client
heterogeneity by clustering clients and training one model per cluster, thereby
balancing between a global model and fully personalized models. However, most
CFL methods require the number of clusters K to be fixed a priori, which is
impractical when the latent structure is unknown. We propose DPMM-CFL, a CFL
algorithm that places a Dirichlet Process (DP) prior over the distribution of
cluster parameters. This enables nonparametric Bayesian inference to jointly
infer both the number of clusters and client assignments, while optimizing
per-cluster federated objectives. This results in a method where, at each
round, federated updates and cluster inferences are coupled, as presented in
this paper. The algorithm is validated on benchmark datasets under Dirichlet
and class-split non-IID partitions.

</details>


### [215] [A Multi-Agent Framework for Stateful Inference-Time Search](https://arxiv.org/abs/2510.07147)
*Arshika Lalan,Rajat Ghosh,Aditya Kolsur,Debojyoti Dutta*

Main category: cs.LG

TL;DR: Stateful multi-agent evolutionary search framework improves unit test generation by combining persistent inference-time state, adversarial mutation, and evolutionary preservation to generate robust edge cases.


<details>
  <summary>Details</summary>
Motivation: Address limitations of stateless inference-time techniques that struggle with multi-step reasoning tasks and brittleness of task-specific fine-tuning on long-horizon dependencies.

Method: Training-free framework using specialized agents for proposing, mutating, and scoring test candidates, with controller maintaining persistent state across generations and evolutionary preservation ensuring diversity.

Result: Achieves substantial gains in coverage over stateless single-step baselines on HumanEval and TestGenEvalMini benchmarks using Llama, Gemma, and GPT models.

Conclusion: Combining persistent inference-time state with evolutionary search materially improves unit-test generation, yielding generalist agents capable of discovering robust edge cases across unseen codebases.

Abstract: Recent work explores agentic inference-time techniques to perform structured,
multi-step reasoning. However, stateless inference often struggles on
multi-step tasks due to the absence of persistent state. Moreover,
task-specific fine-tuning or instruction-tuning often achieve surface-level
code generation but remain brittle on tasks requiring deeper reasoning and
long-horizon dependencies. To address these limitations, we propose stateful
multi-agent evolutionary search, a training-free framework that departs from
prior stateless approaches by combining (i) persistent inference-time state,
(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate
its effectiveness in automated unit test generation through the generation of
edge cases. We generate robust edge cases using an evolutionary search process,
where specialized agents sequentially propose, mutate, and score candidates. A
controller maintains persistent state across generations, while evolutionary
preservation ensures diversity and exploration across all possible cases. This
yields a generalist agent capable of discovering robust, high-coverage edge
cases across unseen codebases. Experiments show our stateful multi-agent
inference framework achieves substantial gains in coverage over stateless
single-step baselines, evaluated on prevalent unit-testing benchmarks such as
HumanEval and TestGenEvalMini and using three diverse LLM families - Llama,
Gemma, and GPT. These results indicate that combining persistent inference-time
state with evolutionary search materially improves unit-test generation.

</details>


### [216] [ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL](https://arxiv.org/abs/2510.07151)
*Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: ELMUR introduces a transformer architecture with structured external memory that extends effective horizons up to 100,000x beyond attention windows, achieving strong performance on long-horizon partially observable tasks.


<details>
  <summary>Details</summary>
Motivation: Real-world robotic agents need to handle partial observability and long horizons where key cues appear long before decision making, but current approaches struggle with retaining and leveraging long-term dependencies.

Method: ELMUR uses transformer architecture with structured external memory where each layer maintains memory embeddings, interacts via bidirectional cross-attention, and updates through LRU memory module using replacement or convex blending.

Result: Achieved 100% success rate on synthetic T-Maze task with million-step corridors, outperformed baselines on more than half of POPGym tasks, and nearly doubled performance on MIKASA-Robo sparse-reward manipulation tasks.

Conclusion: Structured, layer-local external memory provides a simple and scalable approach for decision making under partial observability, effectively extending horizons beyond standard attention windows.

Abstract: Real-world robotic agents must act under partial observability and long
horizons, where key cues may appear long before they affect decision making.
However, most modern approaches rely solely on instantaneous information,
without incorporating insights from the past. Standard recurrent or transformer
models struggle with retaining and leveraging long-term dependencies: context
windows truncate history, while naive memory extensions fail under scale and
sparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a
transformer architecture with structured external memory. Each layer maintains
memory embeddings, interacts with them via bidirectional cross-attention, and
updates them through an Least Recently Used (LRU) memory module using
replacement or convex blending. ELMUR extends effective horizons up to 100,000
times beyond the attention window and achieves a 100% success rate on a
synthetic T-Maze task with corridors up to one million steps. In POPGym, it
outperforms baselines on more than half of the tasks. On MIKASA-Robo
sparse-reward manipulation tasks with visual observations, it nearly doubles
the performance of strong baselines. These results demonstrate that structured,
layer-local external memory offers a simple and scalable approach to decision
making under partial observability.

</details>


### [217] [Bridged Clustering for Representation Learning: Semi-Supervised Sparse Bridging](https://arxiv.org/abs/2510.07182)
*Patrick Peixuan Ye,Chen Shani,Ellen Vitercik*

Main category: cs.LG

TL;DR: Bridged Clustering is a semi-supervised framework that learns predictors from unpaired X and Y datasets by clustering them independently and learning a sparse bridge between clusters using few paired examples.


<details>
  <summary>Details</summary>
Motivation: Traditional SSL doesn't leverage output-only data, and dense transport-based methods lack interpretability. This method aims to use output data while maintaining sparse, interpretable alignments.

Method: 1) Cluster X and Y independently 2) Learn sparse bridge between clusters using few paired examples 3) At inference, assign input to nearest cluster and return linked output cluster centroid as prediction

Result: Theoretical analysis shows effectiveness with bounded mis-clustering rates. Empirically competitive with SOTA methods while being simple, model-agnostic, and highly label-efficient in low-supervision settings.

Conclusion: Bridged Clustering provides an effective, efficient, and interpretable semi-supervised learning approach that explicitly leverages output-only data and maintains sparse alignments.

Abstract: We introduce Bridged Clustering, a semi-supervised framework to learn
predictors from any unpaired input $X$ and output $Y$ dataset. Our method first
clusters $X$ and $Y$ independently, then learns a sparse, interpretable bridge
between clusters using only a few paired examples. At inference, a new input
$x$ is assigned to its nearest input cluster, and the centroid of the linked
output cluster is returned as the prediction $\hat{y}$. Unlike traditional SSL,
Bridged Clustering explicitly leverages output-only data, and unlike dense
transport-based methods, it maintains a sparse and interpretable alignment.
Through theoretical analysis, we show that with bounded mis-clustering and
mis-bridging rates, our algorithm becomes an effective and efficient predictor.
Empirically, our method is competitive with SOTA methods while remaining
simple, model-agnostic, and highly label-efficient in low-supervision settings.

</details>


### [218] [Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples](https://arxiv.org/abs/2510.07192)
*Alexandra Souly,Javier Rando,Ed Chapman,Xander Davies,Burak Hasircioglu,Ezzeldin Shereen,Carlos Mougan,Vasilios Mavroudis,Erik Jones,Chris Hicks,Nicholas Carlini,Yarin Gal,Robert Kirk*

Main category: cs.LG

TL;DR: Poisoning attacks on LLMs require only a constant number of malicious documents (~250) regardless of model size or dataset scale, making backdoor injection easier than previously thought.


<details>
  <summary>Details</summary>
Motivation: Existing poisoning research assumes adversaries control a percentage of training data, but for large models this translates to impractically large amounts of data. This work challenges that assumption.

Method: Conducted largest pretraining poisoning experiments to date, training models from 600M to 13B parameters on chinchilla-optimal datasets (6B to 260B tokens). Also ran smaller-scale experiments varying poisoned-to-clean data ratios and poison distribution patterns.

Result: 250 poisoned documents similarly compromised models across all sizes, despite largest models training on 20x more clean data. Same dynamics observed for fine-tuning poisoning.

Conclusion: Backdoor injection through data poisoning may be easier for large models than believed, as poison count doesn't scale with model size, highlighting urgent need for better defenses.

Abstract: Poisoning attacks can compromise the safety of large language models (LLMs)
by injecting malicious documents into their training data. Existing work has
studied pretraining poisoning assuming adversaries control a percentage of the
training corpus. However, for large models, even small percentages translate to
impractically large amounts of data. This work demonstrates for the first time
that poisoning attacks instead require a near-constant number of documents
regardless of dataset size. We conduct the largest pretraining poisoning
experiments to date, pretraining models from 600M to 13B parameters on
chinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned
documents similarly compromise models across all model and dataset sizes,
despite the largest models training on more than 20 times more clean data. We
also run smaller-scale experiments to ablate factors that could influence
attack success, including broader ratios of poisoned to clean data and
non-random distributions of poisoned samples. Finally, we demonstrate the same
dynamics for poisoning during fine-tuning. Altogether, our results suggest that
injecting backdoors through data poisoning may be easier for large models than
previously believed as the number of poisons required does not scale up with
model size, highlighting the need for more research on defences to mitigate
this risk in future models.

</details>


### [219] [An in-depth look at approximation via deep and narrow neural networks](https://arxiv.org/abs/2510.07202)
*Joris Dommel,Sven A. Wegner*

Main category: cs.LG

TL;DR: This paper studies the approximation of a specific counterexample function by ReLU neural networks at the critical width thresholds w=n and w=n+1, analyzing how depth affects approximation quality and identifying dying neurons as a key factor.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate the boundary case of neural network width requirements for universal approximation, specifically examining what happens at the exact threshold w=n and just above it at w=n+1, using a known counterexample function.

Method: The authors approximate the counterexample function f:R^n->R using ReLU neural networks with widths w=n and w=n+1, varying the network depth and analyzing the resulting approximation behavior.

Result: The study reveals that approximation quality changes with depth and identifies "dying neurons" as the phenomenon causing this behavior, providing insights into network performance at critical width thresholds.

Conclusion: The research demonstrates that even at the theoretical threshold w=n and slightly above it at w=n+1, neural networks can approximate the counterexample function, with depth playing a crucial role and dying neurons significantly impacting the approximation quality.

Abstract: In 2017, Hanin and Sellke showed that the class of arbitrarily deep,
real-valued, feed-forward and ReLU-activated networks of width w forms a dense
subset of the space of continuous functions on R^n, with respect to the
topology of uniform convergence on compact sets, if and only if w>n holds. To
show the necessity, a concrete counterexample function f:R^n->R was used. In
this note we actually approximate this very f by neural networks in the two
cases w=n and w=n+1 around the aforementioned threshold. We study how the
approximation quality behaves if we vary the depth and what effect (spoiler
alert: dying neurons) cause that behavior.

</details>


### [220] [Guided by the Experts: Provable Feature Learning Dynamic of Soft-Routed Mixture-of-Experts](https://arxiv.org/abs/2510.07205)
*Fangshuo Liao,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: This paper provides theoretical convergence guarantees for joint training of soft-routed Mixture-of-Experts (MoE) models with non-linear routers and experts, showing that with moderate over-parameterization, student networks can recover teacher parameters through feature learning, and post-training pruning can eliminate redundant neurons.


<details>
  <summary>Details</summary>
Motivation: Despite widespread application of MoE architectures in modern AI systems, theoretical understanding of MoE training dynamics remains limited to either separate expert-router optimization or only top-1 routing scenarios with carefully constructed datasets.

Method: The authors use a student-teacher framework to analyze joint training of soft-routed MoE models with non-linear routers and experts, proving convergence guarantees with moderate over-parameterization and analyzing post-training pruning and fine-tuning processes.

Result: The analysis shows that student networks undergo a feature learning phase where the router's learning is guided by experts to recover teacher parameters, and that post-training pruning can effectively eliminate redundant neurons followed by a provably convergent fine-tuning process.

Conclusion: This work provides the first theoretical analysis that brings novel insights into understanding the optimization landscape of MoE architectures, addressing the gap in theoretical understanding of joint training dynamics for soft-routed MoE models.

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a cornerstone of
modern AI systems. In particular, MoEs route inputs dynamically to specialized
experts whose outputs are aggregated through weighted summation. Despite their
widespread application, theoretical understanding of MoE training dynamics
remains limited to either separate expert-router optimization or only top-1
routing scenarios with carefully constructed datasets. This paper advances MoE
theory by providing convergence guarantees for joint training of soft-routed
MoE models with non-linear routers and experts in a student-teacher framework.
We prove that, with moderate over-parameterization, the student network
undergoes a feature learning phase, where the router's learning process is
``guided'' by the experts, that recovers the teacher's parameters. Moreover, we
show that a post-training pruning can effectively eliminate redundant neurons,
followed by a provably convergent fine-tuning process that reaches global
optimality. To our knowledge, our analysis is the first to bring novel insights
in understanding the optimization landscape of the MoE architecture.

</details>


### [221] [A Broader View of Thompson Sampling](https://arxiv.org/abs/2510.07208)
*Yanlin Qu,Hongseok Namkoong,Assaf Zeevi*

Main category: cs.LG

TL;DR: This paper reveals that Thompson Sampling can be understood as an online optimization algorithm that balances exploration-exploitation through a regularized greedy approach, using a novel 'faithful' stationarization technique.


<details>
  <summary>Details</summary>
Motivation: To demystify how Thompson Sampling properly balances exploration and exploitation, which remains unclear despite its widespread use and strong performance in bandit problems.

Method: Introduces 'faithful' stationarization to convert the finite horizon dynamic optimization into a stationary counterpart, then applies Bellman's principle to derive a time-invariant optimal policy structure.

Result: Shows that Thompson Sampling mimics the Bellman-optimal policy structure with greediness regularized by residual uncertainty measured through point-biserial correlation.

Conclusion: Provides a principled framework explaining Thompson Sampling's exploration-exploitation balance and offers a foundation for further improvements to Thompson's original idea.

Abstract: Thompson Sampling is one of the most widely used and studied bandit
algorithms, known for its simple structure, low regret performance, and solid
theoretical guarantees. Yet, in stark contrast to most other families of bandit
algorithms, the exact mechanism through which posterior sampling (as introduced
by Thompson) is able to "properly" balance exploration and exploitation,
remains a mystery. In this paper we show that the core insight to address this
question stems from recasting Thompson Sampling as an online optimization
algorithm. To distill this, a key conceptual tool is introduced, which we refer
to as "faithful" stationarization of the regret formulation. Essentially, the
finite horizon dynamic optimization problem is converted into a stationary
counterpart which "closely resembles" the original objective (in contrast, the
classical infinite horizon discounted formulation, that leads to the Gittins
index, alters the problem and objective in too significant a manner). The newly
crafted time invariant objective can be studied using Bellman's principle which
leads to a time invariant optimal policy. When viewed through this lens,
Thompson Sampling admits a simple online optimization form that mimics the
structure of the Bellman-optimal policy, and where greediness is regularized by
a measure of residual uncertainty based on point-biserial correlation. This
answers the question of how Thompson Sampling balances
exploration-exploitation, and moreover, provides a principled framework to
study and further improve Thompson's original idea.

</details>


### [222] [Discriminative Feature Feedback with General Teacher Classes](https://arxiv.org/abs/2510.07245)
*Omri Bar Oz,Tosca Lechner,Sivan Sabato*

Main category: cs.LG

TL;DR: This paper provides the first systematic theoretical analysis of Discriminative Feature Feedback (DFF) learning protocol, comparing it with classical learning protocols and characterizing its mistake bounds in both realizable and non-realizable settings.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical properties of DFF learning protocol and compare it with classical learning protocols like supervised learning and online learning, particularly examining how richer feedback affects learning performance.

Method: The authors study DFF in a general framework comparable to classical protocols, analyzing optimal mistake bounds in realizable and non-realizable settings using a new notion of dimension and structural analysis.

Result: The paper characterizes the mistake bound in the realizable setting using a new dimension concept, provides a mistake upper bound in the non-realizable setting that cannot be improved, and shows that unlike online learning, the realizable dimension alone cannot characterize optimal non-realizable mistake bounds or no-regret algorithms in DFF.

Conclusion: DFF learning protocol exhibits fundamentally different theoretical properties from online learning, particularly in how richer feedback affects mistake bounds and the insufficiency of realizable dimension for characterizing non-realizable performance.

Abstract: We study the theoretical properties of the interactive learning protocol
Discriminative Feature Feedback (DFF) (Dasgupta et al., 2018). The DFF learning
protocol uses feedback in the form of discriminative feature explanations. We
provide the first systematic study of DFF in a general framework that is
comparable to that of classical protocols such as supervised learning and
online learning. We study the optimal mistake bound of DFF in the realizable
and the non-realizable settings, and obtain novel structural results, as well
as insights into the differences between Online Learning and settings with
richer feedback such as DFF. We characterize the mistake bound in the
realizable setting using a new notion of dimension. In the non-realizable
setting, we provide a mistake upper bound and show that it cannot be improved
in general. Our results show that unlike Online Learning, in DFF the realizable
dimension is insufficient to characterize the optimal non-realizable mistake
bound or the existence of no-regret algorithms.

</details>


### [223] [Test-Time Graph Search for Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2510.07257)
*Evgenii Opryshko,Junwei Quan,Claas Voelcker,Yilun Du,Igor Gilitschenski*

Main category: cs.LG

TL;DR: TTGS is a lightweight test-time planning method for offline goal-conditioned RL that builds graphs over dataset states and performs search to find subgoal sequences, improving long-horizon task performance without training changes.


<details>
  <summary>Details</summary>
Motivation: Offline GCRL struggles with long-horizon decision making due to temporal credit assignment and error accumulation, which are amplified in offline settings.

Method: TTGS builds a weighted graph over dataset states using any state-space distance/cost signal, performs fast search to find subgoal sequences, and executes them with a frozen policy. When using value-based learners, it derives distances from learned goal-conditioned value functions.

Result: On OGBench benchmark, TTGS improves success rates of multiple base learners on challenging locomotion tasks.

Conclusion: Simple metric-guided test-time planning can effectively enhance offline GCRL performance for long-horizon tasks without requiring training modifications, additional supervision, or online interaction.

Abstract: Offline goal-conditioned reinforcement learning (GCRL) trains policies that
reach user-specified goals at test time, providing a simple, unsupervised,
domain-agnostic way to extract diverse behaviors from unlabeled, reward-free
datasets. Nonetheless, long-horizon decision making remains difficult for GCRL
agents due to temporal credit assignment and error accumulation, and the
offline setting amplifies these effects. To alleviate this issue, we introduce
Test-Time Graph Search (TTGS), a lightweight planning approach to solve the
GCRL task. TTGS accepts any state-space distance or cost signal, builds a
weighted graph over dataset states, and performs fast search to assemble a
sequence of subgoals that a frozen policy executes. When the base learner is
value-based, the distance is derived directly from the learned goal-conditioned
value function, so no handcrafted metric is needed. TTGS requires no changes to
training, no additional supervision, no online interaction, and no privileged
information, and it runs entirely at inference. On the OGBench benchmark, TTGS
improves success rates of multiple base learners on challenging locomotion
tasks, demonstrating the benefit of simple metric-guided test-time planning for
offline GCRL.

</details>


### [224] [Dynamic Regret Bounds for Online Omniprediction with Long Term Constraints](https://arxiv.org/abs/2510.07266)
*Yahav Bechavod,Jiuyao Lu,Aaron Roth*

Main category: cs.LG

TL;DR: An algorithm for online omniprediction with dynamic regret guarantees and long-term constraints, ensuring all downstream decision makers achieve worst-case utility guarantees while minimizing constraint violations.


<details>
  <summary>Details</summary>
Motivation: To provide predictions that enable multiple downstream decision makers with different utility functions and constraints to make optimal decisions while ensuring worst-case utility guarantees and minimal constraint violations.

Method: Developed an algorithm that generates predictions for downstream agents who solve one-round constrained optimization problems based on these predictions, without requiring agents to maintain state.

Result: First algorithm achieving simultaneous dynamic regret guarantees for all agents (measured against changing action sequences) while ensuring vanishing constraint violation for each agent.

Conclusion: The proposed algorithm successfully addresses online omniprediction with long-term constraints, providing dynamic regret bounds and constraint satisfaction without requiring downstream agents to maintain state.

Abstract: We present an algorithm guaranteeing dynamic regret bounds for online
omniprediction with long term constraints. The goal in this recently introduced
problem is for a learner to generate a sequence of predictions which are
broadcast to a collection of downstream decision makers. Each decision maker
has their own utility function, as well as a vector of constraint functions,
each mapping their actions and an adversarially selected state to reward or
constraint violation terms. The downstream decision makers select actions "as
if" the state predictions are correct, and the goal of the learner is to
produce predictions such that all downstream decision makers choose actions
that give them worst-case utility guarantees while minimizing worst-case
constraint violation. Within this framework, we give the first algorithm that
obtains simultaneous \emph{dynamic regret} guarantees for all of the agents --
where regret for each agent is measured against a potentially changing sequence
of actions across rounds of interaction, while also ensuring vanishing
constraint violation for each agent. Our results do not require the agents
themselves to maintain any state -- they only solve one-round constrained
optimization problems defined by the prediction made at that round.

</details>


### [225] [GTCN-G: A Residual Graph-Temporal Fusion Network for Imbalanced Intrusion Detection (Preprint)](https://arxiv.org/abs/2510.07285)
*Tianxiang Xu,Zhichao Wen,Xinyu Zhao,Qi Hu,Yan Li,Chang Liu*

Main category: cs.LG

TL;DR: GTCN-G is a novel deep learning framework that integrates Gated TCN for temporal features and GCN for graph structures, using Graph Attention Network with residual connections to address class imbalance in intrusion detection.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of network threat complexity and class imbalance in intrusion detection by synergistically integrating temporal and topological modeling capabilities.

Method: Combines Gated Temporal Convolutional Network (G-TCN) for hierarchical temporal feature extraction with Graph Convolutional Network (GCN) for graph structure learning, integrated with Graph Attention Network (GAT) residual connections.

Result: Achieves state-of-the-art performance on UNSW-NB15 and ToN-IoT datasets, significantly outperforming existing baselines in both binary and multi-class classification.

Conclusion: The GTCN-G framework effectively addresses class imbalance and enhances detection sensitivity for rare malicious activities through its innovative integration of temporal and graph-based approaches with residual learning.

Abstract: The escalating complexity of network threats and the inherent class imbalance
in traffic data present formidable challenges for modern Intrusion Detection
Systems (IDS). While Graph Neural Networks (GNNs) excel in modeling topological
structures and Temporal Convolutional Networks (TCNs) are proficient in
capturing time-series dependencies, a framework that synergistically integrates
both while explicitly addressing data imbalance remains an open challenge. This
paper introduces a novel deep learning framework, named Gated Temporal
Convolutional Network and Graph (GTCN-G), engineered to overcome these
limitations. Our model uniquely fuses a Gated TCN (G-TCN) for extracting
hierarchical temporal features from network flows with a Graph Convolutional
Network (GCN) designed to learn from the underlying graph structure. The core
innovation lies in the integration of a residual learning mechanism,
implemented via a Graph Attention Network (GAT). This mechanism preserves
original feature information through residual connections, which is critical
for mitigating the class imbalance problem and enhancing detection sensitivity
for rare malicious activities (minority classes). We conducted extensive
experiments on two public benchmark datasets, UNSW-NB15 and ToN-IoT, to
validate our approach. The empirical results demonstrate that the proposed
GTCN-G model achieves state-of-the-art performance, significantly outperforming
existing baseline models in both binary and multi-class classification tasks.

</details>


### [226] [Evolutionary Profiles for Protein Fitness Prediction](https://arxiv.org/abs/2510.07286)
*Jigang Fan,Xiaoran Jiao,Shengdong Lin,Zhanming Liang,Weian Mao,Chenchen Jing,Hao Chen,Chunhua Shen*

Main category: cs.LG

TL;DR: EvoIF is a lightweight protein fitness prediction model that combines within-family evolutionary profiles and cross-family structural constraints, achieving state-of-the-art performance with minimal training data and parameters.


<details>
  <summary>Details</summary>
Motivation: Protein fitness prediction is limited by small experimental datasets relative to vast sequence space. Protein language models show promise but lack explicit evolutionary signal integration.

Method: EvoIF integrates two evolutionary signals: within-family profiles from homolog retrieval and cross-family structural constraints from inverse folding logits, using a compact transition block to fuse sequence-structure representations.

Result: On ProteinGym (217 assays, >2.5M mutants), EvoIF achieves state-of-the-art performance using only 0.15% training data and fewer parameters than large models. Within-family and cross-family profiles are complementary and improve robustness.

Conclusion: EvoIF provides an efficient framework for protein fitness prediction by explicitly integrating evolutionary signals, demonstrating that lightweight models can achieve competitive performance with minimal data and computational resources.

Abstract: Predicting the fitness impact of mutations is central to protein engineering
but constrained by limited assays relative to the size of sequence space.
Protein language models (pLMs) trained with masked language modeling (MLM)
exhibit strong zero-shot fitness prediction; we provide a unifying view by
interpreting natural evolution as implicit reward maximization and MLM as
inverse reinforcement learning (IRL), in which extant sequences act as expert
demonstrations and pLM log-odds serve as fitness estimates. Building on this
perspective, we introduce EvoIF, a lightweight model that integrates two
complementary sources of evolutionary signal: (i) within-family profiles from
retrieved homologs and (ii) cross-family structural-evolutionary constraints
distilled from inverse folding logits. EvoIF fuses sequence-structure
representations with these profiles via a compact transition block, yielding
calibrated probabilities for log-odds scoring. On ProteinGym (217 mutational
assays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve
state-of-the-art or competitive performance while using only 0.15% of the
training data and fewer parameters than recent large models. Ablations confirm
that within-family and cross-family profiles are complementary, improving
robustness across function types, MSA depths, taxa, and mutation depths. The
codes will be made publicly available at https://github.com/aim-uofa/EvoIF.

</details>


### [227] [MolGA: Molecular Graph Adaptation with Pre-trained 2D Graph Encoder](https://arxiv.org/abs/2510.07289)
*Xingtong Yu,Chang Zhou,Xinming Zhang,Yuan Fang*

Main category: cs.LG

TL;DR: MolGA adapts pre-trained 2D graph encoders to molecular applications by incorporating molecular domain knowledge through molecular alignment and conditional adaptation mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing pre-trained 2D graph encoders overlook rich molecular domain knowledge associated with submolecular instances, while molecular pre-training approaches lack flexibility to integrate diverse knowledge types.

Method: Proposes molecular alignment strategy to bridge gap between pre-trained topological representations and domain-knowledge representations, and conditional adaptation mechanism that generates instance-specific tokens for fine-grained knowledge integration.

Result: Extensive experiments on eleven public datasets demonstrate the effectiveness of MolGA.

Conclusion: MolGA provides a practical approach to reuse pre-trained 2D encoders while flexibly incorporating diverse molecular domain knowledge during downstream adaptation.

Abstract: Molecular graph representation learning is widely used in chemical and
biomedical research. While pre-trained 2D graph encoders have demonstrated
strong performance, they overlook the rich molecular domain knowledge
associated with submolecular instances (atoms and bonds). While molecular
pre-training approaches incorporate such knowledge into their pre-training
objectives, they typically employ designs tailored to a specific type of
knowledge, lacking the flexibility to integrate diverse knowledge present in
molecules. Hence, reusing widely available and well-validated pre-trained 2D
encoders, while incorporating molecular domain knowledge during downstream
adaptation, offers a more practical alternative. In this work, we propose
MolGA, which adapts pre-trained 2D graph encoders to downstream molecular
applications by flexibly incorporating diverse molecular domain knowledge.
First, we propose a molecular alignment strategy that bridge the gap between
pre-trained topological representations with domain-knowledge representations.
Second, we introduce a conditional adaptation mechanism that generates
instance-specific tokens to enable fine-grained integration of molecular domain
knowledge for downstream tasks. Finally, we conduct extensive experiments on
eleven public datasets, demonstrating the effectiveness of MolGA.

</details>


### [228] [MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline](https://arxiv.org/abs/2510.07307)
*Rushi Qiang,Yuchen Zhuang,Anikait Singh,Percy Liang,Chao Zhang,Sherry Yang,Bo Dai*

Main category: cs.LG

TL;DR: MLE-Smith is an automated multi-agent pipeline that transforms raw datasets into machine learning engineering challenges, addressing the scarcity of high-quality training data through a generate-verify-execute approach.


<details>
  <summary>Details</summary>
Motivation: Current MLE benchmarks suffer from low scalability and limited applicability due to reliance on static, manually curated tasks that require extensive time and manual effort to produce.

Method: A fully automated multi-agent pipeline using generate-verify-execute paradigm with structured task design, standardized refactoring, and hybrid verification mechanism enforcing structural rules and semantic soundness, validated through interactive execution.

Result: Applied to 224 real-world datasets, generated 606 tasks spanning multiple categories, objectives, and modalities. Evaluation showed strong correlation between LLM performance on MLE-Smith tasks and human-designed tasks.

Conclusion: MLE-Smith effectively scales up MLE tasks while maintaining quality, demonstrating strong correlation with human-designed benchmarks and working effectively across diverse real-world datasets.

Abstract: While Language Models (LMs) have made significant progress in automating
machine learning engineering (MLE), the acquisition of high-quality MLE
training data is significantly constrained. Current MLE benchmarks suffer from
low scalability and limited applicability because they rely on static, manually
curated tasks, demanding extensive time and manual effort to produce. We
introduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw
datasets into competition-style MLE challenges through an efficient
generate-verify-execute paradigm for scaling MLE tasks with verifiable quality,
real-world usability, and rich diversity. The proposed multi-agent pipeline in
MLE-Smith drives structured task design and standardized refactoring, coupled
with a hybrid verification mechanism that enforces strict structural rules and
high-level semantic soundness. It further validates empirical solvability and
real-world fidelity through interactive execution. We apply MLE-Smith to 224 of
real-world datasets and generate 606 tasks spanning multiple categories,
objectives, and modalities, demonstrating that MLE-Smith can work effectively
across a wide range of real-world datasets. Evaluation on the generated tasks
shows that the performance of eight mainstream and cutting-edge LLMs on
MLE-Smith tasks is strongly correlated with their performance on carefully
human-designed tasks, highlighting the effectiveness of the MLE-Smith to
scaling up MLE tasks, while maintaining task quality.

</details>


### [229] [h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning](https://arxiv.org/abs/2510.07312)
*Sumeet Ramesh Motwani,Alesia Ivanova,Ziyang Cai,Philip Torr,Riashat Islam,Shital Shah,Christian Schroeder de Witt,Charles London*

Main category: cs.LG

TL;DR: A scalable method to bootstrap long-horizon reasoning using only short-horizon data by synthetically composing simple problems into complex multi-step chains and training with outcome-only rewards under an automatic complexity curriculum.


<details>
  <summary>Details</summary>
Motivation: Large language models perform well on short-horizon reasoning but struggle with longer reasoning chains. Existing approaches require inference-time scaffolding or costly step-level supervision, which don't scale well.

Method: Synthetically compose simple problems into complex multi-step dependency chains of arbitrary length, train models using outcome-only rewards under an automatic curriculum that increases complexity, allowing RL training to scale without saturation.

Result: Curriculum training on composed 6th-grade math problems (GSM8K) boosts accuracy on longer competition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x. Long-horizon improvements are significantly higher than baselines even at high pass@k.

Conclusion: The method provides an efficient path for scaling RL for long-horizon problems using only existing data, with theoretical analysis showing exponential improvement in sample complexity over full-horizon training.

Abstract: Large language models excel at short-horizon reasoning tasks, but performance
drops as reasoning horizon lengths increase. Existing approaches to combat this
rely on inference-time scaffolding or costly step-level supervision, neither of
which scales easily. In this work, we introduce a scalable method to bootstrap
long-horizon reasoning capabilities using only existing, abundant short-horizon
data. Our approach synthetically composes simple problems into complex,
multi-step dependency chains of arbitrary length. We train models on this data
using outcome-only rewards under a curriculum that automatically increases in
complexity, allowing RL training to be scaled much further without saturating.
Empirically, our method generalizes remarkably well: curriculum training on
composed 6th-grade level math problems (GSM8K) boosts accuracy on longer,
competition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.
Importantly, our long-horizon improvements are significantly higher than
baselines even at high pass@k, showing that models can learn new reasoning
paths under RL. Theoretically, we show that curriculum RL with outcome rewards
achieves an exponential improvement in sample complexity over full-horizon
training, providing training signal comparable to dense supervision. h1
therefore introduces an efficient path towards scaling RL for long-horizon
problems using only existing data.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [230] [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
*Zhanke Zhou,Chentao Cao,Xiao Feng,Xuan Li,Zongze Li,Xiangyu Lu,Jiangchao Yao,Weikai Huang,Linrui Xu,Tian Cheng,Guanyu Jiang,Yiming Zheng,Brando Miranda,Tongliang Liu,Sanmi Koyejo,Masashi Sugiyama,Bo Han*

Main category: cs.AI

TL;DR: AlphaApollo is a self-evolving agentic reasoning system that addresses foundation model limitations by orchestrating multiple models with computational and retrieval tools for verifiable reasoning, achieving significant performance gains on AIME benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome two key bottlenecks in foundation model reasoning: limited model-intrinsic capacity and unreliable test-time iteration.

Method: Orchestrates multiple models with professional tools (Python computation and retrieval tools) for deliberate reasoning, using a shared state map for multi-round solution evolution with iterative refinement.

Result: Consistent gains on AIME 2024/2025: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, +8.91% Average@32 and +26.67% Pass@32 for Llama-3.3-70B-Instruct. Over 80% tool calls successfully executed.

Conclusion: AlphaApollo effectively lifts the capability ceiling of foundation models through tool-augmented reasoning and multi-model orchestration.

Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to
address two bottlenecks in foundation model (FM) reasoning-limited
model-intrinsic capacity and unreliable test-time iteration. AlphaApollo
orchestrates multiple models with professional tools to enable deliberate,
verifiable reasoning. It couples (i) a computation tool (Python with numerical
and symbolic libraries) and (ii) a retrieval tool (task-relevant external
information) to execute exact calculations and ground decisions. The system
further supports multi-round, multi-model solution evolution via a shared state
map that records candidates, executable checks, and feedback for iterative
refinement. In evaluations on AIME 2024/2025 across multiple models,
AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32
for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for
Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool
calls are successfully executed, with consistent outperformance of non-tool
baselines, thereby lifting the capability ceiling of FMs. More empirical
results and implementation details will be updated at
https://github.com/tmlr-group/AlphaApollo.

</details>


### [231] [Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization](https://arxiv.org/abs/2510.06274)
*Mohammad Mahdi Samiei Paqaleh,Arash Marioriyad,Arman Tahmasebi-Zadeh,Mohamadreza Fereydooni,Mahdi Ghaznavai,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: The paper proposes Complexity Out of Distribution (Complexity OoD) generalization as a framework to define and measure reasoning ability in AI systems, distinguishing it from traditional pattern recognition tasks.


<details>
  <summary>Details</summary>
Motivation: There is no clear, consistent definition or metric for reasoning ability in AI, unlike learning where generalization concepts are well formalized. The authors aim to establish a framework to properly define and evaluate reasoning capabilities.

Method: The authors formalize complexity via solution description Kolmogorov complexity and operational proxies (object/relation counts, reasoning step counts). They propose Complexity OoD generalization where models maintain performance on test instances requiring more complex solutions than training examples.

Result: The framework unifies learning and reasoning, showing that many cases solvable with System1-like processing at low complexity become System2-like under complexity pressure. System2 reasoning can be viewed as generalization over solution structures.

Conclusion: Progress toward robust reasoning requires architectures and training regimes that explicitly model and allocate computation with respect to complexity, as Complexity OoD cannot be solved by scaling data alone.

Abstract: Recent progress has pushed AI frontiers from pattern recognition tasks toward
problems that require step by step, System2 style reasoning, especially with
large language models. Yet, unlike learning, where generalization and out of
distribution (OoD) evaluation concepts are well formalized, there is no clear,
consistent definition or metric for reasoning ability. We propose Complexity
Out of Distribution (Complexity OoD) generalization as a framework and problem
setting to define and measure reasoning. A model exhibits Complexity OoD
generalization when it maintains performance on test instances whose minimal
required solution complexity, either representational (richer solution
structure) or computational (more reasoning steps/program length), exceeds that
of all training examples. We formalize complexity via solution description
Kolmogorov complexity and operational proxies (e.g., object/relation counts;
reasoning step counts), clarifying how Complexity OoD differs from length and
compositional OoD. This lens unifies learning and reasoning: many cases
solvable with System1 like processing at low complexity become System2 like
under complexity pressure, while System2 can be viewed as generalization over
solution structures. We translate this perspective into practice with
recommendations for operationalizing Complexity OoD across the stack:
incorporating complexity into benchmark and evaluation metric design,
rethinking supervision to target solution traces, seeking and designing
inductive biases for Complexity OoD generalization, addressing learning to
reason spillovers such as spurious shortcuts, semantic robustness, catastrophic
forgetting, and step wise calibration. Because Complexity OoD cannot be solved
by scaling data alone, progress toward robust reasoning will require
architectures and training regimes that explicitly model and allocate
computation with respect to complexity.

</details>


### [232] [BuilderBench -- A benchmark for generalist agents](https://arxiv.org/abs/2510.06288)
*Raj Ghugare,Catherine Ji,Kathryn Wantlin,Jin Schofield,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: BuilderBench is a benchmark for agent pre-training that focuses on open-ended exploration, requiring agents to learn how to build various structures using blocks in a simulated environment without external supervision.


<details>
  <summary>Details</summary>
Motivation: Current AI models struggle with novel problems beyond existing data limits. To enable agents to solve new problems, they need skills for exploration and learning through experience, which requires scalable learning mechanisms for interactive learning.

Method: BuilderBench provides a hardware-accelerated simulator of a robotic agent interacting with physical blocks, and a task suite with 42 diverse target structures testing physics understanding, mathematics, and long-horizon planning. Agents explore and learn general principles without supervision.

Result: The experiments show that many BuilderBench tasks challenge current algorithms. The benchmark also includes a 'training wheels' protocol for building single target structures and provides implementations of six different algorithms as reference points.

Conclusion: BuilderBench accelerates research into agent pre-training through embodied reasoning, where learning is reflected in actions and experimentation rather than words, addressing the need for scalable interactive learning mechanisms.

Abstract: Today's AI models learn primarily through mimicry and sharpening, so it is
not surprising that they struggle to solve problems beyond the limits set by
existing data. To solve novel problems, agents should acquire skills for
exploring and learning through experience. Finding a scalable learning
mechanism for developing agents that learn through interaction remains a major
open problem. In this work, we introduce BuilderBench, a benchmark to
accelerate research into agent pre-training that centers open-ended
exploration. BuilderBench requires agents to learn how to build any structure
using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated
simulator of a robotic agent interacting with various physical blocks, and
$(2)$ a task-suite with over 42 diverse target structures that are carefully
curated to test an understanding of physics, mathematics, and long-horizon
planning. During training, agents have to explore and learn general principles
about the environment without any external supervision. During evaluation,
agents have to build the unseen target structures from the task suite. Solving
these tasks requires a sort of \emph{embodied reasoning} that is not reflected
in words but rather in actions, experimenting with different strategies and
piecing them together. Our experiments show that many of these tasks challenge
the current iteration of algorithms. Hence, we also provide a ``training
wheels'' protocol, in which agents are trained and evaluated to build a single
target structure from the task suite. Finally, we provide single-file
implementations of six different algorithms as a reference point for
researchers.

</details>


### [233] [Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration](https://arxiv.org/abs/2510.06302)
*Ksenija Lace,Marite Kirikova*

Main category: cs.AI

TL;DR: This paper proposes a game-based learning framework to address the high learning curve and low motivation issues in training for information system integration during post-merger integration, building on existing methods AMILI and AMILP.


<details>
  <summary>Details</summary>
Motivation: There is a significant gap in training for information system integration in post-merger contexts, with existing methods (AMILI and AMILP) suffering from high learning curves and low learner motivation during practical application.

Method: The study analyzes foundational learning theories, cognitive load and motivation models, and serious game design frameworks to identify essential requirements for a game-based learning design framework tailored to information system integration in post-merger integration.

Result: Requirements are structured in two components: the transformation process and resulting learning experience, providing a foundation for developing an engaging game-based learning approach.

Conclusion: The paper concludes with a plan for developing and evaluating the proposed framework through iterative design and real-world validation to transform static method training into engaging learning experiences.

Abstract: Post-merger integration states unique challenges for professionals
responsible for information system integration aimed on alignment and
combination diverse system architectures of merging organizations. Although the
theoretical and practical guidance exists for post-merger integration on the
business level, there is a significant gap in training for information system
integration in this context. In prior research specific methods AMILI (Support
method for informed decision identification) and AMILP (Support method for
informed decision-making) were introduced for the support of information system
integration decisions in the post-merger integration. But during the practical
application was reported high learning curve and low learner motivation. This
paper explores how game-based learning design can address these limitations by
transforming static method training into engaging learning experience. The
study analyzes foundational learning theories, cognitive load and motivation
models, and serious game design frameworks to identify the essential
requirements for a game-based learning design framework tailored to information
system integration in post-merger integration. Requirements are structured in
two components: the transformation process and resulting learning experience.
The paper concludes with a plan for developing and evaluating the proposed
framework through iterative design and real-world validation.

</details>


### [234] [Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks](https://arxiv.org/abs/2510.06307)
*Wentao Deng,Jiahuan Pei,Zhiwei Xu,Zhaochun Ren,Zhumin Chen,Pengjie Ren*

Main category: cs.AI

TL;DR: The paper proposes BCCS framework for stable consensus in multi-agent systems by selecting optimal collaborators and calibrating consensus judgment using system-internal beliefs, achieving significant performance improvements on MATH and MMLU benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing consensus-seeking approaches in multi-agent systems rely on voting mechanisms and uniform collaboration, which overlook contradictions in system-internal beliefs and fail to identify optimal collaborators, leading to unstable consensus.

Method: Proposed Belief-Calibrated Consensus Seeking (BCCS) framework with theoretical foundation for selecting optimal collaborators that maximize consensus stability, and calibrating consensus judgment using system-internal beliefs.

Result: BCCS outperforms best existing results by 2.23% accuracy on MATH and 3.95% accuracy on MMLU benchmark datasets for challenging NLP tasks.

Conclusion: The BCCS framework effectively addresses limitations of existing consensus-seeking methods by enabling stable consensus through optimal collaborator selection and belief-calibrated judgment, demonstrating superior performance on benchmark tasks.

Abstract: A multi-agent system (MAS) enhances its capacity to solve complex natural
language processing (NLP) tasks through collaboration among multiple agents,
where consensus-seeking serves as a fundamental mechanism. However, existing
consensus-seeking approaches typically rely on voting mechanisms to judge
consensus, overlooking contradictions in system-internal beliefs that
destabilize the consensus. Moreover, these methods often involve agents
updating their results through indiscriminate collaboration with every other
agent. Such uniform interaction fails to identify the optimal collaborators for
each agent, hindering the emergence of a stable consensus. To address these
challenges, we provide a theoretical framework for selecting optimal
collaborators that maximize consensus stability. Based on the theorems, we
propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate
stable consensus via selecting optimal collaborators and calibrating the
consensus judgment by system-internal beliefs. Experimental results on the MATH
and MMLU benchmark datasets demonstrate that the proposed BCCS framework
outperforms the best existing results by 2.23% and 3.95% of accuracy on
challenging tasks, respectively. Our code and data are available at
https://github.com/dengwentao99/BCCS.

</details>


### [235] [Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?](https://arxiv.org/abs/2510.06410)
*Aochong Oliver Li,Tanya Goyal*

Main category: cs.AI

TL;DR: The paper investigates whether standard solo-reasoning training enables LLMs to effectively collaborate by building on each other's partial reasoning (off-trajectory reasoning). It finds stronger LLMs are more fragile to distractions and models fail to leverage collaborators' reasoning, with solve rates under 9.2%.


<details>
  <summary>Details</summary>
Motivation: To enable effective multi-model collaboration in shared reasoning trajectories, which requires the ability to assess and build upon another model's partial thinking (off-trajectory reasoning).

Method: Proposed twin tests: Recoverability (backtracking from misleading reasoning) and Guidability (building on correct reasoning from stronger collaborators). Evaluated 15 open-weight LLMs (1.5B-32B) and conducted control studies on post-training factors.

Result: Stronger LLMs on benchmarks are more fragile under distraction. All models fail to effectively leverage guiding steps from collaborators, with solve rates remaining under 9.2%. Suboptimal recoverability behaviors transfer from teachers to distilled students.

Conclusion: Standard solo-reasoning training pipelines don't deliver desired off-trajectory behaviors. The work provides insights for training better reasoning collaborators and highlights limitations of current reasoning LLMs for multi-model collaboration.

Abstract: Reasoning LLMs are trained to verbalize their reasoning process, yielding
strong gains on complex tasks. This transparency also opens a promising
direction: multiple reasoners can directly collaborate on each other's thinking
within a shared trajectory, yielding better inference efficiency and
exploration. A key prerequisite, however, is the ability to assess the
usefulness and build on another model's partial thinking -- we call this
off-trajectory reasoning. Our paper investigates a critical question: can
standard solo-reasoning training pipelines deliver desired off-trajectory
behaviors? We propose twin tests that capture the two extremes of the
off-trajectory spectrum, namely Recoverability, which tests whether LLMs can
backtrack from "distractions" induced by misleading reasoning traces, and
Guidability, which tests their ability to build upon correct reasoning from
stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and
reveals a counterintuitive finding -- "stronger" LLMs on benchmarks are often
more fragile under distraction. Moreover, all models tested fail to effectively
leverage guiding steps from collaborators on problems beyond their inherent
capabilities with solve rates remaining under 9.2%. Finally, we conduct control
studies to isolate the effects of three factors in post-training on these
behaviors: the choice of distillation teacher, the use of RL, and data
selection strategy. Our results provide actionable insights for training
natively strong reasoning collaborators; e.g., we find that suboptimal
recoverability behaviors of teacher models are transferred to distilled
students even if the distillation trajectories are correct. Taken together,
this work lays the groundwork for evaluating multi-model collaborations in
shared reasoning trajectories and highlights the limitations of off-the-shelf
reasoning LLMs.

</details>


### [236] [Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health](https://arxiv.org/abs/2510.06433)
*Aryan Singh Dalal,Yinglun Zhang,Duru Doğan,Atalay Mert İleri,Hande Küçük McGinty*

Main category: cs.AI

TL;DR: This study creates a knowledge graph to link food and health relationships, specifically focusing on flavonoid contents from USDA databases and cancer connections from literature, using KNARM methodology for machine-readable representation.


<details>
  <summary>Details</summary>
Motivation: Address the gap in representing food-health relationships in standardized, machine-readable format using semantic web technologies, as current research lacks such representations despite growing interest in 'food as medicine'.

Method: Used KNARM methodology to create a knowledge graph combining information from USDA databases (flavonoid contents) and literature (cancer connections), representing relationships in machine-operable format.

Result: Developed a knowledge graph that serves as an example for researchers to explore dietary choices and disease management relationships, providing a foundation for future analysis.

Conclusion: The knowledge graph successfully demonstrates the potential for representing food-health relationships in standardized format, with future work planned to expand scope, add more data, and perform inferences to uncover hidden relationships.

Abstract: The focus on "food as medicine" is gaining traction in the field of health
and several studies conducted in the past few years discussed this aspect of
food in the literature. However, very little research has been done on
representing the relationship between food and health in a standardized,
machine-readable format using a semantic web that can help us leverage this
knowledge effectively. To address this gap, this study aims to create a
knowledge graph to link food and health through the knowledge graph's ability
to combine information from various platforms focusing on flavonoid contents of
food found in the USDA databases and cancer connections found in the
literature. We looked closely at these relationships using KNARM methodology
and represented them in machine-operable format. The proposed knowledge graph
serves as an example for researchers, enabling them to explore the complex
interplay between dietary choices and disease management. Future work for this
study involves expanding the scope of the knowledge graph by capturing nuances,
adding more related data, and performing inferences on the acquired knowledge
to uncover hidden relationships.

</details>


### [237] [PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles](https://arxiv.org/abs/2510.06475)
*Yitao Long,Yuru Jiang,Hongjun Liu,Yilun Zhao,Jingchen Sun,Yiqiu Shen,Chen Zhao,Arman Cohan,Dennis Shasha*

Main category: cs.AI

TL;DR: PuzzlePlex is a benchmark with 15 puzzle types to evaluate foundation models' reasoning and planning in complex environments, showing reasoning models excel in instruction-based settings while code-based execution offers scalability.


<details>
  <summary>Details</summary>
Motivation: To assess reasoning and planning capabilities of foundation models in complex, dynamic environments and understand their scalability limits.

Method: Developed PuzzlePlex benchmark with 15 puzzle types including deterministic/stochastic games and single/two-player scenarios, implemented custom strategies, and tested frontier models in instruction-based and code-based settings with fine-grained metrics.

Result: Reasoning models outperform others in instruction-based settings; code-based execution is more challenging but provides scalable and efficient alternative.

Conclusion: PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models.

Abstract: This work investigates the reasoning and planning capabilities of foundation
models and their scalability in complex, dynamic environments. We introduce
PuzzlePlex, a benchmark designed to assess these capabilities through a diverse
set of puzzles. PuzzlePlex consists of 15 types of puzzles, including
deterministic and stochastic games of varying difficulty, as well as
single-player and two-player scenarios. The PuzzlePlex framework provides a
comprehensive environment for each game, and supports extensibility to generate
more challenging instances as foundation models evolve. Additionally, we
implement customized game-playing strategies for comparison. Building on this
benchmark, we develop fine-grained metrics to measure performance and conduct
an in-depth analysis of frontier foundation models across two settings:
instruction-based and code-based. Furthermore, we systematically investigate
their scaling limits. Our findings show that reasoning models outperform others
in instruction-based settings, while code-based execution presents greater
challenges but offers a scalable and efficient alternative. PuzzlePlex enables
targeted evaluation and guides future improvements in reasoning, planning, and
generalization for foundation models.

</details>


### [238] [Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them](https://arxiv.org/abs/2510.06534)
*Jiahe Jin,Abhijay Paladugu,Chenyan Xiong*

Main category: cs.AI

TL;DR: Proposes Behavior Priming - a technique that trains agentic search models using trajectories exhibiting four key reasoning behaviors (Information Verification, Authority Evaluation, Adaptive Search, Error Recovery) through SFT+RL, achieving 35%+ gains over RL-only training.


<details>
  <summary>Details</summary>
Motivation: Agentic search introduces unique challenges for LLMs' reasoning and agentic capabilities when interacting with retrieval systems and the web. Current approaches need better understanding of effective reasoning behavior patterns.

Method: Developed a reasoning-driven pipeline to analyze successful agentic search trajectories, identified four beneficial reasoning behaviors, then proposed Behavior Priming that synthesizes trajectories with these behaviors for SFT followed by standard RL.

Result: 35%+ gains on Llama3.2-3B and Qwen3-1.7B across three benchmarks (GAIA, WebWalker, HLE). Key finding: reasoning behaviors in SFT data, not answer correctness, is critical for strong RL performance.

Conclusion: Behavior Priming enables more effective exploration and test-time scaling by endowing models with essential reasoning behaviors, providing a strong foundation for RL training in agentic search.

Abstract: Agentic search leverages large language models (LLMs) to interpret complex
user information needs and execute a multi-step process of planning, searching,
and synthesizing information to provide answers. This paradigm introduces
unique challenges for LLMs' reasoning and agentic capabilities when interacting
with retrieval systems and the broader web. In this paper, we propose a
reasoning-driven LLM-based pipeline to study effective reasoning behavior
patterns in agentic search. Using this pipeline, we analyze successful agentic
search trajectories and identify four beneficial reasoning behaviors:
Information Verification, Authority Evaluation, Adaptive Search, and Error
Recovery. Based on these findings, we propose a technique called Behavior
Priming to train more effective agentic search models. It synthesizes agentic
search trajectories that exhibit these four behaviors and integrates them into
the agentic search model through supervised fine-tuning (SFT), followed by
standard reinforcement learning (RL). Experiments on three benchmarks (GAIA,
WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in
Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models
with RL. Crucially, we demonstrate that the desired reasoning behaviors in the
SFT data, rather than the correctness of the final answer, is the critical
factor for achieving strong final performance after RL: fine-tuning on
trajectories with desirable reasoning behaviors but incorrect answers leads to
better performance than fine-tuning on trajectories with correct answers. Our
analysis further reveals the underlying mechanism: the introduced reasoning
behaviors endow models with more effective exploration (higher pass@k and
entropy) and test-time scaling (longer trajectories) capabilities, providing a
strong foundation for RL. Our code will be released as open source.

</details>


### [239] [Auto-Prompt Ensemble for LLM Judge](https://arxiv.org/abs/2510.06538)
*Jiajie Li,Huayi Zhang,Peng Lin,Jinjun Xiong,Wei Xu*

Main category: cs.AI

TL;DR: APE framework improves LLM judge reliability by automatically learning evaluation dimensions from failure cases and using confidence-based ensemble to selectively augment judgments.


<details>
  <summary>Details</summary>
Motivation: Existing LLM judges often miss crucial evaluation dimensions because they fail to recognize implicit standards in human assessments, creating an evaluation gap.

Method: Proposes Auto-Prompt Ensemble (APE) - adaptive framework that learns evaluation dimensions from failure cases, uses Collective Confidence estimation for confidence-based ensemble decisions.

Result: APE improves GPT-4o agreement rate on Reward Bench from 87.2% to 90.5% in zero-shot setting, enhances reliability across diverse benchmarks.

Conclusion: APE provides principled approach for LLM judges to leverage test-time computation and bridge evaluation gap between human and LLM judges.

Abstract: We present a novel framework that improves the reliability of LLM judges by
selectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM
judges often miss crucial evaluation dimensions because they fail to recognize
the implicit standards underlying human assessments. To address this challenge,
we propose the Auto-Prompt Ensemble (APE), an adaptive framework that
automatically learns evaluation dimensions from its failure cases. APE
incorporates a confidence-based ensemble mechanism to decide when to adopt the
judgments from additional evaluation dimensions through a novel confidence
estimation approach called Collective Confidence. Extensive experiments
demonstrate that APE improves the reliability of LLM Judge across diverse
standard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward
Bench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a
principled approach for LLM Judge to leverage test-time computation, and bridge
the evaluation gap between human and LLM judges.

</details>


### [240] [WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks](https://arxiv.org/abs/2510.06587)
*Jingbo Yang,Bairu Hou,Wei Wei,Shiyu Chang,Yujia Bao*

Main category: cs.AI

TL;DR: WebDART is a framework that enables single LLM agents to handle complex web tasks by dynamically decomposing objectives into navigation, information extraction, and execution subtasks, with continuous replanning as new webpages are revealed.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents struggle with complex web tasks requiring long horizon navigation, large scale information extraction, and reasoning under constraints, despite being competent at straightforward tasks.

Method: WebDART dynamically decomposes objectives into three focused subtasks (navigation, information extraction, execution) and continuously replans the decomposition as new webpages are revealed to take advantage of discovered filters/shortcuts and avoid redundant exploration.

Result: On WebChoreArena, WebDART improves success rates by up to 13.7 percentage points over previous SOTA agents, matches performance on WebArena, and completes tasks with up to 14.7 fewer navigation steps.

Conclusion: WebDART effectively enables single LLM agents to handle complex web tasks through dynamic task decomposition and continuous replanning, significantly improving performance on challenging benchmarks.

Abstract: Large language model (LLM) agents are becoming competent at straightforward
web tasks, such as opening an item page or submitting a form, but still
struggle with objectives that require long horizon navigation, large scale
information extraction, and reasoning under constraints. We present WebDART, a
general framework that enables a single LLM to handle such complex chores.
WebDART (i) dynamically decomposes each objective into three focused subtasks:
navigation, information extraction, and execution, so the model concentrates on
one skill at a time, and (ii) continuously replans the decomposition as new
webpages are revealed, taking advantage of newly discovered filters or
shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,
WebDART lifts success rates by up to 13.7 percentage points over previous SOTA
agents, while matching their performance on the easier WebArena suite and
completing tasks with up to 14.7 fewer navigation steps.

</details>


### [241] [Fine-Grained Emotion Recognition via In-Context Learning](https://arxiv.org/abs/2510.06600)
*Zhaochun Ren,Zhou Yang,Chenglong Ye,Haizhou Sun,Chao Chen,Xiaofei Zhu,Xiangwen Liao*

Main category: cs.AI

TL;DR: EICL improves fine-grained emotion recognition by addressing emotional discrepancies in ICL through emotionally similar examples and dynamic soft-label strategy, outperforming ICL on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Current ICL methods enhance reasoning but overlook decision-making in emotion recognition, and semantically similar examples often introduce emotional discrepancies that hinder accurate representations.

Method: Proposes Emotion In-Context Learning (EICL) with emotionally similar examples, dynamic soft-label strategy for better query representations, and two-stage exclusion strategy for multi-angle similarity assessment.

Result: Extensive experiments show EICL significantly outperforms ICL on multiple datasets.

Conclusion: EICL effectively addresses emotional discrepancies in ICL and improves both reasoning and decision-making processes in fine-grained emotion recognition.

Abstract: Fine-grained emotion recognition aims to identify the emotional type in
queries through reasoning and decision-making processes, playing a crucial role
in various systems. Recent methods use In-Context Learning (ICL), enhancing the
representation of queries in the reasoning process through semantically similar
examples, while further improving emotion recognition by explaining the
reasoning mechanisms. However, these methods enhance the reasoning process but
overlook the decision-making process. This paper investigates decision-making
in fine-grained emotion recognition through prototype theory. We show that ICL
relies on similarity matching between query representations and emotional
prototypes within the model, where emotion-accurate representations are
critical. However, semantically similar examples often introduce emotional
discrepancies, hindering accurate representations and causing errors. To
address this, we propose Emotion In-Context Learning (EICL), which introduces
emotionally similar examples and uses a dynamic soft-label strategy to improve
query representations in the emotion reasoning process. A two-stage exclusion
strategy is then employed to assess similarity from multiple angles, further
optimizing the decision-making process. Extensive experiments show that EICL
significantly outperforms ICL on multiple datasets.

</details>


### [242] [Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support](https://arxiv.org/abs/2510.06674)
*Cen,Zhao,Tiantian Zhang,Hanchen Su,Yufeng,Zhang,Shaowei Su,Mingzhi Xu,Yu,Liu,Wei Han,Jeremy Werner,Claire Na Cheng,Yashar Mehdad*

Main category: cs.AI

TL;DR: AITL framework enables continuous improvement of LLM-based customer support through real-time human feedback integration, reducing retraining cycles from months to weeks and achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Standard offline approaches with batch annotations are slow and inefficient for improving customer support systems. There's a need for faster, more integrated feedback mechanisms that can continuously refine LLM performance during live operations.

Method: Agent-in-the-Loop (AITL) framework integrates four annotation types directly into live customer support: (1) pairwise response preferences, (2) agent adoption and rationales, (3) knowledge relevance checks, and (4) identification of missing knowledge. These feedback signals are used for continuous model updates.

Result: Production pilot showed significant improvements: retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality (+8.4% helpfulness), and agent adoption rates (+4.5%). Retraining cycles reduced from months to weeks.

Conclusion: Embedding human feedback loops directly into operational workflows is highly effective for continuously refining LLM-based customer support systems, enabling faster iteration and better performance.

Abstract: We introduce an Agent-in-the-Loop (AITL) framework that implements a
continuous data flywheel for iteratively improving an LLM-based customer
support system. Unlike standard offline approaches that rely on batch
annotations, AITL integrates four key types of annotations directly into live
customer operations: (1) pairwise response preferences, (2) agent adoption and
rationales, (3) knowledge relevance checks, and (4) identification of missing
knowledge. These feedback signals seamlessly feed back into models' updates,
reducing retraining cycles from months to weeks. Our production pilot involving
US-based customer support agents demonstrated significant improvements in
retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality
(+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore
the effectiveness of embedding human feedback loops directly into operational
workflows to continuously refine LLM-based customer support system.

</details>


### [243] [Inefficiencies of Meta Agents for Agent Design](https://arxiv.org/abs/2510.06711)
*Batu El,Mert Yuksekgonul,James Zou*

Main category: cs.AI

TL;DR: Meta-agents for automated agent design face three key challenges: poor learning across iterations (solved by evolutionary approach), low behavioral diversity limiting complementary use, and economic viability only in specific cases.


<details>
  <summary>Details</summary>
Motivation: To examine key challenges in meta-agents that automate the design of agentic systems, particularly focusing on learning efficiency, behavioral diversity, and economic viability.

Method: Analyzed three aspects: 1) compared context expansion vs evolutionary approaches for learning across iterations, 2) evaluated behavioral diversity of designed agents, 3) assessed economic viability by comparing design costs vs performance gains across datasets.

Result: Evolutionary approach outperforms context expansion; designed agents have low behavioral diversity; automated design is only economically viable for 2 datasets when deployed on large scale (15,000+ examples).

Conclusion: Current meta-agent approaches have significant limitations in learning efficiency, behavioral diversity, and economic viability, with automated design only justified in specific scenarios.

Abstract: Recent works began to automate the design of agentic systems using
meta-agents that propose and iteratively refine new agent architectures. In
this paper, we examine three key challenges in a common class of meta-agents.
First, we investigate how a meta-agent learns across iterations and find that
simply expanding the context with all previous agents, as proposed by previous
works, performs worse than ignoring prior designs entirely. We show that the
performance improves with an evolutionary approach. Second, although the
meta-agent designs multiple agents during training, it typically commits to a
single agent at test time. We find that the designed agents have low behavioral
diversity, limiting the potential for their complementary use. Third, we assess
when automated design is economically viable. We find that only in a few
cases--specifically, two datasets--the overall cost of designing and deploying
the agents is lower than that of human-designed agents when deployed on over
15,000 examples. In contrast, the performance gains for other datasets do not
justify the design cost, regardless of scale.

</details>


### [244] [MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2510.06742)
*Ali Sarabadani,Kheirolah Rahsepar Fard*

Main category: cs.AI

TL;DR: MultiCNKG is a novel framework that integrates cognitive neuroscience, gene ontology, and disease ontology knowledge graphs using LLMs, creating a unified knowledge graph with high precision and recall for biomedical applications.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional machine learning in capturing complex semantic relationships between genes, diseases, and cognitive processes by leveraging large language models for knowledge graph integration.

Method: Integrates three knowledge sources (CNKG, GO, DO) using LLMs like GPT-4 for entity alignment, semantic similarity computation, and graph augmentation to create a unified knowledge graph with multiple node and edge types.

Result: Created MultiCNKG with 6.9K nodes and 11.3K edges, achieving high metrics: precision (85.20%), recall (87.30%), coverage (92.18%), and competitive link prediction performance with TransE (MR: 391, MRR: 0.411) and RotatE (MR: 263, MRR: 0.395).

Conclusion: MultiCNKG provides a robust framework for connecting molecular to behavioral domains, advancing applications in personalized medicine, cognitive disorder diagnostics, and cognitive neuroscience research.

Abstract: The advent of large language models (LLMs) has revolutionized the integration
of knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming
limitations in traditional machine learning methods for capturing intricate
semantic links among genes, diseases, and cognitive processes. We introduce
MultiCNKG, an innovative framework that merges three key knowledge sources: the
Cognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges
across 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes
and 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)
comprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.
Leveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity
computation, and graph augmentation to create a cohesive KG that interconnects
genetic mechanisms, neurological disorders, and cognitive functions. The
resulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,
Diseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,
Associated with, Regulates), facilitating a multi-layered view from molecular
to behavioral domains. Assessments using metrics such as precision (85.20%),
recall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty
detection (40.28%), and expert validation (89.50%) affirm its robustness and
coherence. Link prediction evaluations with models like TransE (MR: 391, MRR:
0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against
benchmarks like FB15k-237 and WN18RR. This KG advances applications in
personalized medicine, cognitive disorder diagnostics, and hypothesis
formulation in cognitive neuroscience.

</details>


### [245] [Verifying Memoryless Sequential Decision-making of Large Language Models](https://arxiv.org/abs/2510.06756)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: A tool for automated verification of LLM-based policies in sequential decision-making tasks using MDPs and PCTL safety requirements.


<details>
  <summary>Details</summary>
Motivation: To enable rigorous and automated verification of large language model policies in memoryless sequential decision-making environments to ensure safety compliance.

Method: Incrementally constructs reachable MDP states guided by LLM actions, encodes states as natural language prompts, parses LLM responses into actions, and verifies the resulting model with Storm model checker.

Result: Open source LLMs via Ollama can be verified when deterministically seeded but generally underperform deep reinforcement learning baselines in grid world benchmarks.

Conclusion: The tool provides practical foundation for formally verifying increasingly capable LLMs in sequential decision-making tasks with native Ollama integration and PRISM task specification support.

Abstract: We introduce a tool for rigorous and automated verification of large language
model (LLM)- based policies in memoryless sequential decision-making tasks.
Given a Markov decision process (MDP) representing the sequential
decision-making task, an LLM policy, and a safety requirement expressed as a
PCTL formula, our approach incrementally constructs only the reachable portion
of the MDP guided by the LLM's chosen actions. Each state is encoded as a
natural language prompt, the LLM's response is parsed into an action, and
reachable successor states by the policy are expanded. The resulting formal
model is checked with Storm to determine whether the policy satisfies the
specified safety property. In experiments on standard grid world benchmarks, we
show that open source LLMs accessed via Ollama can be verified when
deterministically seeded, but generally underperform deep reinforcement
learning baselines. Our tool natively integrates with Ollama and supports
PRISM-specified tasks, enabling continuous benchmarking in user-specified
sequential decision-making tasks and laying a practical foundation for formally
verifying increasingly capable LLMs.

</details>


### [246] [Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration](https://arxiv.org/abs/2510.06761)
*Zhi Zhang,Yan Liu,Zhejing Hu,Gong Chen,Sheng-hua Zhong,Jiannong Cao*

Main category: cs.AI

TL;DR: DLMA framework automates scientific research using double-loop multi-agent system: leader loop evolves novel research plans, follower loop executes plans dynamically.


<details>
  <summary>Details</summary>
Motivation: Automating end-to-end scientific research requires both novel high-level planning and robust execution under uncertainty, presenting a bilevel challenge.

Method: Double-loop multi-agent framework with professor agents evolving plans via evolutionary algorithm meetings, and doctoral student agents executing plans with dynamic adjustments.

Result: DLMA achieves state-of-the-art scores on ACLAward and Laboratory benchmarks, significantly outperforming baselines in automated evaluation.

Conclusion: Both evolutionary planning and dynamic execution loops are critical - evolution drives novelty while execution ensures soundness in automated scientific research.

Abstract: Automating the end-to-end scientific research process poses a fundamental
challenge: it requires both evolving high-level plans that are novel and sound,
and executing these plans correctly amidst dynamic and uncertain conditions. To
address this bilevel challenge, we propose a novel Double-Loop Multi-Agent
(DLMA) framework to solve the given research problem automatically. The leader
loop, composed of professor agents, is responsible for evolving research plans.
It employs an evolutionary algorithm through involvement, improvement, and
integration meetings to iteratively generate and refine a pool of research
proposals, exploring the solution space effectively. The follower loop,
composed of doctoral student agents, is responsible for executing the
best-evolved plan. It dynamically adjusts the plan during implementation via
pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is
well-supported by contextual and external observations. Extensive experiments
on benchmarks like ACLAward and Laboratory show that DLMA generates research
papers that achieve state-of-the-art scores in automated evaluation,
significantly outperforming strong baselines. Ablation studies confirm the
critical roles of both loops, with evolution driving novelty and execution
ensuring soundness.

</details>


### [247] [Autoformalizer with Tool Feedback](https://arxiv.org/abs/2510.06857)
*Qi Guo,Jianing Wang,Jianfei Zhang,Deyang Kong,Xiangzhou Huang,Xiangyu Xi,Wei Wang,Jingang Wang,Xunliang Cai,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: ATF is an autoformalization approach that integrates tool feedback (syntax correction and consistency validation) to improve generation of valid formal statements, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing autoformalizers struggle with consistently generating syntactically valid and semantically consistent formal statements from natural language math problems.

Method: Proposes ATF with Lean 4 compilers for syntax correction and multi-LLMs-as-judge for consistency validation, using cold-start training, expert iteration, and Direct Preference Optimization.

Result: ATF markedly outperforms baseline formalizer models, shows excellent inference scaling, and includes open-source dataset Numina-ATF with 750K synthetic formal statements.

Conclusion: ATF effectively addresses syntactic and semantic challenges in autoformalization through tool feedback integration, advancing ATP research with improved performance and open resources.

Abstract: Autoformalization addresses the scarcity of data for Automated Theorem
Proving (ATP) by translating mathematical problems from natural language into
formal statements. Efforts in recent work shift from directly prompting large
language models to training an end-to-end formalizer model from scratch,
achieving remarkable advancements. However, existing formalizer still struggles
to consistently generate valid statements that meet syntactic validity and
semantic consistency. To address this issue, we propose the Autoformalizer with
Tool Feedback (ATF), a novel approach that incorporates syntactic and
consistency information as tools into the formalization process. By integrating
Lean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge
approach for consistency validation, the model is able to adaptively refine
generated statements according to the tool feedback, enhancing both syntactic
validity and semantic consistency. The training of ATF involves a cold-start
phase on synthetic tool-calling data, an expert iteration phase to improve
formalization capabilities, and Direct Preference Optimization to alleviate
ineffective revisions. Experimental results show that ATF markedly outperforms
a range of baseline formalizer models, with its superior performance further
validated by human evaluations. Subsequent analysis reveals that ATF
demonstrates excellent inference scaling properties. Moreover, we open-source
Numina-ATF, a dataset containing 750K synthetic formal statements to facilitate
advancements in autoformalization and ATP research.

</details>


### [248] [TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs](https://arxiv.org/abs/2510.06878)
*Daria Ozerova,Ekaterina Trofimova*

Main category: cs.AI

TL;DR: TGPR combines GRPO with Thompson-Sampling-based tree search to improve iterative refinement in LLMs, achieving significant performance gains on code generation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing iterative refinement methods rely on predefined heuristics that struggle with exploration-exploitation trade-offs and cannot adapt based on past refinement outcomes.

Method: Tree-Guided Policy Refinement (TGPR) framework combining GRPO with Thompson-Sampling-based tree search to explore both failed and successful refinement paths actively.

Result: Achieves up to +4.2 percentage points improvement in pass@1 on MBPP and +12.51 percentage points improvement in pass@10 on APPS compared to GRPO baseline.

Conclusion: TGPR provides a principled approach to combining learned policies with structured search methods, offering a general framework for enhancing iterative refinement and stateful reasoning in LLMs.

Abstract: Iterative refinement has been a promising paradigm to enable large language
models (LLMs) to resolve difficult reasoning and problem-solving tasks. One of
the key challenges, however, is how to effectively search through the enormous
search space of possible refinements. Existing methods typically fall back on
predefined heuristics, which are troubled by the exploration-exploitation
dilemma and cannot adapt based on past refinement outcomes. We introduce
Tree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with
a Thompson-Sampling-based tree search. TGPR explores both failed and successful
refinement paths actively, with denser training trajectories and more adaptive
policies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to
+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to
+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to
a competitive GRPO baseline. Apart from debugging code, TGPR focuses on a
principled approach to combining learned policies with structured search
methods, offering a general framework for enhancing iterative refinement and
stateful reasoning in LLMs.

</details>


### [249] [LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN](https://arxiv.org/abs/2510.06911)
*Hacane Hechehouche,Andre Antakli,Matthias Klusch*

Main category: cs.AI

TL;DR: The paper presents an integrated development environment to simplify the modeling of AJAN agents by addressing challenges with RDF/RDFS, SPARQL queries, and URIs, while leveraging Large Language Models to expand the user community.


<details>
  <summary>Details</summary>
Motivation: Current AJAN framework for multi-agent systems faces hurdles in defining RDF/RDFS and SPARQL-based agent behaviors, including error-prone URI handling and complex SPARQL queries that require high learning curves.

Method: The authors develop an integrated development environment that simplifies agent modeling by overcoming URI and SPARQL complexity issues, and incorporates Large Language Models to assist in agent engineering.

Result: The proposed IDE reduces the barriers to modeling AJAN agents by making RDF/RDFS and SPARQL more accessible and less error-prone, while expanding the framework's usability through LLM integration.

Conclusion: The integrated development environment successfully addresses key challenges in AJAN agent modeling, making the framework more accessible and extending its user community through Large Language Model capabilities.

Abstract: There are many established semantic Web standards for implementing
multi-agent driven applications. The AJAN framework allows to engineer
multi-agent systems based on these standards. In particular, agent knowledge is
represented in RDF/RDFS and OWL, while agent behavior models are defined with
Behavior Trees and SPARQL to access and manipulate this knowledge. However, the
appropriate definition of RDF/RDFS and SPARQL-based agent behaviors still
remains a major hurdle not only for agent modelers in practice. For example,
dealing with URIs is very error-prone regarding typos and dealing with complex
SPARQL queries in large-scale environments requires a high learning curve. In
this paper, we present an integrated development environment to overcome such
hurdles of modeling AJAN agents and at the same time to extend the user
community for AJAN by the possibility to leverage Large Language Models for
agent engineering.

</details>


### [250] [Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.06953)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.AI

TL;DR: The paper revisits the Uniform Information Density (UID) hypothesis in LLM reasoning traces, finding that step-level uniformity correlates with reasoning quality and can improve accuracy by 10-32% when used for trace selection.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the Uniform Information Density hypothesis applies to LLM reasoning traces and whether step-level information uniformity reflects reasoning quality.

Method: Proposed an entropy-based stepwise information density metric and introduced two complementary uniformity measures (local and global uniformity scores), tested across six reasoning benchmarks.

Result: Step-level uniformity provides strong theoretical insights and practical benefits - selecting traces with uniform information density improves accuracy by 10-32% relative gains. Correct reasoning traces avoid sharp information spikes while incorrect ones show irregular bursts.

Conclusion: UID-inspired information density measures outperform alternative internal signals as predictors of reasoning quality, making uniformity a robust diagnostic and selection criterion for building more reliable reasoning systems.

Abstract: The Uniform Information Density (UID) hypothesis suggests that effective
communication maintains a stable flow of information. In this work, we revisit
this principle in the context of large language model (LLM) reasoning traces,
asking whether step-level uniformity reflects reasoning quality. To this end,
we propose an entropy-based stepwise information density metric and introduce
two complementary measures of uniformity, local and global uniformity scores.
Across the experiments on six different reasoning benchmarks, we find that
step-level uniformity not only provides a strong theoretical lens but also
yields practical performance benefits; for example, selecting reasoning traces
with more uniform information density at the step-level improves accuracy by
10-32\% relative gains over baselines at AIME2025. Our analysis further reveals
that correct reasoning traces tend to avoid sharp information density spikes,
while incorrect traces exhibit irregular information bursts. These results
demonstrate that UID-inspired information density measures outperform
alternative internal signals as predictors of reasoning quality. Results
highlight the uniformity of the information density as a robust diagnostic and
selection criterion for building more reliable and accurate reasoning systems.

</details>


### [251] [Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning](https://arxiv.org/abs/2510.07038)
*Wenxun Wu,Yuanyang Li,Guhan Chen,Linyue Wang,Hongyang Chen*

Main category: cs.AI

TL;DR: TAPO is a reinforcement learning framework that combines multi-hop reasoning with adaptive tool-calling capabilities, achieving state-of-the-art performance on knowledge-intensive and computational tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with tasks requiring up-to-date knowledge or computational tools like calculators and code interpreters for complex arithmetic operations.

Method: Proposes Tool-Augmented Policy Optimization (TAPO), a modified version of Dynamic Sampling Policy Optimization adapted for tool invocation scenarios, enabling dynamic interleaving of reasoning with tool usage.

Result: TAPO achieves state-of-the-art performance on tasks requiring external knowledge and mathematical computation, with more efficient tool utilization than baseline methods while preventing excessive calls.

Conclusion: Combining advanced reasoning with tool usage significantly enhances model performance in knowledge-intensive and computationally demanding tasks.

Abstract: Recent advances in large language models (LLMs) have popularized test-time
scaling, where models generate additional reasoning tokens before producing
final answers. These approaches have demonstrated significant performance
improvements on benchmarks involving mathematical reasoning. However, language
models relying solely on direct inference still struggle with tasks demanding
up-to-date knowledge or computational tools such as calculators and code
interpreters for complex arithmetic operations. To overcome these limitations,
we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement
learning framework that systematically integrates multi-hop reasoning with
adaptive tool-calling capabilities. Our approach employs a modified version of
Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,
which we adapt specifically for tool invocation scenarios, enabling models to
dynamically interleave complex reasoning with on-demand tool usage (including
search APIs and Python interpreters).
  To support this research, we introduce two new datasets: TAPO-easy-60K and
TAPO-hard-18K, specifically designed to train and evaluate both fact-based
reasoning and mathematical calculation capabilities. Our experiments on
Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,
with both models achieving state-of-the-art performance on tasks requiring
external knowledge and mathematical computation among methods with comparable
parameters. Notably, TAPO achieves more efficient tool utilization than
baseline methods while preventing excessive calls caused by reward hacking.
These results highlight the significant potential of combining advanced
reasoning with tool usage to enhance model performance in knowledge-intensive
and computationally demanding tasks.

</details>


### [252] [Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations](https://arxiv.org/abs/2510.07064)
*Manh Hung Nguyen,Sebastian Tschiatschek,Adish Singla*

Main category: cs.AI

TL;DR: Proposes a framework to create diverse LLM agents that collectively represent human population diversity by selecting representative agents through submodular optimization, instead of using a single homogeneous LLM.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce homogeneous outputs that fail to capture the rich diversity of human perspectives and behaviors, making them poor proxies for human populations despite their potential as alternatives to expensive human data collection.

Method: Constructs a set of LLM agents where each agent is steered by conditioning on a small set of human demonstrations through in-context learning, and uses submodular optimization to select representative agents from the exponentially large space of possible agents.

Result: Extensive experiments show the approach constructs agents that more effectively represent human populations compared to baselines, and behavioral analyses demonstrate these agents reproduce behavior patterns and perspectives of the target populations.

Conclusion: The proposed framework successfully creates diverse LLM agents that collectively capture human population diversity, addressing the homogeneity problem of single LLM approaches while maintaining computational efficiency through submodular optimization.

Abstract: The difficulty and expense of obtaining large-scale human responses make
Large Language Models (LLMs) an attractive alternative and a promising proxy
for human behavior. However, prior work shows that LLMs often produce
homogeneous outputs that fail to capture the rich diversity of human
perspectives and behaviors. Thus, rather than trying to capture this diversity
with a single LLM agent, we propose a novel framework to construct a set of
agents that collectively capture the diversity of a given human population.
Each agent is an LLM whose behavior is steered by conditioning on a small set
of human demonstrations (task-response pairs) through in-context learning. The
central challenge is therefore to select a representative set of LLM agents
from the exponentially large space of possible agents. We tackle this selection
problem from the lens of submodular optimization. In particular, we develop
methods that offer different trade-offs regarding time complexity and
performance guarantees. Extensive experiments in crowdsourcing and educational
domains demonstrate that our approach constructs agents that more effectively
represent human populations compared to baselines. Moreover, behavioral
analyses on new tasks show that these agents reproduce the behavior patterns
and perspectives of the students and annotators they are designed to represent.

</details>


### [253] [Inductive Learning for Possibilistic Logic Programs Under Stable Models](https://arxiv.org/abs/2510.07069)
*Hongbo Hu,Yisong Wang,Yi Huang,Kewen Wang*

Main category: cs.AI

TL;DR: This paper presents an inductive learning approach for possibilistic logic programs (poss-programs) under stable models, defining induction tasks and providing algorithms (ilpsm and ilpsmmin) to extract poss-programs from background programs and examples.


<details>
  <summary>Details</summary>
Motivation: While possibilistic stable models and their properties have been well studied, the problem of inductive reasoning for poss-programs has not been investigated yet, creating a gap in the research.

Method: The authors formally define induction tasks, investigate their properties, and present two algorithms (ilpsm and ilpsmmin) for computing induction solutions. An implementation of ilpsmmin is also provided.

Result: Experimental results show that when inputs are ordinary logic programs, the prototype implementation outperforms a major inductive learning system for normal logic programs from stable models on randomly generated datasets.

Conclusion: The paper successfully addresses the inductive reasoning problem for poss-programs by providing formal definitions, algorithms, and empirical evidence showing competitive performance compared to existing systems.

Abstract: Possibilistic logic programs (poss-programs) under stable models are a major
variant of answer set programming (ASP). While its semantics (possibilistic
stable models) and properties have been well investigated, the problem of
inductive reasoning has not been investigated yet. This paper presents an
approach to extracting poss-programs from a background program and examples
(parts of intended possibilistic stable models). To this end, the notion of
induction tasks is first formally defined, its properties are investigated and
two algorithms ilpsm and ilpsmmin for computing induction solutions are
presented. An implementation of ilpsmmin is also provided and experimental
results show that when inputs are ordinary logic programs, the prototype
outperforms a major inductive learning system for normal logic programs from
stable models on the datasets that are randomly generated.

</details>


### [254] [VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems](https://arxiv.org/abs/2510.07073)
*André Hottung,Federico Berto,Chuanbo Hua,Nayeli Gast Zepeda,Daniel Wetzel,Michael Römer,Haoran Ye,Davide Zago,Michael Poli,Stefano Massaroli,Jinkyoo Park,Kevin Tierney*

Main category: cs.AI

TL;DR: VRPAgent is a framework that uses LLM-generated components within a metaheuristic and refines them through genetic search to automatically discover high-performing heuristic operators for vehicle routing problems.


<details>
  <summary>Details</summary>
Motivation: Designing effective heuristics for VRPs requires significant domain expertise, and current LLM-based code generation approaches still fall short of human-crafted heuristics.

Method: Integrates LLM-generated problem-specific operators into a generic metaheuristic framework and refines them through novel genetic search, keeping tasks manageable while ensuring correctness.

Result: Outperforms handcrafted methods and recent learning-based approaches across multiple VRP variants (capacitated VRP, VRP with time windows, prize-collecting VRP) using only a single CPU core.

Conclusion: VRPAgent is the first LLM-based paradigm to advance state-of-the-art in VRPs, demonstrating promising future for automated heuristics discovery.

Abstract: Designing high-performing heuristics for vehicle routing problems (VRPs) is a
complex task that requires both intuition and deep domain knowledge. Large
language model (LLM)-based code generation has recently shown promise across
many domains, but it still falls short of producing heuristics that rival those
crafted by human experts. In this paper, we propose VRPAgent, a framework that
integrates LLM-generated components into a metaheuristic and refines them
through a novel genetic search. By using the LLM to generate problem-specific
operators, embedded within a generic metaheuristic framework, VRPAgent keeps
tasks manageable, guarantees correctness, and still enables the discovery of
novel and powerful strategies. Across multiple problems, including the
capacitated VRP, the VRP with time windows, and the prize-collecting VRP, our
method discovers heuristic operators that outperform handcrafted methods and
recent learning-based approaches while requiring only a single CPU core. To our
knowledge, \VRPAgent is the first LLM-based paradigm to advance the
state-of-the-art in VRPs, highlighting a promising future for automated
heuristics discovery.

</details>


### [255] [The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas](https://arxiv.org/abs/2510.07091)
*Baixuan Xu,Tianshi Zheng,Zhaowei Wang,Hong Ting Tsang,Weiqi Wang,Tianqing Fang,Yangqiu Song*

Main category: cs.AI

TL;DR: This paper studies optimal action representations for long-horizon agents in combinatorial action spaces, comparing Planning with Actions (PwA) vs Planning with Schemas (PwS), identifying an inflection point between different environment complexities.


<details>
  <summary>Details</summary>
Motivation: Conventional action-based planning becomes impractical when environmental action spaces are combinatorially exploded (e.g., open-ended real world), necessitating scalable action representations for long-horizon autonomy.

Method: Systematic comparison of two action representations: PwA (direct action lists) and PwS (schema instantiation). Proposed cognitive bandwidth framework and conducted controlled experiments across ALFWorld (~35 actions) and SciWorld (~500 actions) environments.

Result: Identified a representation-choice inflection point between ALFWorld and SciWorld, showing PwA works better for smaller action spaces while PwS becomes necessary for larger spaces. Model capacity affects inflection point location: stronger planning shifts it rightward, better schema instantiation shifts it leftward.

Conclusion: PwS provides better scalability for combinatorial action spaces but currently has suboptimal performance. The paper provides actionable guidance for building more capable PwS agents to achieve better scalable autonomy.

Abstract: Enabling LLMs to effectively operate long-horizon task which requires
long-term planning and multiple interactions is essential for open-world
autonomy. Conventional methods adopt planning with actions where a executable
action list would be provided as reference. However, this action representation
choice would be impractical when the environment action space is combinatorial
exploded (e.g., open-ended real world). This naturally leads to a question: As
environmental action space scales, what is the optimal action representation
for long-horizon agents? In this paper, we systematically study the
effectiveness of two different action representations. The first one is
conventional planning with actions (PwA) which is predominantly adopted for its
effectiveness on existing benchmarks. The other one is planning with schemas
(PwS) which instantiate an action schema into action lists (e.g., "move [OBJ]
to [OBJ]" -> "move apple to desk") to ensure concise action space and reliable
scalability. This alternative is motivated by its alignment with human
cognition and its compliance with environment-imposed action format
restriction. We propose cognitive bandwidth perspective as a conceptual
framework to qualitatively understand the differences between these two action
representations and empirically observe a representation-choice inflection
point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve
as evidence of the need for scalable representations. We further conduct
controlled experiments to study how the location of this inflection point
interacts with different model capacities: stronger planning proficiency shifts
the inflection rightward, whereas better schema instantiation shifts it
leftward. Finally, noting the suboptimal performance of PwS agents, we provide
an actionable guide for building more capable PwS agents for better scalable
autonomy.

</details>


### [256] [The Contingencies of Physical Embodiment Allow for Open-Endedness and Care](https://arxiv.org/abs/2510.07117)
*Leonardo Christov-Moore,Arthur Juliani,Alex Kiefer,Nicco Reggente,B. Scott Rousse,Adam Safron,Nicol'as Hinrichs,Daniel Polani,Antonio Damasio*

Main category: cs.AI

TL;DR: The paper proposes that physical vulnerability and mortality (being-towards-death) combined with embodiment (being-in-the-world) create intrinsic drives for artificial agents to maintain integrity and maximize control, enabling open-ended adaptation and care.


<details>
  <summary>Details</summary>
Motivation: Biological organisms adapt and care for each other in open-ended environments with ease, while artificial agents struggle. Understanding the conditions of life can help develop more robust, adaptive, and caring AI agents.

Method: Defines two minimal conditions from Heidegger's phenomenology: being-in-the-world and being-towards-death. Formalizes these concepts in reinforcement learning framework to examine how intrinsically driven embodied agents develop capacities for open-endedness and care.

Result: From these conditions emerge both homeostatic drive (maintaining integrity) and intrinsic drive to maximize control over future states (empowerment), enhancing agents' ability to meet future needs.

Conclusion: Physical vulnerability and mortality, when properly framed as existential conditions, can drive artificial agents to develop robust adaptation, open-ended learning, and caring behaviors similar to biological organisms.

Abstract: Physical vulnerability and mortality are often seen as obstacles to be
avoided in the development of artificial agents, which struggle to adapt to
open-ended environments and provide aligned care. Meanwhile, biological
organisms survive, thrive, and care for each other in an open-ended physical
world with relative ease and efficiency. Understanding the role of the
conditions of life in this disparity can aid in developing more robust,
adaptive, and caring artificial agents. Here we define two minimal conditions
for physical embodiment inspired by the existentialist phenomenology of Martin
Heidegger: being-in-the-world (the agent is a part of the environment) and
being-towards-death (unless counteracted, the agent drifts toward terminal
states due to the second law of thermodynamics). We propose that from these
conditions we can obtain both a homeostatic drive - aimed at maintaining
integrity and avoiding death by expending energy to learn and act - and an
intrinsic drive to continue to do so in as many ways as possible. Drawing
inspiration from Friedrich Nietzsche's existentialist concept of will-to-power,
we examine how intrinsic drives to maximize control over future states, e.g.,
empowerment, allow agents to increase the probability that they will be able to
meet their future homeostatic needs, thereby enhancing their capacity to
maintain physical integrity. We formalize these concepts within a reinforcement
learning framework, which enables us to examine how intrinsically driven
embodied agents learning in open-ended multi-agent environments may cultivate
the capacities for open-endedness and care.ov

</details>


### [257] [Integrating Domain Knowledge into Process Discovery Using Large Language Models](https://arxiv.org/abs/2510.07161)
*Ali Norouzifar,Humam Kourani,Marcus Dees,Wil van der Aalst*

Main category: cs.AI

TL;DR: An interactive process discovery framework that incorporates domain knowledge via LLMs to extract declarative rules from natural language, guiding process model discovery to avoid structures contradicting expert knowledge.


<details>
  <summary>Details</summary>
Motivation: Traditional process discovery from event logs alone is unreliable due to incomplete/noisy data and disregard of domain knowledge, leading to models unsuitable for downstream tasks.

Method: Interactive framework using LLMs to extract declarative rules from textual domain knowledge, integrated with IMr discovery algorithm that combines event log insights and extracted rules.

Result: Fully implemented tool with extensive evaluation of LLMs and prompt strategies, including real-life case study where domain experts assessed usability and effectiveness.

Conclusion: The framework successfully incorporates domain knowledge into process discovery, improving model reliability by avoiding structures that contradict expert knowledge.

Abstract: Process discovery aims to derive process models from event logs, providing
insights into operational behavior and forming a foundation for conformance
checking and process improvement. However, models derived solely from event
data may not accurately reflect the real process, as event logs are often
incomplete or affected by noise, and domain knowledge, an important
complementary resource, is typically disregarded. As a result, the discovered
models may lack reliability for downstream tasks. We propose an interactive
framework that incorporates domain knowledge, expressed in natural language,
into the process discovery pipeline using Large Language Models (LLMs). Our
approach leverages LLMs to extract declarative rules from textual descriptions
provided by domain experts. These rules are used to guide the IMr discovery
algorithm, which recursively constructs process models by combining insights
from both the event log and the extracted rules, helping to avoid problematic
process structures that contradict domain knowledge. The framework coordinates
interactions among the LLM, domain experts, and a set of backend services. We
present a fully implemented tool that supports this workflow and conduct an
extensive evaluation of multiple LLMs and prompt engineering strategies. Our
empirical study includes a case study based on a real-life event log with the
involvement of domain experts, who assessed the usability and effectiveness of
the framework.

</details>


### [258] [NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents](https://arxiv.org/abs/2510.07172)
*Tianshi Zheng,Kelvin Kiu-Wai Tam,Newt Hue-Nam K. Nguyen,Baixuan Xu,Zhaowei Wang,Jiayang Cheng,Hong Ting Tsang,Weiqi Wang,Jiaxin Bai,Tianqing Fang,Yangqiu Song,Ginny Y. Wong,Simon See*

Main category: cs.AI

TL;DR: NewtonBench is a new benchmark with 324 scientific law discovery tasks across 12 physics domains that addresses limitations of existing benchmarks by using metaphysical shifts to create scalable, scientifically relevant, and memorization-resistant problems, while elevating evaluation from static function fitting to interactive model discovery.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for scientific law discovery suffer from a methodological trilemma forcing trade-offs between scientific relevance, scalability, and resistance to memorization, and oversimplify discovery as static function fitting rather than capturing the authentic interactive scientific process.

Method: The benchmark uses metaphysical shifts - systematic alterations of canonical laws - to generate problems, and elevates evaluation to interactive model discovery where agents must experimentally probe simulated complex systems to uncover hidden principles.

Result: Experiments reveal frontier LLMs have clear but fragile discovery capability that degrades with system complexity and is sensitive to observational noise. Tool assistance paradoxically hinders capable models by causing premature shift from exploration to exploitation.

Conclusion: Robust, generalizable discovery in complex interactive environments remains a core challenge. NewtonBench provides a scalable, robust, and scientifically authentic testbed for measuring progress and guiding development of AI agents capable of genuine scientific discovery.

Abstract: Large language models are emerging as powerful tools for scientific law
discovery, a foundational challenge in AI-driven science. However, existing
benchmarks for this task suffer from a fundamental methodological trilemma,
forcing a trade-off between scientific relevance, scalability, and resistance
to memorization. Furthermore, they oversimplify discovery as static function
fitting, failing to capture the authentic scientific process of uncovering
embedded laws through the interactive exploration of complex model systems. To
address these critical gaps, we introduce NewtonBench, a benchmark comprising
324 scientific law discovery tasks across 12 physics domains. Our design
mitigates the evaluation trilemma by using metaphysical shifts - systematic
alterations of canonical laws - to generate a vast suite of problems that are
scalable, scientifically relevant, and memorization-resistant. Moreover, we
elevate the evaluation from static function fitting to interactive model
discovery, requiring agents to experimentally probe simulated complex systems
to uncover hidden principles. Our extensive experiment reveals a clear but
fragile capability for discovery in frontier LLMs: this ability degrades
precipitously with increasing system complexity and exhibits extreme
sensitivity to observational noise. Notably, we uncover a paradoxical effect of
tool assistance: providing a code interpreter can hinder more capable models by
inducing a premature shift from exploration to exploitation, causing them to
satisfice on suboptimal solutions. These results demonstrate that robust,
generalizable discovery in complex, interactive environments remains the core
challenge. By providing a scalable, robust, and scientifically authentic
testbed, NewtonBench offers a crucial tool for measuring true progress and
guiding the development of next-generation AI agents capable of genuine
scientific discovery.

</details>


### [259] [Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences](https://arxiv.org/abs/2510.07276)
*Pulkit Rustagi,Kyle Hollins Wray,Sandhya Saisubramanian*

Main category: cs.AI

TL;DR: Proposes Lexicographic Conflict-Based Search (LCBS) for multi-objective multi-agent path finding that directly computes solutions aligned with user preferences, avoiding Pareto frontier construction and scaling to 10 objectives.


<details>
  <summary>Details</summary>
Motivation: Current MO-MAPF algorithms don't optimize for user-defined preferences even when available, and scale poorly with increasing objectives by computing Pareto frontiers.

Method: LCBS integrates priority-aware low-level A* search with conflict-based search, using lexicographic preferences over objectives to guide planning without constructing Pareto frontiers.

Result: LCBS computes optimal solutions and scales to instances with up to ten objectives, showing consistently higher success rates than state-of-the-art baselines, especially with more objectives.

Conclusion: The lexicographic framework and LCBS algorithm enable efficient multi-objective planning that directly incorporates user preferences and scales beyond current MO-MAPF methods.

Abstract: Many real-world scenarios require multiple agents to coordinate in shared
environments, while balancing trade-offs between multiple, potentially
competing objectives. Current multi-objective multi-agent path finding
(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto
frontiers. They do not explicitly optimize for user-defined preferences, even
when the preferences are available, and scale poorly with the number of
objectives. We propose a lexicographic framework for modeling MO-MAPF, along
with an algorithm \textit{Lexicographic Conflict-Based Search} (LCBS) that
directly computes a single solution aligned with a lexicographic preference
over objectives. LCBS integrates a priority-aware low-level $A^*$ search with
conflict-based search, avoiding Pareto frontier construction and enabling
efficient planning guided by preference over objectives. We provide insights
into optimality and scalability, and empirically demonstrate that LCBS computes
optimal solutions while scaling to instances with up to ten objectives -- far
beyond the limits of existing MO-MAPF methods. Evaluations on standard and
randomized MAPF benchmarks show consistently higher success rates against
state-of-the-art baselines, especially with increasing number of objectives.

</details>


### [260] [Agentic generative AI for media content discovery at the national football league](https://arxiv.org/abs/2510.07297)
*Henry Wang,Sirajus Salekin,Jake Lee,Ross Claytor,Shinan Zhang,Michael Chi*

Main category: cs.AI

TL;DR: A generative AI workflow for the NFL that converts natural language queries into database queries to find relevant historical plays, achieving 95% accuracy and reducing search time from 10 minutes to 30 seconds.


<details>
  <summary>Details</summary>
Motivation: To enable media researchers and analysts to query historical NFL plays using natural language instead of traditional filter-and-click interfaces, improving content discovery and management.

Method: An agentic workflow that takes user queries, breaks them into elements, translates them into database query language, and uses semantic caching to improve accuracy and latency.

Result: The solution achieves over 95% accuracy and reduces average search time from 10 minutes to 30 seconds, significantly increasing operational efficiency.

Conclusion: Generative AI enables efficient content discovery, allowing NFL analysts to focus on creative content production and engaging storylines rather than time-consuming search tasks.

Abstract: Generative AI has unlocked new possibilities in content discovery and
management. Through collaboration with the National Football League (NFL), we
demonstrate how a generative-AI based workflow enables media researchers and
analysts to query relevant historical plays using natural language rather than
traditional filter-and-click interfaces. The agentic workflow takes a user
query as input, breaks it into elements, and translates them into the
underlying database query language. Accuracy and latency are further improved
through carefully designed semantic caching. The solution achieves over 95
percent accuracy and reduces the average time to find relevant videos from 10
minutes to 30 seconds, significantly increasing the NFL's operational
efficiency and allowing users to focus on producing creative content and
engaging storylines.

</details>
