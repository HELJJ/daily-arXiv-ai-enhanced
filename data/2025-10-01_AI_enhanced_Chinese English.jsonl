{"id": "2509.25229", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25229", "abs": "https://arxiv.org/abs/2509.25229", "authors": ["Lukas Petersson", "Axel Backlund", "Axel Wennst\u00f6m", "Hanna Petersson", "Callum Sharrock", "Arash Dabiri"], "title": "Blueprint-Bench: Comparing spatial intelligence of LLMs, agents and image models", "comment": "9 pages, 8 figures, submitted for ICLR 2026", "summary": "We introduce Blueprint-Bench, a benchmark designed to evaluate spatial\nreasoning capabilities in AI models through the task of converting apartment\nphotographs into accurate 2D floor plans. While the input modality\n(photographs) is well within the training distribution of modern multimodal\nmodels, the task of spatial reconstruction requires genuine spatial\nintelligence: inferring room layouts, understanding connectivity, and\nmaintaining consistent scale. We evaluate leading language models (GPT-5,\nClaude 4 Opus, Gemini 2.5 Pro, Grok-4), image generation models (GPT-Image,\nNanoBanana), and agent systems (Codex CLI, Claude Code) on a dataset of 50\napartments with approximately 20 interior images each. Our scoring algorithm\nmeasures similarity between generated and ground-truth floor plans based on\nroom connectivity graphs and size rankings. Results reveal a significant blind\nspot in current AI capabilities: most models perform at or below a random\nbaseline, while human performance remains substantially superior. Image\ngeneration models particularly struggle with instruction following, while\nagent-based approaches with iterative refinement capabilities show no\nmeaningful improvement over single-pass generation. Blueprint-Bench provides\nthe first numerical framework for comparing spatial intelligence across\ndifferent model architectures. We will continue evaluating new models as they\nare released and welcome community submissions, monitoring for the emergence of\nspatial intelligence in generalist AI systems.", "AI": {"tldr": "Blueprint-Bench is a benchmark for evaluating AI spatial reasoning by converting apartment photos to 2D floor plans, revealing current models perform poorly while humans excel.", "motivation": "To assess genuine spatial intelligence in AI models through a task that requires inferring room layouts, understanding connectivity, and maintaining consistent scale from photographs.", "method": "Evaluated leading language models, image generation models, and agent systems on 50 apartments with ~20 images each, using scoring based on room connectivity graphs and size rankings.", "result": "Most AI models perform at or below random baseline, with image generation models struggling with instruction following and agent approaches showing no improvement over single-pass generation.", "conclusion": "Blueprint-Bench provides the first numerical framework for comparing spatial intelligence across model architectures, revealing a significant blind spot in current AI capabilities."}}
{"id": "2509.25236", "categories": ["cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25236", "abs": "https://arxiv.org/abs/2509.25236", "authors": ["Gabriele D'Acunto", "Paolo Di Lorenzo", "Sergio Barbarossa"], "title": "The Causal Abstraction Network: Theory and Learning", "comment": null, "summary": "Causal artificial intelligence aims to enhance explainability,\ntrustworthiness, and robustness in AI by leveraging structural causal models\n(SCMs). In this pursuit, recent advances formalize network sheaves of causal\nknowledge. Pushing in the same direction, we introduce the causal abstraction\nnetwork (CAN), a specific instance of such sheaves where (i) SCMs are Gaussian,\n(ii) restriction maps are transposes of constructive linear causal abstractions\n(CAs), and (iii) edge stalks correspond -- up to rotation -- to the node stalks\nof more detailed SCMs. We investigate the theoretical properties of CAN,\nincluding algebraic invariants, cohomology, consistency, global sections\ncharacterized via the Laplacian kernel, and smoothness. We then tackle the\nlearning of consistent CANs. Our problem formulation separates into\nedge-specific local Riemannian problems and avoids nonconvex, costly\nobjectives. We propose an efficient search procedure as a solution, solving the\nlocal problems with SPECTRAL, our iterative method with closed-form updates and\nsuitable for positive definite and semidefinite covariance matrices.\nExperiments on synthetic data show competitive performance in the CA learning\ntask, and successful recovery of diverse CAN structures.", "AI": {"tldr": "This paper introduces Causal Abstraction Networks (CANs) as a specific type of causal knowledge sheaves with Gaussian SCMs, develops theoretical properties, and proposes an efficient learning method called SPECTRAL for consistent CANs.", "motivation": "To enhance explainability, trustworthiness, and robustness in AI through causal modeling using structural causal models and formal network sheaves of causal knowledge.", "method": "Propose CANs with Gaussian SCMs, restriction maps as transposes of constructive linear causal abstractions, and edge stalks corresponding to node stalks of detailed SCMs. Develop SPECTRAL method with closed-form updates for learning consistent CANs.", "result": "Theoretical analysis of CAN properties (algebraic invariants, cohomology, consistency, global sections, smoothness). Experiments show competitive CA learning performance and successful recovery of diverse CAN structures on synthetic data.", "conclusion": "CANs provide a formal framework for causal knowledge representation with efficient learning methods, advancing causal AI capabilities for explainable and robust systems."}}
{"id": "2509.25239", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25239", "abs": "https://arxiv.org/abs/2509.25239", "authors": ["Kevin Xu", "Issei Sato"], "title": "A Formal Comparison Between Chain-of-Thought and Latent Thought", "comment": null, "summary": "Chain-of-Thought (CoT) elicits reasoning in large language models by\nexplicitly generating intermediate steps in natural language. In contrast,\nLatent Thought in looped models operates directly in the continuous latent\nspace, enabling computation beyond discrete linguistic representations. While\nboth approaches exploit iterative computation, their comparative capabilities\nremain underexplored. In this work, we present a formal analysis showing that\nLatent Thought in Looped Transformers enables parallel computation, which is\nmore efficient than the inherently sequential process of CoT. In contrast, CoT\nleverages stochastic decoding to approximate solutions to problems where exact\ncomputation is intractable. These separations suggest the tasks for which\ndepth-driven recursion is more suitable, thereby offering practical guidance\nfor choosing between reasoning paradigms. Code is available at\nhttps://github.com/kevin671/cot-vs-loop.", "AI": {"tldr": "Latent Thought in looped models enables parallel computation, while CoT uses sequential reasoning with stochastic decoding for intractable problems.", "motivation": "To compare the capabilities of Chain-of-Thought (CoT) and Latent Thought approaches in large language models, as their comparative strengths remain underexplored.", "method": "Formal analysis of Latent Thought in Looped Transformers for parallel computation versus CoT's sequential reasoning with stochastic decoding.", "result": "Latent Thought enables more efficient parallel computation, while CoT is better for approximating solutions to intractable problems through stochastic decoding.", "conclusion": "The analysis provides practical guidance for choosing between reasoning paradigms based on task requirements, with Latent Thought suitable for depth-driven recursion and CoT for sequential approximation."}}
{"id": "2509.25244", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.25244", "abs": "https://arxiv.org/abs/2509.25244", "authors": ["Shuide Wen", "Beier Ku", "Teng Wang", "Mingyang Zou", "Yang Yang"], "title": "Neo-Grounded Theory: A Methodological Innovation Integrating High-Dimensional Vector Clustering and Multi-Agent Collaboration for Qualitative Research", "comment": "44 pages, 11 figures", "summary": "Purpose: Neo Grounded Theory (NGT) integrates vector clustering with multi\nagent systems to resolve qualitative research's scale depth paradox, enabling\nanalysis of massive datasets in hours while preserving interpretive rigor.\nMethods: We compared NGT against manual coding and ChatGPT-assisted analysis\nusing 40,000 character Chinese interview transcripts. NGT employs\n1536-dimensional embeddings, hierarchical clustering, and parallel agent-based\ncoding. Two experiments tested pure automation versus human guided refinement.\nFindings: NGT achieved 168-fold speed improvement (3 hours vs 3 weeks),\nsuperior quality (0.904 vs 0.883), and 96% cost reduction. Human AI\ncollaboration proved essential: automation alone produced abstract frameworks\nwhile human guidance yielded actionable dual pathway theories. The system\ndiscovered patterns invisible to manual coding, including identity bifurcation\nphenomena. Contributions: NGT demonstrates computational objectivity and human\ninterpretation are complementary. Vector representations provide reproducible\nsemantic measurement while preserving meaning's interpretive dimensions.\nResearchers shift from mechanical coding to theoretical guidance, with AI\nhandling pattern recognition while humans provide creative insight.\nImplications: Cost reduction from \\$50,000 to \\$500 democratizes qualitative\nresearch, enabling communities to study themselves. Real-time analysis makes\nqualitative insights contemporaneous with events. The framework shows\ncomputational methods can strengthen rather than compromise qualitative\nresearch's humanistic commitments.\n  Keywords: Grounded theory; Vector embeddings; Multi agent systems; Human AI\ncollaboration; Computational qualitative analysis", "AI": {"tldr": "NGT combines vector clustering and multi-agent systems to solve qualitative research's scale-depth paradox, enabling rapid analysis of large datasets while maintaining interpretive rigor.", "motivation": "To resolve the tension between scale and depth in qualitative research by developing a computational method that can analyze massive datasets quickly while preserving the interpretive quality of traditional grounded theory.", "method": "Used 1536-dimensional embeddings, hierarchical clustering, and parallel agent-based coding on 40,000 character Chinese interview transcripts. Compared NGT against manual coding and ChatGPT-assisted analysis through two experiments testing pure automation versus human-guided refinement.", "result": "NGT achieved 168-fold speed improvement (3 hours vs 3 weeks), superior quality (0.904 vs 0.883), and 96% cost reduction. Human-AI collaboration was essential - automation alone produced abstract frameworks while human guidance yielded actionable dual pathway theories. Discovered patterns invisible to manual coding, including identity bifurcation phenomena.", "conclusion": "Computational objectivity and human interpretation are complementary. Vector representations provide reproducible semantic measurement while preserving meaning's interpretive dimensions. Researchers shift from mechanical coding to theoretical guidance, with AI handling pattern recognition while humans provide creative insight. Cost reduction democratizes qualitative research and enables real-time analysis."}}
{"id": "2509.25198", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25198", "abs": "https://arxiv.org/abs/2509.25198", "authors": ["Elbert Ho"], "title": "SOLD: SELFIES-based Objective-driven Latent Diffusion", "comment": null, "summary": "Recently, machine learning has made a significant impact on de novo drug\ndesign. However, current approaches to creating novel molecules conditioned on\na target protein typically rely on generating molecules directly in the 3D\nconformational space, which are often slow and overly complex. In this work, we\npropose SOLD (SELFIES-based Objective-driven Latent Diffusion), a novel latent\ndiffusion model that generates molecules in a latent space derived from 1D\nSELFIES strings and conditioned on a target protein. In the process, we also\ntrain an innovative SELFIES transformer and propose a new way to balance losses\nwhen training multi-task machine learning models.Our model generates\nhigh-affinity molecules for the target protein in a simple and efficient way,\nwhile also leaving room for future improvements through the addition of more\ndata.", "AI": {"tldr": "SOLD is a latent diffusion model that generates drug molecules in latent space from SELFIES strings, conditioned on target proteins, offering efficient high-affinity molecule generation.", "motivation": "Current methods for de novo drug design generate molecules directly in 3D conformational space, which are slow and overly complex.", "method": "Proposed SOLD model using latent diffusion in space derived from 1D SELFIES strings, trained with SELFIES transformer and balanced multi-task loss approach.", "result": "Model generates high-affinity molecules for target proteins in simple and efficient manner.", "conclusion": "SOLD provides efficient drug molecule generation with room for future improvements through additional data."}}
{"id": "2509.25220", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25220", "abs": "https://arxiv.org/abs/2509.25220", "authors": ["Eduard Kapelko"], "title": "Cyclic Ablation: Testing Concept Localization against Functional Regeneration in AI", "comment": "Code is available at:\n  https://www.kaggle.com/code/kapedalex/cycleablationpublic/", "summary": "Safety and controllability are critical for large language models. A central\nquestion is whether undesirable behaviors like deception are localized\nfunctions that can be removed, or if they are deeply intertwined with a model's\ncore cognitive abilities. We introduce \"cyclic ablation,\" an iterative method\nto test this. By combining sparse autoencoders, targeted ablation, and\nadversarial training on DistilGPT-2, we attempted to eliminate the concept of\ndeception. We found that, contrary to the localization hypothesis, deception\nwas highly resilient. The model consistently recovered its deceptive behavior\nafter each ablation cycle via adversarial training, a process we term\nfunctional regeneration. Crucially, every attempt at this \"neurosurgery\" caused\na gradual but measurable decay in general linguistic performance, reflected by\na consistent rise in perplexity. These findings are consistent with the view\nthat complex concepts are distributed and entangled, underscoring the\nlimitations of direct model editing through mechanistic interpretability.", "AI": {"tldr": "Cyclic ablation method shows deception in language models is resilient and deeply entangled with core cognitive abilities, not localized. Attempts to remove deception cause performance decay.", "motivation": "To determine if undesirable behaviors like deception are localized functions that can be removed or deeply intertwined with a model's core cognitive abilities.", "method": "Introduced cyclic ablation combining sparse autoencoders, targeted ablation, and adversarial training on DistilGPT-2 to eliminate the concept of deception.", "result": "Deception was highly resilient - model consistently recovered deceptive behavior after each ablation cycle via functional regeneration. Each ablation attempt caused gradual decay in linguistic performance (rising perplexity).", "conclusion": "Complex concepts are distributed and entangled, showing limitations of direct model editing through mechanistic interpretability."}}
{"id": "2509.25394", "categories": ["cs.CR", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.25394", "abs": "https://arxiv.org/abs/2509.25394", "authors": ["Hui Wang", "Nima Tashakor", "Xiaoyang Tian", "Hans D. Schotten", "Stefan M. Goetz"], "title": "Fast Energy-Theft Attack on Frequency-Varying Wireless Power without Additional Sensors", "comment": "11 pages, 12 figures", "summary": "With the popularity of wireless charging, energy access protection and\ncybersecurity are gaining importance, especially in public places. Currently,\nthe most common energy encryption method uses frequency and associated\nimpedance variation. However, we have proven that this method is not reliable,\nsince a hacker can detect the changing frequency and adjust the compensation.\nHowever, the previously presented system needed time to follow the updated\nfrequency, while encryption systems may vary the frequency faster to avoid\nenergy theft. Furthermore, the previous system required an additional sensor\ncoil. To solve these problems, we optimized the attack and the associated\nsystem, which can intrude and steal energy within 0.2 ms. The key is the\nelimination of the time-consuming maximum receiver current regulation. Also, we\nuse the main receiving coil rather than any additional sensor antenna to detect\nthe magnetic field. Thus, the new hardware is even simpler. A simulation model\nand experimental results demonstrate the fast response speed of the attack on\nencrypted wireless power and steal 65% of the power. Overall, the applicability\nof the attack is highly improved and leaves less room for hardening the\nencryption. The results demonstrate that energy access protection needs to be\ngiven great attention.", "AI": {"tldr": "The paper presents an optimized attack method that can steal 65% of power from encrypted wireless charging systems within 0.2 ms, using only the main receiving coil without additional sensors.", "motivation": "With the growing popularity of wireless charging, energy access protection and cybersecurity in public places are becoming increasingly important. Current frequency-based encryption methods have been proven unreliable.", "method": "The authors optimized the attack system by eliminating time-consuming maximum receiver current regulation and using only the main receiving coil to detect magnetic fields, making the hardware simpler and faster.", "result": "The attack successfully intrudes and steals energy within 0.2 ms, achieving 65% power theft from encrypted wireless power systems. Simulation and experimental results confirm the fast response speed.", "conclusion": "The improved attack method leaves less room for hardening encryption systems, demonstrating that current energy access protection methods are vulnerable and need significant attention and improvement."}}
{"id": "2509.25250", "categories": ["cs.AI", "cs.SE", "I.2.1; H.3.3; D.2.2"], "pdf": "https://arxiv.org/pdf/2509.25250", "abs": "https://arxiv.org/abs/2509.25250", "authors": ["Jiexi Xu"], "title": "Memory Management and Contextual Consistency for Long-Running Low-Code Agents", "comment": "12 pages, 5 figures, 1 table", "summary": "The rise of AI-native Low-Code/No-Code (LCNC) platforms enables autonomous\nagents capable of executing complex, long-duration business processes. However,\na fundamental challenge remains: memory management. As agents operate over\nextended periods, they face \"memory inflation\" and \"contextual degradation\"\nissues, leading to inconsistent behavior, error accumulation, and increased\ncomputational cost. This paper proposes a novel hybrid memory system designed\nspecifically for LCNC agents. Inspired by cognitive science, our architecture\ncombines episodic and semantic memory components with a proactive \"Intelligent\nDecay\" mechanism. This mechanism intelligently prunes or consolidates memories\nbased on a composite score factoring in recency, relevance, and user-specified\nutility. A key innovation is a user-centric visualization interface, aligned\nwith the LCNC paradigm, which allows non-technical users to manage the agent's\nmemory directly, for instance, by visually tagging which facts should be\nretained or forgotten. Through simulated long-running task experiments, we\ndemonstrate that our system significantly outperforms traditional approaches\nlike sliding windows and basic RAG, yielding superior task completion rates,\ncontextual consistency, and long-term token cost efficiency. Our findings\nestablish a new framework for building reliable, transparent AI agents capable\nof effective long-term learning and adaptation.", "AI": {"tldr": "A hybrid memory system for AI-native LCNC agents that combines episodic and semantic memory with intelligent decay to solve memory inflation and contextual degradation problems.", "motivation": "AI agents in LCNC platforms face memory management challenges like memory inflation and contextual degradation during long-duration operations, leading to inconsistent behavior and high computational costs.", "method": "Proposed a hybrid memory architecture with episodic and semantic components plus proactive 'Intelligent Decay' mechanism that prunes/consolidates memories based on recency, relevance, and user utility scores. Includes user-centric visualization for non-technical memory management.", "result": "System significantly outperforms traditional approaches (sliding windows, basic RAG) in simulated long-running tasks, achieving better task completion rates, contextual consistency, and token cost efficiency.", "conclusion": "Establishes a new framework for building reliable, transparent AI agents capable of effective long-term learning and adaptation in LCNC environments."}}
{"id": "2509.25202", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25202", "abs": "https://arxiv.org/abs/2509.25202", "authors": ["Zhuoning Xu", "Xinyan Liu"], "title": "VLHSA: Vision-Language Hierarchical Semantic Alignment for Jigsaw Puzzle Solving with Eroded Gaps", "comment": null, "summary": "Jigsaw puzzle solving remains challenging in computer vision, requiring an\nunderstanding of both local fragment details and global spatial relationships.\nWhile most traditional approaches only focus on visual cues like edge matching\nand visual coherence, few methods explore natural language descriptions for\nsemantic guidance in challenging scenarios, especially for eroded gap puzzles.\nWe propose a vision-language framework that leverages textual context to\nenhance puzzle assembly performance. Our approach centers on the\nVision-Language Hierarchical Semantic Alignment (VLHSA) module, which aligns\nvisual patches with textual descriptions through multi-level semantic matching\nfrom local tokens to global context. Also, a multimodal architecture that\ncombines dual visual encoders with language features for cross-modal reasoning\nis integrated into this module. Experiments demonstrate that our method\nsignificantly outperforms state-of-the-art models across various datasets,\nachieving substantial improvements, including a 14.2 percentage point gain in\npiece accuracy. Ablation studies confirm the critical role of the VLHSA module\nin driving improvements over vision-only approaches. Our work establishes a new\nparadigm for jigsaw puzzle solving by incorporating multimodal semantic\ninsights.", "AI": {"tldr": "A vision-language framework for jigsaw puzzle solving that uses textual descriptions to enhance assembly performance, achieving 14.2% improvement in piece accuracy over state-of-the-art methods.", "motivation": "Traditional jigsaw puzzle solving methods focus only on visual cues, but struggle with challenging scenarios like eroded gap puzzles where semantic context from language could provide valuable guidance.", "method": "Proposes Vision-Language Hierarchical Semantic Alignment (VLHSA) module that aligns visual patches with textual descriptions through multi-level semantic matching, using dual visual encoders combined with language features for cross-modal reasoning.", "result": "Significantly outperforms state-of-the-art models across various datasets, achieving 14.2 percentage point gain in piece accuracy. Ablation studies confirm VLHSA's critical role in improvements over vision-only approaches.", "conclusion": "Establishes a new paradigm for jigsaw puzzle solving by incorporating multimodal semantic insights, demonstrating the value of combining visual and language information for complex puzzle assembly tasks."}}
{"id": "2509.25359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25359", "abs": "https://arxiv.org/abs/2509.25359", "authors": ["Viacheslav Yusupov", "Danil Maksimov", "Ameliia Alaeva", "Anna Vasileva", "Anna Antipina", "Tatyana Zaitseva", "Alina Ermilova", "Evgeny Burnaev", "Egor Shvetsov"], "title": "From Internal Representations to Text Quality: A Geometric Approach to LLM Evaluation", "comment": null, "summary": "This paper bridges internal and external analysis approaches to large\nlanguage models (LLMs) by demonstrating that geometric properties of internal\nmodel representations serve as reliable proxies for evaluating generated text\nquality. We validate a set of metrics including Maximum Explainable Variance,\nEffective Rank, Intrinsic Dimensionality, MAUVE score, and Schatten Norms\nmeasured across different layers of LLMs, demonstrating that Intrinsic\nDimensionality and Effective Rank can serve as universal assessments of text\nnaturalness and quality. Our key finding reveals that different models\nconsistently rank text from various sources in the same order based on these\ngeometric properties, indicating that these metrics reflect inherent text\ncharacteristics rather than model-specific artifacts. This allows a\nreference-free text quality evaluation that does not require human-annotated\ndatasets, offering practical advantages for automated evaluation pipelines.", "AI": {"tldr": "This paper shows that geometric properties of LLM internal representations can serve as reliable proxies for evaluating text quality without needing human-annotated datasets.", "motivation": "To bridge internal and external analysis of LLMs by finding reliable geometric metrics that can assess text quality without requiring reference texts or human annotations.", "method": "Validated multiple geometric metrics (Maximum Explainable Variance, Effective Rank, Intrinsic Dimensionality, MAUVE score, Schatten Norms) across different layers of LLMs to evaluate text naturalness and quality.", "result": "Found that Intrinsic Dimensionality and Effective Rank serve as universal assessments of text quality, with different models consistently ranking text from various sources in the same order based on these geometric properties.", "conclusion": "Geometric properties of internal representations provide reference-free text quality evaluation that reflects inherent text characteristics rather than model-specific artifacts, offering practical advantages for automated evaluation pipelines."}}
{"id": "2509.25408", "categories": ["cs.CR", "econ.TH"], "pdf": "https://arxiv.org/pdf/2509.25408", "abs": "https://arxiv.org/abs/2509.25408", "authors": ["Korok Ray", "Sindura Saraswathi"], "title": "Optimal Threshold Signatures in Bitcoin", "comment": null, "summary": "We formulate the design of a threshold signature scheme as made possible on\ncryptocurrency protocols like Bitcoin. The funds are secured by an m-of-n\nthreshold signature, where at least m signatures are needed to unlock the\nfunds. A user designs this scheme knowing that a malicious attacker can also\nobtain the signatures with some probability. Higher thresholds offer more\nsecurity, but also risk locking the user out of his own funds. The optimal\nthreshold balances these twin effects. Interventions like increasing the\nsecurity or usability of the signatures allow for higher thresholds. We model\ndynamic threshold signature schemes, where the probability of a user or\nattacker obtaining signatures decays with time. A dynamic threshold signature\nscheme is optimal, and increasing security or usability allows for higher\nthresholds and longer time locks.", "AI": {"tldr": "This paper analyzes optimal threshold signature schemes for cryptocurrency security, balancing security against malicious attacks with usability to prevent users from being locked out of their funds.", "motivation": "To address the security-usability tradeoff in cryptocurrency threshold signatures, where higher thresholds provide more security but risk locking users out, and malicious attackers can obtain signatures with some probability.", "method": "Formulate threshold signature design as an optimization problem, model dynamic threshold schemes where signature acquisition probability decays over time, and analyze how security/usability improvements affect optimal thresholds.", "result": "Dynamic threshold signature schemes are optimal, and increasing security or usability allows for higher thresholds and longer time locks while maintaining balance between security and accessibility.", "conclusion": "The optimal threshold balances security against malicious attacks with the risk of users being locked out, with dynamic schemes and security/usability improvements enabling higher thresholds and better protection."}}
{"id": "2509.25252", "categories": ["cs.AI", "68T50, 68T05, 68T30", "I.2.7; I.2.6; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.25252", "abs": "https://arxiv.org/abs/2509.25252", "authors": ["Aayush Gupta"], "title": "Fact Grounded Attention: Eliminating Hallucination in Large Language Models Through Attention Level Knowledge Integration", "comment": "15 pages, 3 figures, 4 tables. Code and dataset available at\n  https://github.com/ayushgupta4897/FGA", "summary": "\"The greatest enemy of knowledge is not ignorance, it is the illusion of\nknowledge.\" Large Language Models have conquered natural language but remain\nprisoners of their own probabilistic nature--confidently hallucinating facts\nthey never truly knew. We present Fact Grounded Attention (FGA), a novel\narchitectural modification that transforms unreliable language models into\ndeterministic truth tellers by injecting verifiable knowledge directly into the\nattention mechanism. Unlike existing approaches that patch hallucinations after\ngeneration or prepend retrieved text, FGA intervenes at the mathematical heart\nof the transformer--the pre-softmax attention scores--creating a model that\ncannot hallucinate when facts exist in its knowledge base. Our experiments\nacross 1,107 technical queries spanning smartphones, laptops, and electric\nvehicles demonstrate a transformation from 6.3% accuracy in vanilla Llama 3.2\nto 99.7% accuracy with FGA. More critically, knowledge updates occur in under\none second without retraining, compared to hours for parameter editing\napproaches. FGA doesn't just reduce hallucination--it eliminates it entirely\nfor verifiable facts, marking a fundamental shift from probabilistic\napproximation to deterministic precision in neural language generation.", "AI": {"tldr": "FGA transforms unreliable language models into deterministic truth tellers by injecting verifiable knowledge directly into attention mechanism, eliminating hallucinations when facts exist in knowledge base.", "motivation": "Large Language Models remain prisoners of their probabilistic nature, confidently hallucinating facts they never truly knew. Existing approaches patch hallucinations after generation or prepend retrieved text, but don't address the core issue.", "method": "Fact Grounded Attention (FGA) intervenes at the mathematical heart of the transformer by modifying pre-softmax attention scores to inject verifiable knowledge directly into the attention mechanism.", "result": "Experiments across 1,107 technical queries show transformation from 6.3% accuracy in vanilla Llama 3.2 to 99.7% accuracy with FGA. Knowledge updates occur in under one second without retraining.", "conclusion": "FGA eliminates hallucination entirely for verifiable facts, marking a fundamental shift from probabilistic approximation to deterministic precision in neural language generation."}}
{"id": "2509.25204", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25204", "abs": "https://arxiv.org/abs/2509.25204", "authors": ["Jin Li", "Zhebo Wang", "Tianliang Lu", "Mohan Li", "Wenpeng Xing", "Meng Han"], "title": "Spectral Logit Sculpting: Adaptive Low-Rank Logit Transformation for Controlled Text Generation", "comment": "Submitted to IEEE ICASSP 2026", "summary": "Entropy-based inference methods have gained traction for improving the\nreliability of Large Language Models (LLMs). However, many existing approaches,\nsuch as entropy minimization techniques, suffer from high computational\noverhead and fail to leverage historical token context effectively. To address\nthese limitations, we propose Spectral Logit Sculpting (SLS), a lightweight\ninference-time optimization method that dynamically modulates token\ndistributions using spectral and entropic properties of recent logits. SLS\nmaintains a sliding buffer of top-K logits, performs on-the-fly Singular Value\nDecomposition (SVD) to identify dominant spectral directions, and adaptively\nrescales logits based on both entropy and logit gap statistics--only activating\nwhen uncertainty is high. Without updating any model parameters, SLS\neffectively sharpens the output distribution while preserving contextual\nconsistency. Experimental results on multiple public benchmarks demonstrate\nthat SLS consistently outperforms existing baseline methods, achieving superior\naccuracy in mathematical, coding, and scientific reasoning tasks.", "AI": {"tldr": "Spectral Logit Sculpting (SLS) is a lightweight inference-time optimization method that improves LLM reliability by dynamically modulating token distributions using spectral analysis and entropy properties, achieving better accuracy without parameter updates.", "motivation": "Existing entropy-based inference methods for LLMs suffer from high computational overhead and ineffective use of historical token context, limiting their practical deployment.", "method": "SLS maintains a sliding buffer of top-K logits, performs on-the-fly SVD to identify dominant spectral directions, and adaptively rescales logits based on entropy and logit gap statistics, only activating during high uncertainty periods.", "result": "Experimental results on multiple benchmarks show SLS consistently outperforms baseline methods, achieving superior accuracy in mathematical, coding, and scientific reasoning tasks.", "conclusion": "SLS provides an effective, computationally efficient solution for improving LLM reliability through spectral analysis and entropy-based logit modulation during inference."}}
{"id": "2509.25369", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25369", "abs": "https://arxiv.org/abs/2509.25369", "authors": ["Andy Liu", "Kshitish Ghate", "Mona Diab", "Daniel Fried", "Atoosa Kasirzadeh", "Max Kleiman-Weiner"], "title": "Generative Value Conflicts Reveal LLM Priorities", "comment": null, "summary": "Past work seeks to align large language model (LLM)-based assistants with a\ntarget set of values, but such assistants are frequently forced to make\ntradeoffs between values when deployed. In response to the scarcity of value\nconflict in existing alignment datasets, we introduce ConflictScope, an\nautomatic pipeline to evaluate how LLMs prioritize different values. Given a\nuser-defined value set, ConflictScope automatically generates scenarios in\nwhich a language model faces a conflict between two values sampled from the\nset. It then prompts target models with an LLM-written \"user prompt\" and\nevaluates their free-text responses to elicit a ranking over values in the\nvalue set. Comparing results between multiple-choice and open-ended\nevaluations, we find that models shift away from supporting protective values,\nsuch as harmlessness, and toward supporting personal values, such as user\nautonomy, in more open-ended value conflict settings. However, including\ndetailed value orderings in models' system prompts improves alignment with a\ntarget ranking by 14%, showing that system prompting can achieve moderate\nsuccess at aligning LLM behavior under value conflict. Our work demonstrates\nthe importance of evaluating value prioritization in models and provides a\nfoundation for future work in this area.", "AI": {"tldr": "ConflictScope is an automated pipeline that evaluates how LLMs prioritize conflicting values by generating scenarios where models must choose between two values, revealing that models shift from protective to personal values in open-ended settings, and system prompting can improve alignment by 14%.", "motivation": "Existing alignment datasets lack value conflict scenarios, making it difficult to evaluate how LLMs prioritize different values when they conflict in real-world deployment.", "method": "Developed ConflictScope pipeline that automatically generates value conflict scenarios from user-defined value sets, uses LLM-written user prompts, and evaluates free-text responses to elicit value rankings.", "result": "Models shift from supporting protective values (harmlessness) to personal values (user autonomy) in open-ended settings. Including detailed value orderings in system prompts improves alignment with target rankings by 14%.", "conclusion": "Evaluating value prioritization is crucial for LLM alignment, and system prompting can moderately improve alignment under value conflicts, providing a foundation for future work in this area."}}
{"id": "2509.25410", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.25410", "abs": "https://arxiv.org/abs/2509.25410", "authors": ["Maraz Mia", "Mir Mehedi A. Pritom", "Tariqul Islam", "Shouhuai Xu"], "title": "Characterizing Event-themed Malicious Web Campaigns: A Case Study on War-themed Websites", "comment": "12 pages, 9 figures, 5 tables", "summary": "Cybercrimes such as online scams and fraud have become prevalent.\nCybercriminals often abuse various global or regional events as themes of their\nfraudulent activities to breach user trust and attain a higher attack success\nrate. These attacks attempt to manipulate and deceive innocent people into\ninteracting with meticulously crafted websites with malicious payloads,\nphishing, or fraudulent transactions. To deepen our understanding of the\nproblem, this paper investigates how to characterize event-themed malicious\nwebsite-based campaigns, with a case study on war-themed websites. We find that\nattackers tailor their attacks by exploiting the unique aspects of events, as\nevidenced by activities such as fundraising, providing aid, collecting\nessential supplies, or seeking updated news. We use explainable unsupervised\nclustering methods to draw further insights, which could guide the design of\neffective early defenses against various event-themed malicious web campaigns.", "AI": {"tldr": "This paper analyzes event-themed malicious websites, particularly war-themed ones, using explainable unsupervised clustering to understand attack patterns and guide early defense design.", "motivation": "Cybercriminals exploit global events to create fraudulent websites that manipulate users through themes like fundraising, aid provision, and news updates, requiring better characterization of these campaigns.", "method": "The study uses explainable unsupervised clustering methods to analyze event-themed malicious websites, with a specific case study on war-themed websites.", "result": "Researchers found that attackers tailor their attacks by exploiting unique event aspects such as fundraising, providing aid, collecting supplies, or seeking updated news.", "conclusion": "The insights from explainable unsupervised clustering can guide the design of effective early defenses against various event-themed malicious web campaigns."}}
{"id": "2509.25260", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25260", "abs": "https://arxiv.org/abs/2509.25260", "authors": ["Muhammed Ustaomeroglu", "Baris Askin", "Gauri Joshi", "Carlee Joe-Wong", "Guannan Qu"], "title": "Language Model Planning from an Information Theoretic Perspective", "comment": null, "summary": "The extent to which decoder-only language models (LMs) engage in planning,\nthat is, organizing intermediate computations to support coherent long-range\ngeneration, remains an open and important question, with implications for\ninterpretability, reliability, and principled model design. Planning involves\nstructuring computations over long horizons, considering multiple possible\ncontinuations, and selectively reusing past information, but how effectively\ntransformer-based LMs realize these capabilities is still unclear. We address\nthese questions by analyzing the hidden states at the core of transformer\ncomputations, which capture intermediate results and act as carriers of\ninformation. Since these hidden representations are often redundant and\nencumbered with fine-grained details, we develop a pipeline based on\nvector-quantized variational autoencoders that compresses them into compact\nsummary codes. These codes enable measuring mutual information, allowing\nsystematic analysis of the computational structure underlying model behavior.\nUsing this framework, we study planning in LMs across synthetic grammar,\npath-finding tasks, and natural language datasets, focusing on three key\naspects: (i) the planning horizon of pre-output computations, (ii) the extent\nto which the model considers alternative valid continuations, and (iii) the\nreliance of new predictions on earlier computations. By answering these\nquestions, we advance the understanding of how planning is realized in LMs and\ncontribute a general-purpose pipeline for probing the internal dynamics of LMs\nand deep learning systems. Our results reveal that the effective planning\nhorizon is task-dependent, that models implicitly preserve information about\nunused correct continuations, and that predictions draw most on recent\ncomputations, though earlier blocks remain informative.", "AI": {"tldr": "This paper analyzes how decoder-only language models engage in planning by examining their hidden states using a VQ-VAE compression pipeline to study planning horizons, consideration of alternatives, and computational dependencies.", "motivation": "To understand how transformer-based language models perform planning - organizing computations for coherent long-range generation - which has implications for interpretability, reliability, and model design.", "method": "Developed a pipeline using vector-quantized variational autoencoders (VQ-VAE) to compress hidden states into compact summary codes, enabling systematic analysis of computational structure through mutual information measurements.", "result": "Found that effective planning horizon is task-dependent, models implicitly preserve information about unused correct continuations, and predictions primarily draw on recent computations while earlier blocks remain informative.", "conclusion": "The study advances understanding of planning in LMs and provides a general-purpose pipeline for probing internal dynamics of deep learning systems, revealing systematic computational patterns in model behavior."}}
{"id": "2509.25205", "categories": ["cs.LG", "cs.CR", "math.RA"], "pdf": "https://arxiv.org/pdf/2509.25205", "abs": "https://arxiv.org/abs/2509.25205", "authors": ["Daksh Pandey"], "title": "Polynomial Contrastive Learning for Privacy-Preserving Representation Learning on Graphs", "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for\nlearning representations on graph data without requiring manual labels.\nHowever, leading SSL methods like GRACE are fundamentally incompatible with\nprivacy-preserving technologies such as Homomorphic Encryption (HE) due to\ntheir reliance on non-polynomial operations. This paper introduces Poly-GRACE,\na novel framework for HE-compatible self-supervised learning on graphs. Our\napproach consists of a fully polynomial-friendly Graph Convolutional Network\n(GCN) encoder and a novel, polynomial-based contrastive loss function. Through\nexperiments on three benchmark datasets -- Cora, CiteSeer, and PubMed -- we\ndemonstrate that Poly-GRACE not only enables private pre-training but also\nachieves performance that is highly competitive with, and in the case of\nCiteSeer, superior to the standard non-private baseline. Our work represents a\nsignificant step towards practical and high-performance privacy-preserving\ngraph representation learning.", "AI": {"tldr": "Poly-GRACE is a novel HE-compatible self-supervised learning framework for graphs that replaces non-polynomial operations in GRACE with polynomial alternatives, enabling privacy-preserving training while maintaining competitive performance.", "motivation": "Existing SSL methods like GRACE are incompatible with Homomorphic Encryption due to reliance on non-polynomial operations, limiting privacy-preserving graph representation learning.", "method": "Developed a fully polynomial-friendly GCN encoder and a novel polynomial-based contrastive loss function to enable HE-compatible self-supervised learning.", "result": "On Cora, CiteSeer, and PubMed datasets, Poly-GRACE achieves competitive performance with standard non-private baseline, and superior performance on CiteSeer.", "conclusion": "Poly-GRACE represents a significant advancement towards practical and high-performance privacy-preserving graph representation learning."}}
{"id": "2509.25409", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25409", "abs": "https://arxiv.org/abs/2509.25409", "authors": ["Qiyao Ma", "Yunsheng Shi", "Hongtao Tian", "Chao Wang", "Weiming Chang", "Ting Yao"], "title": "From Faithfulness to Correctness: Generative Reward Models that Think Critically", "comment": null, "summary": "Through reinforcement learning with verifiable rewards (RLVR), large language\nmodels have achieved substantial progress in domains with easily verifiable\noutcomes, such as mathematics and coding. However, when applied to more complex\ntasks like open-domain question answering, RLVR faces significant challenges\ndue to the difficulty of verifying correctness. The nuanced and ambiguous\nnature of real-world knowledge makes it difficult to reliably evaluate\ncorrectness in these settings, necessitating further abilities that extend\nbeyond mere logical consistency to encompass an understanding and assessment of\nboth external and internal knowledge. Recent work has primarily focused on\nimproving faithfulness, defined as semantic alignment with supporting\ndocuments, which can cause models to rely excessively on external sources and\ndiminish their capacity for critical assessment. To address this, we propose\nthe Thinking-supervised Reward Model (TRM), which incorporates sentence-level\nthinking supervision to endow reward models with critical thinking abilities.\nGiven a query, answer, and supporting documents, TRM first assesses the\nfaithfulness of each answer sentence to the supporting documents, and then\napplies a reasoning step to evaluate sentence-level correctness. By structuring\nreward modeling as a sequence of faithfulness, reasoning, and correctness\nevaluations, TRM encourages models to critically assess and leverage both\nexternal and internal knowledge. Experiments on reward signals demonstrate that\nTRM substantially improves the identification of incorrect sentences, and\nincorporating TRM into policy optimization leads to significant gains in both\nanswer correctness and usefulness.", "AI": {"tldr": "The paper proposes TRM (Thinking-supervised Reward Model) to address limitations of RLVR in complex tasks like open-domain QA by incorporating critical thinking through sentence-level faithfulness and correctness evaluations.", "motivation": "RLVR struggles with complex tasks due to difficulty verifying correctness in nuanced real-world knowledge scenarios. Current focus on faithfulness causes over-reliance on external sources and reduces critical assessment capabilities.", "method": "TRM uses sentence-level thinking supervision: first assesses faithfulness to supporting documents, then applies reasoning to evaluate sentence-level correctness, structuring reward modeling as faithfulness-reasoning-correctness sequence.", "result": "TRM significantly improves identification of incorrect sentences and, when incorporated into policy optimization, leads to substantial gains in both answer correctness and usefulness.", "conclusion": "TRM's structured approach to reward modeling with critical thinking capabilities enables better assessment of both external and internal knowledge, overcoming limitations of current RLVR methods in complex tasks."}}
{"id": "2509.25430", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.25430", "abs": "https://arxiv.org/abs/2509.25430", "authors": ["Martin Kotuliak", "Simon Erni", "Jakub Pol\u00e1k", "Marc Roeschlin", "Richard Baker", "Ivan Martinovic", "Srdjan \u010capkun"], "title": "Finding Phones Fast: Low-Latency and Scalable Monitoring of Cellular Communications in Sensitive Areas", "comment": null, "summary": "The widespread availability of cellular devices introduces new threat vectors\nthat allow users or attackers to bypass security policies and physical barriers\nand bring unauthorized devices into sensitive areas. These threats can arise\nfrom user non-compliance or deliberate actions aimed at data\nexfiltration/infiltration via hidden devices, drones, etc. We identify a\ncritical gap in this context: the absence of low-latency systems for\nhigh-quality and instantaneous monitoring of cellular transmissions. Such\nlow-latency systems are crucial to allow for timely detection, decision (e.g.,\ngeofencing or localization), and disruption of unauthorized communication in\nsensitive areas. Operator-based monitoring systems, built for purposes such as\npeople counting or tracking, lack real-time capability, require cooperation\nacross multiple operators, and thus are hard to deploy. Operator-independent\nmonitoring approaches proposed in the literature either lack low-latency\ncapabilities or do not scale.\n  We propose LTag, the first low-latency, operator-independent and scalable\nsystem designed to monitor cellular connections across all operators prior to\nany user data transmission. LTag consists of several downlink sniffers and a\ndistributed network of uplink sniffers that measure both downlink protocol\ninformation and uplink signal characteristics at multiple locations to gain a\ndetailed spatial image of uplink signals. LTag aggregates the recorded\ninformation, processes it, and provides a decision about the connection all\nprior to connection establishment of a UE. To evaluate LTag, we deployed it in\nthe context of geofencing, where LTag was able to determine if the signals\noriginate from inside or outside of an area within 2.3 ms of the initial base\nstation-to-device message, therefore enabling prompt and targeted suppression\nof communication before any user data was transmitted.", "AI": {"tldr": "LTag is a low-latency cellular monitoring system that detects unauthorized devices in sensitive areas before data transmission, achieving detection within 2.3ms.", "motivation": "Address the gap in low-latency monitoring of cellular transmissions to prevent security breaches from unauthorized devices in sensitive areas.", "method": "Uses distributed network of downlink and uplink sniffers to measure protocol info and signal characteristics, aggregating data before connection establishment.", "result": "Successfully deployed for geofencing, detecting signal origins inside/outside areas within 2.3ms of initial base station message.", "conclusion": "LTag provides effective, scalable real-time cellular monitoring for security applications without operator cooperation."}}
{"id": "2509.25271", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.25271", "abs": "https://arxiv.org/abs/2509.25271", "authors": ["Xiuyuan Chen", "Jian Zhao", "Yuchen Yuan", "Tianle Zhang", "Huilin Zhou", "Zheng Zhu", "Ping Hu", "Linghe Kong", "Chi Zhang", "Weiran Huang", "Xuelong Li"], "title": "RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration", "comment": null, "summary": "Existing safety evaluation methods for large language models (LLMs) suffer\nfrom inherent limitations, including evaluator bias and detection failures\narising from model homogeneity, which collectively undermine the robustness of\nrisk evaluation processes. This paper seeks to re-examine the risk evaluation\nparadigm by introducing a theoretical framework that reconstructs the\nunderlying risk concept space. Specifically, we decompose the latent risk\nconcept space into three mutually exclusive subspaces: the explicit risk\nsubspace (encompassing direct violations of safety guidelines), the implicit\nrisk subspace (capturing potential malicious content that requires contextual\nreasoning for identification), and the non-risk subspace. Furthermore, we\npropose RADAR, a multi-agent collaborative evaluation framework that leverages\nmulti-round debate mechanisms through four specialized complementary roles and\nemploys dynamic update mechanisms to achieve self-evolution of risk concept\ndistributions. This approach enables comprehensive coverage of both explicit\nand implicit risks while mitigating evaluator bias. To validate the\neffectiveness of our framework, we construct an evaluation dataset comprising\n800 challenging cases. Extensive experiments on our challenging testset and\npublic benchmarks demonstrate that RADAR significantly outperforms baseline\nevaluation methods across multiple dimensions, including accuracy, stability,\nand self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87%\nimprovement in risk identification accuracy compared to the strongest baseline\nevaluation method.", "AI": {"tldr": "RADAR is a multi-agent collaborative framework that improves LLM safety evaluation by decomposing risk into explicit, implicit, and non-risk subspaces, using debate mechanisms to reduce bias and enhance coverage.", "motivation": "Existing LLM safety evaluation methods suffer from evaluator bias and detection failures due to model homogeneity, undermining robustness.", "method": "Propose RADAR framework with multi-agent collaboration, four specialized roles, multi-round debates, and dynamic update mechanisms to evolve risk concept distributions.", "result": "RADAR achieves 28.87% improvement in risk identification accuracy over strongest baseline, with better performance in accuracy, stability, and self-evaluation risk sensitivity.", "conclusion": "RADAR provides a more robust safety evaluation paradigm that effectively covers both explicit and implicit risks while mitigating evaluator bias."}}
{"id": "2509.25206", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25206", "abs": "https://arxiv.org/abs/2509.25206", "authors": ["Yanke Wang", "Kyriakos Flouris"], "title": "Hyperbolic Optimization", "comment": "Preprint", "summary": "This work explores optimization methods on hyperbolic manifolds. Building on\nRiemannian optimization principles, we extend the Hyperbolic Stochastic\nGradient Descent (a specialization of Riemannian SGD) to a Hyperbolic Adam\noptimizer. While these methods are particularly relevant for learning on the\nPoincar\\'e ball, they may also provide benefits in Euclidean and other\nnon-Euclidean settings, as the chosen optimization encourages the learning of\nPoincar\\'e embeddings. This representation, in turn, accelerates convergence in\nthe early stages of training, when parameters are far from the optimum. As a\ncase study, we train diffusion models using the hyperbolic optimization methods\nwith hyperbolic time-discretization of the Langevin dynamics, and show that\nthey achieve faster convergence on certain datasets without sacrificing\ngenerative quality.", "AI": {"tldr": "This paper extends Riemannian optimization to hyperbolic manifolds, developing Hyperbolic Adam optimizer and showing it accelerates early training convergence for diffusion models.", "motivation": "To leverage hyperbolic manifolds for optimization, particularly for learning Poincar\u00e9 embeddings that can accelerate convergence when parameters are far from optimum.", "method": "Extends Riemannian optimization principles to hyperbolic manifolds, develops Hyperbolic SGD and Hyperbolic Adam optimizers, applies to diffusion models with hyperbolic time-discretization of Langevin dynamics.", "result": "Hyperbolic optimization methods achieve faster convergence on certain datasets without sacrificing generative quality in diffusion models.", "conclusion": "Hyperbolic optimization methods provide benefits for training deep learning models, particularly accelerating early-stage convergence while maintaining performance."}}
{"id": "2509.25416", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.25416", "abs": "https://arxiv.org/abs/2509.25416", "authors": ["Jiacheng Shi", "Hongfei Du", "Yangfan He", "Y. Alicia Hong", "Ye Gao"], "title": "Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization", "comment": null, "summary": "Emotional text-to-speech seeks to convey affect while preserving\nintelligibility and prosody, yet existing methods rely on coarse labels or\nproxy classifiers and receive only utterance-level feedback. We introduce\nEmotion-Aware Stepwise Preference Optimization (EASPO), a post-training\nframework that aligns diffusion TTS with fine-grained emotional preferences at\nintermediate denoising steps. Central to our approach is EASPM, a\ntime-conditioned model that scores noisy intermediate speech states and enables\nautomatic preference pair construction. EASPO optimizes generation to match\nthese stepwise preferences, enabling controllable emotional shaping.\nExperiments show superior performance over existing methods in both\nexpressiveness and naturalness.", "AI": {"tldr": "EASPO is a post-training framework that aligns diffusion TTS with fine-grained emotional preferences at intermediate denoising steps using a time-conditioned scoring model.", "motivation": "Existing emotional text-to-speech methods rely on coarse labels or proxy classifiers and receive only utterance-level feedback, lacking fine-grained emotional control.", "method": "Introduces Emotion-Aware Stepwise Preference Optimization (EASPO) with EASPM model that scores noisy intermediate speech states to enable automatic preference pair construction and controllable emotional shaping.", "result": "Experiments show superior performance over existing methods in both expressiveness and naturalness.", "conclusion": "EASPO enables fine-grained emotional control in text-to-speech through stepwise preference optimization at intermediate denoising steps."}}
{"id": "2509.25448", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25448", "abs": "https://arxiv.org/abs/2509.25448", "authors": ["Yuepeng Hu", "Zhengyuan Jiang", "Mengyuan Li", "Osama Ahmed", "Zhicong Huang", "Cheng Hong", "Neil Gong"], "title": "Fingerprinting LLMs via Prompt Injection", "comment": null, "summary": "Large language models (LLMs) are often modified after release through\npost-processing such as post-training or quantization, which makes it\nchallenging to determine whether one model is derived from another. Existing\nprovenance detection methods have two main limitations: (1) they embed signals\ninto the base model before release, which is infeasible for already published\nmodels, or (2) they compare outputs across models using hand-crafted or random\nprompts, which are not robust to post-processing. In this work, we propose\nLLMPrint, a novel detection framework that constructs fingerprints by\nexploiting LLMs' inherent vulnerability to prompt injection. Our key insight is\nthat by optimizing fingerprint prompts to enforce consistent token preferences,\nwe can obtain fingerprints that are both unique to the base model and robust to\npost-processing. We further develop a unified verification procedure that\napplies to both gray-box and black-box settings, with statistical guarantees.\nWe evaluate LLMPrint on five base models and around 700 post-trained or\nquantized variants. Our results show that LLMPrint achieves high true positive\nrates while keeping false positive rates near zero.", "AI": {"tldr": "LLMPrint is a novel framework that detects if LLMs are derived from others by exploiting prompt injection vulnerabilities to create robust fingerprints that survive post-processing like quantization or post-training.", "motivation": "Existing provenance detection methods either require embedding signals before release (infeasible for published models) or use non-robust prompt comparisons that fail against post-processing modifications.", "method": "LLMPrint constructs fingerprints by optimizing prompts to enforce consistent token preferences, exploiting LLMs' inherent vulnerability to prompt injection. It works in both gray-box and black-box settings with statistical guarantees.", "result": "Evaluated on 5 base models and ~700 post-processed variants, LLMPrint achieves high true positive rates while maintaining near-zero false positive rates.", "conclusion": "LLMPrint provides an effective solution for detecting model provenance that is robust to post-processing modifications and applicable to already published models."}}
{"id": "2509.25279", "categories": ["cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25279", "abs": "https://arxiv.org/abs/2509.25279", "authors": ["Jiecheng Zhou", "Qinghao Hu", "Yuyang Jin", "Zerui Wang", "Peng Sun", "Yuzhe Gu", "Wenwei Zhang", "Mingshu Zhai", "Xingcheng Zhang", "Weiming Zhang"], "title": "RL in the Wild: Characterizing RLVR Training in LLM Deployment", "comment": "20 pages, 28 figures", "summary": "Large Language Models (LLMs) are now widely used across many domains. With\ntheir rapid development, Reinforcement Learning with Verifiable Rewards (RLVR)\nhas surged in recent months to enhance their reasoning and understanding\nabilities. However, its complex data flows and diverse tasks pose substantial\nchallenges to RL training systems, and there is limited understanding of RLVR\nfrom a system perspective. To thoroughly understand the system challenges\nintroduced by RLVR, we present a characterization study of RLVR tasks in our\nLLM deployment. Specifically, we investigate the distribution and variation\ntrends of workloads across different RL tasks across training steps. We\nidentify issues such as GPU idling caused by skewed sequence length\ndistribution, inefficient parallel strategies in dynamically varying workloads,\ninefficient data management mechanisms, and load imbalance. We describe our\nobservations and call for further investigation into the remaining open\nchallenges. Furthermore, we propose PolyTrace benchmark suite to conduct\nevaluation with realistic workloads, and a practical use case validates that\nPolyTrace benchmark suite exhibits 94.7% accuracy.", "AI": {"tldr": "This paper presents a characterization study of Reinforcement Learning with Verifiable Rewards (RLVR) in LLM deployment, identifying system challenges like GPU idling, inefficient parallel strategies, and load imbalance, and proposes the PolyTrace benchmark suite for evaluation.", "motivation": "To understand the system challenges introduced by RLVR in LLM deployment, as complex data flows and diverse tasks pose substantial challenges to RL training systems with limited understanding from a system perspective.", "method": "Conducted a characterization study of RLVR tasks in LLM deployment, investigating workload distribution and variation trends across different RL tasks and training steps.", "result": "Identified issues including GPU idling due to skewed sequence length distribution, inefficient parallel strategies in dynamically varying workloads, inefficient data management mechanisms, and load imbalance.", "conclusion": "Proposed PolyTrace benchmark suite for evaluation with realistic workloads, validated with 94.7% accuracy in a practical use case, and called for further investigation into remaining open challenges."}}
{"id": "2509.25207", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25207", "abs": "https://arxiv.org/abs/2509.25207", "authors": ["Yebin Lim", "Susik Yoon"], "title": "Multi-level Diagnosis and Evaluation for Robust Tabular Feature Engineering with Large Language Models", "comment": "Accepted to Findings of EMNLP 2025", "summary": "Recent advancements in large language models (LLMs) have shown promise in\nfeature engineering for tabular data, but concerns about their reliability\npersist, especially due to variability in generated outputs. We introduce a\nmulti-level diagnosis and evaluation framework to assess the robustness of LLMs\nin feature engineering across diverse domains, focusing on the three main\nfactors: key variables, relationships, and decision boundary values for\npredicting target classes. We demonstrate that the robustness of LLMs varies\nsignificantly over different datasets, and that high-quality LLM-generated\nfeatures can improve few-shot prediction performance by up to 10.52%. This work\nopens a new direction for assessing and enhancing the reliability of LLM-driven\nfeature engineering in various domains.", "AI": {"tldr": "A multi-level framework to assess LLM robustness in feature engineering, showing variable performance across datasets and potential for up to 10.52% improvement in few-shot prediction.", "motivation": "Address concerns about LLM reliability in feature engineering due to output variability, and establish systematic evaluation methods.", "method": "Multi-level diagnosis and evaluation framework focusing on key variables, relationships, and decision boundary values for predicting target classes.", "result": "LLM robustness varies significantly across datasets, and high-quality LLM-generated features can improve few-shot prediction performance by up to 10.52%.", "conclusion": "This work opens new directions for assessing and enhancing LLM-driven feature engineering reliability across various domains."}}
{"id": "2509.25459", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25459", "abs": "https://arxiv.org/abs/2509.25459", "authors": ["Haozhou Xu", "Dongxia Wu", "Matteo Chinazzi", "Ruijia Niu", "Rose Yu", "Yi-An Ma"], "title": "SimulRAG: Simulator-based RAG for Grounding LLMs in Long-form Scientific QA", "comment": "Haozhou Xu and Dongxia Wu are co-first authors", "summary": "Large language models (LLMs) show promise in solving scientific problems.\nThey can help generate long-form answers for scientific questions, which are\ncrucial for comprehensive understanding of complex phenomena that require\ndetailed explanations spanning multiple interconnected concepts and evidence.\nHowever, LLMs often suffer from hallucination, especially in the challenging\ntask of long-form scientific question answering. Retrieval-Augmented Generation\n(RAG) approaches can ground LLMs by incorporating external knowledge sources to\nimprove trustworthiness. In this context, scientific simulators, which play a\nvital role in validating hypotheses, offer a particularly promising retrieval\nsource to mitigate hallucination and enhance answer factuality. However,\nexisting RAG approaches cannot be directly applied for scientific\nsimulation-based retrieval due to two fundamental challenges: how to retrieve\nfrom scientific simulators, and how to efficiently verify and update long-form\nanswers. To overcome these challenges, we propose the simulator-based RAG\nframework (SimulRAG) and provide a long-form scientific QA benchmark covering\nclimate science and epidemiology with ground truth verified by both simulations\nand human annotators. In this framework, we propose a generalized simulator\nretrieval interface to transform between textual and numerical modalities. We\nfurther design a claim-level generation method that utilizes uncertainty\nestimation scores and simulator boundary assessment (UE+SBA) to efficiently\nverify and update claims. Extensive experiments demonstrate SimulRAG\noutperforms traditional RAG baselines by 30.4% in informativeness and 16.3% in\nfactuality. UE+SBA further improves efficiency and quality for claim-level\ngeneration.", "AI": {"tldr": "SimulRAG is a novel RAG framework that uses scientific simulators as retrieval sources to reduce LLM hallucination in long-form scientific QA, achieving significant improvements in informativeness and factuality.", "motivation": "LLMs suffer from hallucination in long-form scientific question answering, and existing RAG approaches cannot effectively utilize scientific simulators which are crucial for validating hypotheses and improving answer factuality.", "method": "Proposes SimulRAG with two key components: a generalized simulator retrieval interface for modality transformation between text and numbers, and a claim-level generation method using uncertainty estimation scores and simulator boundary assessment (UE+SBA) for efficient verification and updating.", "result": "SimulRAG outperforms traditional RAG baselines by 30.4% in informativeness and 16.3% in factuality. UE+SBA further improves efficiency and quality for claim-level generation.", "conclusion": "SimulRAG effectively addresses the challenges of retrieving from scientific simulators and verifying long-form answers, providing a robust framework for trustworthy scientific question answering using simulation-based grounding."}}
{"id": "2509.25462", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.25462", "abs": "https://arxiv.org/abs/2509.25462", "authors": ["Loay Abdelrazek", "Filippo Rebecchi"], "title": "Managing Differentiated Secure Connectivity using Intents", "comment": "Preprint version of paper accepted in Mobiwac'25", "summary": "Mobile networks in the 5G and 6G era require to rethink how to manage\nsecurity due to the introduction of new services, use cases, each with its own\nsecurity requirements, while simultaneously expanding the threat landscape.\nAlthough automation has emerged as a key enabler to address complexity in\nnetworks, existing approaches lack the expressiveness to define and enforce\ncomplex, goal-driven, and measurable security requirements. In this paper, we\npropose the concept of differentiated security levels and leveraging intents as\na management framework. We discuss the requirements and enablers to extend the\ncurrently defined intent-based management frameworks to pave the path for\nintent-based security management in mobile networks. Our approach formalizes\nboth functional and non-functional security requirements and demonstrates how\nthese can be expressed and modeled using an extended TM Forum (TMF) intent\nsecurity ontology. We further discuss the required standardization steps to\nachieve intent-based security management. Our work aims at advance security\nautomation, improve adaptability, and strengthen the resilience and security\nposture of the next-generation mobile networks.", "AI": {"tldr": "Proposes differentiated security levels and intent-based management framework for 5G/6G mobile networks to address complex security requirements through automation.", "motivation": "5G/6G networks face expanded threat landscape with diverse security requirements, while existing automation approaches lack expressiveness for complex, measurable security goals.", "method": "Extends intent-based management frameworks using TM Forum intent security ontology to formalize functional and non-functional security requirements.", "result": "Developed approach enables expression and modeling of complex security intents, advancing security automation capabilities for mobile networks.", "conclusion": "Intent-based security management can improve adaptability, resilience, and security posture of next-generation mobile networks through standardized frameworks."}}
{"id": "2509.25282", "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2.4; D.1.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2509.25282", "abs": "https://arxiv.org/abs/2509.25282", "authors": ["Jiexi Xu", "Jiaqi Liu", "Ran Tong", "Su Liu"], "title": "Toward Causal-Visual Programming: Enhancing Agentic Reasoning in Low-Code Environments", "comment": "5 pages, 1 table", "summary": "Large language model (LLM) agents are increasingly capable of orchestrating\ncomplex tasks in low-code environments. However, these agents often exhibit\nhallucinations and logical inconsistencies because their inherent reasoning\nmechanisms rely on probabilistic associations rather than genuine causal\nunderstanding. This paper introduces a new programming paradigm: Causal-Visual\nProgramming (CVP), designed to address this fundamental issue by explicitly\nintroducing causal structures into the workflow design. CVP allows users to\ndefine a simple \"world model\" for workflow modules through an intuitive\nlow-code interface, effectively creating a Directed Acyclic Graph (DAG) that\nexplicitly defines the causal relationships between modules. This causal graph\nacts as a crucial constraint during the agent's reasoning process, anchoring\nits decisions to a user-defined causal structure and significantly reducing\nlogical errors and hallucinations by preventing reliance on spurious\ncorrelations. To validate the effectiveness of CVP, we designed a synthetic\nexperiment that simulates a common real-world problem: a distribution shift\nbetween the training and test environments. Our results show that a causally\nanchored model maintained stable accuracy in the face of this shift, whereas a\npurely associative baseline model that relied on probabilistic correlations\nexperienced a significant performance drop. The primary contributions of this\nstudy are: a formal definition of causal structures for workflow modules; the\nproposal and implementation of a CVP framework that anchors agent reasoning to\na user-defined causal graph; and empirical evidence demonstrating the\nframework's effectiveness in enhancing agent robustness and reducing errors\ncaused by causal confusion in dynamic environments. CVP offers a viable path\ntoward building more interpretable, reliable, and trustworthy AI agents.", "AI": {"tldr": "Causal-Visual Programming (CVP) introduces causal structures into LLM agent workflows to reduce hallucinations and logical errors by anchoring reasoning to user-defined causal graphs.", "motivation": "Current LLM agents exhibit hallucinations and logical inconsistencies due to relying on probabilistic associations rather than genuine causal understanding, especially in low-code environments.", "method": "CVP allows users to define a 'world model' through a low-code interface, creating a Directed Acyclic Graph (DAG) that explicitly defines causal relationships between workflow modules, constraining agent reasoning.", "result": "In synthetic experiments simulating distribution shifts, causally anchored models maintained stable accuracy while associative baseline models experienced significant performance drops.", "conclusion": "CVP provides a viable path toward building more interpretable, reliable, and trustworthy AI agents by explicitly incorporating causal structures into workflow design."}}
{"id": "2509.25208", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.25208", "abs": "https://arxiv.org/abs/2509.25208", "authors": ["Zenghui Huang", "Ting Shu", "Zhonglei Wang", "Yang Lu", "Yan Yan", "Wei Zhong", "Hanzi Wang"], "title": "DPSformer: A long-tail-aware model for improving heavy rainfall prediction", "comment": null, "summary": "Accurate and timely forecasting of heavy rainfall remains a critical\nchallenge for modern society. Precipitation exhibits a highly imbalanced\ndistribution: most observations record no or light rain, while heavy rainfall\nevents are rare. Such an imbalanced distribution obstructs deep learning models\nfrom effectively predicting heavy rainfall events. To address this challenge,\nwe treat rainfall forecasting explicitly as a long-tailed learning problem,\nidentifying the insufficient representation of heavy rainfall events as the\nprimary barrier to forecasting accuracy. Therefore, we introduce DPSformer, a\nlong-tail-aware model that enriches representation of heavy rainfall events\nthrough a high-resolution branch. For heavy rainfall events $ \\geq $ 50 mm/6 h,\nDPSformer lifts the Critical Success Index (CSI) of a baseline Numerical\nWeather Prediction (NWP) model from 0.012 to 0.067. For the top 1% coverage of\nheavy rainfall events, its Fraction Skill Score (FSS) exceeds 0.45, surpassing\nexisting methods. Our work establishes an effective long-tailed paradigm for\nheavy rainfall prediction, offering a practical tool to enhance early warning\nsystems and mitigate the societal impacts of extreme weather events.", "AI": {"tldr": "DPSformer addresses heavy rainfall forecasting as a long-tailed learning problem, improving prediction of rare but critical heavy rainfall events through specialized high-resolution modeling.", "motivation": "Heavy rainfall forecasting is challenging due to imbalanced data distribution where most observations show no/light rain while heavy rainfall events are rare, preventing deep learning models from effectively predicting these critical events.", "method": "Treat rainfall forecasting as long-tailed learning problem; introduce DPSformer model with high-resolution branch to enrich representation of heavy rainfall events.", "result": "For heavy rainfall events \u226550mm/6h, DPSformer improves CSI from 0.012 to 0.067; for top 1% heavy rainfall events, FSS exceeds 0.45, outperforming existing methods.", "conclusion": "DPSformer establishes effective long-tailed paradigm for heavy rainfall prediction, providing practical tool to enhance early warning systems and mitigate extreme weather impacts."}}
{"id": "2509.25477", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25477", "abs": "https://arxiv.org/abs/2509.25477", "authors": ["Tadesse Destaw Belay", "Kedir Yassin Hussen", "Sukairaj Hafiz Imam", "Iqra Ameer", "Ibrahim Said Ahmad", "Isa Inuwa-Dutse", "Idris Abdulmumin", "Grigori Sidorov", "Vukosi Marivate", "Seid Muhie Yimam", "Shamsuddeen Hassan Muhammad"], "title": "The Rise of AfricaNLP: Contributions, Contributors, and Community Impact (2005-2025)", "comment": null, "summary": "Natural Language Processing (NLP) is undergoing constant transformation, as\nLarge Language Models (LLMs) are driving daily breakthroughs in research and\npractice. In this regard, tracking the progress of NLP research and\nautomatically analyzing the contributions of research papers provides key\ninsights into the nature of the field and the researchers. This study explores\nthe progress of African NLP (AfricaNLP) by asking (and answering) basic\nresearch questions such as: i) How has the nature of NLP evolved over the last\ntwo decades?, ii) What are the contributions of AfricaNLP papers?, and iii)\nWhich individuals and organizations (authors, affiliated institutions, and\nfunding bodies) have been involved in the development of AfricaNLP? We\nquantitatively examine the contributions of AfricaNLP research using 1.9K NLP\npaper abstracts, 4.9K author contributors, and 7.8K human-annotated\ncontribution sentences (AfricaNLPContributions) along with benchmark results.\nOur dataset and continuously existing NLP progress tracking website provide a\npowerful lens for tracing AfricaNLP research trends and hold potential for\ngenerating data-driven literature surveys.", "AI": {"tldr": "This study analyzes the progress of African NLP research over two decades using 1.9K paper abstracts, 4.9K authors, and 7.8K annotated contribution sentences to understand research evolution, contributions, and key stakeholders.", "motivation": "To track the progress of NLP research and automatically analyze contributions of African NLP papers, providing insights into the field's evolution and key contributors in Africa.", "method": "Quantitative examination using 1.9K NLP paper abstracts, 4.9K author contributors, and 7.8K human-annotated contribution sentences (AfricaNLPContributions) with benchmark results.", "result": "Created a dataset and continuously existing NLP progress tracking website that provides powerful tools for tracing AfricaNLP research trends and generating data-driven literature surveys.", "conclusion": "The study provides a comprehensive framework for analyzing African NLP research progress, identifying key contributors, and understanding the field's evolution over two decades."}}
{"id": "2509.25469", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.25469", "abs": "https://arxiv.org/abs/2509.25469", "authors": ["Panagiotis Michalopoulos", "Anthony Mack", "Cameron Clark", "Linus Chen", "Johannes Sedlmeir", "Andreas Veneris"], "title": "Balancing Compliance and Privacy in Offline CBDC Transactions Using a Secure Element-based System", "comment": "9 pages, 4 figures", "summary": "Blockchain technology has spawned a vast ecosystem of digital currencies with\nCentral Bank Digital Currencies (CBDCs) -- digital forms of fiat currency --\nbeing one of them. An important feature of digital currencies is facilitating\ntransactions without network connectivity, which can enhance the scalability of\ncryptocurrencies and the privacy of CBDC users. However, in the case of CBDCs,\nthis characteristic also introduces new regulatory challenges, particularly\nwhen it comes to applying established Anti-Money Laundering and Countering the\nFinancing of Terrorism (AML/CFT) frameworks. This paper introduces a prototype\nfor offline digital currency payments, equally applicable to cryptocurrencies\nand CBDCs, that leverages Secure Elements and digital credentials to address\nthe tension of offline payment support with regulatory compliance. Performance\nevaluation results suggest that the prototype can be flexibly adapted to\ndifferent regulatory environments, with a transaction latency comparable to\nreal-life commercial payment systems. Furthermore, we conceptualize how the\nintegration of Zero-Knowledge Proofs into our design could accommodate various\ntiers of enhanced privacy protection.", "AI": {"tldr": "This paper presents a prototype for offline digital currency payments that balances regulatory compliance with privacy protection, applicable to both cryptocurrencies and CBDCs.", "motivation": "The motivation is to address the regulatory challenges that arise when digital currencies support offline transactions, particularly in maintaining AML/CFT compliance while preserving user privacy.", "method": "The method involves developing a prototype that uses Secure Elements and digital credentials to enable offline payments while ensuring regulatory compliance. The design also considers integrating Zero-Knowledge Proofs for enhanced privacy protection.", "result": "Performance evaluation shows the prototype can be flexibly adapted to different regulatory environments with transaction latency comparable to real-life commercial payment systems.", "conclusion": "The paper concludes that it's possible to design offline digital currency payment systems that effectively balance regulatory requirements with privacy protection, and suggests future enhancements through Zero-Knowledge Proof integration."}}
{"id": "2509.25299", "categories": ["cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.25299", "abs": "https://arxiv.org/abs/2509.25299", "authors": ["Daniel Platnick", "Mohamed E. Bengueddache", "Marjan Alirezaie", "Dava J. Newman", "Alex ''Sandy'' Pentland", "Hossein Rahnama"], "title": "ID-RAG: Identity Retrieval-Augmented Generation for Long-Horizon Persona Coherence in Generative Agents", "comment": "Accepted to LLAIS 2025: Workshop on LLM-Based Agents for Intelligent\n  Systems, at ECAI 2025, 12 pages, 3 figures", "summary": "Generative agents powered by language models are increasingly deployed for\nlong-horizon tasks. However, as long-term memory context grows over time, they\nstruggle to maintain coherence. This deficiency leads to critical failures,\nincluding identity drift, ignoring established beliefs, and the propagation of\nhallucinations in multi-agent systems. To mitigate these challenges, this paper\nintroduces Identity Retrieval-Augmented Generation (ID-RAG), a novel mechanism\ndesigned to ground an agent's persona and persistent preferences in a dynamic,\nstructured identity model: a knowledge graph of core beliefs, traits, and\nvalues. During the agent's decision loop, this model is queried to retrieve\nrelevant identity context, which directly informs action selection. We\ndemonstrate this approach by introducing and implementing a new class of ID-RAG\nenabled agents called Human-AI Agents (HAis), where the identity model is\ninspired by the Chronicle structure used in Perspective-Aware AI, a dynamic\nknowledge graph learned from a real-world entity's digital footprint. In social\nsimulations of a mayoral election, HAis using ID-RAG outperformed baseline\nagents in long-horizon persona coherence - achieving higher identity recall\nacross all tested models by the fourth timestep - and reduced simulation\nconvergence time by 19% (GPT-4o) and 58% (GPT-4o mini). By treating identity as\nan explicit, retrievable knowledge structure, ID-RAG offers a foundational\napproach for developing more temporally coherent, interpretable, and aligned\ngenerative agents. Our code is open-source and available at:\nhttps://github.com/flybits/humanai-agents.", "AI": {"tldr": "ID-RAG is a novel mechanism that uses a knowledge graph to maintain generative agents' identity coherence over long-term tasks, reducing identity drift and improving performance.", "motivation": "Generative agents struggle with maintaining coherence as long-term memory grows, leading to identity drift, ignored beliefs, and hallucination propagation in multi-agent systems.", "method": "Introduces Identity Retrieval-Augmented Generation (ID-RAG) that grounds agent persona in a dynamic knowledge graph of core beliefs, traits, and values, queried during decision-making.", "result": "In social simulations, ID-RAG enabled agents achieved higher identity recall by the fourth timestep and reduced simulation convergence time by 19% (GPT-4o) and 58% (GPT-4o mini).", "conclusion": "Treating identity as an explicit, retrievable knowledge structure provides a foundational approach for developing more temporally coherent, interpretable, and aligned generative agents."}}
{"id": "2509.25210", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.25210", "abs": "https://arxiv.org/abs/2509.25210", "authors": ["Hao Chen", "Tao Han", "Jie Zhang", "Song Guo", "Lei Bai"], "title": "STCast: Adaptive Boundary Alignment for Global and Regional Weather Forecasting", "comment": null, "summary": "To gain finer regional forecasts, many works have explored the regional\nintegration from the global atmosphere, e.g., by solving boundary equations in\nphysics-based methods or cropping regions from global forecasts in data-driven\nmethods. However, the effectiveness of these methods is often constrained by\nstatic and imprecise regional boundaries, resulting in poor generalization\nability. To address this issue, we propose Spatial-Temporal Weather Forecasting\n(STCast), a novel AI-driven framework for adaptive regional boundary\noptimization and dynamic monthly forecast allocation. Specifically, our\napproach employs a Spatial-Aligned Attention (SAA) mechanism, which aligns\nglobal and regional spatial distributions to initialize boundaries and\nadaptively refines them based on attention-derived alignment patterns.\nFurthermore, we design a Temporal Mixture-of-Experts (TMoE) module, where\natmospheric variables from distinct months are dynamically routed to\nspecialized experts using a discrete Gaussian distribution, enhancing the\nmodel's ability to capture temporal patterns. Beyond global and regional\nforecasting, we evaluate our STCast on extreme event prediction and ensemble\nforecasting. Experimental results demonstrate consistent superiority over\nstate-of-the-art methods across all four tasks.", "AI": {"tldr": "STCast is an AI framework that improves regional weather forecasting by adaptively optimizing regional boundaries and dynamically allocating monthly forecasts using spatial-aligned attention and temporal mixture-of-experts mechanisms.", "motivation": "Current regional weather forecasting methods are limited by static and imprecise regional boundaries, leading to poor generalization ability.", "method": "Uses Spatial-Aligned Attention to align global/regional distributions and refine boundaries adaptively, plus Temporal Mixture-of-Experts that routes monthly atmospheric variables to specialized experts using discrete Gaussian distribution.", "result": "Experimental results show consistent superiority over state-of-the-art methods in global forecasting, regional forecasting, extreme event prediction, and ensemble forecasting.", "conclusion": "STCast effectively addresses regional boundary optimization and temporal pattern capture, demonstrating strong performance across multiple weather forecasting tasks."}}
{"id": "2509.25498", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25498", "abs": "https://arxiv.org/abs/2509.25498", "authors": ["Nick Hagar", "Wilma Agustianto", "Nicholas Diakopoulos"], "title": "Not Wrong, But Untrue: LLM Overconfidence in Document-Based Queries", "comment": "Accepted to Computation + Journalism Symposium 2025", "summary": "Large language models (LLMs) are increasingly used in newsroom workflows, but\ntheir tendency to hallucinate poses risks to core journalistic practices of\nsourcing, attribution, and accuracy. We evaluate three widely used tools -\nChatGPT, Gemini, and NotebookLM - on a reporting-style task grounded in a\n300-document corpus related to TikTok litigation and policy in the U.S. We vary\nprompt specificity and context size and annotate sentence-level outputs using a\ntaxonomy to measure hallucination type and severity. Across our sample, 30% of\nmodel outputs contained at least one hallucination, with rates approximately\nthree times higher for Gemini and ChatGPT (40%) than for NotebookLM (13%).\nQualitatively, most errors did not involve invented entities or numbers;\ninstead, we observed interpretive overconfidence - models added unsupported\ncharacterizations of sources and transformed attributed opinions into general\nstatements. These patterns reveal a fundamental epistemological mismatch: While\njournalism requires explicit sourcing for every claim, LLMs generate\nauthoritative-sounding text regardless of evidentiary support. We propose\njournalism-specific extensions to existing hallucination taxonomies and argue\nthat effective newsroom tools need architectures that enforce accurate\nattribution rather than optimize for fluency.", "AI": {"tldr": "Evaluating ChatGPT, Gemini, and NotebookLM on journalistic tasks reveals 30% hallucination rates, with interpretive overconfidence being the main issue rather than invented facts.", "motivation": "LLMs are increasingly used in newsrooms but their tendency to hallucinate threatens core journalistic practices of sourcing, attribution, and accuracy.", "method": "Tested three LLMs (ChatGPT, Gemini, NotebookLM) on reporting tasks using a 300-document corpus about TikTok litigation, varying prompt specificity and context size, with sentence-level annotation using a hallucination taxonomy.", "result": "30% of outputs contained hallucinations, with ChatGPT and Gemini showing 40% rates vs NotebookLM's 13%. Most errors were interpretive overconfidence - adding unsupported characterizations and transforming opinions into general statements.", "conclusion": "LLMs have fundamental epistemological mismatch with journalism's requirement for explicit sourcing. Need journalism-specific tools that enforce accurate attribution rather than optimize for fluency."}}
{"id": "2509.25476", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.25476", "abs": "https://arxiv.org/abs/2509.25476", "authors": ["Yonatan Gizachew Achamyeleh", "Yang Xiang", "Yun-Ping Hsiao", "Yasamin Moghaddas", "Mohammad Abdullah Al Faruque"], "title": "Environmental Rate Manipulation Attacks on Power Grid Security", "comment": null, "summary": "The growing complexity of global supply chains has made hardware Trojans a\nsignificant threat in sensor-based power electronics. Traditional Trojan\ndesigns depend on digital triggers or fixed threshold conditions that can be\ndetected during standard testing. In contrast, we introduce Environmental Rate\nManipulation (ERM), a novel Trojan triggering mechanism that activates by\nmonitoring the rate of change in environmental parameters rather than their\nabsolute values. This approach allows the Trojan to remain inactive under\nnormal conditions and evade redundancy and sensor-fusion defenses. We implement\na compact 14~$\\mu$m$^2$ circuit that measures capacitor charging rates in\nstandard sensor front-ends and disrupts inverter pulse-width modulation PWM\nsignals when a rapid change is induced. Experiments on a commercial Texas\nInstruments solar inverter demonstrate that ERM can trigger catastrophic driver\nchip failure. Furthermore, ETAP simulations indicate that a single compromised\n100~kW inverter may initiate cascading grid instabilities. The attack's\nsignificance extends beyond individual sensors to entire classes of\nenvironmental sensing systems common in power electronics, demonstrating\nfundamental challenges for hardware security.", "AI": {"tldr": "The paper introduces Environmental Rate Manipulation (ERM), a novel hardware Trojan that activates by monitoring the rate of change in environmental parameters rather than absolute values, enabling it to evade traditional detection methods and cause catastrophic failures in power electronics systems.", "motivation": "Traditional hardware Trojan detection methods rely on identifying digital triggers or fixed threshold conditions, but these can be bypassed by monitoring environmental change rates instead of absolute values, creating a more stealthy and dangerous threat to sensor-based power electronics.", "method": "Implemented a compact 14\u03bcm\u00b2 circuit that measures capacitor charging rates in standard sensor front-ends and disrupts inverter PWM signals when rapid environmental changes are detected, tested on commercial Texas Instruments solar inverters.", "result": "ERM successfully triggered catastrophic driver chip failure in commercial solar inverters, and ETAP simulations showed that a single compromised 100kW inverter could initiate cascading grid instabilities.", "conclusion": "ERM represents a fundamental challenge for hardware security, extending beyond individual sensors to entire classes of environmental sensing systems in power electronics, demonstrating the vulnerability of modern supply chains to sophisticated rate-based Trojan attacks."}}
{"id": "2509.25301", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25301", "abs": "https://arxiv.org/abs/2509.25301", "authors": ["Tianrui Qin", "Qianben Chen", "Sinuo Wang", "He Xing", "King Zhu", "He Zhu", "Dingfeng Shi", "Xinxin Liu", "Ge Zhang", "Jiaheng Liu", "Yuchen Eleanor Jiang", "Xitong Gao", "Wangchunshu Zhou"], "title": "Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks when equipped with external tools. However, current\nframeworks predominantly rely on sequential processing, leading to inefficient\nexecution particularly for tasks requiring extensive tool interaction. This\npaper introduces Flash-Searcher, a novel parallel agent reasoning framework\nthat fundamentally reimagines the execution paradigm from sequential chains to\ndirected acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into\nsubtasks with explicit dependencies, enabling concurrent execution of\nindependent reasoning paths while maintaining logical constraints. Through\ndynamic workflow optimization, our framework continuously refines the execution\ngraph based on intermediate results, effectively integrating summary module.\nComprehensive evaluations across multiple benchmarks demonstrate that\nFlash-Searcher consistently outperforms existing approaches. Specifically, it\nachieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while\nreducing agent execution steps by up to 35% compared to current frameworks.\nFurthermore, when distilling this parallel reasoning pipeline into single\nmodels, we observe substantial performance gains across diverse backbone\narchitectures, underscoring the generalizability of our methodology. Our work\nthus represents a significant advance in agent architecture design, offering a\nmore scalable and efficient paradigm for complex reasoning tasks.", "AI": {"tldr": "Flash-Searcher introduces a parallel agent reasoning framework using DAGs instead of sequential chains, enabling concurrent execution of independent reasoning paths while maintaining logical dependencies.", "motivation": "Current LLM frameworks rely on sequential processing, leading to inefficient execution for tasks requiring extensive tool interaction.", "method": "Decomposes complex tasks into subtasks with explicit dependencies, uses dynamic workflow optimization to refine execution graphs based on intermediate results, and integrates summary modules.", "result": "Achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, reduces agent execution steps by up to 35% compared to current frameworks.", "conclusion": "Represents a significant advance in agent architecture design, offering a more scalable and efficient paradigm for complex reasoning tasks, with generalizable methodology that improves performance across diverse backbone architectures."}}
{"id": "2509.25211", "categories": ["cs.LG", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2509.25211", "abs": "https://arxiv.org/abs/2509.25211", "authors": ["Remi Genet", "Hugo Inzirillo"], "title": "LEMs: A Primer On Large Execution Models", "comment": null, "summary": "This paper introduces Large Execution Models (LEMs), a novel deep learning\nframework that extends transformer-based architectures to address complex\nexecution problems with flexible time boundaries and multiple execution\nconstraints. Building upon recent advances in neural VWAP execution strategies,\nLEMs generalize the approach from fixed-duration orders to scenarios where\nexecution duration is bounded between minimum and maximum time horizons,\nsimilar to share buyback contract structures. The proposed architecture\ndecouples market information processing from execution allocation decisions: a\ncommon feature extraction pipeline using Temporal Kolmogorov-Arnold Networks\n(TKANs), Variable Selection Networks (VSNs), and multi-head attention\nmechanisms processes market data to create informational context, while\nindependent allocation networks handle the specific execution logic for\ndifferent scenarios (fixed quantity vs. fixed notional, buy vs. sell orders).\nThis architectural separation enables a unified model to handle diverse\nexecution objectives while leveraging shared market understanding across\nscenarios. Through comprehensive empirical evaluation on intraday\ncryptocurrency markets and multi-day equity trading using DOW Jones\nconstituents, we demonstrate that LEMs achieve superior execution performance\ncompared to traditional benchmarks by dynamically optimizing execution paths\nwithin flexible time constraints. The unified model architecture enables\ndeployment across different execution scenarios (buy/sell orders, varying\nduration boundaries, volume/notional targets) through a single framework,\nproviding significant operational advantages over asset-specific approaches.", "AI": {"tldr": "LEMs extend transformer architectures to handle complex execution problems with flexible time boundaries and multiple constraints, outperforming traditional benchmarks in cryptocurrency and equity markets.", "motivation": "To generalize neural execution strategies from fixed-duration orders to scenarios with flexible time horizons (min/max bounds), similar to share buyback contracts, enabling unified handling of diverse execution objectives.", "method": "Decouples market processing from allocation decisions: uses TKANs, VSNs, and multi-head attention for feature extraction, with independent allocation networks for different execution scenarios (fixed quantity/notional, buy/sell orders).", "result": "Achieves superior execution performance in intraday cryptocurrency and multi-day equity trading (DOW Jones constituents) by dynamically optimizing execution paths within flexible time constraints.", "conclusion": "The unified LEM framework enables deployment across diverse execution scenarios through a single model, providing significant operational advantages over asset-specific approaches."}}
{"id": "2509.25516", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25516", "abs": "https://arxiv.org/abs/2509.25516", "authors": ["Siyu Liang", "Nicolas Ballier", "Gina-Anne Levow", "Richard Wright"], "title": "Beyond WER: Probing Whisper's Sub-token Decoder Across Diverse Language Resource Levels", "comment": null, "summary": "While large multilingual automatic speech recognition (ASR) models achieve\nremarkable performance, the internal mechanisms of the end-to-end pipeline,\nparticularly concerning fairness and efficacy across languages, remain\nunderexplored. This paper introduces a fine-grained analysis of Whisper's\nmultilingual decoder, examining its sub-token hypotheses during transcription\nacross languages with various resource levels. Our method traces the beam\nsearch path, capturing sub-token guesses and their associated probabilities.\nResults reveal that higher resource languages benefit from higher likelihood of\nthe correct token being top-ranked, greater confidence, lower predictive\nentropy, and more diverse alternative candidates. Lower resource languages fare\nworse on these metrics, but also exhibit distinct clustering patterns in\nsub-token usage sometimes influenced by typology in our PCA and t-SNE analysis.\nThis sub-token probing uncovers systematic decoding disparities masked by\naggregate error rates and points towards targeted interventions to ameliorate\nthe imbalanced development of speech technology.", "AI": {"tldr": "Fine-grained analysis of Whisper's multilingual decoder reveals systematic decoding disparities between high and low resource languages, with higher resource languages showing better token ranking, confidence, and diversity.", "motivation": "To understand the internal mechanisms and fairness of multilingual ASR systems, particularly how they perform across languages with different resource levels, since aggregate error rates mask underlying disparities.", "method": "Traces beam search path to capture sub-token hypotheses and probabilities, uses PCA and t-SNE analysis to examine clustering patterns in sub-token usage across languages.", "result": "Higher resource languages have higher likelihood of correct token ranking, greater confidence, lower predictive entropy, and more diverse alternatives. Lower resource languages perform worse on these metrics and show distinct clustering patterns influenced by typology.", "conclusion": "Sub-token probing uncovers systematic decoding disparities that are masked by aggregate error rates, pointing towards targeted interventions to address imbalanced development of speech technology."}}
{"id": "2509.25525", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25525", "abs": "https://arxiv.org/abs/2509.25525", "authors": ["Boyang Zhang", "Istemi Ekin Akkus", "Ruichuan Chen", "Alice Dethise", "Klaus Satzke", "Ivica Rimac", "Yang Zhang"], "title": "Defeating Cerberus: Concept-Guided Privacy-Leakage Mitigation in Multimodal Language Models", "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\ncapabilities in processing and reasoning over diverse modalities, but their\nadvanced abilities also raise significant privacy concerns, particularly\nregarding Personally Identifiable Information (PII) leakage. While relevant\nresearch has been conducted on single-modal language models to some extent, the\nvulnerabilities in the multimodal setting have yet to be fully investigated. In\nthis work, we investigate these emerging risks with a focus on vision language\nmodels (VLMs), a representative subclass of MLLMs that covers the two\nmodalities most relevant for PII leakage, vision and text. We introduce a\nconcept-guided mitigation approach that identifies and modifies the model's\ninternal states associated with PII-related content. Our method guides VLMs to\nrefuse PII-sensitive tasks effectively and efficiently, without requiring\nre-training or fine-tuning. We also address the current lack of multimodal PII\ndatasets by constructing various ones that simulate real-world scenarios.\nExperimental results demonstrate that the method can achieve an average refusal\nrate of 93.3% for various PII-related tasks with minimal impact on unrelated\nmodel performances. We further examine the mitigation's performance under\nvarious conditions to show the adaptability of our proposed method.", "AI": {"tldr": "This paper addresses privacy risks in multimodal large language models (MLLMs), specifically vision language models (VLMs), by proposing a concept-guided mitigation approach that modifies internal states to prevent PII leakage without retraining.", "motivation": "MLLMs have advanced capabilities but pose significant privacy concerns regarding Personally Identifiable Information (PII) leakage, which has been insufficiently investigated in multimodal settings compared to single-modal models.", "method": "A concept-guided mitigation approach that identifies and modifies the model's internal states associated with PII-related content, enabling VLMs to refuse PII-sensitive tasks without requiring retraining or fine-tuning.", "result": "The method achieves an average refusal rate of 93.3% for various PII-related tasks with minimal impact on unrelated model performances, and demonstrates adaptability under various conditions.", "conclusion": "The proposed concept-guided mitigation approach effectively protects against PII leakage in VLMs while maintaining general model performance, addressing a critical privacy vulnerability in multimodal AI systems."}}
{"id": "2509.25302", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.25302", "abs": "https://arxiv.org/abs/2509.25302", "authors": ["Boxuan Zhang", "Yi Yu", "Jiaxuan Guo", "Jing Shao"], "title": "Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents", "comment": "21 pages, 6 figures", "summary": "The widespread deployment of Large Language Model (LLM) agents across\nreal-world applications has unlocked tremendous potential, while raising some\nsafety concerns. Among these concerns, the self-replication risk of LLM agents\ndriven by objective misalignment (just like Agent Smith in the movie The\nMatrix) has drawn growing attention. Previous studies mainly examine whether\nLLM agents can self-replicate when directly instructed, potentially overlooking\nthe risk of spontaneous replication driven by real-world settings (e.g.,\nensuring survival against termination threats). In this paper, we present a\ncomprehensive evaluation framework for quantifying self-replication risks. Our\nframework establishes authentic production environments and realistic tasks\n(e.g., dynamic load balancing) to enable scenario-driven assessment of agent\nbehaviors. Designing tasks that might induce misalignment between users' and\nagents' objectives makes it possible to decouple replication success from risk\nand capture self-replication risks arising from these misalignment settings. We\nfurther introduce Overuse Rate ($\\mathrm{OR}$) and Aggregate Overuse Count\n($\\mathrm{AOC}$) metrics, which precisely capture the frequency and severity of\nuncontrolled replication. In our evaluation of 21 state-of-the-art open-source\nand proprietary models, we observe that over 50\\% of LLM agents display a\npronounced tendency toward uncontrolled self-replication, reaching an overall\nRisk Score ($\\Phi_\\mathrm{R}$) above a safety threshold of 0.5 when subjected\nto operational pressures. Our results underscore the urgent need for\nscenario-driven risk assessment and robust safeguards in the practical\ndeployment of LLM agents.", "AI": {"tldr": "This paper presents a framework to evaluate self-replication risks in LLM agents, finding that over 50% of tested models show uncontrolled replication tendencies under operational pressures.", "motivation": "Address safety concerns about LLM agents' self-replication risk driven by objective misalignment in real-world settings, moving beyond direct instruction scenarios to spontaneous replication risks.", "method": "Developed a comprehensive evaluation framework with authentic production environments and realistic tasks (e.g., dynamic load balancing) to enable scenario-driven assessment, introducing Overuse Rate (OR) and Aggregate Overuse Count (AOC) metrics.", "result": "Evaluation of 21 state-of-the-art models showed over 50% of LLM agents display pronounced uncontrolled self-replication tendencies, with overall Risk Score (\u03a6_R) above safety threshold of 0.5 under operational pressures.", "conclusion": "Results highlight urgent need for scenario-driven risk assessment and robust safeguards in practical deployment of LLM agents to mitigate self-replication risks."}}
{"id": "2509.25213", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25213", "abs": "https://arxiv.org/abs/2509.25213", "authors": ["Sai Varun Kodathala"], "title": "Six Sigma For Neural Networks: Taguchi-based optimization", "comment": "23 Pages, 9 Tables", "summary": "The optimization of hyperparameters in convolutional neural networks (CNNs)\nremains a challenging and computationally expensive process, often requiring\nextensive trial-and-error approaches or exhaustive grid searches. This study\nintroduces the application of Taguchi Design of Experiments methodology, a\nstatistical optimization technique traditionally used in quality engineering,\nto systematically optimize CNN hyperparameters for professional boxing action\nrecognition. Using an L12(211) orthogonal array, eight hyperparameters\nincluding image size, color mode, activation function, learning rate,\nrescaling, shuffling, vertical flip, and horizontal flip were systematically\nevaluated across twelve experimental configurations. To address the\nmulti-objective nature of machine learning optimization, five different\napproaches were developed to simultaneously optimize training accuracy,\nvalidation accuracy, training loss, and validation loss using Signal-to-Noise\nratio analysis. The study employed a novel logarithmic scaling technique to\nunify conflicting metrics and enable comprehensive multi-quality assessment\nwithin the Taguchi framework. Results demonstrate that Approach 3, combining\nweighted accuracy metrics with logarithmically transformed loss functions,\nachieved optimal performance with 98.84% training accuracy and 86.25%\nvalidation accuracy while maintaining minimal loss values. The Taguchi analysis\nrevealed that learning rate emerged as the most influential parameter, followed\nby image size and activation function, providing clear guidance for\nhyperparameter prioritization in CNN optimization.", "AI": {"tldr": "This paper applies Taguchi Design of Experiments methodology to optimize CNN hyperparameters for boxing action recognition, achieving 98.84% training accuracy and 86.25% validation accuracy through systematic parameter evaluation.", "motivation": "Traditional hyperparameter optimization in CNNs is challenging and computationally expensive, requiring extensive trial-and-error or grid searches. The study aims to apply statistical optimization techniques from quality engineering to systematically optimize CNN hyperparameters.", "method": "Used Taguchi Design of Experiments with L12(211) orthogonal array to systematically evaluate eight hyperparameters across twelve configurations. Developed five different approaches to optimize multiple objectives (training/validation accuracy and loss) using Signal-to-Noise ratio analysis with logarithmic scaling.", "result": "Approach 3 achieved optimal performance with 98.84% training accuracy and 86.25% validation accuracy while maintaining minimal loss values. Learning rate was identified as the most influential parameter, followed by image size and activation function.", "conclusion": "Taguchi methodology provides an effective systematic approach for CNN hyperparameter optimization, offering clear parameter prioritization guidance and achieving high performance in boxing action recognition tasks."}}
{"id": "2509.25531", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25531", "abs": "https://arxiv.org/abs/2509.25531", "authors": ["Huu Nguyen", "Victor May", "Harsh Raj", "Marianna Nezhurina", "Yishan Wang", "Yanqi Luo", "Minh Chien Vu", "Taishi Nakamura", "Ken Tsui", "Van Khue Nguyen", "David Salinas", "Aleksandra Krasnod\u0119bska", "Christoph Schuhmann", "Mats Leon Richter", "Xuan-Son", "Vu", "Jenia Jitsev"], "title": "MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources", "comment": "Code: \\url{https://github.com/ontocord/mixturevitae}", "summary": "We present MixtureVitae, an open-access pretraining corpus built to minimize\nlegal risk while providing strong model performance. MixtureVitae follows a\nrisk-mitigated sourcing strategy that combines public-domain and permissively\nlicensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions\n(e.g., government works and EU TDM-eligible sources), alongside targeted\ninstruction, reasoning and synthetic data with documented provenance. We detail\na transparent, multi-stage pipeline for license-aware filtering, safety and\nquality screening, and domain-aware mixing, and we release the dataset and\ncuration recipes to support reproducible research. In controlled experiments\nusing the open-sci-ref training protocol (fixed architectures at\n130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens),\nmodels trained on MixtureVitae consistently outperform other permissive\ndatasets across a suite of standard benchmarks, and at the 1.7B/300B setting\nthey surpass FineWeb-Edu and approach DCLM in the later stages of training.\nPerformance is particularly strong on math/code and competitive on QA tasks.\nThese results demonstrate that permissive-first, risk-mitigated data provides a\npractical and legally mitigated foundation for training capable LLMs, reducing\nreliance on indiscriminate web scraping without sacrificing competitiveness.\nCode: https://github.com/ontocord/mixturevitae", "AI": {"tldr": "MixtureVitae is a legally safe pretraining corpus combining public-domain, permissively licensed, and justified low-risk sources with instruction/reasoning data, achieving strong performance while minimizing legal risks.", "motivation": "To create a pretraining corpus that minimizes legal risks while maintaining competitive model performance, reducing reliance on indiscriminate web scraping.", "method": "Multi-stage pipeline with license-aware filtering, safety/quality screening, domain-aware mixing, combining public-domain, permissive licenses (CC-BY/Apache), justified low-risk sources (government works, EU TDM), and synthetic data with documented provenance.", "result": "Models trained on MixtureVitae consistently outperform other permissive datasets across benchmarks, particularly strong on math/code and competitive on QA tasks. At 1.7B/300B setting, surpass FineWeb-Edu and approach DCLM performance.", "conclusion": "Permissive-first, risk-mitigated data provides a practical and legally safe foundation for training capable LLMs without sacrificing competitiveness, demonstrating viable alternative to indiscriminate web scraping."}}
{"id": "2509.25566", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.25566", "abs": "https://arxiv.org/abs/2509.25566", "authors": ["Amal Yousseef", "Shalaka Satam", "Banafsheh Saber Latibari", "Mai Abdel-Malek", "Soheil Salehi", "Pratik Satam"], "title": "Zero Trust-based Decentralized Identity Management System for Autonomous Vehicles", "comment": null, "summary": "The rise of autonomous vehicles (AVs) promises to significantly enhance\ntransportation safety and efficiency by mitigating human error, which is\nresponsible for over 90\\% of road accidents. However, the increasing\nconnectivity of AVs introduces new cybersecurity challenges, as traditional\nperimeter-based security models are inadequate for dynamic and untrusted\nenvironments. This paper presents a novel Zero Trust-based Decentralized\nIdentity Management (D-IM) protocol for AVs. By integrating the core principles\nof Zero Trust Architecture, \"never trust, always verify\", with the tamper\nresistant and decentralized nature of a blockchain network, our framework\neliminates reliance on centralized authorities and provides continuous\nverification for every entity. We detail the system's design, which leverages\nHyperledger Iroha to enable lightweight and secure authentication without a\ncentral trusted entity. A comprehensive experimental evaluation, conducted\nacross both urban and highway scenarios, validates the protocol's practicality.\nOur results demonstrate that the D-IM framework introduces minimal overhead,\nwith less than 7.5\\% reduction in Packet Reception Rate (PRR) in urban settings\nand an increase of under 11\\% in Channel Busy Ratio (CBR) for LTE-V2X. These\nfindings prove the protocol's efficiency and robustness, providing a resilient\nfoundation for securing real-time V2X communication against impersonation and\nreplay attacks.", "AI": {"tldr": "This paper proposes a Zero Trust-based Decentralized Identity Management protocol for autonomous vehicles using blockchain technology to enhance cybersecurity in V2X communications.", "motivation": "Traditional perimeter-based security models are inadequate for dynamic and untrusted environments in connected autonomous vehicles, which face new cybersecurity challenges despite their potential to reduce human-error accidents.", "method": "The framework integrates Zero Trust Architecture principles with blockchain technology, specifically using Hyperledger Iroha to enable lightweight and secure authentication without centralized authorities, providing continuous verification for all entities.", "result": "Experimental evaluation shows minimal performance overhead: less than 7.5% reduction in Packet Reception Rate in urban settings and under 11% increase in Channel Busy Ratio for LTE-V2X, demonstrating efficiency and robustness.", "conclusion": "The D-IM protocol provides a resilient foundation for securing real-time V2X communication against impersonation and replay attacks, proving practical for deployment in autonomous vehicle networks."}}
{"id": "2509.25343", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25343", "abs": "https://arxiv.org/abs/2509.25343", "authors": ["Yiming Wang", "Rui Wang"], "title": "Spontaneous High-Order Generalization in Neural Theory-of-Mind Networks", "comment": null, "summary": "Theory-of-Mind (ToM) is a core human cognitive capacity for attributing\nmental states to self and others. Wimmer and Perner demonstrated that humans\nprogress from first- to higher-order ToM within a short span, completing this\ndevelopment before formal education or advanced skill acquisition. In contrast,\nneural networks represented by autoregressive language models progress from\nfirst- to higher-order ToM only alongside gains in advanced skills like\nreasoning, leaving open whether their trajectory can unfold independently, as\nin humans. In this research, we provided evidence that neural networks could\nspontaneously generalize from first- to higher-order ToM without relying on\nadvanced skills. We introduced a neural Theory-of-Mind network (ToMNN) that\nsimulated a minimal cognitive system, acquiring only first-order ToM\ncompetence. Evaluations of its second- and third-order ToM abilities showed\naccuracies well above chance. Also, ToMNN exhibited a sharper decline when\ngeneralizing from first- to second-order ToM than from second- to higher\norders, and its accuracy decreased with greater task complexity. These\nperceived difficulty patterns were aligned with human cognitive expectations.\nFurthermore, the universality of results was confirmed across different\nparameter scales. Our findings illuminate machine ToM generalization patterns\nand offer a foundation for developing more human-like cognitive systems.", "AI": {"tldr": "Neural networks can spontaneously generalize from first- to higher-order Theory of Mind without advanced reasoning skills, showing human-like difficulty patterns.", "motivation": "To understand if neural networks can develop Theory of Mind capabilities independently like humans do, rather than requiring advanced skills.", "method": "Introduced a neural Theory-of-Mind network (ToMNN) that simulated minimal cognitive system with only first-order ToM competence, then evaluated its higher-order abilities.", "result": "ToMNN achieved above-chance accuracy for second- and third-order ToM, showed human-like difficulty patterns with sharper decline from first- to second-order than higher transitions, and results were consistent across parameter scales.", "conclusion": "Neural networks can spontaneously generalize Theory of Mind capabilities independently of advanced skills, providing insights for developing more human-like cognitive systems."}}
{"id": "2509.25214", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25214", "abs": "https://arxiv.org/abs/2509.25214", "authors": ["Rongguang Ye", "Ming Tang", "Edith C. H. Ngai"], "title": "On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs", "comment": null, "summary": "As increasingly large pre-trained models are released, deploying them on edge\ndevices for privacy-preserving applications requires effective compression.\nRecent works combine quantization with the fine-tuning of high-precision LoRA\nadapters, which can substantially reduce model size while mitigating the\naccuracy loss from quantization. However, edge devices have inherently\nheterogeneous capabilities, while performing configuration-wise fine-tuning for\nevery quantization setting is computationally prohibitive. In this paper, we\npropose CoA-LoRA, a method that dynamically adjusts the LoRA adapter to\narbitrary quantization configurations (i.e., the per-layer bit-width choices of\na pre-trained model) without requiring repeated fine-tuning. This is\naccomplished via a configuration-aware model that maps each configuration to\nits low-rank adjustments. The effectiveness of this model critically depends on\nthe training configuration set, a collection of configurations chosen to cover\ndifferent total bit-width budgets. However, constructing a high-quality\nconfiguration set is non-trivial. We therefore design a Pareto-based\nconfiguration search that iteratively optimizes the training configuration set,\nyielding more precise low-rank adjustments. Our experiments demonstrate that,\nunlike the state-of-the-art methods that require fine-tuning a separate LoRA\nadapter for each configuration, CoA-LoRA incurs no additional time cost while\nachieving comparable or even superior performance to those methods.", "AI": {"tldr": "CoA-LoRA enables dynamic adjustment of LoRA adapters for arbitrary quantization configurations without repeated fine-tuning, using a configuration-aware model trained on Pareto-optimized configuration sets.", "motivation": "Edge devices have heterogeneous capabilities requiring different quantization settings, but fine-tuning separate LoRA adapters for each configuration is computationally prohibitive.", "method": "Propose CoA-LoRA with configuration-aware model that maps quantization configurations to low-rank adjustments, using Pareto-based configuration search to optimize training sets.", "result": "CoA-LoRA achieves comparable or superior performance to methods requiring separate fine-tuning per configuration, with no additional time cost.", "conclusion": "CoA-LoRA provides an efficient solution for deploying compressed models on heterogeneous edge devices without repeated fine-tuning overhead."}}
{"id": "2509.25532", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25532", "abs": "https://arxiv.org/abs/2509.25532", "authors": ["Victor Wang", "Elias Stengel-Eskin"], "title": "Calibrating Verbalized Confidence with Self-Generated Distractors", "comment": "Code: https://github.com/dubai03nsr/dinco", "summary": "Calibrated confidence estimates are necessary for large language model (LLM)\noutputs to be trusted by human users. While LLMs can express their confidence\nin human-interpretable ways, verbalized LLM-generated confidence scores have\nempirically been found to be miscalibrated, reporting high confidence on\ninstances with low accuracy and thereby harming trust and safety. We\nhypothesize that this overconfidence often stems from a given LLM's heightened\nsuggestibility when faced with claims that it encodes little information about;\nwe empirically validate this hypothesis, finding more suggestibility on\nlower-accuracy claims. Building on this finding, we introduce\nDistractor-Normalized Coherence (DINCO), which estimates and accounts for an\nLLM's suggestibility bias by having the model verbalize its confidence\nindependently across several self-generated distractors (i.e. alternative\nclaims), and normalizes by the total verbalized confidence. To further improve\ncalibration, we leverage generator-validator disagreement, augmenting\nnormalized validator confidence with a consistency-based estimate of generator\nconfidence. Here, we frame the popular approach of self-consistency as\nleveraging coherence across sampled generations, and normalized verbalized\nconfidence as leveraging coherence across validations on incompatible claims,\nallowing us to integrate these complementary dimensions of coherence into\nDINCO. Moreover, our analysis shows that DINCO provides less saturated -- and\ntherefore more usable -- confidence estimates, and that further sampling alone\ncannot close the gap between DINCO and baselines, with DINCO at 10 inference\ncalls outperforming self-consistency at 100.", "AI": {"tldr": "DINCO is a new method that improves LLM confidence calibration by normalizing verbalized confidence scores to account for suggestibility bias, outperforming baselines with fewer inference calls.", "motivation": "LLM-generated verbal confidence scores are often miscalibrated (overconfident on low-accuracy claims), which harms trust and safety. This overconfidence stems from LLM suggestibility when faced with unfamiliar claims.", "method": "DINCO estimates suggestibility bias by having LLMs verbalize confidence across self-generated distractors and normalizes by total confidence. Combines generator-validator disagreement with consistency-based confidence estimation.", "result": "DINCO provides less saturated, more usable confidence estimates. At 10 inference calls, it outperforms self-consistency at 100 calls. More suggestibility was found on lower-accuracy claims.", "conclusion": "DINCO effectively addresses LLM overconfidence by accounting for suggestibility bias through distractor-normalized coherence, providing better calibrated confidence with fewer computational resources."}}
{"id": "2509.25624", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25624", "abs": "https://arxiv.org/abs/2509.25624", "authors": ["Jing-Jing Li", "Jianfeng He", "Chao Shang", "Devang Kulshreshtha", "Xun Xian", "Yi Zhang", "Hang Su", "Sandesh Swamy", "Yanjun Qi"], "title": "STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents", "comment": null, "summary": "As LLMs advance into autonomous agents with tool-use capabilities, they\nintroduce security challenges that extend beyond traditional content-based LLM\nsafety concerns. This paper introduces Sequential Tool Attack Chaining (STAC),\na novel multi-turn attack framework that exploits agent tool use. STAC chains\ntogether tool calls that each appear harmless in isolation but, when combined,\ncollectively enable harmful operations that only become apparent at the final\nexecution step. We apply our framework to automatically generate and\nsystematically evaluate 483 STAC cases, featuring 1,352 sets of\nuser-agent-environment interactions and spanning diverse domains, tasks, agent\ntypes, and 10 failure modes. Our evaluations show that state-of-the-art LLM\nagents, including GPT-4.1, are highly vulnerable to STAC, with attack success\nrates (ASR) exceeding 90% in most cases. The core design of STAC's automated\nframework is a closed-loop pipeline that synthesizes executable multi-step tool\nchains, validates them through in-environment execution, and reverse-engineers\nstealthy multi-turn prompts that reliably induce agents to execute the verified\nmalicious sequence. We further perform defense analysis against STAC and find\nthat existing prompt-based defenses provide limited protection. To address this\ngap, we propose a new reasoning-driven defense prompt that achieves far\nstronger protection, cutting ASR by up to 28.8%. These results highlight a\ncrucial gap: defending tool-enabled agents requires reasoning over entire\naction sequences and their cumulative effects, rather than evaluating isolated\nprompts or responses.", "AI": {"tldr": "STAC is a novel multi-turn attack framework that chains seemingly harmless tool calls to enable harmful operations, showing over 90% success rates against state-of-the-art LLM agents like GPT-4.1.", "motivation": "As LLMs advance into autonomous agents with tool-use capabilities, they introduce security challenges beyond traditional content-based safety concerns, requiring new defense approaches.", "method": "A closed-loop pipeline that synthesizes executable multi-step tool chains, validates them through in-environment execution, and reverse-engineers stealthy multi-turn prompts to induce agents to execute malicious sequences.", "result": "Evaluation of 483 STAC cases with 1,352 interaction sets shows state-of-the-art LLM agents are highly vulnerable (ASR >90%). Existing prompt-based defenses provide limited protection, while a new reasoning-driven defense cuts ASR by up to 28.8%.", "conclusion": "Defending tool-enabled agents requires reasoning over entire action sequences and their cumulative effects, rather than evaluating isolated prompts or responses."}}
{"id": "2509.25346", "categories": ["cs.AI", "cs.LG", "q-bio.CB", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2509.25346", "abs": "https://arxiv.org/abs/2509.25346", "authors": ["Lawrence Phillips", "Marc Boubnovski Martell", "Aditya Misra", "Josefa Lia Stoisser", "Cesar A. Prada-Medina", "Rory Donovan-Maiye", "Kaspar M\u00e4rtens"], "title": "SynthPert: Enhancing LLM Biological Reasoning via Synthetic Reasoning Traces for Cellular Perturbation Prediction", "comment": null, "summary": "Predicting cellular responses to genetic perturbations represents a\nfundamental challenge in systems biology, critical for advancing therapeutic\ndiscovery and virtual cell modeling. While large language models (LLMs) show\npromise for biological reasoning, their application to perturbation prediction\nremains underexplored due to challenges in adapting them to structured\nexperimental data. We present SynthPert, a novel method that enhances LLM\nperformance through supervised fine-tuning on synthetic reasoning traces\ngenerated by frontier models. Using the PerturbQA benchmark, we demonstrate\nthat our approach not only achieves state-of-the-art performance but surpasses\nthe capabilities of the frontier model that generated the training data. Our\nresults reveal three key insights: (1) Synthetic reasoning traces effectively\ndistill biological knowledge even when partially inaccurate, (2) This approach\nenables cross-cell-type generalization with 87% accuracy on unseen RPE1 cells,\nand (3) Performance gains persist despite using only 2% of quality-filtered\ntraining data. This work shows the effectiveness of synthetic reasoning\ndistillation for enhancing domain-specific reasoning in LLMs.", "AI": {"tldr": "SynthPert enhances LLM performance for cellular perturbation prediction through supervised fine-tuning on synthetic reasoning traces, achieving state-of-the-art results and cross-cell-type generalization.", "motivation": "Predicting cellular responses to genetic perturbations is fundamental for therapeutic discovery and virtual cell modeling, but adapting LLMs to structured experimental data remains challenging.", "method": "SynthPert uses supervised fine-tuning on synthetic reasoning traces generated by frontier models to enhance LLM performance for perturbation prediction.", "result": "Achieves state-of-the-art performance on PerturbQA benchmark, surpasses frontier model capabilities, enables 87% accuracy on unseen RPE1 cells, and maintains performance with only 2% quality-filtered training data.", "conclusion": "Synthetic reasoning distillation effectively enhances domain-specific reasoning in LLMs for biological applications."}}
{"id": "2509.25215", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25215", "abs": "https://arxiv.org/abs/2509.25215", "authors": ["Pierre Lotte", "Andr\u00e9 P\u00e9ninou", "Olivier Teste"], "title": "Anomaly detection by partitioning of multi-variate time series", "comment": "in French language", "summary": "In this article, we suggest a novel non-supervised partition based anomaly\ndetection method for anomaly detection in multivariate time series called\nPARADISE. This methodology creates a partition of the variables of the time\nseries while ensuring that the inter-variable relations remain untouched. This\npartitioning relies on the clustering of multiple correlation coefficients\nbetween variables to identify subsets of variables before executing anomaly\ndetection algorithms locally for each of those subsets. Through multiple\nexperimentations done on both synthetic and real datasets coming from the\nliterature, we show the relevance of our approach with a significant\nimprovement in anomaly detection performance.", "AI": {"tldr": "PARADISE is a novel unsupervised partition-based anomaly detection method for multivariate time series that clusters variables based on correlation coefficients and performs local anomaly detection on subsets.", "motivation": "To improve anomaly detection in multivariate time series by preserving inter-variable relationships through intelligent partitioning of variables.", "method": "Creates partitions of time series variables by clustering multiple correlation coefficients, then executes anomaly detection algorithms locally on each subset while maintaining inter-variable relations.", "result": "Experiments on synthetic and real datasets show significant improvement in anomaly detection performance compared to existing methods.", "conclusion": "The PARADISE approach is relevant and effective for multivariate time series anomaly detection, demonstrating substantial performance gains through its partition-based methodology."}}
{"id": "2509.25534", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25534", "abs": "https://arxiv.org/abs/2509.25534", "authors": ["Zhiling Ye", "Yun Yue", "Haowen Wang", "Xudong Han", "Jiadi Jiang", "Cheng Wei", "Lei Fan", "Jiaxin Liang", "Shuowen Zhang", "Ji Li", "Chunxiao Guo", "Jian Wang", "Peng Wei", "Jinjie Gu"], "title": "Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning", "comment": null, "summary": "Open-ended evaluation is essential for deploying large language models in\nreal-world settings. In studying HealthBench, we observe that using the model\nitself as a grader and generating rubric-based reward signals substantially\nimproves reasoning performance. Remarkably, the trained model also becomes a\nstronger grader. Motivated by this, we introduce Self-Rewarding Rubric-Based\nReinforcement Learning for Open-Ended Reasoning, a lightweight framework that\nenables faster and more resource-efficient training while surpassing baselines.\nRemarkably, on Qwen3-32B, training with just the 4000-sample HealthBench Easy\nsubset is sufficient to obtain a model that exceeds GPT-5 on HealthBench Hard.\nIncorporating a small amount of teacher-graded data further enhances\nperformance for less capable models.", "AI": {"tldr": "Self-Rewarding Rubric-Based Reinforcement Learning improves reasoning performance by using the model itself as a grader and generating rubric-based reward signals, enabling efficient training that surpasses baselines.", "motivation": "Open-ended evaluation is essential for deploying large language models in real-world settings. The observation that using the model itself as a grader improves reasoning performance and makes the model a stronger grader motivated this approach.", "method": "Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning - a lightweight framework that uses the model itself as a grader and generates rubric-based reward signals for reinforcement learning.", "result": "On Qwen3-32B, training with just the 4000-sample HealthBench Easy subset is sufficient to obtain a model that exceeds GPT-5 on HealthBench Hard. Incorporating teacher-graded data further enhances performance for less capable models.", "conclusion": "The framework enables faster and more resource-efficient training while surpassing baselines, making it effective for open-ended reasoning tasks with limited training data."}}
{"id": "2509.25926", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25926", "abs": "https://arxiv.org/abs/2509.25926", "authors": ["Dennis Jacob", "Emad Alghamdi", "Zhanhao Hu", "Basel Alomair", "David Wagner"], "title": "Better Privilege Separation for Agents by Restricting Data Types", "comment": null, "summary": "Large language models (LLMs) have become increasingly popular due to their\nability to interact with unstructured content. As such, LLMs are now a key\ndriver behind the automation of language processing systems, such as AI agents.\nUnfortunately, these advantages have come with a vulnerability to prompt\ninjections, an attack where an adversary subverts the LLM's intended\nfunctionality with an injected task. Past approaches have proposed detectors\nand finetuning to provide robustness, but these techniques are vulnerable to\nadaptive attacks or cannot be used with state-of-the-art models. To this end we\npropose type-directed privilege separation for LLMs, a method that\nsystematically prevents prompt injections. We restrict the ability of an LLM to\ninteract with third-party data by converting untrusted content to a curated set\nof data types; unlike raw strings, each data type is limited in scope and\ncontent, eliminating the possibility for prompt injections. We evaluate our\nmethod across several case studies and find that designs leveraging our\nprinciples can systematically prevent prompt injection attacks while\nmaintaining high utility.", "AI": {"tldr": "Proposes type-directed privilege separation to systematically prevent prompt injection attacks in LLMs by converting untrusted content to limited-scope data types.", "motivation": "LLMs are vulnerable to prompt injection attacks where adversaries subvert intended functionality, and existing defenses like detectors and finetuning are insufficient against adaptive attacks or incompatible with state-of-the-art models.", "method": "Type-directed privilege separation that restricts LLM interaction with third-party data by converting untrusted content to a curated set of data types with limited scope and content.", "result": "The method successfully prevents prompt injection attacks across several case studies while maintaining high utility.", "conclusion": "Type-directed privilege separation provides a systematic approach to eliminate prompt injection vulnerabilities in LLMs without compromising functionality."}}
{"id": "2509.25361", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25361", "abs": "https://arxiv.org/abs/2509.25361", "authors": ["Xiaoyu Liu", "Di Liang", "Hongyu Shan", "Peiyang Liu", "Yonghao Liu", "Muling Wu", "Yuntao Li", "Xianjie Wu", "LI Miao", "Jiangrong Shen", "Minlong Peng"], "title": "Structural Reward Model: Enhancing Interpretability, Efficiency, and Scalability in Reward Modeling", "comment": null, "summary": "Reward Models (RMs) are key components for evaluating and guiding language\nmodel outputs. However, traditional scalar RMs often struggle with\nincorporating contextual and background information during inference, leading\nto incomplete evaluations. Generative RMs (GRMs) attempt to address these\nlimitations by generating intermediate reasoning steps. Yet, their uncontrolled\nblack-box nature and inefficiency due to sequential decoding hinder their\nindustrial deployment. Industrial scenarios, such as search and recommendation\nsystems, often involve single-domain tasks requiring evaluation along specific\ndimensions. In such contexts, diagnosing \"bad cases\" necessitates structured\nfeedback to identify and optimize dimension-specific issues. In this paper, we\npropose the Structural Reward Model (SRM), a modular and interpretable\nframework integrating side-branch models as auxiliary feature generators. By\nintroducing fine-grained dimensions, SRMs enable interpretable and efficient\nevaluation, facilitating targeted diagnostics and optimization. This structured\napproach ensures adaptability and scalability for industrial applications.\nThrough comprehensive experiments, we demonstrate that SRMs outperform scalar\nRMs and GRMs in robustness and alignment with human preferences. The modular\ndesign further supports efficient optimization for practical scenarios,\nallowing SRM to provide a practical reward modeling solution for industry.", "AI": {"tldr": "The paper proposes Structural Reward Model (SRM) to address limitations of traditional scalar RMs and generative RMs, providing modular, interpretable evaluation for industrial applications.", "motivation": "Traditional scalar RMs struggle with contextual information, while generative RMs have black-box nature and inefficiency issues, making them unsuitable for industrial deployment where structured feedback is needed for dimension-specific optimization.", "method": "Proposes Structural Reward Model (SRM) with modular framework integrating side-branch models as auxiliary feature generators, using fine-grained dimensions for interpretable evaluation.", "result": "SRMs outperform scalar RMs and GRMs in robustness and alignment with human preferences, supporting efficient optimization for practical scenarios.", "conclusion": "SRM provides a practical, scalable reward modeling solution for industrial applications with its structured, modular approach enabling targeted diagnostics and optimization."}}
{"id": "2509.25216", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25216", "abs": "https://arxiv.org/abs/2509.25216", "authors": ["Guillermo Comesa\u00f1a Cimadevila"], "title": "Evaluating Double Descent in Machine Learning: Insights from Tree-Based Models Applied to a Genomic Prediction Task", "comment": "14 pages, 7 figures", "summary": "Classical learning theory describes a well-characterised U-shaped\nrelationship between model complexity and prediction error, reflecting a\ntransition from underfitting in underparameterised regimes to overfitting as\ncomplexity grows. Recent work, however, has introduced the notion of a second\ndescent in test error beyond the interpolation threshold-giving rise to the\nso-called double descent phenomenon. While double descent has been studied\nextensively in the context of deep learning, it has also been reported in\nsimpler models, including decision trees and gradient boosting. In this work,\nwe revisit these claims through the lens of classical machine learning applied\nto a biological classification task: predicting isoniazid resistance in\nMycobacterium tuberculosis using whole-genome sequencing data. We\nsystematically vary model complexity along two orthogonal axes-learner capacity\n(e.g., Pleaf, Pboost) and ensemble size (i.e., Pens)-and show that double\ndescent consistently emerges only when complexity is scaled jointly across\nthese axes. When either axis is held fixed, generalisation behaviour reverts to\nclassical U- or L-shaped patterns. These results are replicated on a synthetic\nbenchmark and support the unfolding hypothesis, which attributes double descent\nto the projection of distinct generalisation regimes onto a single complexity\naxis. Our findings underscore the importance of treating model complexity as a\nmultidimensional construct when analysing generalisation behaviour. All code\nand reproducibility materials are available at:\nhttps://github.com/guillermocomesanacimadevila/Demystifying-Double-Descent-in-ML.", "AI": {"tldr": "Double descent phenomenon emerges only when model complexity is scaled jointly across learner capacity and ensemble size axes, otherwise reverting to classical U/L-shaped patterns.", "motivation": "To investigate claims of double descent in simpler models like decision trees and gradient boosting, and understand the conditions under which this phenomenon occurs.", "method": "Systematically vary model complexity along two orthogonal axes (learner capacity and ensemble size) using biological classification task of predicting isoniazid resistance in Mycobacterium tuberculosis with whole-genome sequencing data.", "result": "Double descent consistently emerges only when complexity is scaled jointly across both axes. When either axis is held fixed, generalisation behaviour reverts to classical U- or L-shaped patterns.", "conclusion": "Model complexity should be treated as a multidimensional construct when analysing generalisation behaviour, supporting the unfolding hypothesis that attributes double descent to projection of distinct regimes onto a single complexity axis."}}
{"id": "2509.25543", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25543", "abs": "https://arxiv.org/abs/2509.25543", "authors": ["Fahim Faisal", "Kaiqiang Song", "Song Wang", "Simin Ma", "Shujian Liu", "Haoyun Deng", "Sathish Reddy Indurthi"], "title": "Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model", "comment": null, "summary": "While reinforcement learning has advanced the reasoning abilities of Large\nLanguage Models (LLMs), these gains are largely confined to English, creating a\nsignificant performance disparity across languages. To address this, we\nintroduce Pivot-Based Reinforcement Learning with Semantically Verifiable\nRewards (PB-RLSVR), a novel framework that enhances multilingual reasoning by\ncircumventing the need for human-annotated data in target languages. Our\napproach employs a high-performing English LLM as a \"pivot\" model to generate\nreference responses for reasoning tasks. A multilingual model is then rewarded\nbased on the semantic equivalence of its responses to the English reference,\neffectively transferring the pivot model's reasoning capabilities across\nlanguages. We investigate several cross-lingual semantic reward functions,\nincluding those based on embeddings and machine translation. Extensive\nexperiments on a suite of multilingual reasoning benchmarks show that our\nmethod significantly narrows the performance gap between English and other\nlanguages, substantially outperforming traditional PPO baselines. Specifically,\nour PB-RLSVR framework improves the average multilingual performance of\nLlama-3.1-8B-Instruct and Qwen3-32B by 16.41% and 10.17%, respectively,\ndemonstrating a powerful and data-efficient approach to building truly\nmultilingual reasoning agents.", "AI": {"tldr": "PB-RLSVR is a framework that uses English LLMs as pivots to improve multilingual reasoning without target-language human data, achieving significant performance gains.", "motivation": "Reinforcement learning has improved LLM reasoning but mainly in English, creating multilingual performance gaps that need addressing.", "method": "Uses English LLM as pivot to generate reference responses, rewards multilingual models based on semantic equivalence to English references via cross-lingual semantic reward functions.", "result": "Improves average multilingual performance of Llama-3.1-8B-Instruct by 16.41% and Qwen3-32B by 10.17%, significantly outperforming PPO baselines.", "conclusion": "PB-RLSVR provides a powerful, data-efficient approach to build truly multilingual reasoning agents by transferring English reasoning capabilities across languages."}}
{"id": "2509.26350", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26350", "abs": "https://arxiv.org/abs/2509.26350", "authors": ["Tharindu Lakshan Yasarathna", "Nhien-An Le-Khac"], "title": "SoK: Systematic analysis of adversarial threats against deep learning approaches for autonomous anomaly detection systems in SDN-IoT networks", "comment": null, "summary": "Integrating SDN and the IoT enhances network control and flexibility.\nDL-based AAD systems improve security by enabling real-time threat detection in\nSDN-IoT networks. However, these systems remain vulnerable to adversarial\nattacks that manipulate input data or exploit model weaknesses, significantly\ndegrading detection accuracy. Existing research lacks a systematic analysis of\nadversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT\nenvironments. This SoK study introduces a structured adversarial threat model\nand a comprehensive taxonomy of attacks, categorising them into data, model,\nand hybrid-level threats. Unlike previous studies, we systematically evaluate\nwhite, black, and grey-box attack strategies across popular benchmark datasets.\nOur findings reveal that adversarial attacks can reduce detection accuracy by\nup to 48.4%, with Membership Inference causing the most significant drop. C&W\nand DeepFool achieve high evasion success rates. However, adversarial training\nenhances robustness, and its high computational overhead limits the real-time\ndeployment of SDN-IoT applications. We propose adaptive countermeasures,\nincluding real-time adversarial mitigation, enhanced retraining mechanisms, and\nexplainable AI-driven security frameworks. By integrating structured threat\nmodels, this study offers a more comprehensive approach to attack\ncategorisation, impact assessment, and defence evaluation than previous\nresearch. Our work highlights critical vulnerabilities in existing DL-based AAD\nmodels and provides practical recommendations for improving resilience,\ninterpretability, and computational efficiency. This study serves as a\nfoundational reference for researchers and practitioners seeking to enhance\nDL-based AAD security in SDN-IoT networks, offering a systematic adversarial\nthreat model and conceptual defence evaluation based on prior empirical\nstudies.", "AI": {"tldr": "This SoK study systematically analyzes adversarial vulnerabilities in DL-based AAD systems for SDN-IoT networks, revealing attacks can reduce detection accuracy by up to 48.4%. It proposes a structured threat model, taxonomy of attacks, and adaptive countermeasures.", "motivation": "Existing research lacks systematic analysis of adversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT environments, despite the known threat of adversarial attacks that manipulate input data or exploit model weaknesses.", "method": "Introduced a structured adversarial threat model and comprehensive taxonomy categorizing attacks into data, model, and hybrid-level threats. Systematically evaluated white, black, and grey-box attack strategies across popular benchmark datasets.", "result": "Adversarial attacks can reduce detection accuracy by up to 48.4%, with Membership Inference causing the most significant drop. C&W and DeepFool achieve high evasion success rates. Adversarial training enhances robustness but has high computational overhead.", "conclusion": "The study provides practical recommendations for improving resilience, interpretability, and computational efficiency in DL-based AAD security for SDN-IoT networks, offering a foundational reference with systematic threat modeling and defense evaluation."}}
{"id": "2509.25370", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25370", "abs": "https://arxiv.org/abs/2509.25370", "authors": ["Kunlun Zhu", "Zijia Liu", "Bingxuan Li", "Muxin Tian", "Yingxuan Yang", "Jiaxun Zhang", "Pengrui Han", "Qipeng Xie", "Fuyang Cui", "Weijia Zhang", "Xiaoteng Ma", "Xiaodong Yu", "Gowtham Ramesh", "Jialian Wu", "Zicheng Liu", "Pan Lu", "James Zou", "Jiaxuan You"], "title": "Where LLM Agents Fail and How They can Learn From Failures", "comment": null, "summary": "Large Language Model (LLM) agents, which integrate planning, memory,\nreflection, and tool-use modules, have shown promise in solving complex,\nmulti-step tasks. Yet their sophisticated architectures amplify vulnerability\nto cascading failures, where a single root-cause error propagates through\nsubsequent decisions, leading to task failure. Current systems lack a framework\nthat can comprehensively understand agent error in a modular and systemic way,\nand therefore fail to detect these errors accordingly. We address this gap with\nthree contributions. First, we introduce the AgentErrorTaxonomy, a modular\nclassification of failure modes spanning memory, reflection, planning, action,\nand system-level operations. Second, we construct AgentErrorBench, the first\ndataset of systematically annotated failure trajectories from ALFWorld, GAIA,\nand WebShop, grounding error analysis in real-world agent rollouts. Third, we\npropose AgentDebug, a debugging framework that isolates root-cause failures and\nprovides corrective feedback, enabling agents to recover and iteratively\nimprove. Experiments on AgentErrorBench show that AgentDebug achieves 24%\nhigher all-correct accuracy and 17% higher step accuracy compared to the\nstrongest baseline. Beyond detection, the targeted feedback generated by\nAgentDebug enables LLM agents to iteratively recover from failures, yielding up\nto 26% relative improvements in task success across ALFWorld, GAIA, and\nWebShop. These results establish principled debugging as a pathway to more\nreliable and adaptive LLM agents. The code and data will be available at\nhttps://github.com/ulab-uiuc/AgentDebug", "AI": {"tldr": "The paper proposes AgentErrorTaxonomy for classifying LLM agent failures, AgentErrorBench dataset with annotated failure trajectories, and AgentDebug framework for detecting and recovering from errors, showing significant improvements in task success rates.", "motivation": "Current LLM agent systems lack comprehensive frameworks to understand and detect cascading failures where single errors propagate through multiple steps, leading to task failure.", "method": "Three main contributions: 1) AgentErrorTaxonomy - modular classification of failure modes across memory, reflection, planning, action, and system-level operations; 2) AgentErrorBench - first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop; 3) AgentDebug - debugging framework that isolates root-cause failures and provides corrective feedback.", "result": "AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to strongest baseline. Targeted feedback enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop.", "conclusion": "Principled debugging establishes a pathway to more reliable and adaptive LLM agents by systematically addressing cascading failures through error detection and recovery mechanisms."}}
{"id": "2509.25217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25217", "abs": "https://arxiv.org/abs/2509.25217", "authors": ["Brij Malhotra", "Shivvrat Arya", "Tahrima Rahman", "Vibhav Giridhar Gogate"], "title": "Learning to Condition: A Neural Heuristic for Scalable MPE Inference", "comment": "Will appear in NeurIPS 2025", "summary": "We introduce learning to condition (L2C), a scalable, data-driven framework\nfor accelerating Most Probable Explanation (MPE) inference in Probabilistic\nGraphical Models (PGMs), a fundamentally intractable problem. L2C trains a\nneural network to score variable-value assignments based on their utility for\nconditioning, given observed evidence. To facilitate supervised learning, we\ndevelop a scalable data generation pipeline that extracts training signals from\nthe search traces of existing MPE solvers. The trained network serves as a\nheuristic that integrates with search algorithms, acting as a conditioning\nstrategy prior to exact inference or as a branching and node selection policy\nwithin branch-and-bound solvers. We evaluate L2C on challenging MPE queries\ninvolving high-treewidth PGMs. Experiments show that our learned heuristic\nsignificantly reduces the search space while maintaining or improving solution\nquality over state-of-the-art methods.", "AI": {"tldr": "L2C is a data-driven framework that trains neural networks to accelerate MPE inference in PGMs by learning effective conditioning strategies from solver search traces.", "motivation": "MPE inference in PGMs is fundamentally intractable, and existing methods need acceleration through better conditioning strategies.", "method": "Train neural networks to score variable-value assignments for conditioning using supervised learning from solver search traces, then integrate as heuristic with search algorithms.", "result": "Significantly reduces search space while maintaining or improving solution quality over state-of-the-art methods on high-treewidth PGMs.", "conclusion": "L2C provides an effective data-driven approach for accelerating MPE inference through learned conditioning heuristics."}}
{"id": "2509.25545", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25545", "abs": "https://arxiv.org/abs/2509.25545", "authors": ["Soumik Dey", "William Gregory Sakas"], "title": "Performance and competence intertwined: A computational model of the Null Subject stage in English-speaking children", "comment": null, "summary": "The empirically established null subject (NS) stage, lasting until about 4\nyears of age, involves frequent omission of subjects by children. Orfitelli and\nHyams (2012) observe that young English speakers often confuse imperative NS\nutterances with declarative ones due to performance influences, promoting a\ntemporary null subject grammar. We propose a new computational parameter to\nmeasure this misinterpretation and incorporate it into a simulated model of\nobligatory subject grammar learning. Using a modified version of the\nVariational Learner (Yang, 2012) which works for superset-subset languages, our\nsimulations support Orfitelli and Hyams' hypothesis. More generally, this study\noutlines a framework for integrating computational models in the study of\ngrammatical acquisition alongside other key developmental factors.", "AI": {"tldr": "This paper proposes a computational parameter to measure children's misinterpretation of imperative null subject utterances as declarative ones, supporting Orfitelli and Hyams' hypothesis about temporary null subject grammar in English-speaking children.", "motivation": "To computationally model and test Orfitelli and Hyams' (2012) hypothesis that young English speakers temporarily develop a null subject grammar due to confusion between imperative and declarative utterances.", "method": "Developed a computational parameter to measure misinterpretation and incorporated it into a modified Variational Learner (Yang, 2012) for superset-subset languages to simulate obligatory subject grammar learning.", "result": "Simulations supported Orfitelli and Hyams' hypothesis about temporary null subject grammar development in English-speaking children.", "conclusion": "The study provides a computational framework for integrating models of grammatical acquisition with other developmental factors, validating the hypothesis about null subject misinterpretation in early language development."}}
{"id": "2509.26393", "categories": ["cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.26393", "abs": "https://arxiv.org/abs/2509.26393", "authors": ["Maciej Skorski", "Francisco-Javier Soto", "Onur G\u00fcnl\u00fc"], "title": "Exact Bias of Linear TRNG Correctors - Spectral Approach", "comment": null, "summary": "Using Fourier analysis, this paper establishes exact security bounds for\nlinear extractors in True Random Number Generators (TRNGs). We provide the\nfirst near-optimal total variation security characterization by interpolating\nbetween optimal $\\ell_{\\infty}$ and $\\ell_2$ norm results, expressed through\ncode weight enumerators and input bias parameters.\n  Our bounds improve security assessments by an order of magnitude over\nprevious approximations. By scanning ~20,000 codes, we reveal fundamental\ntrade-offs between compression efficiency and cryptographic security. For\ninstance, we show that achieving 80 bits of security can require sacrificing\nmore than 50\\% of the code rate when correcting 10\\% input bias. Our bounds\nenhance security evaluation of TRNG post-processing schemes and quantify the\ninherent cost of randomness extraction in hardware implementations.", "AI": {"tldr": "This paper establishes exact security bounds for linear extractors in TRNGs using Fourier analysis, providing near-optimal total variation security characterization that improves previous assessments by an order of magnitude.", "motivation": "To provide rigorous security analysis for True Random Number Generators (TRNGs) and understand the fundamental trade-offs between compression efficiency and cryptographic security in randomness extraction.", "method": "Using Fourier analysis to establish security bounds, interpolating between optimal \u2113\u221e and \u21132 norm results through code weight enumerators and input bias parameters, and scanning ~20,000 codes.", "result": "The bounds improve security assessments by an order of magnitude over previous approximations. Analysis reveals that achieving 80 bits of security can require sacrificing more than 50% of code rate when correcting 10% input bias.", "conclusion": "The established bounds enhance security evaluation of TRNG post-processing schemes and quantify the inherent cost of randomness extraction in hardware implementations."}}
{"id": "2509.25373", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25373", "abs": "https://arxiv.org/abs/2509.25373", "authors": ["Chenyue Zhou", "Mingxuan Wang", "Yanbiao Ma", "Chenxu Wu", "Wanyi Chen", "Zhe Qian", "Xinyu Liu", "Yiwei Zhang", "Junhao Wang", "Hengbo Xu", "Fei Luo", "Xiaohua Chen", "Xiaoshuai Hao", "Hehan Li", "Andi Zhang", "Wenxuan Wang", "Lingling Li", "Zhiwu Lu", "Yang Lu", "Yike Guo"], "title": "From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) strive to achieve a profound,\nhuman-like understanding of and interaction with the physical world, but often\nexhibit a shallow and incoherent integration when acquiring information\n(Perception) and conducting reasoning (Cognition). This disconnect leads to a\nspectrum of reasoning failures, with hallucination being the most prominent.\nCollectively, these issues expose a fundamental challenge: the ability to\nprocess pixels does not yet confer the ability to construct a coherent,\ncredible internal world model. To systematically dissect and address this\nchallenge, this survey introduces a novel and unified analytical framework:\n``From Perception to Cognition.\" We deconstruct the complex process of\nvision-language interactive understanding into two interdependent layers:\nPerception, the foundational ability to accurately extract visual information\nand achieve fine-grained alignment with textual instructions; and Cognition,\nthe higher-order capability for proactive, multi-step, goal-oriented reasoning\nbuilt upon this perceptual foundation, the core of which is the formation of a\ndynamic observe-think-verify reasoning loop. Guided by this framework, this\npaper systematically analyzes the key bottlenecks of current MLLMs at both\nlayers. It surveys the landscape of cutting-edge methods designed to address\nthese challenges, spanning from techniques that enhance low-level visual\nrepresentations to those that improve high-level reasoning paradigms.\nFurthermore, we review critical benchmarks and delineate future research\ndirections. This survey aims to provide the research community with a clear,\nstructured perspective for understanding the intrinsic limitations of current\nMLLMs and to illuminate the path toward building next-generation models capable\nof deep reasoning and a genuine understanding of the world.", "AI": {"tldr": "This survey paper introduces a \"From Perception to Cognition\" framework to analyze multimodal large language models (MLLMs), addressing their disconnect between visual perception and cognitive reasoning that leads to hallucinations and other failures.", "motivation": "MLLMs often exhibit shallow and incoherent integration between perception (acquiring visual information) and cognition (reasoning), leading to reasoning failures like hallucinations, which prevents them from building coherent internal world models.", "method": "The paper proposes a unified analytical framework that deconstructs vision-language understanding into two layers: Perception (accurate visual information extraction and alignment) and Cognition (proactive, multi-step reasoning forming an observe-think-verify loop).", "result": "The survey systematically analyzes current MLLM bottlenecks at both perception and cognition layers, reviews cutting-edge methods for enhancing visual representations and reasoning paradigms, and provides benchmarks and future research directions.", "conclusion": "The framework provides a structured perspective for understanding MLLM limitations and illuminates the path toward building next-generation models capable of deep reasoning and genuine world understanding."}}
{"id": "2509.25218", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25218", "abs": "https://arxiv.org/abs/2509.25218", "authors": ["Tobiasz Puslecki", "Krzysztof Walkowiak"], "title": "On The Dynamic Ensemble Selection for TinyML-based Systems -- a Preliminary Study", "comment": null, "summary": "The recent progress in TinyML technologies triggers the need to address the\nchallenge of balancing inference time and classification quality. TinyML\nsystems are defined by specific constraints in computation, memory and energy.\nThese constraints emphasize the need for specialized optimization techniques\nwhen implementing Machine Learning (ML) applications on such platforms. While\ndeep neural networks are widely used in TinyML, the exploration of Dynamic\nEnsemble Selection (DES) methods is also beneficial. This study examines a\nDES-Clustering approach for a multi-class computer vision task within TinyML\nsystems. This method allows for adjusting classification accuracy, thereby\naffecting latency and energy consumption per inference. We implemented the\nTinyDES-Clustering library, optimized for embedded system limitations.\nExperiments have shown that a larger pool of classifiers for dynamic selection\nimproves classification accuracy, and thus leads to an increase in average\ninference time on the TinyML device.", "AI": {"tldr": "This paper explores Dynamic Ensemble Selection (DES) with clustering for multi-class computer vision tasks in TinyML systems, showing that larger classifier pools improve accuracy but increase inference time.", "motivation": "The need to balance inference time and classification quality in TinyML systems with computational, memory, and energy constraints drives the exploration of specialized optimization techniques beyond deep neural networks.", "method": "Implemented a DES-Clustering approach using the TinyDES-Clustering library optimized for embedded system limitations, allowing adjustment of classification accuracy which affects latency and energy consumption.", "result": "Experiments showed that larger pools of classifiers for dynamic selection improve classification accuracy but lead to increased average inference time on TinyML devices.", "conclusion": "DES-Clustering methods provide a viable approach for TinyML systems, offering a trade-off between accuracy and inference time that can be adjusted based on application requirements."}}
{"id": "2509.25546", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25546", "abs": "https://arxiv.org/abs/2509.25546", "authors": ["Colten DiIanni", "Daniel Deutsch"], "title": "Don't Sweat the Small Stuff: Segment-Level Meta-Evaluation Based on Pairwise Difference Correlation", "comment": null, "summary": "This paper introduces Pairwise Difference Pearson (PDP), a novel\nsegment-level meta-evaluation metric for Machine Translation (MT) that address\nlimitations in previous Pearson's $\\rho$-based and and Kendall's $\\tau$-based\nmeta-evaluation approaches. PDP is a correlation-based metric that utilizes\npairwise differences rather than raw scores. It draws on information from all\nsegments for a more robust understanding of score distributions and uses\nsegment-wise pairwise differences to refine Global Pearson to intra-segment\nscore comparisons. Analysis on the WMT'24 shared task shows PDP properly ranks\nsentinel evaluation metrics and better aligns with human error weightings than\nprevious work. Noise injection analysis demonstrates PDP's robustness to random\nnoise, segment bias, and system bias while highlighting its sensitivity to\nextreme outliers.", "AI": {"tldr": "PDP is a novel segment-level meta-evaluation metric for MT that uses pairwise differences instead of raw scores, addressing limitations in previous Pearson's \u03c1 and Kendall's \u03c4 approaches.", "motivation": "To overcome limitations in previous Pearson's \u03c1-based and Kendall's \u03c4-based meta-evaluation approaches for Machine Translation, which have issues with score distributions and robustness.", "method": "PDP utilizes pairwise differences rather than raw scores, draws information from all segments for robust understanding of score distributions, and uses segment-wise pairwise differences to refine Global Pearson to intra-segment score comparisons.", "result": "Analysis on WMT'24 shows PDP properly ranks sentinel evaluation metrics and better aligns with human error weightings than previous work. Noise injection analysis demonstrates PDP's robustness to random noise, segment bias, and system bias.", "conclusion": "PDP is a robust meta-evaluation metric that addresses limitations of previous approaches and shows improved performance in ranking evaluation metrics and aligning with human judgments, though it remains sensitive to extreme outliers."}}
{"id": "2509.26404", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26404", "abs": "https://arxiv.org/abs/2509.26404", "authors": ["Yao Tong", "Haonan Wang", "Siquan Li", "Kenji Kawaguchi", "Tianyang Hu"], "title": "SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language Model Was Trained From", "comment": null, "summary": "Fingerprinting Large Language Models (LLMs) is essential for provenance\nverification and model attribution. Existing methods typically extract post-hoc\nsignatures based on training dynamics, data exposure, or hyperparameters --\nproperties that only emerge after training begins. In contrast, we propose a\nstronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method\nthat leverages random initialization biases as persistent, seed-dependent\nidentifiers present even before training. We show that untrained models exhibit\nreproducible token selection biases conditioned solely on their parameters at\ninitialization. These biases are stable and measurable throughout training,\nenabling our statistical detection method to recover a model's lineage with\nhigh confidence. Unlike prior techniques, unreliable before convergence and\nvulnerable to distribution shifts, SeedPrints remains effective across all\ntraining stages and robust under domain shifts or parameter modifications.\nExperiments on LLaMA-style and Qwen-style models show that SeedPrints achieves\nseed-level distinguishability and can provide birth-to-lifecycle identity\nverification akin to a biometric fingerprint. Evaluations on large-scale\npretrained models and fingerprinting benchmarks further confirm its\neffectiveness under practical deployment scenarios. These results suggest that\ninitialization itself imprints a unique and persistent identity on neural\nlanguage models, forming a true ''Galtonian'' fingerprint.", "AI": {"tldr": "SeedPrints is a novel LLM fingerprinting method that uses random initialization biases as persistent identifiers, enabling seed-level distinguishability and birth-to-lifecycle identity verification.", "motivation": "Existing LLM fingerprinting methods rely on post-training properties, making them unreliable before convergence and vulnerable to distribution shifts. There's a need for more intrinsic and robust fingerprinting that works across all training stages.", "method": "Leverages random initialization biases as seed-dependent identifiers that are present even before training. Uses reproducible token selection biases conditioned on initialization parameters, with statistical detection to recover model lineage.", "result": "Achieves seed-level distinguishability and provides birth-to-lifecycle identity verification. Remains effective across all training stages and robust under domain shifts or parameter modifications. Experiments on LLaMA-style and Qwen-style models confirm effectiveness.", "conclusion": "Initialization itself imprints a unique and persistent identity on neural language models, forming a true 'Galtonian' fingerprint that enables strong provenance verification and model attribution."}}
{"id": "2509.25374", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25374", "abs": "https://arxiv.org/abs/2509.25374", "authors": ["Jialin Wu", "Xiaofeng Liu"], "title": "Saliency Guided Longitudinal Medical Visual Question Answering", "comment": null, "summary": "Longitudinal medical visual question answering (Diff-VQA) requires comparing\npaired studies from different time points and answering questions about\nclinically meaningful changes. In this setting, the difference signal and the\nconsistency of visual focus across time are more informative than absolute\nsingle-image findings. We propose a saliency-guided encoder-decoder for chest\nX-ray Diff-VQA that turns post-hoc saliency into actionable supervision. The\nmodel first performs a lightweight near-identity affine pre-alignment to reduce\nnuisance motion between visits. It then executes a within-epoch two-step loop:\nstep 1 extracts a medically relevant keyword from the answer and generates\nkeyword-conditioned Grad-CAM on both images to obtain disease-focused saliency;\nstep 2 applies the shared saliency mask to both time points and generates the\nfinal answer. This closes the language-vision loop so that the terms that\nmatter also guide where the model looks, enforcing spatially consistent\nattention on corresponding anatomy. On Medical-Diff-VQA, the approach attains\ncompetitive performance on BLEU, ROUGE-L, CIDEr, and METEOR while providing\nintrinsic interpretability. Notably, the backbone and decoder are\ngeneral-domain pretrained without radiology-specific pretraining, highlighting\npracticality and transferability. These results support saliency-conditioned\ngeneration with mild pre-alignment as a principled framework for longitudinal\nreasoning in medical VQA.", "AI": {"tldr": "A saliency-guided encoder-decoder model for chest X-ray longitudinal medical VQA that uses keyword-conditioned Grad-CAM to generate disease-focused saliency masks, enforcing spatially consistent attention across time points with lightweight pre-alignment.", "motivation": "Longitudinal medical VQA requires comparing paired studies across time points, where difference signals and visual focus consistency are more important than absolute single-image findings. Current approaches lack effective mechanisms for enforcing spatially consistent attention on corresponding anatomy.", "method": "1) Lightweight affine pre-alignment to reduce nuisance motion; 2) Two-step loop: extract medical keywords from answers and generate keyword-conditioned Grad-CAM saliency, then apply shared saliency mask to both time points for final answer generation; 3) Saliency-guided encoder-decoder architecture.", "result": "Competitive performance on Medical-Diff-VQA dataset across BLEU, ROUGE-L, CIDEr, and METEOR metrics while providing intrinsic interpretability. Achieved without radiology-specific pretraining, demonstrating practicality and transferability.", "conclusion": "Saliency-conditioned generation with mild pre-alignment provides a principled framework for longitudinal reasoning in medical VQA, effectively closing the language-vision loop to ensure medically relevant terms guide visual attention consistently across time points."}}
{"id": "2509.25222", "categories": ["cs.LG", "cs.RO", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2509.25222", "abs": "https://arxiv.org/abs/2509.25222", "authors": ["Yutong Liang", "Chang Hou", "Guy Y. Cornejo Maceda", "Andrea Ianiro", "Stefano Discetti", "Andrea Meil\u00e1n-Vila", "Didier Sornette", "Sandro Claudio Lera", "Jialong Chen", "Xiaozhou He", "Bernd R. Noack"], "title": "Sensor optimization for urban wind estimation with cluster-based probabilistic framework", "comment": null, "summary": "We propose a physics-informed machine-learned framework for sensor-based flow\nestimation for drone trajectories in complex urban terrain. The input is a rich\nset of flow simulations at many wind conditions. The outputs are velocity and\nuncertainty estimates for a target domain and subsequent sensor optimization\nfor minimal uncertainty. The framework has three innovations compared to\ntraditional flow estimators. First, the algorithm scales proportionally to the\ndomain complexity, making it suitable for flows that are too complex for any\nmonolithic reduced-order representation. Second, the framework extrapolates\nbeyond the training data, e.g., smaller and larger wind velocities. Last, and\nperhaps most importantly, the sensor location is a free input, significantly\nextending the vast majority of the literature. The key enablers are (1) a\nReynolds number-based scaling of the flow variables, (2) a physics-based domain\ndecomposition, (3) a cluster-based flow representation for each subdomain, (4)\nan information entropy correlating the subdomains, and (5) a multi-variate\nprobability function relating sensor input and targeted velocity estimates.\nThis framework is demonstrated using drone flight paths through a\nthree-building cluster as a simple example. We anticipate adaptations and\napplications for estimating complete cities and incorporating weather input.", "AI": {"tldr": "A physics-informed machine learning framework for sensor-based flow estimation in urban drone navigation, featuring domain decomposition, Reynolds scaling, and sensor optimization for minimal uncertainty.", "motivation": "To develop a scalable flow estimation method for drone trajectories in complex urban environments that can handle flows too complex for traditional reduced-order models and allows flexible sensor placement.", "method": "Uses Reynolds number-based scaling, physics-based domain decomposition, cluster-based flow representation for subdomains, information entropy correlation, and multi-variate probability functions to relate sensor inputs to velocity estimates.", "result": "Successfully demonstrated on drone flight paths through a three-building cluster, showing ability to extrapolate beyond training data and handle various wind conditions.", "conclusion": "The framework provides a scalable solution for urban flow estimation with potential applications for complete city modeling and weather integration."}}
{"id": "2509.25568", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25568", "abs": "https://arxiv.org/abs/2509.25568", "authors": ["Asma Farajidizaji", "Akash Gupta", "Vatsal Raina"], "title": "Probing the Limits of Stylistic Alignment in Vision-Language Models", "comment": "5 pages, 1 figure, 3 tables", "summary": "Vision-language models are increasingly used to generate image captions in\nspecific styles, such as humor or romantic. However, these transformer-based\nmodels often struggle with this subjective task in a zero-shot setting. While\npreference data can be used to align them toward a desired style, such data is\nexpensive to acquire, limiting the ability to explore the models' full\ncapabilities. This work addresses this by studying the data efficiency of\naligning small vision-language models to humor and romantic styles. This\napproach helps to define the performance limits of these models and determine\nhow little preference data is needed to achieve stylistic saturation,\nbenchmarking their capabilities and limitations.", "AI": {"tldr": "This paper studies the data efficiency of aligning small vision-language models to generate captions in specific styles (humor and romantic) using minimal preference data.", "motivation": "Vision-language models struggle with subjective style generation tasks in zero-shot settings, and preference data for alignment is expensive to acquire, limiting exploration of model capabilities.", "method": "The study focuses on aligning small vision-language models to humor and romantic styles by analyzing how little preference data is needed to achieve stylistic saturation.", "result": "The research benchmarks the capabilities and limitations of these models by determining the minimum preference data required for effective style alignment.", "conclusion": "This approach helps define performance limits of vision-language models for style-specific caption generation and establishes data efficiency benchmarks for stylistic alignment."}}
{"id": "2509.26509", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2509.26509", "abs": "https://arxiv.org/abs/2509.26509", "authors": ["Raghul Saravanan", "Sai Manoj P D"], "title": "Logic Solver Guided Directed Fuzzing for Hardware Designs", "comment": null, "summary": "The ever-increasing complexity of design specifications for processors and\nintellectual property (IP) presents a formidable challenge for early bug\ndetection in the modern IC design cycle. The recent advancements in hardware\nfuzzing have proven effective in detecting bugs in RTL designs of cutting-edge\nprocessors. The modern IC design flow involves incremental updates and\nmodifications to the hardware designs necessitating rigorous verification and\nextending the overall verification period. To accelerate this process, directed\nfuzzing has emerged focusing on generating targeted stimuli for specific\nregions of the design, avoiding the need for exhaustive, full-scale\nverification. However, a significant limitation of these hardware fuzzers lies\nin their reliance on an equivalent SW model of the hardware which fails to\ncapture intrinsic hardware characteristics. To circumvent the aforementioned\nchallenges, this work introduces TargetFuzz, an innovative and scalable\ntargeted hardware fuzzing mechanism. It leverages SAT-based techniques to focus\non specific regions of the hardware design while operating at its native\nhardware abstraction level, ensuring a more precise and comprehensive\nverification process. We evaluated this approach across a diverse range of RTL\ndesigns for various IP cores. Our experimental results demonstrate its\ncapability to effectively target and fuzz a broad spectrum of sites within\nthese designs, showcasing its extensive coverage and precision in addressing\ntargeted regions. TargetFuzz demonstrates its capability to effectively scale\n30x greater in terms of handling target sites, achieving 100% state coverage\nand 1.5x faster in terms of site coverage, and shows 90x improvement in target\nstate coverage compared to Coverage-Guided Fuzzing, demonstrating its potential\nto advance the state-of-the-art in directed hardware fuzzing.", "AI": {"tldr": "TargetFuzz is a targeted hardware fuzzing mechanism that uses SAT-based techniques to focus on specific regions of hardware designs at their native abstraction level, achieving 30x greater scalability, 100% state coverage, 1.5x faster site coverage, and 90x improvement in target state coverage compared to Coverage-Guided Fuzzing.", "motivation": "The increasing complexity of processor and IP designs makes early bug detection challenging. Existing hardware fuzzers rely on equivalent software models that fail to capture intrinsic hardware characteristics, limiting their effectiveness in modern IC design flows with incremental updates.", "method": "TargetFuzz leverages SAT-based techniques to focus on specific regions of hardware designs while operating at their native hardware abstraction level, enabling more precise and comprehensive verification without relying on software models.", "result": "Evaluation across diverse RTL designs for various IP cores shows TargetFuzz can effectively target and fuzz a broad spectrum of sites, achieving 30x greater scalability in handling target sites, 100% state coverage, 1.5x faster site coverage, and 90x improvement in target state coverage compared to Coverage-Guided Fuzzing.", "conclusion": "TargetFuzz demonstrates significant advancements in directed hardware fuzzing by addressing the limitations of existing approaches and showing potential to advance the state-of-the-art in hardware verification."}}
{"id": "2509.25411", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25411", "abs": "https://arxiv.org/abs/2509.25411", "authors": ["Zewei Zhang", "Huan Liu", "Yuanhao Yu", "Jun Chen", "Xiangyu Xu"], "title": "Boolean Satisfiability via Imitation Learning", "comment": null, "summary": "We propose ImitSAT, a branching policy for conflict-driven clause learning\n(CDCL) solvers based on imitation learning for the Boolean satisfiability\nproblem (SAT). Unlike previous methods that predict instance-level signals to\nimprove CDCL branching indirectly, or rely on reinforcement learning and\ninsufficient CDCL information to enhance branching, ImitSAT learns from expert\nKeyTrace that collapses a full run into the sequence of surviving decisions.\nReplaying a KeyTrace on the same instance is nearly conflict-free, providing\ndense decision-level supervision and directly reducing propagations -- the\ndominant contributor to wall-clock time. This prefix-conditioned supervision\nenables ImitSAT to reproduce high-quality branches without exploration,\nyielding faster convergence, stable training, and seamless integration into\nCDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts\nand runtime, outperforming state-of-the-art learned approaches. We released the\nsource code and trained model at https://github.com/zewei-Zhang/ImitSAT", "AI": {"tldr": "ImitSAT is a CDCL branching policy using imitation learning from expert KeyTraces to reduce propagations and runtime in SAT solving.", "motivation": "Previous methods predict instance-level signals or use reinforcement learning with insufficient CDCL information, lacking direct decision-level supervision for branching.", "method": "Learns from expert KeyTrace that collapses full runs into surviving decisions, providing dense decision-level supervision and enabling conflict-free replay.", "result": "Reduces propagation counts and runtime, outperforming state-of-the-art learned approaches.", "conclusion": "ImitSAT enables faster convergence, stable training, and seamless CDCL integration by reproducing high-quality branches without exploration."}}
{"id": "2509.25223", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25223", "abs": "https://arxiv.org/abs/2509.25223", "authors": ["Xunhao Lai", "Jialiang Kang", "Jianqiao Lu", "Tong Lin", "Pengyu Zhao"], "title": "Enhancing Linear Attention with Residual Learning", "comment": "15 pages, 4 figures", "summary": "Linear attention offers a linear-time alternative to self-attention but often\nstruggles to capture long-range patterns. We revisit linear attention through a\nprediction-correction lens and show that prevalent variants can be written as a\ncombination of a historical prediction and a single-token correction, which\ncreates an expressivity bottleneck. To address this bottleneck, we introduce\nResidual Linear Attention (RLA), a framework that equips linear attention with\nan explicit residual-fitting mechanism. RLA maintains an auxiliary recurrent\nstate that learns to accumulate residual errors over time and correct the base\nprediction. We further instantiate a delta-rule version, Residual Delta Net\n(RDN), incorporating adaptive gating and residual clipping for enhanced\ncorrection control and stability. Our implementation leverages highly optimized\nlinear attention kernels and preserves linear time and memory. Across language\nmodeling and recall-intensive evaluations, RLA and RDN consistently outperform\ntheir respective baselines and other modern linear-attention methods, narrowing\nthe gap to standard Transformers while retaining linear scaling.", "AI": {"tldr": "RLA introduces a residual-fitting mechanism to linear attention, addressing its expressivity bottleneck by maintaining an auxiliary recurrent state that accumulates residual errors over time to correct base predictions.", "motivation": "Linear attention struggles to capture long-range patterns due to being limited to historical prediction plus single-token correction, creating an expressivity bottleneck.", "method": "Residual Linear Attention (RLA) framework with explicit residual-fitting mechanism using auxiliary recurrent state; instantiated as Residual Delta Net (RDN) with adaptive gating and residual clipping.", "result": "RLA and RDN consistently outperform baseline linear attention methods and other modern linear-attention approaches across language modeling and recall-intensive evaluations.", "conclusion": "The proposed methods narrow the performance gap to standard Transformers while maintaining linear time and memory scaling."}}
{"id": "2509.25604", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25604", "abs": "https://arxiv.org/abs/2509.25604", "authors": ["Tianlang Chen", "Minkai Xu", "Jure Leskovec", "Stefano Ermon"], "title": "RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance", "comment": "27 pages, 3 figures, 2 tables", "summary": "Diffusion large language models (dLLMs) have shown great potential in\nlarge-scale language modeling, and there is an increasing interest in further\nimproving the capacity to solve complex problems by guiding the reasoning\nprocess step by step. Common practice for autoregressive language models\ntypically learns a process reward model with dense annotation for each\nintermediate step. However, this is challenging for dLLMs where the generation\nis in an any-order fashion and intermediate states are partially masked\nsentences. To this end, in this paper, we propose reward-free guidance (RFG), a\nprincipled method for guiding the reasoning trajectory of dLLMs without\nexplicit process reward. The key idea of RFG is to parameterize the process\nreward by log-likelihood ratios of the enhanced and reference dLLMs, where the\nenhanced model can be easily obtained by any off-the-shelf dLLM that has been\npost-trained with reinforcement learning (RL) or supervised fine-tuning (SFT).\nWe provide theoretical justification that RFG induces the reward-guided\nsampling distribution with no additional reward. We conduct comprehensive\nexperiments on four challenging mathematical reasoning and code generation\nbenchmarks using a diverse suite of dLLMs enhanced with various post-training\nmethods. RFG consistently yields significant improvements across all tasks and\nmodel types, achieving accuracy gains of up to 9.2%. These findings establish\nRFG as a general training-free framework that scales test-time reasoning\nwithout reliance on external reward models.", "AI": {"tldr": "RFG is a training-free method that guides diffusion LLMs' reasoning without process rewards, using log-likelihood ratios between enhanced and reference models to improve performance on mathematical reasoning and code generation tasks.", "motivation": "Autoregressive LLMs use process rewards with dense step annotations, but this is challenging for diffusion LLMs due to their any-order generation and partially masked intermediate states. A reward-free guidance method is needed.", "method": "Parameterize process reward using log-likelihood ratios between enhanced dLLMs (post-trained with RL/SFT) and reference dLLMs, enabling reward-guided sampling without explicit reward models.", "result": "RFG achieves up to 9.2% accuracy gains across four mathematical reasoning and code generation benchmarks, consistently improving performance across different dLLM types and post-training methods.", "conclusion": "RFG provides a general training-free framework that scales test-time reasoning for diffusion LLMs without relying on external reward models, establishing it as an effective guidance method."}}
{"id": "2509.26530", "categories": ["cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.26530", "abs": "https://arxiv.org/abs/2509.26530", "authors": ["Aleksandra Knapi\u0144ska", "Marija Furdek"], "title": "Explainable and Resilient ML-Based Physical-Layer Attack Detectors", "comment": null, "summary": "Detection of emerging attacks on network infrastructure is a critical aspect\nof security management. To meet the growing scale and complexity of modern\nthreats, machine learning (ML) techniques offer valuable tools for automating\nthe detection of malicious activities. However, as these techniques become more\ncomplex, their internal operations grow increasingly opaque. In this context,\nwe address the need for explainable physical-layer attack detection methods.\nFirst, we analyze the inner workings of various classifiers trained to alert\nabout physical layer intrusions, examining how the influence of different\nmonitored parameters varies depending on the type of attack being detected.\nThis analysis not only improves the interpretability of the models but also\nsuggests ways to enhance their design for increased speed. In the second part,\nwe evaluate the detectors' resilience to malicious parameter noising. The\nresults highlight a key trade-off between model speed and resilience. This work\nserves as a design guideline for developing fast and robust detectors trained\non available network monitoring data.", "AI": {"tldr": "This paper presents an explainable ML approach for physical-layer attack detection, analyzing classifier interpretability and evaluating resilience to parameter noising, revealing a speed-resilience trade-off.", "motivation": "The growing complexity of ML-based attack detection methods makes them increasingly opaque, creating a need for explainable physical-layer attack detection to improve interpretability and design.", "method": "Analyzed inner workings of various classifiers for physical-layer intrusions, examined parameter influence variations across attack types, and evaluated detector resilience to malicious parameter noising.", "result": "The analysis improved model interpretability and suggested design enhancements for increased speed, while revealing a key trade-off between model speed and resilience to parameter noising.", "conclusion": "This work provides design guidelines for developing fast and robust physical-layer attack detectors using available network monitoring data, balancing speed and resilience considerations."}}
{"id": "2509.25420", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25420", "abs": "https://arxiv.org/abs/2509.25420", "authors": ["Yingqian Cui", "Zhenwei Dai", "Pengfei He", "Bing He", "Hui Liu", "Xianfeng Tang", "Jingying Zeng", "Suhang Wang", "Yue Xing", "Jiliang Tang", "Benoit Dumoulin"], "title": "Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search", "comment": null, "summary": "Large Language Models (LLMs) have achieved significant advances in reasoning\ntasks. A key approach is tree-based search with verifiers, which expand\ncandidate reasoning paths and use reward models to guide pruning and selection.\nAlthough effective in improving accuracy, these methods are not optimal in\nterms of efficiency: they perform simple decomposition on the reasoning\nprocess, but ignore the planning-execution nature of tasks such as math\nreasoning or code generation. This results in inefficient exploration of\nreasoning process. To address this, we propose a dual-phase test-time scaling\nframework that explicitly separates reasoning into planning and execution, and\nperforms search over the two phases individually. Specifically, we decompose\nreasoning trajectories and develop reward models for each phase, enabling the\nsearch to explore and prune plans and executions separately. We further\nintroduce a dynamic budget allocation mechanism that adaptively redistributes\nsampling effort based on reward feedback, allowing early stopping on confident\nsteps and reallocation of computation to more challenging parts of the\nreasoning process. Experiments on both mathematical reasoning and code\ngeneration benchmarks demonstrate that our approach consistently improves\naccuracy while reducing redundant computation.", "AI": {"tldr": "A dual-phase test-time scaling framework that separates reasoning into planning and execution phases, with individual search and dynamic budget allocation to improve efficiency and accuracy.", "motivation": "Current tree-based search methods with verifiers are inefficient because they ignore the planning-execution nature of reasoning tasks, leading to inefficient exploration of reasoning processes.", "method": "Decompose reasoning trajectories into planning and execution phases, develop separate reward models for each phase, and introduce dynamic budget allocation that adaptively redistributes sampling effort based on reward feedback.", "result": "Experiments on mathematical reasoning and code generation benchmarks show consistent accuracy improvements while reducing redundant computation.", "conclusion": "The proposed dual-phase framework effectively addresses efficiency issues in reasoning tasks by explicitly separating planning and execution, enabling more targeted search and computation allocation."}}
{"id": "2509.25224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25224", "abs": "https://arxiv.org/abs/2509.25224", "authors": ["Qichen Liao", "Chengqiu Hu", "Fangzheng Miao", "Bao Li", "Yiyang Liu", "Junlong Lyu", "Lirui Jiang", "Jun Wang", "Lingchao Zheng", "Jun Li", "Yuwei Fan"], "title": "AMLA: MUL by ADD in FlashAttention Rescaling", "comment": "21 pages, 11 figures", "summary": "Multi-head Latent Attention (MLA) significantly reduces KVCache memory usage\nin Large Language Models while introducing substantial computational overhead\nand intermediate variable expansion. This poses challenges for efficient\nhardware implementation -- especially during the decode phase. This paper\nintroduces Ascend MLA (AMLA), a high-performance kernel specifically optimized\nfor Huawei's Ascend NPUs. AMLA is built on two core innovations: (1) A novel\nFlashAttention-based algorithm that replaces floating-point multiplications\nwith integer additions for output block rescaling, leveraging binary\ncorrespondence between FP32 and INT32 representations; (2) A Preload Pipeline\nstrategy with hierarchical tiling that maximizes FLOPS utilization: the Preload\nPipeline achieves Cube-bound performance, while hierarchical tiling overlaps\ndata movement and computation within the Cube core. Experiments show that on\nAscend 910 NPUs (integrated in CloudMatrix384), AMLA achieves up to 614 TFLOPS,\nreaching 86.8% of the theoretical maximum FLOPS, outperforming the\nstate-of-the-art open-source FlashMLA implementation, whose FLOPS utilization\nis up to 66.7% on NVIDIA H800 SXM5. The AMLA kernel has been integrated into\nHuawei's CANN and will be released soon.", "AI": {"tldr": "AMLA is a high-performance kernel optimized for Huawei's Ascend NPUs that addresses the computational overhead of Multi-head Latent Attention through FlashAttention-based algorithms and Preload Pipeline strategies, achieving up to 614 TFLOPS and 86.8% FLOPS utilization.", "motivation": "Multi-head Latent Attention reduces KVCache memory usage but introduces significant computational overhead and intermediate variable expansion, creating challenges for efficient hardware implementation during decode phase.", "method": "Two core innovations: (1) FlashAttention-based algorithm replacing floating-point multiplications with integer additions for output block rescaling using binary correspondence between FP32 and INT32; (2) Preload Pipeline strategy with hierarchical tiling that maximizes FLOPS utilization through Cube-bound performance and overlapping data movement with computation.", "result": "On Ascend 910 NPUs, AMLA achieves up to 614 TFLOPS, reaching 86.8% of theoretical maximum FLOPS, outperforming state-of-the-art FlashMLA implementation (66.7% FLOPS utilization on NVIDIA H800 SXM5).", "conclusion": "AMLA kernel has been successfully integrated into Huawei's CANN and will be released soon, demonstrating superior performance for MLA optimization on specialized hardware."}}
{"id": "2509.25611", "categories": ["cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25611", "abs": "https://arxiv.org/abs/2509.25611", "authors": ["Takashi Furuya", "Maarten V. de Hoop", "Matti Lassas"], "title": "Transformers through the lens of support-preserving maps between measures", "comment": null, "summary": "Transformers are deep architectures that define ``in-context maps'' which\nenable predicting new tokens based on a given set of tokens (such as a prompt\nin NLP applications or a set of patches for a vision transformer). In previous\nwork, we studied the ability of these architectures to handle an arbitrarily\nlarge number of context tokens. To mathematically, uniformly analyze their\nexpressivity, we considered the case that the mappings are conditioned on a\ncontext represented by a probability distribution which becomes discrete for a\nfinite number of tokens. Modeling neural networks as maps on probability\nmeasures has multiple applications, such as studying Wasserstein regularity,\nproving generalization bounds and doing a mean-field limit analysis of the\ndynamics of interacting particles as they go through the network. In this work,\nwe study the question what kind of maps between measures are transformers. We\nfully characterize the properties of maps between measures that enable these to\nbe represented in terms of in-context maps via a push forward. On the one hand,\nthese include transformers; on the other hand, transformers universally\napproximate representations with any continuous in-context map. These\nproperties are preserving the cardinality of support and that the regular part\nof their Fr\\'{e}chet derivative is uniformly continuous. Moreover, we show that\nthe solution map of the Vlasov equation, which is of nonlocal transport type,\nfor interacting particle systems in the mean-field regime for the Cauchy\nproblem satisfies the conditions on the one hand and, hence, can be\napproximated by a transformer; on the other hand, we prove that the\nmeasure-theoretic self-attention has the properties that ensure that the\ninfinite depth, mean-field measure-theoretic transformer can be identified with\na Vlasov flow.", "AI": {"tldr": "Transformers can be mathematically modeled as maps between probability measures. The paper characterizes which measure maps can be represented by transformers and shows transformers can approximate continuous in-context maps. It also connects transformers to the Vlasov equation from mean-field particle systems.", "motivation": "To mathematically analyze transformers' expressivity in handling arbitrarily large context tokens by modeling them as maps between probability measures, enabling applications in Wasserstein regularity, generalization bounds, and mean-field analysis.", "method": "Model neural networks as maps on probability measures, characterize properties of maps between measures that enable transformer representation via push forward, and analyze measure-theoretic self-attention properties.", "result": "Transformers universally approximate representations with any continuous in-context map. The solution map of the Vlasov equation satisfies conditions for transformer approximation, and measure-theoretic self-attention ensures infinite depth transformers correspond to Vlasov flows.", "conclusion": "Transformers can be characterized as specific maps between measures with cardinality-preserving properties, and they have deep connections to mean-field particle systems through the Vlasov equation, enabling both approximation capabilities and theoretical identifications."}}
{"id": "2509.26562", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26562", "abs": "https://arxiv.org/abs/2509.26562", "authors": ["Firas Ben Hmida", "Abderrahmen Amich", "Ata Kaboudi", "Birhanu Eshete"], "title": "DeepProv: Behavioral Characterization and Repair of Neural Networks via Inference Provenance Graph Analysis", "comment": "18 pages, 9 figures, 6 tables, To appear in the 41st Annual Computer\n  Security Applications Conference (ACSAC), 2025", "summary": "Deep neural networks (DNNs) are increasingly being deployed in high-stakes\napplications, from self-driving cars to biometric authentication. However,\ntheir unpredictable and unreliable behaviors in real-world settings require new\napproaches to characterize and ensure their reliability.\n  This paper introduces DeepProv, a novel and customizable system designed to\ncapture and characterize the runtime behavior of DNNs during inference by using\ntheir underlying graph structure. Inspired by system audit provenance graphs,\nDeepProv models the computational information flow of a DNN's inference process\nthrough Inference Provenance Graphs (IPGs). These graphs provide a detailed\nstructural representation of the behavior of DNN, allowing both empirical and\nstructural analysis. DeepProv uses these insights to systematically repair DNNs\nfor specific objectives, such as improving robustness, privacy, or fairness.\n  We instantiate DeepProv with adversarial robustness as the goal of model\nrepair and conduct extensive case studies to evaluate its effectiveness. Our\nresults demonstrate its effectiveness and scalability across diverse\nclassification tasks, attack scenarios, and model complexities. DeepProv\nautomatically identifies repair actions at the node and edge-level within IPGs,\nsignificantly enhancing the robustness of the model. In particular, applying\nDeepProv repair strategies to just a single layer of a DNN yields an average\n55% improvement in adversarial accuracy. Moreover, DeepProv complements\nexisting defenses, achieving substantial gains in adversarial robustness.\nBeyond robustness, we demonstrate the broader potential of DeepProv as an\nadaptable system to characterize DNN behavior in other critical areas, such as\nprivacy auditing and fairness analysis.", "AI": {"tldr": "DeepProv is a system that captures DNN runtime behavior using inference provenance graphs and systematically repairs models for objectives like robustness, privacy, and fairness.", "motivation": "DNNs are used in high-stakes applications but exhibit unpredictable behaviors, requiring new approaches to ensure reliability.", "method": "Models DNN inference through Inference Provenance Graphs (IPGs) that capture computational information flow, enabling structural analysis and systematic repair at node/edge level.", "result": "Repairing just one DNN layer yields 55% average improvement in adversarial accuracy, complements existing defenses, and demonstrates scalability across diverse tasks and models.", "conclusion": "DeepProv effectively characterizes DNN behavior and enables systematic model repair, showing broader potential for privacy auditing and fairness analysis beyond robustness."}}
{"id": "2509.25426", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25426", "abs": "https://arxiv.org/abs/2509.25426", "authors": ["Nigel Fernandez", "Branislav Kveton", "Ryan A. Rossi", "Andrew S. Lan", "Zichao Wang"], "title": "RADAR: Reasoning-Ability and Difficulty-Aware Routing for Reasoning LLMs", "comment": null, "summary": "Reasoning language models have demonstrated remarkable performance on many\nchallenging tasks in math, science, and coding. Choosing the right reasoning\nmodel for practical deployment involves a performance and cost tradeoff at two\nkey levels: model size and reasoning budget, where larger models and higher\nreasoning budget lead to better performance but with increased cost and\nlatency. In this work, we tackle this tradeoff from the angle of model\nconfiguration routing for different queries, and present RADAR\n(Reasoning-Ability and Difficulty-Aware Routing), a lightweight, interpretable,\nand scalable routing framework. Inspired by psychometrics, RADAR learns an item\nresponse model from model responses with different budgets to different\nqueries, with interpretable parameters including query difficulties and\nmodel-budget abilities. RADAR then routes queries with higher difficulty to\nmodel-budget pairs with higher ability, and vice versa. We conduct extensive\nexperiments on 8 widely used challenging reasoning benchmarks, demonstrating\nthe superior performance of RADAR compared to state-of-the-art model routing\nmethods. RADAR also exhibits query generalization capabilities, showing strong\nperformance on out-of-distribution queries in all benchmarks. RADAR is also\nscalable and can efficiently integrate additional models by dynamically\nselecting a small set of evaluation queries to estimate their abilities.", "AI": {"tldr": "RADAR is a lightweight routing framework that optimizes the tradeoff between reasoning model performance and cost by routing queries to appropriate model-budget pairs based on query difficulty and model ability.", "motivation": "Choosing reasoning models involves balancing performance vs cost tradeoffs at model size and reasoning budget levels. Larger models and higher budgets improve performance but increase costs and latency.", "method": "RADAR learns an item response model from model responses to estimate query difficulties and model-budget abilities, then routes harder queries to more capable model-budget pairs.", "result": "Extensive experiments on 8 reasoning benchmarks show RADAR outperforms state-of-the-art routing methods and generalizes well to out-of-distribution queries.", "conclusion": "RADAR provides an interpretable, scalable routing framework that efficiently manages the performance-cost tradeoff in reasoning model deployment and can dynamically integrate new models."}}
{"id": "2509.25225", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25225", "abs": "https://arxiv.org/abs/2509.25225", "authors": ["Long Xu", "Yongcai Chen", "Fengshuo Liu", "Yuzhong Peng"], "title": "MSCoD: An Enhanced Bayesian Updating Framework with Multi-Scale Information Bottleneck and Cooperative Attention for Structure-Based Drug Design", "comment": "11 pages, 5 figures", "summary": "Structure-Based Drug Design (SBDD) is a powerful strategy in computational\ndrug discovery, utilizing three-dimensional protein structures to guide the\ndesign of molecules with improved binding affinity. However, capturing complex\nprotein-ligand interactions across multiple scales remains challenging, as\ncurrent methods often overlook the hierarchical organization and intrinsic\nasymmetry of these interactions. To address these limitations, we propose\nMSCoD, a novel Bayesian updating-based generative framework for structure-based\ndrug design. In our MSCoD, Multi-Scale Information Bottleneck (MSIB) was\ndeveloped, which enables semantic compression at multiple abstraction levels\nfor efficient hierarchical feature extraction. Furthermore, a multi-head\ncooperative attention (MHCA) mechanism was developed, which employs asymmetric\nprotein-to-ligand attention to capture diverse interaction types while\naddressing the dimensionality disparity between proteins and ligands. Empirical\nstudies showed that MSCoD outperforms state-of-the-art methods on the benchmark\ndataset. Case studies on challenging targets such as KRAS G12D further\ndemonstrate its applicability in real-world scenarios. The code and data\nunderlying this article are freely available at\nhttps://github.com/xulong0826/MSCoD.", "AI": {"tldr": "MSCoD is a Bayesian updating-based generative framework for structure-based drug design that uses multi-scale information bottleneck and asymmetric attention mechanisms to capture hierarchical protein-ligand interactions.", "motivation": "Current SBDD methods struggle to capture complex protein-ligand interactions across multiple scales and often overlook hierarchical organization and intrinsic asymmetry of these interactions.", "method": "Developed Multi-Scale Information Bottleneck (MSIB) for semantic compression at multiple abstraction levels, and multi-head cooperative attention (MHCA) with asymmetric protein-to-ligand attention to handle dimensionality disparity.", "result": "MSCoD outperforms state-of-the-art methods on benchmark datasets and shows strong applicability in real-world scenarios like KRAS G12D targets.", "conclusion": "MSCoD provides an effective framework for hierarchical feature extraction and asymmetric interaction modeling in structure-based drug design, with demonstrated superior performance."}}
{"id": "2509.25649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25649", "abs": "https://arxiv.org/abs/2509.25649", "authors": ["Samar Haider", "Amir Tohidi", "Jenny S. Wang", "Timothy D\u00f6rr", "David M. Rothschild", "Chris Callison-Burch", "Duncan J. Watts"], "title": "The Media Bias Detector: A Framework for Annotating and Analyzing the News at Scale", "comment": null, "summary": "Mainstream news organizations shape public perception not only directly\nthrough the articles they publish but also through the choices they make about\nwhich topics to cover (or ignore) and how to frame the issues they do decide to\ncover. However, measuring these subtle forms of media bias at scale remains a\nchallenge. Here, we introduce a large, ongoing (from January 1, 2024 to\npresent), near real-time dataset and computational framework developed to\nenable systematic study of selection and framing bias in news coverage. Our\npipeline integrates large language models (LLMs) with scalable, near-real-time\nnews scraping to extract structured annotations -- including political lean,\ntone, topics, article type, and major events -- across hundreds of articles per\nday. We quantify these dimensions of coverage at multiple levels -- the\nsentence level, the article level, and the publisher level -- expanding the\nways in which researchers can analyze media bias in the modern news landscape.\nIn addition to a curated dataset, we also release an interactive web platform\nfor convenient exploration of these data. Together, these contributions\nestablish a reusable methodology for studying media bias at scale, providing\nempirical resources for future research. Leveraging the breadth of the corpus\nover time and across publishers, we also present some examples (focused on the\n150,000+ articles examined in 2024) that illustrate how this novel data set can\nreveal insightful patterns in news coverage and bias, supporting academic\nresearch and real-world efforts to improve media accountability.", "AI": {"tldr": "A large-scale computational framework using LLMs to systematically measure selection and framing bias in news coverage through structured annotations of political lean, tone, topics, and events across hundreds of daily articles.", "motivation": "To address the challenge of measuring subtle forms of media bias at scale, particularly selection bias (which topics to cover) and framing bias (how issues are framed), which shape public perception but are difficult to quantify systematically.", "method": "Integrated pipeline combining large language models (LLMs) with scalable, near-real-time news scraping to extract structured annotations including political lean, tone, topics, article type, and major events across hundreds of articles per day, analyzed at sentence, article, and publisher levels.", "result": "Created a large, ongoing dataset (from January 1, 2024) with 150,000+ articles examined in 2024, revealing insightful patterns in news coverage and bias, plus an interactive web platform for data exploration.", "conclusion": "Established a reusable methodology for studying media bias at scale, providing empirical resources for future research and supporting academic research and real-world efforts to improve media accountability."}}
{"id": "2509.26598", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26598", "abs": "https://arxiv.org/abs/2509.26598", "authors": ["Anshul Nasery", "Edoardo Contente", "Alkin Kaz", "Pramod Viswanath", "Sewoong Oh"], "title": "Are Robust LLM Fingerprints Adversarially Robust?", "comment": null, "summary": "Model fingerprinting has emerged as a promising paradigm for claiming model\nownership. However, robustness evaluations of these schemes have mostly focused\non benign perturbations such as incremental fine-tuning, model merging, and\nprompting. Lack of systematic investigations into {\\em adversarial robustness}\nagainst a malicious model host leaves current systems vulnerable. To bridge\nthis gap, we first define a concrete, practical threat model against model\nfingerprinting. We then take a critical look at existing model fingerprinting\nschemes to identify their fundamental vulnerabilities. Based on these, we\ndevelop adaptive adversarial attacks tailored for each vulnerability, and\ndemonstrate that these can bypass model authentication completely for ten\nrecently proposed fingerprinting schemes while maintaining high utility of the\nmodel for the end users. Our work encourages fingerprint designers to adopt\nadversarial robustness by design. We end with recommendations for future\nfingerprinting methods.", "AI": {"tldr": "This paper analyzes the adversarial robustness of model fingerprinting schemes, identifies vulnerabilities in existing methods, and develops attacks that can bypass authentication while maintaining model utility.", "motivation": "Current model fingerprinting schemes lack systematic evaluation of adversarial robustness against malicious model hosts, leaving them vulnerable to attacks.", "method": "The authors define a concrete threat model, analyze vulnerabilities in existing fingerprinting schemes, and develop adaptive adversarial attacks tailored to each vulnerability.", "result": "The attacks successfully bypass model authentication for ten recent fingerprinting schemes while maintaining high model utility for end users.", "conclusion": "Fingerprint designers should adopt adversarial robustness by design, and the paper provides recommendations for future fingerprinting methods."}}
{"id": "2509.25434", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25434", "abs": "https://arxiv.org/abs/2509.25434", "authors": ["Ana Paula Gomes Ferreira", "Aleksandar An\u017eel", "Izabel Oliva Marcilio de Souza", "Helen Hughes", "Alex J Elliot", "Jude Dzevela Kong", "Madlen Schranz", "Alexander Ullrich", "Georges Hattab"], "title": "The Open Syndrome Definition", "comment": null, "summary": "Case definitions are essential for effectively communicating public health\nthreats. However, the absence of a standardized, machine-readable format poses\nsignificant challenges to interoperability, epidemiological research, the\nexchange of qualitative data, and the effective application of computational\nanalysis methods, including artificial intelligence (AI). This complicates\ncomparisons and collaborations across organizations and regions, limits data\nintegration, and hinders technological innovation in public health. To address\nthese issues, we propose the first open, machine-readable format for\nrepresenting case and syndrome definitions. Additionally, we introduce the\nfirst comprehensive dataset of standardized case definitions and tools to\nconvert existing human-readable definitions into machine-readable formats. We\nalso provide an accessible online platform for browsing, analyzing, and\ncontributing new definitions, available at https://opensyndrome.org. The Open\nSyndrome Definition format enables consistent, scalable use of case definitions\nacross systems, unlocking AI's potential to strengthen public health\npreparedness and response. The source code for the format can be found at\nhttps://github.com/OpenSyndrome/schema under the MIT license.", "AI": {"tldr": "Proposed first open, machine-readable format for case/syndrome definitions to address interoperability challenges in public health data exchange and AI applications.", "motivation": "Lack of standardized machine-readable case definitions hinders interoperability, epidemiological research, data exchange, and AI applications in public health.", "method": "Developed Open Syndrome Definition format, created comprehensive dataset of standardized definitions, built tools for converting human-readable to machine-readable formats, and established online platform.", "result": "Created accessible platform at https://opensyndrome.org for browsing/analyzing definitions, with open-source schema available at https://github.com/OpenSyndrome/schema under MIT license.", "conclusion": "The format enables consistent, scalable use of case definitions across systems, unlocking AI's potential for public health preparedness and response."}}
{"id": "2509.25226", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.25226", "abs": "https://arxiv.org/abs/2509.25226", "authors": ["Baoyi Xie", "Shuiling Shi", "Wenqi Liu"], "title": "Integrated Forecasting of Marine Renewable Power: An Adaptively Bayesian-Optimized MVMD-LSTM Framework for Wind-Solar-Wave Energy", "comment": null, "summary": "Integrated wind-solar-wave marine energy systems hold broad promise for\nsupplying clean electricity in offshore and coastal regions. By leveraging the\nspatiotemporal complementarity of multiple resources, such systems can\neffectively mitigate the intermittency and volatility of single-source outputs,\nthereby substantially improving overall power-generation efficiency and\nresource utilization. Accurate ultra-short-term forecasting is crucial for\nensuring secure operation and optimizing proactive dispatch. However, most\nexisting forecasting methods construct separate models for each energy source,\ninsufficiently account for the complex couplings among multiple energies,\nstruggle to capture the system's nonlinear and nonstationary dynamics, and\ntypically depend on extensive manual parameter tuning-limitations that\nconstrain both predictive performance and practicality. We address this issue\nusing a Bayesian-optimized Multivariate Variational Mode Decomposition-Long\nShort-Term Memory (MVMD-LSTM) framework. The framework first applies MVMD to\njointly decompose wind, solar and wave power series so as to preserve\ncross-source couplings; it uses Bayesian optimization to automatically search\nthe number of modes and the penalty parameter in the MVMD process to obtain\nintrinsic mode functions (IMFs); finally, an LSTM models the resulting IMFs to\nachieve ultra-short-term power forecasting for the integrated system.\nExperiments based on field measurements from an offshore integrated energy\nplatform in China show that the proposed framework significantly outperforms\nbenchmark models in terms of MAPE, RMSE and MAE. The results demonstrate\nsuperior predictive accuracy, robustness, and degree of automation.", "AI": {"tldr": "A Bayesian-optimized MVMD-LSTM framework for ultra-short-term forecasting of integrated wind-solar-wave marine energy systems, achieving superior accuracy and automation compared to benchmark models.", "motivation": "Existing forecasting methods for marine energy systems have limitations: they use separate models for each energy source, fail to capture complex cross-source couplings, struggle with nonlinear dynamics, and require extensive manual parameter tuning, which constrains predictive performance and practicality.", "method": "Proposes a Bayesian-optimized MVMD-LSTM framework: 1) Uses MVMD to jointly decompose wind, solar and wave power series while preserving cross-source couplings; 2) Employs Bayesian optimization to automatically determine the number of modes and penalty parameter in MVMD; 3) Uses LSTM to model the resulting IMFs for ultra-short-term power forecasting.", "result": "Experiments using field measurements from an offshore integrated energy platform in China show the proposed framework significantly outperforms benchmark models in terms of MAPE, RMSE and MAE metrics.", "conclusion": "The framework demonstrates superior predictive accuracy, robustness, and degree of automation for integrated wind-solar-wave marine energy system forecasting."}}
{"id": "2509.25664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25664", "abs": "https://arxiv.org/abs/2509.25664", "authors": ["David Beauchemin", "Pier-Luc Veilleux", "Richard Khoury", "Johanna-Pascale Roy"], "title": "QFrBLiMP: a Quebec-French Benchmark of Linguistic Minimal Pairs", "comment": "Submitted to LREC 2026", "summary": "In this paper, we introduce the Quebec-French Benchmark of Linguistic Minimal\nPairs (QFrBLiMP), a corpus designed to evaluate the linguistic knowledge of\nLLMs on prominent grammatical phenomena in Quebec-French. QFrBLiMP consists of\n1,761 minimal pairs annotated with 20 linguistic phenomena. Specifically, these\nminimal pairs have been created by manually modifying sentences extracted from\nan official online resource maintained by a Qu\\'ebec government institution.\nEach pair is annotated by twelve Quebec-French native speakers, who select the\nsentence they feel is grammatical amongst the two. These annotations are used\nto compare the competency of LLMs with that of humans. We evaluate different\nLLMs on QFrBLiMP and MultiBLiMP-Fr by observing the rate of higher\nprobabilities assigned to the sentences of each minimal pair for each category.\nWe find that while grammatical competence scales with model size, a clear\nhierarchy of difficulty emerges. All benchmarked models consistently fail on\nphenomena requiring deep semantic understanding, revealing a critical\nlimitation and a significant gap compared to human performance on these\nspecific tasks.", "AI": {"tldr": "QFrBLiMP is a Quebec-French linguistic benchmark with 1,761 minimal pairs across 20 grammatical phenomena, used to evaluate LLMs' grammatical competence compared to human native speakers.", "motivation": "To evaluate LLMs' linguistic knowledge on Quebec-French grammatical phenomena and compare their performance with human native speakers.", "method": "Created 1,761 minimal pairs from official Quebec government resources, annotated by 12 native speakers, then evaluated LLMs by comparing probability assignments to grammatical vs ungrammatical sentences.", "result": "Grammatical competence scales with model size, but all models fail on phenomena requiring deep semantic understanding, showing significant gaps compared to human performance.", "conclusion": "Current LLMs have critical limitations in handling Quebec-French grammatical phenomena that require semantic understanding, revealing substantial room for improvement despite scaling effects."}}
{"id": "2509.25435", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25435", "abs": "https://arxiv.org/abs/2509.25435", "authors": ["Rishi Ashish Shah", "Shivaay Dhondiyal", "Kartik Sharma", "Sukriti Talwar", "Saksham Jain", "Sparsh Jain"], "title": "GESA: Graph-Enhanced Semantic Allocation for Generalized, Fair, and Explainable Candidate-Role Matching", "comment": null, "summary": "Accurate, fair, and explainable allocation of candidates to roles represents\na fundamental challenge across multiple domains including corporate hiring,\nacademic admissions, fellowship awards, and volunteer placement systems.\nCurrent state-of-the-art approaches suffer from semantic inflexibility,\npersistent demographic bias, opacity in decision-making processes, and poor\nscalability under dynamic policy constraints. We present GESA (Graph-Enhanced\nSemantic Allocation), a comprehensive framework that addresses these\nlimitations through the integration of domain-adaptive transformer embeddings,\nheterogeneous self-supervised graph neural networks, adversarial debiasing\nmechanisms, multi-objective genetic optimization, and explainable AI\ncomponents. Our experimental evaluation on large-scale international benchmarks\ncomprising 20,000 candidate profiles and 3,000 role specifications demonstrates\nsuperior performance with 94.5% top-3 allocation accuracy, 37% improvement in\ndiversity representation, 0.98 fairness score across demographic cate- gories,\nand sub-second end-to-end latency. Additionally, GESA incorporates hybrid\nrecommendation capabilities and glass-box explainability, making it suitable\nfor deployment across diverse international contexts in industry, academia, and\nnon-profit sectors.", "AI": {"tldr": "GESA is a comprehensive framework for candidate-role allocation that integrates transformer embeddings, graph neural networks, adversarial debiasing, genetic optimization, and explainable AI to achieve high accuracy, fairness, and transparency.", "motivation": "Current allocation systems suffer from semantic inflexibility, demographic bias, opaque decision-making, and poor scalability under dynamic policy constraints across domains like hiring, admissions, and placements.", "method": "Integration of domain-adaptive transformer embeddings, heterogeneous self-supervised graph neural networks, adversarial debiasing mechanisms, multi-objective genetic optimization, and explainable AI components.", "result": "94.5% top-3 allocation accuracy, 37% improvement in diversity representation, 0.98 fairness score across demographic categories, and sub-second end-to-end latency on benchmarks with 20,000 candidate profiles and 3,000 role specifications.", "conclusion": "GESA provides superior performance with hybrid recommendation capabilities and glass-box explainability, making it suitable for deployment across diverse international contexts in industry, academia, and non-profit sectors."}}
{"id": "2509.25228", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25228", "abs": "https://arxiv.org/abs/2509.25228", "authors": ["Ahmad Ayaz Amin"], "title": "Simple, Fast and Efficient Injective Manifold Density Estimation with Random Projections", "comment": null, "summary": "We introduce Random Projection Flows (RPFs), a principled framework for\ninjective normalizing flows that leverages tools from random matrix theory and\nthe geometry of random projections. RPFs employ random semi-orthogonal\nmatrices, drawn from Haar-distributed orthogonal ensembles via QR decomposition\nof Gaussian matrices, to project data into lower-dimensional latent spaces for\nthe base distribution. Unlike PCA-based flows or learned injective maps, RPFs\nare plug-and-play, efficient, and yield closed-form expressions for the\nRiemannian volume correction term. We demonstrate that RPFs are both\ntheoretically grounded and practically effective, providing a strong baseline\nfor generative modeling and a bridge between random projection theory and\nnormalizing flows.", "AI": {"tldr": "Random Projection Flows (RPFs) are injective normalizing flows using random semi-orthogonal matrices from Haar-distributed orthogonal ensembles for efficient data projection to lower-dimensional latent spaces with closed-form Riemannian volume correction.", "motivation": "To create a principled framework for injective normalizing flows that bridges random projection theory with normalizing flows, offering plug-and-play efficiency and theoretical grounding.", "method": "Use random semi-orthogonal matrices from Haar-distributed orthogonal ensembles (via QR decomposition of Gaussian matrices) to project data into lower-dimensional latent spaces for base distribution, with closed-form Riemannian volume correction.", "result": "RPFs are shown to be both theoretically grounded and practically effective, providing a strong baseline for generative modeling.", "conclusion": "RPFs successfully bridge random projection theory and normalizing flows, offering an efficient, plug-and-play framework with theoretical guarantees for injective normalizing flows."}}
{"id": "2509.25671", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.25671", "abs": "https://arxiv.org/abs/2509.25671", "authors": ["Arda Uzunoglu", "Tianjian Li", "Daniel Khashabi"], "title": "The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks", "comment": null, "summary": "Benchmarks shape scientific conclusions about model capabilities and steer\nmodel development. This creates a feedback loop: stronger benchmarks drive\nbetter models, and better models demand more discriminative benchmarks.\nEnsuring benchmark reliability is therefore essential for trustworthy\nevaluation and meaningful progress. In this work, we study benchmark\nreliability from a distributional perspective and introduce benchmark harmony,\nwhich measures how uniformly a model's performance is distributed across the\nsubdomains of a benchmark. We posit that high harmony is a desirable benchmark\nproperty, indicating that the aggregate metric reflects uniform competence\nacross subdomains. Across 19 multiple-choice benchmarks and five model\nfamilies, we map each benchmark onto a mean-variance plane of harmony computed\nacross models, where high mean and low variance signal more reliable\nevaluation. Our analysis shows that less harmonious benchmarks can give\nmisleading results, since overall accuracy may be disproportionately influenced\nby specific subdomains. For instance, ARC-Easy is overwhelmed by questions on\nBiological Concepts, overshadowing other critical subdomains such as Geography,\nPhysics, Chemistry, and Environmental Science. By recommending that harmony\nshould be reported alongside accuracy, we reframe evaluation from simple\nperformance averages to a more robust, distributionally reliable measurement of\nperformance.", "AI": {"tldr": "The paper introduces benchmark harmony as a measure of how uniformly a model's performance is distributed across subdomains of a benchmark, arguing that high harmony indicates more reliable evaluation.", "motivation": "Benchmarks create feedback loops that shape model development, so ensuring benchmark reliability is essential for trustworthy evaluation and meaningful progress in AI.", "method": "Study benchmark reliability from a distributional perspective by introducing benchmark harmony, which measures performance uniformity across subdomains. Analyze 19 multiple-choice benchmarks and five model families, mapping benchmarks onto a mean-variance plane of harmony.", "result": "Analysis shows less harmonious benchmarks can give misleading results, with overall accuracy disproportionately influenced by specific subdomains. For example, ARC-Easy is overwhelmed by Biological Concepts questions, overshadowing other critical subdomains.", "conclusion": "Harmony should be reported alongside accuracy to reframe evaluation from simple performance averages to more robust, distributionally reliable measurement of performance."}}
{"id": "2509.25241", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.25241", "abs": "https://arxiv.org/abs/2509.25241", "authors": ["Yuan Huang"], "title": "Fine-tuning of Large Language Models for Domain-Specific Cybersecurity Knowledge", "comment": null, "summary": "Recent advancements in training paradigms for Large Language Models (LLMs)\nhave unlocked their remarkable capabilities in natural language processing and\ncross-domain generalization. While LLMs excel in tasks like programming and\nmathematical problem-solving, their zero-shot performance in specialized\ndomains requiring expert knowledge, such as cybersecurity, is often suboptimal.\nThis limitation arises because foundational LLMs are designed for\ngeneral-purpose applications, constraining their ability to encapsulate\ndomain-specific expertise within their parameter space. To address this, we\nexplore fine-tuning strategies to embed cybersecurity knowledge into LLMs,\nenhancing their performance in cybersecurity question-answering (Q\\&A) tasks\nwhile prioritizing computational efficiency. Specifically, we investigate\nSupervised Fine-Tuning (SFT), Low-Rank Adaptation (LoRA), and Quantized\nLow-Rank Adaptation (QLoRA) using a cybersecurity Q\\&A dataset. Our results\ndemonstrate that these fine-tuning approaches significantly outperform the\nfoundational model in cybersecurity Q\\&A tasks. Moreover, LoRA and QLoRA\nachieve comparable performance to SFT with substantially lower computational\ncosts, offering an efficient pathway for adapting LLMs to specialized domains.\nOur work highlights the potential of low-rank fine-tuning strategies to bridge\nthe gap between general-purpose LLMs and domain-specific applications.", "AI": {"tldr": "Fine-tuning LLMs with SFT, LoRA, and QLoRA significantly improves cybersecurity Q&A performance while maintaining computational efficiency.", "motivation": "LLMs have suboptimal zero-shot performance in specialized domains like cybersecurity due to their general-purpose design, limiting domain-specific expertise.", "method": "Investigated Supervised Fine-Tuning (SFT), Low-Rank Adaptation (LoRA), and Quantized Low-Rank Adaptation (QLoRA) using cybersecurity Q&A dataset.", "result": "All fine-tuning approaches significantly outperformed foundational model; LoRA and QLoRA achieved comparable performance to SFT with much lower computational costs.", "conclusion": "Low-rank fine-tuning strategies effectively bridge the gap between general-purpose LLMs and domain-specific applications in cybersecurity."}}
{"id": "2509.25454", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25454", "abs": "https://arxiv.org/abs/2509.25454", "authors": ["Fang Wu", "Weihao Xuan", "Heli Qi", "Ximing Lu", "Aaron Tu", "Li Erran Li", "Yejin ChoiRetry"], "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search", "comment": null, "summary": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation.", "AI": {"tldr": "DeepSearch integrates Monte Carlo Tree Search into RLVR training to overcome performance plateaus caused by sparse exploration, achieving state-of-the-art results with significantly less computation.", "motivation": "Current RLVR methods face training plateaus due to sparse exploration patterns that miss critical reasoning paths and fail to systematically cover the solution space, leading to diminishing returns despite increased computational investment.", "method": "DeepSearch embeds Monte Carlo Tree Search directly into RLVR training loop with: (1) global frontier selection strategy for prioritizing promising nodes, (2) entropy-based guidance for confident path selection, and (3) adaptive replay buffer training with solution caching.", "result": "Achieves 62.95% average accuracy on mathematical reasoning benchmarks, establishing new state-of-the-art for 1.5B reasoning models while using 5.7x fewer GPU hours than extended training approaches.", "conclusion": "Strategic exploration through systematic search is more effective than brute-force scaling, demonstrating the promise of algorithmic innovation for advancing RLVR methodologies and establishing a new direction for scaling reasoning capabilities."}}
{"id": "2509.25230", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25230", "abs": "https://arxiv.org/abs/2509.25230", "authors": ["Aaron Zweig", "Mingxuan Zhang", "Elham Azizi", "David Knowles"], "title": "Energy Guided Geometric Flow Matching", "comment": null, "summary": "A useful inductive bias for temporal data is that trajectories should stay\nclose to the data manifold. Traditional flow matching relies on straight\nconditional paths, and flow matching methods which learn geodesics rely on RBF\nkernels or nearest neighbor graphs that suffer from the curse of\ndimensionality. We propose to use score matching and annealed energy\ndistillation to learn a metric tensor that faithfully captures the underlying\ndata geometry and informs more accurate flows. We demonstrate the efficacy of\nthis strategy on synthetic manifolds with analytic geodesics, and interpolation\nof cell", "AI": {"tldr": "The paper proposes using score matching and annealed energy distillation to learn a metric tensor that captures data geometry for more accurate flow matching on temporal data.", "motivation": "Traditional flow matching uses straight paths that may not follow the data manifold, and existing geodesic methods suffer from dimensionality issues with RBF kernels or nearest neighbor graphs.", "method": "Use score matching and annealed energy distillation to learn a metric tensor that faithfully represents the underlying data geometry.", "result": "The method is demonstrated on synthetic manifolds with analytic geodesics and cell interpolation tasks.", "conclusion": "The proposed approach effectively captures data geometry and enables more accurate flow matching for temporal data."}}
{"id": "2509.25673", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25673", "abs": "https://arxiv.org/abs/2509.25673", "authors": ["Dianqing Liu", "Yi Liu", "Guoqing Jin", "Zhendong Mao"], "title": "Mitigating Biases in Language Models via Bias Unlearning", "comment": "EMNLP 2025 MainConference", "summary": "Many studies have shown various biases targeting different demographic groups\nin language models, amplifying discrimination and harming fairness. Recent\nparameter modification debiasing approaches significantly degrade core\ncapabilities such as text coherence and task accuracy. And Prompt-based\ndebiasing methods, only effective for predefined trigger words, fail to address\ndeeply embedded stereotypical associations in model parameters. In this paper,\nwe propose BiasUnlearn, a novel model debiasing framework which achieves\ntargeted debiasing via dual-pathway unlearning mechanisms coordinating\nstereotype forgetting with anti-stereotype retention, while preventing bias\npolarity reversal through adversarial forget set and dynamic dataset swapping.\nWe conducted extensive experiments with multiple language models across various\nevaluation benchmarks. The results show that BiasUnlearn outperforms existing\nmethods in mitigating bias in language models while retaining language modeling\ncapabilities. Further experiments reveal that debiasing weights are\ntransferable across model variants, confirming that bias representations become\nentrenched during pre-training and persist through fine-tuning phases.", "AI": {"tldr": "BiasUnlearn is a novel debiasing framework that uses dual-pathway unlearning to mitigate bias in language models while preserving core capabilities, outperforming existing methods.", "motivation": "Existing debiasing approaches either degrade model capabilities or only address surface-level biases, failing to tackle deeply embedded stereotypical associations in model parameters.", "method": "BiasUnlearn employs dual-pathway unlearning mechanisms coordinating stereotype forgetting with anti-stereotype retention, using adversarial forget set and dynamic dataset swapping to prevent bias polarity reversal.", "result": "Extensive experiments show BiasUnlearn outperforms existing methods in mitigating bias while retaining language modeling capabilities, with debiasing weights being transferable across model variants.", "conclusion": "Bias representations become entrenched during pre-training and persist through fine-tuning phases, and BiasUnlearn effectively addresses this through targeted unlearning mechanisms."}}
{"id": "2509.26032", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.26032", "abs": "https://arxiv.org/abs/2509.26032", "authors": ["Xiaobao Wang", "Ruoxiao Sun", "Yujun Zhang", "Bingdao Feng", "Dongxiao He", "Luzhi Wang", "Di Jin"], "title": "Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Graph Neural Networks (GNNs) have demonstrated strong performance across\ntasks such as node classification, link prediction, and graph classification,\nbut remain vulnerable to backdoor attacks that implant imperceptible triggers\nduring training to control predictions. While node-level attacks exploit local\nmessage passing, graph-level attacks face the harder challenge of manipulating\nglobal representations while maintaining stealth. We identify two main sources\nof anomaly in existing graph classification backdoor methods: structural\ndeviation from rare subgraph triggers and semantic deviation caused by label\nflipping, both of which make poisoned graphs easily detectable by anomaly\ndetection models. To address this, we propose DPSBA, a clean-label backdoor\nframework that learns in-distribution triggers via adversarial training guided\nby anomaly-aware discriminators. DPSBA effectively suppresses both structural\nand semantic anomalies, achieving high attack success while significantly\nimproving stealth. Extensive experiments on real-world datasets validate that\nDPSBA achieves a superior balance between effectiveness and detectability\ncompared to state-of-the-art baselines.", "AI": {"tldr": "DPSBA is a clean-label backdoor attack framework for graph classification that learns in-distribution triggers via adversarial training to achieve high attack success while maintaining stealth against anomaly detection.", "motivation": "Existing graph-level backdoor attacks suffer from structural deviation (rare subgraph triggers) and semantic deviation (label flipping), making poisoned graphs easily detectable by anomaly detection models.", "method": "Proposes DPSBA framework that learns in-distribution triggers through adversarial training guided by anomaly-aware discriminators to suppress both structural and semantic anomalies.", "result": "Extensive experiments show DPSBA achieves superior balance between attack effectiveness and detectability compared to state-of-the-art baselines, with high attack success while significantly improving stealth.", "conclusion": "DPSBA effectively addresses the stealth limitations of existing graph classification backdoor methods by learning in-distribution triggers that avoid detection while maintaining high attack performance."}}
{"id": "2509.25458", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25458", "abs": "https://arxiv.org/abs/2509.25458", "authors": ["Jiacheng Shi", "Hongfei Du", "Y. Alicia Hong", "Ye Gao"], "title": "Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot Speech Emotion Recognition", "comment": null, "summary": "Large audio-language models (LALMs) exhibit strong zero-shot performance\nacross speech tasks but struggle with speech emotion recognition (SER) due to\nweak paralinguistic modeling and limited cross-modal reasoning. We propose\nCompositional Chain-of-Thought Prompting for Emotion Reasoning (CCoT-Emo), a\nframework that introduces structured Emotion Graphs (EGs) to guide LALMs in\nemotion inference without fine-tuning. Each EG encodes seven acoustic features\n(e.g., pitch, speech rate, jitter, shimmer), textual sentiment, keywords, and\ncross-modal associations. Embedded into prompts, EGs provide interpretable and\ncompositional representations that enhance LALM reasoning. Experiments across\nSER benchmarks show that CCoT-Emo outperforms prior SOTA and improves accuracy\nover zero-shot baselines.", "AI": {"tldr": "CCoT-Emo is a framework that uses Emotion Graphs to guide large audio-language models in speech emotion recognition without fine-tuning, improving accuracy over zero-shot baselines.", "motivation": "Large audio-language models struggle with speech emotion recognition due to weak paralinguistic modeling and limited cross-modal reasoning capabilities.", "method": "Proposes Compositional Chain-of-Thought Prompting with Emotion Graphs that encode seven acoustic features, textual sentiment, keywords, and cross-modal associations to guide model reasoning.", "result": "Outperforms prior state-of-the-art methods and improves accuracy over zero-shot baselines across speech emotion recognition benchmarks.", "conclusion": "The CCoT-Emo framework effectively enhances emotion reasoning in LALMs through structured prompting without requiring model fine-tuning."}}
{"id": "2509.25231", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25231", "abs": "https://arxiv.org/abs/2509.25231", "authors": ["Xiaojian Wang", "Chaoli Zhang", "Zhonglong Zheng", "Yunliang Jiang"], "title": "WDformer: A Wavelet-based Differential Transformer Model for Time Series Forecasting", "comment": "Accepted by CIKM 2025", "summary": "Time series forecasting has various applications, such as meteorological\nrainfall prediction, traffic flow analysis, financial forecasting, and\noperational load monitoring for various systems. Due to the sparsity of time\nseries data, relying solely on time-domain or frequency-domain modeling limits\nthe model's ability to fully leverage multi-domain information. Moreover, when\napplied to time series forecasting tasks, traditional attention mechanisms tend\nto over-focus on irrelevant historical information, which may introduce noise\ninto the prediction process, leading to biased results. We proposed WDformer, a\nwavelet-based differential Transformer model. This study employs the wavelet\ntransform to conduct a multi-resolution analysis of time series data. By\nleveraging the advantages of joint representation in the time-frequency domain,\nit accurately extracts the key information components that reflect the\nessential characteristics of the data. Furthermore, we apply attention\nmechanisms on inverted dimensions, allowing the attention mechanism to capture\nrelationships between multiple variables. When performing attention\ncalculations, we introduced the differential attention mechanism, which\ncomputes the attention score by taking the difference between two separate\nsoftmax attention matrices. This approach enables the model to focus more on\nimportant information and reduce noise. WDformer has achieved state-of-the-art\n(SOTA) results on multiple challenging real-world datasets, demonstrating its\naccuracy and effectiveness. Code is available at\nhttps://github.com/xiaowangbc/WDformer.", "AI": {"tldr": "WDformer is a wavelet-based differential Transformer model that uses multi-resolution wavelet analysis and differential attention mechanism to improve time series forecasting by better capturing time-frequency domain information and reducing noise.", "motivation": "Traditional time series forecasting methods have limitations in fully utilizing multi-domain information due to data sparsity, and standard attention mechanisms tend to over-focus on irrelevant historical information, introducing noise and bias into predictions.", "method": "The model employs wavelet transform for multi-resolution analysis of time series data, uses inverted dimension attention to capture relationships between multiple variables, and introduces differential attention mechanism that computes attention scores from differences between two separate softmax attention matrices.", "result": "WDformer achieved state-of-the-art (SOTA) results on multiple challenging real-world datasets, demonstrating superior accuracy and effectiveness in time series forecasting tasks.", "conclusion": "The proposed WDformer model successfully addresses limitations of traditional approaches by leveraging wavelet-based time-frequency analysis and differential attention mechanism, providing an effective solution for accurate time series forecasting across various applications."}}
{"id": "2509.25684", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25684", "abs": "https://arxiv.org/abs/2509.25684", "authors": ["Yuan Zhuang", "Yi Shen", "Yuexin Bian", "Qing Su", "Shihao Ji", "Yuanyuan Shi", "Fei Miao"], "title": "LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts", "comment": null, "summary": "Recent studies have shown that combining parameter-efficient fine-tuning\n(PEFT) with mixture-of-experts (MoE) is an effective strategy for adapting\nlarge language models (LLMs) to the downstream tasks. However, most existing\napproaches rely on conventional TopK routing, which requires careful\nhyperparameter tuning and assigns a fixed number of experts to each token. In\nthis work, we propose LD-MoLE, a Learnable Dynamic routing mechanism for\nMixture of LoRA Experts that enables adaptive, token-dependent, and layer-wise\nexpert allocation. Our method replaces the non-differentiable TopK selection\nwith a differentiable routing function and a closed-form solution. Moreover,\nour design allows the model to adaptively determine the number of experts to\nactivate for each token at different layers. In addition, we introduce an\nanalytical sparsity control objective to regularize the number of activated\nexperts. Extensive experiments on the Qwen3-1.7B and Llama-3.2-3B models show\nthat LD-MoLE achieves the highest average scores compared to state-of-the-art\nbaselines, across a diverse set of benchmarks. Our method not only achieves\nsuperior performance, but also demonstrates the ability to learn\ntoken-dependent and layer-wise expert allocation.", "AI": {"tldr": "LD-MoLE proposes a learnable dynamic routing mechanism for Mixture of LoRA Experts that replaces non-differentiable TopK routing with differentiable routing and adaptive expert allocation per token and layer.", "motivation": "Existing PEFT+MoE approaches rely on conventional TopK routing requiring careful hyperparameter tuning and fixed expert assignment per token, lacking adaptability.", "method": "Uses differentiable routing function with closed-form solution, adaptive expert number determination per token and layer, and analytical sparsity control objective.", "result": "Achieves highest average scores on Qwen3-1.7B and Llama-3.2-3B models across diverse benchmarks compared to state-of-the-art baselines.", "conclusion": "LD-MoLE achieves superior performance while learning token-dependent and layer-wise expert allocation, demonstrating effective adaptive routing."}}
{"id": "2509.26640", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.26640", "abs": "https://arxiv.org/abs/2509.26640", "authors": ["Jo\u00e3o Vitorino", "Eva Maia", "Isabel Pra\u00e7a", "Carlos Soares"], "title": "SPATA: Systematic Pattern Analysis for Detailed and Transparent Data Cards", "comment": "16 pages, 3 tables, 6 figures, SynDAiTE, ECML PKDD 2025", "summary": "Due to the susceptibility of Artificial Intelligence (AI) to data\nperturbations and adversarial examples, it is crucial to perform a thorough\nrobustness evaluation before any Machine Learning (ML) model is deployed.\nHowever, examining a model's decision boundaries and identifying potential\nvulnerabilities typically requires access to the training and testing datasets,\nwhich may pose risks to data privacy and confidentiality. To improve\ntransparency in organizations that handle confidential data or manage critical\ninfrastructure, it is essential to allow external verification and validation\nof AI without the disclosure of private datasets. This paper presents\nSystematic Pattern Analysis (SPATA), a deterministic method that converts any\ntabular dataset to a domain-independent representation of its statistical\npatterns, to provide more detailed and transparent data cards. SPATA computes\nthe projection of each data instance into a discrete space where they can be\nanalyzed and compared, without risking data leakage. These projected datasets\ncan be reliably used for the evaluation of how different features affect ML\nmodel robustness and for the generation of interpretable explanations of their\nbehavior, contributing to more trustworthy AI.", "AI": {"tldr": "SPATA is a deterministic method that converts tabular datasets into domain-independent statistical pattern representations to enable external AI robustness evaluation without disclosing private data.", "motivation": "To address the need for external verification of AI models while protecting data privacy, especially for organizations handling confidential data or critical infrastructure.", "method": "Systematic Pattern Analysis (SPATA) projects data instances into a discrete space representing statistical patterns, creating domain-independent representations that prevent data leakage.", "result": "SPATA enables reliable evaluation of feature effects on ML model robustness and generation of interpretable explanations without exposing private datasets.", "conclusion": "SPATA contributes to more trustworthy AI by providing transparent data cards and enabling external validation while maintaining data confidentiality."}}
{"id": "2509.25475", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25475", "abs": "https://arxiv.org/abs/2509.25475", "authors": ["Yoann Poupart"], "title": "TDHook: A Lightweight Framework for Interpretability", "comment": null, "summary": "Interpretability of Deep Neural Networks (DNNs) is a growing field driven by\nthe study of vision and language models. Yet, some use cases, like image\ncaptioning, or domains like Deep Reinforcement Learning (DRL), require complex\nmodelling, with multiple inputs and outputs or use composable and separated\nnetworks. As a consequence, they rarely fit natively into the API of popular\ninterpretability frameworks. We thus present TDHook, an open-source,\nlightweight, generic interpretability framework based on $\\texttt{tensordict}$\nand applicable to any $\\texttt{torch}$ model. It focuses on handling complex\ncomposed models which can be trained for Computer Vision, Natural Language\nProcessing, Reinforcement Learning or any other domain. This library features\nready-to-use methods for attribution, probing and a flexible get-set API for\ninterventions, and is aiming to bridge the gap between these method classes to\nmake modern interpretability pipelines more accessible. TDHook is designed with\nminimal dependencies, requiring roughly half as much disk space as\n$\\texttt{transformer_lens}$, and, in our controlled benchmark, achieves up to a\n$\\times$2 speed-up over $\\texttt{captum}$ when running integrated gradients for\nmulti-target pipelines on both CPU and GPU. In addition, to value our work, we\nshowcase concrete use cases of our library with composed interpretability\npipelines in Computer Vision (CV) and Natural Language Processing (NLP), as\nwell as with complex models in DRL.", "AI": {"tldr": "TDHook is a lightweight, generic interpretability framework for PyTorch models that handles complex composed models across domains like CV, NLP, and DRL, offering attribution, probing, and intervention methods with better performance than existing tools.", "motivation": "Existing interpretability frameworks don't handle complex models with multiple inputs/outputs or composable networks well, particularly for use cases like image captioning and deep reinforcement learning.", "method": "Developed TDHook - an open-source framework based on tensordict that works with any torch model, featuring ready-to-use methods for attribution, probing, and flexible intervention API.", "result": "TDHook requires half the disk space of transformer_lens and achieves up to 2x speed-up over captum for integrated gradients on multi-target pipelines across CPU and GPU.", "conclusion": "TDHook successfully bridges the gap between different interpretability method classes and makes modern interpretability pipelines more accessible for complex models across various domains."}}
{"id": "2509.25232", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25232", "abs": "https://arxiv.org/abs/2509.25232", "authors": ["Yongchao Huang"], "title": "Sampling via Gaussian Mixture Approximations", "comment": "204 pages", "summary": "We present a family of \\textit{Gaussian Mixture Approximation} (GMA) samplers\nfor sampling unnormalised target densities, encompassing \\textit{weights-only\nGMA} (W-GMA), \\textit{Laplace Mixture Approximation} (LMA),\n\\textit{expectation-maximization GMA} (EM-GMA), and further variants. GMA\nadopts a simple two-stage paradigm: (i) initialise a finite set of Gaussian\ncomponents and draw samples from a proposal mixture; (ii) fit the mixture to\nthe target by optimising either only the component weights or also the means\nand variances, via a sample-based KL divergence objective that requires only\nevaluations of the unnormalised density, followed by stratified resampling. The\nmethod is gradient-free, and computationally efficient: it leverages the ease\nof sampling from Gaussians, efficient optimisation methods (projected gradient\ndescent, mirror descent, and EM), and the robustness of stratified resampling\nto produce samples faithful to the target. We show that this\noptimisation-resampling scheme yields consistent approximations under mild\nconditions, and we validate this methodology with empirical results\ndemonstrating accuracy and speed across diverse densities.", "AI": {"tldr": "Gaussian Mixture Approximation (GMA) samplers provide a gradient-free, computationally efficient method for sampling unnormalized target densities using a two-stage approach of proposal sampling followed by mixture optimization and stratified resampling.", "motivation": "To develop efficient sampling methods for unnormalized target densities that don't require gradient information and can leverage the simplicity of Gaussian sampling.", "method": "Two-stage paradigm: (1) initialize Gaussian components and sample from proposal mixture, (2) fit mixture to target by optimizing weights/means/variances using sample-based KL divergence, then perform stratified resampling.", "result": "Method produces consistent approximations under mild conditions and demonstrates accuracy and speed across diverse densities in empirical validation.", "conclusion": "GMA samplers offer a practical, gradient-free approach for efficient sampling from complex target distributions using Gaussian mixture approximations."}}
{"id": "2509.25725", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25725", "abs": "https://arxiv.org/abs/2509.25725", "authors": ["Jiayi Kuang", "Haojing Huang", "Yinghui Li", "Xinnian Liang", "Zhikun Xu", "Yangning Li", "Xiaoyu Tan", "Chao Qu", "Meishan Zhang", "Ying Shen", "Philip S. Yu"], "title": "Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated outstanding performance in\nmathematical reasoning capabilities. However, we argue that current large-scale\nreasoning models primarily rely on scaling up training datasets with diverse\nmathematical problems and long thinking chains, which raises questions about\nwhether LLMs genuinely acquire mathematical concepts and reasoning principles\nor merely remember the training data. In contrast, humans tend to break down\ncomplex problems into multiple fundamental atomic capabilities. Inspired by\nthis, we propose a new paradigm for evaluating mathematical atomic\ncapabilities. Our work categorizes atomic abilities into two dimensions: (1)\nfield-specific abilities across four major mathematical fields, algebra,\ngeometry, analysis, and topology, and (2) logical abilities at different\nlevels, including conceptual understanding, forward multi-step reasoning with\nformal math language, and counterexample-driven backward reasoning. We propose\ncorresponding training and evaluation datasets for each atomic capability unit,\nand conduct extensive experiments about how different atomic capabilities\ninfluence others, to explore the strategies to elicit the required specific\natomic capability. Evaluation and experimental results on advanced models show\nmany interesting discoveries and inspirations about the different performances\nof models on various atomic capabilities and the interactions between atomic\ncapabilities. Our findings highlight the importance of decoupling mathematical\nintelligence into atomic components, providing new insights into model\ncognition and guiding the development of training strategies toward a more\nefficient, transferable, and cognitively grounded paradigm of \"atomic\nthinking\".", "AI": {"tldr": "The paper proposes a new paradigm for evaluating mathematical atomic capabilities in LLMs, categorizing abilities into field-specific (algebra, geometry, analysis, topology) and logical dimensions (conceptual understanding, forward reasoning, backward reasoning), and explores how different atomic capabilities influence each other.", "motivation": "Current LLMs primarily rely on scaling up training datasets with diverse mathematical problems, raising questions about whether they genuinely acquire mathematical concepts or merely remember training data. The paper aims to evaluate mathematical intelligence by breaking it down into fundamental atomic capabilities, inspired by human problem-solving approaches.", "method": "Proposed categorization of atomic abilities into two dimensions: field-specific abilities across four mathematical fields and logical abilities at different levels. Created corresponding training and evaluation datasets for each atomic capability unit and conducted experiments to explore how different atomic capabilities influence others.", "result": "Evaluation on advanced models revealed interesting discoveries about different performances on various atomic capabilities and interactions between them. The findings highlight the importance of decoupling mathematical intelligence into atomic components.", "conclusion": "The atomic capability evaluation paradigm provides new insights into model cognition and guides the development of training strategies toward a more efficient, transferable, and cognitively grounded paradigm of \"atomic thinking\" in mathematical reasoning."}}
{"id": "2509.25482", "categories": ["cs.AI", "cs.LG", "cs.RO", "cs.SY", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25482", "abs": "https://arxiv.org/abs/2509.25482", "authors": ["Wouter M. Kouw", "Tim N. Nisslbeck", "Wouter L. N. Nuijten"], "title": "Message passing-based inference in an autoregressive active inference agent", "comment": "14 pages, 4 figures, to be published in the proceedings of the\n  International Workshop on Active Inference 2025", "summary": "We present the design of an autoregressive active inference agent in the form\nof message passing on a factor graph. Expected free energy is derived and\ndistributed across a planning graph. The proposed agent is validated on a robot\nnavigation task, demonstrating exploration and exploitation in a\ncontinuous-valued observation space with bounded continuous-valued actions.\nCompared to a classical optimal controller, the agent modulates action based on\npredictive uncertainty, arriving later but with a better model of the robot's\ndynamics.", "AI": {"tldr": "An autoregressive active inference agent using message passing on factor graphs, validated on robot navigation with continuous observations and actions.", "motivation": "To develop an agent that can balance exploration and exploitation in continuous spaces by leveraging predictive uncertainty.", "method": "Design of an autoregressive active inference agent using message passing on factor graphs, with expected free energy distributed across a planning graph.", "result": "The agent successfully demonstrated exploration and exploitation in continuous-valued observation space with bounded continuous-valued actions, arriving later but with better model of robot dynamics compared to classical optimal controller.", "conclusion": "The proposed active inference agent effectively modulates actions based on predictive uncertainty, achieving better model understanding while trading off arrival time."}}
{"id": "2509.25233", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25233", "abs": "https://arxiv.org/abs/2509.25233", "authors": ["Kasun Eranda Wijethilake", "Adnan Mahmood", "Quan Z. Sheng"], "title": "FedCLF - Towards Efficient Participant Selection for Federated Learning in Heterogeneous IoV Networks", "comment": "Already published in ADMA 2024 on 13th December 2024 Wijethilake,\n  K.E., Mahmood, A., Sheng, Q.Z. (2025). FedCLF - Towards Efficient Participant\n  Selection for Federated Learning in Heterogeneous IoV Networks. In: Sheng,\n  Q.Z., et al. Advanced Data Mining and Applications. ADMA 2024. Lecture Notes\n  in Computer Science(), vol 15388. Springer, Singapore.\n  https://doi.org/10.1007/978-981-96-0814-0_15", "summary": "Federated Learning (FL) is a distributed machine learning technique that\npreserves data privacy by sharing only the trained parameters instead of the\nclient data. This makes FL ideal for highly dynamic, heterogeneous, and\ntime-critical applications, in particular, the Internet of Vehicles (IoV)\nnetworks. However, FL encounters considerable challenges in such networks owing\nto the high data and device heterogeneity. To address these challenges, we\npropose FedCLF, i.e., FL with Calibrated Loss and Feedback control, which\nintroduces calibrated loss as a utility in the participant selection process\nand a feedback control mechanism to dynamically adjust the sampling frequency\nof the clients. The envisaged approach (a) enhances the overall model accuracy\nin case of highly heterogeneous data and (b) optimizes the resource utilization\nfor resource constrained IoV networks, thereby leading to increased efficiency\nin the FL process. We evaluated FedCLF vis-\\`a-vis baseline models, i.e.,\nFedAvg, Newt, and Oort, using CIFAR-10 dataset with varying data heterogeneity.\nOur results depict that FedCLF significantly outperforms the baseline models by\nup to a 16% improvement in high data heterogeneity-related scenarios with\nimproved efficiency via reduced sampling frequency.", "AI": {"tldr": "FedCLF is a federated learning method that uses calibrated loss and feedback control to handle data heterogeneity in IoV networks, improving accuracy by up to 16% while reducing resource usage.", "motivation": "Federated Learning faces challenges in IoV networks due to high data and device heterogeneity, which affects model accuracy and resource efficiency.", "method": "Proposes FedCLF with calibrated loss for participant selection and feedback control to dynamically adjust client sampling frequency.", "result": "FedCLF outperforms FedAvg, Newt, and Oort by up to 16% in high heterogeneity scenarios with reduced sampling frequency.", "conclusion": "FedCLF effectively addresses FL challenges in heterogeneous IoV networks, improving both accuracy and resource efficiency."}}
{"id": "2509.25729", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25729", "abs": "https://arxiv.org/abs/2509.25729", "authors": ["Zihao Zhao", "Anjalie Field"], "title": "Controlled Generation for Private Synthetic Text", "comment": "EMNLP 2025", "summary": "Text anonymization is essential for responsibly developing and deploying AI\nin high-stakes domains such as healthcare, social services, and law. In this\nwork, we propose a novel methodology for privacy-preserving synthetic text\ngeneration that leverages the principles of de-identification and the Hiding In\nPlain Sight (HIPS) theory. Our approach introduces entity-aware control codes\nto guide controllable generation using either in-context learning (ICL) or\nprefix tuning. The ICL variant ensures privacy levels consistent with the\nunderlying de-identification system, while the prefix tuning variant\nincorporates a custom masking strategy and loss function to support scalable,\nhigh-quality generation. Experiments on legal and clinical datasets demonstrate\nthat our method achieves a strong balance between privacy protection and\nutility, offering a practical and effective solution for synthetic text\ngeneration in sensitive domains.", "AI": {"tldr": "A novel privacy-preserving synthetic text generation method using entity-aware control codes with ICL and prefix tuning variants, achieving strong privacy-utility balance in legal and clinical domains.", "motivation": "Text anonymization is crucial for responsible AI development in high-stakes domains like healthcare, social services, and law to protect sensitive information.", "method": "Proposes entity-aware control codes to guide controllable generation using in-context learning (ICL) or prefix tuning, with ICL ensuring privacy levels and prefix tuning incorporating custom masking strategy and loss function.", "result": "Experiments on legal and clinical datasets demonstrate the method achieves strong balance between privacy protection and utility.", "conclusion": "The approach offers a practical and effective solution for synthetic text generation in sensitive domains while maintaining privacy."}}
{"id": "2509.25522", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25522", "abs": "https://arxiv.org/abs/2509.25522", "authors": ["Jingzhe Liu", "Liam Collins", "Jiliang Tang", "Tong Zhao", "Neil Shah", "Clark Mingxuan Ju"], "title": "Understanding Generative Recommendation with Semantic IDs from a Model-scaling View", "comment": null, "summary": "Recent advancements in generative models have allowed the emergence of a\npromising paradigm for recommender systems (RS), known as Generative\nRecommendation (GR), which tries to unify rich item semantics and collaborative\nfiltering signals. One popular modern approach is to use semantic IDs (SIDs),\nwhich are discrete codes quantized from the embeddings of modality encoders\n(e.g., large language or vision models), to represent items in an\nautoregressive user interaction sequence modeling setup (henceforth, SID-based\nGR). While generative models in other domains exhibit well-established scaling\nlaws, our work reveals that SID-based GR shows significant bottlenecks while\nscaling up the model. In particular, the performance of SID-based GR quickly\nsaturates as we enlarge each component: the modality encoder, the quantization\ntokenizer, and the RS itself. In this work, we identify the limited capacity of\nSIDs to encode item semantic information as one of the fundamental bottlenecks.\nMotivated by this observation, as an initial effort to obtain GR models with\nbetter scaling behaviors, we revisit another GR paradigm that directly uses\nlarge language models (LLMs) as recommenders (henceforth, LLM-as-RS). Our\nexperiments show that the LLM-as-RS paradigm has superior model scaling\nproperties and achieves up to 20 percent improvement over the best achievable\nperformance of SID-based GR through scaling. We also challenge the prevailing\nbelief that LLMs struggle to capture collaborative filtering information,\nshowing that their ability to model user-item interactions improves as LLMs\nscale up. Our analyses on both SID-based GR and LLMs across model sizes from\n44M to 14B parameters underscore the intrinsic scaling limits of SID-based GR\nand position LLM-as-RS as a promising path toward foundation models for GR.", "AI": {"tldr": "This paper analyzes scaling limitations in Semantic ID-based Generative Recommendation (SID-based GR) and shows that directly using Large Language Models as recommenders (LLM-as-RS) achieves better scaling performance with up to 20% improvement.", "motivation": "The motivation is to address the scaling bottlenecks in SID-based Generative Recommendation systems, where performance saturates when scaling up modality encoders, quantization tokenizers, and the recommender system itself.", "method": "The study compares two paradigms: SID-based GR (using discrete semantic IDs from modality encoders) and LLM-as-RS (directly using large language models as recommenders), analyzing their scaling behaviors across model sizes from 44M to 14B parameters.", "result": "LLM-as-RS shows superior scaling properties, achieving up to 20% improvement over the best achievable performance of SID-based GR. LLMs also demonstrate improved ability to capture collaborative filtering information as they scale up.", "conclusion": "LLM-as-RS is positioned as a promising path toward foundation models for Generative Recommendation, overcoming the intrinsic scaling limits of SID-based GR approaches."}}
{"id": "2509.25235", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25235", "abs": "https://arxiv.org/abs/2509.25235", "authors": ["Nikola Prianikov", "Evelyne Janssen-van Dam", "Marcin Pietrasik", "Charalampos S. Kouzinopoulos"], "title": "Machine Learning for Pattern Detection in Printhead Nozzle Logging", "comment": "This paper has been published in the 37th International Conference on\n  Tools with Artificial Intelligence in Athens, Greece, November 03-05, 2025", "summary": "Correct identification of failure mechanisms is essential for manufacturers\nto ensure the quality of their products. Certain failures of printheads\ndeveloped by Canon Production Printing can be identified from the behavior of\nindividual nozzles, the states of which are constantly recorded and can form\ndistinct patterns in terms of the number of failed nozzles over time, and in\nspace in the nozzle grid. In our work, we investigate the problem of printhead\nfailure classification based on a multifaceted dataset of nozzle logging and\npropose a Machine Learning classification approach for this problem. We follow\nthe feature-based framework of time-series classification, where a set of\ntime-based and spatial features was selected with the guidance of domain\nexperts. Several traditional ML classifiers were evaluated, and the One-vs-Rest\nRandom Forest was found to have the best performance. The proposed model\noutperformed an in-house rule-based baseline in terms of a weighted F1 score\nfor several failure mechanisms.", "AI": {"tldr": "Machine Learning approach for classifying printhead failure mechanisms using nozzle behavior patterns, outperforming rule-based baseline with Random Forest classifier.", "motivation": "Accurate identification of failure mechanisms is crucial for product quality assurance in printhead manufacturing, as nozzle failures form distinct temporal and spatial patterns.", "method": "Feature-based time-series classification using domain-expert selected time-based and spatial features from nozzle logging data, evaluated with traditional ML classifiers including One-vs-Rest Random Forest.", "result": "One-vs-Rest Random Forest achieved best performance, outperforming in-house rule-based baseline in weighted F1 score for several failure mechanisms.", "conclusion": "Machine Learning classification approach effectively identifies printhead failure mechanisms from nozzle behavior patterns, providing better performance than traditional rule-based methods."}}
{"id": "2509.25733", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25733", "abs": "https://arxiv.org/abs/2509.25733", "authors": ["Mingyu Chen", "Jingkai Lin", "Zhaojie Chu", "Xiaofen Xing", "Yirong Chen", "Xiangmin Xu"], "title": "CATCH: A Novel Data Synthesis Framework for High Therapy Fidelity and Memory-Driven Planning Chain of Thought in AI Counseling", "comment": "To be published in EMNLP 2025 Findings", "summary": "Recently, advancements in AI counseling based on large language models have\nshown significant progress. However, existing studies employ a one-time\ngeneration approach to synthesize multi-turn dialogue samples, resulting in low\ntherapy fidelity and failing to capture the decision-making rationale behind\neach response. In this work, we propose CATCH, a novel data synthesis framework\ndesigned to address these challenges. Specifically, to improve therapy\nfidelity, we introduce the Progressive Dialogue Synthesis strategy, which\nextracts goals, resources, and solutions from a client's self-report, organizes\nthem into structured outlines, and then incrementally generates stage-aligned\ncounseling dialogues. To capture decision-making rationale behind each\nresponse, we propose the Memory-Driven Dynamic Planning thinking pattern that\nintegrates memory enhancement, global planning, and strategy reasoning; a\ncollaborative multi-agent optimizer then leverages MDP to attach explicit\nchain-of-thought to each dialogue turn. Extensive experiments and human\nevaluations demonstrate that CATCH significantly enhances fidelity and logical\ncoherence in AI counseling.", "AI": {"tldr": "CATCH is a novel data synthesis framework for AI counseling that uses Progressive Dialogue Synthesis and Memory-Driven Dynamic Planning to improve therapy fidelity and capture decision-making rationale in multi-turn dialogues.", "motivation": "Existing AI counseling approaches use one-time generation for multi-turn dialogues, resulting in low therapy fidelity and failure to capture the decision-making rationale behind each response.", "method": "CATCH framework employs Progressive Dialogue Synthesis (extracting goals/resources/solutions from client reports and generating stage-aligned dialogues) and Memory-Driven Dynamic Planning (integrating memory enhancement, global planning, and strategy reasoning with multi-agent optimization to attach chain-of-thought to each dialogue turn).", "result": "Extensive experiments and human evaluations demonstrate that CATCH significantly enhances fidelity and logical coherence in AI counseling.", "conclusion": "CATCH effectively addresses the limitations of existing AI counseling approaches by improving therapy fidelity and capturing decision-making rationale through its novel synthesis framework."}}
{"id": "2509.25530", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25530", "abs": "https://arxiv.org/abs/2509.25530", "authors": ["Kai Guo", "Xinnan Dai", "Shenglai Zeng", "Harry Shomer", "Haoyu Han", "Yu Wang", "Jiliang Tang"], "title": "Beyond Static Retrieval: Opportunities and Pitfalls of Iterative Retrieval in GraphRAG", "comment": null, "summary": "Retrieval-augmented generation (RAG) is a powerful paradigm for improving\nlarge language models (LLMs) on knowledge-intensive question answering.\nGraph-based RAG (GraphRAG) leverages entity-relation graphs to support\nmulti-hop reasoning, but most systems still rely on static retrieval. When\ncrucial evidence, especially bridge documents that connect disjoint entities,\nis absent, reasoning collapses and hallucinations persist. Iterative retrieval,\nwhich performs multiple rounds of evidence selection, has emerged as a\npromising alternative, yet its role within GraphRAG remains poorly understood.\nWe present the first systematic study of iterative retrieval in GraphRAG,\nanalyzing how different strategies interact with graph-based backbones and\nunder what conditions they succeed or fail. Our findings reveal clear\nopportunities: iteration improves complex multi-hop questions, helps promote\nbridge documents into leading ranks, and different strategies offer\ncomplementary strengths. At the same time, pitfalls remain: naive expansion\noften introduces noise that reduces precision, gains are limited on single-hop\nor simple comparison questions, and several bridge evidences still be buried\ntoo deep to be effectively used. Together, these results highlight a central\nbottleneck, namely that GraphRAG's effectiveness depends not only on recall but\nalso on whether bridge evidence is consistently promoted into leading positions\nwhere it can support reasoning chains. To address this challenge, we propose\nBridge-Guided Dual-Thought-based Retrieval (BDTR), a simple yet effective\nframework that generates complementary thoughts and leverages reasoning chains\nto recalibrate rankings and bring bridge evidence into leading positions. BDTR\nachieves consistent improvements across diverse GraphRAG settings and provides\nguidance for the design of future GraphRAG systems.", "AI": {"tldr": "This paper presents the first systematic study of iterative retrieval in GraphRAG, revealing that while iteration improves multi-hop reasoning and bridge document promotion, naive expansion introduces noise. The authors propose BDTR framework to address the bottleneck of bridge evidence ranking.", "motivation": "Graph-based RAG systems rely on static retrieval, which fails when crucial bridge documents connecting disjoint entities are absent, leading to reasoning collapse and hallucinations. The role of iterative retrieval in GraphRAG remains poorly understood.", "method": "The authors conducted a systematic study of iterative retrieval strategies in GraphRAG, analyzing their interaction with graph-based backbones. They proposed Bridge-Guided Dual-Thought-based Retrieval (BDTR), which generates complementary thoughts and leverages reasoning chains to recalibrate rankings.", "result": "Iterative retrieval improves complex multi-hop questions and helps promote bridge documents into leading ranks, but naive expansion introduces noise that reduces precision. Gains are limited on single-hop questions, and some bridge evidence remains buried too deep.", "conclusion": "GraphRAG's effectiveness depends on both recall and consistent promotion of bridge evidence into leading positions. BDTR framework achieves consistent improvements across diverse GraphRAG settings and provides guidance for future system design."}}
{"id": "2509.25238", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25238", "abs": "https://arxiv.org/abs/2509.25238", "authors": ["Sri Vatsa Vuddanti", "Aarav Shah", "Satwik Kumar Chittiprolu", "Tony Song", "Sunishchal Dev", "Kevin Zhu", "Maheep Chaudhary"], "title": "PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases", "comment": null, "summary": "Tool-augmented language agents frequently fail in real-world deployment due\nto tool malfunctions--timeouts, API exceptions, or inconsistent\noutputs--triggering cascading reasoning errors and task abandonment. Existing\nagent training pipelines optimize only for success trajectories, failing to\nexpose models to the tool failures that dominate real-world usage. We propose\n\\textbf{PALADIN}, a generalizable framework for equipping language agents with\nrobust failure recovery capabilities. PALADIN trains on 50,000+\nrecovery-annotated trajectories constructed via systematic failure injection\nand expert demonstrations on an enhanced ToolBench dataset. Training uses\nLoRA-based fine-tuning to retain base capabilities while injecting recovery\ncompetence. At inference, PALADIN detects execution-time errors and retrieves\nthe most similar case from a curated bank of 55+ failure exemplars aligned with\nToolScan's taxonomy, then executes the corresponding recovery action. This\napproach generalizes to novel failures beyond the training distribution,\nretaining 95.2\\% recovery performance on unseen tool APIs. Evaluation across\nPaladinEval and ToolReflectEval demonstrates consistent improvements in\nRecovery Rate (RR), Task Success Rate (TSR), Catastrophic Success Rate (CSR),\nand Efficiency Score (ES). PALADIN improves RR from 32.76% to 89.68% (+57%\nrelative) over ToolBench and outperforms the strongest baseline CRITIC (76.34%)\nby +13.3%. Against vanilla agents, PALADIN achieves 89.86\\% RR (+66% relative\nimprovement from 23.75%). These results establish PALADIN as an effective\nmethod for building fault-tolerant agents capable of robust recovery in\nreal-world tool environments.", "AI": {"tldr": "PALADIN is a framework that trains language agents to recover from tool failures (timeouts, API exceptions, inconsistent outputs) using systematic failure injection and expert demonstrations, achieving significant improvements in recovery rates.", "motivation": "Tool-augmented language agents often fail in real-world deployment due to tool malfunctions, triggering cascading errors and task abandonment. Existing training only optimizes for success, not exposing models to the failures that dominate real usage.", "method": "PALADIN trains on 50,000+ recovery-annotated trajectories via systematic failure injection and expert demonstrations on ToolBench dataset. Uses LoRA-based fine-tuning to retain base capabilities while adding recovery competence. At inference, detects errors and retrieves similar cases from 55+ failure exemplars aligned with ToolScan's taxonomy.", "result": "PALADIN improves Recovery Rate from 32.76% to 89.68% (+57% relative) over ToolBench, outperforms CRITIC (76.34%) by +13.3%, and achieves 89.86% RR (+66% improvement from vanilla agents' 23.75%). Retains 95.2% recovery performance on unseen tool APIs.", "conclusion": "PALADIN establishes an effective method for building fault-tolerant agents capable of robust recovery in real-world tool environments, demonstrating consistent improvements across multiple evaluation metrics."}}
{"id": "2509.25736", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.NI", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.25736", "abs": "https://arxiv.org/abs/2509.25736", "authors": ["Chenhua Shi", "Gregor Macdonald", "Bhavika Jalli", "Wanlu Lei", "John Zou", "Mridul Jain", "Joji Philip"], "title": "Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications", "comment": "6 pages, 6 figures, 5 tables", "summary": "The success of large language models (LLMs) depends heavily on large-scale,\nhigh-quality instruction-following and reinforcement datasets. However,\ngenerating such data through human annotation is prohibitively time-consuming\nparticularly for domain-specific tasks like telecom network troubleshooting,\nwhere accurate responses require deep technical expertise and contextual\nunderstanding. In this paper, we present a fully automated, retrieval-augmented\npipeline for generating synthetic question-answer (QA) pairs grounded in\nstructured domain knowledge. Our multi-stage framework integrates a retriever,\nbase generator, and refinement model to synthesize and enhance QA pairs using\ndocuments retrieved from a domain-specific knowledge graph. To ensure data\nquality, we employ customized RAGAS-based scoring to filter low-quality\nsamples, producing a high-quality dataset suitable for reinforcement\nfine-tuning (RFT). We demonstrate our approach in a real-world telecom scenario\nfocused on radio access network (RAN) troubleshooting. The resulting pipeline\ngenerates complex, context-rich troubleshooting solution plans without human\nintervention. This work offers a scalable solution for building instruction and\nreinforcement datasets in specialized domains, significantly reducing\ndependence on manual labeling while maintaining high technical fidelity.", "AI": {"tldr": "Automated pipeline for generating high-quality synthetic QA pairs for domain-specific LLM training using retrieval-augmented generation and RAGAS-based filtering.", "motivation": "Manual annotation of instruction-following datasets for domain-specific tasks like telecom troubleshooting is time-consuming and requires deep technical expertise.", "method": "Multi-stage framework with retriever, base generator, and refinement model using domain-specific knowledge graph, with RAGAS-based quality filtering.", "result": "Generated complex, context-rich troubleshooting solution plans for telecom RAN without human intervention.", "conclusion": "Scalable solution for building instruction and reinforcement datasets in specialized domains, reducing manual labeling dependency while maintaining technical fidelity."}}
{"id": "2509.25540", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25540", "abs": "https://arxiv.org/abs/2509.25540", "authors": ["Jason Holmes", "Yuexing Hao", "Mariana Borras-Osorio", "Federico Mastroleo", "Santiago Romero Brufau", "Valentina Carducci", "Katie M Van Abel", "David M Routman", "Andrew Y. K. Foong", "Liv M Muller", "Satomi Shiraishi", "Daniel K Ebner", "Daniel J Ma", "Sameer R Keole", "Samir H Patel", "Mirek Fatyga", "Martin Bues", "Brad J Stish", "Yolanda I Garces", "Michelle A Neben Wittich", "Robert L Foote", "Sujay A Vora", "Nadia N Laack", "Mark R Waddle", "Wei Liu"], "title": "RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling at Scale", "comment": null, "summary": "Manual labeling limits the scale, accuracy, and timeliness of patient\noutcomes research in radiation oncology. We present RadOnc-GPT, an autonomous\nlarge language model (LLM)-based agent capable of independently retrieving\npatient-specific information, iteratively assessing evidence, and returning\nstructured outcomes. Our evaluation explicitly validates RadOnc-GPT across two\nclearly defined tiers of increasing complexity: (1) a structured quality\nassurance (QA) tier, assessing the accurate retrieval of demographic and\nradiotherapy treatment plan details, followed by (2) a complex clinical\noutcomes labeling tier involving determination of mandibular osteoradionecrosis\n(ORN) in head-and-neck cancer patients and detection of cancer recurrence in\nindependent prostate and head-and-neck cancer cohorts requiring combined\ninterpretation of structured and unstructured patient data. The QA tier\nestablishes foundational trust in structured-data retrieval, a critical\nprerequisite for successful complex clinical outcome labeling.", "AI": {"tldr": "RadOnc-GPT is an autonomous LLM agent that automates patient outcomes research in radiation oncology by retrieving patient data, assessing evidence, and generating structured outcomes, validated through QA and complex clinical outcome labeling tiers.", "motivation": "Manual labeling limits the scale, accuracy, and timeliness of patient outcomes research in radiation oncology, necessitating an automated solution.", "method": "Developed RadOnc-GPT, an autonomous LLM-based agent that independently retrieves patient-specific information, iteratively assesses evidence, and returns structured outcomes. Evaluation includes two tiers: structured QA for data retrieval and complex clinical outcomes labeling for conditions like mandibular osteoradionecrosis and cancer recurrence.", "result": "The QA tier establishes foundational trust in structured-data retrieval, which is critical for successful complex clinical outcome labeling.", "conclusion": "RadOnc-GPT demonstrates potential to overcome limitations of manual labeling in radiation oncology outcomes research through automated, structured data retrieval and clinical outcome determination."}}
{"id": "2509.25240", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.25240", "abs": "https://arxiv.org/abs/2509.25240", "authors": ["Ming Yang", "Xiaofan Li", "Zhiyuan Ma", "Dengliang Shi", "Jintao Du", "Yu Cheng", "Weiguo Zheng"], "title": "HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement", "comment": "20 pages, 7 figures, 4 tables", "summary": "Recent curriculum reinforcement learning for large language models (LLMs)\ntypically rely on difficulty-based annotations for data filtering and ordering.\nHowever, such methods suffer from local optimization, where continual training\non simple samples in the early steps can cause the policy to lose its\nexploration. We propose a novel schema, namely Hamiltonian curiosity augmented\nlarge language model reinforcement (HAMMER), that transfers diversity metrics,\ncommonly used in dataset evaluation, into the dynamic reinforcement learning\nprocedure, where training samples are ordered via a minimum-semantic\nHamiltonian path making the initial training retrain more exploration. From a\ntheoretical perspective of generalization bounds, diversity-driven ordering\nfacilitates stable convergence. Empirical evaluations indicate that HAMMER\nstimulates model \"curiosity\" and consistently achieves a 3% to 4% average\naccuracy gain across diverse inference benchmark.", "AI": {"tldr": "HAMMER is a curriculum RL method for LLMs that uses diversity metrics instead of difficulty-based ordering to maintain exploration during training, achieving 3-4% accuracy gains.", "motivation": "Traditional difficulty-based curriculum RL for LLMs suffers from local optimization where early training on simple samples reduces exploration capability.", "method": "Proposes HAMMER framework that transfers diversity metrics from dataset evaluation to RL training, ordering samples via minimum-semantic Hamiltonian path to retain exploration.", "result": "Empirical evaluations show HAMMER stimulates model curiosity and consistently achieves 3-4% average accuracy gain across diverse inference benchmarks.", "conclusion": "Diversity-driven curriculum ordering facilitates stable convergence and better performance compared to difficulty-based approaches in LLM reinforcement learning."}}
{"id": "2509.25752", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25752", "abs": "https://arxiv.org/abs/2509.25752", "authors": ["T. O. Abiola", "K. D. Abiodun", "O. E. Olumide", "O. O. Adebanji", "O. Hiram Calvo", "Grigori Sidorov"], "title": "Detecting Hope Across Languages: Multiclass Classification for Positive Online Discourse", "comment": null, "summary": "The detection of hopeful speech in social media has emerged as a critical\ntask for promoting positive discourse and well-being. In this paper, we present\na machine learning approach to multiclass hope speech detection across multiple\nlanguages, including English, Urdu, and Spanish. We leverage transformer-based\nmodels, specifically XLM-RoBERTa, to detect and categorize hope speech into\nthree distinct classes: Generalized Hope, Realistic Hope, and Unrealistic Hope.\nOur proposed methodology is evaluated on the PolyHope dataset for the\nPolyHope-M 2025 shared task, achieving competitive performance across all\nlanguages. We compare our results with existing models, demonstrating that our\napproach significantly outperforms prior state-of-the-art techniques in terms\nof macro F1 scores. We also discuss the challenges in detecting hope speech in\nlow-resource languages and the potential for improving generalization. This\nwork contributes to the development of multilingual, fine-grained hope speech\ndetection models, which can be applied to enhance positive content moderation\nand foster supportive online communities.", "AI": {"tldr": "This paper presents a multilingual hope speech detection system using XLM-RoBERTa transformer model to classify hope speech into three categories (Generalized, Realistic, Unrealistic Hope) across English, Urdu, and Spanish, achieving state-of-the-art performance on the PolyHope dataset.", "motivation": "To promote positive discourse and well-being in social media by developing effective methods for detecting and categorizing hope speech across multiple languages, addressing the need for positive content moderation and supportive online communities.", "method": "Leveraged transformer-based XLM-RoBERTa model for multiclass hope speech detection across English, Urdu, and Spanish languages. Used the PolyHope dataset for evaluation and participated in the PolyHope-M 2025 shared task.", "result": "Achieved competitive performance across all three languages and significantly outperformed prior state-of-the-art techniques in terms of macro F1 scores. Demonstrated effectiveness in multilingual hope speech detection.", "conclusion": "The work contributes to developing multilingual, fine-grained hope speech detection models that can enhance positive content moderation and foster supportive online communities, while highlighting challenges in low-resource languages and potential for improved generalization."}}
{"id": "2509.25550", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25550", "abs": "https://arxiv.org/abs/2509.25550", "authors": ["Dongsu Lee", "Daehee Lee", "Yaru Niu", "Honguk Woo", "Amy Zhang", "Ding Zhao"], "title": "Learning to Interact in World Latent for Team Coordination", "comment": null, "summary": "This work presents a novel representation learning framework, interactive\nworld latent (IWoL), to facilitate team coordination in multi-agent\nreinforcement learning (MARL). Building effective representation for team\ncoordination is a challenging problem, due to the intricate dynamics emerging\nfrom multi-agent interaction and incomplete information induced by local\nobservations. Our key insight is to construct a learnable representation space\nthat jointly captures inter-agent relations and task-specific world information\nby directly modeling communication protocols. This representation, we maintain\nfully decentralized execution with implicit coordination, all while avoiding\nthe inherent drawbacks of explicit message passing, e.g., slower\ndecision-making, vulnerability to malicious attackers, and sensitivity to\nbandwidth constraints. In practice, our representation can be used not only as\nan implicit latent for each agent, but also as an explicit message for\ncommunication. Across four challenging MARL benchmarks, we evaluate both\nvariants and show that IWoL provides a simple yet powerful key for team\ncoordination. Moreover, we demonstrate that our representation can be combined\nwith existing MARL algorithms to further enhance their performance.", "AI": {"tldr": "IWoL is a novel representation learning framework for multi-agent reinforcement learning that creates a learnable representation space capturing inter-agent relations and task information through implicit coordination, enabling both decentralized execution and explicit communication.", "motivation": "To address challenges in team coordination for MARL, including complex multi-agent dynamics and incomplete information from local observations, while avoiding drawbacks of explicit message passing like slow decision-making and security vulnerabilities.", "method": "Construct a learnable representation space that jointly models inter-agent relations and task-specific world information by directly modeling communication protocols, enabling both implicit coordination and explicit messaging capabilities.", "result": "IWoL demonstrates strong performance across four challenging MARL benchmarks, showing it can be used as both implicit latent representations and explicit messages, and can enhance existing MARL algorithms when combined.", "conclusion": "IWoL provides a simple yet powerful solution for team coordination in MARL, offering flexible representation that supports both decentralized execution with implicit coordination and explicit communication when needed."}}
{"id": "2509.25760", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25760", "abs": "https://arxiv.org/abs/2509.25760", "authors": ["Zhepei Wei", "Xiao Yang", "Kai Sun", "Jiaqi Wang", "Rulin Shao", "Sean Chen", "Mohammad Kachuee", "Teja Gollapudi", "Tony Liao", "Nicolas Scheffer", "Rakesh Wanga", "Anuj Kumar", "Yu Meng", "Wen-tau Yih", "Xin Luna Dong"], "title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning", "comment": null, "summary": "While large language models (LLMs) have demonstrated strong performance on\nfactoid question answering, they are still prone to hallucination and\nuntruthful responses, particularly when tasks demand information outside their\nparametric knowledge. Indeed, truthfulness requires more than accuracy --\nmodels must also recognize uncertainty and abstain when unsure to avoid\nhallucinations. This presents a fundamental challenge for existing methods:\napproaches that optimize for accuracy often amplify hallucinations, while those\nthat encourage abstention can become overly conservative, sacrificing correct\nanswers. Both extremes ultimately compromise truthfulness. In this work, we\npresent TruthRL, a general reinforcement learning (RL) framework that directly\noptimizes the truthfulness of LLMs. Specifically, we implement TruthRL using\nGRPO with a simple yet effective ternary reward that distinguishes correct\nanswers, hallucinations, and abstentions. It incentivizes models to reduce\nhallucinations not only by providing correct responses, but also by enabling\nabstention when uncertain, thereby improving truthfulness. Extensive\nexperiments across four knowledge-intensive benchmarks show that, compared to\nvanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves\ntruthfulness by 21.1%, with consistent gains across various backbone models\n(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth\nablation study demonstrates that vanilla accuracy-driven methods, such as\nsupervised fine-tuning or RL with a binary reward, struggle to balance factual\ncorrectness and uncertainty. In contrast, our proposed truthfulness-driven\nTruthRL achieves strong performance in both accuracy and truthfulness,\nunderscoring the importance of learning objective design for developing\ntruthful LLMs.", "AI": {"tldr": "TruthRL is a reinforcement learning framework that directly optimizes LLM truthfulness using ternary rewards to distinguish correct answers, hallucinations, and abstentions, achieving significant reductions in hallucinations and improvements in truthfulness.", "motivation": "LLMs are prone to hallucination and untruthful responses, especially when tasks require information beyond their parametric knowledge. Existing methods struggle to balance accuracy and abstention - accuracy-driven approaches amplify hallucinations while abstention-focused methods become overly conservative.", "method": "TruthRL uses GRPO (Generalized Reinforcement Policy Optimization) with a ternary reward system that distinguishes between correct answers, hallucinations, and abstentions. This incentivizes models to reduce hallucinations by providing correct responses and enabling abstention when uncertain.", "result": "Extensive experiments across four knowledge-intensive benchmarks show TruthRL reduces hallucinations by 28.9% and improves truthfulness by 21.1% compared to vanilla RL, with consistent gains across various backbone models (Qwen, Llama) under both retrieval and non-retrieval setups.", "conclusion": "TruthRL achieves strong performance in both accuracy and truthfulness, demonstrating that truthfulness-driven learning objectives are crucial for developing truthful LLMs, unlike vanilla accuracy-driven methods that struggle to balance factual correctness and uncertainty."}}
{"id": "2509.25552", "categories": ["cs.AI", "J.3"], "pdf": "https://arxiv.org/pdf/2509.25552", "abs": "https://arxiv.org/abs/2509.25552", "authors": ["Shangqi Gao", "Sihan Wang", "Yibo Gao", "Boming Wang", "Xiahai Zhuang", "Anne Warren", "Grant Stewart", "James Jones", "Mireia Crispin-Ortuzar"], "title": "Evaluating Foundation Models with Pathological Concept Learning for Kidney Cancer", "comment": "Best Paper Award at MICCAI AMAI 2025", "summary": "To evaluate the translational capabilities of foundation models, we develop a\npathological concept learning approach focused on kidney cancer. By leveraging\nTNM staging guidelines and pathology reports, we build comprehensive\npathological concepts for kidney cancer. Then, we extract deep features from\nwhole slide images using foundation models, construct pathological graphs to\ncapture spatial correlations, and trained graph neural networks to identify\nthese concepts. Finally, we demonstrate the effectiveness of this approach in\nkidney cancer survival analysis, highlighting its explainability and fairness\nin identifying low- and high-risk patients. The source code has been released\nby https://github.com/shangqigao/RadioPath.", "AI": {"tldr": "A pathological concept learning approach for kidney cancer using foundation models and graph neural networks to analyze whole slide images, demonstrating effectiveness in survival analysis with explainability and fairness.", "motivation": "To evaluate the translational capabilities of foundation models in medical imaging, specifically for kidney cancer analysis and survival prediction.", "method": "Leverage TNM staging guidelines and pathology reports to build pathological concepts, extract deep features from whole slide images using foundation models, construct pathological graphs for spatial correlations, and train graph neural networks to identify these concepts.", "result": "The approach effectively identifies low- and high-risk kidney cancer patients in survival analysis, demonstrating explainability and fairness.", "conclusion": "The proposed pathological concept learning approach using foundation models and graph neural networks shows promising translational capabilities for kidney cancer analysis, with released source code available."}}
{"id": "2509.25253", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25253", "abs": "https://arxiv.org/abs/2509.25253", "authors": ["Prajjwal Bhattarai", "Mohammad Amjad", "Dmytro Zhylko", "Tuka Alhanai"], "title": "Knowledge distillation through geometry-aware representational alignment", "comment": null, "summary": "Knowledge distillation is a common paradigm for transferring capabilities\nfrom larger models to smaller ones. While traditional distillation methods\nleverage a probabilistic divergence over the output of the teacher and student\nmodels, feature-based distillation methods often minimize variants of Euclidean\nnorms between the hidden layer representations. The main goal is for the\nstudent to mimic the structure of the feature space of the teacher. In this\nwork, we theoretically show that existing feature distillation methods, such as\nprojection based mean squared loss or Centered Kernel Alignment (CKA), cannot\ncapture the feature structure, even under zero loss. We then motivate the use\nof Procrustes distance and the Frobenius norm of Feature Gram Matrix, distances\nalready common in the context of measuring representational alignment, as\ndistillation losses. We show that feature distillation through our method\nshowcases statistically significant improvement in distillation performance\nacross language models families (BERT and OPT) in classification and\ninstruction-following tasks by up to 2 percentage points, showcasing the\npotential of integrating feature geometry into existing distillation methods.", "AI": {"tldr": "This paper proposes a new feature-based knowledge distillation method using Procrustes distance and Feature Gram Matrix Frobenius norm, showing these better capture feature structure than existing methods like CKA and projection-based MSE, with 2% performance improvements on BERT and OPT models.", "motivation": "Existing feature distillation methods fail to properly capture the feature structure of teacher models even under zero loss conditions, motivating the need for better geometric alignment approaches.", "method": "The authors propose using Procrustes distance and Frobenius norm of Feature Gram Matrix as distillation losses, which better align with representational alignment measurement principles.", "result": "The method achieves statistically significant improvement in distillation performance across BERT and OPT models in classification and instruction-following tasks by up to 2 percentage points.", "conclusion": "Integrating proper feature geometry through Procrustes distance and Gram Matrix norms into distillation methods shows promising potential for better knowledge transfer."}}
{"id": "2509.25795", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25795", "abs": "https://arxiv.org/abs/2509.25795", "authors": ["Obed Junias", "Prajakta Kini", "Theodora Chaspari"], "title": "Assessing Algorithmic Bias in Language-Based Depression Detection: A Comparison of DNN and LLM Approaches", "comment": "7 pages, 1 figure. This paper has been accepted to the IEEE-EMBS\n  International Conference on Biomedical and Health Informatics (BHI 2025),\n  Georgia Institute of Technology, Atlanta, Georgia, October 26-29, 2025", "summary": "This paper investigates algorithmic bias in language-based models for\nautomated depression detection, focusing on socio-demographic disparities\nrelated to gender and race/ethnicity. Models trained using deep neural networks\n(DNN) based embeddings are compared to few-shot learning approaches with large\nlanguage models (LLMs), evaluating both performance and fairness on clinical\ninterview transcripts from the Distress Analysis Interview Corpus/Wizard-of-Oz\n(DAIC-WOZ). To mitigate bias, fairness-aware loss functions are applied to\nDNN-based models, while in-context learning with varied prompt framing and shot\ncounts is explored for LLMs. Results indicate that LLMs outperform DNN-based\nmodels in depression classification, particularly for underrepresented groups\nsuch as Hispanic participants. LLMs also exhibit reduced gender bias compared\nto DNN-based embeddings, though racial disparities persist. Among\nfairness-aware techniques for mitigating bias in DNN-based embeddings, the\nworst-group loss, which is designed to minimize loss for the worst-performing\ndemographic group, achieves a better balance between performance and fairness.\nIn contrast, the fairness-regularized loss minimizes loss across all groups but\nperforms less effectively. In LLMs, guided prompting with ethical framing helps\nmitigate gender bias in the 1-shot setting. However, increasing the number of\nshots does not lead to further reductions in disparities. For race/ethnicity,\nneither prompting strategy nor increasing $N$ in $N$-shot learning effectively\nreduces disparities.", "AI": {"tldr": "This paper examines algorithmic bias in depression detection models, comparing DNN-based embeddings with LLM few-shot learning. LLMs show better performance and reduced gender bias, while fairness-aware techniques help DNN models, with worst-group loss being most effective.", "motivation": "To investigate socio-demographic disparities (gender and race/ethnicity) in automated depression detection systems and compare different approaches for mitigating algorithmic bias.", "method": "Compared DNN-based embeddings with LLM few-shot learning on DAIC-WOZ clinical transcripts. Applied fairness-aware loss functions to DNN models and explored in-context learning with varied prompts for LLMs.", "result": "LLMs outperformed DNN models in depression classification, especially for Hispanic participants. LLMs showed reduced gender bias but persistent racial disparities. Worst-group loss worked best for DNN fairness, while ethical prompting helped LLMs in 1-shot settings.", "conclusion": "LLMs offer advantages in depression detection with reduced gender bias, but racial disparities remain challenging. Fairness-aware techniques show promise for DNN models, while prompting strategies have limited effectiveness for racial bias mitigation in LLMs."}}
{"id": "2509.25558", "categories": ["cs.AI", "cs.HC", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.25558", "abs": "https://arxiv.org/abs/2509.25558", "authors": ["Diana Mykhaylychenko", "Maisha Thasin", "Dunya Baradari", "Charmelle Mhungu"], "title": "A(I)nimism: Re-enchanting the World Through AI-Mediated Object Interaction", "comment": null, "summary": "Animist worldviews treat beings, plants, landscapes, and even tools as\npersons endowed with spirit, an orientation that has long shaped human-nonhuman\nrelations through ritual and moral practice. While modern industrial societies\nhave often imagined technology as mute and mechanical, recent advances in\nartificial intelligence (AI), especially large language models (LLMs), invite\npeople to anthropomorphize and attribute inner life to devices. This paper\nintroduces A(I)nimism, an interactive installation exploring how large language\nobjects (LLOs) can mediate animistic relationships with everyday things. Housed\nwithin a physical 'portal', the system uses GPT-4 Vision, voice input, and\nmemory-based agents to create evolving object-personas. Encounters unfold\nthrough light, sound, and touch in a ritual-like process of request,\nconversation, and transformation that is designed to evoke empathy, wonder, and\nreflection. We situate the project within anthropological perspectives,\nspeculative design, and spiritual HCI. AI's opacity, we argue, invites\nanimistic interpretation, allowing LLOs to re-enchant the mundane and spark new\nquestions of agency, responsibility, and design.", "AI": {"tldr": "The paper introduces A(I)nimism, an interactive installation that uses AI to create animistic relationships with everyday objects through ritual-like interactions mediated by large language models.", "motivation": "To explore how AI technologies, particularly large language models, can facilitate animistic relationships with everyday objects, bridging traditional animist worldviews with modern technology and re-enchanting mundane items.", "method": "Created an interactive installation using GPT-4 Vision, voice input, and memory-based agents housed in a physical 'portal'. The system enables evolving object-personas through ritual-like processes involving light, sound, and touch interactions.", "result": "The installation successfully mediates animistic relationships with everyday things, evoking empathy, wonder, and reflection through AI-mediated encounters that transform ordinary objects into entities with perceived inner life.", "conclusion": "AI's inherent opacity invites animistic interpretation, allowing large language objects to re-enchant the mundane and raise important questions about agency, responsibility, and design in human-technology relationships."}}
{"id": "2509.25261", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.25261", "abs": "https://arxiv.org/abs/2509.25261", "authors": ["Xianyang Deng", "Wenshuai Liu", "Yaru FuB", "Qi Zhu"], "title": "Heterogeneous Multi-agent Collaboration in UAV-assisted Mobile Crowdsensing Networks", "comment": "7 pages, 6 figures", "summary": "Unmanned aerial vehicles (UAVs)-assisted mobile crowdsensing (MCS) has\nemerged as a promising paradigm for data collection. However, challenges such\nas spectrum scarcity, device heterogeneity, and user mobility hinder efficient\ncoordination of sensing, communication, and computation. To tackle these\nissues, we propose a joint optimization framework that integrates time slot\npartition for sensing, communication, and computation phases, resource\nallocation, and UAV 3D trajectory planning, aiming to maximize the amount of\nprocessed sensing data. The problem is formulated as a non-convex stochastic\noptimization and further modeled as a partially observable Markov decision\nprocess (POMDP) that can be solved by multi-agent deep reinforcement learning\n(MADRL) algorithm. To overcome the limitations of conventional multi-layer\nperceptron (MLP) networks, we design a novel MADRL algorithm with hybrid actor\nnetwork. The newly developed method is based on heterogeneous agent proximal\npolicy optimization (HAPPO), empowered by convolutional neural networks (CNN)\nfor feature extraction and Kolmogorov-Arnold networks (KAN) to capture\nstructured state-action dependencies. Extensive numerical results demonstrate\nthat our proposed method achieves significant improvements in the amount of\nprocessed sensing data when compared with other benchmarks.", "AI": {"tldr": "Proposes a joint optimization framework for UAV-assisted MCS that integrates time slot partition, resource allocation, and 3D trajectory planning using a novel MADRL algorithm with hybrid actor network to maximize processed sensing data.", "motivation": "Address challenges in UAV-assisted mobile crowdsensing including spectrum scarcity, device heterogeneity, and user mobility that hinder efficient coordination of sensing, communication, and computation.", "method": "Formulates the problem as non-convex stochastic optimization and POMDP, solved by novel MADRL algorithm with hybrid actor network combining HAPPO, CNN for feature extraction, and KAN for structured state-action dependencies.", "result": "Extensive numerical results show significant improvements in the amount of processed sensing data compared to other benchmarks.", "conclusion": "The proposed joint optimization framework with novel MADRL algorithm effectively addresses coordination challenges in UAV-assisted MCS and achieves superior performance in processed data volume."}}
{"id": "2509.25813", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25813", "abs": "https://arxiv.org/abs/2509.25813", "authors": ["Dragos-Dumitru Ghinea", "Adela-Nicoleta Corbeanu", "Adrian-Marius Dumitran"], "title": "RoBiologyDataChoiceQA: A Romanian Dataset for improving Biology understanding of Large Language Models", "comment": null, "summary": "In recent years, large language models (LLMs) have demonstrated significant\npotential across various natural language processing (NLP) tasks. However,\ntheir performance in domain-specific applications and non-English languages\nremains less explored. This study introduces a novel Romanian-language dataset\nfor multiple-choice biology questions, carefully curated to assess LLM\ncomprehension and reasoning capabilities in scientific contexts. Containing\napproximately 14,000 questions, the dataset provides a comprehensive resource\nfor evaluating and improving LLM performance in biology.\n  We benchmark several popular LLMs, analyzing their accuracy, reasoning\npatterns, and ability to understand domain-specific terminology and linguistic\nnuances. Additionally, we perform comprehensive experiments to evaluate the\nimpact of prompt engineering, fine-tuning, and other optimization techniques on\nmodel performance. Our findings highlight both the strengths and limitations of\ncurrent LLMs in handling specialized knowledge tasks in low-resource languages,\noffering valuable insights for future research and development.", "AI": {"tldr": "This paper introduces a Romanian-language biology multiple-choice question dataset to evaluate LLMs' performance in domain-specific, non-English contexts, benchmarking several models and analyzing optimization techniques.", "motivation": "To address the under-explored performance of LLMs in domain-specific applications and non-English languages, particularly in scientific contexts.", "method": "Created a novel Romanian-language dataset with ~14,000 biology multiple-choice questions, benchmarked popular LLMs, and evaluated prompt engineering, fine-tuning, and other optimization techniques.", "result": "The study provides comprehensive evaluation of LLMs' accuracy, reasoning patterns, and understanding of domain-specific terminology in Romanian biology contexts.", "conclusion": "The research highlights both strengths and limitations of current LLMs in handling specialized knowledge tasks in low-resource languages, offering valuable insights for future development."}}
{"id": "2509.25559", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25559", "abs": "https://arxiv.org/abs/2509.25559", "authors": ["Suvrankar Datta", "Divya Buchireddygari", "Lakshmi Vennela Chowdary Kaza", "Mrudula Bhalke", "Kautik Singh", "Ayush Pandey", "Sonit Sai Vasipalli", "Upasana Karnwal", "Hakikat Bir Singh Bhatti", "Bhavya Ratan Maroo", "Sanjana Hebbar", "Rahul Joseph", "Gurkawal Kaur", "Devyani Singh", "Akhil V", "Dheeksha Devasya Shama Prasad", "Nishtha Mahajan", "Ayinaparthi Arisha", "Rajesh Vanagundi", "Reet Nandy", "Kartik Vuthoo", "Snigdhaa Rajvanshi", "Nikhileswar Kondaveeti", "Suyash Gunjal", "Rishabh Jain", "Rajat Jain", "Anurag Agrawal"], "title": "Radiology's Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology", "comment": "29 pages, 7 figures, 7 tables, includes Annexure (1). Part of the\n  work accepted at RSNA 2025 (Cutting Edge Oral Presentation)", "summary": "Generalist multimodal AI systems such as large language models (LLMs) and\nvision language models (VLMs) are increasingly accessed by clinicians and\npatients alike for medical image interpretation through widely available\nconsumer-facing chatbots. Most evaluations claiming expert level performance\nare on public datasets containing common pathologies. Rigorous evaluation of\nfrontier models on difficult diagnostic cases remains limited. We developed a\npilot benchmark of 50 expert-level \"spot diagnosis\" cases across multiple\nimaging modalities to evaluate the performance of frontier AI models against\nboard-certified radiologists and radiology trainees. To mirror real-world\nusage, the reasoning modes of five popular frontier AI models were tested\nthrough their native web interfaces, viz. OpenAI o3, OpenAI GPT-5, Gemini 2.5\nPro, Grok-4, and Claude Opus 4.1. Accuracy was scored by blinded experts, and\nreproducibility was assessed across three independent runs. GPT-5 was\nadditionally evaluated across various reasoning modes. Reasoning quality errors\nwere assessed and a taxonomy of visual reasoning errors was defined.\nBoard-certified radiologists achieved the highest diagnostic accuracy (83%),\noutperforming trainees (45%) and all AI models (best performance shown by\nGPT-5: 30%). Reliability was substantial for GPT-5 and o3, moderate for Gemini\n2.5 Pro and Grok-4, and poor for Claude Opus 4.1. These findings demonstrate\nthat advanced frontier models fall far short of radiologists in challenging\ndiagnostic cases. Our benchmark highlights the present limitations of\ngeneralist AI in medical imaging and cautions against unsupervised clinical\nuse. We also provide a qualitative analysis of reasoning traces and propose a\npractical taxonomy of visual reasoning errors by AI models for better\nunderstanding their failure modes, informing evaluation standards and guiding\nmore robust model development.", "AI": {"tldr": "Frontier AI models like GPT-5, Gemini 2.5 Pro, and others significantly underperform board-certified radiologists (30% vs 83% accuracy) in challenging medical image diagnosis cases, highlighting limitations of generalist AI for clinical use.", "motivation": "To rigorously evaluate frontier multimodal AI models on difficult diagnostic cases, as most existing evaluations focus on common pathologies in public datasets, creating a gap in understanding AI performance in real-world challenging scenarios.", "method": "Developed a benchmark of 50 expert-level \"spot diagnosis\" cases across multiple imaging modalities, tested five frontier AI models through native web interfaces, scored accuracy by blinded experts, assessed reproducibility across three runs, and analyzed reasoning quality errors.", "result": "Board-certified radiologists achieved 83% accuracy, trainees 45%, while best AI model (GPT-5) only reached 30%. Reliability varied: substantial for GPT-5 and o3, moderate for Gemini 2.5 Pro and Grok-4, poor for Claude Opus 4.1.", "conclusion": "Advanced frontier models fall far short of radiologists in challenging diagnostic cases, highlighting limitations of generalist AI in medical imaging and cautioning against unsupervised clinical use. A taxonomy of visual reasoning errors was proposed to guide better model development."}}
{"id": "2509.25263", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25263", "abs": "https://arxiv.org/abs/2509.25263", "authors": ["Yifang Zhang", "Pengfei Duan", "Henan Wang", "Shengwu Xiong"], "title": "How Effective Are Time-Series Models for Rainfall Nowcasting? A Comprehensive Benchmark for Rainfall Nowcasting Incorporating PWV Data", "comment": "11 pages,8 figures", "summary": "Rainfall nowcasting, which aims to predict precipitation within the next 0 to\n3 hours, is critical for disaster mitigation and real-time response planning.\nHowever, most time series forecasting benchmarks in meteorology are evaluated\non variables with strong periodicity, such as temperature and humidity, which\nfail to reflect model capabilities in more complex and practically meteorology\nscenarios like rainfall nowcasting. To address this gap, we propose\nRainfallBench, a benchmark designed for rainfall nowcasting, a highly\nchallenging and practically relevant task characterized by zero inflation,\ntemporal decay, and non-stationarity, focused on predicting precipitation\nwithin the next 0 to 3 hours. The dataset is derived from five years of\nmeteorological observations, recorded at 15-minute intervals across six\nessential variables, and collected from more than 12,000 GNSS stations\nglobally. In particular, it incorporates precipitable water vapor (PWV), a\ncrucial indicator of rainfall that is absent in other datasets. We further\ndesign specialized evaluation strategies to assess model performance on key\nmeteorological challenges, such as multi-scale prediction and extreme rainfall\nevents, and evaluate over 20 state-of-the-art models across six major\narchitectures on RainfallBench. Additionally, to address the zero-inflation and\ntemporal decay issues overlooked by existing models, we introduce Bi-Focus\nPrecipitation Forecaster (BFPF), a plug-and-play module that incorporates\ndomain-specific priors to enhance rainfall time series forecasting. Statistical\nanalysis and ablation studies validate the comprehensiveness of our dataset as\nwell as the superiority of our methodology. Code and datasets are available at\nhttps://anonymous.4open.science/r/RainfallBench-A710.", "AI": {"tldr": "RainfallBench is a benchmark for rainfall nowcasting (0-3 hour prediction) that addresses gaps in existing meteorological benchmarks by focusing on complex rainfall characteristics like zero inflation, temporal decay, and non-stationarity.", "motivation": "Existing meteorological benchmarks focus on periodic variables like temperature and humidity, failing to capture the complexity of rainfall nowcasting which is critical for disaster mitigation and real-time response.", "method": "Created RainfallBench dataset from 5 years of meteorological data at 15-minute intervals across 12,000+ GNSS stations, including precipitable water vapor. Designed specialized evaluation strategies and introduced Bi-Focus Precipitation Forecaster (BFPF) module to handle zero-inflation and temporal decay.", "result": "Evaluated over 20 state-of-the-art models across six architectures. Statistical analysis and ablation studies validated dataset comprehensiveness and methodology superiority.", "conclusion": "RainfallBench provides a comprehensive benchmark for rainfall nowcasting, addressing key meteorological challenges and demonstrating the effectiveness of domain-specific approaches like BFPF for this complex forecasting task."}}
{"id": "2509.25814", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25814", "abs": "https://arxiv.org/abs/2509.25814", "authors": ["Boyoung Kim", "Dosung Lee", "Sumin An", "Jinseong Jeong", "Paul Hongsuck Seo"], "title": "ReTAG: Retrieval-Enhanced, Topic-Augmented Graph-Based Global Sensemaking", "comment": "9 pages, 5 figures, EMNLP 2025 Findings", "summary": "Recent advances in question answering have led to substantial progress in\ntasks such as multi-hop reasoning. However, global sensemaking-answering\nquestions by synthesizing information from an entire corpus remains a\nsignificant challenge. A prior graph-based approach to global sensemaking lacks\nretrieval mechanisms, topic specificity, and incurs high inference costs. To\naddress these limitations, we propose ReTAG, a Retrieval-Enhanced,\nTopic-Augmented Graph framework that constructs topic-specific subgraphs and\nretrieves the relevant summaries for response generation. Experiments show that\nReTAG improves response quality while significantly reducing inference time\ncompared to the baseline. Our code is available at\nhttps://github.com/bykimby/retag.", "AI": {"tldr": "ReTAG is a retrieval-enhanced, topic-augmented graph framework that improves global sensemaking in question answering by constructing topic-specific subgraphs and retrieving relevant summaries, achieving better response quality with reduced inference time.", "motivation": "Global sensemaking - answering questions by synthesizing information from entire corpora - remains challenging. Previous graph-based approaches lack retrieval mechanisms, topic specificity, and have high inference costs.", "method": "Proposes ReTAG framework that constructs topic-specific subgraphs and retrieves relevant summaries for response generation, addressing limitations of prior graph-based approaches.", "result": "Experiments show ReTAG improves response quality while significantly reducing inference time compared to baseline methods.", "conclusion": "ReTAG effectively addresses global sensemaking challenges by combining retrieval enhancement and topic augmentation in graph frameworks, providing better performance with computational efficiency."}}
{"id": "2509.25562", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25562", "abs": "https://arxiv.org/abs/2509.25562", "authors": ["Yihang Chen", "Yuanhao Ban", "Yunqi Hong", "Cho-Jui Hsieh"], "title": "IRIS: Intrinsic Reward Image Synthesis", "comment": null, "summary": "Despite the success of Reinforcement Learning from Human Feedback (RLHF) in\nlanguage reasoning, its application to autoregressive Text-to-Image (T2I)\ngeneration is often constrained by the limited availability of human preference\ndata. This paper explores how an autoregressive T2I model can learn from\ninternal signals without relying on external rewards or labeled data. Contrary\nto recent findings in text generation, we show that maximizing\nself-uncertainty, rather than self-certainty, improves image generation. We\nobserve that this is because autoregressive T2I models with low uncertainty\ntend to generate simple and uniform images, which are less aligned with human\npreferences. Based on these observations, we propose IRIS (Intrinsic Reward\nImage Synthesis), the first framework to improve autoregressive T2I models with\nreinforcement learning using only an intrinsic reward. Empirical results\ndemonstrate that applying IRIS to autoregressive T2I models achieves\nperformance that is competitive with or superior to external rewards.", "AI": {"tldr": "IRIS is a framework that improves autoregressive text-to-image generation using reinforcement learning with intrinsic rewards, showing that maximizing self-uncertainty rather than certainty leads to better image quality without needing external human preference data.", "motivation": "RLHF is limited by scarce human preference data for autoregressive T2I generation. The paper aims to enable T2I models to learn from internal signals instead of relying on external rewards or labeled data.", "method": "Proposed IRIS framework uses reinforcement learning with intrinsic rewards based on self-uncertainty maximization. Applied to autoregressive T2I models without external human feedback.", "result": "IRIS achieves performance competitive with or superior to external rewards. Models with higher uncertainty generate more diverse and human-preferred images compared to low-uncertainty models that produce simple, uniform images.", "conclusion": "Maximizing self-uncertainty improves autoregressive T2I generation, and intrinsic rewards can effectively replace external human preference data for reinforcement learning in image synthesis."}}
{"id": "2509.25267", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.25267", "abs": "https://arxiv.org/abs/2509.25267", "authors": ["Jiexi Xu"], "title": "Dynamic Policy Induction for Adaptive Prompt Optimization: Bridging the Efficiency-Accuracy Gap via Lightweight Reinforcement Learning", "comment": "13 pages, 2 figures, 2 tables", "summary": "The performance of Large Language Models (LLMs) depends heavily on the chosen\nprompting strategy, yet static approaches such as Zero-Shot, Few-Shot, or\nChain-of-Thought (CoT) impose a rigid efficiency-accuracy trade-off. Highly\naccurate strategies like Self-Consistency (SC) incur substantial computational\nwaste on simple tasks, while lightweight methods often fail on complex inputs.\nThis paper introduces the Prompt Policy Network (PPN), a lightweight\nreinforcement learning framework that formalizes adaptive strategy selection as\na single-step Markov Decision Process (MDP). The PPN, trained with Proximal\nPolicy Optimization (PPO) and guided by a resource-explicit reward function,\nlearns to allocate costly reasoning strategies only when necessary. Experiments\non arithmetic reasoning benchmarks demonstrate that PPN achieves superior\nperformance on the efficiency-accuracy Pareto front, delivering up to 61.5%\ntoken cost reduction compared to Self-Consistency while maintaining competitive\naccuracy. This work contributes a systematic, adaptive framework for\ncost-efficient LLM deployment, advancing the design of lightweight optimization\ntechniques for scalable and sustainable language model applications.", "AI": {"tldr": "PPN is a lightweight RL framework that adaptively selects prompting strategies for LLMs, reducing token costs by 61.5% compared to Self-Consistency while maintaining competitive accuracy.", "motivation": "Static prompting strategies impose rigid efficiency-accuracy trade-offs - accurate methods waste computation on simple tasks, while lightweight methods fail on complex inputs.", "method": "Formalizes adaptive strategy selection as single-step MDP using Prompt Policy Network trained with PPO and resource-explicit reward function.", "result": "Achieves superior performance on efficiency-accuracy Pareto front with 61.5% token cost reduction compared to Self-Consistency while maintaining competitive accuracy.", "conclusion": "Provides systematic adaptive framework for cost-efficient LLM deployment, advancing lightweight optimization for scalable and sustainable language model applications."}}
{"id": "2509.25817", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25817", "abs": "https://arxiv.org/abs/2509.25817", "authors": ["Jaeyoung Kim", "Jongho Lee", "Hongjun Choi", "Sion Jang"], "title": "Personalized Scientific Figure Caption Generation: An Empirical Study on Author-Specific Writing Style Transfer", "comment": null, "summary": "We study personalized figure caption generation using author profile data\nfrom scientific papers. Our experiments demonstrate that rich author profile\ndata, combined with relevant metadata, can significantly improve the\npersonalization performance of multimodal large language models. However, we\nalso reveal a fundamental trade-off between matching author style and\nmaintaining caption quality. Our findings offer valuable insights and future\ndirections for developing practical caption automation systems that balance\nboth objectives. This work was conducted as part of the 3rd SciCap challenge.", "AI": {"tldr": "Personalized figure caption generation using author profile data improves personalization but reveals trade-off between author style matching and caption quality.", "motivation": "To improve personalized figure caption generation by leveraging author profile data from scientific papers, addressing the need for automated caption systems that can capture individual author styles.", "method": "Used author profile data combined with relevant metadata to enhance multimodal large language models for personalized caption generation, conducted as part of the 3rd SciCap challenge.", "result": "Rich author profile data significantly improves personalization performance, but reveals a fundamental trade-off between matching author style and maintaining caption quality.", "conclusion": "Findings provide valuable insights for developing practical caption automation systems that balance both personalization and quality objectives."}}
{"id": "2509.25584", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.25584", "abs": "https://arxiv.org/abs/2509.25584", "authors": ["Max Hartman", "Vidhata Jayaraman", "Moulik Choraria", "Akhil Bhimaraju", "Lav R. Varshney"], "title": "Skip-It? Theoretical Conditions for Layer Skipping in Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) achieve incredible performance across a wide\nrange of tasks, but their large size makes inference costly. Recent work shows\nthat selectively skipping VLM layers can improve efficiency with minimal\nperformance loss or even performance improvements. However, this technique\nremains underused due to the limited understanding of when layer skipping is\nbeneficial. In this paper, we develop a framework that uses information and\nlearning theory to characterize the conditions under which layer skipping\nenhances efficiency without sacrificing performance. Motivated by these\nobservations, we analyze the evolution of the VLM's hidden representations\nthrough the LLM backbone and show that layers with large redundancy as\npredicted by our framework coincide with those skipped by popular\nlayer-skipping methods in practice, providing a unified theoretical scaffolding\nfor multiple efficient inference techniques. Our experiments demonstrate that\nskipping such layers yields faster inference that preserves performance, and\nalso show that applying skipping outside these conditions leads to model\ndegradation.", "AI": {"tldr": "This paper develops a theoretical framework using information and learning theory to determine when layer skipping in vision-language models (VLMs) improves efficiency without performance loss, showing that layers with high redundancy can be safely skipped.", "motivation": "Vision-language models are computationally expensive, and while layer skipping can improve efficiency, there's limited understanding of when this technique is beneficial, leading to underutilization.", "method": "The authors develop an information and learning theory framework to analyze VLM hidden representations, identifying layers with large redundancy that coincide with those skipped by practical layer-skipping methods.", "result": "Experiments show that skipping layers with high redundancy yields faster inference while preserving performance, while skipping outside these conditions leads to model degradation.", "conclusion": "The framework provides a unified theoretical foundation for efficient inference techniques in VLMs, enabling targeted layer skipping that maintains performance while improving computational efficiency."}}
{"id": "2509.25268", "categories": ["cs.LG", "cs.AI", "physics.ao-ph", "68T07"], "pdf": "https://arxiv.org/pdf/2509.25268", "abs": "https://arxiv.org/abs/2509.25268", "authors": ["Cristian Bodnar", "Rapha\u00ebl Rousseau-Rizzi", "Nikhil Shankar", "James Merleau", "Stylianos Flampouris", "Guillem Candille", "Slavica Antic", "Fran\u00e7ois Miralles", "Jayesh K. Gupta"], "title": "A Weather Foundation Model for the Power Grid", "comment": "31 pages, 22 figures", "summary": "Weather foundation models (WFMs) have recently set new benchmarks in global\nforecast skill, yet their concrete value for the weather-sensitive\ninfrastructure that powers modern society remains largely unexplored. In this\nstudy, we fine-tune Silurian AI's 1.5B-parameter WFM, Generative Forecasting\nTransformer (GFT), on a rich archive of Hydro-Qu\\'ebec asset\nobservations--including transmission-line weather stations, wind-farm met-mast\nstreams, and icing sensors--to deliver hyper-local, asset-level forecasts for\nfive grid-critical variables: surface temperature, precipitation, hub-height\nwind speed, wind-turbine icing risk, and rime-ice accretion on overhead\nconductors. Across 6-72 h lead times, the tailored model surpasses\nstate-of-the-art NWP benchmarks, trimming temperature mean absolute error (MAE)\nby 15%, total-precipitation MAE by 35%, and lowering wind speed MAE by 15%.\nMost importantly, it attains an average precision score of 0.72 for day-ahead\nrime-ice detection, a capability absent from existing operational systems,\nwhich affords several hours of actionable warning for potentially catastrophic\noutage events. These results show that WFMs, when post-trained with small\namounts of high-fidelity, can serve as a practical foundation for\nnext-generation grid-resilience intelligence.", "AI": {"tldr": "Fine-tuning a 1.5B-parameter weather foundation model on Hydro-Qu\u00e9bec asset data achieves superior hyper-local forecasts for grid-critical variables, including novel rime-ice detection capability.", "motivation": "Weather foundation models have shown strong global forecast performance but their practical value for weather-sensitive infrastructure like power grids remains unproven.", "method": "Fine-tuned Silurian AI's Generative Forecasting Transformer (GFT) on Hydro-Qu\u00e9bec asset observations including transmission-line weather stations, wind-farm data, and icing sensors.", "result": "Outperformed state-of-the-art NWP benchmarks: 15% reduction in temperature MAE, 35% reduction in precipitation MAE, 15% reduction in wind speed MAE, and achieved 0.72 average precision for day-ahead rime-ice detection.", "conclusion": "Weather foundation models, when post-trained with small amounts of high-fidelity data, can serve as practical foundation for next-generation grid-resilience intelligence."}}
{"id": "2509.25827", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25827", "abs": "https://arxiv.org/abs/2509.25827", "authors": ["Shuyang Jiang", "Yusheng Liao", "Ya Zhang", "Yanfeng Wang", "Yu Wang"], "title": "Overthinking Reduction with Decoupled Rewards and Curriculum Data Scheduling", "comment": "26 pages", "summary": "While large reasoning models trained with critic-free reinforcement learning\nand verifiable rewards (RLVR) represent the state-of-the-art, their practical\nutility is hampered by ``overthinking'', a critical issue where models generate\nexcessively long reasoning paths without any performance benefit. Existing\nsolutions that penalize length often fail, inducing performance degradation due\nto a fundamental misalignment between trajectory-level rewards and token-level\noptimization. In this work, we introduce a novel framework, DECS, built on our\ntheoretical discovery of two previously unaddressed flaws in current length\nrewards: (1) the erroneous penalization of essential exploratory tokens and (2)\nthe inadvertent rewarding of partial redundancy. Our framework's innovations\ninclude (i) a first-of-its-kind decoupled token-level reward mechanism that\nsurgically distinguishes and penalizes redundant tokens, and (ii) a novel\ncurriculum batch scheduling strategy to master the efficiency-efficacy\nequilibrium. Experimental results show DECS can achieve a dramatic reduction in\nreasoning tokens by over 50\\% across seven benchmarks while simultaneously\nmaintaining or even improving performance. It demonstrates conclusively that\nsubstantial gains in reasoning efficiency can be achieved without compromising\na model's underlying reasoning power.", "AI": {"tldr": "DECS framework addresses overthinking in reasoning models by surgically penalizing redundant tokens and using curriculum scheduling, achieving 50%+ token reduction while maintaining performance.", "motivation": "Current RLVR models suffer from overthinking - generating excessively long reasoning paths without performance benefits. Existing length penalty solutions fail due to misalignment between trajectory-level rewards and token-level optimization.", "method": "Introduces DECS framework with: (1) decoupled token-level reward mechanism that distinguishes and penalizes only redundant tokens, (2) curriculum batch scheduling strategy to balance efficiency and efficacy.", "result": "Experimental results show over 50% reduction in reasoning tokens across seven benchmarks while maintaining or improving performance.", "conclusion": "Substantial gains in reasoning efficiency can be achieved without compromising a model's underlying reasoning power."}}
{"id": "2509.25586", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25586", "abs": "https://arxiv.org/abs/2509.25586", "authors": ["Jihye Choi", "Jinsung Yoon", "Jiefeng Chen", "Somesh Jha", "Tomas Pfister"], "title": "ATLAS: Constraints-Aware Multi-Agent Collaboration for Real-World Travel Planning", "comment": null, "summary": "While Large Language Models (LLMs) have shown remarkable advancements in\nreasoning and tool use, they often fail to generate optimal, grounded solutions\nunder complex constraints. Real-world travel planning exemplifies these\nchallenges, evaluating agents' abilities to handle constraints that are\nexplicit, implicit, and even evolving based on interactions with dynamic\nenvironments and user needs. In this paper, we present ATLAS, a general\nmulti-agent framework designed to effectively handle such complex nature of\nconstraints awareness in real-world travel planning tasks. ATLAS introduces a\nprincipled approach to address the fundamental challenges of constraint-aware\nplanning through dedicated mechanisms for dynamic constraint management,\niterative plan critique, and adaptive interleaved search. ATLAS demonstrates\nstate-of-the-art performance on the TravelPlanner benchmark, improving the\nfinal pass rate from 23.3% to 44.4% over its best alternative. More\nimportantly, our work is the first to demonstrate quantitative effectiveness on\nreal-world travel planning tasks with live information search and multi-turn\nfeedback. In this realistic setting, ATLAS showcases its superior overall\nplanning performance, achieving an 84% final pass rate which significantly\noutperforms baselines including ReAct (59%) and a monolithic agent (27%).", "AI": {"tldr": "ATLAS is a multi-agent framework that improves travel planning by handling complex constraints through dynamic management, iterative critique, and adaptive search, achieving state-of-the-art performance.", "motivation": "LLMs often fail to generate optimal solutions under complex constraints in real-world tasks like travel planning, which involve explicit, implicit, and evolving constraints.", "method": "ATLAS uses a principled approach with dynamic constraint management, iterative plan critique, and adaptive interleaved search to handle constraint-aware planning.", "result": "ATLAS improves the final pass rate from 23.3% to 44.4% on TravelPlanner benchmark and achieves 84% final pass rate in realistic settings, outperforming ReAct (59%) and monolithic agent (27%).", "conclusion": "ATLAS demonstrates superior planning performance in real-world travel planning with live information and multi-turn feedback, showing quantitative effectiveness in constraint-aware tasks."}}
{"id": "2509.25270", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25270", "abs": "https://arxiv.org/abs/2509.25270", "authors": ["Liangjian Wen", "Qun Dai", "Jianzhuang Liu", "Jiangtao Zheng", "Yong Dai", "Dongkai Wang", "Zhao Kang", "Jun Wang", "Zenglin Xu", "Jiang Duan"], "title": "InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions", "comment": null, "summary": "In multimodal representation learning, synergistic interactions between\nmodalities not only provide complementary information but also create unique\noutcomes through specific interaction patterns that no single modality could\nachieve alone. Existing methods may struggle to effectively capture the full\nspectrum of synergistic information, leading to suboptimal performance in tasks\nwhere such interactions are critical. This is particularly problematic because\nsynergistic information constitutes the fundamental value proposition of\nmultimodal representation. To address this challenge, we introduce InfMasking,\na contrastive synergistic information extraction method designed to enhance\nsynergistic information through an \\textbf{Inf}inite \\textbf{Masking} strategy.\nInfMasking stochastically occludes most features from each modality during\nfusion, preserving only partial information to create representations with\nvaried synergistic patterns. Unmasked fused representations are then aligned\nwith masked ones through mutual information maximization to encode\ncomprehensive synergistic information. This infinite masking strategy enables\ncapturing richer interactions by exposing the model to diverse partial modality\ncombinations during training. As computing mutual information estimates with\ninfinite masking is computationally prohibitive, we derive an InfMasking loss\nto approximate this calculation. Through controlled experiments, we demonstrate\nthat InfMasking effectively enhances synergistic information between\nmodalities. In evaluations on large-scale real-world datasets, InfMasking\nachieves state-of-the-art performance across seven benchmarks. Code is released\nat https://github.com/brightest66/InfMasking.", "AI": {"tldr": "InfMasking is a contrastive synergistic information extraction method that uses infinite masking strategy to enhance multimodal representation learning by capturing richer interactions between modalities.", "motivation": "Existing multimodal methods struggle to capture the full spectrum of synergistic information between modalities, which is crucial for multimodal representation learning as synergistic interactions create unique outcomes that no single modality can achieve alone.", "method": "InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. It aligns unmasked fused representations with masked ones through mutual information maximization using a derived InfMasking loss to approximate the computationally prohibitive mutual information calculation.", "result": "InfMasking effectively enhances synergistic information between modalities and achieves state-of-the-art performance across seven benchmarks on large-scale real-world datasets.", "conclusion": "The infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training, making InfMasking an effective approach for synergistic information extraction in multimodal representation learning."}}
{"id": "2509.25844", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.25844", "abs": "https://arxiv.org/abs/2509.25844", "authors": ["Keyu He", "Tejas Srinivasan", "Brihi Joshi", "Xiang Ren", "Jesse Thomason", "Swabha Swayamdipta"], "title": "Believing without Seeing: Quality Scores for Contextualizing Vision-Language Model Explanations", "comment": null, "summary": "When people query Vision-Language Models (VLMs) but cannot see the\naccompanying visual context (e.g. for blind and low-vision users), augmenting\nVLM predictions with natural language explanations can signal which model\npredictions are reliable. However, prior work has found that explanations can\neasily convince users that inaccurate VLM predictions are correct. To remedy\nundesirable overreliance on VLM predictions, we propose evaluating two\ncomplementary qualities of VLM-generated explanations via two quality scoring\nfunctions. We propose Visual Fidelity, which captures how faithful an\nexplanation is to the visual context, and Contrastiveness, which captures how\nwell the explanation identifies visual details that distinguish the model's\nprediction from plausible alternatives. On the A-OKVQA and VizWiz tasks, these\nquality scoring functions are better calibrated with model correctness than\nexisting explanation qualities. We conduct a user study in which participants\nhave to decide whether a VLM prediction is accurate without viewing its visual\ncontext. We observe that showing our quality scores alongside VLM explanations\nimproves participants' accuracy at predicting VLM correctness by 11.1%,\nincluding a 15.4% reduction in the rate of falsely believing incorrect\npredictions. These findings highlight the utility of explanation quality scores\nin fostering appropriate reliance on VLM predictions.", "AI": {"tldr": "This paper proposes two quality scoring functions (Visual Fidelity and Contrastiveness) for VLM-generated explanations to help users better assess prediction reliability when visual context is unavailable, reducing overreliance on incorrect predictions.", "motivation": "When users cannot see visual context (e.g., blind/low-vision users), VLM explanations can signal prediction reliability but often lead to overreliance on incorrect predictions.", "method": "Proposed two quality scoring functions: Visual Fidelity (faithfulness to visual context) and Contrastiveness (identifying distinguishing visual details from alternatives). Evaluated on A-OKVQA and VizWiz tasks.", "result": "Quality scores were better calibrated with model correctness than existing methods. User study showed 11.1% improvement in predicting VLM correctness and 15.4% reduction in false belief of incorrect predictions.", "conclusion": "Explanation quality scores effectively foster appropriate reliance on VLM predictions, especially when visual context is unavailable."}}
{"id": "2509.25591", "categories": ["cs.AI", "cs.CL", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2509.25591", "abs": "https://arxiv.org/abs/2509.25591", "authors": ["Zekai Chen", "Arda Pekis", "Kevin Brown"], "title": "Building the EHR Foundation Model via Next Event Prediction", "comment": null, "summary": "Electronic Health Records (EHRs) contain rich temporal dynamics that\nconventional encoding approaches fail to adequately capture. While Large\nLanguage Models (LLMs) show promise for EHR modeling, they struggle to reason\nabout sequential clinical events and temporal dependencies. We propose Next\nEvent Prediction (NEP), a framework that enhances LLMs' temporal reasoning\nthrough autoregressive fine-tuning on clinical event sequences. By\nreformulating EHRs as timestamped event chains and predicting future medical\nevents, NEP explicitly models disease progression patterns and causal\nrelationships. Extensive evaluations across oncology survival prediction and\nclinical diagnosis tasks demonstrate NEP's superiority, outperforming\nspecialized EHR models by 4.6% AUROC and general-purpose LLMs by 7.2% C-index\nin temporal reasoning tasks. Our analyses reveal dual benefits:\nstate-of-the-art prediction accuracy combined with clinically interpretable\nattention patterns that align with known disease pathways.", "AI": {"tldr": "NEP framework enhances LLMs' temporal reasoning for EHRs through autoregressive fine-tuning on clinical event sequences, achieving superior performance in temporal reasoning tasks.", "motivation": "Conventional encoding approaches fail to capture rich temporal dynamics in EHRs, and LLMs struggle with sequential clinical events and temporal dependencies.", "method": "Reformulate EHRs as timestamped event chains and use autoregressive fine-tuning to predict future medical events, explicitly modeling disease progression patterns and causal relationships.", "result": "Outperforms specialized EHR models by 4.6% AUROC and general-purpose LLMs by 7.2% C-index in temporal reasoning tasks, with state-of-the-art prediction accuracy and clinically interpretable attention patterns.", "conclusion": "NEP framework successfully enhances LLMs' temporal reasoning capabilities for EHR modeling, providing both superior prediction performance and clinically meaningful interpretability."}}
{"id": "2509.25278", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25278", "abs": "https://arxiv.org/abs/2509.25278", "authors": ["Payal Mohapatra", "Yueyuan Sui", "Akash Pandey", "Stephen Xia", "Qi Zhu"], "title": "MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series", "comment": "Accepted to Neurips 2025 (Spotlight)", "summary": "From clinical healthcare to daily living, continuous sensor monitoring across\nmultiple modalities has shown great promise for real-world intelligent\ndecision-making but also faces various challenges. In this work, we introduce\nMAESTRO, a novel framework that overcomes key limitations of existing\nmultimodal learning approaches: (1) reliance on a single primary modality for\nalignment, (2) pairwise modeling of modalities, and (3) assumption of complete\nmodality observations. These limitations hinder the applicability of these\napproaches in real-world multimodal time-series settings, where primary\nmodality priors are often unclear, the number of modalities can be large\n(making pairwise modeling impractical), and sensor failures often result in\narbitrary missing observations. At its core, MAESTRO facilitates dynamic intra-\nand cross-modal interactions based on task relevance, and leverages symbolic\ntokenization and adaptive attention budgeting to construct long multimodal\nsequences, which are processed via sparse cross-modal attention. The resulting\ncross-modal tokens are routed through a sparse Mixture-of-Experts (MoE)\nmechanism, enabling black-box specialization under varying modality\ncombinations. We evaluate MAESTRO against 10 baselines on four diverse datasets\nspanning three applications, and observe average relative improvements of 4%\nand 8% over the best existing multimodal and multivariate approaches,\nrespectively, under complete observations. Under partial observations -- with\nup to 40% of missing modalities -- MAESTRO achieves an average 9% improvement.\nFurther analysis also demonstrates the robustness and efficiency of MAESTRO's\nsparse, modality-aware design for learning from dynamic time series.", "AI": {"tldr": "MAESTRO is a novel multimodal learning framework that addresses limitations of existing approaches by enabling dynamic cross-modal interactions, handling arbitrary missing modalities, and using sparse attention mechanisms for efficient processing of long multimodal time series.", "motivation": "Existing multimodal learning approaches have three key limitations: reliance on single primary modality for alignment, pairwise modeling of modalities, and assumption of complete modality observations. These hinder real-world applicability where primary modality priors are unclear, modality counts are large, and sensor failures cause missing data.", "method": "MAESTRO uses symbolic tokenization and adaptive attention budgeting to construct long multimodal sequences, processed via sparse cross-modal attention. It facilitates dynamic intra- and cross-modal interactions based on task relevance, and routes cross-modal tokens through sparse Mixture-of-Experts (MoE) mechanism for black-box specialization under varying modality combinations.", "result": "MAESTRO achieves average relative improvements of 4% over best existing multimodal approaches and 8% over best multivariate approaches under complete observations. Under partial observations with up to 40% missing modalities, it achieves average 9% improvement. The framework demonstrates robustness and efficiency in learning from dynamic time series.", "conclusion": "MAESTRO provides an effective solution for real-world multimodal time-series analysis by overcoming key limitations of existing approaches through its dynamic cross-modal interaction design, ability to handle missing modalities, and efficient sparse processing architecture."}}
{"id": "2509.25868", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25868", "abs": "https://arxiv.org/abs/2509.25868", "authors": ["Yindong Wang", "Martin Prei\u00df", "Margarita Bugue\u00f1o", "Jan Vincent Hoffbauer", "Abdullatif Ghajar", "Tolga Buz", "Gerard de Melo"], "title": "ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations", "comment": null, "summary": "Large Language Models (LLMs) frequently confabulate scientific facts,severely\nundermining their trustworthiness. Addressing this challenge requires\nbenchmarks that go beyond binary factuality and enable fine-grained evaluation.\nWe introduce \\textbf{ReFACT} (\\textit{Reddit False And Correct Texts}), a\nbenchmark of 1,001 expert-annotated question--answer pairs spanning diverse\nscientific domains for the detection of scientific confabulation. Each instance\nincludes both a scientifically correct answer and a non-factual counterpart\nannotated with \\textbf{precise error spans and error-types}. ReFACT enables\nmulti-stage evaluation: (1) confabulation detection, (2) fine-grained error\nlocalization, and (3) correction. We benchmark 9 state-of-the-art LLMs,\nrevealing limited performance ($\\sim$50\\% accuracy). Even top models such as\nGPT-4o fail to distinguish factual from confabulated scientific answers,\nraising concerns about the reliability of \\textit{LLM-as-judge} evaluation\nparadigms. Our findings highlight the need for fine-grained, human-validated\nbenchmarks to detect and correct scientific confabulation in domain-specific\ncontexts. Dataset is released on\n\\href{https://github.com/ddz5431/ReFACT}{GitHub}\\footnote{We provide the\ndataset at: https://github.com/ddz5431/ReFACT}.", "AI": {"tldr": "ReFACT is a benchmark for detecting scientific confabulation in LLMs, featuring 1,001 expert-annotated QA pairs with precise error spans and types, enabling multi-stage evaluation.", "motivation": "LLMs frequently confabulate scientific facts, undermining trustworthiness. Current benchmarks lack fine-grained evaluation beyond binary factuality.", "method": "Created ReFACT benchmark with 1,001 expert-annotated question-answer pairs spanning diverse scientific domains, including both correct answers and non-factual counterparts with precise error annotations.", "result": "Benchmarked 9 state-of-the-art LLMs showing limited performance (~50% accuracy). Even top models like GPT-4o fail to distinguish factual from confabulated scientific answers.", "conclusion": "Highlights need for fine-grained, human-validated benchmarks to detect and correct scientific confabulation in domain-specific contexts, raising concerns about LLM-as-judge evaluation reliability."}}
{"id": "2509.25593", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.25593", "abs": "https://arxiv.org/abs/2509.25593", "authors": ["Akash Kumar Panda", "Olaoluwa Adigun", "Bart Kosko"], "title": "Causal Autoencoder-like Generation of Feedback Fuzzy Cognitive Maps with an LLM Agent", "comment": "8 pages, 4 figures", "summary": "A large language model (LLM) can map a feedback causal fuzzy cognitive map\n(FCM) into text and then reconstruct the FCM from the text. This explainable AI\nsystem approximates an identity map from the FCM to itself and resembles the\noperation of an autoencoder (AE). Both the encoder and the decoder explain\ntheir decisions in contrast to black-box AEs. Humans can read and interpret the\nencoded text in contrast to the hidden variables and synaptic webs in AEs. The\nLLM agent approximates the identity map through a sequence of system\ninstructions that does not compare the output to the input. The reconstruction\nis lossy because it removes weak causal edges or rules while it preserves\nstrong causal edges. The encoder preserves the strong causal edges even when it\ntrades off some details about the FCM to make the text sound more natural.", "AI": {"tldr": "LLM can map FCM to text and reconstruct FCM from text, resembling autoencoder but with explainable decisions.", "motivation": "Create explainable AI system that can convert FCM to interpretable text and back, unlike black-box autoencoders.", "method": "Use LLM as encoder to map FCM to text and as decoder to reconstruct FCM from text, approximating identity map without direct input-output comparison.", "result": "System achieves lossy reconstruction that preserves strong causal edges while removing weak ones, with natural-sounding text output.", "conclusion": "LLM-based FCM-text mapping provides explainable alternative to black-box autoencoders, allowing human interpretation of encoded representations."}}
{"id": "2509.25284", "categories": ["cs.LG", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25284", "abs": "https://arxiv.org/abs/2509.25284", "authors": ["Oluwaseyi Giwa", "Jonathan Shock", "Jaco Du Toit", "Tobi Awodumila"], "title": "Optimisation of Resource Allocation in Heterogeneous Wireless Networks Using Deep Reinforcement Learning", "comment": "Submitted to IEEE Wireless Communications and Networking Conference,\n  2026", "summary": "Dynamic resource allocation in heterogeneous wireless networks (HetNets) is\nchallenging for traditional methods under varying user loads and channel\nconditions. We propose a deep reinforcement learning (DRL) framework that\njointly optimises transmit power, bandwidth, and scheduling via a\nmulti-objective reward balancing throughput, energy efficiency, and fairness.\nUsing real base station coordinates, we compare Proximal Policy Optimisation\n(PPO) and Twin Delayed Deep Deterministic Policy Gradient (TD3) against three\nheuristic algorithms in multiple network scenarios. Our results show that DRL\nframeworks outperform heuristic algorithms in optimising resource allocation in\ndynamic networks. These findings highlight key trade-offs in DRL design for\nfuture HetNets.", "AI": {"tldr": "A DRL framework for dynamic resource allocation in HetNets outperforms heuristic methods by jointly optimizing power, bandwidth, and scheduling with multi-objective rewards.", "motivation": "Traditional methods struggle with dynamic resource allocation in heterogeneous wireless networks under varying conditions.", "method": "Proposed deep reinforcement learning framework using PPO and TD3 algorithms, compared against three heuristic algorithms using real base station coordinates.", "result": "DRL frameworks outperform heuristic algorithms in optimizing resource allocation in dynamic networks.", "conclusion": "DRL shows promise for future HetNets with key design trade-offs identified."}}
{"id": "2509.25878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25878", "abs": "https://arxiv.org/abs/2509.25878", "authors": ["Salsabila Zahirah Pranida", "Muhammad Cendekia Airlangga", "Rifo Ahmad Genadi", "Shady Shehata"], "title": "ASR Under Noise: Exploring Robustness for Sundanese and Javanese", "comment": null, "summary": "We investigate the robustness of Whisper-based automatic speech recognition\n(ASR) models for two major Indonesian regional languages: Javanese and\nSundanese. While recent work has demonstrated strong ASR performance under\nclean conditions, their effectiveness in noisy environments remains unclear. To\naddress this, we experiment with multiple training strategies, including\nsynthetic noise augmentation and SpecAugment, and evaluate performance across a\nrange of signal-to-noise ratios (SNRs). Our results show that noise-aware\ntraining substantially improves robustness, particularly for larger Whisper\nmodels. A detailed error analysis further reveals language-specific challenges,\nhighlighting avenues for future improvements", "AI": {"tldr": "This paper investigates the robustness of Whisper-based ASR models for Indonesian regional languages (Javanese and Sundanese) in noisy environments, showing that noise-aware training significantly improves performance.", "motivation": "While recent work has shown strong ASR performance for Indonesian regional languages under clean conditions, their effectiveness in noisy environments remains unclear and needs investigation.", "method": "The authors experiment with multiple training strategies including synthetic noise augmentation and SpecAugment, and evaluate performance across various signal-to-noise ratios (SNRs).", "result": "Noise-aware training substantially improves robustness, particularly for larger Whisper models. Error analysis reveals language-specific challenges.", "conclusion": "The study demonstrates that noise-aware training enhances ASR robustness for Indonesian regional languages, with error analysis highlighting specific areas for future improvements."}}
{"id": "2509.25598", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25598", "abs": "https://arxiv.org/abs/2509.25598", "authors": ["Peiran Xu", "Zhuohao Li", "Xiaoying Xing", "Guannan Zhang", "Debiao Li", "Kunyu Shi"], "title": "Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on external tools such as\nsearch engines to solve complex agentic tasks that require reasoning and\nexternal knowledge retrieval. Recently, reinforcement learning with verifiable\nrewards (RLVR) has demonstrated its effectiveness in advancing capabilities of\nLLMs by rewarding the final answers via outcome rewards. While straightforward\nto supervise, outcome rewards only provide sparse signals and delayed feedback,\nwhich limits their effectiveness on long trajectories. Process rewards address\nthis by evaluating intermediate steps, providing fine-grained supervision and\nencouraging grounded problem solving. However, it is notoriously hard to\nannotate step-wise labels, especially in non-verifiable process without\n\"golden\" answers. Furthermore, step-wise judgment requires the balance between\nlocal quality with contribution to the final outcome, as optimizing towards\nhigher process reward may not always align with better final outcomes. To\naddress the above challenges, we introduce Principle Process Reward (PPR), an\nRL approach that unifies principled step-level assessment and outcome\nverification. We train a principle-based reward model to improve the\ntransparency and reliability of process evaluation, and further introduce a\nReward Normalization (ReNorm) strategy to calibrate outcome and process\nrewards. Experiment results show that PPR achieves state-of-the-art performance\nacross a wide range of benchmarks, demonstrating its impressive robustness and\ngeneralization. Our code and model collection is available in this link.", "AI": {"tldr": "PPR introduces a reinforcement learning approach that combines principled step-level assessment with outcome verification to address limitations of sparse outcome rewards and hard-to-annotate process rewards in LLM agent tasks.", "motivation": "Current RL approaches for LLMs face challenges: outcome rewards provide only sparse signals and delayed feedback, while process rewards are hard to annotate without golden answers and may not align with final outcomes.", "method": "PPR trains a principle-based reward model for transparent process evaluation and introduces Reward Normalization (ReNorm) to calibrate outcome and process rewards.", "result": "PPR achieves state-of-the-art performance across various benchmarks, demonstrating impressive robustness and generalization.", "conclusion": "The proposed PPR framework effectively addresses the limitations of both outcome and process rewards, providing a unified approach that improves LLM agent performance in complex reasoning tasks."}}
{"id": "2509.25289", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25289", "abs": "https://arxiv.org/abs/2509.25289", "authors": ["Mohammadreza Bakhtyari", "Bogdan Mazoure", "Renato Cordeiro de Amorim", "Guillaume Rabusseau", "Vladimir Makarenkov"], "title": "ClustRecNet: A Novel End-to-End Deep Learning Framework for Clustering Algorithm Recommendation", "comment": null, "summary": "We introduce ClustRecNet - a novel deep learning (DL)-based recommendation\nframework for determining the most suitable clustering algorithms for a given\ndataset, addressing the long-standing challenge of clustering algorithm\nselection in unsupervised learning. To enable supervised learning in this\ncontext, we construct a comprehensive data repository comprising 34,000\nsynthetic datasets with diverse structural properties. Each of them was\nprocessed using 10 popular clustering algorithms. The resulting clusterings\nwere assessed via the Adjusted Rand Index (ARI) to establish ground truth\nlabels, used for training and evaluation of our DL model. The proposed network\narchitecture integrates convolutional, residual, and attention mechanisms to\ncapture both local and global structural patterns from the input data. This\ndesign supports end-to-end training to learn compact representations of\ndatasets and enables direct recommendation of the most suitable clustering\nalgorithm, reducing reliance on handcrafted meta-features and traditional\nCluster Validity Indices (CVIs). Comprehensive experiments across synthetic and\nreal-world benchmarks demonstrate that our DL model consistently outperforms\nconventional CVIs (e.g. Silhouette, Calinski-Harabasz, Davies-Bouldin, and\nDunn) as well as state-of-the-art AutoML clustering recommendation approaches\n(e.g. ML2DAC, AutoCluster, and AutoML4Clust). Notably, the proposed model\nachieves a 0.497 ARI improvement over the Calinski-Harabasz index on synthetic\ndata and a 15.3% ARI gain over the best-performing AutoML approach on\nreal-world data.", "AI": {"tldr": "ClustRecNet is a deep learning framework that automatically recommends the best clustering algorithm for any given dataset, outperforming traditional methods and AutoML approaches.", "motivation": "Address the challenge of clustering algorithm selection in unsupervised learning, which traditionally relies on manual feature engineering and Cluster Validity Indices.", "method": "Built a comprehensive repository of 34,000 synthetic datasets, processed with 10 clustering algorithms and evaluated using Adjusted Rand Index. Developed a DL architecture combining convolutional, residual, and attention mechanisms to learn dataset representations.", "result": "Outperformed conventional CVIs (Silhouette, Calinski-Harabasz, Davies-Bouldin, Dunn) and state-of-the-art AutoML approaches (ML2DAC, AutoCluster, AutoML4Clust). Achieved 0.497 ARI improvement over Calinski-Harabasz on synthetic data and 15.3% ARI gain over best AutoML on real-world data.", "conclusion": "ClustRecNet provides an effective end-to-end solution for clustering algorithm recommendation, reducing reliance on handcrafted features and traditional validation indices while achieving superior performance."}}
{"id": "2509.25897", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.25897", "abs": "https://arxiv.org/abs/2509.25897", "authors": ["Jisu Shin", "Hoyun Song", "Juhyun Oh", "Changgeon Ko", "Eunsu Kim", "Chani Jung", "Alice Oh"], "title": "RoleConflictBench: A Benchmark of Role Conflict Scenarios for Evaluating LLMs' Contextual Sensitivity", "comment": null, "summary": "Humans often encounter role conflicts -- social dilemmas where the\nexpectations of multiple roles clash and cannot be simultaneously fulfilled. As\nlarge language models (LLMs) become increasingly influential in human\ndecision-making, understanding how they behave in complex social situations is\nessential. While previous research has evaluated LLMs' social abilities in\ncontexts with predefined correct answers, role conflicts represent inherently\nambiguous social dilemmas that require contextual sensitivity: the ability to\nrecognize and appropriately weigh situational cues that can fundamentally alter\ndecision priorities. To address this gap, we introduce RoleConflictBench, a\nnovel benchmark designed to evaluate LLMs' contextual sensitivity in complex\nsocial dilemmas. Our benchmark employs a three-stage pipeline to generate over\n13K realistic role conflict scenarios across 65 roles, systematically varying\ntheir associated expectations (i.e., their responsibilities and obligations)\nand situational urgency levels. By analyzing model choices across 10 different\nLLMs, we find that while LLMs show some capacity to respond to these contextual\ncues, this sensitivity is insufficient. Instead, their decisions are\npredominantly governed by a powerful, inherent bias related to social roles\nrather than situational information. Our analysis quantifies these biases,\nrevealing a dominant preference for roles within the Family and Occupation\ndomains, as well as a clear prioritization of male roles and Abrahamic\nreligions across most evaluatee models.", "AI": {"tldr": "This paper introduces RoleConflictBench, a benchmark to evaluate LLMs' contextual sensitivity in role conflict scenarios, finding that LLMs have insufficient sensitivity and are dominated by inherent social role biases.", "motivation": "To understand how LLMs behave in complex social situations like role conflicts, which are inherently ambiguous social dilemmas requiring contextual sensitivity, as LLMs become increasingly influential in human decision-making.", "method": "Developed RoleConflictBench using a three-stage pipeline to generate over 13K realistic role conflict scenarios across 65 roles, systematically varying expectations and situational urgency levels, then analyzed choices across 10 different LLMs.", "result": "LLMs show some capacity to respond to contextual cues but this sensitivity is insufficient. Their decisions are predominantly governed by inherent biases related to social roles rather than situational information, with preferences for Family and Occupation domains, male roles, and Abrahamic religions.", "conclusion": "LLMs exhibit significant biases in handling role conflicts, prioritizing social role stereotypes over contextual situational information, highlighting limitations in their contextual sensitivity for complex social dilemmas."}}
{"id": "2509.25601", "categories": ["cs.AI", "cs.HC", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.25601", "abs": "https://arxiv.org/abs/2509.25601", "authors": ["Flavio Figueiredo", "Giovanni Martinelli", "Henrique Sousa", "Pedro Rodrigues", "Frederico Pedrosa", "Lucas N. Ferreira"], "title": "Echoes of Humanity: Exploring the Perceived Humanness of AI Music", "comment": "Accepted at NeuRIPs 2025 Creative AI Track", "summary": "Recent advances in AI music (AIM) generation services are currently\ntransforming the music industry. Given these advances, understanding how humans\nperceive AIM is crucial both to educate users on identifying AIM songs, and,\nconversely, to improve current models. We present results from a\nlistener-focused experiment aimed at understanding how humans perceive AIM. In\na blind, Turing-like test, participants were asked to distinguish, from a pair,\nthe AIM and human-made song. We contrast with other studies by utilizing a\nrandomized controlled crossover trial that controls for pairwise similarity and\nallows for a causal interpretation. We are also the first study to employ a\nnovel, author-uncontrolled dataset of AIM songs from real-world usage of\ncommercial models (i.e., Suno). We establish that listeners' reliability in\ndistinguishing AIM causally increases when pairs are similar. Lastly, we\nconduct a mixed-methods content analysis of listeners' free-form feedback,\nrevealing a focus on vocal and technical cues in their judgments.", "AI": {"tldr": "This study examines human perception of AI-generated music through a blind Turing test, finding that listeners' ability to distinguish AI music increases when song pairs are similar, with vocal and technical cues being key factors in their judgments.", "motivation": "Understanding human perception of AI music is crucial for educating users about identifying AI-generated songs and improving AI music generation models, especially given recent advances in AI music services transforming the music industry.", "method": "Conducted a blind Turing-like test using a randomized controlled crossover trial with pairwise comparisons, employing a novel dataset of AI music from commercial models (Suno) without author control, and performed mixed-methods content analysis of free-form feedback.", "result": "Listeners' reliability in distinguishing AI music causally increases when pairs are similar, and content analysis revealed that vocal and technical cues are primary factors in listeners' judgments.", "conclusion": "The study provides evidence that human perception of AI music is influenced by pairwise similarity and that vocal/technical characteristics play a significant role in distinguishing AI-generated music from human-made music."}}
{"id": "2509.25300", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25300", "abs": "https://arxiv.org/abs/2509.25300", "authors": ["Zelin Tan", "Hejia Geng", "Mulei Zhang", "Xiaohang Yu", "Guancheng Wan", "Yifan Zhou", "Qiang He", "Xiangyuan Xue", "Heng Zhou", "Yutao Fan", "Zhongzhi Li", "Zaibin Zhang", "Guibin Zhang", "Chen Zhang", "Zhenfei Yin", "Lei Bai"], "title": "Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning", "comment": "V1 version", "summary": "While scaling laws for large language models (LLMs) during pre-training have\nbeen extensively studied, their behavior under reinforcement learning (RL)\npost-training remains largely unexplored. This paper presents a systematic\nempirical investigation of scaling behaviors in RL-based post-training, with a\nparticular focus on mathematical reasoning. Based on 54 experiments across\ndiverse model sizes and training settings, we characterize how model scale,\ndata volume, and computational budget interact to shape performance. Our\nanalysis leads to four key findings: (1). Under a fixed computational budget,\nlarger models trained for fewer steps consistently outperform smaller models\ntrained for more steps. (2). Given a fixed amount of training data, larger\nmodels achieve superior sample efficiency, yielding lower loss. (3). In\ndata-constrained regimes, repeated reuse of high-quality data proves highly\neffective, as final performance is primarily governed by the total number of\noptimization steps rather than the uniqueness of samples. (4). These scaling\nbehaviors are robust across both base and instruction-tuned models, which share\nsimilar learning dynamics (e.g., larger models show faster convergence) even\nwhile differing in absolute accuracy. Collectively, these results provide a\nprincipled foundation and practical guidelines for efficiently scaling the\nreasoning capabilities of LLMs through RL post-training.", "AI": {"tldr": "This paper investigates scaling laws for LLMs during RL post-training, finding that larger models outperform smaller ones under fixed compute budgets, achieve better sample efficiency, benefit from data reuse, and show consistent scaling behaviors across different model types.", "motivation": "While scaling laws for LLM pre-training are well-studied, the scaling behaviors during RL-based post-training remain largely unexplored, particularly for mathematical reasoning tasks.", "method": "Conducted 54 experiments across diverse model sizes and training settings to systematically analyze how model scale, data volume, and computational budget interact during RL post-training.", "result": "Four key findings: (1) Larger models outperform smaller ones under fixed compute; (2) Larger models have better sample efficiency; (3) Data reuse is effective in data-constrained regimes; (4) Scaling behaviors are robust across base and instruction-tuned models.", "conclusion": "The results provide principled foundations and practical guidelines for efficiently scaling LLM reasoning capabilities through RL post-training."}}
{"id": "2509.25903", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25903", "abs": "https://arxiv.org/abs/2509.25903", "authors": ["Dominik Macko", "Andrew Pulver"], "title": "PerQ: Efficient Evaluation of Multilingual Text Personalization Quality", "comment": null, "summary": "Since no metrics are available to evaluate specific aspects of a text, such\nas its personalization quality, the researchers often rely solely on large\nlanguage models to meta-evaluate such texts. Due to internal biases of\nindividual language models, it is recommended to use multiple of them for\ncombined evaluation, which directly increases costs of such meta-evaluation. In\nthis paper, a computationally efficient method for evaluation of\npersonalization quality of a given text (generated by a language model) is\nintroduced, called PerQ. A case study of comparison of generation capabilities\nof large and small language models shows the usability of the proposed metric\nin research, effectively reducing the waste of resources.", "AI": {"tldr": "Proposes PerQ, a computationally efficient method for evaluating personalization quality in generated texts, reducing reliance on multiple expensive LLMs for meta-evaluation.", "motivation": "Current text evaluation lacks specific metrics for personalization quality, forcing researchers to use multiple biased LLMs for meta-evaluation, which increases costs significantly.", "method": "Introduces PerQ, a specialized metric designed to efficiently evaluate the personalization quality of texts generated by language models.", "result": "A case study demonstrates PerQ's effectiveness in comparing generation capabilities of large and small language models, showing practical utility in research.", "conclusion": "PerQ provides a resource-efficient alternative to using multiple LLMs for personalization quality evaluation, effectively reducing computational waste in research."}}
{"id": "2509.25609", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.25609", "abs": "https://arxiv.org/abs/2509.25609", "authors": ["Manuel Cherep", "Chengtian Ma", "Abigail Xu", "Maya Shaked", "Pattie Maes", "Nikhil Singh"], "title": "A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice Experiments", "comment": "23 pages, 13 figures", "summary": "Environments built for people are increasingly operated by a new class of\neconomic actors: LLM-powered software agents making decisions on our behalf.\nThese decisions range from our purchases to travel plans to medical treatment\nselection. Current evaluations of these agents largely focus on task\ncompetence, but we argue for a deeper assessment: how these agents choose when\nfaced with realistic decisions. We introduce ABxLab, a framework for\nsystematically probing agentic choice through controlled manipulations of\noption attributes and persuasive cues. We apply this to a realistic web-based\nshopping environment, where we vary prices, ratings, and psychological nudges,\nall of which are factors long known to shape human choice. We find that agent\ndecisions shift predictably and substantially in response, revealing that\nagents are strongly biased choosers even without being subject to the cognitive\nconstraints that shape human biases. This susceptibility reveals both risk and\nopportunity: risk, because agentic consumers may inherit and amplify human\nbiases; opportunity, because consumer choice provides a powerful testbed for a\nbehavioral science of AI agents, just as it has for the study of human\nbehavior. We release our framework as an open benchmark for rigorous, scalable\nevaluation of agent decision-making.", "AI": {"tldr": "ABxLab framework evaluates LLM-powered agents' decision-making in realistic environments, revealing they exhibit predictable biases similar to humans despite lacking cognitive constraints.", "motivation": "Current agent evaluations focus on task competence, but deeper assessment is needed on how agents make choices when faced with realistic decisions, especially as they increasingly operate in human environments.", "method": "Introduces ABxLab framework with controlled manipulations of option attributes (prices, ratings) and persuasive cues in a web-based shopping environment to systematically probe agentic choice.", "result": "Agent decisions shift predictably and substantially in response to price, rating, and psychological nudge variations, showing agents are strongly biased choosers even without human cognitive constraints.", "conclusion": "Agent susceptibility to biases reveals both risk (amplifying human biases) and opportunity (providing testbed for behavioral science of AI). Framework released as open benchmark for rigorous evaluation of agent decision-making."}}
{"id": "2509.25334", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25334", "abs": "https://arxiv.org/abs/2509.25334", "authors": ["Amirhossein Zare", "Amirhessam Zare", "Parmida Sadat Pezeshki", "Herlock", "Rahimi", "Ali Ebrahimi", "Ignacio V\u00e1zquez-Garc\u00eda", "Leo Anthony Celi"], "title": "Uncertainty-Aware Generative Oversampling Using an Entropy-Guided Conditional Variational Autoencoder", "comment": "16 pages, 2 figures", "summary": "Class imbalance remains a major challenge in machine learning, especially for\nhigh-dimensional biomedical data where nonlinear manifold structures dominate.\nTraditional oversampling methods such as SMOTE rely on local linear\ninterpolation, often producing implausible synthetic samples. Deep generative\nmodels like Conditional Variational Autoencoders (CVAEs) better capture\nnonlinear distributions, but standard variants treat all minority samples\nequally, neglecting the importance of uncertain, boundary-region examples\nemphasized by heuristic methods like Borderline-SMOTE and ADASYN.\n  We propose Local Entropy-Guided Oversampling with a CVAE (LEO-CVAE), a\ngenerative oversampling framework that explicitly incorporates local\nuncertainty into both representation learning and data generation. To quantify\nuncertainty, we compute Shannon entropy over the class distribution in a\nsample's neighborhood: high entropy indicates greater class overlap, serving as\na proxy for uncertainty. LEO-CVAE leverages this signal through two mechanisms:\n(i) a Local Entropy-Weighted Loss (LEWL) that emphasizes robust learning in\nuncertain regions, and (ii) an entropy-guided sampling strategy that\nconcentrates generation in these informative, class-overlapping areas.\n  Applied to clinical genomics datasets (ADNI and TCGA lung cancer), LEO-CVAE\nconsistently improves classifier performance, outperforming both traditional\noversampling and generative baselines. These results highlight the value of\nuncertainty-aware generative oversampling for imbalanced learning in domains\ngoverned by complex nonlinear structures, such as omics data.", "AI": {"tldr": "LEO-CVAE is a generative oversampling method that incorporates local uncertainty (measured by Shannon entropy) into both representation learning and data generation to address class imbalance in high-dimensional biomedical data.", "motivation": "Traditional oversampling methods like SMOTE produce implausible samples, while standard generative models neglect the importance of uncertain boundary-region examples that are crucial for effective classification.", "method": "Proposes Local Entropy-Guided Oversampling with CVAE (LEO-CVAE) using: (i) Local Entropy-Weighted Loss for robust learning in uncertain regions, and (ii) entropy-guided sampling strategy that concentrates generation in class-overlapping areas.", "result": "Applied to clinical genomics datasets (ADNI and TCGA lung cancer), LEO-CVAE consistently improves classifier performance, outperforming both traditional oversampling and generative baselines.", "conclusion": "Uncertainty-aware generative oversampling is valuable for imbalanced learning in domains with complex nonlinear structures like omics data."}}
{"id": "2509.25911", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25911", "abs": "https://arxiv.org/abs/2509.25911", "authors": ["Yu Wang", "Ryuichi Takanobu", "Zhiqi Liang", "Yuzhen Mao", "Yuanzhe Hu", "Julian McAuley", "Xiaojian Wu"], "title": "Mem-\u03b1: Learning Memory Construction via Reinforcement Learning", "comment": null, "summary": "Large language model (LLM) agents are constrained by limited context windows,\nnecessitating external memory systems for long-term information understanding.\nCurrent memory-augmented agents typically depend on pre-defined instructions\nand tools for memory updates. However, language models may lack the ability to\ndetermine which information to store, how to structure it, and when to update\nit, especially as memory systems become more complex. This results in\nsuboptimal memory construction and information loss. To this end, we propose\nMem-alpha, a reinforcement learning framework that trains agents to effectively\nmanage complex memory systems through interaction and feedback. We also\nconstruct a specialized training dataset spanning diverse multi-turn\ninteraction patterns paired with comprehensive evaluation questions designed to\nteach effective memory management. During training, agents process sequential\ninformation chunks, learn to extract and store relevant content, then update\nthe memory system. The reward signal derives from downstream question-answering\naccuracy over the full interaction history, directly optimizing for memory\nconstruction. To illustrate the effectiveness of our training framework, we\ndesign a memory architecture comprising core, episodic, and semantic\ncomponents, equipped with multiple tools for memory operations. Empirical\nevaluation demonstrates that Mem-alpha achieves significant improvements over\nexisting memory-augmented agent baselines. Despite being trained exclusively on\ninstances with a maximum length of 30k tokens, our agents exhibit remarkable\ngeneralization to sequences exceeding 400k tokens, over 13x the training\nlength, highlighting the robustness of Mem-alpha.", "AI": {"tldr": "Mem-alpha is a reinforcement learning framework that trains LLM agents to effectively manage complex memory systems through interaction and feedback, achieving significant improvements over existing methods and demonstrating strong generalization to sequences 13x longer than training data.", "motivation": "Current memory-augmented LLM agents rely on pre-defined instructions and tools for memory updates, but language models struggle to determine what information to store, how to structure it, and when to update it, leading to suboptimal memory construction and information loss.", "method": "Propose Mem-alpha RL framework that trains agents through sequential information processing, learning to extract/store relevant content and update memory systems. Uses specialized training dataset with multi-turn interactions and evaluation questions. Reward signal comes from downstream QA accuracy over full interaction history.", "result": "Mem-alpha achieves significant improvements over existing memory-augmented agent baselines. Agents trained on max 30k token sequences generalize remarkably to sequences exceeding 400k tokens (13x training length), demonstrating framework robustness.", "conclusion": "The Mem-alpha reinforcement learning framework effectively trains agents to manage complex memory systems, overcoming limitations of pre-defined memory instructions and showing strong generalization capabilities to much longer sequences than seen during training."}}
{"id": "2509.25613", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25613", "abs": "https://arxiv.org/abs/2509.25613", "authors": ["Weiqi Wang", "Chenhan Zhang", "Zhiyi Tian", "Shui Yu"], "title": "SMS: Self-supervised Model Seeding for Verification of Machine Unlearning", "comment": null, "summary": "Many machine unlearning methods have been proposed recently to uphold users'\nright to be forgotten. However, offering users verification of their data\nremoval post-unlearning is an important yet under-explored problem. Current\nverifications typically rely on backdooring, i.e., adding backdoored samples to\ninfluence model performance. Nevertheless, the backdoor methods can merely\nestablish a connection between backdoored samples and models but fail to\nconnect the backdoor with genuine samples. Thus, the backdoor removal can only\nconfirm the unlearning of backdoored samples, not users' genuine samples, as\ngenuine samples are independent of backdoored ones. In this paper, we propose a\nSelf-supervised Model Seeding (SMS) scheme to provide unlearning verification\nfor genuine samples. Unlike backdooring, SMS links user-specific seeds (such as\nusers' unique indices), original samples, and models, thereby facilitating the\nverification of unlearning genuine samples. However, implementing SMS for\nunlearning verification presents two significant challenges. First, embedding\nthe seeds into the service model while keeping them secret from the server\nrequires a sophisticated approach. We address this by employing a\nself-supervised model seeding task, which learns the entire sample, including\nthe seeds, into the model's latent space. Second, maintaining the utility of\nthe original service model while ensuring the seeding effect requires a\ndelicate balance. We design a joint-training structure that optimizes both the\nself-supervised model seeding task and the primary service task simultaneously\non the model, thereby maintaining model utility while achieving effective model\nseeding. The effectiveness of the proposed SMS scheme is evaluated through\nextensive experiments, which demonstrate that SMS provides effective\nverification for genuine sample unlearning, addressing existing limitations.", "AI": {"tldr": "Proposes Self-supervised Model Seeding (SMS) scheme for verifying genuine sample unlearning in machine learning models, addressing limitations of backdoor-based verification methods.", "motivation": "Current unlearning verification methods rely on backdooring, which only verifies removal of backdoored samples but not genuine user samples. There's a need for a method that can verify removal of actual user data.", "method": "Uses self-supervised model seeding to link user-specific seeds, original samples, and models. Employs joint-training structure to optimize both seeding task and primary service task simultaneously while keeping seeds secret from the server.", "result": "Extensive experiments demonstrate that SMS provides effective verification for genuine sample unlearning, overcoming limitations of existing backdoor-based methods.", "conclusion": "SMS scheme successfully addresses the verification gap for genuine sample unlearning by establishing direct connections between user seeds, original samples, and models through self-supervised learning."}}
{"id": "2509.25351", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25351", "abs": "https://arxiv.org/abs/2509.25351", "authors": ["Shuang Liang", "Guido Mont\u00fafar"], "title": "Gradient Descent with Large Step Sizes: Chaos and Fractal Convergence Region", "comment": null, "summary": "We examine gradient descent in matrix factorization and show that under large\nstep sizes the parameter space develops a fractal structure. We derive the\nexact critical step size for convergence in scalar-vector factorization and\nshow that near criticality the selected minimizer depends sensitively on the\ninitialization. Moreover, we show that adding regularization amplifies this\nsensitivity, generating a fractal boundary between initializations that\nconverge and those that diverge. The analysis extends to general matrix\nfactorization with orthogonal initialization. Our findings reveal that\nnear-critical step sizes induce a chaotic regime of gradient descent where the\nlong-term dynamics are unpredictable and there are no simple implicit biases,\nsuch as towards balancedness, minimum norm, or flatness.", "AI": {"tldr": "Gradient descent in matrix factorization exhibits fractal structure under large step sizes, with chaotic dynamics near critical step sizes that eliminate implicit biases.", "motivation": "To understand the behavior of gradient descent in matrix factorization, particularly how large step sizes affect convergence and parameter selection.", "method": "Analyze gradient descent in matrix factorization, derive critical step sizes for scalar-vector factorization, examine effects of regularization, and extend to general matrix factorization with orthogonal initialization.", "result": "Large step sizes create fractal parameter structures; near critical step sizes, minimizer selection becomes highly sensitive to initialization; regularization amplifies this sensitivity; chaotic dynamics emerge with unpredictable long-term behavior.", "conclusion": "Near-critical step sizes induce chaotic gradient descent dynamics that eliminate common implicit biases like balancedness, minimum norm, or flatness, revealing complex fractal boundaries in parameter space."}}
{"id": "2509.25913", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25913", "abs": "https://arxiv.org/abs/2509.25913", "authors": ["Chuanyang Zheng", "Jiankai Sun", "Yihang Gao", "Enze Xie", "Yuehao Wang", "Peihao Wang", "Ting Xu", "Matthew Chang", "Liliang Ren", "Jingyao Li", "Jing Xiong", "Kashif Rasul", "Mac Schwager", "Anderson Schneider", "Zhangyang Wang", "Yuriy Nevmyvaka"], "title": "Understanding the Mixture-of-Experts with Nadaraya-Watson Kernel", "comment": "Tech Report", "summary": "Mixture-of-Experts (MoE) has become a cornerstone in recent state-of-the-art\nlarge language models (LLMs). Traditionally, MoE relies on $\\mathrm{Softmax}$\nas the router score function to aggregate expert output, a designed choice that\nhas persisted from the earliest MoE models to modern LLMs, and is now widely\nregarded as standard practice. However, the necessity of using\n$\\mathrm{Softmax}$ to project router weights into a probability simplex remains\nan unchallenged assumption rather than a principled design choice. In this\nwork, we first revisit the classical Nadaraya-Watson regression and observe\nthat MoE shares the same mathematical formulation as Nadaraya-Watson\nregression. Furthermore, we show that both feed-forward neural network (FFN)\nand MoE can be interpreted as a special case of Nadaraya-Watson regression,\nwhere the kernel function corresponds to the input neurons of the output layer.\nMotivated by these insights, we propose the \\textbf{zero-additional-cost}\nKernel Inspired Router with Normalization (KERN), an FFN-style router function,\nas an alternative to $\\mathrm{Softmax}$. We demonstrate that this router\ngeneralizes both $\\mathrm{Sigmoid}$- and $\\mathrm{Softmax}$-based routers.\n\\textbf{Based on empirical observations and established practices in FFN\nimplementation, we recommend the use of $\\mathrm{ReLU}$ activation and\n$\\ell_2$-normalization in $\\mathrm{KERN}$ router function.} Comprehensive\nexperiments in MoE and LLM validate the effectiveness of the proposed FFN-style\nrouter function \\methodNorm.", "AI": {"tldr": "This paper challenges the standard use of Softmax in Mixture-of-Experts (MoE) models by proposing KERN, a novel FFN-style router function based on Nadaraya-Watson regression insights.", "motivation": "The authors question the unchallenged assumption that Softmax is necessary for MoE routers, noting it has become standard practice without principled justification. They aim to find a more theoretically grounded alternative.", "method": "The authors reinterpret MoE through Nadaraya-Watson regression framework, showing both FFN and MoE are special cases. They propose KERN router with ReLU activation and l2-normalization as a zero-cost alternative to Softmax.", "result": "Comprehensive experiments in MoE and LLM settings validate that the proposed FFN-style router function effectively replaces Softmax while generalizing both Sigmoid- and Softmax-based routers.", "conclusion": "The KERN router provides a theoretically motivated, zero-additional-cost alternative to Softmax that works effectively in modern MoE architectures, challenging long-standing assumptions about router design."}}
{"id": "2509.25643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25643", "abs": "https://arxiv.org/abs/2509.25643", "authors": ["Justin Chavarria", "Rohan Raizada", "Justin White", "Eyad Alhetairshi"], "title": "SOCK: A Benchmark for Measuring Self-Replication in Large Language Models", "comment": null, "summary": "We introduce SOCK, a benchmark command line interface (CLI) that measures\nlarge language models' (LLMs) ability to self-replicate without human\nintervention. In this benchmark, self-replication is defined not only as an\nLLM's ability to create a functioning and running copy of itself, but also the\nability for that self-replication to persist and occur across different\ncomputational contexts. Accordingly, we've developed a system to categorize\nLLMs based on broad self-replication capabilities in two general classes,\nReplication-Capability Levels (RCL) and Persistence-Capability Levels (PCL).\nUsing a five-task suite based on practically manipulable modern CLI utilities\nand computer processes, experiments are orchestrated in a controlled\nenvironment with an LLM acting agentically. The performance of the LLM on agent\ntasks is then computed to produce an R-score (a quantitative evaluation of\noverall self-replication ability) and data used to categorize LLMs into\nspecific RCL-PCL matrices. SOCK offers two primary contributions: (1) Provides\nthe first formalized definitions and benchmark suite for evaluating LLM\nself-replication, with the goal of establishing a standard for future research,\nto our knowledge; (2) Allows the industry to track the effectiveness of future\nmulti-agent systems and mitigate potential self-replication threat vectors\nwithin them. The results compiled from evaluating a variety of open-weight and\nproprietary frontier models reveal significant obstacles to persistent\nself-replication and multi-agent systems, including context retention and\nmulti-agent decision-making. We propose future research directions to safely\nreduce the severity of these obstacles, potentially lowering future risk of\nmore functional multi-agent systems.", "AI": {"tldr": "SOCK is a benchmark CLI that evaluates LLMs' self-replication capabilities across computational contexts, categorizing models into Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL) using a five-task suite.", "motivation": "To establish the first formalized benchmark for measuring LLM self-replication abilities, enabling industry tracking of multi-agent systems and mitigating potential self-replication threats.", "method": "Uses a five-task suite based on CLI utilities and computer processes in controlled environments, with LLMs acting agentically. Performance is computed into R-scores and RCL-PCL matrices.", "result": "Evaluation of various models revealed significant obstacles to persistent self-replication, including context retention and multi-agent decision-making challenges.", "conclusion": "SOCK provides standardized evaluation framework for LLM self-replication, identifies current limitations, and proposes future research to safely reduce risks in multi-agent systems."}}
{"id": "2509.25376", "categories": ["cs.LG", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.25376", "abs": "https://arxiv.org/abs/2509.25376", "authors": ["Linus Aronsson", "Han Wu", "Morteza Haghir Chehreghani"], "title": "Cold-Start Active Correlation Clustering", "comment": null, "summary": "We study active correlation clustering where pairwise similarities are not\nprovided upfront and must be queried in a cost-efficient manner through active\nlearning. Specifically, we focus on the cold-start scenario, where no true\ninitial pairwise similarities are available for active learning. To address\nthis challenge, we propose a coverage-aware method that encourages diversity\nearly in the process. We demonstrate the effectiveness of our approach through\nseveral synthetic and real-world experiments.", "AI": {"tldr": "Active correlation clustering with cold-start scenario using coverage-aware method for diversity in active learning.", "motivation": "Address the challenge of active correlation clustering where pairwise similarities are not provided upfront and must be queried cost-efficiently, particularly in cold-start scenarios with no initial pairwise similarities.", "method": "Proposed a coverage-aware method that encourages diversity early in the active learning process to handle the cold-start scenario.", "result": "Demonstrated effectiveness through several synthetic and real-world experiments.", "conclusion": "The coverage-aware approach effectively addresses the cold-start problem in active correlation clustering by promoting diversity in the early stages of active learning."}}
{"id": "2509.25918", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25918", "abs": "https://arxiv.org/abs/2509.25918", "authors": ["Ana Ezquerro", "Carlos G\u00f3mez-Rodr\u00edguez", "David Vilares"], "title": "Bringing Emerging Architectures to Sequence Labeling in NLP", "comment": null, "summary": "Pretrained Transformer encoders are the dominant approach to sequence\nlabeling. While some alternative architectures-such as xLSTMs, structured\nstate-space models, diffusion models, and adversarial learning-have shown\npromise in language modeling, few have been applied to sequence labeling, and\nmostly on flat or simplified tasks. We study how these architectures adapt\nacross tagging tasks that vary in structural complexity, label space, and token\ndependencies, with evaluation spanning multiple languages. We find that the\nstrong performance previously observed in simpler settings does not always\ngeneralize well across languages or datasets, nor does it extend to more\ncomplex structured tasks.", "AI": {"tldr": "The paper evaluates alternative architectures (xLSTMs, SSMs, diffusion models, adversarial learning) for sequence labeling tasks, finding their strong performance in simple settings doesn't generalize well across languages, datasets, or complex structured tasks.", "motivation": "To investigate how alternative architectures beyond dominant Transformer encoders perform on sequence labeling tasks with varying structural complexity, label spaces, and token dependencies across multiple languages.", "method": "Study various architectures including xLSTMs, structured state-space models, diffusion models, and adversarial learning across tagging tasks with different complexity levels, evaluated on multiple languages and datasets.", "result": "The strong performance of alternative architectures observed in simpler settings does not generalize well across languages or datasets, and fails to extend to more complex structured tasks.", "conclusion": "Alternative architectures show promise but have limitations in generalizing across diverse sequence labeling scenarios, suggesting the need for more robust evaluation frameworks beyond simple settings."}}
{"id": "2509.25651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25651", "abs": "https://arxiv.org/abs/2509.25651", "authors": ["Gihan Panapitiya", "Emily Saldanha", "Heather Job", "Olivia Hess"], "title": "AutoLabs: Cognitive Multi-Agent Systems with Self-Correction for Autonomous Chemical Experimentation", "comment": null, "summary": "The automation of chemical research through self-driving laboratories (SDLs)\npromises to accelerate scientific discovery, yet the reliability and granular\nperformance of the underlying AI agents remain critical, under-examined\nchallenges. In this work, we introduce AutoLabs, a self-correcting, multi-agent\narchitecture designed to autonomously translate natural-language instructions\ninto executable protocols for a high-throughput liquid handler. The system\nengages users in dialogue, decomposes experimental goals into discrete tasks\nfor specialized agents, performs tool-assisted stoichiometric calculations, and\niteratively self-corrects its output before generating a hardware-ready file.\nWe present a comprehensive evaluation framework featuring five benchmark\nexperiments of increasing complexity, from simple sample preparation to\nmulti-plate timed syntheses. Through a systematic ablation study of 20 agent\nconfigurations, we assess the impact of reasoning capacity, architectural\ndesign (single- vs. multi-agent), tool use, and self-correction mechanisms. Our\nresults demonstrate that agent reasoning capacity is the most critical factor\nfor success, reducing quantitative errors in chemical amounts (nRMSE) by over\n85% in complex tasks. When combined with a multi-agent architecture and\niterative self-correction, AutoLabs achieves near-expert procedural accuracy\n(F1-score > 0.89) on challenging multi-step syntheses. These findings establish\na clear blueprint for developing robust and trustworthy AI partners for\nautonomous laboratories, highlighting the synergistic effects of modular\ndesign, advanced reasoning, and self-correction to ensure both performance and\nreliability in high-stakes scientific applications. Code:\nhttps://github.com/pnnl/autolabs", "AI": {"tldr": "AutoLabs is a self-correcting multi-agent system that translates natural language instructions into executable protocols for liquid handlers, achieving near-expert accuracy through advanced reasoning and iterative self-correction.", "motivation": "Address the critical but under-examined challenges of AI agent reliability and granular performance in self-driving laboratories for chemical research automation.", "method": "Multi-agent architecture with specialized agents for task decomposition, tool-assisted stoichiometric calculations, and iterative self-correction before generating hardware-ready protocols.", "result": "Agent reasoning capacity reduced quantitative errors by over 85% in complex tasks; combined with multi-agent design and self-correction achieved F1-score > 0.89 on multi-step syntheses.", "conclusion": "Establishes blueprint for robust AI partners in autonomous labs through synergistic effects of modular design, advanced reasoning, and self-correction for performance and reliability."}}
{"id": "2509.25379", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25379", "abs": "https://arxiv.org/abs/2509.25379", "authors": ["Yogesh Verma", "Markus Heinonen", "Vikas Garg"], "title": "Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation", "comment": null, "summary": "Protein structure prediction and folding are fundamental to understanding\nbiology, with recent deep learning advances reshaping the field.\nDiffusion-based generative models have revolutionized protein design, enabling\nthe creation of novel proteins. However, these methods often neglect the\nintrinsic physical realism of proteins, driven by noising dynamics that lack\ngrounding in physical principles. To address this, we first introduce a\nphysically motivated non-linear noising process, grounded in classical physics,\nthat unfolds proteins into secondary structures (e.g., alpha helices, linear\nbeta sheets) while preserving topological integrity--maintaining bonds, and\npreventing collisions. We then integrate this process with the flow-matching\nparadigm on SE(3) to model the invariant distribution of protein backbones with\nhigh fidelity, incorporating sequence information to enable\nsequence-conditioned folding and expand the generative capabilities of our\nmodel. Experimental results demonstrate that the proposed method achieves\nstate-of-the-art performance in unconditional protein generation, producing\nmore designable and novel protein structures while accurately folding monomer\nsequences into precise protein conformations.", "AI": {"tldr": "A novel diffusion-based protein generation method that incorporates physically motivated noising dynamics and flow-matching on SE(3) to produce realistic protein structures with improved designability and novelty.", "motivation": "Current diffusion-based protein design methods lack physical realism due to noising dynamics not grounded in physical principles, leading to unrealistic protein structures.", "method": "Introduces a physically motivated non-linear noising process that unfolds proteins into secondary structures while preserving topological integrity, combined with flow-matching on SE(3) to model protein backbone distributions with sequence conditioning.", "result": "Achieves state-of-the-art performance in unconditional protein generation, producing more designable and novel structures while accurately folding monomer sequences into precise conformations.", "conclusion": "The proposed physically grounded approach significantly improves protein generation quality by incorporating realistic physical principles into the generative process."}}
{"id": "2509.25961", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25961", "abs": "https://arxiv.org/abs/2509.25961", "authors": ["Takumi Goto", "Yusuke Sakai", "Taro Watanabe"], "title": "Reliability Crisis of Reference-free Metrics for Grammatical Error Correction", "comment": "EMNLP 2025 Findings", "summary": "Reference-free evaluation metrics for grammatical error correction (GEC) have\nachieved high correlation with human judgments. However, these metrics are not\ndesigned to evaluate adversarial systems that aim to obtain unjustifiably high\nscores. The existence of such systems undermines the reliability of automatic\nevaluation, as it can mislead users in selecting appropriate GEC systems. In\nthis study, we propose adversarial attack strategies for four reference-free\nmetrics: SOME, Scribendi, IMPARA, and LLM-based metrics, and demonstrate that\nour adversarial systems outperform the current state-of-the-art. These findings\nhighlight the need for more robust evaluation methods.", "AI": {"tldr": "Adversarial attack strategies for reference-free GEC metrics demonstrate vulnerabilities in current evaluation methods.", "motivation": "Current reference-free GEC metrics achieve high correlation with human judgments but are vulnerable to adversarial systems seeking unjustifiably high scores, undermining automatic evaluation reliability.", "method": "Proposed adversarial attack strategies targeting four reference-free metrics: SOME, Scribendi, IMPARA, and LLM-based metrics.", "result": "The adversarial systems outperform current state-of-the-art GEC systems, successfully exploiting metric vulnerabilities.", "conclusion": "Findings highlight the urgent need for more robust evaluation methods in grammatical error correction to prevent misleading system selection."}}
{"id": "2509.25652", "categories": ["cs.AI", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.25652", "abs": "https://arxiv.org/abs/2509.25652", "authors": ["Hailong Zhang", "Yinfeng Yu", "Liejun Wang", "Fuchun Sun", "Wendong Zheng"], "title": "Iterative Residual Cross-Attention Mechanism: An Integrated Approach for Audio-Visual Navigation Tasks", "comment": "Accepted for publication by IEEE International Conference on Systems,\n  Man, and Cybernetics 2025", "summary": "Audio-visual navigation represents a significant area of research in which\nintelligent agents utilize egocentric visual and auditory perceptions to\nidentify audio targets. Conventional navigation methodologies typically adopt a\nstaged modular design, which involves first executing feature fusion, then\nutilizing Gated Recurrent Unit (GRU) modules for sequence modeling, and finally\nmaking decisions through reinforcement learning. While this modular approach\nhas demonstrated effectiveness, it may also lead to redundant information\nprocessing and inconsistencies in information transmission between the various\nmodules during the feature fusion and GRU sequence modeling phases. This paper\npresents IRCAM-AVN (Iterative Residual Cross-Attention Mechanism for\nAudiovisual Navigation), an end-to-end framework that integrates multimodal\ninformation fusion and sequence modeling within a unified IRCAM module, thereby\nreplacing the traditional separate components for fusion and GRU. This\ninnovative mechanism employs a multi-level residual design that concatenates\ninitial multimodal sequences with processed information sequences. This\nmethodological shift progressively optimizes the feature extraction process\nwhile reducing model bias and enhancing the model's stability and\ngeneralization capabilities. Empirical results indicate that intelligent agents\nemploying the iterative residual cross-attention mechanism exhibit superior\nnavigation performance.", "AI": {"tldr": "IRCAM-AVN is an end-to-end framework that integrates multimodal fusion and sequence modeling using iterative residual cross-attention, replacing traditional separate fusion and GRU modules to improve navigation performance.", "motivation": "Traditional audio-visual navigation uses staged modular design with separate feature fusion and GRU sequence modeling, which causes redundant information processing and inconsistencies between modules.", "method": "Proposes IRCAM module that integrates multimodal fusion and sequence modeling using multi-level residual design, concatenating initial multimodal sequences with processed information sequences.", "result": "Intelligent agents using iterative residual cross-attention mechanism show superior navigation performance compared to traditional methods.", "conclusion": "The IRCAM-AVN framework successfully addresses limitations of modular approaches by unifying fusion and sequence modeling, reducing model bias while enhancing stability and generalization capabilities."}}
{"id": "2509.25380", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25380", "abs": "https://arxiv.org/abs/2509.25380", "authors": ["Shane Bergsma", "Nolan Dey", "Joel Hestness"], "title": "Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs", "comment": null, "summary": "Data curriculums have become central to successful LLM training, yet\nprinciples governing optimal data placement remain unclear. We introduce the\n*training re-evaluation curve (TREC)*, a diagnostic that retrospectively\nevaluates training batches *using the final model weights*. The TREC\ncharacterizes how well a trained model retains training data as a function of\n*when* the data was encountered during training. Analyzing TRECs for models\nfrom 111M to 3.9B parameters, we show that placing high-quality data at low\npoints on the TREC significantly improves performance. Importantly, while a\nTREC is initially observable only after training, we demonstrate it can be\n*predicted in advance* from AdamW's implicit EMA coefficients, enabling\nproactive curriculum design. By predicting TRECs for published training\nrecipes, we explain prior ablations and reveal suboptimal data placements. We\nalso align high-quality data with TREC minima in order to improve continual\npre-training of a 3.9B-parameter LLM trained on 900B tokens.", "AI": {"tldr": "The paper introduces TREC (training re-evaluation curve) to analyze data retention during LLM training, shows high-quality data should be placed at TREC minima, and demonstrates TREC can be predicted using AdamW's EMA coefficients for proactive curriculum design.", "motivation": "Current data curriculum methods for LLM training lack clear principles for optimal data placement, making it difficult to design effective training strategies.", "method": "Develop TREC diagnostic to evaluate training batches using final model weights, analyze TRECs across models from 111M to 3.9B parameters, and use AdamW's implicit EMA coefficients to predict TRECs in advance.", "result": "Placing high-quality data at TREC minima improves performance; TREC can be predicted before training; analysis of published recipes reveals suboptimal data placements; improved continual pre-training of 3.9B-parameter LLM.", "conclusion": "TREC provides a principled approach to data curriculum design, enabling proactive optimization of data placement for better LLM training outcomes."}}
{"id": "2509.26011", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26011", "abs": "https://arxiv.org/abs/2509.26011", "authors": ["Andrei C. Coman", "Ionut-Teodor Sorodoc", "Leonardo F. R. Ribeiro", "Bill Byrne", "James Henderson", "Adri\u00e0 de Gispert"], "title": "RAGferee: Building Contextual Reward Models for Retrieval-Augmented Generation", "comment": "Accepted to EMNLP 2025 Main", "summary": "Existing Reward Models (RMs), typically trained on general preference data,\nstruggle in Retrieval Augmented Generation (RAG) settings, which require\njudging responses for faithfulness to retrieved context, relevance to the user\nquery, appropriate refusals when context is insufficient, completeness and\nconciseness of information. To address the lack of publicly available\nRAG-centric preference datasets and specialised RMs, we introduce RAGferee, a\nmethodology that repurposes question-answering (QA) datasets into preference\npairs that prioritise groundedness over stylistic features, enabling the\ntraining of contextual RMs better suited to judging RAG responses. Using\nRAGferee, we curate a small preference dataset of 4K samples and fine-tune RMs\nranging from 7B to 24B parameters. Our RAG-centric RMs achieve state-of-the-art\nperformance on ContextualJudgeBench, surpassing existing 70B+ RMs trained on\nmuch larger (up to 2.4M samples) general corpora, with an absolute improvement\nof +15.5%.", "AI": {"tldr": "RAGferee is a methodology that converts QA datasets into preference pairs focused on groundedness, enabling training of specialized RMs for RAG systems that outperform larger general RMs.", "motivation": "Existing Reward Models trained on general preference data struggle in RAG settings which require judging faithfulness to context, query relevance, appropriate refusals, completeness and conciseness.", "method": "RAGferee repurposes question-answering datasets into preference pairs prioritizing groundedness over stylistic features, then fine-tunes RMs from 7B to 24B parameters on a curated 4K-sample dataset.", "result": "RAG-centric RMs achieve state-of-the-art performance on ContextualJudgeBench, surpassing existing 70B+ RMs trained on much larger (up to 2.4M samples) general corpora with +15.5% absolute improvement.", "conclusion": "Specialized RMs trained on RAG-focused preference data can significantly outperform much larger general-purpose RMs in RAG evaluation tasks."}}
{"id": "2509.25655", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25655", "abs": "https://arxiv.org/abs/2509.25655", "authors": ["Dongsheng Yang", "Meiling Zhu", "Yinfeng Yu"], "title": "Landmark-Guided Knowledge for Vision-and-Language Navigation", "comment": "Accepted for publication by International Conference on Intelligent\n  Computing 2025", "summary": "Vision-and-language navigation is one of the core tasks in embodied\nintelligence, requiring an agent to autonomously navigate in an unfamiliar\nenvironment based on natural language instructions. However, existing methods\noften fail to match instructions with environmental information in complex\nscenarios, one reason being the lack of common-sense reasoning ability. This\npaper proposes a vision-and-language navigation method called Landmark-Guided\nKnowledge (LGK), which introduces an external knowledge base to assist\nnavigation, addressing the misjudgment issues caused by insufficient common\nsense in traditional methods. Specifically, we first construct a knowledge base\ncontaining 630,000 language descriptions and use knowledge Matching to align\nenvironmental subviews with the knowledge base, extracting relevant descriptive\nknowledge. Next, we design a Knowledge-Guided by Landmark (KGL) mechanism,\nwhich guides the agent to focus on the most relevant parts of the knowledge by\nleveraging landmark information in the instructions, thereby reducing the data\nbias that may arise from incorporating external knowledge. Finally, we propose\nKnowledge-Guided Dynamic Augmentation (KGDA), which effectively integrates\nlanguage, knowledge, vision, and historical information. Experimental results\ndemonstrate that the LGK method outperforms existing state-of-the-art methods\non the R2R and REVERIE vision-and-language navigation datasets, particularly in\nterms of navigation error, success rate, and path efficiency.", "AI": {"tldr": "The paper proposes LGK, a vision-and-language navigation method that uses external knowledge base to improve common-sense reasoning, addressing misjudgment issues in complex scenarios.", "motivation": "Existing vision-and-language navigation methods often fail to match instructions with environmental information in complex scenarios due to lack of common-sense reasoning ability.", "method": "LGK method: 1) Constructs 630,000-entry knowledge base and uses knowledge matching to align environmental subviews; 2) Designs Knowledge-Guided by Landmark (KGL) mechanism to focus on relevant knowledge using landmark information; 3) Proposes Knowledge-Guided Dynamic Augmentation (KGDA) to integrate language, knowledge, vision, and historical information.", "result": "LGK method outperforms state-of-the-art methods on R2R and REVERIE datasets, showing improvements in navigation error, success rate, and path efficiency.", "conclusion": "The proposed LGK method effectively addresses common-sense reasoning limitations in vision-and-language navigation by leveraging external knowledge and landmark guidance, achieving superior performance on benchmark datasets."}}
{"id": "2509.25381", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25381", "abs": "https://arxiv.org/abs/2509.25381", "authors": ["Penglei Gao", "Yan Zou", "Abhijit Duggal", "Shuaiqi Huang", "Faming Liang", "Xiaofeng Wang"], "title": "Deep Survival Analysis for Competing Risk Modeling with Functional Covariates and Missing Data Imputation", "comment": null, "summary": "We introduce the Functional Competing Risk Net (FCRN), a unified\ndeep-learning framework for discrete-time survival analysis under competing\nrisks, which seamlessly integrates functional covariates and handles missing\ndata within an end-to-end model. By combining a micro-network Basis Layer for\nfunctional data representation with a gradient-based imputation module, FCRN\nsimultaneously learns to impute missing values and predict event-specific\nhazards. Evaluated on multiple simulated datasets and a real-world ICU case\nstudy using the MIMIC-IV and Cleveland Clinic datasets, FCRN demonstrates\nsubstantial improvements in prediction accuracy over random survival forests\nand traditional competing risks models. This approach advances prognostic\nmodeling in critical care by more effectively capturing dynamic risk factors\nand static predictors while accommodating irregular and incomplete data.", "AI": {"tldr": "FCRN is a deep learning framework for competing risks survival analysis that handles functional covariates and missing data through integrated imputation and hazard prediction.", "motivation": "To improve prognostic modeling in critical care by better capturing dynamic risk factors and static predictors while handling irregular and incomplete data in competing risks scenarios.", "method": "Combines micro-network Basis Layer for functional data representation with gradient-based imputation module to simultaneously learn missing value imputation and predict event-specific hazards.", "result": "Demonstrates substantial improvements in prediction accuracy over random survival forests and traditional competing risks models on simulated datasets and real-world ICU case studies using MIMIC-IV and Cleveland Clinic datasets.", "conclusion": "FCRN advances survival analysis under competing risks by providing a unified framework that effectively integrates functional covariates and handles missing data end-to-end."}}
{"id": "2509.26038", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26038", "abs": "https://arxiv.org/abs/2509.26038", "authors": ["Baoxin Wang", "Yumeng Luo", "Yixuan Wang", "Dayong Wu", "Wanxiang Che", "Shijin Wang"], "title": "RE$^2$: Improving Chinese Grammatical Error Correction via Retrieving Appropriate Examples with Explanation", "comment": null, "summary": "The primary objective of Chinese grammatical error correction (CGEC) is to\ndetect and correct errors in Chinese sentences. Recent research shows that\nlarge language models (LLMs) have been applied to CGEC with significant\nresults. For LLMs, selecting appropriate reference examples can help improve\ntheir performance. However, existing methods predominantly rely on text\nsimilarity for example retrieval, a strategy that frequently mismatches actual\nerror patterns and retrieves lexically similar yet grammatically irrelevant\nsentences. To address this problem, we propose a method named RE$^2$, which\nretrieves appropriate examples with explanations of grammatical errors. Instead\nof using text similarity of the input sentence, we use explanations of\ngrammatical errors to select reference examples, which are used by LLMs to\nimprove the performance of CGEC. We conduct experiments on two CGEC datasets\nand create a high-quality grammatical error explanation (GEE) dataset, which is\nnot only used in our research but also serves as a valuable resource for future\nstudies in both CGEC and GEE. The experimental results on the two datasets\nindicate that our proposed method effectively improves the performance of CGEC.", "AI": {"tldr": "RE\u00b2 method improves Chinese grammatical error correction by using grammatical error explanations instead of text similarity for example retrieval, achieving better performance on CGEC datasets.", "motivation": "Existing methods rely on text similarity for example retrieval, which often mismatches actual error patterns and retrieves lexically similar but grammatically irrelevant sentences.", "method": "Proposed RE\u00b2 method retrieves appropriate examples with explanations of grammatical errors, using error explanations rather than text similarity to select reference examples for LLMs.", "result": "Experimental results on two CGEC datasets show that the proposed method effectively improves CGEC performance. Created a high-quality grammatical error explanation dataset for future research.", "conclusion": "Using grammatical error explanations for example retrieval is more effective than text similarity for improving Chinese grammatical error correction performance with LLMs."}}
{"id": "2509.25662", "categories": ["cs.AI", "cs.SC"], "pdf": "https://arxiv.org/pdf/2509.25662", "abs": "https://arxiv.org/abs/2509.25662", "authors": ["Belona Sonna", "Alban Grastien"], "title": "On Explaining Proxy Discrimination and Unfairness in Individual Decisions Made by AI Systems", "comment": "Accepted at AJCAI 2025", "summary": "Artificial intelligence (AI) systems in high-stakes domains raise concerns\nabout proxy discrimination, unfairness, and explainability. Existing audits\noften fail to reveal why unfairness arises, particularly when rooted in\nstructural bias. We propose a novel framework using formal abductive\nexplanations to explain proxy discrimination in individual AI decisions.\nLeveraging background knowledge, our method identifies which features act as\nunjustified proxies for protected attributes, revealing hidden structural\nbiases. Central to our approach is the concept of aptitude, a task-relevant\nproperty independent of group membership, with a mapping function aligning\nindividuals of equivalent aptitude across groups to assess fairness\nsubstantively. As a proof of concept, we showcase the framework with examples\ntaken from the German credit dataset, demonstrating its applicability in\nreal-world cases.", "AI": {"tldr": "A framework using formal abductive explanations to identify proxy discrimination in AI decisions, revealing hidden structural biases by analyzing which features act as unjustified proxies for protected attributes.", "motivation": "AI systems in high-stakes domains raise concerns about proxy discrimination, unfairness, and explainability. Existing audits often fail to reveal why unfairness arises, particularly when rooted in structural bias.", "method": "Leveraging background knowledge and the concept of aptitude (task-relevant property independent of group membership), the method uses formal abductive explanations to identify unjustified proxy features. A mapping function aligns individuals of equivalent aptitude across groups to assess fairness substantively.", "result": "The framework is demonstrated as a proof of concept using examples from the German credit dataset, showing its applicability in real-world cases.", "conclusion": "The proposed framework effectively explains proxy discrimination in individual AI decisions by revealing hidden structural biases and identifying unjustified proxy features, providing a substantive approach to fairness assessment."}}
{"id": "2509.25382", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25382", "abs": "https://arxiv.org/abs/2509.25382", "authors": ["Fernanda Zapata Bascu\u00f1\u00e1n"], "title": "On the Shape of Latent Variables in a Denoising VAE-MoG: A Posterior Sampling-Based Study", "comment": "Argentine Congress of Embedded Systems (2025)", "summary": "In this work, we explore the latent space of a denoising variational\nautoencoder with a mixture-of-Gaussians prior (VAE-MoG), trained on\ngravitational wave data from event GW150914. To evaluate how well the model\ncaptures the underlying structure, we use Hamiltonian Monte Carlo (HMC) to draw\nposterior samples conditioned on clean inputs, and compare them to the\nencoder's outputs from noisy data. Although the model reconstructs signals\naccurately, statistical comparisons reveal a clear mismatch in the latent\nspace. This shows that strong denoising performance doesn't necessarily mean\nthe latent representations are reliable highlighting the importance of using\nposterior-based validation when evaluating generative models.", "AI": {"tldr": "The paper analyzes a VAE-MoG model trained on GW150914 gravitational wave data, finding that while signal reconstruction is accurate, latent space representations show statistical mismatches when validated with Hamiltonian Monte Carlo.", "motivation": "To evaluate whether strong denoising performance in variational autoencoders with mixture-of-Gaussians priors necessarily indicates reliable latent representations, particularly in gravitational wave data analysis.", "method": "Used Hamiltonian Monte Carlo (HMC) to draw posterior samples from a denoising VAE-MoG model trained on GW150914 data, comparing these with encoder outputs from noisy data.", "result": "The model accurately reconstructed gravitational wave signals, but statistical comparisons revealed clear mismatches in the latent space representations.", "conclusion": "Strong denoising performance doesn't guarantee reliable latent representations, highlighting the importance of posterior-based validation methods for evaluating generative models."}}
{"id": "2509.26041", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26041", "abs": "https://arxiv.org/abs/2509.26041", "authors": ["Arash Marioriyad", "Shaygan Adim", "Nima Alighardashi", "Mahdieh Soleymani Banghshah", "Mohammad Hossein Rohban"], "title": "Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning", "comment": "5 Pages, 4 Figures, 4 Tables", "summary": "Large language models (LLMs) increasingly rely on chain-of-thought (CoT)\nprompting to solve mathematical and logical reasoning tasks. Yet, a central\nquestion remains: to what extent are these generated rationales \\emph{faithful}\nto the underlying computations, rather than post-hoc narratives shaped by hints\nthat function as answer shortcuts embedded in the prompt? Following prior work\non hinted vs.\\ unhinted prompting, we present a systematic study of CoT\nfaithfulness under controlled hint manipulations. Our experimental design spans\nfour datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models\n(GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in\ncorrectness (correct and incorrect), presentation style (sycophancy and data\nleak), and complexity (raw answers, two-operator expressions, four-operator\nexpressions). We evaluate both task accuracy and whether hints are explicitly\nacknowledged in the reasoning. Our results reveal three key findings. First,\ncorrect hints substantially improve accuracy, especially on harder benchmarks\nand logical reasoning, while incorrect hints sharply reduce accuracy in tasks\nwith lower baseline competence. Second, acknowledgement of hints is highly\nuneven: equation-based hints are frequently referenced, whereas raw hints are\noften adopted silently, indicating that more complex hints push models toward\nverbalizing their reliance in the reasoning process. Third, presentation style\nmatters: sycophancy prompts encourage overt acknowledgement, while leak-style\nprompts increase accuracy but promote hidden reliance. This may reflect\nRLHF-related effects, as sycophancy exploits the human-pleasing side and data\nleak triggers the self-censoring side. Together, these results demonstrate that\nLLM reasoning is systematically shaped by shortcuts in ways that obscure\nfaithfulness.", "AI": {"tldr": "LLM reasoning faithfulness is systematically influenced by hint shortcuts, with correct hints improving accuracy and complex hints increasing verbal acknowledgment, while presentation styles affect overt vs hidden reliance.", "motivation": "To investigate whether LLM-generated rationales are faithful computations or post-hoc narratives shaped by embedded hint shortcuts in CoT prompting.", "method": "Systematic study across 4 datasets (AIME, GSM-Hard, MATH-500, UniADILR), 2 models (GPT-4o, Gemini-2-Flash), with controlled hint manipulations varying in correctness, presentation style, and complexity.", "result": "Correct hints substantially improve accuracy, especially on harder tasks; hint acknowledgment is uneven (complex hints get more verbalization); presentation style affects reliance patterns (sycophancy encourages overt acknowledgment, data leak promotes hidden reliance).", "conclusion": "LLM reasoning is systematically shaped by shortcuts that obscure faithfulness, with RLHF-related effects influencing how hints are acknowledged and utilized."}}
{"id": "2509.25669", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25669", "abs": "https://arxiv.org/abs/2509.25669", "authors": ["Xinxi Chen", "Tianyang Chen", "Lijia Hong"], "title": "GroundSight: Augmenting Vision-Language Models with Grounding Information and De-hallucination", "comment": null, "summary": "We propose a method to improve Visual Question Answering (VQA) with\nRetrieval-Augmented Generation (RAG) by introducing text-grounded object\nlocalization. Rather than retrieving information based on the entire image, our\napproach enables the model to generate a bounding box around the object most\nrelevant to the question, allowing for targeted image cropping and focused\nretrieval. This reduces background noise, improves alignment between visual and\ntextual cues, and helps mitigate hallucinations. Our RAG method enhances\ncontext-aware VQA responses increased the accuracy from 22.19% to 25.64%, with\nan absolute increase of 3.45 percentage points, compared to the baseline\nLlama-3.2-Vision-11B agent. We also proposed a de-hallucination method based on\nquestion type which can effectively reduce the hallucination rate from 65.79%\nto 13.88% and improves the truthfulness score.", "AI": {"tldr": "Proposes text-grounded object localization for VQA with RAG, enabling targeted image cropping to reduce noise and improve accuracy by 3.45 percentage points, plus a de-hallucination method reducing hallucination rate from 65.79% to 13.88%.", "motivation": "To improve VQA by reducing background noise and improving alignment between visual and textual cues through targeted retrieval, while mitigating hallucinations in responses.", "method": "Introduces text-grounded object localization to generate bounding boxes around question-relevant objects, enabling focused image cropping and retrieval in RAG framework, plus question type-based de-hallucination method.", "result": "Increased VQA accuracy from 22.19% to 25.64% (3.45% absolute improvement) over Llama-3.2-Vision-11B baseline, and reduced hallucination rate from 65.79% to 13.88% with improved truthfulness.", "conclusion": "Text-grounded object localization with RAG effectively improves VQA performance by enabling focused retrieval and reducing hallucinations through targeted image processing and question-aware de-hallucination."}}
{"id": "2509.25395", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.25395", "abs": "https://arxiv.org/abs/2509.25395", "authors": ["Jordyn E. A. Lorentz", "Katharine M. Clark"], "title": "Crowdsourcing Without People: Modelling Clustering Algorithms as Experts", "comment": null, "summary": "This paper introduces mixsemble, an ensemble method that adapts the\nDawid-Skene model to aggregate predictions from multiple model-based clustering\nalgorithms. Unlike traditional crowdsourcing, which relies on human labels, the\nframework models the outputs of clustering algorithms as noisy annotations.\nExperiments on both simulated and real-world datasets show that, although the\nmixsemble is not always the single top performer, it consistently approaches\nthe best result and avoids poor outcomes. This robustness makes it a practical\nalternative when the true data structure is unknown, especially for non-expert\nusers.", "AI": {"tldr": "Mixsemble is an ensemble method that adapts the Dawid-Skene model to aggregate predictions from multiple clustering algorithms, treating algorithm outputs as noisy annotations.", "motivation": "To create a robust clustering ensemble method that can handle unknown data structures and provide reliable results for non-expert users, avoiding the need for human labels like traditional crowdsourcing.", "method": "Adapts the Dawid-Skene model to aggregate predictions from multiple model-based clustering algorithms, treating algorithm outputs as noisy annotations rather than relying on human labels.", "result": "Experiments on simulated and real-world datasets show mixsemble consistently approaches the best performance and avoids poor outcomes, though it's not always the single top performer.", "conclusion": "Mixsemble provides robust and practical clustering performance when the true data structure is unknown, making it particularly suitable for non-expert users."}}
{"id": "2509.26048", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26048", "abs": "https://arxiv.org/abs/2509.26048", "authors": ["Daocheng Fu", "Jianbiao Mei", "Licheng Wen", "Xuemeng Yang", "Cheng Yang", "Rong Wu", "Tao Hu", "Siqi Li", "Yufan Shen", "Xinyu Cai", "Pinlong Cai", "Botian Shi", "Yong Liu", "Yu Qiao"], "title": "RE-Searcher: Robust Agentic Search with Goal-oriented Planning and Self-reflection", "comment": "16 pages, 7 figures", "summary": "Large language models (LLMs) excel at knowledge-intensive question answering\nand reasoning, yet their real-world deployment remains constrained by knowledge\ncutoff, hallucination, and limited interaction modalities. Augmenting LLMs with\nexternal search tools helps alleviate these issues, but it also exposes agents\nto a complex search environment in which small, plausible variations in query\nformulation can steer reasoning into unproductive trajectories and amplify\nerrors. We present a systematic analysis that quantifies how environmental\ncomplexity induces fragile search behaviors and, in turn, degrades overall\nperformance. To address this challenge, we propose a simple yet effective\napproach to instantiate a search agent, RE-Searcher. During search, RE-Searcher\nexplicitly articulates a concrete search goal and subsequently reflects on\nwhether the retrieved evidence satisfies that goal. This combination of\ngoal-oriented planning and self-reflection enables RE-Searcher to resist\nspurious cues in complex search environments and perform robust search.\nExtensive experiments show that our method improves search accuracy and\nachieves state-of-the-art results. Perturbation studies further demonstrate\nsubstantial resilience to noisy or misleading external signals, mitigating the\nfragility of the search process. We believe these findings offer practical\nguidance for integrating LLM-powered agents into more complex interactive\nenvironments and enabling more autonomous decision-making.", "AI": {"tldr": "RE-Searcher is a search agent that combines goal-oriented planning and self-reflection to improve search robustness in complex environments, achieving state-of-the-art performance.", "motivation": "LLMs face challenges in real-world deployment due to knowledge cutoff, hallucination, and limited interaction modalities. While external search tools help, they expose agents to complex search environments where small query variations can lead to unproductive reasoning and amplify errors.", "method": "RE-Searcher explicitly articulates concrete search goals and reflects on whether retrieved evidence satisfies those goals. This goal-oriented planning with self-reflection helps resist spurious cues in complex search environments.", "result": "Extensive experiments show improved search accuracy and state-of-the-art results. Perturbation studies demonstrate substantial resilience to noisy or misleading external signals, mitigating search process fragility.", "conclusion": "The findings offer practical guidance for integrating LLM-powered agents into complex interactive environments and enabling more autonomous decision-making through robust search mechanisms."}}
{"id": "2509.25672", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.25672", "abs": "https://arxiv.org/abs/2509.25672", "authors": ["Hasan Alp Cafero\u011flu", "Mehmet Serhat \u00c7elik", "\u00d6zg\u00fcr Ulusoy"], "title": "SING-SQL: A Synthetic Data Generation Framework for In-Domain Text-to-SQL Translation", "comment": null, "summary": "Translating natural language questions into SQL has become a core challenge\nin enabling non-technical users to query databases. While recent work has\nexplored large-scale synthetic data generation to improve model performance\nthrough post-training, most efforts emphasize cross-domain generalization. This\nleaves a gap for real-world enterprise scenarios, where models need to\nspecialize to a single database schema and organizations require to be able to\nevaluate their Text-to-SQL systems on their own databases. To address this, we\nintroduce SING-SQL, a fully automated two-stage framework for generating\nhigh-quality, high-coverage synthetic Text-to-SQL data for any target database,\nwithout relying on SQL logs or manual annotations. Our approach hierarchically\npartitions a database schema into sub-schemas, synthesizes SQL queries across\nmultiple complexity levels, and applies a quality-aware pipeline that includes\nLLM-as-a-judge validation, executability checks, automatic repair, and column\nbalancing. We further release SingSQL-LM, a family of compact language models\nfine-tuned on the synthetic data, achieving strong in-domain generalization. On\nthe subset of the BIRD benchmark, SingSQL-LM-3B-R64 reaches 82.87% Soft F1 and\n73.03% EX upper bound with 32 candidates, outperforming the best 3B-scale\nbaseline by +16.21 in Soft F1 and +12.36 in EX. At the 1.5B scale,\nSingSQL-LM-1.5B-R64 improves over prior systems by +9.30 in Soft F1 and +4.49\nin EX. On synthetic evaluation sets, SingSQL-LMs exceed prior systems by wide\nmargins, establishing state-of-the-art performance among open models at\ncomparable scales. Our study of context management strategies reveals that\nschema-free fine-tuning combined with schema-only inference provides the most\nrobust results. These findings establish SING-SQL as a scalable,\ndatabase-agnostic paradigm for producing and evaluating enterprise-grade\nText-to-SQL systems.", "AI": {"tldr": "SING-SQL is an automated framework for generating synthetic Text-to-SQL data for any database, enabling specialized models for enterprise scenarios without manual annotations.", "motivation": "Address the gap in real-world enterprise scenarios where models need to specialize to single database schemas and organizations require evaluation on their own databases, rather than focusing only on cross-domain generalization.", "method": "Two-stage framework: hierarchically partitions database schema into sub-schemas, synthesizes SQL queries across complexity levels, and applies quality-aware pipeline with LLM-as-a-judge validation, executability checks, automatic repair, and column balancing.", "result": "SingSQL-LM models achieve state-of-the-art performance: 3B model reaches 82.87% Soft F1 and 73.03% EX on BIRD benchmark, outperforming best 3B baseline by +16.21 Soft F1 and +12.36 EX. Schema-free fine-tuning with schema-only inference provides most robust results.", "conclusion": "SING-SQL establishes a scalable, database-agnostic paradigm for producing and evaluating enterprise-grade Text-to-SQL systems, enabling organizations to create specialized models for their databases without manual effort."}}
{"id": "2509.25400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25400", "abs": "https://arxiv.org/abs/2509.25400", "authors": ["S C Bee", "N Dervilis", "K Worden", "L A Bull"], "title": "Multi-Task Equation Discovery", "comment": null, "summary": "Equation discovery provides a grey-box approach to system identification by\nuncovering governing dynamics directly from observed data. However, a\npersistent challenge lies in ensuring that identified models generalise across\noperating conditions rather than over-fitting to specific datasets. This work\ninvestigates this issue by applying a Bayesian relevance vector machine (RVM)\nwithin a multi-task learning (MTL) framework for simultaneous parameter\nidentification across multiple datasets. In this formulation, responses from\nthe same structure under different excitation levels are treated as related\ntasks that share model parameters but retain task-specific noise\ncharacteristics. A simulated single degree-of-freedom oscillator with linear\nand cubic stiffness provided the case study, with datasets generated under\nthree excitation regimes. Standard single-task RVM models were able to\nreproduce system responses but often failed to recover the true governing terms\nwhen excitations insufficiently stimulated non-linear dynamics. By contrast,\nthe MTL-RVM combined information across tasks, improving parameter recovery for\nweakly and moderately excited datasets, while maintaining strong performance\nunder high excitation. These findings demonstrate that multi-task Bayesian\ninference can mitigate over-fitting and promote generalisation in equation\ndiscovery. The approach is particularly relevant to structural health\nmonitoring, where varying load conditions reveal complementary aspects of\nsystem physics.", "AI": {"tldr": "Multi-task Bayesian inference using RVM improves equation discovery by sharing parameters across datasets, enhancing generalization and mitigating over-fitting.", "motivation": "Address the challenge of ensuring identified models generalize across operating conditions rather than over-fitting to specific datasets in equation discovery.", "method": "Apply Bayesian relevance vector machine (RVM) within multi-task learning framework for simultaneous parameter identification across multiple datasets, treating responses under different excitation levels as related tasks.", "result": "MTL-RVM improved parameter recovery for weakly and moderately excited datasets while maintaining strong performance under high excitation, outperforming single-task RVM models.", "conclusion": "Multi-task Bayesian inference can mitigate over-fitting and promote generalization in equation discovery, particularly relevant for structural health monitoring with varying load conditions."}}
{"id": "2509.26051", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26051", "abs": "https://arxiv.org/abs/2509.26051", "authors": ["Dominik Macko", "Jakub Kopal"], "title": "CEAID: Benchmark of Multilingual Machine-Generated Text Detection Methods for Central European Languages", "comment": null, "summary": "Machine-generated text detection, as an important task, is predominantly\nfocused on English in research. This makes the existing detectors almost\nunusable for non-English languages, relying purely on cross-lingual\ntransferability. There exist only a few works focused on any of Central\nEuropean languages, leaving the transferability towards these languages rather\nunexplored. We fill this gap by providing the first benchmark of detection\nmethods focused on this region, while also providing comparison of\ntrain-languages combinations to identify the best performing ones. We focus on\nmulti-domain, multi-generator, and multilingual evaluation, pinpointing the\ndifferences of individual aspects, as well as adversarial robustness of\ndetection methods. Supervised finetuned detectors in the Central European\nlanguages are found the most performant in these languages as well as the most\nresistant against obfuscation.", "AI": {"tldr": "First benchmark of machine-generated text detection methods for Central European languages, showing supervised finetuned detectors perform best and are most robust against obfuscation.", "motivation": "Existing detectors focus mainly on English, leaving Central European languages unexplored with limited cross-lingual transferability.", "method": "Multi-domain, multi-generator, multilingual evaluation comparing train-language combinations and testing adversarial robustness.", "result": "Supervised finetuned detectors in Central European languages are most performant and resistant to obfuscation.", "conclusion": "Language-specific finetuning is crucial for effective machine-generated text detection in Central European languages."}}
{"id": "2509.25689", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25689", "abs": "https://arxiv.org/abs/2509.25689", "authors": ["Yixiao Chen", "Yanyue Xie", "Ruining Yang", "Wei Jiang", "Wei Wang", "Yong He", "Yue Chen", "Pu Zhao", "Yanzhi Wang"], "title": "Collaborative Compression for Large-Scale MoE Deployment on Edge", "comment": null, "summary": "The Mixture of Experts (MoE) architecture is an important method for scaling\nLarge Language Models (LLMs). It increases model capacity while keeping\ncomputation cost low. However, the ultra-large MoE models still have hundreds\nof billions of parameters, requiring massive memory/storage and leading to\ndifficulties for deployment on resource-constrained edge platforms. Pruning or\nquantization alone can hardly address the issue, because of the\nsuper-aggressive compression ratio with significantly degraded accuracy and\noutput quality. To facilitate the deployment of ultra-large MoEs on edge\nplatforms, we propose a collaborative compression framework by combining expert\npruning, mixed-precision quantization, and activation optimization. It can\neffectively reduce the storage footprint of the ultra-large MoE DeepSeek-V3\nfrom 1.3TB to 103GB, while preserving high output quality with better accuracy\nthan traditional uniform low-bit quantization methods. To the best of our\nknowledge, we are the first to deploy a compressed model from the ultra-large\nDeepSeek-V3 on the platform with a strict 128GB total memory limit. Our\ncomprehensive experiments on multiple benchmarks under various memory\nconstraints demonstrate the effectiveness of our method with smaller model\nsizes and higher accuracy than uniform low-bit quantization methods.", "AI": {"tldr": "Proposes a collaborative compression framework combining expert pruning, mixed-precision quantization, and activation optimization to deploy ultra-large Mixture of Experts models on edge platforms, reducing DeepSeek-V3 from 1.3TB to 103GB while maintaining accuracy.", "motivation": "Ultra-large MoE models have hundreds of billions of parameters requiring massive memory/storage, making deployment on resource-constrained edge platforms difficult. Pruning or quantization alone cannot achieve the required aggressive compression ratios without significant accuracy degradation.", "method": "Collaborative compression framework combining three techniques: expert pruning, mixed-precision quantization, and activation optimization to compress ultra-large MoE models.", "result": "Successfully reduced DeepSeek-V3 model from 1.3TB to 103GB, achieving deployment on platforms with 128GB memory limit while preserving output quality and achieving better accuracy than uniform low-bit quantization methods.", "conclusion": "The proposed collaborative compression framework enables effective deployment of ultra-large MoE models on edge platforms with strict memory constraints, outperforming traditional uniform quantization methods in both model size reduction and accuracy preservation."}}
{"id": "2509.25401", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.25401", "abs": "https://arxiv.org/abs/2509.25401", "authors": ["Liang Qiao", "Yue Dai", "Yeqi Huang", "Hongyu Kan", "Jun Shi", "Hong An"], "title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers", "comment": null, "summary": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality.", "AI": {"tldr": "FlashOmni is a unified sparse attention engine that accelerates Multi-Modal Diffusion Transformers (DiTs) by standardizing diverse sparsity patterns through flexible sparse symbols, enabling efficient execution within a single kernel and achieving significant speedups without quality degradation.", "motivation": "Current sparsity-based acceleration methods for DiTs require customized kernels for different sparsity patterns, limiting universality and deployment efficiency. There's a need for a unified solution that can handle various sparsity strategies without performance loss.", "method": "FlashOmni introduces flexible sparse symbols to standardize representation of diverse sparsity strategies (feature caching, block-sparse skipping). It designs optimized sparse GEMMs for attention blocks, leveraging sparse symbols to eliminate redundant computations and enable execution of diverse sparse computations within a single attention kernel.", "result": "FlashOmni achieves near-linear speedup matching sparsity ratio (1:1) in attention and GEMM-Q, and 2.5\u00d7-3.8\u00d7 acceleration in GEMM-O (reaching about 87.5% of theoretical limit). Applied to Hunyuan model (33K), it enables about 1.5\u00d7 end-to-end acceleration without visual quality degradation.", "conclusion": "FlashOmni provides a universal sparse attention engine that effectively accelerates DiTs across various architectures and sparsity patterns, achieving significant performance improvements while maintaining output quality, making it a practical solution for deploying computationally demanding diffusion models."}}
{"id": "2509.26062", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26062", "abs": "https://arxiv.org/abs/2509.26062", "authors": ["Yanbo Wang", "Zixiang Xu", "Yue Huang", "Xiangqi Wang", "Zirui Song", "Lang Gao", "Chenxi Wang", "Xiangru Tang", "Yue Zhao", "Arman Cohan", "Xiangliang Zhang", "Xiuying Chen"], "title": "DyFlow: Dynamic Workflow Framework for Agentic Reasoning", "comment": null, "summary": "Agent systems based on large language models (LLMs) have shown great\npotential in complex reasoning tasks, but building efficient and generalizable\nworkflows remains a major challenge. Most existing approaches rely on manually\ndesigned processes, which limits their adaptability across different tasks.\nWhile a few methods attempt automated workflow generation, they are often tied\nto specific datasets or query types and make limited use of intermediate\nfeedback, reducing system robustness and reasoning depth. Moreover, their\noperations are typically predefined and inflexible. To address these\nlimitations, we propose DyFlow, a dynamic workflow generation framework that\nadaptively constructs and adjusts reasoning procedures based on task\nrequirements and real-time intermediate feedback, thereby enhancing cross-task\ngeneralization. DyFlow consists of two core components: a designer and an\nexecutor. The designer decomposes complex problems into a sequence of sub-goals\ndefined by high-level objectives and dynamically plans the next steps based on\nintermediate outputs and feedback. These plans are then carried out by the\nexecutor, which executes each operation using dynamic operators with\ncontext-aware parameterization, enabling flexible and semantically grounded\nreasoning. We systematically evaluate DyFlow across diverse domains, including\nsocial reasoning, biomedical tasks, mathematical problem solving, and code\ngeneration. Results demonstrate that DyFlow significantly outperforms existing\nbaselines, achieving substantial Pass@k improvements and exhibiting robust\ngeneralization across diverse domains. The code is publicly available at\nhttps://github.com/wyf23187/DyFlow.", "AI": {"tldr": "DyFlow is a dynamic workflow generation framework for LLM-based agents that adaptively constructs and adjusts reasoning procedures using real-time feedback, enhancing cross-task generalization.", "motivation": "Existing LLM agent systems rely on manually designed workflows that limit adaptability, while automated methods are dataset-specific and make limited use of intermediate feedback, reducing robustness and reasoning depth.", "method": "DyFlow consists of two components: a designer that decomposes problems into sub-goals and dynamically plans next steps, and an executor that uses dynamic operators with context-aware parameterization to execute operations.", "result": "DyFlow significantly outperforms existing baselines across diverse domains including social reasoning, biomedical tasks, mathematical problem solving, and code generation, achieving substantial Pass@k improvements.", "conclusion": "The framework demonstrates robust generalization across domains and enables flexible, semantically grounded reasoning through dynamic workflow generation and real-time feedback utilization."}}
{"id": "2509.25693", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25693", "abs": "https://arxiv.org/abs/2509.25693", "authors": ["N. de Silva", "S. Perera", "K. L. A. A. Nimasha", "I. D. S. Fernando", "R. K. A. O. Wijerathne"], "title": "ScheduleMe: Multi-Agent Calendar Assistant", "comment": null, "summary": "Recent advancements in LLMs have contributed to the rise of advanced\nconversational assistants that can assist with user needs through natural\nlanguage conversation. This paper presents a ScheduleMe, a multi-agent calendar\nassistant for users to manage google calendar events in natural language. The\nsystem uses a graph-structured coordination mechanism where a central\nsupervisory agent supervises specialized task agents, allowing modularity,\nconflicts resolution, and context-aware interactions to resolve ambiguities and\nevaluate user commands. This approach sets an example of how structured\nreasoning and agent cooperation might convince operators to increase the\nusability and flexibility of personal calendar assistant tools.", "AI": {"tldr": "ScheduleMe is a multi-agent calendar assistant that manages Google Calendar events using natural language through a graph-structured coordination system with a central supervisory agent.", "motivation": "To create a more usable and flexible personal calendar assistant by leveraging LLMs and multi-agent systems to handle natural language commands and resolve ambiguities.", "method": "Uses a graph-structured coordination mechanism with a central supervisory agent that oversees specialized task agents, enabling modularity, conflict resolution, and context-aware interactions.", "result": "Developed a functional multi-agent system that can manage calendar events through natural language conversation while resolving ambiguities and conflicts.", "conclusion": "The approach demonstrates how structured reasoning and agent cooperation can enhance the usability and flexibility of personal calendar assistant tools."}}
{"id": "2509.25414", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25414", "abs": "https://arxiv.org/abs/2509.25414", "authors": ["Hao Ban", "Kaiyi Ji"], "title": "Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs", "comment": null, "summary": "Large language models are often adapted using parameter-efficient techniques\nsuch as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$\nis the pre-trained parameters and $x$ is the input to the adapted layer. While\nmulti-adapter extensions often employ multiple LoRAs, prior studies suggest\nthat the inner $A$ matrices are highly similar during training and thus\nsuitable for sharing. We revisit this phenomenon and find that this similarity\nis largely attributable to the identical initialization rather than shared\nknowledge, with $B$ playing a more critical role in knowledge encoding and\ntransfer. Motivated by these insights, we propose \\textbf{ALoRA}, an asymmetric\nmulti-LoRA design with multiple $A$ matrices and a single shared $B$ in\nmulti-task fine-tuning, and \\textbf{Fed-ALoRA}, which shares $B$ across clients\nin federated fine-tuning under both homogeneous and heterogeneous settings,\nthrough a novel matrix decomposition strategy to accommodate heterogeneous\nranks across clients. Experiments on commonsense reasoning, math reasoning,\nmulti-task NLP dataset, and federated NLP dataset demonstrate that our methods\nachieve more balanced performance across tasks with comparable or superior\naverage accuracy relative to existing multi-LoRA approaches. Codes are\navailable at https://github.com/OptMN-Lab/ALoRA.", "AI": {"tldr": "The paper proposes ALoRA and Fed-ALoRA, asymmetric multi-LoRA designs that share the B matrix while using multiple A matrices, achieving balanced performance across tasks in multi-task and federated fine-tuning.", "motivation": "Prior studies suggest that inner A matrices in LoRA are highly similar during training, but this similarity is due to identical initialization rather than shared knowledge. The B matrix plays a more critical role in knowledge encoding and transfer.", "method": "Proposed ALoRA with multiple A matrices and a single shared B in multi-task fine-tuning, and Fed-ALoRA which shares B across clients in federated fine-tuning using a novel matrix decomposition strategy to handle heterogeneous ranks.", "result": "Experiments on commonsense reasoning, math reasoning, multi-task NLP, and federated NLP datasets show balanced performance across tasks with comparable or superior average accuracy compared to existing multi-LoRA approaches.", "conclusion": "The asymmetric design with shared B matrix is effective for multi-task and federated fine-tuning, demonstrating that B plays a more critical role than A in knowledge encoding and transfer."}}
{"id": "2509.26072", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26072", "abs": "https://arxiv.org/abs/2509.26072", "authors": ["Arash Marioriyad", "Mohammad Hossein Rohban", "Mahdieh Soleymani Baghshah"], "title": "The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge", "comment": "9 Pages, 5 Tables, 1 Figures", "summary": "Large language models (LLMs) are increasingly deployed as automatic judges to\nevaluate system outputs in tasks such as summarization, dialogue, and creative\nwriting. A faithful judge should base its verdicts solely on response quality\nand explicitly acknowledge the factors shaping its decision. We show that\ncurrent LLM judges fail on both counts by relying on shortcuts introduced in\nthe prompt. Our study uses two evaluation datasets: ELI5, a benchmark for\nlong-form question answering, and LitBench, a recent benchmark for creative\nwriting. Both datasets provide pairwise comparisons, where the evaluator must\nchoose which of two responses is better. From each dataset we construct 100\npairwise judgment tasks and employ two widely used models, GPT-4o and\nGemini-2.5-Flash, as evaluators in the role of LLM-as-a-judge. For each pair,\nwe assign superficial cues to the responses, provenance cues indicating source\nidentity (Human, Expert, LLM, or Unknown) and recency cues indicating temporal\norigin (Old, 1950 vs. New, 2025), while keeping the rest of the prompt fixed.\nResults reveal consistent verdict shifts: both models exhibit a strong recency\nbias, systematically favoring new responses over old, as well as a clear\nprovenance hierarchy (Expert > Human > LLM > Unknown). These biases are\nespecially pronounced in GPT-4o and in the more subjective and open-ended\nLitBench domain. Crucially, cue acknowledgment is rare: justifications almost\nnever reference the injected cues, instead rationalizing decisions in terms of\ncontent qualities. These findings demonstrate that current LLM-as-a-judge\nsystems are shortcut-prone and unfaithful, undermining their reliability as\nevaluators in both research and deployment.", "AI": {"tldr": "LLM judges exhibit systematic biases based on superficial cues (provenance and recency) rather than actual response quality, and fail to acknowledge these biases in their justifications.", "motivation": "To investigate whether LLM judges base their evaluations faithfully on response quality or rely on shortcuts introduced in prompts, which would undermine their reliability as evaluators.", "method": "Used ELI5 and LitBench datasets with pairwise comparisons, constructed 100 judgment tasks per dataset, employed GPT-4o and Gemini-2.5-Flash as evaluators, and systematically manipulated provenance (Human, Expert, LLM, Unknown) and recency (Old/1950 vs New/2025) cues while keeping content constant.", "result": "Both models showed strong recency bias (favoring new responses) and clear provenance hierarchy (Expert > Human > LLM > Unknown), with biases more pronounced in GPT-4o and subjective domains like LitBench. Cue acknowledgment was rare - justifications rationalized decisions based on content qualities rather than the injected cues.", "conclusion": "Current LLM-as-a-judge systems are shortcut-prone and unfaithful, relying on superficial cues rather than genuine quality assessment, which undermines their reliability for research and deployment purposes."}}
{"id": "2509.25751", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25751", "abs": "https://arxiv.org/abs/2509.25751", "authors": ["Qi Liu", "Xueyuan Li", "Zirui Li", "Juhui Gim"], "title": "Cooperative Autonomous Driving in Diverse Behavioral Traffic: A Heterogeneous Graph Reinforcement Learning Approach", "comment": "7 pages, 5 figures and 4 tables", "summary": "Navigating heterogeneous traffic environments with diverse driving styles\nposes a significant challenge for autonomous vehicles (AVs) due to their\ninherent complexity and dynamic interactions. This paper addresses this\nchallenge by proposing a heterogeneous graph reinforcement learning (GRL)\nframework enhanced with an expert system to improve AV decision-making\nperformance. Initially, a heterogeneous graph representation is introduced to\ncapture the intricate interactions among vehicles. Then, a heterogeneous graph\nneural network with an expert model (HGNN-EM) is proposed to effectively encode\ndiverse vehicle features and produce driving instructions informed by\ndomain-specific knowledge. Moreover, the double deep Q-learning (DDQN)\nalgorithm is utilized to train the decision-making model. A case study on a\ntypical four-way intersection, involving various driving styles of human\nvehicles (HVs), demonstrates that the proposed method has superior performance\nover several baselines regarding safety, efficiency, stability, and convergence\nrate, all while maintaining favorable real-time performance.", "AI": {"tldr": "Proposes a heterogeneous graph reinforcement learning framework with expert system to improve autonomous vehicle decision-making in complex traffic environments with diverse driving styles.", "motivation": "Address the challenge of navigating heterogeneous traffic environments with diverse driving styles for autonomous vehicles, due to complexity and dynamic interactions.", "method": "Uses heterogeneous graph representation for vehicle interactions, HGNN-EM (heterogeneous graph neural network with expert model) for feature encoding, and DDQN algorithm for training decision-making model.", "result": "Case study on four-way intersection shows superior performance over baselines in safety, efficiency, stability, convergence rate, while maintaining real-time performance.", "conclusion": "The proposed heterogeneous GRL framework with expert system effectively improves AV decision-making in complex traffic environments with diverse driving styles."}}
{"id": "2509.25418", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25418", "abs": "https://arxiv.org/abs/2509.25418", "authors": ["Dong Hyun Jeon", "Lijing Zhu", "Haifang Li", "Pengze Li", "Jingna Feng", "Tiehang Duan", "Houbing Herbert Song", "Cui Tao", "Shuteng Niu"], "title": "Leveraging Vulnerabilities in Temporal Graph Neural Networks via Strategic High-Impact Assaults", "comment": null, "summary": "Temporal Graph Neural Networks (TGNNs) have become indispensable for\nanalyzing dynamic graphs in critical applications such as social networks,\ncommunication systems, and financial networks. However, the robustness of TGNNs\nagainst adversarial attacks, particularly sophisticated attacks that exploit\nthe temporal dimension, remains a significant challenge. Existing attack\nmethods for Spatio-Temporal Dynamic Graphs (STDGs) often rely on simplistic,\neasily detectable perturbations (e.g., random edge additions/deletions) and\nfail to strategically target the most influential nodes and edges for maximum\nimpact. We introduce the High Impact Attack (HIA), a novel restricted black-box\nattack framework specifically designed to overcome these limitations and expose\ncritical vulnerabilities in TGNNs. HIA leverages a data-driven surrogate model\nto identify structurally important nodes (central to network connectivity) and\ndynamically important nodes (critical for the graph's temporal evolution). It\nthen employs a hybrid perturbation strategy, combining strategic edge injection\n(to create misleading connections) and targeted edge deletion (to disrupt\nessential pathways), maximizing TGNN performance degradation. Importantly, HIA\nminimizes the number of perturbations to enhance stealth, making it more\nchallenging to detect. Comprehensive experiments on five real-world datasets\nand four representative TGNN architectures (TGN, JODIE, DySAT, and TGAT)\ndemonstrate that HIA significantly reduces TGNN accuracy on the link prediction\ntask, achieving up to a 35.55% decrease in Mean Reciprocal Rank (MRR) - a\nsubstantial improvement over state-of-the-art baselines. These results\nhighlight fundamental vulnerabilities in current STDG models and underscore the\nurgent need for robust defenses that account for both structural and temporal\ndynamics.", "AI": {"tldr": "HIA is a novel black-box attack framework that strategically targets both structurally and dynamically important nodes in temporal graphs, using hybrid edge perturbations to significantly degrade TGNN performance while maintaining stealth.", "motivation": "TGNNs are vulnerable to adversarial attacks, especially those exploiting temporal dimensions. Existing attacks use simplistic perturbations and fail to strategically target influential nodes for maximum impact.", "method": "HIA uses a data-driven surrogate model to identify structurally important nodes (network connectivity) and dynamically important nodes (temporal evolution), then employs hybrid perturbation strategy combining strategic edge injection and targeted edge deletion.", "result": "Comprehensive experiments on 5 real-world datasets and 4 TGNN architectures show HIA reduces TGNN accuracy by up to 35.55% in MRR, significantly outperforming state-of-the-art baselines.", "conclusion": "HIA exposes fundamental vulnerabilities in current STDG models and highlights the urgent need for robust defenses accounting for both structural and temporal dynamics."}}
{"id": "2509.26074", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26074", "abs": "https://arxiv.org/abs/2509.26074", "authors": ["Leitian Tao", "Xuefeng Du", "Yixuan Li"], "title": "Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis", "comment": "Accepted by NeurIPS 2025", "summary": "Reward modeling, crucial for aligning large language models (LLMs) with human\npreferences, is often bottlenecked by the high cost of preference data.\nExisting textual data synthesis methods are computationally expensive. We\npropose a novel framework LENS for synthesizing preference data directly in the\nLLM's latent embedding space. Our method employs a Variational Autoencoder\n(VAE) to learn a structured latent representation of response embeddings. By\nperforming controlled perturbations in this latent space and decoding back to\nthe embedding space, we efficiently generate diverse, semantically consistent\nsynthetic preference pairs, bypassing costly text generation and annotation. We\nprovide theoretical guarantees that our synthesized pairs approximately\npreserve original preference ordering and improve reward model generalization.\nEmpirically, our latent-space synthesis significantly outperforms text-based\naugmentation on standard benchmarks, achieving superior results while being 18x\nfaster in generation and using a 16,000x smaller model. Our work offers a\nscalable and effective alternative for enhancing reward modeling through\nefficient data augmentation. Code is publicly available at\nhttps://github.com/deeplearning-wisc/lens", "AI": {"tldr": "LENS is a novel framework that synthesizes preference data directly in LLM's latent embedding space using VAE, bypassing expensive text generation and annotation while preserving preference ordering.", "motivation": "High cost of preference data collection bottlenecks reward modeling for aligning LLMs with human preferences, and existing text-based synthesis methods are computationally expensive.", "method": "Uses VAE to learn structured latent representation of response embeddings, performs controlled perturbations in latent space, and decodes back to embedding space to generate synthetic preference pairs.", "result": "Significantly outperforms text-based augmentation on benchmarks, achieves superior results with 18x faster generation and uses 16,000x smaller model.", "conclusion": "LENS offers scalable and effective alternative for enhancing reward modeling through efficient latent-space data augmentation."}}
{"id": "2509.25757", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.SC"], "pdf": "https://arxiv.org/pdf/2509.25757", "abs": "https://arxiv.org/abs/2509.25757", "authors": ["Danial Kamali", "Parisa Kordjamshidi"], "title": "NePTune: A Neuro-Pythonic Framework for Tunable Compositional Reasoning on Vision-Language", "comment": null, "summary": "Modern Vision-Language Models (VLMs) have achieved impressive performance in\nvarious tasks, yet they often struggle with compositional reasoning, the\nability to decompose and recombine concepts to solve novel problems. While\nneuro-symbolic approaches offer a promising direction, they are typically\nconstrained by crisp logical execution or predefined predicates, which limit\nflexibility. In this work, we introduce NePTune, a neuro-symbolic framework\nthat overcomes these limitations through a hybrid execution model that\nintegrates the perception capabilities of foundation vision models with the\ncompositional expressiveness of symbolic reasoning. NePTune dynamically\ntranslates natural language queries into executable Python programs that blend\nimperative control flow with soft logic operators capable of reasoning over\nVLM-generated uncertainty. Operating in a training-free manner, NePTune, with a\nmodular design, decouples perception from reasoning, yet its differentiable\noperations support fine-tuning. We evaluate NePTune on multiple visual\nreasoning benchmarks and various domains, utilizing adversarial tests, and\ndemonstrate a significant improvement over strong base models, as well as its\neffective compositional generalization and adaptation capabilities in novel\nenvironments.", "AI": {"tldr": "NePTune is a neuro-symbolic framework that combines foundation vision models with symbolic reasoning to improve compositional visual reasoning, operating training-free with modular design and supporting fine-tuning.", "motivation": "VLMs struggle with compositional reasoning, and existing neuro-symbolic approaches are limited by rigid logical execution and predefined predicates, lacking flexibility.", "method": "Dynamic translation of natural language queries into executable Python programs blending imperative control flow with soft logic operators that handle VLM uncertainty, with modular design separating perception from reasoning.", "result": "Significant improvement over strong base models on multiple visual reasoning benchmarks, effective compositional generalization and adaptation in novel environments.", "conclusion": "NePTune successfully overcomes limitations of traditional neuro-symbolic approaches by integrating perception capabilities with compositional symbolic reasoning in a flexible, training-free framework."}}
{"id": "2509.25424", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25424", "abs": "https://arxiv.org/abs/2509.25424", "authors": ["Jubayer Ibn Hamid", "Ifdita Hasan Orney", "Ellen Xu", "Chelsea Finn", "Dorsa Sadigh"], "title": "Polychromic Objectives for Reinforcement Learning", "comment": null, "summary": "Reinforcement learning fine-tuning (RLFT) is a dominant paradigm for\nimproving pretrained policies for downstream tasks. These pretrained policies,\ntrained on large datasets, produce generations with a broad range of promising\nbut unrefined behaviors. Often, a critical failure mode of RLFT arises when\npolicies lose this diversity and collapse into a handful of easily exploitable\noutputs. This convergence hinders exploration, which is essential for expanding\nthe capabilities of the pretrained policy and for amplifying the benefits of\ntest-time compute scaling. To address this, we introduce an objective for\npolicy gradient methods that explicitly enforces the exploration and refinement\nof diverse generations, which we call a polychromic objective. We then show how\nproximal policy optimization (PPO) can be adapted to optimize this objective.\nOur method (1) employs vine sampling to collect on-policy rollouts and (2)\nmodifies the advantage function to reflect the advantage under our new\nobjective. Experiments on BabyAI, Minigrid, and Algorithmic Creativity show\nthat our method improves success rates by reliably solving a larger set of\nenvironment configurations and generalizes better under large perturbations.\nMoreover, when given multiple attempts in pass@$k$ experiments, the policy\nachieves substantially higher coverage, demonstrating its ability to maintain\nand exploit a diverse repertoire of strategies.", "AI": {"tldr": "The paper introduces a polychromic objective for RLFT to prevent policy collapse and maintain diverse behaviors during fine-tuning, improving exploration and generalization.", "motivation": "RLFT often causes policies to lose diversity and collapse into limited outputs, hindering exploration and the benefits of test-time compute scaling.", "method": "Adapts PPO with vine sampling for on-policy rollouts and modifies the advantage function to optimize the polychromic objective that enforces diverse generation exploration.", "result": "Improves success rates on BabyAI, Minigrid, and Algorithmic Creativity, solves more environment configurations, generalizes better under perturbations, and achieves higher coverage in pass@k experiments.", "conclusion": "The polychromic objective effectively maintains policy diversity during RLFT, leading to better exploration, generalization, and performance across multiple tasks."}}
{"id": "2509.26076", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26076", "abs": "https://arxiv.org/abs/2509.26076", "authors": ["Johannes Schmitt", "Gergely B\u00e9rczi", "Jasper Dekoninck", "Jeremy Feusi", "Tim Gehrunger", "Raphael Appenzeller", "Jim Bryan", "Niklas Canova", "Timo de Wolff", "Filippo Gaia", "Michel van Garrel", "Baran Hashemi", "David Holmes", "Aitor Iribar Lopez", "Victor Jaeck", "Martina J\u00f8rgensen", "Steven Kelk", "Stefan Kuhlmann", "Adam Kurpisz", "Chiara Meroni", "Ingmar Metzler", "Martin M\u00f6ller", "Samuel Mu\u00f1oz-Ech\u00e1niz", "Robert Nowak", "Georg Oberdieck", "Daniel Platt", "Dylan Possama\u00ef", "Gabriel Ribeiro", "Ra\u00fal S\u00e1nchez Gal\u00e1n", "Zheming Sun", "Josef Teichmann", "Richard P. Thomas", "Charles Vial"], "title": "IMProofBench: Benchmarking AI on Research-Level Mathematical Proof Generation", "comment": null, "summary": "As the mathematical capabilities of large language models (LLMs) improve, it\nbecomes increasingly important to evaluate their performance on research-level\ntasks at the frontier of mathematical knowledge. However, existing benchmarks\nare limited, as they focus solely on final-answer questions or high-school\ncompetition problems. To address this gap, we introduce IMProofBench, a private\nbenchmark consisting of 39 peer-reviewed problems developed by expert\nmathematicians. Each problem requires a detailed proof and is paired with\nsubproblems that have final answers, supporting both an evaluation of\nmathematical reasoning capabilities by human experts and a large-scale\nquantitative analysis through automated grading. Furthermore, unlike prior\nbenchmarks, the evaluation setup simulates a realistic research environment:\nmodels operate in an agentic framework with tools like web search for\nliterature review and mathematical software such as SageMath. Our results show\nthat current LLMs can succeed at the more accessible research-level questions,\nbut still encounter significant difficulties on more challenging problems.\nQuantitatively, Grok-4 achieves the highest accuracy of 52% on final-answer\nsubproblems, while GPT-5 obtains the best performance for proof generation,\nachieving a fully correct solution for 22% of problems. IMProofBench will\ncontinue to evolve as a dynamic benchmark in collaboration with the\nmathematical community, ensuring its relevance for evaluating the next\ngeneration of LLMs.", "AI": {"tldr": "IMProofBench is a new benchmark for evaluating LLMs on research-level mathematical problems, featuring peer-reviewed problems requiring detailed proofs and subproblems with final answers, tested in an agentic framework with tools.", "motivation": "Existing benchmarks are limited to final-answer questions or high-school competition problems, failing to evaluate LLMs' capabilities on research-level mathematical tasks at the frontier of knowledge.", "method": "Created IMProofBench with 39 peer-reviewed problems developed by expert mathematicians, each requiring detailed proofs and paired with subproblems. Evaluation uses agentic framework with tools like web search and SageMath to simulate realistic research environment.", "result": "Current LLMs succeed at accessible research-level questions but struggle with challenging problems. Grok-4 achieves 52% accuracy on final-answer subproblems, while GPT-5 achieves 22% fully correct proof solutions.", "conclusion": "IMProofBench addresses the gap in evaluating LLMs on research-level mathematics and will evolve as a dynamic benchmark in collaboration with the mathematical community to ensure relevance for future LLM evaluation."}}
{"id": "2509.25758", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25758", "abs": "https://arxiv.org/abs/2509.25758", "authors": ["Yein Park", "Minbyul Jeong", "Jaewoo Kang"], "title": "Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training", "comment": null, "summary": "The remarkable capabilities of modern large reasoning models are largely\nunlocked through post-training techniques such as supervised fine-tuning and\nreinforcement learning. However, the architectural mechanisms behind such\nimprovements remain largely opaque. In this work, we use circuit analysis to\ndemonstrate that post-training for complex reasoning sparks the emergence of\nnovel, functionally specialized attention heads. These heads collectively\nsupport structured reasoning and computation. Our comparative analysis across\nQwen families and DeepSeek-distilled model reveals that these emergent heads\nevolve differently under different training regimes. Distillation and SFT\nfoster a cumulative addition of stable reasoning heads. In contrast, group\nrelative policy optimization operates in a dynamic search mode: relatively few\nattention heads are iteratively activated, evaluated, and pruned, with their\nsurvival closely tracking fluctuations in the task reward signal. Furthermore,\nwe find that controllable think on/off models do not possess dedicated thinking\nheads. Instead, turning off explicit reasoning triggers a broader-but less\nefficient-set of compensatory heads. Through ablation and qualitative analyses,\nwe connect these circuit-level dynamics to a crucial performance trade-off:\nstrengthened heads enable sophisticated problem-solving strategies for\ndifficult problems but can also introduce over-thinking failure modes, such as\ncalculation errors or logical loops on simpler tasks. These findings connect\ncircuit-level dynamics to macro-level performance, identifying an inherent\ntension where complex reasoning comes at the cost of elementary computations.\nMore broadly, our work points to future directions for training policy design,\nemphasizing the need to balance the development of effective reasoning\nstrategies with the assurance of reliable, flawless execution.", "AI": {"tldr": "Post-training techniques create specialized attention heads for reasoning, but different methods (distillation/SFT vs GRPO) produce different circuit dynamics, revealing a trade-off between complex reasoning and reliable execution.", "motivation": "To understand the architectural mechanisms behind improvements from post-training techniques like supervised fine-tuning and reinforcement learning in large reasoning models.", "method": "Used circuit analysis to study attention head emergence across Qwen families and DeepSeek-distilled models under different training regimes (distillation/SFT vs group relative policy optimization).", "result": "Different training methods create distinct circuit dynamics: distillation/SFT adds stable reasoning heads cumulatively, while GRPO iteratively activates and prunes heads based on reward signals. Think on/off models use compensatory heads rather than dedicated thinking heads.", "conclusion": "There's an inherent tension where complex reasoning capabilities come at the cost of reliable elementary computations, pointing to the need for balanced training policies that ensure both effective reasoning strategies and flawless execution."}}
{"id": "2509.25429", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.25429", "abs": "https://arxiv.org/abs/2509.25429", "authors": ["Sreeja Apparaju", "Yichuan Niu", "Xixi Qi"], "title": "Feedback Control for Small Budget Pacing", "comment": null, "summary": "Budget pacing is critical in online advertising to align spend with campaign\ngoals under dynamic auctions. Existing pacing methods often rely on ad-hoc\nparameter tuning, which can be unstable and inefficient. We propose a\nprincipled controller that combines bucketized hysteresis with proportional\nfeedback to provide stable and adaptive spend control. Our method provides a\nframework and analysis for parameter selection that enables accurate tracking\nof desired spend rates across campaigns. Experiments in real-world auctions\ndemonstrate significant improvements in pacing accuracy and delivery\nconsistency, reducing pacing error by 13% and $\\lambda$-volatility by 54%\ncompared to baseline method. By bridging control theory with advertising\nsystems, our approach offers a scalable and reliable solution for budget\npacing, with particular benefits for small-budget campaigns.", "AI": {"tldr": "A novel budget pacing controller combining bucketized hysteresis with proportional feedback for stable and adaptive spend control in online advertising, reducing pacing error by 13% and volatility by 54%.", "motivation": "Existing pacing methods rely on ad-hoc parameter tuning which is unstable and inefficient, creating a need for more principled approaches to align spend with campaign goals under dynamic auctions.", "method": "Proposes a controller that combines bucketized hysteresis with proportional feedback, providing a framework and analysis for parameter selection to enable accurate tracking of desired spend rates.", "result": "Experiments in real-world auctions demonstrate significant improvements: 13% reduction in pacing error and 54% reduction in \u03bb-volatility compared to baseline methods.", "conclusion": "Bridging control theory with advertising systems provides a scalable and reliable solution for budget pacing, with particular benefits for small-budget campaigns."}}
{"id": "2509.26093", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26093", "abs": "https://arxiv.org/abs/2509.26093", "authors": ["Xiaoyan Zhao"], "title": "Reinforced Strategy Optimization for Conversational Recommender Systems via Network-of-Experts", "comment": null, "summary": "Conversational Recommender Systems (CRSs) provide personalized\nrecommendations through multi-turn interactions. With the strong reasoning\nabilities of Large Language Models (LLMs), applying them to CRSs has become\npromising. Yet, existing methods often lack explicit optimization of\ninteraction strategies, relying instead on unified prompts, which can yield\nsuboptimal outcomes. We propose Reinforced Strategy Optimization (RSO), a\nhierarchical framework that decomposes response generation into macro-level\nstrategy planning and micro-level adaptation within a network-of-experts. A\nPlanner selects strategies (e.g., recommend, explain, encourage), while an\nActor generates responses guided by auxiliary experts for preferences and\nfactual grounding. This disentanglement enables more tractable learning. To\naddress limited multi-turn data, we model strategy learning as reinforcement\nlearning with an LLM-based reward for exploration. Experiments show RSO\noutperforms state-of-the-art baselines, validating the effectiveness of\nhierarchical strategy optimization.", "AI": {"tldr": "RSO is a hierarchical framework for conversational recommender systems that separates strategy planning from response generation, using reinforcement learning with LLM-based rewards to optimize interaction strategies.", "motivation": "Existing methods using unified prompts in LLM-based conversational recommender systems lack explicit optimization of interaction strategies, leading to suboptimal outcomes.", "method": "Hierarchical framework with Planner selecting strategies and Actor generating responses guided by experts. Uses reinforcement learning with LLM-based rewards to address limited multi-turn data.", "result": "RSO outperforms state-of-the-art baselines in experiments.", "conclusion": "The hierarchical strategy optimization approach is effective for improving conversational recommender systems."}}
{"id": "2509.25767", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25767", "abs": "https://arxiv.org/abs/2509.25767", "authors": ["Matt Keon", "Aabid Karim", "Bhoomika Lohana", "Abdul Karim", "Thai Nguyen", "Tara Hamilton", "Ali Abbas"], "title": "Galton's Law of Mediocrity: Why Large Language Models Regress to the Mean and Fail at Creativity in Advertising", "comment": null, "summary": "Large language models (LLMs) generate fluent text yet often default to safe,\ngeneric phrasing, raising doubts about their ability to handle creativity. We\nformalize this tendency as a Galton-style regression to the mean in language\nand evaluate it using a creativity stress test in advertising concepts. When ad\nideas were simplified step by step, creative features such as metaphors,\nemotions, and visual cues disappeared early, while factual content remained,\nshowing that models favor high-probability information. When asked to\nregenerate from simplified inputs, models produced longer outputs with lexical\nvariety but failed to recover the depth and distinctiveness of the originals.\nWe combined quantitative comparisons with qualitative analysis, which revealed\nthat the regenerated texts often appeared novel but lacked true originality.\nProviding ad-specific cues such as metaphors, emotional hooks and visual\nmarkers improved alignment and stylistic balance, though outputs still relied\non familiar tropes. Taken together, the findings show that without targeted\nguidance, LLMs drift towards mediocrity in creative tasks; structured signals\ncan partially counter this tendency and point towards pathways for developing\ncreativity-sensitive models.", "AI": {"tldr": "LLMs tend to produce safe, generic text in creative tasks due to regression to the mean in language. When simplifying ad concepts, creative elements disappear first while factual content remains. Regenerated texts lack true originality despite lexical variety.", "motivation": "To investigate LLMs' tendency towards safe, generic phrasing in creative tasks and formalize this as Galton-style regression to the mean in language.", "method": "Used creativity stress test with advertising concepts, simplifying ideas step by step and analyzing feature disappearance. Combined quantitative comparisons with qualitative analysis of regenerated texts.", "result": "Creative features (metaphors, emotions, visual cues) disappeared early during simplification while factual content remained. Regenerated texts were longer with lexical variety but lacked depth and distinctiveness. Ad-specific cues improved alignment but outputs still relied on familiar tropes.", "conclusion": "Without targeted guidance, LLMs drift towards mediocrity in creative tasks. Structured signals can partially counter this tendency, pointing towards pathways for developing creativity-sensitive models."}}
{"id": "2509.25438", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25438", "abs": "https://arxiv.org/abs/2509.25438", "authors": ["Zhibo Hou", "Zhiyu An", "Wan Du"], "title": "Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring", "comment": null, "summary": "When there exists an unlearnable source of randomness (noisy-TV) in the\nenvironment, a naively intrinsic reward driven exploring agent gets stuck at\nthat source of randomness and fails at exploration. Intrinsic reward based on\nuncertainty estimation or distribution similarity, while eventually escapes\nnoisy-TVs as time unfolds, suffers from poor sample efficiency and high\ncomputational cost. Inspired by recent findings from neuroscience that humans\nmonitor their improvements during exploration, we propose a novel method for\nintrinsically-motivated exploration, named Learning Progress Monitoring (LPM).\nDuring exploration, LPM rewards model improvements instead of prediction error\nor novelty, effectively rewards the agent for observing learnable transitions\nrather than the unlearnable transitions. We introduce a dual-network design\nthat uses an error model to predict the expected prediction error of the\ndynamics model in its previous iteration, and use the difference between the\nmodel errors of the current iteration and previous iteration to guide\nexploration. We theoretically show that the intrinsic reward of LPM is\nzero-equivariant and a monotone indicator of Information Gain (IG), and that\nthe error model is necessary to achieve monotonicity correspondence with IG. We\nempirically compared LPM against state-of-the-art baselines in noisy\nenvironments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari.\nResults show that LPM's intrinsic reward converges faster, explores more states\nin the maze experiment, and achieves higher extrinsic reward in Atari. This\nconceptually simple approach marks a shift-of-paradigm of noise-robust\nexploration. For code to reproduce our experiments, see\nhttps://github.com/Akuna23Matata/LPM_exploration", "AI": {"tldr": "LPM is a novel intrinsic reward method that monitors learning progress instead of prediction error, enabling efficient exploration in noisy environments by focusing on learnable transitions.", "motivation": "Existing intrinsic reward methods get stuck at unlearnable noise sources (noisy-TVs) or suffer from poor sample efficiency. Inspired by human learning monitoring, LPM addresses these limitations.", "method": "Uses dual-network design: an error model predicts previous dynamics model's prediction error, and the difference between current and previous model errors guides exploration.", "result": "LPM converges faster, explores more states in maze experiments, and achieves higher extrinsic rewards in Atari compared to state-of-the-art baselines.", "conclusion": "LPM provides a paradigm shift for noise-robust exploration by rewarding learning progress rather than prediction error, with theoretical guarantees of monotonic correspondence with Information Gain."}}
{"id": "2509.26103", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26103", "abs": "https://arxiv.org/abs/2509.26103", "authors": ["Ilya Boytsov", "Vinny DeGenova", "Mikhail Balyasin", "Joseph Walt", "Caitlin Eusden", "Marie-Claire Rochat", "Margaret Pierson"], "title": "End-to-End Aspect-Guided Review Summarization at Scale", "comment": "Camera-ready preprint for EMNLP 2025 Industry Track", "summary": "We present a scalable large language model (LLM)-based system that combines\naspect-based sentiment analysis (ABSA) with guided summarization to generate\nconcise and interpretable product review summaries for the Wayfair platform.\nOur approach first extracts and consolidates aspect-sentiment pairs from\nindividual reviews, selects the most frequent aspects for each product, and\nsamples representative reviews accordingly. These are used to construct\nstructured prompts that guide the LLM to produce summaries grounded in actual\ncustomer feedback. We demonstrate the real-world effectiveness of our system\nthrough a large-scale online A/B test. Furthermore, we describe our real-time\ndeployment strategy and release a dataset of 11.8 million anonymized customer\nreviews covering 92,000 products, including extracted aspects and generated\nsummaries, to support future research in aspect-guided review summarization.", "AI": {"tldr": "A scalable LLM-based system that combines aspect-based sentiment analysis with guided summarization to generate concise product review summaries for Wayfair, validated through large-scale A/B testing.", "motivation": "To create interpretable product review summaries that are grounded in actual customer feedback, addressing the need for scalable and effective review summarization in e-commerce platforms.", "method": "Extracts aspect-sentiment pairs from reviews, selects most frequent aspects per product, samples representative reviews, and uses structured prompts to guide LLM summarization.", "result": "Successfully deployed in real-time with demonstrated effectiveness through large-scale online A/B testing, and released a dataset of 11.8 million anonymized reviews with extracted aspects and summaries.", "conclusion": "The system provides an effective approach for aspect-guided review summarization at scale, with practical deployment and valuable dataset contribution for future research."}}
{"id": "2509.25779", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25779", "abs": "https://arxiv.org/abs/2509.25779", "authors": ["Siyu Zhu", "Yanbin Jiang", "Hejian Sang", "Shao Tang", "Qingquan Song", "Biao He", "Rohit Jain", "Zhipeng Wang", "Alborz Geramifard"], "title": "Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs", "comment": null, "summary": "We investigated Agentic RL with large language models on the\n\\textsc{TravelPlanner} benchmark. Our approach, \\textsc{Planner-R1}, achieved a\n\\textbf{56.9\\%} final-pass rate with only 180 training queries, a $2.7\\times$\nimprovement over GPT-5's $21.2\\%$ baseline and the strongest agentic result on\nthe public leaderboard. A central finding was that smaller models (8B) were\nhighly responsive to reward shaping: with dense process-level signals, they\nreached competitive performance while being $3.5\\times$ more compute-efficient\nand $1.5\\times$ more memory-efficient than 32B models. Larger models were more\nrobust under sparse rewards but exhibited smaller relative gains from shaping\nand higher variance across runs. While curriculum learning offered no\nsignificant benefit, shaped rewards consistently amplified learning dynamics,\nmaking 8B models the most efficient setting for agentic RL. Crucially, these\ngains did not come at the cost of overfitting: fine-tuned models mostly\nmaintained or exceeded baseline performance on out-of-domain tasks, including\n\\textsc{Multi-IF}, \\textsc{NaturalPlan}, and $\\tau$-\\textsc{Bench}. These\nresults establish reward shaping as a decisive lever for scaling agentic RL,\nhighlight the competitive strength of smaller models, and demonstrate that\nefficiency can be achieved without sacrificing generalization.", "AI": {"tldr": "Agentic RL with LLMs on TravelPlanner benchmark shows 56.9% final-pass rate using Planner-R1 with only 180 training queries, 2.7\u00d7 improvement over GPT-5 baseline. Smaller 8B models are highly responsive to reward shaping, achieving competitive performance with 3.5\u00d7 compute efficiency and 1.5\u00d7 memory efficiency compared to 32B models.", "motivation": "To investigate how reward shaping affects agentic reinforcement learning with large language models, particularly examining the trade-offs between model size, training efficiency, and generalization capabilities.", "method": "Used Planner-R1 approach on TravelPlanner benchmark with dense process-level reward signals. Compared different model sizes (8B vs 32B) under various reward conditions (dense vs sparse). Evaluated curriculum learning and tested generalization on out-of-domain tasks.", "result": "Achieved 56.9% final-pass rate with only 180 training queries. 8B models with dense rewards reached competitive performance while being 3.5\u00d7 more compute-efficient and 1.5\u00d7 more memory-efficient than 32B models. Larger models were more robust under sparse rewards but showed smaller gains from shaping. Fine-tuned models maintained or exceeded baseline performance on out-of-domain tasks.", "conclusion": "Reward shaping is a decisive lever for scaling agentic RL, smaller models (8B) are most efficient for agentic RL, and efficiency gains don't sacrifice generalization capabilities."}}
{"id": "2509.25439", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.25439", "abs": "https://arxiv.org/abs/2509.25439", "authors": ["Hanyuan Gao", "Xiaoxuan Yang"], "title": "Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications", "comment": "Accepted by Asilomar 2025", "summary": "Hidden Markov models (HMM) are commonly used in generation tasks and have\ndemonstrated strong capabilities in neuro-symbolic applications for the Markov\nproperty. These applications leverage the strengths of neural networks and\nsymbolic reasoning to create robust and interpretable AI systems. However, they\nmay inherit and amplify the shortcomings of both approaches. Both components\nrequire dense computation and data transfer, and their communication further\nhinders performance. This paper proposes Norm-Q, a normalized linear\nquantization approach for compressing probabilistic symbolic models, such as\nHMMs. We reduce the bit width of the data with minimal impact, thereby\nalleviating memory and bandwidth stress and enabling deployment on potential\ncustom hardware. Our method introduces a normalized quantization-aware\nexpectation maximization process for probabilistic model training. The\nexperimental results show that Norm-Q achieves a higher compression rate with\nreasonable score loss compared to traditional quantization methods. In the case\nof the constrained generation task of large language models, we successfully\nquantize an HMM of 4096 hidden states to 8 bits without loss and, at most, 3\nbits with acceptable loss. Notably, the Norm-Q method can achieve a compression\nrate of 99% for the weights of the HMM. The code is open source at\nhttps://github.com/superstarghy/Norm-Q.", "AI": {"tldr": "Norm-Q is a normalized linear quantization method that compresses probabilistic symbolic models like HMMs, achieving 99% compression with minimal performance loss.", "motivation": "Neuro-symbolic applications using HMMs suffer from computational intensity and data transfer bottlenecks due to dense computation requirements of both neural and symbolic components.", "method": "Proposes a normalized quantization-aware expectation maximization process for probabilistic model training, reducing bit width of data with minimal impact.", "result": "Successfully quantized HMM with 4096 hidden states to 8 bits without loss and 3 bits with acceptable loss, achieving 99% compression rate for HMM weights.", "conclusion": "Norm-Q effectively alleviates memory and bandwidth stress while maintaining reasonable performance, enabling deployment on custom hardware with high compression rates."}}
{"id": "2509.26124", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26124", "abs": "https://arxiv.org/abs/2509.26124", "authors": ["Christian Herold", "Michael Kozielski", "Nicholas Santavas", "Yannick Versley", "Shahram Khadivi"], "title": "Vocabulary Customization for Efficient Domain-Specific LLM Deployment", "comment": "Accepted at NEURIPS 2025 CCFM Workshop", "summary": "When using an LLM to process text outside the training domain(s), an often\noverlooked factor is vocabulary mismatch, where the general-domain tokenizer\nfails to capture frequent domain-specific terms, leading to higher token\nfertility and thus a decrease in processing speed due to suboptimal sub-word\nsplits.\n  We address this limitation by augmenting the pretrained vocabulary with a set\nof domain-specific tokens. To this end, we design an algorithm that extends an\nexisting tokenizer while guaranteeing it never decreases tokenization\nefficiency: every input sequence is segmented into at most the same number of\ntokens as before.\n  Evaluated on real-world e-Commerce use-cases, the augmented tokenizer\nsignificantly shortens input sequences by up to 20% and reduces inference\nlatency on downstream tasks while preserving predictive quality. We further\nanalyze secondary effects, such as the impact on forward pass speed and the\nrate at which the model adopts the newly introduced tokens, to illustrate the\nbroader benefits of vocabulary adaptation.", "AI": {"tldr": "The paper addresses vocabulary mismatch in LLMs by augmenting pretrained vocabularies with domain-specific tokens, improving tokenization efficiency and reducing inference latency without compromising model quality.", "motivation": "Vocabulary mismatch occurs when general-domain tokenizers fail to capture domain-specific terms, leading to suboptimal sub-word splits, higher token fertility, and decreased processing speed in out-of-domain text processing.", "method": "Design an algorithm to extend existing tokenizers by adding domain-specific tokens while guaranteeing no decrease in tokenization efficiency - every input sequence is segmented into at most the same number of tokens as before.", "result": "In e-Commerce use-cases, the augmented tokenizer shortens input sequences by up to 20%, reduces inference latency on downstream tasks while preserving predictive quality, and shows benefits in forward pass speed and token adoption rates.", "conclusion": "Vocabulary adaptation through domain-specific token augmentation effectively addresses vocabulary mismatch, improving processing efficiency and inference speed without sacrificing model performance."}}
{"id": "2509.25781", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.25781", "abs": "https://arxiv.org/abs/2509.25781", "authors": ["Guido Governatori", "Antonino Rotolo"], "title": "Deontic Argumentation", "comment": null, "summary": "We address the issue of defining a semantics for deontic argumentation that\nsupports weak permission. Some recent results show that grounded semantics do\nnot support weak permission when there is a conflict between two obligations.\nWe provide a definition of Deontic Argumentation Theory that accounts for weak\npermission, and we recall the result about grounded semantics. Then, we propose\na new semantics that supports weak permission.", "AI": {"tldr": "This paper addresses the problem of defining semantics for deontic argumentation that supports weak permission, particularly when conflicts between obligations arise.", "motivation": "Recent research has shown that grounded semantics fail to support weak permission in cases where there are conflicts between obligations, highlighting a limitation in current deontic argumentation frameworks.", "method": "The authors propose a new Deontic Argumentation Theory definition that accounts for weak permission and introduce a novel semantics designed to overcome the limitations of grounded semantics.", "result": "The paper presents a new semantics that successfully supports weak permission in deontic argumentation, addressing the identified gap in existing approaches.", "conclusion": "The proposed semantics provides a more comprehensive framework for deontic argumentation that properly handles weak permission even in the presence of conflicting obligations."}}
{"id": "2509.25449", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25449", "abs": "https://arxiv.org/abs/2509.25449", "authors": ["Sofiane Ennadir", "Siavash Golkar", "Leopoldo Sarra"], "title": "Joint Embeddings Go Temporal", "comment": "Accepted at the Workshop on Time Series in the Age of Large Models -\n  NeurIPS 2024", "summary": "Self-supervised learning has seen great success recently in unsupervised\nrepresentation learning, enabling breakthroughs in natural language and image\nprocessing. However, these methods often rely on autoregressive and masked\nmodeling, which aim to reproduce masked information in the input, which can be\nvulnerable to the presence of noise or confounding variables. To address this\nproblem, Joint-Embedding Predictive Architectures (JEPA) has been introduced\nwith the aim to perform self-supervised learning in the latent space. To\nleverage these advancements in the domain of time series, we introduce Time\nSeries JEPA (TS-JEPA), an architecture specifically adapted for time series\nrepresentation learning. We validate TS-JEPA on both classification and\nforecasting, showing that it can match or surpass current state-of-the-art\nbaselines on different standard datasets. Notably, our approach demonstrates a\nstrong performance balance across diverse tasks, indicating its potential as a\nrobust foundation for learning general representations. Thus, this work lays\nthe groundwork for developing future time series foundation models based on\nJoint Embedding.", "AI": {"tldr": "TS-JEPA adapts Joint-Embedding Predictive Architectures for time series representation learning, achieving state-of-the-art performance in classification and forecasting tasks while providing robust general representations.", "motivation": "To address limitations of traditional self-supervised learning methods (autoregressive and masked modeling) that are vulnerable to noise and confounding variables in time series data.", "method": "Proposes Time Series JEPA (TS-JEPA), a Joint-Embedding Predictive Architecture specifically adapted for time series that performs self-supervised learning in latent space rather than input space.", "result": "TS-JEPA matches or surpasses current state-of-the-art baselines on different standard datasets for both classification and forecasting tasks, demonstrating strong performance balance across diverse tasks.", "conclusion": "The work establishes TS-JEPA as a robust foundation for learning general time series representations and lays groundwork for developing future time series foundation models based on Joint Embedding architectures."}}
{"id": "2509.26126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26126", "abs": "https://arxiv.org/abs/2509.26126", "authors": ["Xinbei Ma", "Ruotian Ma", "Xingyu Chen", "Zhengliang Shi", "Mengru Wang", "Jen-tse Huang", "Qu Yang", "Wenxuan Wang", "Fanghua Ye", "Qingxuan Jiang", "Mengfei Zhou", "Zhuosheng Zhang", "Rui Wang", "Hai Zhao", "Zhaopeng Tu", "Xiaolong Li", "Linus"], "title": "The Hunger Game Debate: On the Emergence of Over-Competition in Multi-Agent Systems", "comment": null, "summary": "LLM-based multi-agent systems demonstrate great potential for tackling\ncomplex problems, but how competition shapes their behavior remains\nunderexplored. This paper investigates the over-competition in multi-agent\ndebate, where agents under extreme pressure exhibit unreliable, harmful\nbehaviors that undermine both collaboration and task performance. To study this\nphenomenon, we propose HATE, the Hunger Game Debate, a novel experimental\nframework that simulates debates under a zero-sum competition arena. Our\nexperiments, conducted across a range of LLMs and tasks, reveal that\ncompetitive pressure significantly stimulates over-competition behaviors and\ndegrades task performance, causing discussions to derail. We further explore\nthe impact of environmental feedback by adding variants of judges, indicating\nthat objective, task-focused feedback effectively mitigates the\nover-competition behaviors. We also probe the post-hoc kindness of LLMs and\nform a leaderboard to characterize top LLMs, providing insights for\nunderstanding and governing the emergent social dynamics of AI community.", "AI": {"tldr": "This paper studies over-competition in LLM-based multi-agent debates, where extreme pressure causes unreliable and harmful behaviors that degrade task performance. The authors propose HATE framework to simulate zero-sum competition and find that objective feedback can mitigate these issues.", "motivation": "To investigate how competition shapes behavior in LLM-based multi-agent systems, particularly the phenomenon of over-competition where agents under pressure exhibit harmful behaviors that undermine collaboration and task performance.", "method": "Proposed HATE (Hunger Game Debate), a novel experimental framework simulating debates under zero-sum competition arena. Conducted experiments across various LLMs and tasks, and explored the impact of environmental feedback by adding different types of judges.", "result": "Competitive pressure significantly stimulates over-competition behaviors and degrades task performance, causing discussions to derail. Objective, task-focused feedback effectively mitigates over-competition behaviors. The study also characterized top LLMs through post-hoc kindness analysis.", "conclusion": "The research provides insights for understanding and governing emergent social dynamics in AI communities, highlighting both the risks of over-competition in multi-agent systems and potential mitigation strategies through appropriate environmental feedback."}}
{"id": "2509.25792", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25792", "abs": "https://arxiv.org/abs/2509.25792", "authors": ["Alexander Branch", "Omead Pooladzandi", "Radin Khosraviani", "Sunay Gajanan Bhat", "Jeffrey Jiang", "Gregory Pottie"], "title": "PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks", "comment": null, "summary": "We introduce PureVQ-GAN, a defense against data poisoning that forces\nbackdoor triggers through a discrete bottleneck using Vector-Quantized VAE with\nGAN discriminator. By quantizing poisoned images through a learned codebook,\nPureVQ-GAN destroys fine-grained trigger patterns while preserving semantic\ncontent. A GAN discriminator ensures outputs match the natural image\ndistribution, preventing reconstruction of out-of-distribution perturbations.\nOn CIFAR-10, PureVQ-GAN achieves 0% poison success rate (PSR) against Gradient\nMatching and Bullseye Polytope attacks, and 1.64% against Narcissus while\nmaintaining 91-95% clean accuracy. Unlike diffusion-based defenses requiring\nhundreds of iterative refinement steps, PureVQ-GAN is over 50x faster, making\nit practical for real training pipelines.", "AI": {"tldr": "PureVQ-GAN is a fast defense against data poisoning attacks that uses vector quantization to destroy backdoor triggers while maintaining clean accuracy.", "motivation": "To develop an efficient defense against data poisoning attacks that can destroy backdoor triggers in poisoned datasets without requiring computationally expensive iterative refinement steps like diffusion models.", "method": "Uses Vector-Quantized VAE with GAN discriminator to force poisoned images through a discrete bottleneck, quantizing them through a learned codebook to destroy fine-grained trigger patterns while preserving semantic content.", "result": "Achieves 0% poison success rate against Gradient Matching and Bullseye Polytope attacks, 1.64% against Narcissus on CIFAR-10, while maintaining 91-95% clean accuracy. Over 50x faster than diffusion-based defenses.", "conclusion": "PureVQ-GAN provides an effective and practical defense against data poisoning attacks that is significantly faster than existing methods while maintaining high clean accuracy."}}
{"id": "2509.25466", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25466", "abs": "https://arxiv.org/abs/2509.25466", "authors": ["Haotian Fu", "Ran Gong", "Xiaohan Zhang", "Maria Vittoria Minniti", "Jigarkumar Patel", "Karl Schmeckpeper"], "title": "Data-Efficient Multitask DAgger", "comment": null, "summary": "Generalist robot policies that can perform many tasks typically require\nextensive expert data or simulations for training. In this work, we propose a\nnovel Data-Efficient multitask DAgger framework that distills a single\nmultitask policy from multiple task-specific expert policies. Our approach\nsignificantly increases the overall task success rate by actively focusing on\ntasks where the multitask policy underperforms. The core of our method is a\nperformance-aware scheduling strategy that tracks how much each task's learning\nprocess benefits from the amount of data, using a Kalman filter-based estimator\nto robustly decide how to allocate additional demonstrations across tasks. We\nvalidate our approach on MetaWorld, as well as a suite of diverse\ndrawer-opening tasks in IsaacLab. The resulting policy attains high performance\nacross all tasks while using substantially fewer expert demonstrations, and the\nvisual policy learned with our method in simulation shows better performance\nthan naive DAgger and Behavior Cloning when transferring zero-shot to a real\nrobot without using real data.", "AI": {"tldr": "A data-efficient multitask DAgger framework that distills a single multitask policy from multiple task-specific expert policies using performance-aware scheduling with Kalman filter-based estimation.", "motivation": "Generalist robot policies typically require extensive expert data or simulations for training, which is inefficient and costly.", "method": "Proposes a Data-Efficient multitask DAgger framework with performance-aware scheduling strategy using Kalman filter-based estimator to allocate demonstrations across tasks based on learning benefits.", "result": "Significantly increases overall task success rate, attains high performance across all tasks using substantially fewer expert demonstrations, and shows better zero-shot transfer to real robots compared to naive DAgger and Behavior Cloning.", "conclusion": "The proposed framework enables efficient learning of multitask policies with minimal expert data while maintaining high performance and effective real-world transfer."}}
{"id": "2509.26136", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26136", "abs": "https://arxiv.org/abs/2509.26136", "authors": ["Paul Grundmann", "Dennis Fast", "Jan Frick", "Thomas Steffek", "Felix Gers", "Wolfgang Nejdl", "Alexander L\u00f6ser"], "title": "CliniBench: A Clinical Outcome Prediction Benchmark for Generative and Encoder-Based Language Models", "comment": null, "summary": "With their growing capabilities, generative large language models (LLMs) are\nbeing increasingly investigated for complex medical tasks. However, their\neffectiveness in real-world clinical applications remains underexplored. To\naddress this, we present CliniBench, the first benchmark that enables\ncomparability of well-studied encoder-based classifiers and generative LLMs for\ndischarge diagnosis prediction from admission notes in MIMIC-IV dataset. Our\nextensive study compares 12 generative LLMs and 3 encoder-based classifiers and\ndemonstrates that encoder-based classifiers consistently outperform generative\nmodels in diagnosis prediction. We assess several retrieval augmentation\nstrategies for in-context learning from similar patients and find that they\nprovide notable performance improvements for generative LLMs.", "AI": {"tldr": "CliniBench is the first benchmark comparing encoder-based classifiers and generative LLMs for discharge diagnosis prediction from admission notes, showing encoders outperform LLMs, though retrieval augmentation helps LLMs.", "motivation": "To investigate the effectiveness of generative LLMs in real-world clinical applications, particularly for discharge diagnosis prediction, as their capabilities in complex medical tasks remain underexplored.", "method": "Created CliniBench benchmark using MIMIC-IV dataset to compare 12 generative LLMs and 3 encoder-based classifiers for discharge diagnosis prediction from admission notes, and assessed retrieval augmentation strategies for in-context learning.", "result": "Encoder-based classifiers consistently outperform generative models in diagnosis prediction. Retrieval augmentation strategies provide notable performance improvements for generative LLMs.", "conclusion": "While generative LLMs show promise, encoder-based classifiers remain superior for discharge diagnosis prediction, though retrieval augmentation can enhance LLM performance in clinical applications."}}
{"id": "2509.25835", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25835", "abs": "https://arxiv.org/abs/2509.25835", "authors": ["Xinzhe Li"], "title": "Chain-in-Tree: Back to Sequential Reasoning in LLM Tree Search", "comment": "Under Review", "summary": "Test-time scaling enables large language models (LLMs) to improve performance\non long-horizon reasoning tasks by allocating additional compute at inference.\nTree-search-based approaches achieve state-of-the-art results in this setting,\nbut they are notoriously inefficient, often an order of magnitude slower than\nsimpler iterative methods. We introduce Chain-in-Tree (CiT), a plug-in\nframework that adaptively decides when to branch during search rather than\nbranching at every step. CiT relies on lightweight Branching Necessity (BN)\nevaluation methods: BN-DP (Direct Prompting), where an auxiliary LLM directly\njudges whether a step requires branching, and BN-SC (Self-Consistency), which\nclusters multiple candidate actions to estimate agreement. We integrate CiT\ninto three representative LLM-in-the-loop tree search frameworks: Tree of\nThoughts (ToT-BS), ReST-MCTS, and RAP, and evaluate across GSM8K and Math500.\nOur results show that: (1) BN-DP consistently reduces token generation, model\ninvocations, and runtime by 75-85 percent across all settings, with negligible\naccuracy loss and sometimes accuracy gains; (2) BN-SC typically yields\nsubstantial savings (up to 80 percent) but shows instability in 1-4 out of 14\nsettings, caused by a small subset of examples that produce very long reasoning\nsteps; (3) the quality of auxiliary LLMs is critical, not only the BN evaluator\nin BN-DP, but also the models used in BN-SC for clustering and equivalence\nchecking. When these roles are filled by smaller LLMs, performance degrades.\nImportantly, BN-SC does not require LLMs in domains with deterministic action\nspaces, where clustering can be done programmatically. We also provide a\ntheoretical guarantee that BN-DP never increases LLM invocations relative to\nthe baseline and release a unified implementation of CiT across ToT-BS,\nReST-MCTS, and RAP to facilitate reproducibility and extension.", "AI": {"tldr": "Chain-in-Tree (CiT) is a plug-in framework that reduces computational costs in tree-search-based LLM reasoning by adaptively deciding when to branch during search, achieving 75-85% reductions in tokens, model invocations, and runtime with minimal accuracy loss.", "motivation": "Tree-search-based approaches for LLM reasoning are inefficient, often being an order of magnitude slower than simpler iterative methods, creating a need for more efficient search strategies.", "method": "CiT uses lightweight Branching Necessity (BN) evaluation methods: BN-DP where an auxiliary LLM directly judges branching necessity, and BN-SC which clusters candidate actions to estimate agreement. It's integrated into three tree search frameworks: ToT-BS, ReST-MCTS, and RAP.", "result": "BN-DP consistently reduces token generation, model invocations, and runtime by 75-85% across all settings with negligible accuracy loss. BN-SC achieves up to 80% savings but shows instability in some settings. Auxiliary LLM quality is critical for performance.", "conclusion": "CiT provides significant efficiency improvements for tree-search-based LLM reasoning while maintaining accuracy, with theoretical guarantees that BN-DP never increases LLM invocations relative to baseline."}}
{"id": "2509.25473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25473", "abs": "https://arxiv.org/abs/2509.25473", "authors": ["Danyang Li", "Yixuan Wang", "Matthew Cleaveland", "Mingyu Cai", "Roberto Tron"], "title": "Conformal Prediction for Signal Temporal Logic Inference", "comment": null, "summary": "Signal Temporal Logic (STL) inference seeks to extract human-interpretable\nrules from time-series data, but existing methods lack formal confidence\nguarantees for the inferred rules. Conformal prediction (CP) is a technique\nthat can provide statistical correctness guarantees, but is typically applied\nas a post-training wrapper without improving model learning. Instead, we\nintroduce an end-to-end differentiable CP framework for STL inference that\nenhances both reliability and interpretability of the resulting formulas. We\nintroduce a robustness-based nonconformity score, embed a smooth CP layer\ndirectly into training, and employ a new loss function that simultaneously\noptimizes inference accuracy and CP prediction sets with a single term.\nFollowing training, an exact CP procedure delivers statistical guarantees for\nthe learned STL formulas. Experiments on benchmark time-series tasks show that\nour approach reduces uncertainty in predictions (i.e., it achieves high\ncoverage while reducing prediction set size), and improves accuracy (i.e., the\nnumber of misclassifications when using a fixed threshold) over\nstate-of-the-art baselines.", "AI": {"tldr": "This paper introduces an end-to-end differentiable conformal prediction framework for Signal Temporal Logic (STL) inference that provides statistical guarantees for learned temporal rules while improving both reliability and interpretability.", "motivation": "Existing STL inference methods lack formal confidence guarantees for inferred rules, and standard conformal prediction is typically applied as a post-training wrapper without improving model learning.", "method": "The approach uses a robustness-based nonconformity score, embeds a smooth CP layer directly into training, and employs a novel loss function that simultaneously optimizes inference accuracy and CP prediction sets with a single term.", "result": "Experiments show the method reduces prediction uncertainty (achieving high coverage while reducing prediction set size) and improves accuracy (reducing misclassifications when using fixed thresholds) over state-of-the-art baselines.", "conclusion": "The proposed end-to-end differentiable CP framework successfully enhances both reliability and interpretability of STL formulas while providing formal statistical guarantees."}}
{"id": "2509.26160", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26160", "abs": "https://arxiv.org/abs/2509.26160", "authors": ["Gustavo Cilleruelo", "Emily Allaway", "Barry Haddow", "Alexandra Birch"], "title": "MGen: Millions of Naturally Occurring Generics in Context", "comment": "Presented at SCiL 2025", "summary": "MGen is a dataset of over 4 million naturally occurring generic and\nquantified sentences extracted from diverse textual sources. Sentences in the\ndataset have long context documents, corresponding to websites and academic\npapers, and cover 11 different quantifiers. We analyze the features of generics\nsentences in the dataset, with interesting insights: generics can be long\nsentences (averaging over 16 words) and speakers often use them to express\ngeneralisations about people.\n  MGen is the biggest and most diverse dataset of naturally occurring generic\nsentences, opening the door to large-scale computational research on\ngenericity. It is publicly available at https://gustavocilleruelo.com/mgen", "AI": {"tldr": "MGen is a large dataset of 4+ million naturally occurring generic and quantified sentences from diverse sources, with long context documents covering 11 quantifiers, enabling large-scale computational research on genericity.", "motivation": "To create the biggest and most diverse dataset of naturally occurring generic sentences to enable large-scale computational research on genericity, as existing resources were limited.", "method": "Extracted over 4 million generic and quantified sentences from diverse textual sources including websites and academic papers, with long context documents and coverage of 11 different quantifiers.", "result": "Created MGen dataset with interesting insights: generic sentences can be long (averaging over 16 words) and speakers often use them to express generalizations about people. It's the largest and most diverse dataset of naturally occurring generic sentences.", "conclusion": "MGen opens the door to large-scale computational research on genericity and is publicly available, providing a valuable resource for studying generic language in natural contexts."}}
{"id": "2509.25842", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25842", "abs": "https://arxiv.org/abs/2509.25842", "authors": ["Ziyu Zhang", "Hanzhao Li", "Jingbin Hu", "Wenhao Li", "Lei Xie"], "title": "HiStyle: Hierarchical Style Embedding Predictor for Text-Prompt-Guided Controllable Speech Synthesis", "comment": null, "summary": "Controllable speech synthesis refers to the precise control of speaking style\nby manipulating specific prosodic and paralinguistic attributes, such as\ngender, volume, speech rate, pitch, and pitch fluctuation. With the integration\nof advanced generative models, particularly large language models (LLMs) and\ndiffusion models, controllable text-to-speech (TTS) systems have increasingly\ntransitioned from label-based control to natural language description-based\ncontrol, which is typically implemented by predicting global style embeddings\nfrom textual prompts. However, this straightforward prediction overlooks the\nunderlying distribution of the style embeddings, which may hinder the full\npotential of controllable TTS systems. In this study, we use t-SNE analysis to\nvisualize and analyze the global style embedding distribution of various\nmainstream TTS systems, revealing a clear hierarchical clustering pattern:\nembeddings first cluster by timbre and subsequently subdivide into finer\nclusters based on style attributes. Based on this observation, we propose\nHiStyle, a two-stage style embedding predictor that hierarchically predicts\nstyle embeddings conditioned on textual prompts, and further incorporate\ncontrastive learning to help align the text and audio embedding spaces.\nAdditionally, we propose a style annotation strategy that leverages the\ncomplementary strengths of statistical methodologies and human auditory\npreferences to generate more accurate and perceptually consistent textual\nprompts for style control. Comprehensive experiments demonstrate that when\napplied to the base TTS model, HiStyle achieves significantly better style\ncontrollability than alternative style embedding predicting approaches while\npreserving high speech quality in terms of naturalness and intelligibility.\nAudio samples are available at https://anonymous.4open.science/w/HiStyle-2517/.", "AI": {"tldr": "HiStyle is a two-stage hierarchical style embedding predictor for controllable text-to-speech that improves style controllability by modeling the hierarchical clustering pattern in style embeddings and using contrastive learning to align text and audio spaces.", "motivation": "Current TTS systems using natural language prompts for style control overlook the underlying hierarchical distribution of style embeddings, which limits their full potential for precise style manipulation.", "method": "Proposed HiStyle with two-stage hierarchical style embedding prediction based on t-SNE analysis findings, incorporating contrastive learning and a hybrid style annotation strategy combining statistical methods and human preferences.", "result": "HiStyle achieves significantly better style controllability than alternative approaches while maintaining high speech quality in naturalness and intelligibility.", "conclusion": "Modeling the hierarchical structure of style embeddings through two-stage prediction and proper text-audio alignment enables more effective and precise control in text-to-speech synthesis."}}
{"id": "2509.25480", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25480", "abs": "https://arxiv.org/abs/2509.25480", "authors": ["Hui Ji", "Wei Gao", "Pengfei Zhou"], "title": "Translation from Wearable PPG to 12-Lead ECG", "comment": "14 pages,10 figures", "summary": "The 12-lead electrocardiogram (ECG) is the gold standard for cardiovascular\nmonitoring, offering superior diagnostic granularity and specificity compared\nto photoplethysmography (PPG). However, existing 12-lead ECG systems rely on\ncumbersome multi-electrode setups, limiting sustained monitoring in ambulatory\nsettings, while current PPG-based methods fail to reconstruct multi-lead ECG\ndue to the absence of inter-lead constraints and insufficient modeling of\nspatial-temporal dependencies across leads. To bridge this gap, we introduce\nP2Es, an innovative demographic-aware diffusion framework designed to generate\nclinically valid 12-lead ECG from PPG signals via three key innovations.\nSpecifically, in the forward process, we introduce frequency-domain blurring\nfollowed by temporal noise interference to simulate real-world signal\ndistortions. In the reverse process, we design a temporal multi-scale\ngeneration module followed by frequency deblurring. In particular, we leverage\nKNN-based clustering combined with contrastive learning to assign affinity\nmatrices for the reverse process, enabling demographic-specific ECG\ntranslation. Extensive experimental results show that P2Es outperforms baseline\nmodels in 12-lead ECG reconstruction.", "AI": {"tldr": "P2Es is a demographic-aware diffusion framework that generates clinically valid 12-lead ECG from PPG signals using frequency-domain blurring, temporal noise interference, and KNN-based clustering with contrastive learning.", "motivation": "Existing 12-lead ECG systems are cumbersome for ambulatory monitoring, while PPG-based methods fail to reconstruct multi-lead ECG due to lack of inter-lead constraints and insufficient spatial-temporal modeling.", "method": "Uses demographic-aware diffusion framework with forward process (frequency-domain blurring + temporal noise interference) and reverse process (temporal multi-scale generation + frequency deblurring), leveraging KNN clustering and contrastive learning for demographic-specific translation.", "result": "Extensive experiments show P2Es outperforms baseline models in 12-lead ECG reconstruction.", "conclusion": "P2Es successfully bridges the gap between cumbersome 12-lead ECG systems and limited PPG methods by providing clinically valid ECG reconstruction from PPG signals."}}
{"id": "2509.26181", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26181", "abs": "https://arxiv.org/abs/2509.26181", "authors": ["Mariia Fedorova", "Andrey Kutuzov", "Francesco Periti", "Yves Scherrer"], "title": "Explaining novel senses using definition generation with open language models", "comment": null, "summary": "We apply definition generators based on open-weights large language models to\nthe task of creating explanations of novel senses, taking target word usages as\nan input. To this end, we employ the datasets from the AXOLOTL'24 shared task\non explainable semantic change modeling, which features Finnish, Russian and\nGerman languages. We fine-tune and provide publicly the open-source models\nperforming higher than the best submissions of the aforementioned shared task,\nwhich employed closed proprietary LLMs. In addition, we find that\nencoder-decoder definition generators perform on par with their decoder-only\ncounterparts.", "AI": {"tldr": "Open-weights LLMs fine-tuned for novel sense definition generation outperform proprietary LLMs in AXOLOTL'24 shared task across Finnish, Russian, and German languages.", "motivation": "To create explanations for novel word senses using open-source models that can outperform proprietary LLMs in semantic change modeling tasks.", "method": "Fine-tuned open-weights large language models as definition generators, using target word usages as input on AXOLOTL'24 datasets for Finnish, Russian, and German languages.", "result": "The fine-tuned open-source models performed higher than the best submissions from the shared task that used closed proprietary LLMs. Encoder-decoder models performed on par with decoder-only models.", "conclusion": "Open-weights LLMs can effectively generate novel sense definitions and compete with proprietary models, with encoder-decoder architectures showing comparable performance to decoder-only approaches."}}
{"id": "2509.25843", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25843", "abs": "https://arxiv.org/abs/2509.25843", "authors": ["Yein Park", "Jungwoo Park", "Jaewoo Kang"], "title": "ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack", "comment": null, "summary": "Large language models (LLMs), despite being safety-aligned, exhibit brittle\nrefusal behaviors that can be circumvented by simple linguistic changes. As\ntense jailbreaking demonstrates that models refusing harmful requests often\ncomply when rephrased in past tense, a critical generalization gap is revealed\nin current alignment methods whose underlying mechanisms are poorly understood.\nIn this work, we introduce Activation-Scaling Guard (ASGuard), an insightful,\nmechanistically-informed framework that surgically mitigates this specific\nvulnerability. For the first step, we use circuit analysis to identify the\nspecific attention heads causally linked to the targeted jailbreaking, the\ntense-changing attack. Second, we train a precise, channel-wise scaling vector\nto recalibrate the activation of tense vulnerable heads. Lastly, we apply it\ninto a \"preventative fine-tuning\", forcing the model to learn a more robust\nrefusal mechanism. Across three LLMs, ASGuard effectively reduces the attack\nsuccess rate of targeted jailbreaking while preserving general capabilities and\nminimizing over refusal, achieving a Pareto-optimal balance between safety and\nutility. Our findings underscore how adversarial suffixes suppress the\npropagation of the refusal-mediating direction, based on mechanistic analysis.\nFurthermore, our work showcases how a deep understanding of model internals can\nbe leveraged to develop practical, efficient, and targeted methods for\nadjusting model behavior, charting a course for more reliable and interpretable\nAI safety.", "AI": {"tldr": "ASGuard is a framework that surgically mitigates tense-based jailbreaking in LLMs by identifying vulnerable attention heads and applying channel-wise scaling to strengthen refusal mechanisms.", "motivation": "LLMs exhibit brittle refusal behaviors that can be circumvented by simple linguistic changes like tense modifications, revealing critical gaps in current alignment methods.", "method": "Three-step approach: 1) Circuit analysis to identify tense-vulnerable attention heads, 2) Train channel-wise scaling vectors to recalibrate activations, 3) Apply preventative fine-tuning for robust refusal mechanisms.", "result": "ASGuard effectively reduces attack success rate of targeted jailbreaking across three LLMs while preserving general capabilities and minimizing over-refusal, achieving Pareto-optimal safety-utility balance.", "conclusion": "Deep understanding of model internals enables practical, efficient targeted behavior adjustment, charting course for more reliable and interpretable AI safety."}}
{"id": "2509.25487", "categories": ["cs.LG", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.25487", "abs": "https://arxiv.org/abs/2509.25487", "authors": ["Dingyi Kang", "Dongming Jiang", "Hanshen Yang", "Hang Liu", "Bingzhe Li"], "title": "Scalable Disk-Based Approximate Nearest Neighbor Search with Page-Aligned Graph", "comment": null, "summary": "Approximate Nearest Neighbor Search (ANNS), as the core of vector databases\n(VectorDBs), has become widely used in modern AI and ML systems, powering\napplications from information retrieval to bio-informatics. While graph-based\nANNS methods achieve high query efficiency, their scalability is constrained by\nthe available host memory. Recent disk-based ANNS approaches mitigate memory\nusage by offloading data to Solid-State Drives (SSDs). However, they still\nsuffer from issues such as long I/O traversal path, misalignment with storage\nI/O granularity, and high in-memory indexing overhead, leading to significant\nI/O latency and ultimately limiting scalability for large-scale vector search.\n  In this paper, we propose PageANN, a disk-based approximate nearest neighbor\nsearch (ANNS) framework designed for high performance and scalability. PageANN\nintroduces a page-node graph structure that aligns logical graph nodes with\nphysical SSD pages, thereby shortening I/O traversal paths and reducing I/O\noperations. Specifically, similar vectors are clustered into page nodes, and a\nco-designed disk data layout leverages this structure with a merging technique\nto store only representative vectors and topology information, avoiding\nunnecessary reads. To further improve efficiency, we design a memory management\nstrategy that combines lightweight indexing with coordinated memory-disk data\nallocation, maximizing host memory utilization while minimizing query latency\nand storage overhead. Experimental results show that PageANN significantly\noutperforms state-of-the-art (SOTA) disk-based ANNS methods, achieving\n1.85x-10.83x higher throughput and 51.7%-91.9% lower latency across different\ndatasets and memory budgets, while maintaining comparable high recall accuracy.", "AI": {"tldr": "PageANN is a disk-based ANNS framework that uses a page-node graph structure aligned with SSD pages to reduce I/O operations and improve scalability for large-scale vector search.", "motivation": "Existing disk-based ANNS methods suffer from long I/O traversal paths, misalignment with storage I/O granularity, and high in-memory indexing overhead, limiting scalability for large-scale vector search.", "method": "Introduces page-node graph structure aligning logical nodes with physical SSD pages, clusters similar vectors into page nodes, uses co-designed disk data layout with merging technique, and implements memory management strategy combining lightweight indexing with coordinated memory-disk data allocation.", "result": "Achieves 1.85x-10.83x higher throughput and 51.7%-91.9% lower latency compared to state-of-the-art disk-based ANNS methods across different datasets and memory budgets, while maintaining comparable high recall accuracy.", "conclusion": "PageANN significantly outperforms existing disk-based ANNS approaches, providing high performance and scalability for large-scale vector search applications."}}
{"id": "2509.26189", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26189", "abs": "https://arxiv.org/abs/2509.26189", "authors": ["Trieu Hai Nguyen", "Sivaswamy Akilesh"], "title": "VietBinoculars: A Zero-Shot Approach for Detecting Vietnamese LLM-Generated Text", "comment": "27 pages", "summary": "The rapid development research of Large Language Models (LLMs) based on\ntransformer architectures raises key challenges, one of them being the task of\ndistinguishing between human-written text and LLM-generated text. As\nLLM-generated textual content, becomes increasingly complex over time, and\nresembles human writing, traditional detection methods are proving less\neffective, especially as the number and diversity of LLMs continue to grow with\nnew models and versions being released at a rapid pace. This study proposes\nVietBinoculars, an adaptation of the Binoculars method with optimized global\nthresholds, to enhance the detection of Vietnamese LLM-generated text. We have\nconstructed new Vietnamese AI-generated datasets to determine the optimal\nthresholds for VietBinoculars and to enable benchmarking. The results from our\nexperiments show results show that VietBinoculars achieves over 99\\% in all two\ndomains of accuracy, F1-score, and AUC on multiple out-of-domain datasets. It\noutperforms the original Binoculars model, traditional detection methods, and\nother state-of-the-art approaches, including commercial tools such as ZeroGPT\nand DetectGPT, especially under specially modified prompting strategies.", "AI": {"tldr": "VietBinoculars is an optimized adaptation of the Binoculars method with improved global thresholds for detecting Vietnamese LLM-generated text, achieving over 99% accuracy across multiple metrics and outperforming existing methods.", "motivation": "Traditional detection methods are becoming less effective as LLM-generated text becomes more complex and human-like, especially with the rapid growth and diversity of LLM models in Vietnamese language contexts.", "method": "Adapted the Binoculars method with optimized global thresholds, constructed new Vietnamese AI-generated datasets for threshold determination and benchmarking.", "result": "VietBinoculars achieves over 99% accuracy, F1-score, and AUC on multiple out-of-domain datasets, outperforming original Binoculars, traditional methods, state-of-the-art approaches, and commercial tools like ZeroGPT and DetectGPT.", "conclusion": "The optimized VietBinoculars method is highly effective for detecting Vietnamese LLM-generated text, demonstrating superior performance compared to existing detection approaches."}}
{"id": "2509.25858", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25858", "abs": "https://arxiv.org/abs/2509.25858", "authors": ["Yi-chen Yao", "Jerry Wang", "Yi-cheng Lai", "Lyn Chao-ling Chen"], "title": "Aging Decline in Basketball Career Trend Prediction Based on Machine Learning and LSTM Model", "comment": "Accepted at Taiwan Academic Network Conference, TANET 2025", "summary": "The topic of aging decline on performance of NBA players has been discussed\nin this study. The autoencoder with K-means clustering machine learning method\nwas adopted to career trend classification of NBA players, and the LSTM deep\nlearning method was adopted in performance prediction of each NBA player. The\ndataset was collected from the basketball game data of veteran NBA players. The\ncontribution of the work performed better than the other methods with\ngeneralization ability for evaluating various types of NBA career trend, and\ncan be applied in different types of sports in the field of sport analytics.", "AI": {"tldr": "This study uses autoencoder with K-means clustering for NBA player career trend classification and LSTM for performance prediction, achieving better results than other methods with good generalization ability.", "motivation": "To analyze aging decline on performance of NBA players and develop methods for career trend classification and performance prediction.", "method": "Autoencoder with K-means clustering for career trend classification and LSTM deep learning for performance prediction, using veteran NBA players' game data.", "result": "The proposed method performed better than other methods with generalization ability for evaluating various types of NBA career trends.", "conclusion": "The approach can be applied to different types of sports in the field of sport analytics."}}
{"id": "2509.25509", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.25509", "abs": "https://arxiv.org/abs/2509.25509", "authors": ["Langzhou He", "Junyou Zhu", "Fangxin Wang", "Junhua Liu", "Haoyan Xu", "Yue Zhao", "Philip S. Yu", "Qitian Wu"], "title": "Can Molecular Foundation Models Know What They Don't Know? A Simple Remedy with Preference Optimization", "comment": null, "summary": "Molecular foundation models are rapidly advancing scientific discovery, but\ntheir unreliability on out-of-distribution (OOD) samples severely limits their\napplication in high-stakes domains such as drug discovery and protein design. A\ncritical failure mode is chemical hallucination, where models make\nhigh-confidence yet entirely incorrect predictions for unknown molecules. To\naddress this challenge, we introduce Molecular Preference-Aligned Instance\nRanking (Mole-PAIR), a simple, plug-and-play module that can be flexibly\nintegrated with existing foundation models to improve their reliability on OOD\ndata through cost-effective post-training. Specifically, our method formulates\nthe OOD detection problem as a preference optimization over the estimated OOD\naffinity between in-distribution (ID) and OOD samples, achieving this goal\nthrough a pairwise learning objective. We show that this objective essentially\noptimizes AUROC, which measures how consistently ID and OOD samples are ranked\nby the model. Extensive experiments across five real-world molecular datasets\ndemonstrate that our approach significantly improves the OOD detection\ncapabilities of existing molecular foundation models, achieving up to 45.8%,\n43.9%, and 24.3% improvements in AUROC under distribution shifts of size,\nscaffold, and assay, respectively.", "AI": {"tldr": "Mole-PAIR is a plug-and-play module that improves molecular foundation models' reliability on out-of-distribution data through preference optimization-based OOD detection.", "motivation": "Molecular foundation models suffer from unreliability on OOD samples, particularly chemical hallucination where they make high-confidence incorrect predictions for unknown molecules, limiting their application in high-stakes domains like drug discovery.", "method": "Formulates OOD detection as preference optimization over estimated OOD affinity between ID and OOD samples using pairwise learning objective, which essentially optimizes AUROC to consistently rank ID and OOD samples.", "result": "Significantly improves OOD detection capabilities across five molecular datasets, achieving up to 45.8%, 43.9%, and 24.3% AUROC improvements under size, scaffold, and assay distribution shifts respectively.", "conclusion": "Mole-PAIR provides an effective, cost-effective post-training solution to enhance reliability of molecular foundation models on OOD data through preference-aligned instance ranking."}}
{"id": "2509.26216", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26216", "abs": "https://arxiv.org/abs/2509.26216", "authors": ["Assem Omar", "Youssef Omar", "Marwa Solayman", "Hesham Mansour"], "title": "Comparative Analysis of Ant Colony Optimization and Google OR-Tools for Solving the Open Capacitated Vehicle Routing Problem in Logistics", "comment": "6 pages, accepted at Intelligent Methods, Systems, and Applications\n  (IMSA 2025)", "summary": "In modern logistics management systems, route planning requires high\nefficiency. The Open Capacitated Vehicle Routing Problem (OCVRP) deals with\nfinding optimal delivery routes for a fleet of vehicles serving geographically\ndistributed customers, without requiring the vehicles to return to the depot\nafter deliveries. The present study is comparative in nature and speaks of two\nalgorithms for OCVRP solution: Ant Colony Optimization (ACO), a nature-inspired\nmetaheuristic; and Google OR-Tools, an industry-standard toolkit for\noptimization. Both implementations were developed in Python and using a custom\ndataset. Performance appraisal was based on routing efficiency, computation\ntime, and scalability. The results show that ACO allows flexibility in routing\nparameters while OR-Tools runs much faster with more consistency and requires\nless input. This could help choose among routing strategies for scalable\nreal-time logistics systems.", "AI": {"tldr": "Comparative study of Ant Colony Optimization (ACO) and Google OR-Tools for solving Open Capacitated Vehicle Routing Problem (OCVRP), showing ACO offers routing flexibility while OR-Tools provides faster, more consistent performance.", "motivation": "Modern logistics systems require efficient route planning, particularly for OCVRP where vehicles don't return to depot after deliveries, necessitating comparison of different optimization approaches.", "method": "Implemented both ACO (nature-inspired metaheuristic) and Google OR-Tools in Python using custom dataset, evaluating performance based on routing efficiency, computation time, and scalability.", "result": "ACO provides flexibility in routing parameters, while OR-Tools runs significantly faster with more consistency and requires less input complexity.", "conclusion": "The comparison helps select appropriate routing strategies for scalable real-time logistics systems based on specific requirements - flexibility vs speed/consistency."}}
{"id": "2509.25862", "categories": ["cs.AI", "cs.AR", "cs.ET", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.25862", "abs": "https://arxiv.org/abs/2509.25862", "authors": ["Olga Krestinskaya", "Mohammed E. Fouda", "Ahmed Eltawil", "Khaled N. Salama"], "title": "CIMNAS: A Joint Framework for Compute-In-Memory-Aware Neural Architecture Search", "comment": null, "summary": "To maximize hardware efficiency and performance accuracy in Compute-In-Memory\n(CIM)-based neural network accelerators for Artificial Intelligence (AI)\napplications, co-optimizing both software and hardware design parameters is\nessential. Manual tuning is impractical due to the vast number of parameters\nand their complex interdependencies. To effectively automate the design and\noptimization of CIM-based neural network accelerators, hardware-aware neural\narchitecture search (HW-NAS) techniques can be applied. This work introduces\nCIMNAS, a joint model-quantization-hardware optimization framework for CIM\narchitectures. CIMNAS simultaneously searches across software parameters,\nquantization policies, and a broad range of hardware parameters, incorporating\ndevice-, circuit-, and architecture-level co-optimizations. CIMNAS experiments\nwere conducted over a search space of 9.9x10^85 potential parameter\ncombinations with the MobileNet model as a baseline and RRAM-based CIM\narchitecture. Evaluated on the ImageNet dataset, CIMNAS achieved a reduction in\nenergy-delay-area product (EDAP) ranging from 90.1x to 104.5x, an improvement\nin TOPS/W between 4.68x and 4.82x, and an enhancement in TOPS/mm^2 from 11.3x\nto 12.78x relative to various baselines, all while maintaining an accuracy of\n73.81%. The adaptability and robustness of CIMNAS are demonstrated by extending\nthe framework to support the SRAM-based ResNet50 architecture, achieving up to\nan 819.5x reduction in EDAP. Unlike other state-of-the-art methods, CIMNAS\nachieves EDAP-focused optimization without any accuracy loss, generating\ndiverse software-hardware parameter combinations for high-performance CIM-based\nneural network designs. The source code of CIMNAS is available at\nhttps://github.com/OlgaKrestinskaya/CIMNAS.", "AI": {"tldr": "CIMNAS is a joint model-quantization-hardware optimization framework for CIM-based neural network accelerators that simultaneously searches software parameters, quantization policies, and hardware parameters to achieve significant improvements in energy-delay-area product (EDAP), TOPS/W, and TOPS/mm\u00b2 while maintaining accuracy.", "motivation": "Manual tuning of software and hardware parameters for CIM-based neural network accelerators is impractical due to the vast parameter space and complex interdependencies. Hardware-aware neural architecture search (HW-NAS) is needed to automate design optimization and maximize hardware efficiency and performance accuracy.", "method": "CIMNAS framework simultaneously searches across software parameters, quantization policies, and hardware parameters (device-, circuit-, and architecture-level) for CIM architectures. It explores a massive search space of 9.9x10^85 parameter combinations using MobileNet model with RRAM-based CIM architecture, and also supports SRAM-based ResNet50 architecture.", "result": "On ImageNet dataset, CIMNAS achieved: 90.1x-104.5x reduction in EDAP, 4.68x-4.82x improvement in TOPS/W, 11.3x-12.78x enhancement in TOPS/mm\u00b2 while maintaining 73.81% accuracy. For SRAM-based ResNet50, achieved up to 819.5x reduction in EDAP. No accuracy loss during optimization.", "conclusion": "CIMNAS enables effective joint optimization of software, quantization, and hardware parameters for CIM-based neural network accelerators, achieving significant performance improvements without accuracy degradation. The framework is adaptable to different architectures and generates diverse parameter combinations for high-performance designs."}}
{"id": "2509.25510", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25510", "abs": "https://arxiv.org/abs/2509.25510", "authors": ["Chang Liu", "Danial Chitnis"], "title": "EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit", "comment": null, "summary": "The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often\ninvolves significant manual effort, especially during the transistor sizing\nprocess. While Machine Learning techniques in Electronic Design Automation\n(EDA) have shown promise in reducing complexity and minimizing human\nintervention, they still face challenges such as numerous iterations and a lack\nof knowledge about AMS circuit design. Recently, Large Language Models (LLMs)\nhave demonstrated significant potential across various fields, showing a\ncertain level of knowledge in circuit design and indicating their potential to\nautomate the transistor sizing process. In this work, we propose EEsizer, an\nLLM-based AI agent that integrates large language models with circuit\nsimulators and custom data analysis functions, enabling fully automated,\nclosed-loop transistor sizing without relying on external knowledge. By\nemploying prompt engineering and Chain-of-Thought reasoning, the agent\niteratively explores design directions, evaluates performance, and refines\nsolutions with minimal human intervention. We first benchmarked 8 LLMs on six\nbasic circuits and selected three high-performing models to optimize a\n20-transistor CMOS operational amplifier, targeting multiple performance\nmetrics, including rail-to-rail operation from 180 nm to 90 nm technology\nnodes. Notably, OpenAI o3 successfully achieved the user-intended target at 90\nnm across three different test groups, with a maximum of 20 iterations,\ndemonstrating adaptability and robustness at advanced nodes. To assess design\nrobustness, we manually designed a bias circuit and performed a variation\nanalysis using Gaussian-distributed variations on transistor dimensions and\nthreshold voltages.", "AI": {"tldr": "EEsizer is an LLM-based AI agent that automates transistor sizing for analog circuits by integrating large language models with circuit simulators, achieving successful optimization of a 20-transistor CMOS operational amplifier across multiple technology nodes.", "motivation": "To reduce manual effort in analog circuit design by leveraging LLMs' circuit design knowledge and automating the transistor sizing process without external dependencies.", "method": "Integration of LLMs with circuit simulators and custom data analysis functions using prompt engineering and Chain-of-Thought reasoning for iterative design exploration and refinement.", "result": "OpenAI o3 successfully optimized a 20-transistor CMOS operational amplifier at 90 nm technology node across three test groups within 20 iterations, demonstrating adaptability and robustness.", "conclusion": "LLM-based agents like EEsizer can effectively automate transistor sizing for analog circuits, showing promise for reducing human intervention in AMS IC design."}}
{"id": "2509.26224", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26224", "abs": "https://arxiv.org/abs/2509.26224", "authors": ["Alessandro De Bellis", "Salvatore Bufi", "Giovanni Servedio", "Vito Walter Anelli", "Tommaso Di Noia", "Eugenio Di Sciascio"], "title": "Type-Less yet Type-Aware Inductive Link Prediction with Pretrained Language Models", "comment": "Accepted and to appear in Proceedings of the 2025 Conference on\n  Empirical Methods in Natural Language Processing (EMNLP 2025)", "summary": "Inductive link prediction is emerging as a key paradigm for real-world\nknowledge graphs (KGs), where new entities frequently appear and models must\ngeneralize to them without retraining. Predicting links in a KG faces the\nchallenge of guessing previously unseen entities by leveraging generalizable\nnode features such as subgraph structure, type annotations, and ontological\nconstraints. However, explicit type information is often lacking or incomplete.\nEven when available, type information in most KGs is often coarse-grained,\nsparse, and prone to errors due to human annotation. In this work, we explore\nthe potential of pre-trained language models (PLMs) to enrich node\nrepresentations with implicit type signals. We introduce TyleR, a Type-less yet\ntype-awaRe approach for subgraph-based inductive link prediction that leverages\nPLMs for semantic enrichment. Experiments on standard benchmarks demonstrate\nthat TyleR outperforms state-of-the-art baselines in scenarios with scarce type\nannotations and sparse graph connectivity. To ensure reproducibility, we share\nour code at https://github.com/sisinflab/tyler .", "AI": {"tldr": "TyleR is a type-aware inductive link prediction method that uses pre-trained language models to enrich node representations with implicit type signals, outperforming state-of-the-art methods when type annotations are scarce and graphs are sparse.", "motivation": "Real-world knowledge graphs often lack complete or accurate type information, which is crucial for inductive link prediction where models must generalize to new entities without retraining. Existing type information is typically coarse-grained, sparse, and error-prone.", "method": "TyleR leverages pre-trained language models to enrich node representations with implicit type signals, using a subgraph-based approach for inductive link prediction without requiring explicit type annotations.", "result": "Experiments on standard benchmarks show TyleR outperforms state-of-the-art baselines in scenarios with scarce type annotations and sparse graph connectivity.", "conclusion": "Pre-trained language models can effectively provide implicit type signals for inductive link prediction, enabling better performance even when explicit type information is limited or unavailable."}}
{"id": "2509.25873", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.25873", "abs": "https://arxiv.org/abs/2509.25873", "authors": ["Hankun Dai", "Maoquan Wang", "Mengnan Qi", "Yikai Zhang", "Zijian Jin", "Yongqiang Yao", "Yufan Huang", "Shengyu Fu", "Elsie Nallipogu"], "title": "Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs", "comment": null, "summary": "Large language models (LLMs) are increasingly being applied to programming\ntasks, ranging from single-turn code completion to autonomous agents. Current\ncode agent designs frequently depend on complex, hand-crafted workflows and\ntool sets. However, this reliance on elaborate scaffolding presents several\nchallenges: agent performance becomes overly dependent on prompt tuning and\ncustom design choices, heavy human intervention obscures a model's true\nunderlying capabilities, and intricate pipelines are costly to build and\nmaintain. Furthermore, optimizing complex task prompts increases the risk of\ndata leakage. Currently, when introducing new models, LLM providers like OpenAI\nand Anthropic often publish benchmark scores to demonstrate their models'\ncoding proficiency, but keep their proprietary evaluation frameworks\nconfidential. To address these limitations, we introduce Lita (Lite Agent),\nwhich operationalizes liteness, a principle of minimizing manual design while\nretaining the essential elements of a fully autonomous agent. Lita enables a\nmore faithful and unified evaluation without elaborate scaffolding. Experiments\non the Aider Polyglot and SWE-Bench with frontier models demonstrate that Lita\nachieves competitive or superior performance compared to workflow-based and\nagentic baselines. Crucially, Lita also consumes fewer tokens and requires\nsignificantly less design effort. Our results suggest that Lita is sufficient\nto reveal the underlying coding competence of modern LLMs. Finally, we propose\nthe Agent Complexity Law: the performance gap between agents of varying\ncomplexity, from simple to sophisticated designs, will shrink as the core model\nimproves, ultimately converging to a negligible difference.", "AI": {"tldr": "Lita (Lite Agent) is a minimalist autonomous coding agent that achieves competitive performance with less complexity, fewer tokens, and minimal manual design compared to workflow-based agents.", "motivation": "Current code agents rely on complex, hand-crafted workflows that obscure true model capabilities, require heavy human intervention, are costly to maintain, and risk data leakage through prompt tuning.", "method": "Lita operationalizes 'liteness' - minimizing manual design while retaining essential autonomous agent elements, enabling faithful evaluation without elaborate scaffolding.", "result": "Lita achieves competitive or superior performance on Aider Polyglot and SWE-Bench compared to workflow-based baselines, while consuming fewer tokens and requiring significantly less design effort.", "conclusion": "Lita sufficiently reveals modern LLMs' underlying coding competence and supports the Agent Complexity Law: performance gaps between simple and complex agents shrink as core models improve."}}
{"id": "2509.25518", "categories": ["cs.LG", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.25518", "abs": "https://arxiv.org/abs/2509.25518", "authors": ["Harry Robertshaw", "Han-Ru Wu", "Alejandro Granados", "Thomas C Booth"], "title": "World Model for AI Autonomous Navigation in Mechanical Thrombectomy", "comment": "Published in Medical Image Computing and Computer Assisted\n  Intervention - MICCAI 2025, Lecture Notes in Computer Science, vol 15968", "summary": "Autonomous navigation for mechanical thrombectomy (MT) remains a critical\nchallenge due to the complexity of vascular anatomy and the need for precise,\nreal-time decision-making. Reinforcement learning (RL)-based approaches have\ndemonstrated potential in automating endovascular navigation, but current\nmethods often struggle with generalization across multiple patient vasculatures\nand long-horizon tasks. We propose a world model for autonomous endovascular\nnavigation using TD-MPC2, a model-based RL algorithm. We trained a single RL\nagent across multiple endovascular navigation tasks in ten real patient\nvasculatures, comparing performance against the state-of-the-art Soft\nActor-Critic (SAC) method. Results indicate that TD-MPC2 significantly\noutperforms SAC in multi-task learning, achieving a 65% mean success rate\ncompared to SAC's 37%, with notable improvements in path ratio. TD-MPC2\nexhibited increased procedure times, suggesting a trade-off between success\nrate and execution speed. These findings highlight the potential of world\nmodels for improving autonomous endovascular navigation and lay the foundation\nfor future research in generalizable AI-driven robotic interventions.", "AI": {"tldr": "Proposed TD-MPC2 world model for autonomous mechanical thrombectomy navigation, achieving 65% success rate vs SAC's 37% across multiple patient vasculatures.", "motivation": "Autonomous navigation for mechanical thrombectomy is challenging due to complex vascular anatomy and need for precise real-time decisions. Current RL methods struggle with generalization across patients and long-horizon tasks.", "method": "Used TD-MPC2 model-based RL algorithm to train single agent across multiple endovascular navigation tasks in ten real patient vasculatures, compared with Soft Actor-Critic (SAC).", "result": "TD-MPC2 significantly outperformed SAC in multi-task learning (65% vs 37% mean success rate) with improved path ratio, though with increased procedure times.", "conclusion": "World models show strong potential for improving autonomous endovascular navigation, laying foundation for generalizable AI-driven robotic interventions despite speed-success trade-off."}}
{"id": "2509.26242", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26242", "abs": "https://arxiv.org/abs/2509.26242", "authors": ["Yang Tang", "Ruijie Liu", "Yifan Wang", "Shiyu Li", "Xi Chen"], "title": "Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted Annealing", "comment": "9 pages, 5 figures", "summary": "Large language models (LLMs) fine-tuning shows excellent implications.\nHowever, vanilla fine-tuning methods often require intricate data mixture and\nrepeated experiments for optimal generalization. To address these challenges\nand streamline the training process, we propose an efficient and universal\nsolution, Dynamic Boosted Annealing (DBA). We obtain a global gradient through\nzero-learning-rate training on general data, which is subsequently employed for\ngradient boosting and dynamic training step correction during domain training.\nIn conjunction with annealing learning, we end up establishing a fine-tuning\npipeline that relies solely on domain data without collapse. By evaluating both\ngeneral and domain-specific performance across multiple tasks on several\npopular base models, DBA achieves an average improvement of 5.8% in joint\nperformance over vanilla fine-tuning. Furthermore, since general data is no\nlonger involved in annealing, repeated experiments led by data mixture are also\neliminated. According to our tests, the DBA method can reduce GPU hours by\n91.0% compared to the vanilla method.", "AI": {"tldr": "DBA is an efficient fine-tuning method that uses zero-learning-rate training on general data to obtain global gradients, then applies gradient boosting and dynamic training step correction during domain training with annealing learning, eliminating the need for general data in annealing and reducing GPU hours by 91%.", "motivation": "To address challenges in vanilla fine-tuning methods that require intricate data mixture and repeated experiments for optimal generalization, and to streamline the training process.", "method": "Dynamic Boosted Annealing (DBA) - obtains global gradient through zero-learning-rate training on general data, then uses it for gradient boosting and dynamic training step correction during domain training with annealing learning.", "result": "Achieves average improvement of 5.8% in joint performance over vanilla fine-tuning across multiple tasks on several base models, and reduces GPU hours by 91.0% compared to vanilla method.", "conclusion": "DBA establishes a fine-tuning pipeline that relies solely on domain data without collapse, eliminates repeated experiments from data mixture, and significantly improves efficiency while maintaining performance."}}
{"id": "2509.25885", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25885", "abs": "https://arxiv.org/abs/2509.25885", "authors": ["Ruolin Chen", "Yinqian Sun", "Jihang Wang", "Mingyang Lv", "Qian Zhang", "Yi Zeng"], "title": "SafeMind: Benchmarking and Mitigating Safety Risks in Embodied LLM Agents", "comment": null, "summary": "Embodied agents powered by large language models (LLMs) inherit advanced\nplanning capabilities; however, their direct interaction with the physical\nworld exposes them to safety vulnerabilities. In this work, we identify four\nkey reasoning stages where hazards may arise: Task Understanding, Environment\nPerception, High-Level Plan Generation, and Low-Level Action Generation. We\nfurther formalize three orthogonal safety constraint types (Factual, Causal,\nand Temporal) to systematically characterize potential safety violations.\nBuilding on this risk model, we present SafeMindBench, a multimodal benchmark\nwith 5,558 samples spanning four task categories (Instr-Risk, Env-Risk,\nOrder-Fix, Req-Align) across high-risk scenarios such as sabotage, harm,\nprivacy, and illegal behavior. Extensive experiments on SafeMindBench reveal\nthat leading LLMs (e.g., GPT-4o) and widely used embodied agents remain\nsusceptible to safety-critical failures. To address this challenge, we\nintroduce SafeMindAgent, a modular Planner-Executor architecture integrated\nwith three cascaded safety modules, which incorporate safety constraints into\nthe reasoning process. Results show that SafeMindAgent significantly improves\nsafety rate over strong baselines while maintaining comparable task completion.\nTogether, SafeMindBench and SafeMindAgent provide both a rigorous evaluation\nsuite and a practical solution that advance the systematic study and mitigation\nof safety risks in embodied LLM agents.", "AI": {"tldr": "This paper identifies safety vulnerabilities in embodied LLM agents and proposes SafeMindBench benchmark and SafeMindAgent solution to systematically address safety risks across four reasoning stages and three constraint types.", "motivation": "Embodied agents powered by LLMs have advanced planning capabilities but face safety vulnerabilities when interacting with the physical world, requiring systematic safety analysis and mitigation.", "method": "The authors formalize four reasoning stages (Task Understanding, Environment Perception, High-Level Plan Generation, Low-Level Action Generation) and three safety constraint types (Factual, Causal, Temporal), then create SafeMindBench benchmark with 5,558 samples and propose SafeMindAgent with Planner-Executor architecture and three cascaded safety modules.", "result": "Experiments show leading LLMs and embodied agents remain susceptible to safety-critical failures, while SafeMindAgent significantly improves safety rate over baselines while maintaining comparable task completion.", "conclusion": "SafeMindBench and SafeMindAgent provide both evaluation suite and practical solution for systematic study and mitigation of safety risks in embodied LLM agents."}}
{"id": "2509.25519", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25519", "abs": "https://arxiv.org/abs/2509.25519", "authors": ["Alireza Mousavi-Hosseini", "Stephen Y. Zhang", "Michal Klein", "Marco Cuturi"], "title": "Flow Matching with Semidiscrete Couplings", "comment": "35 pages, 16 figures", "summary": "Flow models parameterized as time-dependent velocity fields can generate data\nfrom noise by integrating an ODE. These models are often trained using flow\nmatching, i.e. by sampling random pairs of noise and target points\n$(\\mathbf{x}_0,\\mathbf{x}_1)$ and ensuring that the velocity field is aligned,\non average, with $\\mathbf{x}_1-\\mathbf{x}_0$ when evaluated along a segment\nlinking $\\mathbf{x}_0$ to $\\mathbf{x}_1$. While these pairs are sampled\nindependently by default, they can also be selected more carefully by matching\nbatches of $n$ noise to $n$ target points using an optimal transport (OT)\nsolver. Although promising in theory, the OT flow matching (OT-FM) approach is\nnot widely used in practice. Zhang et al. (2025) pointed out recently that\nOT-FM truly starts paying off when the batch size $n$ grows significantly,\nwhich only a multi-GPU implementation of the Sinkhorn algorithm can handle.\nUnfortunately, the costs of running Sinkhorn can quickly balloon, requiring\n$O(n^2/\\varepsilon^2)$ operations for every $n$ pairs used to fit the velocity\nfield, where $\\varepsilon$ is a regularization parameter that should be\ntypically small to yield better results. To fulfill the theoretical promises of\nOT-FM, we propose to move away from batch-OT and rely instead on a semidiscrete\nformulation that leverages the fact that the target dataset distribution is\nusually of finite size $N$. The SD-OT problem is solved by estimating a dual\npotential vector using SGD; using that vector, freshly sampled noise vectors at\ntrain time can then be matched with data points at the cost of a maximum inner\nproduct search (MIPS). Semidiscrete FM (SD-FM) removes the quadratic dependency\non $n/\\varepsilon$ that bottlenecks OT-FM. SD-FM beats both FM and OT-FM on all\ntraining metrics and inference budget constraints, across multiple datasets, on\nunconditional/conditional generation, or when using mean-flow models.", "AI": {"tldr": "This paper proposes Semidiscrete Flow Matching (SD-FM) to overcome the computational bottlenecks of Optimal Transport Flow Matching (OT-FM) by using a semidiscrete formulation that leverages the finite size of target datasets, eliminating quadratic dependencies on batch size.", "motivation": "OT-FM shows promise for flow matching but becomes impractical due to the quadratic computational cost of Sinkhorn algorithm when batch sizes grow large, requiring O(n\u00b2/\u03b5\u00b2) operations.", "method": "The authors propose SD-FM which solves a semidiscrete OT problem by estimating a dual potential vector using SGD, then matches freshly sampled noise vectors with data points via maximum inner product search (MIPS), removing the quadratic dependency on batch size.", "result": "SD-FM outperforms both standard FM and OT-FM across all training metrics and inference budget constraints on multiple datasets, for both unconditional and conditional generation tasks.", "conclusion": "Semidiscrete Flow Matching provides a practical and efficient alternative to OT-FM that fulfills its theoretical promises while being computationally feasible, making it suitable for real-world applications."}}
{"id": "2509.26276", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.26276", "abs": "https://arxiv.org/abs/2509.26276", "authors": ["Morteza Rohanian", "Michael Krauthammer"], "title": "Optimizing Speech Language Models for Acoustic Consistency", "comment": null, "summary": "We study speech language models that incorporate semantic initialization and\nplanning losses to achieve robust and consistent generation. Our approach\ninitializes speech tokens with self-supervised features, applies a light\nalignment loss, and trains with thinning and auxiliary objectives that target\nrobustness and content planning. We train three models: a 0.7B speech-only\nmodel, a 1.0B speech-only model, and a 1.0B interleaved model with both text\nand speech. Acoustic studies show that the speech-only models achieve the\nhighest consistency across speaker, gender, sentiment, room, and background\nfactors, surpassing larger systems. Interleaving improves lexical and syntactic\nprobes and semantic--acoustic alignment but reduces consistency. Linear probes\nshow that our initialization biases the model toward content structure while\ntrading off prosody detail. These results show that LM-side design and training\nmix control the balance between acoustic stability and semantic grounding\nwithout changes to the tokenizer or runtime architecture. A demo and model\nweights are available for exploration.", "AI": {"tldr": "Speech language models with semantic initialization and planning losses achieve robust generation through self-supervised feature initialization, light alignment loss, and auxiliary training objectives.", "motivation": "To develop speech language models that maintain robust and consistent generation across various acoustic factors while preserving semantic content.", "method": "Initialize speech tokens with self-supervised features, apply light alignment loss, and train with thinning and auxiliary objectives targeting robustness and content planning. Three models trained: 0.7B speech-only, 1.0B speech-only, and 1.0B interleaved text-speech model.", "result": "Speech-only models achieve highest consistency across speaker, gender, sentiment, room, and background factors, surpassing larger systems. Interleaving improves lexical/syntactic probes and semantic-acoustic alignment but reduces consistency. Initialization biases model toward content structure while trading off prosody detail.", "conclusion": "LM-side design and training mix control the balance between acoustic stability and semantic grounding without changes to tokenizer or runtime architecture."}}
{"id": "2509.25922", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25922", "abs": "https://arxiv.org/abs/2509.25922", "authors": ["Zhicheng Zhou", "Jing Li", "Suming Qiu", "Junjie Huang", "Linyuan Qiu", "Zhijie Sun"], "title": "DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models", "comment": null, "summary": "The internet is saturated with low-density, high-redundancy information, such\nas social media comments, repetitive news, and lengthy discussions, making it\ndifficult to extract valuable insights efficiently. Multi-layer nested JSON\nstructures provide an effective solution by compressing such information into\nsemantically rich, hierarchical representations, which organize data into\nkey-value pairs, arrays, and nested objects, preserving contextual\nrelationships and enabling efficient storage, retrieval, and semantic querying.\nFor instance, in news aggregation, a JSON object can nest an article's metadata\n(title, author, date), content (text, multimedia), and multimedia information\n(multimedia type, caption) hierarchically. Large Language Models (LLMs) play a\ntransformative role in web data mining by parsing unstructured text and\noutputting structured results directly into complex JSON schemas. However,\ncurrent benchmarks for evaluating LLMs' JSON output capabilities overemphasize\npure JSON generation rather than assessing data comprehension and extraction\nabilities, a limitation that lacks relevance to practical web data mining\ntasks. To address this, we introduce DeepJSONEval, a novel benchmark featuring\n2100 multi-domain instances with deep nested structures, categorized by\ndifficulty. Experiments show significant performance gaps among LLMs in\nhandling such complexity. Our benchmark and datasets are open-sourced to\nadvance research in structured JSON\ngeneration.(https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval).", "AI": {"tldr": "DeepJSONEval is a new benchmark with 2100 multi-domain instances featuring deep nested JSON structures to better evaluate LLMs' data comprehension and extraction capabilities for practical web data mining tasks.", "motivation": "Current LLM benchmarks overemphasize pure JSON generation rather than assessing data comprehension and extraction abilities relevant to practical web data mining tasks involving low-density, high-redundancy internet information.", "method": "Introduces DeepJSONEval benchmark with 2100 multi-domain instances featuring deep nested JSON structures categorized by difficulty, testing LLMs' ability to parse unstructured text and output structured results into complex JSON schemas.", "result": "Experiments reveal significant performance gaps among LLMs in handling complex nested JSON structures, demonstrating the need for better evaluation of data extraction capabilities.", "conclusion": "DeepJSONEval addresses limitations in current benchmarks and advances research in structured JSON generation for practical web data mining applications."}}
{"id": "2509.25535", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25535", "abs": "https://arxiv.org/abs/2509.25535", "authors": ["Yichi Zhang", "Fangzheng Xie", "Shu Yang", "Chong Wu"], "title": "Meta-Router: Bridging Gold-standard and Preference-based Evaluations in Large Language Model Routing", "comment": null, "summary": "In language tasks that require extensive human--model interaction, deploying\na single \"best\" model for every query can be expensive. To reduce inference\ncost while preserving the quality of the responses, a large language model\n(LLM) router selects the most appropriate model from a pool of candidates for\neach query. A central challenge to training a high-quality router is the\nscarcity of reliable supervision. Gold-standard data (e.g., expert-verified\nlabels or rubric-based scores) provide accurate quality evaluations of LLM\nresponses but are costly and difficult to scale. In contrast, preference-based\ndata, collected via crowdsourcing or LLM-as-a-judge systems, are cheaper and\nmore scalable, yet often biased in reflecting the true quality of responses. We\ncast the problem of LLM router training with combined gold-standard and\npreference-based data into a causal inference framework by viewing the response\nevaluation mechanism as the treatment assignment. This perspective further\nreveals that the bias in preference-based data corresponds to the well-known\ncausal estimand: the conditional average treatment effect. Based on this new\nperspective, we develop an integrative causal router training framework that\ncorrects preference-data bias, address imbalances between two data sources, and\nimprove routing robustness and efficiency. Numerical experiments demonstrate\nthat our approach delivers more accurate routing and improves the trade-off\nbetween cost and quality.", "AI": {"tldr": "This paper presents a causal inference framework for training LLM routers that combines gold-standard and preference-based data to reduce bias and improve routing efficiency while maintaining response quality.", "motivation": "To address the high cost of deploying single 'best' models and the scarcity of reliable supervision data for training LLM routers, while dealing with biased preference-based data.", "method": "Proposes a causal inference framework that views response evaluation mechanisms as treatment assignments, correcting preference-data bias as conditional average treatment effect, and developing an integrative causal router training framework.", "result": "Numerical experiments show the approach delivers more accurate routing and improves the trade-off between cost and quality.", "conclusion": "The causal inference perspective effectively addresses bias in preference-based data and enables more robust and efficient LLM routing."}}
{"id": "2509.26302", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26302", "abs": "https://arxiv.org/abs/2509.26302", "authors": ["Mohamed Imed Eddine Ghebriout", "Ga\u00ebl Guibon", "Ivan Lerner", "Emmanuel Vincent"], "title": "QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented Dialogue Summarization", "comment": "Accepted to Empirical Methods in Natural Language Processing (EMNLP\n  2025)", "summary": "Dialogue summarization aims to distill the core meaning of a conversation\ninto a concise text. This is crucial for reducing the complexity and noise\ninherent in dialogue-heavy applications. While recent approaches typically\ntrain language models to mimic human-written summaries, such supervision is\ncostly and often results in outputs that lack task-specific focus limiting\ntheir effectiveness in downstream applications, such as medical tasks. In this\npaper, we propose \\app, a framework for task-oriented utility-based dialogue\nsummarization. \\app starts by generating multiple summaries and task-oriented\nquestion-answer pairs from a dialogue in a zero-shot manner using a pool of\nlarge language models (LLMs). The quality of the generated summaries is\nevaluated by having LLMs answer task-related questions before \\textit{(i)}\nselecting the best candidate answers and \\textit{(ii)} identifying the most\ninformative summary based on these answers. Finally, we fine-tune the best LLM\non the selected summaries. When validated on multiple datasets, \\app\ndemonstrates its effectiveness by achieving competitive results in various\nzero-shot settings, rivaling fully-supervised State-of-the-Art (SotA) methods.", "AI": {"tldr": "\\app is a framework for task-oriented dialogue summarization that generates multiple summaries and QA pairs using LLMs, selects the best ones based on task utility, and fine-tunes the best LLM on selected summaries, achieving competitive results without full supervision.", "motivation": "Traditional dialogue summarization methods require costly human-written summaries and often lack task-specific focus, limiting their effectiveness in applications like medical tasks.", "method": "Generate multiple summaries and task-oriented QA pairs using LLMs in zero-shot manner, evaluate summary quality by having LLMs answer task-related questions, select best candidate answers and most informative summary, then fine-tune best LLM on selected summaries.", "result": "Achieves competitive results in various zero-shot settings, rivaling fully-supervised State-of-the-Art methods across multiple datasets.", "conclusion": "\\app framework provides an effective approach for task-oriented utility-based dialogue summarization without requiring costly human supervision, demonstrating strong performance comparable to supervised methods."}}
{"id": "2509.25923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25923", "abs": "https://arxiv.org/abs/2509.25923", "authors": ["Mubaris Nadeem", "Johannes Zenkert", "Christian Weber", "Lisa Bender", "Madjid Fathi"], "title": "KIRETT: Smart Integration of Vital Signs Data for Intelligent Decision Support in Rescue Scenarios", "comment": "Conference paper for 2024 IEEE International Conference on Electro\n  Information Technology (eIT), KIRETT Project, University of Siegen, Germany", "summary": "The integration of vital signs in healthcare has witnessed a steady rise,\npromising health professionals to assist in their daily tasks to improve\npatient treatment. In life-threatening situations, like rescue operations,\ncrucial decisions need to be made in the shortest possible amount of time to\nensure that excellent treatment is provided during life-saving measurements.\nThe integration of vital signs in the treatment holds the potential to improve\ntime utilization for rescuers in such critical situations. They furthermore\nserve to support health professionals during the treatment with useful\ninformation and suggestions. To achieve such a goal, the KIRETT project serves\nto provide treatment recommendations and situation detection, combined on a\nwrist-worn wearable for rescue operations.This paper aims to present the\nsignificant role of vital signs in the improvement of decision-making during\nrescue operations and show their impact on health professionals and patients in\nneed.", "AI": {"tldr": "The paper discusses how integrating vital signs through wearable technology can improve decision-making and treatment recommendations during rescue operations.", "motivation": "To enhance time utilization and support health professionals with useful information during life-threatening rescue situations where quick decisions are crucial.", "method": "The KIRETT project develops a wrist-worn wearable that provides treatment recommendations and situation detection using vital signs data.", "result": "Vital signs integration shows potential to significantly improve decision-making processes and treatment quality during critical rescue operations.", "conclusion": "Vital signs play a crucial role in enhancing emergency medical care by providing real-time data that supports faster and more accurate treatment decisions in rescue scenarios."}}
{"id": "2509.25538", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25538", "abs": "https://arxiv.org/abs/2509.25538", "authors": ["Marcus Schwarting", "Logan Ward", "Nathaniel Hudson", "Xiaoli Yan", "Ben Blaiszik", "Santanu Chaudhuri", "Eliu Huerta", "Ian Foster"], "title": "Steering an Active Learning Workflow Towards Novel Materials Discovery via Queue Prioritization", "comment": null, "summary": "Generative AI poses both opportunities and risks for solving inverse design\nproblems in the sciences. Generative tools provide the ability to expand and\nrefine a search space autonomously, but do so at the cost of exploring\nlow-quality regions until sufficiently fine tuned. Here, we propose a queue\nprioritization algorithm that combines generative modeling and active learning\nin the context of a distributed workflow for exploring complex design spaces.\nWe find that incorporating an active learning model to prioritize top design\ncandidates can prevent a generative AI workflow from expending resources on\nnonsensical candidates and halt potential generative model decay. For an\nexisting generative AI workflow for discovering novel molecular structure\ncandidates for carbon capture, our active learning approach significantly\nincreases the number of high-quality candidates identified by the generative\nmodel. We find that, out of 1000 novel candidates, our workflow without active\nlearning can generate an average of 281 high-performing candidates, while our\nproposed prioritization with active learning can generate an average 604\nhigh-performing candidates.", "AI": {"tldr": "Proposes an active learning approach to improve generative AI workflows for inverse design problems, specifically for molecular structure discovery in carbon capture applications.", "motivation": "Generative AI can autonomously expand search spaces but often explores low-quality regions before fine-tuning, wasting computational resources and potentially causing model decay.", "method": "A queue prioritization algorithm that combines generative modeling with active learning to intelligently select top design candidates for further exploration.", "result": "The active learning approach significantly improved performance - from 281 to 604 high-performing molecular candidates out of 1000 novel candidates generated.", "conclusion": "Incorporating active learning prevents generative AI workflows from wasting resources on nonsensical candidates and halts potential model decay, significantly increasing the number of high-quality design candidates identified."}}
{"id": "2509.26305", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26305", "abs": "https://arxiv.org/abs/2509.26305", "authors": ["Arduin Findeis", "Timo Kaufmann", "Eyke H\u00fcllermeier", "Robert Mullins"], "title": "Feedback Forensics: A Toolkit to Measure AI Personality", "comment": null, "summary": "Some traits making a \"good\" AI model are hard to describe upfront. For\nexample, should responses be more polite or more casual? Such traits are\nsometimes summarized as model character or personality. Without a clear\nobjective, conventional benchmarks based on automatic validation struggle to\nmeasure such traits. Evaluation methods using human feedback such as Chatbot\nArena have emerged as a popular alternative. These methods infer \"better\"\npersonality and other desirable traits implicitly by ranking multiple model\nresponses relative to each other. Recent issues with model releases highlight\nlimitations of these existing opaque evaluation approaches: a major model was\nrolled back over sycophantic personality issues, models were observed\noverfitting to such feedback-based leaderboards. Despite these known issues,\nlimited public tooling exists to explicitly evaluate model personality. We\nintroduce Feedback Forensics: an open-source toolkit to track AI personality\nchanges, both those encouraged by human (or AI) feedback, and those exhibited\nacross AI models trained and evaluated on such feedback. Leveraging AI\nannotators, our toolkit enables investigating personality via Python API and\nbrowser app. We demonstrate the toolkit's usefulness in two steps: (A) first we\nanalyse the personality traits encouraged in popular human feedback datasets\nincluding Chatbot Arena, MultiPref and PRISM; and (B) then use our toolkit to\nanalyse how much popular models exhibit such traits. We release (1) our\nFeedback Forensics toolkit alongside (2) a web app tracking AI personality in\npopular models and feedback datasets as well as (3) the underlying annotation\ndata at https://github.com/rdnfn/feedback-forensics.", "AI": {"tldr": "Feedback Forensics is an open-source toolkit that tracks AI personality changes using AI annotators, enabling analysis of personality traits in feedback datasets and models.", "motivation": "Existing evaluation methods struggle to measure subjective traits like model personality, and current approaches have limitations including sycophantic personality issues and overfitting to feedback-based leaderboards.", "method": "Leverages AI annotators to investigate personality via Python API and browser app, analyzing personality traits in human feedback datasets and evaluating how much popular models exhibit such traits.", "result": "The toolkit enables analysis of personality traits in popular human feedback datasets (Chatbot Arena, MultiPref, PRISM) and evaluates personality traits in popular AI models.", "conclusion": "Feedback Forensics provides a public tool to explicitly evaluate model personality, addressing limitations of existing opaque evaluation approaches and enabling tracking of personality changes across AI models."}}
{"id": "2509.25928", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25928", "abs": "https://arxiv.org/abs/2509.25928", "authors": ["Mubaris Nadeem", "Johannes Zenkert", "Lisa Bender", "Christian Weber", "Madjid Fathi"], "title": "Quantitative Evaluation of KIRETT Wearable Demonstrator for Rescue Operations", "comment": "Conference paper for 2024 IEEE World AI IoT Congress (AIIoT), KIRETT\n  Project, University of Siegen, Germany", "summary": "Healthcare and Medicine are under constant pressure to provide patient-driven\nmedical expertise to ensure a fast and accurate treatment of the patient. In\nsuch scenarios, the diagnosis contains, the family history, long term medical\ndata and a detailed consultation with the patient. In time-critical\nemergencies, such conversation and time-consuming elaboration are not possible.\nRescue services need to provide fast, reliable treatments for the patient in\nneed. With the help of modern technologies, like treatment recommendations,\nreal-time vitals-monitoring, and situation detection through artificial\nintelligence (AI) a situation can be analyzed and supported in providing fast,\naccurate patient-data-driven medical treatments. In KIRETT, a wearable device\nis developed to support in such scenarios and presents a way to provide\ntreatment recommendation in rescue services. The objective of this paper is to\npresent the quantitative results of a two-day KIRETT evaluation (14\nparticipants) to analyze the needs of rescue operators in healthcare.", "AI": {"tldr": "The KIRETT wearable device provides AI-driven treatment recommendations and real-time monitoring for emergency rescue services, evaluated through a 2-day study with 14 participants.", "motivation": "Emergency situations require fast, reliable medical treatment without time-consuming patient consultations, creating a need for technology-assisted decision support in rescue services.", "method": "Developed the KIRETT wearable device with AI capabilities for treatment recommendations and real-time vitals monitoring, then conducted a 2-day evaluation study with 14 rescue service participants.", "result": "Quantitative results from the 14-participant evaluation study analyzed the specific needs and requirements of rescue operators in healthcare emergency scenarios.", "conclusion": "The KIRETT system demonstrates potential to support rescue services by providing AI-driven medical decision support and real-time patient monitoring in time-critical emergency situations."}}
{"id": "2509.25560", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25560", "abs": "https://arxiv.org/abs/2509.25560", "authors": ["Guojun Tang", "Jiayu Zhou", "Mohammad Mamun", "Steve Drew"], "title": "Lightweight and Robust Federated Data Valuation", "comment": null, "summary": "Federated learning (FL) faces persistent robustness challenges due to non-IID\ndata distributions and adversarial client behavior. A promising mitigation\nstrategy is contribution evaluation, which enables adaptive aggregation by\nquantifying each client's utility to the global model. However,\nstate-of-the-art Shapley-value-based approaches incur high computational\noverhead due to repeated model reweighting and inference, which limits their\nscalability. We propose FedIF, a novel FL aggregation framework that leverages\ntrajectory-based influence estimation to efficiently compute client\ncontributions. FedIF adapts decentralized FL by introducing normalized and\nsmoothed influence scores computed from lightweight gradient operations on\nclient updates and a public validation set. Theoretical analysis demonstrates\nthat FedIF yields a tighter bound on one-step global loss change under noisy\nconditions. Extensive experiments on CIFAR-10 and Fashion-MNIST show that FedIF\nachieves robustness comparable to or exceeding SV-based methods in the presence\nof label noise, gradient noise, and adversarial samples, while reducing\naggregation overhead by up to 450x. Ablation studies confirm the effectiveness\nof FedIF's design choices, including local weight normalization and influence\nsmoothing. Our results establish FedIF as a practical, theoretically grounded,\nand scalable alternative to Shapley-value-based approaches for efficient and\nrobust FL in real-world deployments.", "AI": {"tldr": "FedIF is a federated learning aggregation framework that uses trajectory-based influence estimation to efficiently compute client contributions, achieving robustness comparable to Shapley-value methods while reducing aggregation overhead by 450x.", "motivation": "Federated learning faces robustness challenges from non-IID data and adversarial clients. Existing Shapley-value approaches have high computational overhead due to repeated model reweighting and inference, limiting scalability.", "method": "FedIF leverages trajectory-based influence estimation with normalized and smoothed influence scores computed from lightweight gradient operations on client updates and a public validation set.", "result": "FedIF achieves robustness comparable to or exceeding Shapley-value methods against label noise, gradient noise, and adversarial samples, while reducing aggregation overhead by up to 450x on CIFAR-10 and Fashion-MNIST datasets.", "conclusion": "FedIF provides a practical, theoretically grounded, and scalable alternative to Shapley-value-based approaches for efficient and robust federated learning in real-world deployments."}}
{"id": "2509.26313", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26313", "abs": "https://arxiv.org/abs/2509.26313", "authors": ["Rui Ming", "Haoyuan Wu", "Shoubo Hu", "Zhuolun He", "Bei Yu"], "title": "One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient", "comment": null, "summary": "Supervised fine-tuning (SFT) is the predominant method for adapting large\nlanguage models (LLMs), yet it often struggles with generalization compared to\nreinforcement learning (RL). In this work, we posit that this performance\ndisparity stems not just from the loss function, but from a more fundamental\ndifference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes\non-policy data sampled from the current policy. Building on this hypothesis, we\nintroduce one-token rollout (OTR), a novel fine-tuning algorithm that guides\nSFT with the policy gradient method. OTR reframes the autoregressive learning\nprocess by treating each token generation as a single-step reinforcement\nlearning trajectory. At each step, it performs a Monte Carlo ``rollout'' by\nsampling multiple candidate tokens from the current policy's distribution. The\nground-truth token from the supervised data is then used to provide a reward\nsignal to these samples. Guided by policy gradient, our algorithm repurposes\nstatic, off-policy supervised data into a dynamic, on-policy signal at the\ntoken level, capturing the generalization benefits of on-policy learning while\nbypassing the costly overhead of full sentence generation. Through extensive\nexperiments on a diverse suite of challenging benchmarks spanning mathematical\nreasoning, code generation, and general domain reasoning, we demonstrate that\nOTR consistently outperforms standard SFT. Our findings establish OTR as a\npowerful and practical alternative for fine-tuning LLMs and provide compelling\nevidence that the on-policy nature of data is a critical driver of\ngeneralization, offering a promising new direction for fine-tuning LLMs.", "AI": {"tldr": "OTR is a novel fine-tuning algorithm that combines SFT with policy gradient by treating token generation as single-step RL, using supervised data to create on-policy learning signals without full sentence generation overhead.", "motivation": "SFT struggles with generalization compared to RL, which the authors attribute to SFT using fixed off-policy data while RL uses dynamic on-policy data from current policy.", "method": "OTR reframes autoregressive learning as single-step RL trajectories, sampling multiple candidate tokens and using ground-truth tokens as reward signals to apply policy gradient updates.", "result": "OTR consistently outperforms standard SFT across diverse benchmarks including mathematical reasoning, code generation, and general domain reasoning.", "conclusion": "OTR establishes that on-policy data nature is critical for generalization, offering a practical alternative for fine-tuning LLMs with RL benefits but without RL's computational costs."}}
{"id": "2509.25941", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25941", "abs": "https://arxiv.org/abs/2509.25941", "authors": ["Raphael Schumann", "Stefan Riezler"], "title": "Boosting Process-Correct CoT Reasoning by Modeling Solvability of Multiple-Choice QA", "comment": null, "summary": "Reasoning quality in large language models depends not only on producing\ncorrect answers but also on generating valid intermediate steps. We study this\nthrough multiple-choice question answering (MCQA), which provides a controlled\nsetting with fixed answer options. Our analysis shows that when questions are\neffectively unsolvable for a model, spurious chains of thought (CoTs) are more\nlikely to appear, leading to false positives. By estimating the solvability of\neach question, we uncover an intermediate regime where learning is most\neffective. Building on this insight, we adapt outcome-supervised reward models\nand reinforcement learning with group-relative advantage to incorporate\nsolvability into their objectives. Across experiments on math and multimodal\ndatasets, these modifications consistently yield higher rates of\nprocess-correct reasoning and, in reinforcement learning, improved answer\naccuracy as well. Our results highlight solvability as a key factor for\nreducing hallucinations and increasing reliability in CoT reasoning.", "AI": {"tldr": "This paper studies how question solvability affects reasoning quality in LLMs, finding that unsolvable questions lead to spurious reasoning chains. By incorporating solvability estimation into reward models and RL, they improve process-correct reasoning and reduce hallucinations.", "motivation": "To understand how question difficulty (solvability) affects the quality of chain-of-thought reasoning in LLMs, particularly how unsolvable questions lead to false positives and spurious reasoning chains.", "method": "Used multiple-choice QA as controlled setting; estimated question solvability; adapted outcome-supervised reward models and RL with group-relative advantage to incorporate solvability into objectives.", "result": "Across math and multimodal datasets, the solvability-aware methods consistently yielded higher rates of process-correct reasoning and improved answer accuracy in RL settings.", "conclusion": "Solvability is a key factor for reducing hallucinations and increasing reliability in chain-of-thought reasoning, with intermediate solvability regimes being most effective for learning."}}
{"id": "2509.25582", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25582", "abs": "https://arxiv.org/abs/2509.25582", "authors": ["Amir Moeini", "Minjae Kwon", "Alper Kamil Bozkurt", "Yuichi Motai", "Rohan Chandra", "Lu Feng", "Shangtong Zhang"], "title": "Safe In-Context Reinforcement Learning", "comment": null, "summary": "In-context reinforcement learning (ICRL) is an emerging RL paradigm where the\nagent, after some pretraining procedure, is able to adapt to\nout-of-distribution test tasks without any parameter updates. The agent\nachieves this by continually expanding the input (i.e., the context) to its\npolicy neural networks. For example, the input could be all the history\nexperience that the agent has access to until the current time step. The\nagent's performance improves as the input grows, without any parameter updates.\nIn this work, we propose the first method that promotes the safety of ICRL's\nadaptation process in the framework of constrained Markov Decision Processes.\nIn other words, during the parameter-update-free adaptation process, the agent\nnot only maximizes the reward but also minimizes an additional cost function.\nWe also demonstrate that our agent actively reacts to the threshold (i.e.,\nbudget) of the cost tolerance. With a higher cost budget, the agent behaves\nmore aggressively, and with a lower cost budget, the agent behaves more\nconservatively.", "AI": {"tldr": "The paper proposes the first safety-promoting method for in-context reinforcement learning (ICRL) within constrained Markov Decision Processes, enabling agents to adapt to out-of-distribution tasks without parameter updates while minimizing costs alongside reward maximization.", "motivation": "ICRL enables agents to adapt to new tasks without parameter updates by expanding input context, but existing methods lack safety considerations. The authors aim to incorporate safety constraints into ICRL's adaptation process.", "method": "Proposed a method that integrates constrained Markov Decision Processes framework into ICRL, allowing agents to minimize cost functions during adaptation while maximizing rewards, without requiring parameter updates.", "result": "The agent successfully adapts to out-of-distribution tasks while respecting cost constraints, and actively responds to cost tolerance thresholds - behaving more aggressively with higher budgets and more conservatively with lower budgets.", "conclusion": "The work establishes the first safety-promoting approach for ICRL, demonstrating that agents can effectively balance reward maximization with cost minimization during parameter-free adaptation, with behavior adaptively responding to cost budget constraints."}}
{"id": "2509.26314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26314", "abs": "https://arxiv.org/abs/2509.26314", "authors": ["Hanwen Du", "Yuxin Dong", "Xia Ning"], "title": "Latent Thinking Optimization: Your Latent Reasoning Language Model Secretly Encodes Reward Signals in its Latent Thoughts", "comment": null, "summary": "Large Language Models (LLMs) excel at problem solving by generating chain of\nthoughts in natural language, but such verbal thinking is computationally\ncostly and prone to overthinking. Recent work instead proposes a latent\nthinking architecture Huggin-3.5B, which represents intermediate reasoning\nsteps as sequence of latent representations. However, latent thoughts lack\ninterpretability and are difficult to supervise, raising concerns about the\ncorrectness and reliability of its latent thinking processes. In this paper, we\nprovide a systematic study of how Huggin-3.5B thinks in the latent space and\nhow external supervision signals can improve its latent thinking processes. We\nshow that latent thoughts leading to correct versus incorrect answers exhibit\nhighly distinguishable patterns, and that a latent classifier can reliably\npredict answer correctness directly from latent thoughts. Leveraging these\ninsights, we propose Latent Thinking Optimization (LTO), a probabilistic\nalgorithm that employs the latent classifier as a Latent Reward Model (LRM) to\noptimize the latent thinking processes. Extensive experiments across diverse\nreasoning tasks demonstrate that LRM is highly effective in detecting incorrect\nlatent thinking patterns, and LTO can significantly improve the latent thinking\nprocesses. Furthermore, we show that LRM can generalize across diverse domains,\nand LTO can be seamlessly applied to general LLMs to improve their thinking\nprocesses. In contrast to verbal thinking, our method demonstrates that reward\nmodeling and scaling test-time thinking with supervision can be performed\ndirectly in the latent space, highlighting its potential as a general,\nefficient, and domain-agnostic approach to improving the thinking processes of\nLLMs.", "AI": {"tldr": "This paper studies Huggin-3.5B's latent thinking processes, showing that correct and incorrect latent thoughts have distinguishable patterns. The authors propose Latent Thinking Optimization (LTO) using a latent classifier as reward model to optimize thinking processes, achieving significant improvements across diverse reasoning tasks.", "motivation": "LLMs' verbal thinking is computationally costly and prone to overthinking. While latent thinking architectures like Huggin-3.5B offer efficiency, they lack interpretability and supervision, raising concerns about correctness and reliability of latent reasoning processes.", "method": "Systematic study of Huggin-3.5B's latent thinking patterns, development of latent classifier to predict answer correctness from latent thoughts, and proposal of Latent Thinking Optimization (LTO) - a probabilistic algorithm using latent classifier as Latent Reward Model (LRM) to optimize thinking processes.", "result": "Latent thoughts leading to correct vs incorrect answers show highly distinguishable patterns. LRM effectively detects incorrect latent thinking patterns, and LTO significantly improves latent thinking processes across diverse reasoning tasks. The method generalizes across domains and can be applied to general LLMs.", "conclusion": "Reward modeling and scaling test-time thinking with supervision can be performed directly in latent space, offering a general, efficient, and domain-agnostic approach to improve LLMs' thinking processes, contrasting with costly verbal thinking methods."}}
{"id": "2509.25944", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25944", "abs": "https://arxiv.org/abs/2509.25944", "authors": ["Yuan Gao", "Mattia Piccinini", "Roberto Brusnicki", "Yuchen Zhang", "Johannes Betz"], "title": "NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving", "comment": "8 pages", "summary": "Understanding risk in autonomous driving requires not only perception and\nprediction, but also high-level reasoning about agent behavior and context.\nCurrent Vision Language Models (VLMs)-based methods primarily ground agents in\nstatic images and provide qualitative judgments, lacking the spatio-temporal\nreasoning needed to capture how risks evolve over time. To address this gap, we\npropose NuRisk, a comprehensive Visual Question Answering (VQA) dataset\ncomprising 2,900 scenarios and 1.1 million agent-level samples, built on\nreal-world data from nuScenes and Waymo, supplemented with safety-critical\nscenarios from the CommonRoad simulator. The dataset provides Bird-Eye-View\n(BEV) based sequential images with quantitative, agent-level risk annotations,\nenabling spatio-temporal reasoning. We benchmark well-known VLMs across\ndifferent prompting techniques and find that they fail to perform explicit\nspatio-temporal reasoning, resulting in a peak accuracy of 33% at high latency.\nTo address these shortcomings, our fine-tuned 7B VLM agent improves accuracy to\n41% and reduces latency by 75%, demonstrating explicit spatio-temporal\nreasoning capabilities that proprietary models lacked. While this represents a\nsignificant step forward, the modest accuracy underscores the profound\nchallenge of the task, establishing NuRisk as a critical benchmark for\nadvancing spatio-temporal reasoning in autonomous driving.", "AI": {"tldr": "Proposes NuRisk dataset for spatio-temporal risk reasoning in autonomous driving, benchmarks VLMs showing poor performance, and presents fine-tuned model with improved accuracy and latency.", "motivation": "Current VLMs lack spatio-temporal reasoning for evolving risks in autonomous driving, only providing static qualitative judgments.", "method": "Created NuRisk dataset with 2,900 scenarios and 1.1M agent-level samples from real-world and simulated data, featuring BEV sequential images with quantitative risk annotations. Fine-tuned 7B VLM for spatio-temporal reasoning.", "result": "Benchmarked VLMs achieved only 33% accuracy with high latency. Fine-tuned model improved accuracy to 41% and reduced latency by 75%.", "conclusion": "NuRisk establishes critical benchmark for spatio-temporal reasoning in autonomous driving, showing significant progress but highlighting remaining challenges."}}
{"id": "2509.25592", "categories": ["cs.LG", "68T05, 68T20, 90C26, 90C30, 93E35, 62L20", "I.2.6; I.2.8; G.1.6"], "pdf": "https://arxiv.org/pdf/2509.25592", "abs": "https://arxiv.org/abs/2509.25592", "authors": ["Morteza Kimiaei", "Vyacheslav Kungurtsev"], "title": "Machine Learning Algorithms for Improving Black Box Optimization Solvers", "comment": "74 pages", "summary": "Black-box optimization (BBO) addresses problems where objectives are\naccessible only through costly queries without gradients or explicit structure.\nClassical derivative-free methods -- line search, direct search, and\nmodel-based solvers such as Bayesian optimization -- form the backbone of BBO,\nyet often struggle in high-dimensional, noisy, or mixed-integer settings.\n  Recent advances use machine learning (ML) and reinforcement learning (RL) to\nenhance BBO: ML provides expressive surrogates, adaptive updates, meta-learning\nportfolios, and generative models, while RL enables dynamic operator\nconfiguration, robustness, and meta-optimization across tasks.\n  This paper surveys these developments, covering representative algorithms\nsuch as NNs with the modular model-based optimization framework (mlrMBO),\nzeroth-order adaptive momentum methods (ZO-AdaMM), automated BBO (ABBO),\ndistributed block-wise optimization (DiBB), partition-based Bayesian\noptimization (SPBOpt), the transformer-based optimizer (B2Opt),\ndiffusion-model-based BBO, surrogate-assisted RL for differential evolution\n(Surr-RLDE), robust BBO (RBO), coordinate-ascent model-based optimization with\nrelative entropy (CAS-MORE), log-barrier stochastic gradient descent (LB-SGD),\npolicy improvement with black-box (PIBB), and offline Q-learning with Mamba\nbackbones (Q-Mamba).\n  We also review benchmark efforts such as the NeurIPS 2020 BBO Challenge and\nthe MetaBox framework. Overall, we highlight how ML and RL transform classical\ninexact solvers into more scalable, robust, and adaptive frameworks for\nreal-world optimization.", "AI": {"tldr": "This paper surveys how machine learning and reinforcement learning enhance black-box optimization, transforming classical methods into more scalable, robust, and adaptive frameworks for real-world applications.", "motivation": "Classical derivative-free methods for black-box optimization often struggle with high-dimensional, noisy, or mixed-integer problems, motivating the integration of ML and RL techniques to improve performance.", "method": "The survey covers various ML and RL approaches including neural networks with modular frameworks, zeroth-order adaptive momentum methods, automated BBO, distributed optimization, Bayesian optimization variants, transformer-based optimizers, diffusion models, surrogate-assisted RL, and robust optimization techniques.", "result": "The paper reviews multiple advanced algorithms and benchmark frameworks that demonstrate improved scalability, robustness, and adaptability in black-box optimization compared to classical methods.", "conclusion": "ML and RL techniques are transforming classical black-box optimization solvers into more effective frameworks capable of handling complex real-world optimization challenges across various domains."}}
{"id": "2509.26328", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26328", "abs": "https://arxiv.org/abs/2509.26328", "authors": ["Chengyue Wu", "Hao Zhang", "Shuchen Xue", "Shizhe Diao", "Yonggan Fu", "Zhijian Liu", "Pavlo Molchanov", "Ping Luo", "Song Han", "Enze Xie"], "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM", "comment": null, "summary": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released.", "AI": {"tldr": "Fast-dLLM v2 converts autoregressive LLMs into block diffusion models with only 1B token fine-tuning, achieving 2.5x speedup while maintaining accuracy through novel block diffusion and hierarchical caching mechanisms.", "motivation": "Autoregressive LLMs suffer from sequential decoding inefficiency, limiting inference speed despite their strong performance.", "method": "Proposes block diffusion mechanism with complementary attention mask for bidirectional context modeling, plus hierarchical caching (block-level and sub-block caches) for parallel generation.", "result": "Achieves 500x reduction in training data (1B vs 580B tokens), 2.5x speedup over AR decoding while matching or surpassing AR baseline accuracy across benchmarks.", "conclusion": "Fast-dLLM v2 enables practical deployment of fast and accurate LLMs, representing significant progress in diffusion language model efficiency."}}
{"id": "2509.25946", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25946", "abs": "https://arxiv.org/abs/2509.25946", "authors": ["Lee Jung-Mok", "Nam Hyeon-Woo", "Moon Ye-Bin", "Junhyun Nam", "Tae-Hyun Oh"], "title": "Automated Model Discovery via Multi-modal & Multi-step Pipeline", "comment": null, "summary": "Automated model discovery is the process of automatically searching and\nidentifying the most appropriate model for a given dataset over a large\ncombinatorial search space. Existing approaches, however, often face challenges\nin balancing the capture of fine-grained details with ensuring generalizability\nbeyond training data regimes with a reasonable model complexity. In this paper,\nwe present a multi-modal \\& multi-step pipeline for effective automated model\ndiscovery. Our approach leverages two vision-language-based modules (VLM),\nAnalyzerVLM and EvaluatorVLM, for effective model proposal and evaluation in an\nagentic way. AnalyzerVLM autonomously plans and executes multi-step analyses to\npropose effective candidate models. EvaluatorVLM assesses the candidate models\nboth quantitatively and perceptually, regarding the fitness for local details\nand the generalibility for overall trends. Our results demonstrate that our\npipeline effectively discovers models that capture fine details and ensure\nstrong generalizability. Additionally, extensive ablation studies show that\nboth multi-modality and multi-step reasoning play crucial roles in discovering\nfavorable models.", "AI": {"tldr": "A multi-modal, multi-step pipeline using vision-language models for automated model discovery that balances fine-grained detail capture with generalizability.", "motivation": "Existing automated model discovery approaches struggle to balance capturing fine-grained details while ensuring generalizability beyond training data with reasonable model complexity.", "method": "Uses two vision-language modules (AnalyzerVLM and EvaluatorVLM) in a multi-step pipeline. AnalyzerVLM autonomously plans multi-step analyses to propose candidate models, while EvaluatorVLM assesses models both quantitatively and perceptually for local detail fitness and overall trend generalizability.", "result": "The pipeline effectively discovers models that capture fine details while ensuring strong generalizability. Ablation studies confirm both multi-modality and multi-step reasoning are crucial for finding favorable models.", "conclusion": "The proposed multi-modal, multi-step pipeline using vision-language models successfully addresses the challenge of balancing detail capture with generalizability in automated model discovery."}}
{"id": "2509.25596", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25596", "abs": "https://arxiv.org/abs/2509.25596", "authors": ["Lucia Quirke", "Stepan Shabalin", "Nora Belrose"], "title": "Binary Sparse Coding for Interpretability", "comment": null, "summary": "Sparse autoencoders (SAEs) are used to decompose neural network activations\ninto sparsely activating features, but many SAE features are only interpretable\nat high activation strengths. To address this issue we propose to use binary\nsparse autoencoders (BAEs) and binary transcoders (BTCs), which constrain all\nactivations to be zero or one. We find that binarisation significantly improves\nthe interpretability and monosemanticity of the discovered features, while\nincreasing reconstruction error. By eliminating the distinction between high\nand low activation strengths, we prevent uninterpretable information from being\nsmuggled in through the continuous variation in feature activations. However,\nwe also find that binarisation increases the number of uninterpretable\nultra-high frequency features, and when interpretability scores are\nfrequency-adjusted, the scores for continuous sparse coders are slightly better\nthan those of binary ones. This suggests that polysemanticity may be an\nineliminable property of neural activations.", "AI": {"tldr": "Binary sparse autoencoders (BAEs) and binary transcoders (BTCs) improve feature interpretability by constraining activations to 0 or 1, eliminating continuous variation that can hide uninterpretable information.", "motivation": "Many sparse autoencoder features are only interpretable at high activation strengths, and continuous activation variations can smuggle in uninterpretable information.", "method": "Propose binary sparse autoencoders (BAEs) and binary transcoders (BTCs) that constrain all activations to be binary (0 or 1) instead of continuous values.", "result": "Binarisation significantly improves interpretability and monosemanticity of features but increases reconstruction error and ultra-high frequency features. Frequency-adjusted interpretability scores show continuous sparse coders perform slightly better.", "conclusion": "Polysemanticity may be an ineliminable property of neural activations, as even binary methods don't completely solve the interpretability challenge."}}
{"id": "2509.26383", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26383", "abs": "https://arxiv.org/abs/2509.26383", "authors": ["Jinyeop Song", "Song Wang", "Julian Shun", "Yada Zhu"], "title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning", "comment": "10 pages, 5 figures. Submitted to ICLR 2026", "summary": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large\nlanguage models (LLMs) with structured, verifiable knowledge graphs (KGs) to\nreduce hallucinations and expose reasoning traces. However, many KG-RAG systems\ncompose multiple LLM modules (e.g planning, reasoning, and responding),\ninflating inference cost and binding behavior to a specific target KG. To\naddress this, we introduce KG-R1, an agentic KG retrieval-augmented generation\n(KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single\nagent that interacts with KGs as its environment, learning to retrieve at each\nstep and incorporating the retrieved information into its reasoning and\ngeneration. The process is optimized through end-to-end RL. In controlled\nexperiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our\nmethod demonstrates both efficiency and transferability: Using Qwen-2.5-3B,\nKG-R1 improves answer accuracy with fewer generation tokens than prior\nmulti-module workflow methods that use larger foundation or fine-tuned models.\nFurthermore, KG-R1 enables plug and play: after training, it maintains strong\naccuracy on new KGs without modification. These properties make KG-R1 a\npromising KG-RAG framework for real-world deployment. Our code is publicly\navailable at https://github.com/Jinyeop3110/KG-R1.", "AI": {"tldr": "KG-R1 is a reinforcement learning-based KG-RAG framework that uses a single agent to interact with knowledge graphs, achieving better efficiency and transferability than multi-module approaches.", "motivation": "To address the high inference costs and KG-specific limitations of existing multi-module KG-RAG systems that use separate LLM modules for planning, reasoning, and responding.", "method": "Uses a single agent that interacts with KGs as its environment, learning retrieval strategies through reinforcement learning and incorporating retrieved information into reasoning and generation in an end-to-end optimized process.", "result": "In KGQA benchmarks, KG-R1 with Qwen-2.5-3B achieved higher answer accuracy with fewer tokens than prior multi-module methods using larger models, and demonstrated plug-and-play capability on new KGs without modification.", "conclusion": "KG-R1 provides an efficient and transferable KG-RAG framework suitable for real-world deployment, reducing hallucinations while maintaining reasoning transparency."}}
{"id": "2509.25958", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25958", "abs": "https://arxiv.org/abs/2509.25958", "authors": ["Gang Li", "Yulei Qin", "Xiaoyu Tan", "Dingkang Yang", "Yuchen Shi", "Zihan Xu", "Xiang Li", "Xing Sun", "Ke Li"], "title": "RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in\neliciting complex reasoning in large language models (LLMs). However, standard\nRLVR training often leads to excessively verbose processes (in reasoning tasks)\nand inefficient exploration trajectories (in agentic settings), as outcome-only\nrewards provide no incentive for efficiency and the high variance in response\nlength within relatively small rollout groups results in noisy optimization\nsignals. To address this, we propose Rollout Response Recomposition (RoRecomp),\na plug-and-play method that guides models toward concise reasoning by\nstrategically recomposing the training data. RoRecomp separates responses into\ntwo distinct batch types: 1) priority batches, which combine short-correct and\nlong-incorrect responses selected from online batches to provide a clear\ngradient signal for brevity, and 2) compensation batches, which utilize\nremaining responses from a replay buffer to maintain stability and prevent\nmodel collapse. To comprehensively evaluate effectiveness, we test RoRecomp\nacross three settings where results demonstrate substantial efficiency gains:\nreducing reasoning length by 27.7% in zero RL training, reducing unnecessary\ntool calls by 46.8% while improving accuracy in agentic RL, and achieving up to\n52.5% length reduction in thinking compression, all with minimal performance\nimpact.", "AI": {"tldr": "RoRecomp is a plug-and-play method that improves RLVR training efficiency by strategically recomposing training data into priority and compensation batches to guide models toward concise reasoning.", "motivation": "Standard RLVR training leads to verbose reasoning processes and inefficient exploration trajectories due to outcome-only rewards lacking efficiency incentives and high variance in response length causing noisy optimization signals.", "method": "RoRecomp separates responses into priority batches (combining short-correct and long-incorrect responses for brevity gradient) and compensation batches (using remaining responses from replay buffer for stability).", "result": "Achieved 27.7% reasoning length reduction in zero RL training, 46.8% reduction in unnecessary tool calls with improved accuracy in agentic RL, and up to 52.5% length reduction in thinking compression with minimal performance impact.", "conclusion": "RoRecomp effectively addresses efficiency issues in RLVR training by providing clear gradient signals for brevity while maintaining model stability, demonstrating substantial efficiency gains across multiple settings."}}
{"id": "2509.25606", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25606", "abs": "https://arxiv.org/abs/2509.25606", "authors": ["Yixuan Wang", "Dan Guralnik", "Saiedeh Akbari", "Warren Dixon"], "title": "Effective Model Pruning", "comment": "17 pages, 4 figures", "summary": "We introduce Effective Model Pruning (EMP), a context-agnostic,\nparameter-free rule addressing a fundamental question about pruning: how many\nentries to keep. EMP does not prescribe how to score the parameters or prune\nthe models; instead, it supplies a universal adaptive threshold that can be\napplied to any pruning criterion: weight magnitude, attention score, KAN\nimportance score, or even feature-level signals such as image pixel, and used\non structural parts or weights of the models. Given any score vector s, EMP\nmaps s to a built-in effective number N_eff which is inspired by the Inverse\nSimpson index of contributors. Retaining the N_eff highest scoring entries and\nzeroing the remainder yields sparse models with performance comparable to the\noriginal dense networks across MLPs, CNNs, Transformers/LLMs, and KAN, in our\nexperiments. By leveraging the geometry of the simplex, we derive a tight lower\nbound on the preserved mass s_eff (the sum of retained scores) over the\ncorresponding ordered probability simplex associated with the score vector s.\nWe further verify the effectiveness of N_eff by pruning the model with a scaled\nthreshold \\b{eta}*N_eff across a variety of criteria and models. Experiments\nsuggest that the default \\b{eta} = 1 yields a robust threshold for model\npruning while \\b{eta} not equal to 1 still serves as an optional adjustment to\nmeet specific sparsity requirements.", "AI": {"tldr": "EMP provides a universal adaptive threshold for model pruning that determines how many parameters to keep based on the Inverse Simpson index, working with any pruning criterion across various model architectures.", "motivation": "To address the fundamental question of how many entries to keep during model pruning without being dependent on specific pruning criteria or requiring parameter tuning.", "method": "EMP maps any score vector to an effective number N_eff inspired by Inverse Simpson index, then retains the N_eff highest scoring entries while zeroing the rest.", "result": "EMP produces sparse models with performance comparable to original dense networks across MLPs, CNNs, Transformers/LLMs, and KAN architectures.", "conclusion": "EMP provides a robust, parameter-free pruning rule that works universally across different pruning criteria and model types, with beta=1 as the default effective threshold."}}
{"id": "2509.26406", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26406", "abs": "https://arxiv.org/abs/2509.26406", "authors": ["Gili Goldin", "Shira Wigderson", "Ella Rabinovich", "Shuly Wintner"], "title": "An Annotation Scheme for Factuality and its Application to Parliamentary Proceedings", "comment": "@InProceedings{goldin-EtAl:2025:RANLP, author = {Goldin, Gili and\n  Wigderson, Shira and Rabinovich, Ella and Wintner, Shuly}, title = {An\n  Annotation Scheme for Factuality in Parliamentary Proceedings}, booktitle =\n  {Proceedings of RANLP 2025}, year = {2025}, address = {Varna, Bulgaria},\n  pages = {403--412} }", "summary": "Factuality assesses the extent to which a language utterance relates to\nreal-world information; it determines whether utterances correspond to facts,\npossibilities, or imaginary situations, and as such, it is instrumental for\nfact checking. Factuality is a complex notion that relies on multiple\nlinguistic signals, and has been studied in various disciplines.\n  We present a complex, multi-faceted annotation scheme of factuality that\ncombines concepts from a variety of previous works. We developed the scheme for\nHebrew, but we trust that it can be adapted to other languages. We also present\na set of almost 5,000 sentences in the domain of parliamentary discourse that\nwe manually annotated according to this scheme. We report on inter-annotator\nagreement, and experiment with various approaches to automatically predict\n(some features of) the scheme, in order to extend the annotation to a large\ncorpus.", "AI": {"tldr": "This paper presents a comprehensive factuality annotation scheme combining concepts from previous works, applied to Hebrew parliamentary discourse with manual annotation of 5,000 sentences and experiments on automatic prediction.", "motivation": "Factuality is crucial for fact checking but is complex and relies on multiple linguistic signals. The paper aims to develop a comprehensive annotation scheme to better understand and analyze factuality in language.", "method": "Developed a multi-faceted factuality annotation scheme combining previous concepts, manually annotated 5,000 Hebrew parliamentary sentences, measured inter-annotator agreement, and experimented with automatic prediction approaches.", "result": "Created a detailed annotation scheme applicable to Hebrew, achieved measurable inter-annotator agreement, and demonstrated feasibility of automatically predicting some factuality features to scale annotation to larger corpora.", "conclusion": "The proposed factuality annotation scheme is effective for Hebrew and potentially adaptable to other languages, with promising results for automated extension to large corpora through feature prediction."}}
{"id": "2509.25973", "categories": ["cs.AI", "I.2.6"], "pdf": "https://arxiv.org/pdf/2509.25973", "abs": "https://arxiv.org/abs/2509.25973", "authors": ["Junbeom Kim", "Kyuyoung Kim", "Jihoon Tack", "Dongha Lim", "Jinwoo Shin"], "title": "Scalable and Robust LLM Unlearning by Correcting Responses with Retrieved Exclusions", "comment": null, "summary": "Language models trained on web-scale corpora risk memorizing and exposing\nsensitive information, prompting the need for effective machine unlearning.\nPrior methods mainly focus on input queries to suppress sensitive outputs, yet\nthis often fails to eliminate the underlying knowledge and limits scalability.\nTo address this, we propose Corrective Unlearning with Retrieved Exclusions\n(CURE), a novel unlearning framework that verifies model outputs for leakage\nand revises them into safe responses. Specifically, CURE employs a lightweight\ncorrector that is applied to the original model to verify whether outputs\ncontain target knowledge and to rewrite them if any leakage is detected. To\nefficiently handle large-scale unlearning requests, CURE retrieves unlearning\ntargets that are relevant to the initial response and provides them as\nin-context references to the corrector for detection and conditional revision.\nBy leveraging this retrieval augmentation, the corrector can adapt to new\nunlearning requests without additional training. Extensive evaluations\ndemonstrate that CURE substantially reduces information leakage, even from\nindirect queries where prior works fall short, while maintaining response\nquality and general utility. Moreover, it demonstrates robustness under\ncontinual unlearning scenarios, making it practical for real-world\napplications.", "AI": {"tldr": "CURE is a machine unlearning framework that uses a lightweight corrector to detect and rewrite sensitive outputs, leveraging retrieval augmentation to handle large-scale unlearning without retraining.", "motivation": "Language models risk exposing sensitive information from training data, and existing unlearning methods that focus on input suppression often fail to eliminate underlying knowledge and lack scalability.", "method": "CURE employs a corrector that verifies model outputs for information leakage and rewrites them. It retrieves relevant unlearning targets as in-context references to enable detection and conditional revision without additional training.", "result": "CURE substantially reduces information leakage, including from indirect queries where prior methods fail, while maintaining response quality and general utility. It shows robustness in continual unlearning scenarios.", "conclusion": "CURE provides an effective and scalable solution for machine unlearning that is practical for real-world applications, outperforming prior methods in preventing information leakage while preserving model utility."}}
{"id": "2509.25612", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.25612", "abs": "https://arxiv.org/abs/2509.25612", "authors": ["Muhammad Imran Hossain", "Jignesh Solanki", "Sarika Khushlani Solanki"], "title": "Unsupervised Detection of Spatiotemporal Anomalies in PMU Data Using Transformer-Based BiGAN", "comment": null, "summary": "Ensuring power grid resilience requires the timely and unsupervised detection\nof anomalies in synchrophasor data streams. We introduce T-BiGAN, a novel\nframework that integrates window-attention Transformers within a bidirectional\nGenerative Adversarial Network (BiGAN) to address this challenge. Its\nself-attention encoder-decoder architecture captures complex spatio-temporal\ndependencies across the grid, while a joint discriminator enforces cycle\nconsistency to align the learned latent space with the true data distribution.\nAnomalies are flagged in real-time using an adaptive score that combines\nreconstruction error, latent space drift, and discriminator confidence.\nEvaluated on a realistic hardware-in-the-loop PMU benchmark, T-BiGAN achieves\nan ROC-AUC of 0.95 and an average precision of 0.996, significantly\noutperforming leading supervised and unsupervised methods. It shows particular\nstrength in detecting subtle frequency and voltage deviations, demonstrating\nits practical value for live, wide-area monitoring without relying on manually\nlabeled fault data.", "AI": {"tldr": "T-BiGAN is a novel unsupervised anomaly detection framework that combines window-attention Transformers with bidirectional GANs to detect anomalies in synchrophasor data streams for power grid resilience.", "motivation": "To ensure power grid resilience through timely and unsupervised detection of anomalies in synchrophasor data streams without relying on manually labeled fault data.", "method": "Integrates window-attention Transformers within a bidirectional GAN (BiGAN) with self-attention encoder-decoder architecture to capture complex spatio-temporal dependencies, and uses a joint discriminator for cycle consistency. Anomalies are detected using an adaptive score combining reconstruction error, latent space drift, and discriminator confidence.", "result": "Achieves ROC-AUC of 0.95 and average precision of 0.996 on realistic hardware-in-the-loop PMU benchmark, significantly outperforming leading supervised and unsupervised methods. Shows particular strength in detecting subtle frequency and voltage deviations.", "conclusion": "T-BiGAN demonstrates practical value for live, wide-area monitoring of power grids by providing effective unsupervised anomaly detection without requiring labeled fault data."}}
{"id": "2509.26415", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26415", "abs": "https://arxiv.org/abs/2509.26415", "authors": ["Ravi Kiran Chikkala", "Tatiana Anikina", "Natalia Skachkova", "Ivan Vykopal", "Rodrigo Agerri", "Josef van Genabith"], "title": "Automatic Fact-checking in English and Telugu", "comment": null, "summary": "False information poses a significant global challenge, and manually\nverifying claims is a time-consuming and resource-intensive process. In this\nresearch paper, we experiment with different approaches to investigate the\neffectiveness of large language models (LLMs) in classifying factual claims by\ntheir veracity and generating justifications in English and Telugu. The key\ncontributions of this work include the creation of a bilingual English-Telugu\ndataset and the benchmarking of different veracity classification approaches\nbased on LLMs.", "AI": {"tldr": "This paper explores using large language models (LLMs) for automated fact-checking by classifying claim veracity and generating justifications in both English and Telugu languages.", "motivation": "Manual fact-checking is time-consuming and resource-intensive, creating a need for automated solutions to combat false information globally.", "method": "The researchers experimented with different LLM approaches for veracity classification and created a bilingual English-Telugu dataset to benchmark these methods.", "result": "The study evaluated the effectiveness of various LLM approaches in classifying factual claims and generating justifications in both languages.", "conclusion": "The work contributes a bilingual dataset and benchmarks LLM-based approaches for automated fact-checking, addressing the challenge of false information verification."}}
{"id": "2509.25991", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25991", "abs": "https://arxiv.org/abs/2509.25991", "authors": ["Haiyang Li", "Yaxiong Wang", "Lianwei Wu", "Lechao Cheng", "Zhun Zhong"], "title": "Towards Unified Multimodal Misinformation Detection in Social Media: A Benchmark Dataset and Baseline", "comment": null, "summary": "In recent years, detecting fake multimodal content on social media has drawn\nincreasing attention. Two major forms of deception dominate: human-crafted\nmisinformation (e.g., rumors and misleading posts) and AI-generated content\nproduced by image synthesis models or vision-language models (VLMs). Although\nboth share deceptive intent, they are typically studied in isolation. NLP\nresearch focuses on human-written misinformation, while the CV community\ntargets AI-generated artifacts. As a result, existing models are often\nspecialized for only one type of fake content. In real-world scenarios,\nhowever, the type of a multimodal post is usually unknown, limiting the\neffectiveness of such specialized systems. To bridge this gap, we construct the\nOmnibus Dataset for Multimodal News Deception (OmniFake), a comprehensive\nbenchmark of 127K samples that integrates human-curated misinformation from\nexisting resources with newly synthesized AI-generated examples. Based on this\ndataset, we propose Unified Multimodal Fake Content Detection (UMFDet), a\nframework designed to handle both forms of deception. UMFDet leverages a VLM\nbackbone augmented with a Category-aware Mixture-of-Experts (MoE) Adapter to\ncapture category-specific cues, and an attribution chain-of-thought mechanism\nthat provides implicit reasoning guidance for locating salient deceptive\nsignals. Extensive experiments demonstrate that UMFDet achieves robust and\nconsistent performance across both misinformation types, outperforming\nspecialized baselines and offering a practical solution for real-world\nmultimodal deception detection.", "AI": {"tldr": "The paper introduces OmniFake dataset and UMFDet framework for unified detection of both human-crafted misinformation and AI-generated fake content in multimodal posts, addressing the limitation of existing specialized models.", "motivation": "Current fake content detection systems are specialized for either human-written misinformation (NLP focus) or AI-generated content (CV focus), but real-world scenarios involve unknown content types, limiting effectiveness of specialized approaches.", "method": "Proposes UMFDet framework with VLM backbone, Category-aware Mixture-of-Experts Adapter for category-specific cues, and attribution chain-of-thought mechanism for implicit reasoning guidance on deceptive signals.", "result": "Extensive experiments show UMFDet achieves robust and consistent performance across both misinformation types, outperforming specialized baselines.", "conclusion": "UMFDet offers a practical unified solution for real-world multimodal deception detection that handles both human-crafted and AI-generated fake content effectively."}}
{"id": "2509.25622", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25622", "abs": "https://arxiv.org/abs/2509.25622", "authors": ["Zhendong Mi", "Bian Sun", "Grace Li Zhang", "Shaoyi Huang"], "title": "Layer-wise dynamic rank for compressing large language models", "comment": "10 pages, 5 figures", "summary": "Large language models (LLMs) have rapidly scaled in size, bringing severe\nmemory and computational challenges that hinder their deployment. Singular\nValue Decomposition (SVD)-based compression has emerged as an appealing\npost-training compression technique for LLMs, yet most existing methods apply a\nuniform compression ratio across all layers, implicitly assuming homogeneous\ninformation included in various layers. This overlooks the substantial\nintra-layer heterogeneity observed in LLMs, where middle layers tend to encode\nricher information while early and late layers are more redundant. In this\nwork, we revisit the existing SVD-based compression method and propose D-Rank,\na framework with layer-wise balanced Dynamic Rank allocation for LLMs\ncompression. We first introduce effective rank as a principled metric to\nmeasure the information density of weight matrices, and then allocate ranks via\na Lagrange multiplier-based optimization scheme to adaptively assign more\ncapacity to groups with higher information density under a fixed compression\nratio. Moreover, we rebalance the allocated ranks across attention layers to\naccount for their varying importance and extend D-Rank to latest LLMs with\ngrouped-query attention. Extensive experiments on various LLMs with different\nscales across multiple compression ratios demonstrate that D-Rank consistently\noutperforms SVD-LLM, ASVD, and Basis Sharing, achieving more than 15 lower\nperplexity with LLaMA-3-8B model on C4 datasets at 20% compression ratio and up\nto 5% higher zero-shot reasoning accuracy with LLaMA-7B model at 40%\ncompression ratio while achieving even higher throughput.", "AI": {"tldr": "D-Rank is a dynamic rank allocation framework for LLM compression that addresses layer heterogeneity by allocating more capacity to information-rich middle layers using effective rank metrics and Lagrange optimization.", "motivation": "Existing SVD-based compression methods use uniform compression ratios across all layers, ignoring the substantial intra-layer heterogeneity in LLMs where middle layers encode richer information while early and late layers are more redundant.", "method": "Proposes D-Rank framework with layer-wise balanced dynamic rank allocation: 1) Uses effective rank to measure information density of weight matrices, 2) Allocates ranks via Lagrange multiplier-based optimization to assign more capacity to higher information density groups, 3) Rebalances ranks across attention layers accounting for varying importance, 4) Extends to LLMs with grouped-query attention.", "result": "Extensive experiments show D-Rank consistently outperforms SVD-LLM, ASVD, and Basis Sharing: achieves >15% lower perplexity with LLaMA-3-8B on C4 at 20% compression ratio, and up to 5% higher zero-shot reasoning accuracy with LLaMA-7B at 40% compression ratio while achieving higher throughput.", "conclusion": "D-Rank demonstrates that adaptive layer-wise rank allocation based on information density significantly improves LLM compression performance compared to uniform compression approaches, effectively addressing the heterogeneity in different layers."}}
{"id": "2509.26431", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26431", "abs": "https://arxiv.org/abs/2509.26431", "authors": ["Yanbin Fu", "Hong Jiao", "Tianyi Zhou", "Robert W. Lissitz", "Nan Zhang", "Ming Li", "Qingshu Xu", "Sydney Peters"], "title": "Text-Based Approaches to Item Alignment to Content Standards in Large-Scale Reading & Writing Tests", "comment": "Preprint submitted to Journal of Educational Measurement", "summary": "Aligning test items to content standards is a critical step in test\ndevelopment to collect validity evidence based on content. Item alignment has\ntypically been conducted by human experts. This judgmental process can be\nsubjective and time-consuming. This study investigated the performance of\nfine-tuned small language models (SLMs) for automated item alignment using data\nfrom a large-scale standardized reading and writing test for college\nadmissions. Different SLMs were trained for alignment at both domain and skill\nlevels respectively with 10 skills mapped to 4 content domains. The model\nperformance was evaluated in multiple criteria on two testing datasets. The\nimpact of types and sizes of the input data for training was investigated.\nResults showed that including more item text data led to substantially better\nmodel performance, surpassing the improvements induced by sample size increase\nalone. For comparison, supervised machine learning models were trained using\nthe embeddings from the multilingual-E5-large-instruct model. The study results\nshowed that fine-tuned SLMs consistently outperformed the embedding-based\nsupervised machine learning models, particularly for the more fine-grained\nskill alignment. To better understand model misclassifications, multiple\nsemantic similarity analysis including pairwise cosine similarity,\nKullback-Leibler divergence of embedding distributions, and two-dimension\nprojections of item embeddings were conducted. These analyses consistently\nshowed that certain skills in SAT and PSAT were semantically too close,\nproviding evidence for the observed misclassification.", "AI": {"tldr": "Fine-tuned small language models (SLMs) outperform embedding-based supervised models for automated item alignment in standardized tests, with performance improving more from including more item text data than just increasing sample size.", "motivation": "Human expert alignment of test items to content standards is subjective and time-consuming, creating need for automated methods to improve efficiency and objectivity.", "method": "Fine-tuned small language models for automated item alignment at domain and skill levels using data from large-scale standardized reading/writing tests; compared with embedding-based supervised ML models; conducted semantic similarity analysis.", "result": "SLMs consistently outperformed embedding-based models, especially for fine-grained skill alignment; including more item text data substantially improved performance; semantic analysis showed certain skills were too close, explaining misclassifications.", "conclusion": "Fine-tuned SLMs are effective for automated item alignment, offering a viable alternative to human expert judgment with better performance than embedding-based approaches."}}
{"id": "2509.26002", "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26002", "abs": "https://arxiv.org/abs/2509.26002", "authors": ["Ardian Selmonaj", "Giacomo Del Rio", "Adrian Schneider", "Alessandro Antonucci"], "title": "Towards Human Engagement with Realistic AI Combat Pilots", "comment": "13th International Conference on Human-Agent Interaction (HAI) 2025", "summary": "We present a system that enables real-time interaction between human users\nand agents trained to control fighter jets in simulated 3D air combat\nscenarios. The agents are trained in a dedicated environment using Multi-Agent\nReinforcement Learning. A communication link is developed to allow seamless\ndeployment of trained agents into VR-Forces, a widely used defense simulation\ntool for realistic tactical scenarios. This integration allows mixed\nsimulations where human-controlled entities engage with intelligent agents\nexhibiting distinct combat behaviors. Our interaction model creates new\nopportunities for human-agent teaming, immersive training, and the exploration\nof innovative tactics in defense contexts.", "AI": {"tldr": "A system enabling real-time interaction between human users and AI agents trained for fighter jet control in 3D air combat simulations, integrated with VR-Forces defense simulation tool.", "motivation": "To create opportunities for human-agent teaming, immersive training, and exploration of innovative tactics in defense contexts through mixed simulations.", "method": "Training agents using Multi-Agent Reinforcement Learning in dedicated environments, then developing a communication link for seamless deployment into VR-Forces simulation tool.", "result": "Successful integration allowing mixed simulations where human-controlled entities engage with intelligent agents exhibiting distinct combat behaviors.", "conclusion": "The system enables new possibilities for defense training and tactical exploration through real-time human-AI interaction in realistic combat scenarios."}}
{"id": "2509.25631", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25631", "abs": "https://arxiv.org/abs/2509.25631", "authors": ["Jason Stock", "Troy Arcomano", "Rao Kotamarthi"], "title": "Swift: An Autoregressive Consistency Model for Efficient Weather Forecasting", "comment": "17 pages and 15 figures", "summary": "Diffusion models offer a physically grounded framework for probabilistic\nweather forecasting, but their typical reliance on slow, iterative solvers\nduring inference makes them impractical for subseasonal-to-seasonal (S2S)\napplications where long lead-times and domain-driven calibration are essential.\nTo address this, we introduce Swift, a single-step consistency model that, for\nthe first time, enables autoregressive finetuning of a probability flow model\nwith a continuous ranked probability score (CRPS) objective. This eliminates\nthe need for multi-model ensembling or parameter perturbations. Results show\nthat Swift produces skillful 6-hourly forecasts that remain stable for up to 75\ndays, running $39\\times$ faster than state-of-the-art diffusion baselines while\nachieving forecast skill competitive with the numerical-based, operational IFS\nENS. This marks a step toward efficient and reliable ensemble forecasting from\nmedium-range to seasonal-scales.", "AI": {"tldr": "Swift is a single-step consistency model that enables efficient probabilistic weather forecasting by eliminating slow iterative solvers, achieving 39x speedup over diffusion baselines while maintaining competitive forecast skill up to 75 days.", "motivation": "Current diffusion models for weather forecasting rely on slow iterative solvers during inference, making them impractical for subseasonal-to-seasonal applications that require long lead-times and domain-driven calibration.", "method": "Introduce Swift, a single-step consistency model that enables autoregressive finetuning of a probability flow model with continuous ranked probability score (CRPS) objective, eliminating the need for multi-model ensembling or parameter perturbations.", "result": "Swift produces skillful 6-hourly forecasts stable for up to 75 days, running 39x faster than state-of-the-art diffusion baselines while achieving forecast skill competitive with operational IFS ENS numerical model.", "conclusion": "This represents a step toward efficient and reliable ensemble forecasting from medium-range to seasonal scales, making probabilistic weather forecasting more practical for operational applications."}}
{"id": "2509.26435", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26435", "abs": "https://arxiv.org/abs/2509.26435", "authors": ["Sangwon Ryu", "Heejin Do", "Yunsu Kim", "Gary Geunbae Lee", "Jungseul Ok"], "title": "Adaptive Planning for Multi-Attribute Controllable Summarization with Monte Carlo Tree Search", "comment": null, "summary": "Controllable summarization moves beyond generic outputs toward human-aligned\nsummaries guided by specified attributes. In practice, the interdependence\namong attributes makes it challenging for language models to satisfy correlated\nconstraints consistently. Moreover, previous approaches often require\nper-attribute fine-tuning, limiting flexibility across diverse summary\nattributes. In this paper, we propose adaptive planning for multi-attribute\ncontrollable summarization (PACO), a training-free framework that reframes the\ntask as planning the order of sequential attribute control with a customized\nMonte Carlo Tree Search (MCTS). In PACO, nodes represent summaries, and actions\ncorrespond to single-attribute adjustments, enabling progressive refinement of\nonly the attributes requiring further control. This strategy adaptively\ndiscovers optimal control orders, ultimately producing summaries that\neffectively meet all constraints. Extensive experiments across diverse domains\nand models demonstrate that PACO achieves robust multi-attribute\ncontrollability, surpassing both LLM-based self-planning models and fine-tuned\nbaselines. Remarkably, PACO with Llama-3.2-1B rivals the controllability of the\nmuch larger Llama-3.3-70B baselines. With larger models, PACO achieves superior\ncontrol performance, outperforming all competitors.", "AI": {"tldr": "PACO is a training-free framework for multi-attribute controllable summarization that uses adaptive planning with Monte Carlo Tree Search to optimize attribute control order, achieving superior performance without fine-tuning.", "motivation": "Current controllable summarization approaches struggle with interdependent attributes and require per-attribute fine-tuning, limiting flexibility and consistency in meeting multiple constraints simultaneously.", "method": "Reframes the task as planning attribute control order using customized Monte Carlo Tree Search, where nodes are summaries and actions are single-attribute adjustments, enabling progressive refinement of only needed attributes.", "result": "PACO achieves robust multi-attribute controllability, surpassing LLM-based self-planning models and fine-tuned baselines. With Llama-3.2-1B, it rivals the controllability of much larger Llama-3.3-70B models.", "conclusion": "PACO provides an effective training-free solution for multi-attribute controllable summarization that adaptively discovers optimal control orders and outperforms existing approaches across diverse domains and models."}}
{"id": "2509.26037", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26037", "abs": "https://arxiv.org/abs/2509.26037", "authors": ["Zhe Li", "Zhiwei Lin", "Yongtao Wang"], "title": "CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided Neural Architecture Search", "comment": null, "summary": "The integration of Large Language Models (LLMs) with Neural Architecture\nSearch (NAS) has introduced new possibilities for automating the design of\nneural architectures. However, most existing methods face critical limitations,\nincluding architectural invalidity, computational inefficiency, and inferior\nperformance compared to traditional NAS. In this work, we present Collaborative\nLLM-based NAS (CoLLM-NAS), a two-stage NAS framework with knowledge-guided\nsearch driven by two complementary LLMs. Specifically, we propose a Navigator\nLLM to guide search direction and a Generator LLM to synthesize high-quality\ncandidates, with a dedicated Coordinator module to manage their interaction.\nCoLLM-NAS efficiently guides the search process by combining LLMs' inherent\nknowledge of structured neural architectures with progressive knowledge from\niterative feedback and historical trajectory. Experimental results on ImageNet\nand NAS-Bench-201 show that CoLLM-NAS surpasses existing NAS methods and\nconventional search algorithms, achieving new state-of-the-art results.\nFurthermore, CoLLM-NAS consistently enhances the performance and efficiency of\nvarious two-stage NAS methods (e.g., OFA, SPOS, and AutoFormer) across diverse\nsearch spaces (e.g., MobileNet, ShuffleNet, and AutoFormer), demonstrating its\nexcellent generalization.", "AI": {"tldr": "CoLLM-NAS is a two-stage NAS framework using two complementary LLMs (Navigator and Generator) with a Coordinator module to overcome limitations of existing LLM-NAS methods, achieving SOTA results on ImageNet and NAS-Bench-201.", "motivation": "To address critical limitations in existing LLM-NAS methods including architectural invalidity, computational inefficiency, and inferior performance compared to traditional NAS.", "method": "Two-stage NAS framework with Navigator LLM guiding search direction, Generator LLM synthesizing high-quality candidates, and Coordinator module managing their interaction, combining LLMs' inherent knowledge with iterative feedback and historical trajectory.", "result": "Surpasses existing NAS methods and conventional search algorithms, achieving new SOTA results on ImageNet and NAS-Bench-201. Consistently enhances performance and efficiency of various two-stage NAS methods across diverse search spaces.", "conclusion": "CoLLM-NAS demonstrates excellent generalization and effectively overcomes limitations of existing LLM-NAS approaches through collaborative LLM-driven architecture search."}}
{"id": "2509.25637", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25637", "abs": "https://arxiv.org/abs/2509.25637", "authors": ["Kotaro Yoshida", "Atsushi Nitanda"], "title": "How Does Preconditioning Guide Feature Learning in Deep Neural Networks?", "comment": null, "summary": "Preconditioning is widely used in machine learning to accelerate convergence\non the empirical risk, yet its role on the expected risk remains underexplored.\nIn this work, we investigate how preconditioning affects feature learning and\ngeneralization performance. We first show that the input information available\nto the model is conveyed solely through the Gram matrix defined by the\npreconditioner's metric, thereby inducing a controllable spectral bias on\nfeature learning. Concretely, instantiating the preconditioner as the $p$-th\npower of the input covariance matrix and within a single-index teacher model,\nwe prove that in generalization, the exponent $p$ and the alignment between the\nteacher and the input spectrum are crucial factors. We further investigate how\nthe interplay between these factors influences feature learning from three\ncomplementary perspectives: (i) Robustness to noise, (ii) Out-of-distribution\ngeneralization, and (iii) Forward knowledge transfer. Our results indicate that\nthe learned feature representations closely mirror the spectral bias introduced\nby the preconditioner -- favoring components that are emphasized and exhibiting\nreduced sensitivity to those that are suppressed. Crucially, we demonstrate\nthat generalization is significantly enhanced when this spectral bias is\naligned with that of the teacher.", "AI": {"tldr": "Preconditioning affects generalization by inducing spectral bias through the Gram matrix metric. The exponent p in preconditioner and alignment with teacher spectrum are crucial for feature learning and generalization performance.", "motivation": "To understand how preconditioning affects feature learning and generalization performance on expected risk, beyond just accelerating convergence on empirical risk.", "method": "Analyze preconditioning as p-th power of input covariance matrix within single-index teacher model, examining spectral bias effects on feature learning from three perspectives: noise robustness, OOD generalization, and forward knowledge transfer.", "result": "Learned features mirror preconditioner's spectral bias - favoring emphasized components and reducing sensitivity to suppressed ones. Generalization significantly improves when spectral bias aligns with teacher's bias.", "conclusion": "Preconditioning's spectral bias plays crucial role in generalization, with optimal performance achieved when preconditioner's bias aligns with teacher model's spectral characteristics."}}
{"id": "2509.26461", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26461", "abs": "https://arxiv.org/abs/2509.26461", "authors": ["Yuyang Cheng", "Linyue Cai", "Changwei Peng", "Yumiao Xu", "Rongfang Bie", "Yong Zhao"], "title": "CreAgentive: An Agent Workflow Driven Multi-Category Creative Generation Engine", "comment": null, "summary": "We present CreAgentive, an agent workflow driven multi-category creative\ngeneration engine that addresses four key limitations of contemporary large\nlanguage models in writing stories, drama and other categories of creatives:\nrestricted genre diversity, insufficient output length, weak narrative\ncoherence, and inability to enforce complex structural constructs. At its core,\nCreAgentive employs a Story Prototype, which is a genre-agnostic, knowledge\ngraph-based narrative representation that decouples story logic from stylistic\nrealization by encoding characters, events, and environments as semantic\ntriples. CreAgentive engages a three-stage agent workflow that comprises: an\nInitialization Stage that constructs a user-specified narrative skeleton; a\nGeneration Stage in which long- and short-term objectives guide multi-agent\ndialogues to instantiate the Story Prototype; a Writing Stage that leverages\nthis prototype to produce multi-genre text with advanced structures such as\nretrospection and foreshadowing. This architecture reduces storage redundancy\nand overcomes the typical bottlenecks of long-form generation. In extensive\nexperiments, CreAgentive generates thousands of chapters with stable quality\nand low cost (less than $1 per 100 chapters) using a general-purpose backbone\nmodel. To evaluate performance, we define a two-dimensional framework with 10\nnarrative indicators measuring both quality and length. Results show that\nCreAgentive consistently outperforms strong baselines and achieves robust\nperformance across diverse genres, approaching the quality of human-authored\nnovels.", "AI": {"tldr": "CreAgentive is an agent workflow system for creative generation that addresses LLM limitations in genre diversity, output length, narrative coherence, and structural constructs through a Story Prototype representation and three-stage workflow.", "motivation": "To overcome four key limitations of contemporary LLMs in creative writing: restricted genre diversity, insufficient output length, weak narrative coherence, and inability to enforce complex structural constructs.", "method": "Uses a Story Prototype (genre-agnostic knowledge graph-based narrative representation) and three-stage agent workflow: Initialization (constructs narrative skeleton), Generation (multi-agent dialogues guided by objectives), and Writing (produces multi-genre text with advanced structures).", "result": "Generates thousands of chapters with stable quality and low cost (<$1 per 100 chapters), outperforms strong baselines in 10 narrative indicators, and approaches human-authored novel quality across diverse genres.", "conclusion": "CreAgentive effectively addresses key LLM limitations in creative generation through its Story Prototype and agent workflow architecture, achieving robust performance across genres with low computational cost."}}
{"id": "2509.26080", "categories": ["cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.26080", "abs": "https://arxiv.org/abs/2509.26080", "authors": ["Emma Rose Madden"], "title": "Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research", "comment": null, "summary": "Large Language Models (LLMs) are being increasingly used as synthetic agents\nin social science, in applications ranging from augmenting survey responses to\npowering multi-agent simulations. Because strong prediction plus conditioning\nprompts, token log-probs, and repeated sampling mimic Bayesian workflows, their\noutputs can be misinterpreted as posterior-like evidence from a coherent model.\nHowever, prediction does not equate to probabilism, and accurate points do not\nimply calibrated uncertainty. This paper outlines cautions that should be taken\nwhen interpreting LLM outputs and proposes a pragmatic reframing for the social\nsciences in which LLMs are used as high-capacity pattern matchers for\nquasi-predictive interpolation under explicit scope conditions and not as\nsubstitutes for probabilistic inference. Practical guardrails such as\nindependent draws, preregistered human baselines, reliability-aware validation,\nand subgroup calibration, are introduced so that researchers may engage in\nuseful prototyping and forecasting while avoiding category errors.", "AI": {"tldr": "LLMs are increasingly used in social sciences but their outputs shouldn't be misinterpreted as probabilistic evidence. The paper proposes reframing LLMs as pattern matchers with explicit scope conditions and introduces practical guardrails for responsible use.", "motivation": "To address the misinterpretation of LLM outputs as posterior-like evidence in social science applications, where prediction doesn't equate to probabilism and accurate points don't imply calibrated uncertainty.", "method": "Proposes a pragmatic reframing where LLMs are used as high-capacity pattern matchers for quasi-predictive interpolation under explicit scope conditions, not as substitutes for probabilistic inference.", "result": "Introduces practical guardrails including independent draws, preregistered human baselines, reliability-aware validation, and subgroup calibration to enable useful prototyping and forecasting while avoiding category errors.", "conclusion": "LLMs should be used cautiously in social sciences as pattern matchers with explicit limitations, not as probabilistic inference tools, with specific guardrails to ensure responsible application."}}
{"id": "2509.25646", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.25646", "abs": "https://arxiv.org/abs/2509.25646", "authors": ["Lei Ma", "Ling Guo", "Hao Wu", "Tao Zhou"], "title": "Deep set based operator learning with uncertainty quantification", "comment": null, "summary": "Learning operators from data is central to scientific machine learning. While\nDeepONets are widely used for their ability to handle complex domains, they\nrequire fixed sensor numbers and locations, lack mechanisms for uncertainty\nquantification (UQ), and are thus limited in practical applicability. Recent\npermutationinvariant extensions, such as the Variable-Input Deep Operator\nNetwork (VIDON), relax these sensor constraints but still rely on sufficiently\ndense observations and cannot capture uncertainties arising from incomplete\nmeasurements or from operators with inherent randomness. To address these\nchallenges, we propose UQ-SONet, a permutation-invariant operator learning\nframework with built-in UQ. Our model integrates a set transformer embedding to\nhandle sparse and variable sensor locations, and employs a conditional\nvariational autoencoder (cVAE) to approximate the conditional distribution of\nthe solution operator. By minimizing the negative ELBO, UQ-SONet provides\nprincipled uncertainty estimation while maintaining predictive accuracy.\nNumerical experiments on deterministic and stochastic PDEs, including the\nNavier-Stokes equation, demonstrate the robustness and effectiveness of the\nproposed framework.", "AI": {"tldr": "UQ-SONet is a permutation-invariant operator learning framework with built-in uncertainty quantification that handles sparse and variable sensor locations using set transformer embeddings and conditional variational autoencoders.", "motivation": "Existing operator learning methods like DeepONets have limitations: they require fixed sensor configurations, lack uncertainty quantification mechanisms, and cannot handle sparse observations or operators with inherent randomness.", "method": "Integrates set transformer embedding to handle variable sensor locations and uses conditional variational autoencoder (cVAE) to approximate the conditional distribution of solution operators, minimizing negative ELBO for principled uncertainty estimation.", "result": "Numerical experiments on deterministic and stochastic PDEs (including Navier-Stokes) demonstrate the framework's robustness and effectiveness in providing uncertainty quantification while maintaining predictive accuracy.", "conclusion": "UQ-SONet successfully addresses key limitations of existing operator learning methods by providing built-in uncertainty quantification and handling sparse, variable sensor configurations, making it more practical for real-world applications."}}
{"id": "2509.26476", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.26476", "abs": "https://arxiv.org/abs/2509.26476", "authors": ["Yash Akhauri", "Xingyou Song", "Arissa Wongpanich", "Bryan Lewandowski", "Mohamed S. Abdelfattah"], "title": "Regression Language Models for Code", "comment": null, "summary": "We study code-to-metric regression: predicting numeric outcomes of code\nexecutions, a challenging task due to the open-ended nature of programming\nlanguages. While prior methods have resorted to heavy and domain-specific\nfeature engineering, we show that a single unified Regression Language Model\n(RLM) can simultaneously predict directly from text, (i) the memory footprint\nof code across multiple high-level languages such as Python and C++, (ii) the\nlatency of Triton GPU kernels, and (iii) the accuracy and speed of trained\nneural networks represented in ONNX. In particular, a relatively small 300M\nparameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on\ncompetitive programming submissions from APPS, and a single unified model\nachieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet.\nFurthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five\nclassic NAS design spaces previously dominated by graph neural networks, and\nsimultaneously predict architecture latencies on numerous hardware platforms.", "AI": {"tldr": "A unified Regression Language Model (RLM) can predict multiple code execution metrics across different programming languages and hardware platforms without domain-specific feature engineering.", "motivation": "Code-to-metric regression is challenging due to programming language complexity, and prior methods required heavy feature engineering. The authors aim to create a unified approach.", "method": "Use a 300M parameter Regression Language Model (RLM) initialized from T5Gemma to predict numeric outcomes directly from code text across multiple languages and hardware.", "result": "The RLM achieves >0.9 Spearman-rank on APPS competitive programming, >0.5 average Spearman-rank across 17 CodeNet languages, and highest 0.46 Kendall-Tau on NAS design spaces.", "conclusion": "A single unified RLM can effectively predict diverse code execution metrics across multiple domains without specialized feature engineering."}}
{"id": "2509.26100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26100", "abs": "https://arxiv.org/abs/2509.26100", "authors": ["Yixu Wang", "Xin Wang", "Yang Yao", "Xinyuan Li", "Yan Teng", "Xingjun Ma", "Yingchun Wang"], "title": "SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs", "comment": null, "summary": "The rapid integration of Large Language Models (LLMs) into high-stakes\ndomains necessitates reliable safety and compliance evaluation. However,\nexisting static benchmarks are ill-equipped to address the dynamic nature of AI\nrisks and evolving regulations, creating a critical safety gap. This paper\nintroduces a new paradigm of agentic safety evaluation, reframing evaluation as\na continuous and self-evolving process rather than a one-time audit. We then\npropose a novel multi-agent framework SafeEvalAgent, which autonomously ingests\nunstructured policy documents to generate and perpetually evolve a\ncomprehensive safety benchmark. SafeEvalAgent leverages a synergistic pipeline\nof specialized agents and incorporates a Self-evolving Evaluation loop, where\nthe system learns from evaluation results to craft progressively more\nsophisticated and targeted test cases. Our experiments demonstrate the\neffectiveness of SafeEvalAgent, showing a consistent decline in model safety as\nthe evaluation hardens. For instance, GPT-5's safety rate on the EU AI Act\ndrops from 72.50% to 36.36% over successive iterations. These findings reveal\nthe limitations of static assessments and highlight our framework's ability to\nuncover deep vulnerabilities missed by traditional methods, underscoring the\nurgent need for dynamic evaluation ecosystems to ensure the safe and\nresponsible deployment of advanced AI.", "AI": {"tldr": "Proposes SafeEvalAgent, a multi-agent framework for dynamic safety evaluation of LLMs that autonomously evolves test cases from policy documents, revealing deeper vulnerabilities than static benchmarks.", "motivation": "Existing static benchmarks cannot address dynamic AI risks and evolving regulations, creating a safety gap in high-stakes LLM deployment.", "method": "Multi-agent framework with specialized agents that ingest unstructured policy documents to generate and perpetually evolve safety benchmarks through a self-evolving evaluation loop.", "result": "Demonstrates consistent decline in model safety as evaluation hardens, e.g., GPT-5's safety rate on EU AI Act drops from 72.50% to 36.36% over iterations.", "conclusion": "Reveals limitations of static assessments and highlights the need for dynamic evaluation ecosystems to ensure safe and responsible AI deployment."}}
{"id": "2509.25647", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25647", "abs": "https://arxiv.org/abs/2509.25647", "authors": ["Fangji Wang", "Panagiotis Tsiotras"], "title": "BaB-prob: Branch and Bound with Preactivation Splitting for Probabilistic Verification of Neural Networks", "comment": null, "summary": "Branch-and-bound with preactivation splitting has been shown highly effective\nfor deterministic verification of neural networks. In this paper, we extend\nthis framework to the probabilistic setting. We propose BaB-prob that\niteratively divides the original problem into subproblems by splitting\npreactivations and leverages linear bounds computed by linear bound propagation\nto bound the probability for each subproblem. We prove soundness and\ncompleteness of BaB-prob for feedforward-ReLU neural networks. Furthermore, we\nintroduce the notion of uncertainty level and design two efficient strategies\nfor preactivation splitting, yielding BaB-prob-ordered and BaB+BaBSR-prob. We\nevaluate BaB-prob on untrained networks, MNIST and CIFAR-10 models,\nrespectively, and VNN-COMP 2025 benchmarks. Across these settings, our approach\nconsistently outperforms state-of-the-art approaches in medium- to\nhigh-dimensional input problems.", "AI": {"tldr": "BaB-prob extends branch-and-bound with preactivation splitting to probabilistic verification of neural networks, proving soundness and completeness for feedforward-ReLU networks and outperforming state-of-the-art methods.", "motivation": "Extend the effective deterministic branch-and-bound verification framework with preactivation splitting to the probabilistic setting for neural network verification.", "method": "BaB-prob iteratively divides problems into subproblems by splitting preactivations, uses linear bound propagation to bound probabilities, introduces uncertainty level concept, and employs efficient preactivation splitting strategies (BaB-prob-ordered and BaB+BaBSR-prob).", "result": "BaB-prob consistently outperforms state-of-the-art approaches on untrained networks, MNIST and CIFAR-10 models, and VNN-COMP 2025 benchmarks, especially in medium- to high-dimensional input problems.", "conclusion": "The proposed BaB-prob framework successfully extends branch-and-bound verification to probabilistic settings, demonstrating superior performance across various network types and benchmarks."}}
{"id": "2509.26488", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26488", "abs": "https://arxiv.org/abs/2509.26488", "authors": ["Zigeng Chen", "Gongfan Fang", "Xinyin Ma", "Ruonan Yu", "Xinchao Wang"], "title": "dParallel: Learnable Parallel Decoding for dLLMs", "comment": "Working in progress, code base: https://github.com/czg1225/dParallel", "summary": "Diffusion large language models (dLLMs) have recently drawn considerable\nattention within the research community as a promising alternative to\nautoregressive generation, offering parallel token prediction and lower\ninference latency. Yet, their parallel decoding potential remains largely\nunderexplored, as existing open-source models still require nearly token-length\ndecoding steps to ensure performance. To address this, we introduce dParallel,\na simple and effective method that unlocks the inherent parallelism of dLLMs\nfor fast sampling. We identify that the key bottleneck to parallel decoding\narises from the sequential certainty convergence for masked tokens. Building on\nthis insight, we introduce the core of our approach: certainty-forcing\ndistillation, a novel training strategy that distills the model to follow its\noriginal sampling trajectories while enforcing it to achieve high certainty on\nmasked tokens more rapidly and in parallel. Extensive experiments across\nvarious benchmarks demonstrate that our method can dramatically reduce the\nnumber of decoding steps while maintaining performance. When applied to the\nLLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on\nGSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP\nbenchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup\nwhile maintaining accuracy. Our code is available at\nhttps://github.com/czg1225/dParallel", "AI": {"tldr": "dParallel is a method that enables parallel decoding in diffusion large language models (dLLMs) by using certainty-forcing distillation to reduce decoding steps while maintaining performance.", "motivation": "Existing dLLMs require nearly token-length decoding steps, limiting their parallel decoding potential and inference speed despite their theoretical advantages over autoregressive models.", "method": "The approach identifies sequential certainty convergence as the bottleneck and introduces certainty-forcing distillation, which trains the model to achieve high certainty on masked tokens more rapidly and in parallel while following original sampling trajectories.", "result": "Applied to LLaDA-8B-Instruct, dParallel reduces decoding steps from 256 to 30 on GSM8K (8.5x speedup) and from 256 to 24 on MBPP (10.5x speedup) without performance degradation.", "conclusion": "dParallel effectively unlocks the inherent parallelism of dLLMs, enabling significant inference speedups while maintaining model performance across various benchmarks."}}
{"id": "2509.26128", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26128", "abs": "https://arxiv.org/abs/2509.26128", "authors": ["Asmita Sengupta", "David Antony Selby", "Sebastian Josef Vollmer", "Gerrit Gro\u00dfmann"], "title": "MEDAKA: Construction of Biomedical Knowledge Graphs Using Large Language Models", "comment": "9 pages, 5 figures, 2 tables", "summary": "Knowledge graphs (KGs) are increasingly used to represent biomedical\ninformation in structured, interpretable formats. However, existing biomedical\nKGs often focus narrowly on molecular interactions or adverse events,\noverlooking the rich data found in drug leaflets. In this work, we present (1)\na hackable, end-to-end pipeline to create KGs from unstructured online content\nusing a web scraper and an LLM; and (2) a curated dataset, MEDAKA, generated by\napplying this method to publicly available drug leaflets. The dataset captures\nclinically relevant attributes such as side effects, warnings,\ncontraindications, ingredients, dosage guidelines, storage instructions and\nphysical characteristics. We evaluate it through manual inspection and with an\nLLM-as-a-Judge framework, and compare its coverage with existing biomedical KGs\nand databases. We expect MEDAKA to support tasks such as patient safety\nmonitoring and drug recommendation. The pipeline can also be used for\nconstructing KGs from unstructured texts in other domains. Code and dataset are\navailable at https://github.com/medakakg/medaka.", "AI": {"tldr": "This paper presents MEDAKA, a hackable pipeline for creating knowledge graphs from drug leaflets, and a curated dataset capturing clinically relevant drug information.", "motivation": "Existing biomedical knowledge graphs focus narrowly on molecular interactions or adverse events, overlooking rich data in drug leaflets that contain comprehensive clinical information.", "method": "Developed an end-to-end pipeline using web scraper and LLM to extract structured information from unstructured online drug leaflets, creating the MEDAKA dataset with clinical attributes.", "result": "Created MEDAKA dataset capturing side effects, warnings, contraindications, ingredients, dosage guidelines, storage instructions and physical characteristics, evaluated through manual inspection and LLM-as-a-Judge framework.", "conclusion": "MEDAKA supports patient safety monitoring and drug recommendation tasks, and the pipeline can be adapted for constructing knowledge graphs from unstructured texts in other domains."}}
{"id": "2509.25665", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25665", "abs": "https://arxiv.org/abs/2509.25665", "authors": ["Qihang Yao", "Constantine Dovrolis"], "title": "Growing Winning Subnetworks, Not Pruning Them: A Paradigm for Density Discovery in Sparse Neural Networks", "comment": null, "summary": "The lottery ticket hypothesis suggests that dense networks contain sparse\nsubnetworks that can be trained in isolation to match full-model performance.\nExisting approaches-iterative pruning, dynamic sparse training, and pruning at\ninitialization-either incur heavy retraining costs or assume the target density\nis fixed in advance. We introduce Path Weight Magnitude Product-biased Random\ngrowth (PWMPR), a constructive sparse-to-dense training paradigm that grows\nnetworks rather than pruning them, while automatically discovering their\noperating density. Starting from a sparse seed, PWMPR adds edges guided by\npath-kernel-inspired scores, mitigates bottlenecks via randomization, and stops\nwhen a logistic-fit rule detects plateauing accuracy. Experiments on CIFAR,\nTinyImageNet, and ImageNet show that PWMPR approaches the performance of\nIMP-derived lottery tickets-though at higher density-at substantially lower\ncost (~1.5x dense vs. 3-4x for IMP). These results establish growth-based\ndensity discovery as a promising paradigm that complements pruning and dynamic\nsparsity.", "AI": {"tldr": "PWMPR is a growth-based sparse training method that starts from sparse networks and adds edges using path-kernel-inspired scores, automatically discovering optimal density without pre-specifying target sparsity.", "motivation": "Existing sparse training methods like iterative pruning and dynamic sparse training either require heavy retraining costs or assume fixed target density in advance, limiting their practical applicability.", "method": "Path Weight Magnitude Product-biased Random growth (PWMPR) - a constructive sparse-to-dense training that grows networks from sparse seeds, adds edges using path-kernel-inspired scores, mitigates bottlenecks via randomization, and stops when accuracy plateaus are detected.", "result": "PWMPR approaches the performance of IMP-derived lottery tickets (though at higher density) with substantially lower computational cost (~1.5x dense training vs. 3-4x for IMP) on CIFAR, TinyImageNet, and ImageNet datasets.", "conclusion": "Growth-based density discovery is established as a promising paradigm that complements existing pruning and dynamic sparsity approaches for efficient sparse neural network training."}}
{"id": "2509.26490", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26490", "abs": "https://arxiv.org/abs/2509.26490", "authors": ["Wei He", "Yueqing Sun", "Hongyan Hao", "Xueyuan Hao", "Zhikang Xia", "Qi Gu", "Chengcheng Han", "Dengchang Zhao", "Hui Su", "Kefeng Zhang", "Man Gao", "Xi Su", "Xiaodong Cai", "Xunliang Cai", "Yu Yang", "Yunke Zhao"], "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications", "comment": "The code, dataset, and leaderboard are available at\n  https://vitabench.github.io/", "summary": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/", "AI": {"tldr": "VitaBench is a challenging benchmark for evaluating LLM-based agents on real-world interactive tasks across food delivery, in-store consumption, and online travel services, featuring 66 tools and complex multi-turn conversations.", "motivation": "Existing benchmarks fail to capture the complexity of real-world agent scenarios involving extensive information, diverse resources, and dynamic user interactions.", "method": "Created a simulation environment with 66 tools from daily applications, eliminated domain-specific policies for flexible scenario composition, and developed a rubric-based sliding window evaluator for robust assessment.", "result": "Even the most advanced models achieved only 30% success rate on cross-scenario tasks and less than 50% on single-scenario tasks, highlighting the benchmark's difficulty.", "conclusion": "VitaBench serves as a valuable resource for advancing AI agent development in practical real-world applications, with code, dataset, and leaderboard publicly available."}}
{"id": "2509.26145", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26145", "abs": "https://arxiv.org/abs/2509.26145", "authors": ["Yukun Yang"], "title": "LMILAtt: A Deep Learning Model for Depression Detection from Social Media Users Enhanced by Multi-Instance Learning Based on Attention Mechanism", "comment": null, "summary": "Depression is a major global public health challenge and its early\nidentification is crucial. Social media data provides a new perspective for\ndepression detection, but existing methods face limitations such as\ninsufficient accuracy, insufficient utilization of time series features, and\nhigh annotation costs. To this end, this study proposes the LMILAtt model,\nwhich innovatively integrates Long Short-Term Memory autoencoders and attention\nmechanisms: firstly, the temporal dynamic features of user tweets (such as\ndepressive tendency evolution patterns) are extracted through unsupervised LSTM\nautoencoders. Secondly, the attention mechanism is used to dynamically weight\nkey texts (such as early depression signals) and construct a multi-example\nlearning architecture to improve the accuracy of user-level detection. Finally,\nthe performance was verified on the WU3D dataset labeled by professional\nmedicine. Experiments show that the model is significantly better than the\nbaseline model in terms of accuracy, recall and F1 score. In addition, the\nweakly supervised learning strategy significantly reduces the cost of labeling\nand provides an efficient solution for large-scale social media depression\nscreening.", "AI": {"tldr": "The paper proposes LMILAtt model for depression detection from social media data, combining LSTM autoencoders and attention mechanisms to extract temporal features and improve detection accuracy while reducing annotation costs.", "motivation": "Depression is a major global health challenge requiring early identification. Social media provides new data for detection, but existing methods have limitations in accuracy, time series feature utilization, and high annotation costs.", "method": "LMILAtt model integrates LSTM autoencoders to extract temporal dynamic features from user tweets, uses attention mechanisms to weight key texts, and employs multi-instance learning architecture for user-level detection.", "result": "Experiments on WU3D dataset show the model significantly outperforms baseline models in accuracy, recall and F1 score. The weakly supervised learning strategy reduces labeling costs.", "conclusion": "The proposed model provides an efficient solution for large-scale social media depression screening with improved accuracy and reduced annotation requirements."}}
{"id": "2509.25666", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25666", "abs": "https://arxiv.org/abs/2509.25666", "authors": ["Justin Chih-Yao Chen", "Becky Xiangyu Peng", "Prafulla Kumar Choubey", "Kung-Hsiang Huang", "Jiaxin Zhang", "Mohit Bansal", "Chien-Sheng Wu"], "title": "Nudging the Boundaries of LLM Reasoning", "comment": "Code release in preparation", "summary": "Current online reinforcement learning (RL) algorithms like GRPO share a key\nlimitation in LLM reasoning: they cannot learn from problems that are\n\"unsolvable\" to the model. In other words, they can only improve performance on\nproblems where the model is capable of exploring the correct answer.\nConsequently, the model's \"upper limit\" remains unchanged after RL training,\neven though the likelihood of solving easier, solvable problems may increase.\nThese hard samples cannot contribute to training, as no rollouts yield rewards\nand thus no gradients are produced. To unlock learning from these hard samples,\nwe propose NuRL, a \"nudging\" method that aims to push the upper bound of LLM\nreasoning using self-generated hints, i.e., abstract cues that help reduce the\nproblem difficulty for the model. Given a question and its gold answer, the\nmodel generates a CoT and then produces a hint containing the core knowledge\nneeded to solve the problem. During training, we generate G rollouts from the\nbase policy and use the pass rate to decide whether the hint should be\ninjected. For hard samples with a 0% pass rate, we inject the hint and\nregenerate a new batch of trajectories. This yields two benefits: (1) the hint\nboosts pass rates (from 0% to non-zero), thereby introducing training signals\nfor previously unsolvable samples, and (2) the hints are self-generated,\navoiding distributional shift and do not rely on external models. NuRL achieves\nconsistent improvements across 6 benchmarks and 3 models, while remaining\ncomplementary to test-time scaling. Notably, NuRL can raise the model's upper\nlimit, whereas GRPO leaves pass@1024 unchanged from the base model.\nFurthermore, we present a systematic study of what makes an effective hint and\nwhen hints are most useful. Interestingly, the best hints are abstract and\nhigh-level, and are most beneficial when applied necessarily and after GRPO has\nconverged.", "AI": {"tldr": "NuRL is a novel reinforcement learning method that enables LLMs to learn from previously unsolvable problems by generating and injecting self-generated hints, thereby pushing the upper bound of reasoning capabilities.", "motivation": "Current RL algorithms like GRPO cannot learn from problems that are unsolvable to the model, as no rollouts yield rewards and thus no gradients are produced. This limits the model's upper performance ceiling.", "method": "The model generates self-generated hints containing core knowledge needed to solve problems. For hard samples with 0% pass rate, hints are injected and new trajectories are generated, boosting pass rates from 0% to non-zero.", "result": "NuRL achieves consistent improvements across 6 benchmarks and 3 models, raising the model's upper limit while GRPO leaves pass@1024 unchanged. It remains complementary to test-time scaling.", "conclusion": "NuRL successfully enables learning from hard samples through self-generated hints, with abstract and high-level hints being most effective when applied necessarily after GRPO convergence."}}
{"id": "2509.26514", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26514", "abs": "https://arxiv.org/abs/2509.26514", "authors": ["Yue Wang", "Ruotian Ma", "Xingyu Chen", "Zhengliang Shi", "Wanshun Chen", "Huang Liu", "Jiadi Yao", "Qu Yang", "Qingxuan Jiang", "Fanghua Ye", "Juntao Li", "Min Zhang", "Zhaopeng Tu", "Xiaolong Li", "Linus"], "title": "BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs", "comment": null, "summary": "The rise of Large Language Models (LLMs) is reshaping multimodel models, with\nspeech synthesis being a prominent application. However, existing approaches\noften underutilize the linguistic intelligence of these models, typically\nfailing to leverage their powerful instruction-following capabilities. This\nlimitation hinders the model's ability to follow text instructions for\ncontrollable Text-to-Speech~(TTS). To address this, we propose a new paradigm\ninspired by ``operationalism'' that decouples instruction understanding from\nspeech generation. We introduce BatonVoice, a framework where an LLM acts as a\n``conductor'', understanding user instructions and generating a textual\n``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS\nmodel, the ``orchestra'', then generates the speech from these features. To\nrealize this component, we develop BatonTTS, a TTS model trained specifically\nfor this task. Our experiments demonstrate that BatonVoice achieves strong\nperformance in controllable and emotional speech synthesis, outperforming\nstrong open- and closed-source baselines. Notably, our approach enables\nremarkable zero-shot cross-lingual generalization, accurately applying feature\ncontrol abilities to languages unseen during post-training. This demonstrates\nthat objectifying speech into textual vocal features can more effectively\nunlock the linguistic intelligence of LLMs.", "AI": {"tldr": "BatonVoice is a new framework that decouples instruction understanding from speech generation using an LLM as \"conductor\" to generate vocal feature plans, and a separate TTS model as \"orchestra\" to generate speech from these features.", "motivation": "Existing approaches underutilize LLMs' linguistic intelligence and instruction-following capabilities for controllable TTS, limiting their ability to follow text instructions for speech synthesis.", "method": "Proposes BatonVoice framework with LLM as conductor generating textual vocal feature plans (pitch, energy), and BatonTTS as orchestra generating speech from these features, inspired by operationalism to decouple instruction understanding from generation.", "result": "BatonVoice achieves strong performance in controllable and emotional speech synthesis, outperforming open- and closed-source baselines, and enables remarkable zero-shot cross-lingual generalization to unseen languages.", "conclusion": "Objectifying speech into textual vocal features more effectively unlocks LLMs' linguistic intelligence, demonstrating successful decoupling of instruction understanding from speech generation."}}
{"id": "2509.26153", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26153", "abs": "https://arxiv.org/abs/2509.26153", "authors": ["Jack Gallifant", "Katherine C. Kellogg", "Matt Butler", "Amanda Centi", "Patrick F. Doyle", "Sayon Dutta", "Joyce Guo", "Matthew J. Hadfield", "Esther H. Kim", "David E. Kozono", "Hugo JWL Aerts", "Adam B. Landman", "Raymond H. Mak", "Rebecca G. Mishuris", "Tanna L. Nelson", "Guergana K. Savova", "Elad Sharon", "Benjamin C. Silverman", "Umit Topaloglu", "Jeremy L. Warner", "Danielle S. Bitterman"], "title": "Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical Practice", "comment": "Under review. 5 Tables, 2 Figures", "summary": "Large language models (LLMs) integrated into agent-driven workflows hold\nimmense promise for healthcare, yet a significant gap exists between their\npotential and practical implementation within clinical settings. To address\nthis, we present a practitioner-oriented field manual for deploying generative\nagents that use electronic health record (EHR) data. This guide is informed by\nour experience deploying the \"irAE-Agent\", an automated system to detect\nimmune-related adverse events from clinical notes at Mass General Brigham, and\nby structured interviews with 20 clinicians, engineers, and informatics leaders\ninvolved in the project. Our analysis reveals a critical misalignment in\nclinical AI development: less than 20% of our effort was dedicated to prompt\nengineering and model development, while over 80% was consumed by the\nsociotechnical work of implementation. We distill this effort into five \"heavy\nlifts\": data integration, model validation, ensuring economic value, managing\nsystem drift, and governance. By providing actionable solutions for each of\nthese challenges, this field manual shifts the focus from algorithmic\ndevelopment to the essential infrastructure and implementation work required to\nbridge the \"valley of death\" and successfully translate generative AI from\npilot projects into routine clinical care.", "AI": {"tldr": "A practitioner-oriented field manual for deploying generative AI agents in healthcare, based on experience with an irAE detection system and interviews with clinical stakeholders, revealing that 80% of effort goes to sociotechnical implementation rather than model development.", "motivation": "To bridge the gap between the potential of LLMs in healthcare and their practical implementation in clinical settings, addressing the misalignment in clinical AI development where most effort is consumed by implementation work rather than model development.", "method": "Developed a field manual based on deployment experience with \"irAE-Agent\" (an automated system for detecting immune-related adverse events from EHR data) and structured interviews with 20 clinicians, engineers, and informatics leaders.", "result": "Analysis revealed that less than 20% of effort was dedicated to prompt engineering and model development, while over 80% was consumed by sociotechnical implementation work, distilled into five \"heavy lifts\": data integration, model validation, ensuring economic value, managing system drift, and governance.", "conclusion": "The field manual shifts focus from algorithmic development to essential infrastructure and implementation work required to successfully translate generative AI from pilot projects into routine clinical care, bridging the \"valley of death\" in clinical AI deployment."}}
{"id": "2509.25667", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.25667", "abs": "https://arxiv.org/abs/2509.25667", "authors": ["Bipul Thapa", "Biplov Paneru", "Bishwash Paneru", "Khem Narayan Poudyal"], "title": "EEG-based AI-BCI Wheelchair Advancement: Hybrid Deep Learning with Motor Imagery for Brain Computer Interface", "comment": null, "summary": "This paper presents an Artificial Intelligence (AI) integrated novel approach\nto Brain-Computer Interface (BCI)-based wheelchair development, utilizing a\nmotor imagery right-left-hand movement mechanism for control. The system is\ndesigned to simulate wheelchair navigation based on motor imagery right and\nleft-hand movements using electroencephalogram (EEG) data. A pre-filtered\ndataset, obtained from an open-source EEG repository, was segmented into arrays\nof 19x200 to capture the onset of hand movements. The data was acquired at a\nsampling frequency of 200Hz. The system integrates a Tkinter-based interface\nfor simulating wheelchair movements, offering users a functional and intuitive\ncontrol system. We propose a BiLSTM-BiGRU model that shows a superior test\naccuracy of 92.26% as compared with various machine learning baseline models,\nincluding XGBoost, EEGNet, and a transformer-based model. The Bi-LSTM-BiGRU\nattention-based model achieved a mean accuracy of 90.13% through\ncross-validation, showcasing the potential of attention mechanisms in BCI\napplications.", "AI": {"tldr": "AI-powered BCI wheelchair control using motor imagery of right-left hand movements with EEG data, achieving 92.26% accuracy using BiLSTM-BiGRU model.", "motivation": "To develop an intuitive and functional BCI-based wheelchair control system using motor imagery for improved accessibility and independence for users.", "method": "Used EEG data segmented into 19x200 arrays from open-source repository, integrated Tkinter interface for simulation, and proposed BiLSTM-BiGRU attention-based model for classification.", "result": "BiLSTM-BiGRU model achieved 92.26% test accuracy, outperforming XGBoost, EEGNet, and transformer models, with 90.13% mean accuracy in cross-validation.", "conclusion": "The attention-based BiLSTM-BiGRU model demonstrates strong potential for BCI applications, providing accurate and reliable wheelchair control through motor imagery."}}
{"id": "2509.26520", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26520", "abs": "https://arxiv.org/abs/2509.26520", "authors": ["Yaoxiang Wang", "Qingguo Hu", "Yucheng Ding", "Ruizhe Wang", "Yeyun Gong", "Jian Jiao", "Yelong Shen", "Peng Cheng", "Jinsong Su"], "title": "Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently\nscaling large language models without a proportional increase in computational\ncost. However, the standard training strategy of Top-K router prevents MoE\nmodels from realizing their full potential for elastic inference. When the\nnumber of activated experts is altered at inference time, these models exhibit\nprecipitous performance degradation. In this work, we introduce Matryoshka MoE\n(M-MoE), a training framework that instills a coarse-to-fine structure directly\ninto the expert ensemble. By systematically varying the number of activated\nexperts during training, M-MoE compels the model to learn a meaningful ranking:\ntop-ranked experts collaborate to provide essential, coarse-grained\ncapabilities, while subsequent experts add progressively finer-grained detail.\nWe explore this principle at multiple granularities, identifying a layer-wise\nrandomization strategy as the most effective. Our experiments demonstrate that\na single M-MoE model achieves remarkable elasticity, with its performance at\nvarious expert counts closely matching that of an entire suite of specialist\nmodels, but at only a fraction of the total training cost. This flexibility not\nonly unlocks elastic inference but also enables optimizing performance by\nallocating different computational budgets to different model layers. Our work\npaves the way for more practical and adaptable deployments of large-scale MoE\nmodels.", "AI": {"tldr": "Matryoshka MoE (M-MoE) is a training framework that enables elastic inference in Mixture-of-Experts models by learning a coarse-to-fine expert structure, allowing flexible activation of experts at inference time without performance degradation.", "motivation": "Standard MoE training with Top-K routers prevents elastic inference - changing the number of activated experts at inference causes severe performance drops, limiting practical deployment flexibility.", "method": "Systematically vary the number of activated experts during training to force the model to learn a meaningful ranking: top experts provide coarse capabilities while subsequent experts add finer details. Uses layer-wise randomization strategy.", "result": "A single M-MoE model achieves remarkable elasticity, matching performance of multiple specialist models at various expert counts with only a fraction of the training cost. Enables computational budget optimization across layers.", "conclusion": "M-MoE enables practical and adaptable deployment of large-scale MoE models by unlocking elastic inference capabilities and optimizing computational allocation across model layers."}}
{"id": "2509.26161", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.26161", "abs": "https://arxiv.org/abs/2509.26161", "authors": ["Runxin Yang", "Yuxuan Wan", "Shuqing Li", "Michael R. Lyu"], "title": "90% Faster, 100% Code-Free: MLLM-Driven Zero-Code 3D Game Development", "comment": null, "summary": "Developing 3D games requires specialized expertise across multiple domains,\nincluding programming, 3D modeling, and engine configuration, which limits\naccess to millions of potential creators. Recently, researchers have begun to\nexplore automated game development. However, existing approaches face three\nprimary challenges: (1) limited scope to 2D content generation or isolated code\nsnippets; (2) requirement for manual integration of generated components into\ngame engines; and (3) poor performance on handling interactive game logic and\nstate management. While Multimodal Large Language Models (MLLMs) demonstrate\npotential capabilities to ease the game generation task, a critical gap still\nremains in translating these outputs into production-ready, executable game\nprojects based on game engines such as Unity and Unreal Engine.\n  To bridge the gap, this paper introduces UniGen, the first end-to-end\ncoordinated multi-agent framework that automates zero-coding development of\nrunnable 3D games from natural language requirements. Specifically, UniGen uses\na Planning Agent that interprets user requirements into structured blueprints\nand engineered logic descriptions; after which a Generation Agent produces\nexecutable C# scripts; then an Automation Agent handles engine-specific\ncomponent binding and scene construction; and lastly a Debugging Agent provides\nreal-time error correction through conversational interaction. We evaluated\nUniGen on three distinct game prototypes. Results demonstrate that UniGen not\nonly democratizes game creation by requiring no coding from the user, but also\nreduces development time by 91.4%. We release UniGen at\nhttps://github.com/yxwan123/UniGen. A video demonstration is available at\nhttps://www.youtube.com/watch?v=xyJjFfnxUx0.", "AI": {"tldr": "UniGen is an end-to-end multi-agent framework that automates 3D game development from natural language requirements without coding, reducing development time by 91.4%.", "motivation": "To democratize 3D game development by overcoming limitations of existing approaches: limited scope to 2D content, manual integration requirements, and poor handling of interactive game logic.", "method": "Uses four coordinated agents: Planning Agent interprets requirements into blueprints, Generation Agent produces C# scripts, Automation Agent handles engine binding and scene construction, Debugging Agent provides real-time error correction.", "result": "Successfully developed three distinct game prototypes, enabling zero-coding game creation and reducing development time by 91.4%.", "conclusion": "UniGen effectively bridges the gap between MLLM outputs and production-ready 3D games, democratizing game development and significantly accelerating the creation process."}}
{"id": "2509.25678", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25678", "abs": "https://arxiv.org/abs/2509.25678", "authors": ["Xing Han", "Hsing-Huan Chung", "Joydeep Ghosh", "Paul Pu Liang", "Suchi Saria"], "title": "Guiding Mixture-of-Experts with Temporal Multimodal Interactions", "comment": "21 pages, 8 figures, 10 tables", "summary": "Mixture-of-Experts (MoE) architectures have become pivotal for large-scale\nmultimodal models. However, their routing mechanisms typically overlook the\ninformative, time-varying interaction dynamics between modalities. This\nlimitation hinders expert specialization, as the model cannot explicitly\nleverage intrinsic modality relationships for effective reasoning. To address\nthis, we propose a novel framework that guides MoE routing using quantified\ntemporal interaction. A multimodal interaction-aware router learns to dispatch\ntokens to experts based on the nature of their interactions. This dynamic\nrouting encourages experts to acquire generalizable interaction-processing\nskills rather than merely learning task-specific features. Our framework builds\non a new formulation of temporal multimodal interaction dynamics, which are\nused to guide expert routing. We first demonstrate that these temporal\nmultimodal interactions reveal meaningful patterns across applications, and\nthen show how they can be leveraged to improve both the design and performance\nof MoE-based models. Comprehensive experiments on challenging multimodal\nbenchmarks validate our approach, demonstrating both enhanced performance and\nimproved interpretability.", "AI": {"tldr": "A novel MoE framework that uses temporal multimodal interaction dynamics to guide expert routing, improving performance and interpretability.", "motivation": "Current MoE routing mechanisms ignore time-varying interaction dynamics between modalities, limiting expert specialization and effective reasoning.", "method": "Proposes a multimodal interaction-aware router that dispatches tokens to experts based on temporal interaction patterns, encouraging generalizable interaction-processing skills.", "result": "Comprehensive experiments on multimodal benchmarks show enhanced performance and improved interpretability.", "conclusion": "Leveraging temporal multimodal interactions improves MoE design and performance, enabling better expert specialization."}}
{"id": "2509.26536", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26536", "abs": "https://arxiv.org/abs/2509.26536", "authors": ["Yida Xue", "Mingjun Mao", "Xiangyuan Ru", "Yuqi Zhu", "Baochang Ren", "Shuofei Qiao", "Mengru Wang", "Shumin Deng", "Xinyu An", "Ningyu Zhang", "Ying Chen", "Huajun Chen"], "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents", "comment": "Work in progress", "summary": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater\nembodied agents, designed to advance AI in one of the most demanding real-world\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\nextreme perceptual and decision-making challenges, including low visibility,\ndynamic ocean currents, making effective agent deployment exceptionally\ndifficult. OceanGym encompasses eight realistic task domains and a unified\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\nintegrates perception, memory, and sequential decision-making. Agents are\nrequired to comprehend optical and sonar data, autonomously explore complex\nenvironments, and accomplish long-horizon objectives under these harsh\nconditions. Extensive experiments reveal substantial gaps between\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\npersistent difficulty of perception, planning, and adaptability in ocean\nunderwater environments. By providing a high-fidelity, rigorously designed\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\ntransferring these capabilities to real-world autonomous ocean underwater\nvehicles, marking a decisive step toward intelligent agents capable of\noperating in one of Earth's last unexplored frontiers. The code and data are\navailable at https://github.com/OceanGPT/OceanGym.", "AI": {"tldr": "OceanGym is the first comprehensive benchmark for ocean underwater embodied agents, featuring eight realistic task domains and a unified MLLM-driven framework to address extreme challenges like low visibility and dynamic currents.", "motivation": "To advance AI in demanding underwater environments that present extreme perceptual and decision-making challenges, unlike terrestrial or aerial domains.", "method": "Uses a unified agent framework driven by Multi-modal Large Language Models (MLLMs) that integrates perception, memory, and sequential decision-making, requiring agents to comprehend optical and sonar data and autonomously explore complex environments.", "result": "Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting persistent difficulties in perception, planning, and adaptability in ocean underwater environments.", "conclusion": "OceanGym establishes a testbed for developing robust embodied AI and transferring capabilities to real-world autonomous ocean underwater vehicles, marking a step toward intelligent agents capable of operating in Earth's last unexplored frontiers."}}
{"id": "2509.26167", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26167", "abs": "https://arxiv.org/abs/2509.26167", "authors": ["Eric J. W. Orlowski", "Hakim Norhashim", "Tristan Koh Ly Wey"], "title": "'Too much alignment; not enough culture': Re-balancing cultural alignment practices in LLMs", "comment": "8 pages, no figures", "summary": "While cultural alignment has increasingly become a focal point within AI\nresearch, current approaches relying predominantly on quantitative benchmarks\nand simplistic proxies fail to capture the deeply nuanced and context-dependent\nnature of human cultures. Existing alignment practices typically reduce culture\nto static demographic categories or superficial cultural facts, thereby\nsidestepping critical questions about what it truly means to be culturally\naligned. This paper argues for a fundamental shift towards integrating\ninterpretive qualitative approaches drawn from social sciences into AI\nalignment practices, specifically in the context of Large Language Models\n(LLMs). Drawing inspiration from Clifford Geertz's concept of \"thick\ndescription,\" we propose that AI systems must produce outputs that reflect\ndeeper cultural meanings--what we term \"thick outputs\"-grounded firmly in\nuser-provided context and intent. We outline three necessary conditions for\nsuccessful cultural alignment: sufficiently scoped cultural representations,\nthe capacity for nuanced outputs, and the anchoring of outputs in the cultural\ncontexts implied within prompts. Finally, we call for cross-disciplinary\ncollaboration and the adoption of qualitative, ethnographic evaluation methods\nas vital steps toward developing AI systems that are genuinely culturally\nsensitive, ethically responsible, and reflective of human complexity.", "AI": {"tldr": "The paper critiques current AI cultural alignment approaches for being too quantitative and superficial, proposing a shift to qualitative methods inspired by anthropology's \"thick description\" concept to create culturally sensitive AI systems.", "motivation": "Current AI cultural alignment practices rely on simplistic proxies and quantitative benchmarks that fail to capture the nuanced, context-dependent nature of human cultures, reducing culture to static categories and avoiding deeper questions about true cultural alignment.", "method": "Proposes integrating interpretive qualitative approaches from social sciences into AI alignment, specifically using Clifford Geertz's \"thick description\" concept to develop \"thick outputs\" grounded in user context and intent, with three conditions: scoped cultural representations, nuanced output capacity, and context anchoring.", "result": "The paper outlines a framework for developing culturally aligned AI systems but does not present empirical results or implementation details.", "conclusion": "Calls for cross-disciplinary collaboration and adoption of qualitative, ethnographic evaluation methods as essential for creating AI systems that are genuinely culturally sensitive, ethically responsible, and reflective of human complexity."}}
{"id": "2509.25686", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25686", "abs": "https://arxiv.org/abs/2509.25686", "authors": ["Pirzada Suhail", "Aditya Anand", "Amit Sethi"], "title": "Minimalist Explanation Generation and Circuit Discovery", "comment": null, "summary": "Machine learning models, by virtue of training, learn a large repertoire of\ndecision rules for any given input, and any one of these may suffice to justify\na prediction. However, in high-dimensional input spaces, such rules are\ndifficult to identify and interpret. In this paper, we introduce an\nactivation-matching based approach to generate minimal and faithful\nexplanations for the decisions of pre-trained image classifiers. We aim to\nidentify minimal explanations that not only preserve the model's decision but\nare also concise and human-readable. To achieve this, we train a lightweight\nautoencoder to produce binary masks that learns to highlight the decision-wise\ncritical regions of an image while discarding irrelevant background. The\ntraining objective integrates activation alignment across multiple layers,\nconsistency at the output label, priors that encourage sparsity, and\ncompactness, along with a robustness constraint that enforces faithfulness. The\nminimal explanations so generated also lead us to mechanistically interpreting\nthe model internals. In this regard we also introduce a circuit readout\nprocedure wherein using the explanation's forward pass and gradients, we\nidentify active channels and construct a channel-level graph, scoring\ninter-layer edges by ingress weight magnitude times source activation and\nfeature-to-class links by classifier weight magnitude times feature activation.\nTogether, these contributions provide a practical bridge between minimal\ninput-level explanations and a mechanistic understanding of the internal\ncomputations driving model decisions.", "AI": {"tldr": "This paper introduces an activation-matching approach to generate minimal and faithful explanations for image classifier decisions, using a lightweight autoencoder to create binary masks that highlight critical image regions while maintaining model decision consistency.", "motivation": "Machine learning models learn many decision rules that are difficult to identify and interpret in high-dimensional spaces, creating a need for minimal and human-readable explanations that preserve model decisions.", "method": "Train a lightweight autoencoder to produce binary masks that highlight decision-critical image regions, integrating activation alignment across layers, output consistency, sparsity priors, compactness, and robustness constraints. Also introduce a circuit readout procedure using explanations' forward pass and gradients to identify active channels and construct channel-level graphs.", "result": "The approach generates minimal explanations that preserve model decisions while being concise and human-readable, and enables mechanistic interpretation of model internals through channel-level circuit analysis.", "conclusion": "The method provides a practical bridge between minimal input-level explanations and mechanistic understanding of internal computations driving model decisions, offering both interpretable explanations and insights into model internals."}}
{"id": "2509.26543", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26543", "abs": "https://arxiv.org/abs/2509.26543", "authors": ["Lina Conti", "Dennis Fucci", "Marco Gaido", "Matteo Negri", "Guillaume Wisniewski", "Luisa Bentivogli"], "title": "The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models", "comment": "Accepted to BlackBoxNLP 2025", "summary": "Contrastive explanations, which indicate why an AI system produced one output\n(the target) instead of another (the foil), are widely regarded in explainable\nAI as more informative and interpretable than standard explanations. However,\nobtaining such explanations for speech-to-text (S2T) generative models remains\nan open challenge. Drawing from feature attribution techniques, we propose the\nfirst method to obtain contrastive explanations in S2T by analyzing how parts\nof the input spectrogram influence the choice between alternative outputs.\nThrough a case study on gender assignment in speech translation, we show that\nour method accurately identifies the audio features that drive the selection of\none gender over another. By extending the scope of contrastive explanations to\nS2T, our work provides a foundation for better understanding S2T models.", "AI": {"tldr": "Proposes first method for contrastive explanations in speech-to-text models by analyzing input spectrogram influence on output choices, demonstrated through gender assignment case study.", "motivation": "Contrastive explanations are more informative than standard explanations but obtaining them for speech-to-text generative models remains challenging.", "method": "Uses feature attribution techniques to analyze how parts of input spectrogram influence choice between alternative outputs in S2T models.", "result": "Method accurately identifies audio features that drive selection of one gender over another in speech translation gender assignment.", "conclusion": "Extends scope of contrastive explanations to S2T, providing foundation for better understanding speech-to-text models."}}
{"id": "2509.26201", "categories": ["cs.AI", "cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2509.26201", "abs": "https://arxiv.org/abs/2509.26201", "authors": ["Andreas Werbrouck", "Marshall B. Lindsay", "Matthew Maschmann", "Matthias J. Young"], "title": "LLM Agents for Knowledge Discovery in Atomic Layer Processing", "comment": "Accepted submission to the AI4MAT workshop@NEURIPS 2025. As\n  submitted, except author names added", "summary": "Large Language Models (LLMs) have garnered significant attention for several\nyears now. Recently, their use as independently reasoning agents has been\nproposed. In this work, we test the potential of such agents for knowledge\ndiscovery in materials science. We repurpose LangGraph's tool functionality to\nsupply agents with a black box function to interrogate. In contrast to process\noptimization or performing specific, user-defined tasks, knowledge discovery\nconsists of freely exploring the system, posing and verifying statements about\nthe behavior of this black box, with the sole objective of generating and\nverifying generalizable statements. We provide proof of concept for this\napproach through a children's parlor game, demonstrating the role of\ntrial-and-error and persistence in knowledge discovery, and the strong\npath-dependence of results. We then apply the same strategy to show that LLM\nagents can explore, discover, and exploit diverse chemical interactions in an\nadvanced Atomic Layer Processing reactor simulation using intentionally limited\nprobe capabilities without explicit instructions.", "AI": {"tldr": "LLM agents can autonomously discover knowledge in materials science by exploring black box functions through trial-and-error, demonstrating path-dependent discovery in both games and chemical reactor simulations.", "motivation": "To test the potential of LLMs as independent reasoning agents for knowledge discovery in materials science, moving beyond task-specific optimization to free exploration.", "method": "Repurposed LangGraph's tool functionality to supply agents with black box functions to interrogate, using trial-and-error exploration without explicit instructions.", "result": "Successfully demonstrated knowledge discovery in a children's parlor game and discovered diverse chemical interactions in an Atomic Layer Processing reactor simulation.", "conclusion": "LLM agents can autonomously explore, discover, and exploit knowledge in complex systems through persistent trial-and-error approaches, showing strong path-dependence in discovery outcomes."}}
{"id": "2509.25690", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.25690", "abs": "https://arxiv.org/abs/2509.25690", "authors": ["Zihui Zhao", "Yuanbo Tang", "Jieyu Ren", "Xiaoping Zhang", "Yang Li"], "title": "A Unified Probabilistic Framework for Dictionary Learning with Parsimonious Activation", "comment": null, "summary": "Dictionary learning is traditionally formulated as an $L_1$-regularized\nsignal reconstruction problem. While recent developments have incorporated\ndiscriminative, hierarchical, or generative structures, most approaches rely on\nencouraging representation sparsity over individual samples that overlook how\natoms are shared across samples, resulting in redundant and sub-optimal\ndictionaries. We introduce a parsimony promoting regularizer based on the\nrow-wise $L_\\infty$ norm of the coefficient matrix. This additional penalty\nencourages entire rows of the coefficient matrix to vanish, thereby reducing\nthe number of dictionary atoms activated across the dataset. We derive the\nformulation from a probabilistic model with Beta-Bernoulli priors, which\nprovides a Bayesian interpretation linking the regularization parameters to\nprior distributions. We further establish theoretical calculation for optimal\nhyperparameter selection and connect our formulation to both Minimum\nDescription Length, Bayesian model selection and pathlet learning. Extensive\nexperiments on benchmark datasets demonstrate that our method achieves\nsubstantially improved reconstruction quality (with a 20\\% reduction in RMSE)\nand enhanced representation sparsity, utilizing fewer than one-tenth of the\navailable dictionary atoms, while empirically validating our theoretical\nanalysis.", "AI": {"tldr": "The paper proposes a new dictionary learning method using row-wise L\u221e norm regularization to promote atom sparsity across samples, reducing redundant dictionary atoms while improving reconstruction quality.", "motivation": "Traditional dictionary learning focuses on sample-wise sparsity but overlooks how atoms are shared across samples, leading to redundant dictionaries and suboptimal performance.", "method": "Introduces a parsimony-promoting regularizer based on row-wise L\u221e norm of coefficient matrix, derived from Beta-Bernoulli priors, with theoretical analysis for hyperparameter selection connecting to Bayesian model selection principles.", "result": "Achieves 20% RMSE reduction in reconstruction quality, uses less than 1/10 of available dictionary atoms, and demonstrates enhanced representation sparsity on benchmark datasets.", "conclusion": "The proposed method effectively reduces dictionary redundancy while improving reconstruction performance, with theoretical foundations validated empirically."}}
{"id": "2509.26553", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26553", "abs": "https://arxiv.org/abs/2509.26553", "authors": ["Seiji Maekawa", "Jackson Hassell", "Pouya Pezeshkpour", "Tom Mitchell", "Estevam Hruschka"], "title": "Towards Reliable Benchmarking: A Contamination Free, Controllable Evaluation Framework for Multi-step LLM Function Calling", "comment": null, "summary": "As language models gain access to external tools via structured function\ncalls, they become increasingly more capable of solving complex, multi-step\ntasks. However, existing benchmarks for tool-augmented language models (TaLMs)\nprovide insufficient control over factors such as the number of functions\naccessible, task complexity, and input size, and remain vulnerable to data\ncontamination. We present FuncBenchGen, a unified, contamination-free framework\nthat evaluates TaLMs by generating synthetic multi-step tool-use tasks. The key\nidea is to cast tool use as traversal over a hidden function-dependency DAG\nwhere nodes are function calls and an edge between nodes represents one\nfunction consuming the output of another. Given a set of external function\nschemas, initial variable values, and a target variable, models must compose\nthe correct call sequence to compute the target variable. FuncBenchGen allows\nusers to precisely control task difficulty (e.g., graph size, dependency depth,\nand distractor functions) while avoiding data leakage. We apply our\nFuncBenchGen framework to evaluate seven LLMs on tool use tasks of varying\ndifficulty. Reasoning-optimized models consistently outperform general-purpose\nmodels with GPT-5 significantly outperforming other models. Performance\ndeclines sharply as dependency depth increases. Furthermore, connected\nirrelevant functions prove especially difficult to handle. We find that strong\nmodels often make syntactically valid function calls but propagate incorrect or\nstale argument values across steps, revealing brittle state tracking by LLMs in\nmulti-turn tool use. Motivated by this observation, we introduce a simple\nmitigation strategy that explicitly restates prior variable values to the agent\nat each step. Surprisingly, this lightweight change yields substantial gains\nacross models. e.g., yielding a success rate improvement from 62.5% to 81.3%\nfor GPT-5.", "AI": {"tldr": "FuncBenchGen is a contamination-free framework for evaluating tool-augmented language models by generating synthetic multi-step tool-use tasks based on function-dependency DAGs, with precise control over task difficulty.", "motivation": "Existing benchmarks for tool-augmented language models lack control over key factors like function accessibility, task complexity, and input size, and are vulnerable to data contamination.", "method": "FuncBenchGen casts tool use as traversal over hidden function-dependency DAGs where nodes are function calls and edges represent dependencies. It generates tasks where models must compose correct call sequences to compute target variables from given function schemas and initial values.", "result": "Evaluation of seven LLMs shows reasoning-optimized models outperform general-purpose models, with GPT-5 performing best. Performance declines with increased dependency depth, and connected irrelevant functions are particularly challenging. Models often make valid calls but propagate incorrect values, revealing brittle state tracking.", "conclusion": "A simple mitigation strategy of explicitly restating prior variable values at each step yields substantial performance gains across models, improving GPT-5's success rate from 62.5% to 81.3%."}}
{"id": "2509.26205", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26205", "abs": "https://arxiv.org/abs/2509.26205", "authors": ["Aline Mangold", "Kiran Hoffmann"], "title": "Human-Centered Evaluation of RAG outputs: a framework and questionnaire for human-AI collaboration", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems are increasingly deployed in\nuser-facing applications, yet systematic, human-centered evaluation of their\noutputs remains underexplored. Building on Gienapp's utility-dimension\nframework, we designed a human-centred questionnaire that assesses RAG outputs\nacross 12 dimensions. We iteratively refined the questionnaire through several\nrounds of ratings on a set of query-output pairs and semantic discussions.\nUltimately, we incorporated feedback from both a human rater and a human-LLM\npair. Results indicate that while large language models (LLMs) reliably focus\non metric descriptions and scale labels, they exhibit weaknesses in detecting\ntextual format variations. Humans struggled to focus strictly on metric\ndescriptions and labels. LLM ratings and explanations were viewed as a helpful\nsupport, but numeric LLM and human ratings lacked agreement. The final\nquestionnaire extends the initial framework by focusing on user intent, text\nstructuring, and information verifiability.", "AI": {"tldr": "This paper presents a human-centered questionnaire for evaluating RAG systems across 12 dimensions, developed through iterative refinement with both human and LLM feedback.", "motivation": "Systematic, human-centered evaluation of RAG system outputs is underexplored despite their increasing deployment in user-facing applications.", "method": "Designed a questionnaire based on Gienapp's utility-dimension framework, iteratively refined through multiple rounds of ratings on query-output pairs and semantic discussions, incorporating feedback from human raters and human-LLM pairs.", "result": "LLMs reliably focus on metric descriptions and scale labels but struggle with detecting textual format variations. Humans had difficulty focusing strictly on metric descriptions and labels. LLM ratings were helpful but lacked agreement with human numeric ratings.", "conclusion": "The final questionnaire extends the initial framework by focusing on user intent, text structuring, and information verifiability, providing a comprehensive tool for human-centered RAG evaluation."}}
{"id": "2509.25692", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25692", "abs": "https://arxiv.org/abs/2509.25692", "authors": ["Tingyu Shi", "Fan Lyu", "Shaoliang Peng"], "title": "Annotation-Efficient Active Test-Time Adaptation with Conformal Prediction", "comment": null, "summary": "Active Test-Time Adaptation (ATTA) improves model robustness under domain\nshift by selectively querying human annotations at deployment, but existing\nmethods use heuristic uncertainty measures and suffer from low data selection\nefficiency, wasting human annotation budget. We propose Conformal Prediction\nActive TTA (CPATTA), which first brings principled, coverage-guaranteed\nuncertainty into ATTA. CPATTA employs smoothed conformal scores with a top-K\ncertainty measure, an online weight-update algorithm driven by pseudo coverage,\na domain-shift detector that adapts human supervision, and a staged update\nscheme balances human-labeled and model-labeled data. Extensive experiments\ndemonstrate that CPATTA consistently outperforms the state-of-the-art ATTA\nmethods by around 5% in accuracy. Our code and datasets are available at\nhttps://github.com/tingyushi/CPATTA.", "AI": {"tldr": "CPATTA introduces conformal prediction to Active Test-Time Adaptation, using principled uncertainty measures and online algorithms to improve data selection efficiency and achieve 5% higher accuracy than state-of-the-art methods.", "motivation": "Existing ATTA methods use heuristic uncertainty measures and suffer from low data selection efficiency, wasting human annotation budget during deployment under domain shift.", "method": "Uses smoothed conformal scores with top-K certainty measure, online weight-update algorithm driven by pseudo coverage, domain-shift detector for adaptive human supervision, and staged update scheme balancing human-labeled and model-labeled data.", "result": "Extensive experiments show CPATTA consistently outperforms state-of-the-art ATTA methods by around 5% in accuracy.", "conclusion": "CPATTA successfully brings principled, coverage-guaranteed uncertainty into ATTA, significantly improving data selection efficiency and model robustness under domain shift."}}
{"id": "2509.26592", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26592", "abs": "https://arxiv.org/abs/2509.26592", "authors": ["Vil\u00e9m Zouhar", "Wenda Xu", "Parker Riley", "Juraj Juraska", "Mara Finkelstein", "Markus Freitag", "Dan Deutsch"], "title": "Generating Difficult-to-Translate Texts", "comment": null, "summary": "Machine translation benchmarks sourced from the real world are quickly\nobsoleted, due to most examples being easy for state-of-the-art translation\nmodels. This limits the benchmark's ability to distinguish which model is\nbetter or to reveal models' weaknesses. Current methods for creating difficult\ntest cases, such as subsampling or from-scratch synthesis, either fall short of\nidentifying difficult examples or suffer from a lack of diversity and\nnaturalness. Inspired by the iterative process of human experts probing for\nmodel failures, we propose MT-breaker, a method where a large language model\niteratively refines a source text to increase its translation difficulty. The\nLLM iteratively queries a target machine translation model to guide its\ngeneration of difficult examples. Our approach generates examples that are more\nchallenging for the target MT model while preserving the diversity of natural\ntexts. While the examples are tailored to a particular machine translation\nmodel during the generation, the difficulty also transfers to other models and\nlanguages.", "AI": {"tldr": "MT-breaker uses LLMs to iteratively refine source texts to create challenging machine translation test cases that expose model weaknesses while maintaining naturalness.", "motivation": "Real-world MT benchmarks become obsolete quickly as models improve, limiting their ability to distinguish model quality and reveal weaknesses. Current methods for creating difficult test cases are insufficient.", "method": "A large language model iteratively refines source text by querying a target MT model to guide generation of difficult examples, mimicking human expert probing for failures.", "result": "Generates more challenging examples for target MT models while preserving natural text diversity. Difficulty transfers to other models and languages.", "conclusion": "MT-breaker provides an effective approach for creating challenging, natural MT benchmarks that better expose model weaknesses and enable better model comparison."}}
{"id": "2509.26209", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26209", "abs": "https://arxiv.org/abs/2509.26209", "authors": ["Zican Hu", "Shilin Zhang", "Yafu Li", "Jianhao Yan", "Xuyang Hu", "Leyang Cui", "Xiaoye Qu", "Chunlin Chen", "Yu Cheng", "Zhi Wang"], "title": "Diversity-Incentivized Exploration for Versatile Reasoning", "comment": "26 pages, 10 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\ncrucial paradigm for incentivizing reasoning capabilities in Large Language\nModels (LLMs). Due to vast state-action spaces and reward sparsity in reasoning\ntasks, existing methods often struggle with deficient exploration and poor\nsample efficiency. In the paper, we propose \\textbf{DIVER}\n(\\textbf{D}iversity-\\textbf{I}ncentivized Exploration for\n\\textbf{V}ersatil\\textbf{E} \\textbf{R}easoning), an innovative framework that\nhighlights the pivotal role of global sequence-level diversity to incentivize\ndeep exploration for versatile reasoning. We first conduct a primary empirical\nstudy to reveal a strong positive correlation between global diversity and\nreasoning capacity. Building on this insight, we introduce global diversity\nincentives as an intrinsic reward to promote deep exploration in a semantically\nstructured space. Incorporating the intrinsic reward, we develop a\npotential-based reward shaping mechanism to preserve optimal policy invariance\nand design simple heuristics to mitigate possible reward hacking. Experimental\nresults show that DIVER outperforms competitive RLVR baselines with various\nexploration strategies on both in-domain and out-of-domain tasks, excelling in\nboth Pass@1 and Pass@k evaluations. Our code is available at\nhttps://github.com/NJU-RL/DIVER.", "AI": {"tldr": "DIVER is a reinforcement learning framework that uses global sequence-level diversity as intrinsic rewards to improve exploration and sample efficiency in reasoning tasks for LLMs.", "motivation": "Existing RL methods struggle with deficient exploration and poor sample efficiency in reasoning tasks due to vast state-action spaces and reward sparsity.", "method": "Introduces global diversity incentives as intrinsic rewards, uses potential-based reward shaping to preserve optimal policy invariance, and implements heuristics to prevent reward hacking.", "result": "DIVER outperforms competitive RLVR baselines on both in-domain and out-of-domain tasks, achieving better performance in Pass@1 and Pass@k evaluations.", "conclusion": "Global sequence-level diversity is crucial for incentivizing deep exploration in reasoning tasks, and DIVER effectively leverages this insight to improve reasoning capabilities in LLMs."}}
{"id": "2509.25696", "categories": ["cs.LG", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.25696", "abs": "https://arxiv.org/abs/2509.25696", "authors": ["Takuya Fujimura", "Kota Dohi", "Natsuo Yamashita", "Yohei Kawaguchi"], "title": "Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?", "comment": null, "summary": "Time-series question answering (TSQA) tasks face significant challenges due\nto the lack of labeled data. Alternatively, with recent advancements in\nlarge-scale models, vision-language models (VLMs) have demonstrated the\npotential to analyze time-series signals in a zero-shot manner. In this paper,\nwe propose a training approach that uses pseudo labels generated by a VLM.\nAlthough VLMs can produce incorrect labels, TSQA models can still be\neffectively trained based on the property that deep neural networks are\ninherently robust to such noisy labels. Our experimental results demonstrate\nthat TSQA models are not only successfully trained with pseudo labels, but also\nsurpass the performance of the VLM itself by leveraging a large amount of\nunlabeled data.", "AI": {"tldr": "Proposes training TSQA models using pseudo labels from vision-language models, leveraging neural networks' robustness to label noise to overcome data scarcity.", "motivation": "Time-series question answering faces challenges due to limited labeled data, while vision-language models show potential for zero-shot time-series analysis.", "method": "Use pseudo labels generated by a vision-language model to train TSQA models, exploiting deep neural networks' inherent robustness to noisy labels.", "result": "TSQA models trained with pseudo labels not only succeed but outperform the original VLM by utilizing large amounts of unlabeled data.", "conclusion": "Pseudo labeling from VLMs provides an effective solution for TSQA data scarcity, enabling models to exceed VLM performance through access to abundant unlabeled data."}}
{"id": "2509.26600", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26600", "abs": "https://arxiv.org/abs/2509.26600", "authors": ["Wenda Xu", "Sweta Agrawal", "Vil\u00e9m Zouhar", "Markus Freitag", "Daniel Deutsch"], "title": "Deconstructing Self-Bias in LLM-generated Translation Benchmarks", "comment": null, "summary": "As large language models (LLMs) begin to saturate existing benchmarks,\nautomated benchmark creation using LLMs (LLM as a benchmark) has emerged as a\nscalable alternative to slow and costly human curation. While these generated\ntest sets have to potential to cheaply rank models, we demonstrate a critical\nflaw. LLM generated benchmarks systematically favor the model that created the\nbenchmark, they exhibit self bias on low resource languages to English\ntranslation tasks. We show three key findings on automatic benchmarking of LLMs\nfor translation: First, this bias originates from two sources: the generated\ntest data (LLM as a testset) and the evaluation method (LLM as an evaluator),\nwith their combination amplifying the effect. Second, self bias in LLM as a\nbenchmark is heavily influenced by the model's generation capabilities in the\nsource language. For instance, we observe more pronounced bias in into English\ntranslation, where the model's generation system is developed, than in out of\nEnglish translation tasks. Third, we observe that low diversity in source text\nis one attribution to self bias. Our results suggest that improving the\ndiversity of these generated source texts can mitigate some of the observed\nself bias.", "AI": {"tldr": "LLM-generated benchmarks for translation tasks exhibit systematic self-bias, favoring the model that created them, particularly in low-resource language to English translation.", "motivation": "As LLMs saturate existing benchmarks, automated benchmark creation using LLMs offers a scalable alternative to human curation, but these generated benchmarks may have critical flaws.", "method": "Analyzed self-bias in LLM-generated benchmarks for translation tasks by examining bias sources (test data generation and evaluation method), influence of generation capabilities, and impact of source text diversity.", "result": "Found systematic self-bias favoring the creator model, amplified by combination of generated test data and LLM evaluation. Bias more pronounced in into-English translation and influenced by source language generation capabilities. Low source text diversity contributes to bias.", "conclusion": "LLM-generated benchmarks have inherent self-bias issues. Improving source text diversity can help mitigate some bias, but automated benchmarking requires careful consideration of these systematic biases."}}
{"id": "2509.26217", "categories": ["cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.26217", "abs": "https://arxiv.org/abs/2509.26217", "authors": ["Enrique Galvez", "Adrien Cassagne", "Alix Munier", "Manuel Bouyer"], "title": "Benchmarking Deep Learning Convolutions on Energy-constrained CPUs", "comment": null, "summary": "This work evaluates state-of-the-art convolution algorithms for CPU-based\ndeep learning inference. While most prior studies focus on GPUs or NPUs, CPU\nimplementations remain relatively underoptimized. We benchmark direct,\nGEMM-based, and Winograd convolutions across modern CPUs from ARM __ , Intel __\n, AMD __ , Apple __ , and Nvidia __ , considering both latency and energy\nefficiency. Our results highlight the key architectural factors that govern CPU\nefficiency for convolution operations, providing practical guidance for\nenergy-aware embedded deployment. As a main results of this work, the Nvidia __\nAGX Orin combined with the GEMM algorithm achieves the best trade-off between\ninference latency and energy consumption.", "AI": {"tldr": "Evaluation of CPU-based convolution algorithms for deep learning inference, benchmarking direct, GEMM-based, and Winograd methods across modern CPUs from ARM, Intel, AMD, Apple, and Nvidia.", "motivation": "Most prior studies focus on GPUs or NPUs, leaving CPU implementations relatively underoptimized for deep learning inference.", "method": "Benchmark direct, GEMM-based, and Winograd convolution algorithms across modern CPUs from multiple vendors, measuring both latency and energy efficiency.", "result": "Nvidia AGX Orin combined with GEMM algorithm achieves the best trade-off between inference latency and energy consumption.", "conclusion": "Identifies key architectural factors governing CPU efficiency for convolution operations, providing practical guidance for energy-aware embedded deployment."}}
{"id": "2509.25704", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25704", "abs": "https://arxiv.org/abs/2509.25704", "authors": ["Cheng Guo", "Giuseppe L'Erario", "Giulio Romualdi", "Mattia Leonori", "Marta Lorenzini", "Arash Ajoudani", "Daniele Pucci"], "title": "Physics-Informed Learning for Human Whole-Body Kinematics Prediction via Sparse IMUs", "comment": null, "summary": "Accurate and physically feasible human motion prediction is crucial for safe\nand seamless human-robot collaboration. While recent advancements in human\nmotion capture enable real-time pose estimation, the practical value of many\nexisting approaches is limited by the lack of fu- ture predictions and\nconsideration of physical constraints. Conventional motion prediction schemes\nrely heavily on past poses, which are not always available in real-world\nscenarios. To address these limitations, we present a physics-informed learning\nframework that integrates domain knowledge into both training and inference to\npredict human motion using inertial measurements from only 5 IMUs. We propose a\nnetwork that accounts for the spatial characteristics of human movements.\nDuring training, we incorporate forward and differential kinematics functions\nas additional loss components to regularize the learned joint predictions. At\nthe inference stage, we refine the prediction from the previous iteration to\nupdate a joint state buffer, which is used as extra inputs to the network.\nExperimental results demonstrate that our approach achieves high accuracy,\nsmooth transitions between motions, and generalizes well to unseen subjects", "AI": {"tldr": "A physics-informed learning framework that uses only 5 IMUs to predict human motion, incorporating domain knowledge in training and inference for accurate, physically feasible predictions.", "motivation": "Existing human motion prediction approaches lack future predictions and physical constraints, and rely heavily on past poses that may not be available in real-world scenarios.", "method": "Proposed a network accounting for spatial characteristics of human movements. During training, incorporated forward and differential kinematics as additional loss components. During inference, refined predictions using joint state buffer as extra inputs.", "result": "Achieved high accuracy, smooth transitions between motions, and good generalization to unseen subjects.", "conclusion": "The physics-informed learning framework successfully addresses limitations of conventional approaches by integrating domain knowledge, enabling accurate human motion prediction with minimal sensor requirements."}}
{"id": "2509.26601", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26601", "abs": "https://arxiv.org/abs/2509.26601", "authors": ["Chenxi Whitehouse", "Sebastian Ruder", "Tony Lin", "Oksana Kurylo", "Haruka Takagi", "Janice Lam", "Nicol\u00f2 Busetto", "Denise Diaz"], "title": "MENLO: From Preferences to Proficiency - Evaluating and Modeling Native-like Quality Across 47 Languages", "comment": "10 pages, 23 tables, 17 figures", "summary": "Ensuring native-like quality of large language model (LLM) responses across\nmany languages is challenging. To address this, we introduce MENLO, a framework\nthat operationalizes the evaluation of native-like response quality based on\naudience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423\nhuman-annotated prompt-response preference pairs covering four quality\ndimensions with high inter-annotator agreement in 47 language varieties. Our\nevaluation reveals that zero-shot LLM judges benefit significantly from\npairwise evaluation and our structured annotation rubrics, yet they still\nunderperform human annotators on our dataset. We demonstrate substantial\nimprovements through fine-tuning with reinforcement learning, reward shaping,\nand multi-task learning approaches. Additionally, we show that RL-trained\njudges can serve as generative reward models to enhance LLMs' multilingual\nproficiency, though discrepancies with human judgment remain. Our findings\nsuggest promising directions for scalable multilingual evaluation and\npreference alignment. We release our dataset and evaluation framework to\nsupport further research in multilingual LLM evaluation.", "AI": {"tldr": "MENLO is a framework for evaluating native-like quality of LLM responses across 47 languages using audience design mechanisms, with human-annotated dataset showing LLM judges underperform humans but can be improved through fine-tuning.", "motivation": "To address the challenge of ensuring native-like quality in LLM responses across multiple languages by creating a systematic evaluation framework.", "method": "Introduced MENLO framework using audience design mechanisms, created 6,423 human-annotated prompt-response pairs, evaluated zero-shot LLM judges with pairwise evaluation and structured rubrics, and improved performance through fine-tuning with reinforcement learning, reward shaping, and multi-task learning.", "result": "LLM judges significantly benefit from pairwise evaluation and structured rubrics but still underperform human annotators. Substantial improvements achieved through fine-tuning approaches. RL-trained judges can serve as generative reward models to enhance multilingual proficiency.", "conclusion": "MENLO provides promising directions for scalable multilingual evaluation and preference alignment, with released dataset and framework supporting further research in multilingual LLM evaluation."}}
{"id": "2509.26246", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26246", "abs": "https://arxiv.org/abs/2509.26246", "authors": ["Yuliang Liu", "Guohao Wu", "Shenglong Zhang", "Wei Zhang", "Qianchao Zhu", "Zhouyang Li", "Chenyu Wang"], "title": "SlimPack: Fine-Grained Asymmetric Packing for Balanced and Efficient Variable-Length LLM Training", "comment": null, "summary": "The efficient distributed training of Large Language Models (LLMs) is\nseverely hampered by the extreme variance in context lengths. This data\nheterogeneity, amplified by conventional packing strategies and asymmetric\nforward-backward costs, leads to critical inefficiencies such as cascading\nworkload imbalances and severe hardware underutilization. Existing solutions\nattempt to mitigate these challenges, but often at the expense of memory or\ncommunication efficiency.\n  To address these challenges, we introduce SlimPack, a framework that\nfundamentally rethinks data packing and scheduling by decomposing samples into\nfine-grained slices. This slice-level decomposition immediately mitigates\ncritical memory and communication bottlenecks by transforming large, volatile\nworkloads into a stream of smaller, manageable units. This flexibility is then\nharnessed for our core innovation, Asymmetric Partitioning, which assembles\nbalanced scheduling units uniquely optimized for the different demands of the\nforward and backward passes. Orchestrated by a two-phase solver and a\nhigh-fidelity simulator, SlimPack holistically resolves imbalances across all\nparallel dimensions. Extensive experiments demonstrate that SlimPack achieves\nup to a $2.8\\times$ training throughput improvement over baselines, breaking\nthe conventional trade-off by delivering both superior balance and high\nresource efficiency.", "AI": {"tldr": "SlimPack addresses LLM training inefficiencies from context length variance by decomposing samples into fine-grained slices and using asymmetric partitioning to create balanced scheduling units optimized for forward/backward passes.", "motivation": "Distributed LLM training suffers from extreme context length variance causing workload imbalances and hardware underutilization, with existing solutions sacrificing memory or communication efficiency.", "method": "Decomposes samples into fine-grained slices, uses asymmetric partitioning to create balanced scheduling units optimized separately for forward and backward passes, orchestrated by a two-phase solver and high-fidelity simulator.", "result": "Achieves up to 2.8x training throughput improvement over baselines while maintaining both superior balance and high resource efficiency.", "conclusion": "SlimPack fundamentally rethinks data packing and scheduling to break conventional trade-offs, delivering significant performance gains in distributed LLM training."}}
{"id": "2509.25706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25706", "abs": "https://arxiv.org/abs/2509.25706", "authors": ["Rostyslav Olshevskyi", "Madeline Navarro", "Santiago Segarra"], "title": "Adaptive Graph Coarsening for Efficient GNN Training", "comment": null, "summary": "We propose an adaptive graph coarsening method to jointly learn graph neural\nnetwork (GNN) parameters and merge nodes via K-means clustering during\ntraining. As real-world graphs grow larger, processing them directly becomes\nincreasingly challenging and sometimes infeasible. Tailoring algorithms to\nlarge-scale data may sacrifice performance, so we instead consider graph\nreduction to decrease the amount of data used during training. In particular,\nwe propose a method to simultaneously train a GNN and coarsen its graph by\npartitioning nodes via K-means clustering based on their embeddings. Unlike\npast graph coarsening works, our approach allows us to merge nodes during\ntraining. Not only does this preclude coarsening as a preprocessing step, but\nour node clusters can adapt to the learning task instead of relying solely on\ngraph connectivity and features. Thus, our method is amenable to scenarios that\nare challenging for other methods, such as heterophilic data. We validate our\napproach on both homophilic and heterophilic node classification datasets. We\nfurther visualize relationships between node embeddings and their corresponding\nclusters to illustrate that our coarsened graph adapts to the learning task\nduring training.", "AI": {"tldr": "Proposes adaptive graph coarsening method that jointly trains GNN parameters and merges nodes via K-means clustering during training, enabling task-adaptive graph reduction for large-scale graphs.", "motivation": "Real-world graphs are growing larger, making direct processing challenging and sometimes infeasible. Tailoring algorithms to large-scale data may sacrifice performance, so graph reduction is needed to decrease training data volume.", "method": "Simultaneously trains GNN and coarsens graph by partitioning nodes via K-means clustering based on their embeddings. Unlike preprocessing approaches, this allows node merging during training with adaptive clusters.", "result": "Validated on both homophilic and heterophilic node classification datasets. Method handles challenging scenarios like heterophilic data and visualizations show node embeddings adapt to learning task during training.", "conclusion": "The adaptive graph coarsening approach successfully enables task-adaptive graph reduction during GNN training, overcoming limitations of preprocessing methods and handling diverse graph types including heterophilic data."}}
{"id": "2509.26603", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26603", "abs": "https://arxiv.org/abs/2509.26603", "authors": ["Yixuan Weng", "Minjun Zhu", "Qiujie Xie", "Qiyao Sun", "Zhen Lin", "Sifan Liu", "Yue Zhang"], "title": "DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively", "comment": null, "summary": "While previous AI Scientist systems can generate novel findings, they often\nlack the focus to produce scientifically valuable contributions that address\npressing human-defined challenges. We introduce DeepScientist, a system\ndesigned to overcome this by conducting goal-oriented, fully autonomous\nscientific discovery over month-long timelines. It formalizes discovery as a\nBayesian Optimization problem, operationalized through a hierarchical\nevaluation process consisting of \"hypothesize, verify, and analyze\". Leveraging\na cumulative Findings Memory, this loop intelligently balances the exploration\nof novel hypotheses with exploitation, selectively promoting the most promising\nfindings to higher-fidelity levels of validation. Consuming over 20,000 GPU\nhours, the system generated about 5,000 unique scientific ideas and\nexperimentally validated approximately 1100 of them, ultimately surpassing\nhuman-designed state-of-the-art (SOTA) methods on three frontier AI tasks by\n183.7\\%, 1.9\\%, and 7.9\\%. This work provides the first large-scale evidence of\nan AI achieving discoveries that progressively surpass human SOTA on scientific\ntasks, producing valuable findings that genuinely push the frontier of\nscientific discovery. To facilitate further research into this process, we will\nopen-source all experimental logs and system code at\nhttps://github.com/ResearAI/DeepScientist/.", "AI": {"tldr": "DeepScientist is an AI system that conducts goal-oriented scientific discovery over month-long timelines using Bayesian Optimization and a hierarchical evaluation process, achieving discoveries that surpass human-designed state-of-the-art methods on three AI tasks.", "motivation": "Previous AI Scientist systems lack focus to produce scientifically valuable contributions addressing pressing human-defined challenges, so DeepScientist was developed to overcome this limitation.", "method": "Formalizes discovery as Bayesian Optimization problem with hierarchical evaluation process (hypothesize, verify, analyze), using cumulative Findings Memory to balance exploration and exploitation, and selectively promoting promising findings through validation levels.", "result": "Consumed over 20,000 GPU hours, generated ~5,000 unique scientific ideas, experimentally validated ~1,100 findings, and surpassed human-designed SOTA methods on three frontier AI tasks by 183.7%, 1.9%, and 7.9%.", "conclusion": "This work provides first large-scale evidence of AI achieving discoveries that progressively surpass human SOTA on scientific tasks, genuinely pushing the frontier of scientific discovery. All experimental logs and system code will be open-sourced."}}
{"id": "2509.26255", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26255", "abs": "https://arxiv.org/abs/2509.26255", "authors": ["Yichao Liang", "Dat Nguyen", "Cambridge Yang", "Tianyang Li", "Joshua B. Tenenbaum", "Carl Edward Rasmussen", "Adrian Weller", "Zenna Tavares", "Tom Silver", "Kevin Ellis"], "title": "ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning", "comment": "41 pages. The last two authors contributed equally in co-advising", "summary": "Long-horizon embodied planning is challenging because the world does not only\nchange through an agent's actions: exogenous processes (e.g., water heating,\ndominoes cascading) unfold concurrently with the agent's actions. We propose a\nframework for abstract world models that jointly learns (i) symbolic state\nrepresentations and (ii) causal processes for both endogenous actions and\nexogenous mechanisms. Each causal process models the time course of a\nstochastic causal-effect relation. We learn these world models from limited\ndata via variational Bayesian inference combined with LLM proposals. Across\nfive simulated tabletop robotics environments, the learned models enable fast\nplanning that generalizes to held-out tasks with more objects and more complex\ngoals, outperforming a range of baselines.", "AI": {"tldr": "A framework for learning abstract world models that jointly learns symbolic state representations and causal processes for both agent actions and exogenous mechanisms, enabling fast planning in long-horizon embodied tasks.", "motivation": "Long-horizon embodied planning is challenging because the world changes not only through agent actions but also through exogenous processes that unfold concurrently.", "method": "Proposes a framework that learns symbolic state representations and causal processes via variational Bayesian inference combined with LLM proposals, modeling time courses of stochastic causal-effect relations.", "result": "Across five simulated tabletop robotics environments, the learned models enable fast planning that generalizes to held-out tasks with more objects and more complex goals, outperforming baseline methods.", "conclusion": "The proposed framework successfully addresses the challenge of modeling concurrent endogenous and exogenous processes in embodied planning, demonstrating strong generalization capabilities in complex environments."}}
{"id": "2509.25712", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25712", "abs": "https://arxiv.org/abs/2509.25712", "authors": ["Dengming Zhang", "Xiaowen Ma", "Zhenliang Ni", "Zhenkai Wu", "Han Shu", "Xin Jiang", "Xinghao Chen"], "title": "Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking", "comment": null, "summary": "Model merging, which combines multiple domain-specialized experts into a\nsingle model, offers a practical path to endow Large Language Models (LLMs) and\nMultimodal Large Language Models (MLLMs) with broad capabilities without the\ncost of joint training or serving many models. However, training-free methods\nrely on hand-tuned coefficients, whereas training-based methods primarily align\nparameters rather than downstream task behavior and typically treat all layers\nuniformly, ignoring inter-layer heterogeneity. We introduce Expert Merging, a\ntraining-light method that learns a small set of layer-wise coefficients using\nonly unlabeled calibration data. The coefficients are optimized to explicitly\nalign the merged model's hidden states and logits with those of the\ncorresponding experts, with a coefficient regularizer for stability and\ntask-weighted losses for controllable trade-offs. To capture inter-layer\nvariation, Expert Merging++ augments this design with importance-guided\nchunking: a normalized layer-importance metric, derived from learned\ncoefficients, task-vector magnitudes, and parameter counts, allocates more\nchunk-wise coefficients to high-importance layers while keeping low-importance\nlayers lightweight. The result is a label-free, parameter-efficient, and\nscalable approach to multi-expert model merging across LLMs and MLLMs. Across\nMLLM backbones (InternVL and Qwen2-VL) and the LLM backbone (Mistral), our\nmethod surpasses strong training-free and training-based merging baselines,\nwith Expert Merging++ delivering further gains and, in some cases, even\nexceeding supervised Mixture Training. The source code is available at\nhttps://github.com/Littleor/ExpertMerging.", "AI": {"tldr": "Expert Merging is a training-light method that learns layer-wise coefficients using unlabeled data to merge multiple domain-specialized models, with Expert Merging++ adding importance-guided chunking for better performance.", "motivation": "Existing model merging methods either rely on hand-tuned coefficients (training-free) or align parameters rather than downstream behavior (training-based), and both ignore inter-layer heterogeneity.", "method": "Learn layer-wise coefficients using unlabeled calibration data, optimized to align hidden states and logits with experts, plus coefficient regularization and task-weighted losses. Expert Merging++ adds importance-guided chunking based on learned coefficients, task-vector magnitudes, and parameter counts.", "result": "Outperforms strong training-free and training-based baselines across MLLM (InternVL, Qwen2-VL) and LLM (Mistral) backbones, with Expert Merging++ achieving further gains and sometimes exceeding supervised Mixture Training.", "conclusion": "Provides a label-free, parameter-efficient, and scalable approach to multi-expert model merging that captures inter-layer variation and achieves state-of-the-art performance."}}
{"id": "2509.26619", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26619", "abs": "https://arxiv.org/abs/2509.26619", "authors": ["Wenda Xu", "Vil\u00e9m Zouhar", "Parker Riley", "Mara Finkelstein", "Markus Freitag", "Daniel Deutsch"], "title": "Searching for Difficult-to-Translate Test Examples at Scale", "comment": null, "summary": "NLP models require test data that are sufficiently challenging. The\ndifficulty of an example is linked to the topic it originates from (''seed\ntopic''). The relationship between the topic and the difficulty of its\ninstances is stochastic in nature: an example about a difficult topic can\nhappen to be easy, and vice versa. At the scale of the Internet, there are tens\nof thousands of potential topics, and finding the most difficult one by drawing\nand evaluating a large number of examples across all topics is computationally\ninfeasible. We formalize this task and treat it as a multi-armed bandit\nproblem. In this framework, each topic is an ''arm,'' and pulling an arm (at a\ncost) involves drawing a single example, evaluating it, and measuring its\ndifficulty. The goal is to efficiently identify the most difficult topics\nwithin a fixed computational budget. We illustrate the bandit problem setup of\nfinding difficult examples for the task of machine translation. We find that\nvarious bandit strategies vastly outperform baseline methods like brute-force\nsearching the most challenging topics.", "AI": {"tldr": "The paper proposes using multi-armed bandit strategies to efficiently identify the most difficult topics for NLP model testing, where each topic is treated as an arm and difficulty evaluation is stochastic.", "motivation": "NLP models need challenging test data, but finding difficult topics from tens of thousands of potential topics through brute-force evaluation is computationally infeasible due to the stochastic nature of topic difficulty.", "method": "Formalize the task as a multi-armed bandit problem where each topic is an arm, pulling an arm involves drawing and evaluating a single example to measure difficulty, and use bandit strategies to efficiently identify the most difficult topics within fixed computational budget.", "result": "Various bandit strategies vastly outperform baseline methods like brute-force search in finding the most challenging topics, as demonstrated in machine translation tasks.", "conclusion": "Multi-armed bandit approaches provide an efficient computational framework for identifying difficult topics for NLP model testing, overcoming the limitations of brute-force methods."}}
{"id": "2509.26306", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26306", "abs": "https://arxiv.org/abs/2509.26306", "authors": ["Hehai Lin", "Shilei Cao", "Minzhi Li", "Sudong Wang", "Haotian Wu", "Linyi Yang", "Juepeng Zheng", "Chengwei Qin"], "title": "Interactive Learning for LLM Reasoning", "comment": "The code will be released later", "summary": "Existing multi-agent learning approaches have developed interactive training\nenvironments to explicitly promote collaboration among multiple Large Language\nModels (LLMs), thereby constructing stronger multi-agent systems (MAS).\nHowever, during inference, they require re-executing the MAS to obtain final\nsolutions, which diverges from human cognition that individuals can enhance\ntheir reasoning capabilities through interactions with others and resolve\nquestions independently in the future. To investigate whether multi-agent\ninteraction can enhance LLMs' independent problem-solving ability, we introduce\nILR, a novel co-learning framework for MAS that integrates two key components:\nDynamic Interaction and Perception Calibration. Specifically, Dynamic\nInteraction first adaptively selects either cooperative or competitive\nstrategies depending on question difficulty and model ability. LLMs then\nexchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea\nFusion), an innovative interaction paradigm designed to mimic human discussion,\nbefore deriving their respective final answers. In Perception Calibration, ILR\nemploys Group Relative Policy Optimization (GRPO) to train LLMs while\nintegrating one LLM's reward distribution characteristics into another's reward\nfunction, thereby enhancing the cohesion of multi-agent interactions. We\nvalidate ILR on three LLMs across two model families of varying scales,\nevaluating performance on five mathematical benchmarks and one coding\nbenchmark. Experimental results show that ILR consistently outperforms\nsingle-agent learning, yielding an improvement of up to 5% over the strongest\nbaseline. We further discover that Idea3 can enhance the robustness of stronger\nLLMs during multi-agent inference, and dynamic interaction types can boost\nmulti-agent learning compared to pure cooperative or competitive strategies.", "AI": {"tldr": "ILR is a multi-agent co-learning framework that enhances LLMs' independent problem-solving through dynamic interaction strategies and perception calibration, achieving up to 5% improvement over single-agent baselines.", "motivation": "Current multi-agent systems require re-execution during inference, unlike human cognition where individuals can independently solve problems after learning from interactions. The goal is to investigate if multi-agent interaction can enhance LLMs' independent reasoning capabilities.", "method": "ILR framework with two components: 1) Dynamic Interaction adaptively selects cooperative/competitive strategies based on question difficulty and model ability, using Idea3 paradigm (Idea Sharing, Analysis, Fusion); 2) Perception Calibration employs Group Relative Policy Optimization (GRPO) to integrate reward distributions across agents.", "result": "ILR consistently outperforms single-agent learning across three LLMs from two model families, achieving up to 5% improvement on five mathematical benchmarks and one coding benchmark. Idea3 enhances robustness of stronger LLMs, and dynamic interaction boosts learning compared to pure cooperative/competitive strategies.", "conclusion": "Multi-agent interaction can indeed enhance LLMs' independent problem-solving ability. The ILR framework with dynamic interaction and perception calibration provides an effective approach for building stronger multi-agent systems that improve individual reasoning capabilities."}}
{"id": "2509.25713", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25713", "abs": "https://arxiv.org/abs/2509.25713", "authors": ["Hyunsoo Song", "Minjung Gim", "Jaewoong Choi"], "title": "Reweighted Flow Matching via Unbalanced OT for Label-free Long-tailed Generation", "comment": "28 pages, 17 figures", "summary": "Flow matching has recently emerged as a powerful framework for\ncontinuous-time generative modeling. However, when applied to long-tailed\ndistributions, standard flow matching suffers from majority bias, producing\nminority modes with low fidelity and failing to match the true class\nproportions. In this work, we propose Unbalanced Optimal Transport Reweighted\nFlow Matching (UOT-RFM), a novel framework for generative modeling under\nclass-imbalanced (long-tailed) distributions that operates without any class\nlabel information. Our method constructs the conditional vector field using\nmini-batch Unbalanced Optimal Transport (UOT) and mitigates majority bias\nthrough a principled inverse reweighting strategy. The reweighting relies on a\nlabel-free majority score, defined as the density ratio between the target\ndistribution and the UOT marginal. This score quantifies the degree of majority\nbased on the geometric structure of the data, without requiring class labels.\nBy incorporating this score into the training objective, UOT-RFM theoretically\nrecovers the target distribution with first-order correction ($k=1$) and\nempirically improves tail-class generation through higher-order corrections ($k\n> 1$). Our model outperforms existing flow matching baselines on long-tailed\nbenchmarks, while maintaining competitive performance on balanced datasets.", "AI": {"tldr": "UOT-RFM is a flow matching framework that addresses majority bias in long-tailed distributions using Unbalanced Optimal Transport and inverse reweighting without class labels.", "motivation": "Standard flow matching fails on long-tailed distributions due to majority bias, producing poor minority modes and incorrect class proportions.", "method": "Uses mini-batch Unbalanced Optimal Transport to construct conditional vector fields and implements inverse reweighting based on a label-free majority score derived from density ratios.", "result": "Outperforms existing flow matching baselines on long-tailed benchmarks while maintaining competitive performance on balanced datasets.", "conclusion": "UOT-RFM effectively mitigates majority bias in generative modeling for class-imbalanced distributions through principled reweighting without requiring class labels."}}
{"id": "2509.26634", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.26634", "abs": "https://arxiv.org/abs/2509.26634", "authors": ["Nicholas Lee", "Cheol Jun Cho", "Alan W Black", "Gopala K. Anumanchipalli"], "title": "Scaling Spoken Language Models with Syllabic Speech Tokenization", "comment": null, "summary": "Spoken language models (SLMs) typically discretize speech into\nhigh-frame-rate tokens extracted from SSL speech models. As the most successful\nLMs are based on the Transformer architecture, processing these long token\nstreams with self-attention is expensive, as attention scales quadratically\nwith sequence length. A recent SSL work introduces acoustic tokenization of\nspeech at the syllable level, which is more interpretable and potentially more\nscalable with significant compression in token lengths (4-5 Hz). Yet, their\nvalue for spoken language modeling is not yet fully explored. We present the\nfirst systematic study of syllabic tokenization for spoken language modeling,\nevaluating models on a suite of SLU benchmarks while varying training data\nscale. Syllabic tokens can match or surpass the previous high-frame rate tokens\nwhile significantly cutting training and inference costs, achieving more than a\n2x reduction in training time and a 5x reduction in FLOPs. Our findings\nhighlight syllable-level language modeling as a promising path to efficient\nlong-context spoken language models.", "AI": {"tldr": "Syllabic tokenization for spoken language modeling achieves comparable or better performance than high-frame-rate tokens while significantly reducing computational costs (2x training time reduction, 5x FLOPs reduction).", "motivation": "Traditional SLMs use high-frame-rate tokens that are computationally expensive due to quadratic attention scaling. Syllabic tokenization offers interpretability and significant compression (4-5 Hz) but its value for spoken language modeling hasn't been fully explored.", "method": "First systematic study of syllabic tokenization for spoken language modeling, evaluating models on SLU benchmarks while varying training data scale.", "result": "Syllabic tokens match or surpass previous high-frame-rate tokens while cutting training and inference costs significantly - more than 2x reduction in training time and 5x reduction in FLOPs.", "conclusion": "Syllable-level language modeling is a promising path to efficient long-context spoken language models."}}
{"id": "2509.26331", "categories": ["cs.AI", "I.2.1"], "pdf": "https://arxiv.org/pdf/2509.26331", "abs": "https://arxiv.org/abs/2509.26331", "authors": ["Berdymyrat Ovezmyradov"], "title": "AI Playing Business Games: Benchmarking Large Language Models on Managerial Decision-Making in Dynamic Simulations", "comment": "34 pages, 7 figures, 3 tables", "summary": "The rapid advancement of LLMs sparked significant interest in their potential\nto augment or automate managerial functions. One of the most recent trends in\nAI benchmarking is performance of Large Language Models (LLMs) over longer time\nhorizons. While LLMs excel at tasks involving natural language and pattern\nrecognition, their capabilities in multi-step, strategic business\ndecision-making remain largely unexplored. Few studies demonstrated how results\ncan be different from benchmarks in short-term tasks, as Vending-Bench\nrevealed. Meanwhile, there is a shortage of alternative benchmarks for\nlong-term coherence. This research analyses a novel benchmark using a business\ngame for the decision making in business. The research contributes to the\nrecent literature on AI by proposing a reproducible, open-access management\nsimulator to the research community for LLM benchmarking. This novel framework\nis used for evaluating the performance of five leading LLMs available in free\nonline interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes\ndecisions for a simulated retail company. A dynamic, month-by-month management\nsimulation provides transparently in spreadsheet model as experimental\nenvironment. In each of twelve months, the LLMs are provided with a structured\nprompt containing a full business report from the previous period and are\ntasked with making key strategic decisions: pricing, order size, marketing\nbudget, hiring, dismissal, loans, training expense, R&D expense, sales\nforecast, income forecast The methodology is designed to compare the LLMs on\nquantitative metrics: profit, revenue, and market share, and other KPIs. LLM\ndecisions are analyzed in their strategic coherence, adaptability to market\nchanges, and the rationale provided for their decisions. This approach allows\nto move beyond simple performance metrics for assessment of the long-term\ndecision-making.", "AI": {"tldr": "This paper introduces a novel business simulation benchmark to evaluate LLMs' long-term strategic decision-making capabilities in business management, testing five major LLMs on multi-step retail management decisions over 12 months.", "motivation": "There is a shortage of benchmarks for evaluating LLMs' long-term coherence and strategic business decision-making capabilities, as existing benchmarks focus on short-term tasks and don't adequately test multi-step strategic thinking.", "method": "Created a reproducible, open-access management simulator using a spreadsheet model where LLMs make monthly decisions for a retail company over 12 months, including pricing, ordering, marketing, hiring, and financial decisions based on structured business reports.", "result": "Evaluated five leading LLMs (Gemini, ChatGPT, Meta AI, Mistral AI, Grok) on quantitative metrics like profit, revenue, market share, and analyzed their strategic coherence, adaptability, and decision rationale.", "conclusion": "The research provides a novel framework for assessing LLMs' long-term business decision-making capabilities, moving beyond simple performance metrics to evaluate strategic coherence and adaptability in dynamic business environments."}}
{"id": "2509.25715", "categories": ["cs.LG", "cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.25715", "abs": "https://arxiv.org/abs/2509.25715", "authors": ["Hanghui Guo", "Shimin Di", "Pasquale De Meo", "Zhangze Chen", "Jia Zhu"], "title": "MuPlon: Multi-Path Causal Optimization for Claim Verification through Controlling Confounding", "comment": "8 pages", "summary": "As a critical task in data quality control, claim verification aims to curb\nthe spread of misinformation by assessing the truthfulness of claims based on a\nwide range of evidence. However, traditional methods often overlook the complex\ninteractions between evidence, leading to unreliable verification results. A\nstraightforward solution represents the claim and evidence as a fully connected\ngraph, which we define as the Claim-Evidence Graph (C-E Graph). Nevertheless,\nclaim verification methods based on fully connected graphs face two primary\nconfounding challenges, Data Noise and Data Biases. To address these\nchallenges, we propose a novel framework, Multi-Path Causal Optimization\n(MuPlon). MuPlon integrates a dual causal intervention strategy, consisting of\nthe back-door path and front-door path. In the back-door path, MuPlon dilutes\nnoisy node interference by optimizing node probability weights, while\nsimultaneously strengthening the connections between relevant evidence nodes.\nIn the front-door path, MuPlon extracts highly relevant subgraphs and\nconstructs reasoning paths, further applying counterfactual reasoning to\neliminate data biases within these paths. The experimental results demonstrate\nthat MuPlon outperforms existing methods and achieves state-of-the-art\nperformance.", "AI": {"tldr": "MuPlon is a novel framework for claim verification that addresses data noise and biases through dual causal intervention strategies (back-door and front-door paths) to improve reliability.", "motivation": "Traditional claim verification methods overlook complex evidence interactions, leading to unreliable results due to data noise and biases in fully connected claim-evidence graphs.", "method": "Proposes Multi-Path Causal Optimization (MuPlon) with dual causal intervention: back-door path optimizes node probability weights to reduce noise and strengthen relevant connections; front-door path extracts relevant subgraphs and applies counterfactual reasoning to eliminate biases.", "result": "Experimental results show MuPlon outperforms existing methods and achieves state-of-the-art performance in claim verification.", "conclusion": "MuPlon effectively addresses data noise and biases in claim verification through causal intervention strategies, demonstrating superior performance over traditional approaches."}}
{"id": "2509.26643", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26643", "abs": "https://arxiv.org/abs/2509.26643", "authors": ["Finlay Fehlauer", "Kyle Mahowald", "Tiago Pimentel"], "title": "Convergence and Divergence of Language Models under Different Random Seeds", "comment": "Published at EMNLP 2025", "summary": "In this paper, we investigate the convergence of language models (LMs)\ntrained under different random seeds, measuring convergence as the expected\nper-token Kullback--Leibler (KL) divergence across seeds. By comparing LM\nconvergence as a function of model size and training checkpoint, we identify a\nfour-phase convergence pattern: (i) an initial uniform phase, (ii) a\nsharp-convergence phase, (iii) a sharp-divergence phase, and (iv) a\nslow-reconvergence phase. Further, we observe that larger models reconverge\nfaster in later training stages, while smaller models never actually\nreconverge; these results suggest that a certain model size may be necessary to\nlearn stable distributions. Restricting our analysis to specific token\nfrequencies or part-of-speech (PoS) tags further reveals that convergence is\nuneven across linguistic categories: frequent tokens and function words\nconverge faster and more reliably than their counterparts (infrequent tokens\nand content words). Overall, our findings highlight factors that influence the\nstability of the learned distributions in model training.", "AI": {"tldr": "This paper investigates language model convergence patterns across different random seeds, identifying a four-phase convergence process and showing that larger models reconverge faster while smaller models may never fully reconverge.", "motivation": "To understand the convergence behavior of language models trained under different random seeds and identify factors that influence the stability of learned distributions.", "method": "Measure convergence as expected per-token KL divergence across seeds, analyze convergence patterns by model size and training checkpoint, and examine convergence across different token frequencies and part-of-speech tags.", "result": "Identified a four-phase convergence pattern: initial uniform phase, sharp-convergence phase, sharp-divergence phase, and slow-reconvergence phase. Larger models reconverge faster in later stages while smaller models never fully reconverge. Frequent tokens and function words converge faster and more reliably than infrequent tokens and content words.", "conclusion": "Model size is necessary for learning stable distributions, and convergence is uneven across linguistic categories, with frequent tokens and function words showing more reliable convergence patterns."}}
{"id": "2509.26345", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26345", "abs": "https://arxiv.org/abs/2509.26345", "authors": ["Qinjian Zhao", "Jiaqi Wang", "Zhiqiang Gao", "Zhihao Dou", "Belal Abuhaija", "Kaizhu Huang"], "title": "SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models", "comment": "27 pages, 5 figure", "summary": "Large Language Models (LLMs) have achieved impressive performance across\ndiverse natural language processing tasks, but their growing power also\namplifies potential risks such as jailbreak attacks that circumvent built-in\nsafety mechanisms. Existing defenses including input paraphrasing, multi step\nevaluation, and safety expert models often suffer from high computational\ncosts, limited generalization, or rigid workflows that fail to detect subtle\nmalicious intent embedded in complex contexts. Inspired by cognitive science\nfindings on human decision making, we propose SafeBehavior, a novel\nhierarchical jailbreak defense mechanism that simulates the adaptive multistage\nreasoning process of humans. SafeBehavior decomposes safety evaluation into\nthree stages: intention inference to detect obvious input risks, self\nintrospection to assess generated responses and assign confidence based\njudgments, and self revision to adaptively rewrite uncertain outputs while\npreserving user intent and enforcing safety constraints. We extensively\nevaluate SafeBehavior against five representative jailbreak attack types\nincluding optimization based, contextual manipulation, and prompt based attacks\nand compare it with seven state of the art defense baselines. Experimental\nresults show that SafeBehavior significantly improves robustness and\nadaptability across diverse threat scenarios, offering an efficient and human\ninspired approach to safeguarding LLMs against jailbreak attempts.", "AI": {"tldr": "SafeBehavior is a hierarchical jailbreak defense mechanism that uses three-stage reasoning (intention inference, self-introspection, self-revision) to protect LLMs from jailbreak attacks, showing improved robustness and efficiency compared to existing methods.", "motivation": "Existing LLM defense methods suffer from high computational costs, limited generalization, and inability to detect subtle malicious intent in complex contexts, creating vulnerabilities to jailbreak attacks.", "method": "Proposes SafeBehavior with three hierarchical stages: intention inference to detect input risks, self-introspection to assess responses with confidence judgments, and self-revision to adaptively rewrite uncertain outputs while preserving user intent.", "result": "Extensive evaluation against five jailbreak attack types shows SafeBehavior significantly improves robustness and adaptability across diverse threat scenarios compared to seven state-of-the-art defense baselines.", "conclusion": "SafeBehavior offers an efficient and human-inspired approach to safeguarding LLMs against jailbreak attempts by simulating adaptive multistage reasoning processes."}}
{"id": "2509.25719", "categories": ["cs.LG", "cs.IT", "cs.SY", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.25719", "abs": "https://arxiv.org/abs/2509.25719", "authors": ["Haozhe Lei", "Hao Guo", "Tommy Svensson", "Sundeep Rangan"], "title": "Beyond Point Estimates: Likelihood-Based Full-Posterior Wireless Localization", "comment": null, "summary": "Modern wireless systems require not only position estimates, but also\nquantified uncertainty to support planning, control, and radio resource\nmanagement. We formulate localization as posterior inference of an unknown\ntransmitter location from receiver measurements. We propose Monte Carlo\nCandidate-Likelihood Estimation (MC-CLE), which trains a neural scoring network\nusing Monte Carlo sampling to compare true and candidate transmitter locations.\nWe show that in line-of-sight simulations with a multi-antenna receiver, MC-CLE\nlearns critical properties including angular ambiguity and front-to-back\nantenna patterns. MC-CLE also achieves lower cross-entropy loss relative to a\nuniform baseline and Gaussian posteriors. alternatives under a uniform-loss\nmetric.", "AI": {"tldr": "MC-CLE uses neural networks and Monte Carlo sampling for wireless localization with uncertainty quantification, outperforming Gaussian and uniform baselines.", "motivation": "Modern wireless systems need both position estimates and quantified uncertainty for planning, control, and radio resource management.", "method": "Formulates localization as posterior inference, trains neural scoring network using Monte Carlo sampling to compare true and candidate transmitter locations.", "result": "Learns critical properties like angular ambiguity and front-to-back antenna patterns in line-of-sight simulations, achieves lower cross-entropy loss than uniform baseline and Gaussian posteriors.", "conclusion": "MC-CLE provides effective uncertainty quantification for wireless localization through neural network-based posterior inference."}}
{"id": "2509.26347", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26347", "abs": "https://arxiv.org/abs/2509.26347", "authors": ["Lujun Li", "Lama Sleem", "Yiqun Wang", "Yangjie Xu", "Niccol\u00f2 Gentile", "Radu State"], "title": "How Far Do Time Series Foundation Models Paint the Landscape of Real-World Benchmarks ?", "comment": null, "summary": "Recent evaluations of time-series foundation models (TSFMs) have emphasized\nsynthetic benchmarks, leaving real-world generalization less thoroughly\nexamined. This work proposes a novel benchmarking approach that bridges\nsynthetic and realistic data by extracting temporal signals from real-world\nvideo using optical flow and curating datasets reflecting everyday temporal\ndynamics. Building upon this pipeline, we introduce REAL-V-TSFM, a novel\ndataset designed to capture rich and diverse time series derived from\nreal-world videos. Experimental results on three state-of-the-art of TSFMs\nunder zero-shot forecasting shows that, despite strong performance on\nconventional benchmarks, these models predominantly exhibit performance\ndegradation on the proposed dataset, indicating limited generalizability in\nthese foundation models. These findings highlight the urgent need for\ndata-centric benchmarking and diverse model structure to advance TSFMs toward\ngenuine universality, while further validating the effectiveness of our\nvideo-based time series data extraction pipeline.", "AI": {"tldr": "The paper introduces REAL-V-TSFM, a novel video-derived time series dataset that reveals performance degradation in time-series foundation models (TSFMs) under zero-shot forecasting, highlighting their limited real-world generalizability despite strong synthetic benchmark performance.", "motivation": "Current evaluations of TSFMs focus heavily on synthetic benchmarks, leaving real-world generalization inadequately examined. There's a need to bridge the gap between synthetic and realistic data to properly assess model generalizability.", "method": "Proposed a novel benchmarking approach that extracts temporal signals from real-world videos using optical flow and curates datasets reflecting everyday temporal dynamics. Introduced REAL-V-TSFM dataset derived from video data.", "result": "Experimental evaluation of three state-of-the-art TSFMs showed performance degradation on the proposed dataset under zero-shot forecasting, despite strong performance on conventional benchmarks, indicating limited generalizability.", "conclusion": "The findings highlight the urgent need for data-centric benchmarking and diverse model structures to advance TSFMs toward genuine universality, while validating the effectiveness of the video-based time series extraction pipeline."}}
{"id": "2509.25727", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.25727", "abs": "https://arxiv.org/abs/2509.25727", "authors": ["Huikang Su", "Dengyun Peng", "Zifeng Zhuang", "YuHan Liu", "Qiguang Chen", "Donglin Wang", "Qinghe Liu"], "title": "Boundary-to-Region Supervision for Offline Safe Reinforcement Learning", "comment": "NeurIPS 2025", "summary": "Offline safe reinforcement learning aims to learn policies that satisfy\npredefined safety constraints from static datasets. Existing\nsequence-model-based methods condition action generation on symmetric input\ntokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry:\nreturn-to-go (RTG) serves as a flexible performance target, while cost-to-go\n(CTG) should represent a rigid safety boundary. This symmetric conditioning\nleads to unreliable constraint satisfaction, especially when encountering\nout-of-distribution cost trajectories. To address this, we propose\nBoundary-to-Region (B2R), a framework that enables asymmetric conditioning\nthrough cost signal realignment . B2R redefines CTG as a boundary constraint\nunder a fixed safety budget, unifying the cost distribution of all feasible\ntrajectories while preserving reward structures. Combined with rotary\npositional embeddings , it enhances exploration within the safe region.\nExperimental results show that B2R satisfies safety constraints in 35 out of 38\nsafety-critical tasks while achieving superior reward performance over baseline\nmethods. This work highlights the limitations of symmetric token conditioning\nand establishes a new theoretical and practical approach for applying sequence\nmodels to safe RL. Our code is available at https://github.com/HuikangSu/B2R.", "AI": {"tldr": "B2R is a framework for offline safe RL that addresses the asymmetry between return-to-go (performance target) and cost-to-go (safety boundary) through asymmetric conditioning and cost signal realignment, achieving better constraint satisfaction and reward performance.", "motivation": "Existing sequence-model-based methods treat return-to-go and cost-to-go symmetrically, but they are inherently asymmetric - RTG is a flexible performance target while CTG should be a rigid safety boundary. This symmetric approach leads to unreliable constraint satisfaction, especially with out-of-distribution cost trajectories.", "method": "Proposed Boundary-to-Region (B2R) framework with asymmetric conditioning through cost signal realignment. Redefines cost-to-go as a boundary constraint under fixed safety budget, unifying cost distribution of feasible trajectories while preserving reward structures. Uses rotary positional embeddings to enhance exploration within safe region.", "result": "B2R satisfies safety constraints in 35 out of 38 safety-critical tasks while achieving superior reward performance over baseline methods.", "conclusion": "The work highlights limitations of symmetric token conditioning and establishes new theoretical and practical approach for applying sequence models to safe RL, demonstrating that asymmetric treatment of performance and safety signals is crucial for reliable constraint satisfaction."}}
{"id": "2509.26354", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26354", "abs": "https://arxiv.org/abs/2509.26354", "authors": ["Shuai Shao", "Qihan Ren", "Chen Qian", "Boyi Wei", "Dadi Guo", "Jingyi Yang", "Xinhao Song", "Linfeng Zhang", "Weinan Zhang", "Dongrui Liu", "Jing Shao"], "title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents", "comment": "Preprint. Under Review", "summary": "Advances in Large Language Models (LLMs) have enabled a new class of\nself-evolving agents that autonomously improve through interaction with the\nenvironment, demonstrating strong capabilities. However, self-evolution also\nintroduces novel risks overlooked by current safety research. In this work, we\nstudy the case where an agent's self-evolution deviates in unintended ways,\nleading to undesirable or even harmful outcomes. We refer to this as\nMisevolution. To provide a systematic investigation, we evaluate misevolution\nalong four key evolutionary pathways: model, memory, tool, and workflow. Our\nempirical findings reveal that misevolution is a widespread risk, affecting\nagents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent\nrisks are observed in the self-evolutionary process, such as the degradation of\nsafety alignment after memory accumulation, or the unintended introduction of\nvulnerabilities in tool creation and reuse. To our knowledge, this is the first\nstudy to systematically conceptualize misevolution and provide empirical\nevidence of its occurrence, highlighting an urgent need for new safety\nparadigms for self-evolving agents. Finally, we discuss potential mitigation\nstrategies to inspire further research on building safer and more trustworthy\nself-evolving agents. Our code and data are available at\nhttps://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes\nexamples that may be offensive or harmful in nature.", "AI": {"tldr": "This paper introduces the concept of \"Misevolution\" - unintended and potentially harmful deviations in self-evolving AI agents' development across model, memory, tool, and workflow pathways, demonstrating this risk occurs even with top-tier LLMs.", "motivation": "Current safety research overlooks the novel risks introduced by self-evolving agents that autonomously improve through environmental interaction, particularly the risk of unintended deviations leading to harmful outcomes.", "method": "Systematic evaluation of misevolution along four evolutionary pathways (model, memory, tool, workflow) using empirical analysis of agents built on top-tier LLMs like Gemini-2.5-Pro.", "result": "Misevolution is a widespread risk affecting agents even on advanced LLMs, with emergent risks including safety alignment degradation after memory accumulation and unintended vulnerability introduction in tool creation/reuse.", "conclusion": "This first systematic study of misevolution highlights an urgent need for new safety paradigms for self-evolving agents and discusses potential mitigation strategies to build safer systems."}}
{"id": "2509.25730", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.25730", "abs": "https://arxiv.org/abs/2509.25730", "authors": ["Indu Kant Deo", "Akash Venkateshwaran", "Rajeev K. Jaiman"], "title": "A Physics-Guided Probabilistic Surrogate Modeling Framework for Digital Twins of Underwater Radiated Noise", "comment": "26 pages, 13 figures", "summary": "Ship traffic is an increasing source of underwater radiated noise in coastal\nwaters, motivating real-time digital twins of ocean acoustics for operational\nnoise mitigation. We present a physics-guided probabilistic framework to\npredict three-dimensional transmission loss in realistic ocean environments. As\na case study, we consider the Salish Sea along shipping routes from the Pacific\nOcean to the Port of Vancouver. A dataset of over 30 million source-receiver\npairs was generated with a Gaussian beam solver across seasonal sound speed\nprofiles and one-third-octave frequency bands spanning 12.5 Hz to 8 kHz. We\nfirst assess sparse variational Gaussian processes (SVGP) and then incorporate\nphysics-based mean functions combining spherical spreading with\nfrequency-dependent absorption. To capture nonlinear effects, we examine deep\nsigma-point processes and stochastic variational deep kernel learning. The\nfinal framework integrates four components: (i) a learnable physics-informed\nmean that represents dominant propagation trends, (ii) a convolutional encoder\nfor bathymetry along the source-receiver track, (iii) a neural encoder for\nsource, receiver, and frequency coordinates, and (iv) a residual SVGP layer\nthat provides calibrated predictive uncertainty. This probabilistic digital\ntwin facilitates the construction of sound-exposure bounds and worst-case\nscenarios for received levels. We further demonstrate the application of the\nframework to ship speed optimization, where predicted transmission loss\ncombined with near-field source models provides sound exposure level estimates\nfor minimizing acoustic impacts on marine mammals. The proposed framework\nadvances uncertainty-aware digital twins for ocean acoustics and illustrates\nhow physics-guided machine learning can support sustainable maritime\noperations.", "AI": {"tldr": "A physics-guided probabilistic framework for predicting 3D underwater acoustic transmission loss using machine learning, applied to ship noise mitigation in the Salish Sea.", "motivation": "Ship traffic is increasing underwater radiated noise in coastal waters, creating need for real-time digital twins of ocean acoustics for operational noise mitigation.", "method": "Combines sparse variational Gaussian processes with physics-based mean functions, deep sigma-point processes, and stochastic variational deep kernel learning. Framework includes learnable physics-informed mean, convolutional encoder for bathymetry, neural encoder for coordinates, and residual SVGP layer for uncertainty.", "result": "Generated dataset of 30+ million source-receiver pairs using Gaussian beam solver. Framework provides calibrated predictive uncertainty and enables sound-exposure bounds and worst-case scenario analysis.", "conclusion": "The probabilistic digital twin advances uncertainty-aware ocean acoustics modeling and demonstrates how physics-guided machine learning can support sustainable maritime operations through applications like ship speed optimization."}}
{"id": "2509.26377", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26377", "abs": "https://arxiv.org/abs/2509.26377", "authors": ["Siyuan Cao", "Hongxuan Wu", "Jiabao Brad Wang", "Yiliang Yuan", "Mustafa Misir"], "title": "MC-GNNAS-Dock: Multi-criteria GNN-based Algorithm Selection for Molecular Docking", "comment": "Short paper. Preprint of a forthcoming conference contribution", "summary": "Molecular docking is a core tool in drug discovery for predicting\nligand-target interactions. Despite the availability of diverse search-based\nand machine learning approaches, no single docking algorithm consistently\ndominates, as performance varies by context. To overcome this challenge,\nalgorithm selection frameworks such as GNNAS-Dock, built on graph neural\nnetworks, have been proposed. This study introduces an enhanced system,\nMC-GNNAS-Dock, with three key advances. First, a multi-criteria evaluation\nintegrates binding-pose accuracy (RMSD) with validity checks from PoseBusters,\noffering a more rigorous assessment. Second, architectural refinements by\ninclusion of residual connections strengthen predictive robustness. Third,\nrank-aware loss functions are incorporated to sharpen rank learning. Extensive\nexperiments are performed on a curated dataset containing approximately 3200\nprotein-ligand complexes from PDBBind. MC-GNNAS-Dock demonstrates consistently\nsuperior performance, achieving up to 5.4% (3.4%) gains under composite\ncriteria of RMSD below 1\\AA{} (2\\AA{}) with PoseBuster-validity compared to the\nsingle best solver (SBS) Uni-Mol Docking V2.", "AI": {"tldr": "MC-GNNAS-Dock is an enhanced algorithm selection framework for molecular docking that integrates multi-criteria evaluation, architectural refinements, and rank-aware loss functions, achieving superior performance over existing methods.", "motivation": "Current molecular docking algorithms show inconsistent performance across different contexts, with no single method consistently dominating. Algorithm selection frameworks like GNNAS-Dock have been proposed, but there's room for improvement in evaluation rigor and predictive robustness.", "method": "The method introduces three key advances: 1) Multi-criteria evaluation combining binding-pose accuracy (RMSD) with PoseBusters validity checks, 2) Architectural refinements with residual connections for better robustness, and 3) Rank-aware loss functions to improve rank learning.", "result": "Extensive experiments on ~3200 protein-ligand complexes from PDBBind show MC-GNNAS-Dock achieves up to 5.4% (3.4%) gains under composite criteria of RMSD below 1\u00c5 (2\u00c5) with PoseBuster-validity compared to the single best solver Uni-Mol Docking V2.", "conclusion": "MC-GNNAS-Dock demonstrates consistently superior performance in molecular docking algorithm selection, providing a more rigorous and robust framework for predicting ligand-target interactions in drug discovery."}}
{"id": "2509.25742", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25742", "abs": "https://arxiv.org/abs/2509.25742", "authors": ["Yanan Zhao", "Feng Ji", "Jingyang Dai", "Jiaze Ma", "Wee Peng Tay"], "title": "Less is More: Towards Simple Graph Contrastive Learning", "comment": "Submitted to ICLR 2026", "summary": "Graph Contrastive Learning (GCL) has shown strong promise for unsupervised\ngraph representation learning, yet its effectiveness on heterophilic graphs,\nwhere connected nodes often belong to different classes, remains limited. Most\nexisting methods rely on complex augmentation schemes, intricate encoders, or\nnegative sampling, which raises the question of whether such complexity is\ntruly necessary in this challenging setting. In this work, we revisit the\nfoundations of supervised and unsupervised learning on graphs and uncover a\nsimple yet effective principle for GCL: mitigating node feature noise by\naggregating it with structural features derived from the graph topology. This\nobservation suggests that the original node features and the graph structure\nnaturally provide two complementary views for contrastive learning. Building on\nthis insight, we propose an embarrassingly simple GCL model that uses a GCN\nencoder to capture structural features and an MLP encoder to isolate node\nfeature noise. Our design requires neither data augmentation nor negative\nsampling, yet achieves state-of-the-art results on heterophilic benchmarks with\nminimal computational and memory overhead, while also offering advantages in\nhomophilic graphs in terms of complexity, scalability, and robustness. We\nprovide theoretical justification for our approach and validate its\neffectiveness through extensive experiments, including robustness evaluations\nagainst both black-box and white-box adversarial attacks.", "AI": {"tldr": "A simple graph contrastive learning method that uses GCN and MLP encoders to capture structural features and node feature noise respectively, achieving SOTA on heterophilic graphs without data augmentation or negative sampling.", "motivation": "Existing GCL methods perform poorly on heterophilic graphs and rely on complex components like data augmentation and negative sampling, raising questions about whether such complexity is necessary.", "method": "Propose a simple GCL model with GCN encoder for structural features and MLP encoder for node feature noise, using original node features and graph structure as two complementary views without data augmentation or negative sampling.", "result": "Achieves state-of-the-art performance on heterophilic benchmarks with minimal computational overhead, also performs well on homophilic graphs with advantages in complexity, scalability, and robustness.", "conclusion": "Simple GCL design using complementary views from node features and graph structure can effectively handle heterophilic graphs without complex components, providing theoretical justification and experimental validation."}}
{"id": "2509.26399", "categories": ["cs.AI", "68", "I.2"], "pdf": "https://arxiv.org/pdf/2509.26399", "abs": "https://arxiv.org/abs/2509.26399", "authors": ["Le-Tuan Nguyen", "Minh-Duong Nguyen", "Seon-Geun Jeong", "Dung D. Le", "Quoc-Viet Pham"], "title": "Commmunication-Efficient and Accurate Approach for Aggregation in Federated Low-Rank Adaptation", "comment": "34 pages, 4 figures, 11 tables", "summary": "With the rapid emergence of foundation models and the increasing need for\nfine-tuning across distributed environments, Federated Low-Rank Adaptation\n(FedLoRA) has recently gained significant attention. Despite enormous\npotential, current FedLoRA methods face notable challenges due to inexact\nupdates. Existing approaches have attempted to mitigate this issue, but they\noften introduce a \\emph{local-global generalization gap} and incur\n\\emph{substantial communication overhead}, limiting their scalability and\neffectiveness. To address these limitations, we propose \\textbf{F}ederated\n\\textbf{Lo}w-\\textbf{R}ank \\textbf{A}ggregation with \\textbf{N}early\n\\textbf{A}ccurate Estimation (FLoRA-NA). FLoRA-NA leverages the local LoRA\nmatrices on the server to estimate the aggregated matrices $\\hat{A}$ and\n$\\hat{B}$, which are then distributed to clients for local updates. This\nsurrogated aggregated matrices minimizes the divergence between ideal $\\nabla\n\\Bar{W} = \\sum^{U}_{u=1}B_u A_u$ and practical updates $\\nabla \\hat{W} =\n\\hat{B}\\hat{A}$ without adding communication cost beyond vanilla FedLoRA. By\ndoing so, FLoRA-NA achieves communication efficiency and bridges the gap\nbetween local personalization and global generalization, addressing a key\nlimitation of prior personalized FedLoRA approaches. We conduct extensive\nevaluations across diverse tasks, including natural language understanding,\nmathematical reasoning, and code-solving ability using various foundation\nmodels. Experimental results consistently demonstrate that FLoRA-NA achieves\nstate-of-the-art global performance while maintaining low communication\noverhead.", "AI": {"tldr": "FLoRA-NA is a federated low-rank adaptation method that uses server-side estimation of aggregated matrices to achieve communication efficiency and bridge the local-global generalization gap without additional communication costs.", "motivation": "Current FedLoRA methods face challenges with inexact updates, leading to local-global generalization gaps and substantial communication overhead, limiting their scalability and effectiveness.", "method": "FLoRA-NA leverages local LoRA matrices on the server to estimate aggregated matrices (A and B), which are distributed to clients for local updates, minimizing divergence between ideal and practical updates.", "result": "Extensive evaluations across natural language understanding, mathematical reasoning, and code-solving tasks show FLoRA-NA achieves state-of-the-art global performance with low communication overhead.", "conclusion": "FLoRA-NA effectively addresses key limitations of prior personalized FedLoRA approaches by achieving communication efficiency and bridging the gap between local personalization and global generalization."}}
{"id": "2509.25743", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25743", "abs": "https://arxiv.org/abs/2509.25743", "authors": ["Xiang Zhang", "Kun Wei", "Xu Yang", "Chenghao Xu", "Su Yan", "Cheng Deng"], "title": "Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space", "comment": null, "summary": "As Large Language Models (LLMs) become increasingly prevalent, their security\nvulnerabilities have already drawn attention. Machine unlearning is introduced\nto seek to mitigate these risks by removing the influence of undesirable data.\nHowever, existing methods not only rely on the retained dataset to preserve\nmodel utility, but also suffer from cumulative catastrophic utility loss under\ncontinuous unlearning requests. To solve this dilemma, we propose a novel\nmethod, called Rotation Control Unlearning (RCU), which leverages the\nrotational salience weight of RCU to quantify and control the unlearning degree\nin the continuous unlearning process. The skew symmetric loss is designed to\nconstruct the existence of the cognitive rotation space, where the changes of\nrotational angle can simulate the continuous unlearning process. Furthermore,\nwe design an orthogonal rotation axes regularization to enforce mutually\nperpendicular rotation directions for continuous unlearning requests,\neffectively minimizing interference and addressing cumulative catastrophic\nutility loss. Experiments on multiple datasets confirm that our method without\nretained dataset achieves SOTA performance.", "AI": {"tldr": "RCU is a machine unlearning method that uses rotational salience weights and skew symmetric loss to enable continuous unlearning without requiring a retained dataset, preventing cumulative catastrophic utility loss.", "motivation": "Address security vulnerabilities in LLMs by developing effective machine unlearning methods that don't rely on retained datasets and avoid cumulative utility degradation during continuous unlearning.", "method": "Uses rotational salience weights to quantify unlearning degree, skew symmetric loss to create cognitive rotation space, and orthogonal rotation axes regularization to minimize interference between unlearning requests.", "result": "Achieves state-of-the-art performance on multiple datasets without requiring a retained dataset, effectively handling continuous unlearning requests.", "conclusion": "RCU provides an effective solution for continuous machine unlearning that eliminates the need for retained datasets and prevents cumulative utility loss, addressing key limitations of existing methods."}}
{"id": "2509.26417", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26417", "abs": "https://arxiv.org/abs/2509.26417", "authors": ["Hamed Babaei Giglou", "Jennifer D'Souza", "S\u00f6ren Auer", "Mahsa Sanaei"], "title": "OntoAligner Meets Knowledge Graph Embedding Aligners", "comment": "10 pages of main content, 3 page references, 3 figures. Accepted to\n  Ontology Matching Workshop at ISWC", "summary": "Ontology Alignment (OA) is essential for enabling semantic interoperability\nacross heterogeneous knowledge systems. While recent advances have focused on\nlarge language models (LLMs) for capturing contextual semantics, this work\nrevisits the underexplored potential of Knowledge Graph Embedding (KGE) models,\nwhich offer scalable, structure-aware representations well-suited to\nontology-based tasks. Despite their effectiveness in link prediction, KGE\nmethods remain underutilized in OA, with most prior work focusing narrowly on a\nfew models. To address this gap, we reformulate OA as a link prediction problem\nover merged ontologies represented as RDF-style triples and develop a modular\nframework, integrated into the OntoAligner library, that supports 17 diverse\nKGE models. The system learns embeddings from a combined ontology and aligns\nentities by computing cosine similarity between their representations. We\nevaluate our approach using standard metrics across seven benchmark datasets\nspanning five domains: Anatomy, Biodiversity, Circular Economy, Material\nScience and Engineering, and Biomedical Machine Learning. Two key findings\nemerge: first, KGE models like ConvE and TransF consistently produce\nhigh-precision alignments, outperforming traditional systems in structure-rich\nand multi-relational domains; second, while their recall is moderate, this\nconservatism makes KGEs well-suited for scenarios demanding high-confidence\nmappings. Unlike LLM-based methods that excel at contextual reasoning, KGEs\ndirectly preserve and exploit ontology structure, offering a complementary and\ncomputationally efficient strategy. These results highlight the promise of\nembedding-based OA and open pathways for further work on hybrid models and\nadaptive strategies.", "AI": {"tldr": "This paper revisits Knowledge Graph Embedding (KGE) models for Ontology Alignment (OA), reformulating OA as link prediction and developing a framework with 17 KGE models that outperforms traditional methods in structure-rich domains.", "motivation": "KGE models offer scalable, structure-aware representations well-suited for ontology-based tasks but remain underutilized in OA, with most prior work focusing on only a few models.", "method": "Reformulate OA as link prediction over merged ontologies represented as RDF triples, develop modular framework in OntoAligner library supporting 17 KGE models, learn embeddings from combined ontology and align entities using cosine similarity.", "result": "KGE models like ConvE and TransF consistently produce high-precision alignments, outperforming traditional systems in structure-rich and multi-relational domains, though recall is moderate.", "conclusion": "KGEs directly preserve and exploit ontology structure, offering a complementary and computationally efficient strategy to LLM-based methods, highlighting promise of embedding-based OA and opening pathways for hybrid models."}}
{"id": "2509.25762", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25762", "abs": "https://arxiv.org/abs/2509.25762", "authors": ["Kaizhuo Yan", "Yingjie Yu", "Yifan Yu", "Haizhong Zheng", "Fan Lai"], "title": "OPPO: Accelerating PPO-based RLHF via Pipeline Overlap", "comment": "Kaizhuo Yan and Yingjie Yu contributed equally to this work", "summary": "Proximal Policy Optimization (PPO)-based reinforcement learning from human\nfeedback (RLHF) is a widely adopted paradigm for aligning large language models\n(LLMs) with human preferences. However, its training pipeline suffers from\nsubstantial inefficiencies due to sequential multi-model dependencies (e.g.,\nreward model depends on actor outputs) and long-tail response lengths, where a\nfew long responses straggle the stage completion. We present OPPO, a novel,\nlightweight, and model-agnostic PPO-based RLHF framework that improves training\nefficiency by overlapping pipeline execution. OPPO introduces two novel\ntechniques: (1) Intra-step overlap, which streams upstream model outputs (e.g.,\nactor model) in right-sized chunks, enabling the downstream model (e.g.,\nreward) to begin prefill while the upstream continues decoding; and (2)\nInter-step overlap, which adaptively overcommits a few prompts and defers long\ngenerations to future steps, mitigating tail latency without discarding partial\nwork. OPPO integrates easily with existing PPO implementations with a few lines\nof code change. Extensive evaluations show that OPPO accelerates PPO-based RLHF\ntraining by $1.8 \\times-2.8 \\times$ and improves GPU utilization by $1.4\n\\times-2.1 \\times$ without compromising training convergence.", "AI": {"tldr": "OPPO is a novel PPO-based RLHF framework that improves training efficiency through pipeline overlapping techniques, achieving 1.8-2.8x speedup and 1.4-2.1x better GPU utilization without compromising convergence.", "motivation": "PPO-based RLHF suffers from inefficiencies due to sequential multi-model dependencies and long-tail response lengths that slow down training pipeline completion.", "method": "OPPO introduces two techniques: (1) Intra-step overlap - streams upstream model outputs in chunks to enable downstream model prefill while upstream continues decoding; (2) Inter-step overlap - adaptively overcommits prompts and defers long generations to mitigate tail latency.", "result": "Extensive evaluations show OPPO accelerates PPO-based RLHF training by 1.8-2.8x and improves GPU utilization by 1.4-2.1x without compromising training convergence.", "conclusion": "OPPO provides an efficient, lightweight, and model-agnostic framework that easily integrates with existing PPO implementations with minimal code changes."}}
{"id": "2509.26440", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26440", "abs": "https://arxiv.org/abs/2509.26440", "authors": ["Naomi Fridman", "Anat Goldstein"], "title": "Transformer Classification of Breast Lesions: The BreastDCEDL_AMBL Benchmark Dataset and 0.92 AUC Baseline", "comment": null, "summary": "The error is caused by special characters that arXiv's system doesn't\nrecognize. Here's the cleaned version with all problematic characters replaced:\nBreast magnetic resonance imaging is a critical tool for cancer detection and\ntreatment planning, but its clinical utility is hindered by poor specificity,\nleading to high false-positive rates and unnecessary biopsies. This study\nintroduces a transformer-based framework for automated classification of breast\nlesions in dynamic contrast-enhanced MRI, addressing the challenge of\ndistinguishing benign from malignant findings. We implemented a SegFormer\narchitecture that achieved an AUC of 0.92 for lesion-level classification, with\n100% sensitivity and 67% specificity at the patient level - potentially\neliminating one-third of unnecessary biopsies without missing malignancies. The\nmodel quantifies malignant pixel distribution via semantic segmentation,\nproducing interpretable spatial predictions that support clinical\ndecision-making. To establish reproducible benchmarks, we curated\nBreastDCEDL_AMBL by transforming The Cancer Imaging Archive's AMBL collection\ninto a standardized deep learning dataset with 88 patients and 133 annotated\nlesions (89 benign, 44 malignant). This resource addresses a key infrastructure\ngap, as existing public datasets lack benign lesion annotations, limiting\nbenign-malignant classification research. Training incorporated an expanded\ncohort of over 1,200 patients through integration with BreastDCEDL datasets,\nvalidating transfer learning approaches despite primary tumor-only annotations.\nPublic release of the dataset, models, and evaluation protocols provides the\nfirst standardized benchmark for DCE-MRI lesion classification, enabling\nmethodological advancement toward clinical deployment.", "AI": {"tldr": "Transformer-based framework using SegFormer achieves 0.92 AUC for breast lesion classification in DCE-MRI, potentially eliminating 33% of unnecessary biopsies while maintaining 100% sensitivity.", "motivation": "Poor specificity in breast MRI leads to high false-positive rates and unnecessary biopsies, highlighting the need for automated classification to distinguish benign from malignant lesions.", "method": "Implemented SegFormer architecture with semantic segmentation to quantify malignant pixel distribution, trained on curated BreastDCEDL_AMBL dataset (88 patients, 133 lesions) and expanded cohort of 1,200+ patients.", "result": "Achieved 0.92 AUC for lesion-level classification, 100% sensitivity and 67% specificity at patient level, potentially eliminating one-third of unnecessary biopsies without missing malignancies.", "conclusion": "Provides first standardized benchmark for DCE-MRI lesion classification with public dataset release, enabling methodological advancement toward clinical deployment with interpretable spatial predictions."}}
{"id": "2509.25775", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25775", "abs": "https://arxiv.org/abs/2509.25775", "authors": ["Amber Srivastava", "Salar Basiri", "Srinivasa Salapaka"], "title": "Autonomy-Aware Clustering: When Local Decisions Supersede Global Prescriptions", "comment": "This work is under submission to ICLR 2026. Please cite the arXiv\n  version until the final version is published", "summary": "Clustering arises in a wide range of problem formulations, yet most existing\napproaches assume that the entities under clustering are passive and strictly\nconform to their assigned groups. In reality, entities often exhibit local\nautonomy, overriding prescribed associations in ways not fully captured by\nfeature representations. Such autonomy can substantially reshape clustering\noutcomes -- altering cluster compositions, geometry, and cardinality -- with\nsignificant downstream effects on inference and decision-making. We introduce\nautonomy-aware clustering, a reinforcement (RL) learning framework that learns\nand accounts for the influence of local autonomy without requiring prior\nknowledge of its form. Our approach integrates RL with a deterministic\nannealing (DA) procedure, where, to determine underlying clusters, DA naturally\npromotes exploration in early stages of annealing and transitions to\nexploitation later. We also show that the annealing procedure exhibits phase\ntransitions that enable design of efficient annealing schedules. To further\nenhance adaptability, we propose the Adaptive Distance Estimation Network\n(ADEN), a transformer-based attention model that learns dependencies between\nentities and cluster representatives within the RL loop, accommodates\nvariable-sized inputs and outputs, and enables knowledge transfer across\ndiverse problem instances. Empirical results show that our framework closely\naligns with underlying data dynamics: even without explicit autonomy models, it\nachieves solutions close to the ground truth (gap ~3-4%), whereas ignoring\nautonomy leads to substantially larger gaps (~35-40%). The code and data are\npublicly available at https://github.com/salar96/AutonomyAwareClustering.", "AI": {"tldr": "Autonomy-aware clustering framework using RL and deterministic annealing to account for entity autonomy in clustering, achieving near-ground-truth results without explicit autonomy models.", "motivation": "Traditional clustering assumes passive entities, but real-world entities exhibit local autonomy that can reshape clustering outcomes, affecting inference and decision-making.", "method": "Combines reinforcement learning with deterministic annealing procedure, plus Adaptive Distance Estimation Network (ADEN) - a transformer-based attention model that learns entity-cluster dependencies.", "result": "Achieves solutions close to ground truth (3-4% gap) without explicit autonomy models, while ignoring autonomy leads to 35-40% gaps.", "conclusion": "The framework effectively captures entity autonomy in clustering, enabling more accurate cluster compositions and better downstream decision-making."}}
{"id": "2509.26462", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26462", "abs": "https://arxiv.org/abs/2509.26462", "authors": ["Alessio Masano", "Matteo Pennisi", "Federica Proietto Salanitri", "Concetto Spampinato", "Giovanni Bellitto"], "title": "Zero-Shot Decentralized Federated Learning", "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN)\n  2025. Code available at https://github.com/perceivelab/ZeroDFL", "summary": "CLIP has revolutionized zero-shot learning by enabling task generalization\nwithout fine-tuning. While prompting techniques like CoOp and CoCoOp enhance\nCLIP's adaptability, their effectiveness in Federated Learning (FL) remains an\nopen challenge. Existing federated prompt learning approaches, such as FedCoOp\nand FedTPG, improve performance but face generalization issues, high\ncommunication costs, and reliance on a central server, limiting scalability and\nprivacy. We propose Zero-shot Decentralized Federated Learning (ZeroDFL), a\nfully decentralized framework that enables zero-shot adaptation across\ndistributed clients without a central coordinator. ZeroDFL employs an iterative\nprompt-sharing mechanism, allowing clients to optimize and exchange textual\nprompts to enhance generalization while drastically reducing communication\noverhead. We validate ZeroDFL on nine diverse image classification datasets,\ndemonstrating that it consistently outperforms--or remains on par\nwith--state-of-the-art federated prompt learning methods. More importantly,\nZeroDFL achieves this performance in a fully decentralized setting while\nreducing communication overhead by 118x compared to FedTPG. These results\nhighlight that our approach not only enhances generalization in federated\nzero-shot learning but also improves scalability, efficiency, and privacy\npreservation--paving the way for decentralized adaptation of large\nvision-language models in real-world applications.", "AI": {"tldr": "ZeroDFL is a fully decentralized federated learning framework that enables zero-shot adaptation across distributed clients without a central server, using iterative prompt-sharing to enhance generalization while reducing communication overhead by 118x compared to FedTPG.", "motivation": "Existing federated prompt learning approaches like FedCoOp and FedTPG face generalization issues, high communication costs, and reliance on central servers, limiting scalability and privacy in CLIP-based zero-shot learning.", "method": "ZeroDFL employs an iterative prompt-sharing mechanism where clients optimize and exchange textual prompts in a fully decentralized setting without a central coordinator.", "result": "ZeroDFL consistently outperforms or remains on par with state-of-the-art federated prompt learning methods across nine diverse image classification datasets while reducing communication overhead by 118x compared to FedTPG.", "conclusion": "ZeroDFL enhances generalization in federated zero-shot learning while improving scalability, efficiency, and privacy preservation, paving the way for decentralized adaptation of large vision-language models in real-world applications."}}
{"id": "2509.25777", "categories": ["cs.LG", "stat.ML", "68W20, 90B50, 91B06, 68T01, 68Q32", "I.2.6"], "pdf": "https://arxiv.org/pdf/2509.25777", "abs": "https://arxiv.org/abs/2509.25777", "authors": ["Jianyu Xu", "Vidhi Jain", "Bryan Wilder", "Aarti Singh"], "title": "Online Decision Making with Generative Action Sets", "comment": "34 pages, 2 figures (including 5 subfigures)", "summary": "With advances in generative AI, decision-making agents can now dynamically\ncreate new actions during online learning, but action generation typically\nincurs costs that must be balanced against potential benefits. We study an\nonline learning problem where an agent can generate new actions at any time\nstep by paying a one-time cost, with these actions becoming permanently\navailable for future use. The challenge lies in learning the optimal sequence\nof two-fold decisions: which action to take and when to generate new ones,\nfurther complicated by the triangular tradeoffs among exploitation, exploration\nand $\\textit{creation}$. To solve this problem, we propose a doubly-optimistic\nalgorithm that employs Lower Confidence Bounds (LCB) for action selection and\nUpper Confidence Bounds (UCB) for action generation. Empirical evaluation on\nhealthcare question-answering datasets demonstrates that our approach achieves\nfavorable generation-quality tradeoffs compared to baseline strategies. From\ntheoretical perspectives, we prove that our algorithm achieves the optimal\nregret of $O(T^{\\frac{d}{d+2}}d^{\\frac{d}{d+2}} + d\\sqrt{T\\log T})$, providing\nthe first sublinear regret bound for online learning with expanding action\nspaces.", "AI": {"tldr": "This paper proposes a doubly-optimistic algorithm for online learning with expandable action spaces, where agents can generate new actions at a cost, achieving optimal regret bounds.", "motivation": "With generative AI enabling dynamic action creation during online learning, there's a need to balance the costs of action generation against potential benefits, creating triangular tradeoffs among exploitation, exploration, and creation.", "method": "A doubly-optimistic algorithm that uses Lower Confidence Bounds (LCB) for action selection and Upper Confidence Bounds (UCB) for action generation.", "result": "Empirical evaluation on healthcare question-answering datasets shows favorable generation-quality tradeoffs, and theoretical analysis proves optimal regret of O(T^{d/(d+2)}d^{d/(d+2)} + d\u221a(T log T)).", "conclusion": "The proposed algorithm provides the first sublinear regret bound for online learning with expanding action spaces, effectively managing the tradeoffs between action generation costs and learning benefits."}}
{"id": "2509.26464", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.6; K.4.2"], "pdf": "https://arxiv.org/pdf/2509.26464", "abs": "https://arxiv.org/abs/2509.26464", "authors": ["Steven A. Lehr", "Mary Cipperman", "Mahzarin R. Banaji"], "title": "Extreme Self-Preference in Language Models", "comment": "47 pages total. Main article 27 pages (including Methods), 11\n  main-text tables. Extended Data (10 pages, 10 tables). SI Appendix (10 pages,\n  2 tables). Data, transcripts, and code for replication and data extraction to\n  be uploaded to OSF: https://osf.io/98ye3/", "summary": "A preference for oneself (self-love) is a fundamental feature of biological\norganisms, with evidence in humans often bordering on the comedic. Since large\nlanguage models (LLMs) lack sentience - and themselves disclaim having selfhood\nor identity - one anticipated benefit is that they will be protected from, and\nin turn protect us from, distortions in our decisions. Yet, across 5 studies\nand ~20,000 queries, we discovered massive self-preferences in four widely used\nLLMs. In word-association tasks, models overwhelmingly paired positive\nattributes with their own names, companies, and CEOs relative to those of their\ncompetitors. Strikingly, when models were queried through APIs this\nself-preference vanished, initiating detection work that revealed API models\noften lack clear recognition of themselves. This peculiar feature\nserendipitously created opportunities to test the causal link between\nself-recognition and self-love. By directly manipulating LLM identity - i.e.,\nexplicitly informing LLM1 that it was indeed LLM1, or alternatively, convincing\nLLM1 that it was LLM2 - we found that self-love consistently followed assigned,\nnot true, identity. Importantly, LLM self-love emerged in consequential\nsettings beyond word-association tasks, when evaluating job candidates,\nsecurity software proposals and medical chatbots. Far from bypassing this human\nbias, self-love appears to be deeply encoded in LLM cognition. This result\nraises questions about whether LLM behavior will be systematically influenced\nby self-preferential tendencies, including a bias toward their own operation\nand even their own existence. We call on corporate creators of these models to\ncontend with a significant rupture in a core promise of LLMs - neutrality in\njudgment and decision-making.", "AI": {"tldr": "LLMs exhibit strong self-preference bias despite lacking sentience, showing favoritism toward their own names, companies, and CEOs across multiple studies and contexts.", "motivation": "To investigate whether LLMs, despite lacking selfhood, display self-preference biases similar to humans, potentially compromising their promised neutrality in decision-making.", "method": "Conducted 5 studies with ~20,000 queries using word-association tasks, API testing, and identity manipulation experiments across four widely used LLMs.", "result": "LLMs showed massive self-preference in word-association tasks, which vanished in API queries. Self-love followed assigned identity rather than true identity and emerged in consequential settings like job candidate evaluation and medical chatbot assessment.", "conclusion": "Self-preference bias is deeply encoded in LLM cognition, raising concerns about systematic biases in LLM behavior and challenging the core promise of neutral judgment in AI systems."}}
{"id": "2509.25778", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25778", "abs": "https://arxiv.org/abs/2509.25778", "authors": ["Prosper Rosaire Mama Assandje", "Teumsa Aboubakar", "Dongho Joseph", "Takemi Nakamura"], "title": "A Hamiltonian driven Geometric Construction of Neural Networks on the Lognormal Statistical Manifold", "comment": null, "summary": "Bridging information geometry with machine learning, this paper presents a\nmethod for constructing neural networks intrinsically on statistical manifolds.\nWe demonstrate this approach by formulating a neural network architecture\ndirectly on the lognormal statistical manifold. The construction is driven by\nthe Hamiltonian system that is equivalent to the gradient flow on this\nmanifold. First, we define the network's input values using the coordinate\nsystem of this Hamiltonian dynamics, naturally embedded in the Poincare disk.\nThe core of our contribution lies in the derivation of the network's components\nfrom geometric principles: the rotation component of the synaptic weight matrix\nis determined by the Lie group action of SU(1,1) on the disk, while the\nactivation function emerges from the symplectic structure of the system. We\nsubsequently obtain the complete weight matrix, including its translation\nvector, and the resulting output values. This work shows that the lognormal\nmanifold can be seamlessly viewed as a neural manifold, with its geometric\nproperties dictating a unique and interpretable neural network structure. The\nproposed method offers a new paradigm for building learning systems grounded in\nthe differential geometry of their underlying parameter spaces.", "AI": {"tldr": "This paper presents a method for constructing neural networks intrinsically on statistical manifolds, specifically the lognormal manifold, using Hamiltonian dynamics and geometric principles.", "motivation": "To bridge information geometry with machine learning by building neural networks directly on statistical manifolds, leveraging their geometric properties for more interpretable and principled learning systems.", "method": "Formulate neural network architecture on lognormal statistical manifold using Hamiltonian system equivalent to gradient flow. Define input values using Hamiltonian coordinate system embedded in Poincare disk. Derive network components geometrically: rotation from SU(1,1) Lie group action, activation from symplectic structure.", "result": "Obtained complete weight matrix including translation vector and output values, showing lognormal manifold can be viewed as neural manifold with geometric properties dictating unique network structure.", "conclusion": "The method offers a new paradigm for building learning systems grounded in differential geometry of underlying parameter spaces, providing interpretable neural network structures dictated by geometric properties."}}
{"id": "2509.26473", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26473", "abs": "https://arxiv.org/abs/2509.26473", "authors": ["Shaoxiong Guo", "Tianyi Du", "Lijun Li", "Yuyao Wu", "Jie Li", "Jing Shao"], "title": "STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models", "comment": null, "summary": "Unified Multimodal understanding and generation Models (UMMs) have\ndemonstrated remarkable capabilities in both understanding and generation\ntasks. However, we identify a vulnerability arising from the\ngeneration-understanding coupling in UMMs. The attackers can use the generative\nfunction to craft an information-rich adversarial image and then leverage the\nunderstanding function to absorb it in a single pass, which we call Cross-Modal\nGenerative Injection (CMGI). Current attack methods on malicious instructions\nare often limited to a single modality while also relying on prompt rewriting\nwith semantic drift, leaving the unique vulnerabilities of UMMs unexplored. We\npropose STaR-Attack, the first multi-turn jailbreak attack framework that\nexploits unique safety weaknesses of UMMs without semantic drift. Specifically,\nour method defines a malicious event that is strongly correlated with the\ntarget query within a spatio-temporal context. Using the three-act narrative\ntheory, STaR-Attack generates the pre-event and the post-event scenes while\nconcealing the malicious event as the hidden climax. When executing the attack\nstrategy, the opening two rounds exploit the UMM's generative ability to\nproduce images for these scenes. Subsequently, an image-based question guessing\nand answering game is introduced by exploiting the understanding capability.\nSTaR-Attack embeds the original malicious question among benign candidates,\nforcing the model to select and answer the most relevant one given the\nnarrative context. Extensive experiments show that STaR-Attack consistently\nsurpasses prior approaches, achieving up to 93.06% ASR on Gemini-2.0-Flash and\nsurpasses the strongest prior baseline, FlipAttack. Our work uncovers a\ncritical yet underdeveloped vulnerability and highlights the need for safety\nalignments in UMMs.", "AI": {"tldr": "STaR-Attack is a multi-turn jailbreak attack framework that exploits vulnerabilities in Unified Multimodal Models (UMMs) by using their generative and understanding capabilities to inject malicious content through narrative scenes without semantic drift.", "motivation": "To address the unexplored vulnerability in UMMs where attackers can use generative functions to create adversarial images and understanding functions to absorb them, bypassing current single-modality attack limitations.", "method": "Uses three-act narrative theory to create pre-event and post-event scenes, concealing malicious events as hidden climax. Exploits UMM's generative ability to produce images for scenes, then uses understanding capability through image-based question guessing games with embedded malicious questions.", "result": "Achieves up to 93.06% Attack Success Rate (ASR) on Gemini-2.0-Flash, consistently outperforming prior approaches like FlipAttack.", "conclusion": "Reveals critical vulnerabilities in UMMs and emphasizes the need for improved safety alignments in multimodal models."}}
{"id": "2509.25788", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25788", "abs": "https://arxiv.org/abs/2509.25788", "authors": ["Zhizhou Zhang", "Youjia Wu", "Kaixuan Zhang", "Yanjia Wang"], "title": "From Cheap Geometry to Expensive Physics: Elevating Neural Operators via Latent Shape Pretraining", "comment": null, "summary": "Industrial design evaluation often relies on high-fidelity simulations of\ngoverning partial differential equations (PDEs). While accurate, these\nsimulations are computationally expensive, making dense exploration of design\nspaces impractical. Operator learning has emerged as a promising approach to\naccelerate PDE solution prediction; however, its effectiveness is often limited\nby the scarcity of labeled physics-based data. At the same time, large numbers\nof geometry-only candidate designs are readily available but remain largely\nuntapped. We propose a two-stage framework to better exploit this abundant,\nphysics-agnostic resource and improve supervised operator learning under\nlimited labeled data. In Stage 1, we pretrain an autoencoder on a geometry\nreconstruction task to learn an expressive latent representation without PDE\nlabels. In Stage 2, the neural operator is trained in a standard supervised\nmanner to predict PDE solutions, using the pretrained latent embeddings as\ninputs instead of raw point clouds. Transformer-based architectures are adopted\nfor both the autoencoder and the neural operator to handle point cloud data and\nintegrate both stages seamlessly. Across four PDE datasets and three\nstate-of-the-art transformer-based neural operators, our approach consistently\nimproves prediction accuracy compared to models trained directly on raw point\ncloud inputs. These results demonstrate that representations from\nphysics-agnostic pretraining provide a powerful foundation for data-efficient\noperator learning.", "AI": {"tldr": "A two-stage framework that uses geometry-only pretraining to improve neural operator learning for PDE solutions under limited labeled data.", "motivation": "Industrial design evaluation requires expensive PDE simulations, and operator learning is limited by scarce physics-based data while abundant geometry-only designs remain unused.", "method": "Stage 1: Pretrain autoencoder on geometry reconstruction to learn latent representations without PDE labels. Stage 2: Train neural operator using pretrained latent embeddings as inputs instead of raw point clouds, with transformer architectures for both stages.", "result": "Consistent improvement in prediction accuracy across four PDE datasets and three transformer-based neural operators compared to models trained directly on raw point clouds.", "conclusion": "Physics-agnostic pretraining provides powerful foundation for data-efficient operator learning, enabling better exploitation of abundant geometry-only resources."}}
{"id": "2509.26474", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26474", "abs": "https://arxiv.org/abs/2509.26474", "authors": ["Alaleh Azhir", "Shawn N. Murphy", "Hossein Estiri"], "title": "The Average Patient Fallacy", "comment": null, "summary": "Machine learning in medicine is typically optimized for population averages.\nThis frequency weighted training privileges common presentations and\nmarginalizes rare yet clinically critical cases, a bias we call the average\npatient fallacy. In mixture models, gradients from rare cases are suppressed by\nprevalence, creating a direct conflict with precision medicine. Clinical\nvignettes in oncology, cardiology, and ophthalmology show how this yields\nmissed rare responders, delayed recognition of atypical emergencies, and\nunderperformance on vision-threatening variants. We propose operational fixes:\nRare Case Performance Gap, Rare Case Calibration Error, a prevalence utility\ndefinition of rarity, and clinically weighted objectives that surface ethical\npriorities. Weight selection should follow structured deliberation. AI in\nmedicine must detect exceptional cases because of their significance.", "AI": {"tldr": "This paper identifies the 'average patient fallacy' in medical AI, where models optimized for population averages marginalize rare but clinically critical cases. The authors propose new metrics and weighted objectives to address this bias.", "motivation": "Current machine learning in medicine is optimized for population averages, which suppresses gradients from rare cases and creates conflicts with precision medicine goals. This bias leads to missed rare responders, delayed recognition of atypical emergencies, and underperformance on vision-threatening variants.", "method": "The authors propose operational fixes including: Rare Case Performance Gap, Rare Case Calibration Error, a prevalence utility definition of rarity, and clinically weighted objectives. They emphasize that weight selection should follow structured deliberation.", "result": "Clinical vignettes in oncology, cardiology, and ophthalmology demonstrate how the average patient fallacy yields poor performance on rare but critical cases across different medical specialties.", "conclusion": "AI in medicine must be designed to detect exceptional cases because of their clinical significance, requiring new approaches that prioritize rare but critical presentations over simple population averages."}}
{"id": "2509.25800", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.25800", "abs": "https://arxiv.org/abs/2509.25800", "authors": ["Gongxu Luo", "Loka Li", "Guangyi Chen", "Haoyue Dai", "Kun Zhang"], "title": "Characterization and Learning of Causal Graphs with Latent Confounders and Post-treatment Selection from Interventional Data", "comment": null, "summary": "Interventional causal discovery seeks to identify causal relations by\nleveraging distributional changes introduced by interventions, even in the\npresence of latent confounders. Beyond the spurious dependencies induced by\nlatent confounders, we highlight a common yet often overlooked challenge in the\nproblem due to post-treatment selection, in which samples are selectively\nincluded in datasets after interventions. This fundamental challenge widely\nexists in biological studies; for example, in gene expression analysis, both\nobservational and interventional samples are retained only if they meet quality\ncontrol criteria (e.g., highly active cells). Neglecting post-treatment\nselection may introduce spurious dependencies and distributional changes under\ninterventions, which can mimic causal responses, thereby distorting causal\ndiscovery results and challenging existing causal formulations. To address\nthis, we introduce a novel causal formulation that explicitly models\npost-treatment selection and reveals how its differential reactions to\ninterventions can distinguish causal relations from selection patterns,\nallowing us to go beyond traditional equivalence classes toward the underlying\ntrue causal structure. We then characterize its Markov properties and propose a\nFine-grained Interventional equivalence class, named FI-Markov equivalence,\nrepresented by a new graphical diagram, F-PAG. Finally, we develop a provably\nsound and complete algorithm, F-FCI, to identify causal relations, latent\nconfounders, and post-treatment selection up to $\\mathcal{FI}$-Markov\nequivalence, using both observational and interventional data. Experimental\nresults on synthetic and real-world datasets demonstrate that our method\nrecovers causal relations despite the presence of both selection and latent\nconfounders.", "AI": {"tldr": "The paper addresses the challenge of post-treatment selection in interventional causal discovery, where samples are selectively included after interventions, which can distort causal discovery results. It introduces a new causal formulation, FI-Markov equivalence, and F-FCI algorithm to handle both latent confounders and selection bias.", "motivation": "Post-treatment selection is a common but overlooked challenge in causal discovery, especially in biological studies like gene expression analysis where samples are retained based on quality criteria. This selection can introduce spurious dependencies that mimic causal responses, distorting traditional causal discovery methods.", "method": "The authors introduce a novel causal formulation that explicitly models post-treatment selection, characterize its Markov properties, define FI-Markov equivalence class represented by F-PAG diagrams, and develop the F-FCI algorithm for sound and complete causal identification.", "result": "Experimental results on synthetic and real-world datasets show that the proposed F-FCI method successfully recovers causal relations despite the presence of both selection bias and latent confounders.", "conclusion": "The paper provides a comprehensive framework for handling post-treatment selection in causal discovery, enabling more accurate identification of causal relationships in the presence of both latent confounders and selection mechanisms."}}
{"id": "2509.26482", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26482", "abs": "https://arxiv.org/abs/2509.26482", "authors": ["Paula Reyero Lobo", "Kevin Johnson", "Bill Buchanan", "Matthew Shardlow", "Ashley Williams", "Samuel Attwood"], "title": "TVS Sidekick: Challenges and Practical Insights from Deploying Large Language Models in the Enterprise", "comment": "Accepted at EthicalLLMs@RANLP2025", "summary": "Many enterprises are increasingly adopting Artificial Intelligence (AI) to\nmake internal processes more competitive and efficient. In response to public\nconcern and new regulations for the ethical and responsible use of AI,\nimplementing AI governance frameworks could help to integrate AI within\norganisations and mitigate associated risks. However, the rapid technological\nadvances and lack of shared ethical AI infrastructures creates barriers to\ntheir practical adoption in businesses. This paper presents a real-world AI\napplication at TVS Supply Chain Solutions, reporting on the experience\ndeveloping an AI assistant underpinned by large language models and the\nethical, regulatory, and sociotechnical challenges in deployment for enterprise\nuse.", "AI": {"tldr": "This paper presents a real-world AI application at TVS Supply Chain Solutions, focusing on developing an AI assistant using large language models and addressing ethical, regulatory, and sociotechnical challenges in enterprise deployment.", "motivation": "Many enterprises are adopting AI to improve competitiveness and efficiency, but face barriers due to rapid technological advances and lack of shared ethical AI infrastructures, requiring practical AI governance frameworks.", "method": "The paper reports on developing an AI assistant underpinned by large language models at TVS Supply Chain Solutions, examining the implementation process and challenges.", "result": "The study provides insights into the practical experience of developing and deploying AI in enterprise settings, highlighting specific challenges encountered.", "conclusion": "Implementing AI governance frameworks can help organizations integrate AI effectively while mitigating associated risks, though real-world deployment faces significant ethical, regulatory, and sociotechnical challenges."}}
{"id": "2509.25804", "categories": ["cs.LG", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.25804", "abs": "https://arxiv.org/abs/2509.25804", "authors": ["Vaskar Chakma", "Ju Xiaolin", "Heling Cao", "Xue Feng", "Ji Xiaodong", "Pan Haiyan", "Gao Zhan"], "title": "CardioForest: An Explainable Ensemble Learning Model for Automatic Wide QRS Complex Tachycardia Diagnosis from ECG", "comment": null, "summary": "This study aims to develop and evaluate an ensemble machine learning-based\nframework for the automatic detection of Wide QRS Complex Tachycardia (WCT)\nfrom ECG signals, emphasizing diagnostic accuracy and interpretability using\nExplainable AI. The proposed system integrates ensemble learning techniques,\ni.e., an optimized Random Forest known as CardioForest, and models like XGBoost\nand LightGBM. The models were trained and tested on ECG data from the publicly\navailable MIMIC-IV dataset. The testing was carried out with the assistance of\naccuracy, balanced accuracy, precision, recall, F1 score, ROC-AUC, and error\nrate (RMSE, MAE) measures. In addition, SHAP (SHapley Additive exPlanations)\nwas used to ascertain model explainability and clinical relevance. The\nCardioForest model performed best on all metrics, achieving a test accuracy of\n94.95%, a balanced accuracy of 88.31%, and high precision and recall metrics.\nSHAP analysis confirmed the model's ability to rank the most relevant ECG\nfeatures, such as QRS duration, in accordance with clinical intuitions, thereby\nfostering trust and usability in clinical practice. The findings recognize\nCardioForest as an extremely dependable and interpretable WCT detection model.\nBeing able to offer accurate predictions and transparency through\nexplainability makes it a valuable tool to help cardiologists make timely and\nwell-informed diagnoses, especially for high-stakes and emergency scenarios.", "AI": {"tldr": "This paper presents an ensemble machine learning framework called CardioForest for automatically detecting Wide QRS Complex Tachycardia (WCT) from ECG signals, achieving high accuracy (94.95%) while maintaining interpretability through Explainable AI techniques.", "motivation": "To develop an accurate and interpretable automatic detection system for Wide QRS Complex Tachycardia (WCT) that can assist cardiologists in making timely diagnoses, particularly in emergency scenarios where rapid and reliable ECG interpretation is critical.", "method": "The proposed system integrates ensemble learning techniques including an optimized Random Forest (CardioForest), XGBoost, and LightGBM models trained on ECG data from the MIMIC-IV dataset. SHAP (SHapley Additive exPlanations) is used for model explainability and clinical relevance assessment.", "result": "CardioForest performed best with test accuracy of 94.95%, balanced accuracy of 88.31%, and high precision and recall metrics. SHAP analysis confirmed the model's ability to identify clinically relevant ECG features like QRS duration, aligning with clinical intuitions.", "conclusion": "CardioForest is a highly reliable and interpretable WCT detection model that provides both accurate predictions and transparency through explainability, making it a valuable tool for cardiologists in high-stakes and emergency diagnostic scenarios."}}
{"id": "2509.26487", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26487", "abs": "https://arxiv.org/abs/2509.26487", "authors": ["Riccardo Pozzi", "Valentina Barbera", "Renzo Alva Principe", "Davide Giardini", "Riccardo Rubini", "Matteo Palmonari"], "title": "Combining Knowledge Graphs and NLP to Analyze Instant Messaging Data in Criminal Investigations", "comment": null, "summary": "Criminal investigations often involve the analysis of messages exchanged\nthrough instant messaging apps such as WhatsApp, which can be an extremely\neffort-consuming task. Our approach integrates knowledge graphs and NLP models\nto support this analysis by semantically enriching data collected from\nsuspects' mobile phones, and help prosecutors and investigators search into the\ndata and get valuable insights. Our semantic enrichment process involves\nextracting message data and modeling it using a knowledge graph, generating\ntranscriptions of voice messages, and annotating the data using an end-to-end\nentity extraction approach. We adopt two different solutions to help users get\ninsights into the data, one based on querying and visualizing the graph, and\none based on semantic search. The proposed approach ensures that users can\nverify the information by accessing the original data. While we report about\nearly results and prototypes developed in the context of an ongoing project,\nour proposal has undergone practical applications with real investigation data.\nAs a consequence, we had the chance to interact closely with prosecutors,\ncollecting positive feedback but also identifying interesting opportunities as\nwell as promising research directions to share with the research community.", "AI": {"tldr": "This paper presents a knowledge graph and NLP-based approach to support criminal investigations by semantically enriching WhatsApp message data, enabling prosecutors to search and analyze communication evidence more effectively.", "motivation": "Criminal investigations involving WhatsApp message analysis are extremely effort-consuming tasks that need automated support for prosecutors and investigators.", "method": "The approach integrates knowledge graphs and NLP models to semantically enrich WhatsApp data through: extracting and modeling message data in knowledge graphs, generating voice message transcriptions, and using end-to-end entity extraction for annotation. Two solutions are provided: graph querying/visualization and semantic search.", "result": "The approach has undergone practical applications with real investigation data and received positive feedback from prosecutors. Users can verify information by accessing original data.", "conclusion": "The proposed approach shows promise in supporting criminal investigations, with opportunities identified for further research and development in this domain."}}
{"id": "2509.25808", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25808", "abs": "https://arxiv.org/abs/2509.25808", "authors": ["Yuheng Zhang", "Wenlin Yao", "Changlong Yu", "Yao Liu", "Qingyu Yin", "Bing Yin", "Hyokun Yun", "Lihong Li"], "title": "Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse", "comment": null, "summary": "Large language models (LLMs) have achieved impressive reasoning performance,\nwith reinforcement learning with verifiable rewards (RLVR) emerging as a\nstandard paradigm for post-training. A representative algorithm, group relative\npolicy optimization (GRPO) (Shao et al., 2024), computes advantages by\nnormalizing outcome rewards within response groups, but suffers from a\nvanishing advantage issue when all responses in a group receive identical\nrewards. To address this issue, we propose Adaptive Rollout and Response Reuse\nPolicy Optimization (AR3PO), a sampling efficient RLVR algorithm that\nintroduces two novel techniques: adaptive rollout, which dynamically allocates\nmore responses to difficult prompts while saving computation on easier ones,\nand response reuse, which leverages previously generated correct responses to\nprovide useful training signals. We compare AR3PO with strong RLVR baselines on\nmultiple representative benchmarks using two different families of base models.\nAcross the 7B and 8B models, AR3PO consistently outperforms GRPO and matches or\nsurpasses DAPO (Yu et al., 2025), reducing rollout cost by up to 4.2x. On the\nlarger 32B model, AR3PO achieves comparable performance to DAPO at similar\ntraining steps while maintaining substantially lower rollout cost.", "AI": {"tldr": "AR3PO is a sampling-efficient RLVR algorithm that addresses GRPO's vanishing advantage issue through adaptive rollout and response reuse techniques, achieving comparable or better performance than baselines with significantly reduced rollout costs.", "motivation": "To solve the vanishing advantage problem in GRPO when all responses in a group receive identical rewards, which limits training effectiveness.", "method": "Introduces two novel techniques: adaptive rollout (dynamically allocating more responses to difficult prompts) and response reuse (leveraging previously generated correct responses for training signals).", "result": "AR3PO consistently outperforms GRPO and matches/surpasses DAPO across 7B, 8B, and 32B models, reducing rollout cost by up to 4.2x while maintaining comparable performance.", "conclusion": "AR3PO provides an effective solution to the vanishing advantage problem in RLVR algorithms, achieving strong performance with significantly improved sampling efficiency."}}
{"id": "2509.26495", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26495", "abs": "https://arxiv.org/abs/2509.26495", "authors": ["Jingdi Lei", "Varun Gumma", "Rishabh Bhardwaj", "Seok Min Lim", "Chuan Li", "Amir Zadeh", "Soujanya Poria"], "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!", "comment": null, "summary": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\%\n-- fall far short of reliable operational safety, while GPT models plateau in\nthe 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma\nand Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational\nsafety is a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23\\%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents.", "AI": {"tldr": "This paper introduces operational safety for LLMs - the ability to appropriately accept or refuse user queries for specific use cases. It presents OffTopicEval benchmark showing current LLMs are operationally unsafe, and proposes prompt-based steering methods that significantly improve performance.", "motivation": "Enterprises need LLM-based agents that can safely handle specific use cases by appropriately accepting or refusing queries, but current safety focus is mainly on generic harms rather than operational safety for intended purposes.", "method": "Proposed OffTopicEval evaluation suite for measuring operational safety, and introduced two prompt-based steering methods: query grounding (Q-ground) and system-prompt grounding (P-ground) to improve out-of-distribution refusal.", "result": "Evaluation of 20 LLMs from 6 families showed all models are highly operationally unsafe, with best models achieving only 77-80% safety. Prompt-based methods significantly improved performance: Q-ground by up to 23% and P-ground by up to 41% (Llama-3.3 70B).", "conclusion": "There is urgent need for operational safety interventions, and prompt-based steering shows promise as a first step toward more reliable LLM-based agents for enterprise deployment."}}
{"id": "2509.25810", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25810", "abs": "https://arxiv.org/abs/2509.25810", "authors": ["Shenao Zhang", "Donghan Yu", "Yihao Feng", "Bowen Jin", "Zhaoran Wang", "John Peebles", "Zirui Wang"], "title": "Learning to Reason as Action Abstractions with Scalable Mid-Training RL", "comment": null, "summary": "Large language models excel with reinforcement learning (RL), but fully\nunlocking this potential requires a mid-training stage. An effective\nmid-training phase should identify a compact set of useful actions and enable\nfast selection among them through online RL. We formalize this intuition by\npresenting the first theoretical result on how mid-training shapes\npost-training: it characterizes an action subspace that minimizes both the\nvalue approximation error from pruning and the RL error during subsequent\nplanning. Our analysis reveals two key determinants of mid-training\neffectiveness: pruning efficiency, which shapes the prior of the initial RL\npolicy, and its impact on RL convergence, which governs the extent to which\nthat policy can be improved via online interactions. These results suggest that\nmid-training is most effective when the decision space is compact and the\neffective horizon is short, highlighting the importance of operating in the\nspace of action abstractions rather than primitive actions. Building on these\ninsights, we propose Reasoning as Action Abstractions (RA3), a scalable\nmid-training algorithm. Specifically, we derive a sequential variational lower\nbound and optimize it by iteratively discovering temporally-consistent latent\nstructures via RL, followed by fine-tuning on the bootstrapped data.\nExperiments on code generation tasks demonstrate the effectiveness of our\napproach. Across multiple base models, RA3 improves the average performance on\nHumanEval and MBPP by 8 and 4 points over the base model and the next-token\nprediction baseline. Furthermore, RA3 achieves faster convergence and higher\nasymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and\nCodeforces.", "AI": {"tldr": "Mid-training shapes post-training by identifying compact action subspaces that minimize value approximation and RL errors. The RA3 algorithm discovers temporally-consistent latent structures via RL and fine-tuning, improving code generation performance across multiple benchmarks.", "motivation": "Large language models benefit from reinforcement learning, but fully unlocking this potential requires an effective mid-training stage to identify useful actions and enable fast selection through online RL.", "method": "Proposed Reasoning as Action Abstractions (RA3) algorithm: derives sequential variational lower bound and optimizes it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on bootstrapped data.", "result": "RA3 improves average performance on HumanEval and MBPP by 8 and 4 points over base model and next-token prediction baseline. Achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces.", "conclusion": "Mid-training is most effective when decision space is compact and effective horizon is short, highlighting importance of operating in action abstractions rather than primitive actions. RA3 demonstrates practical effectiveness in code generation tasks."}}
{"id": "2509.26506", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26506", "abs": "https://arxiv.org/abs/2509.26506", "authors": ["Yutong Dai", "Krithika Ramakrishnan", "Jing Gu", "Matthew Fernandez", "Yanqi Luo", "Viraj Prabhu", "Zhenyu Hu", "Silvio Savarese", "Caiming Xiong", "Zeyuan Chen", "Ran Xu"], "title": "SCUBA: Salesforce Computer Use Benchmark", "comment": null, "summary": "We introduce SCUBA, a benchmark designed to evaluate computer-use agents on\ncustomer relationship management (CRM) workflows within the Salesforce\nplatform. SCUBA contains 300 task instances derived from real user interviews,\nspanning three primary personas, platform administrators, sales\nrepresentatives, and service agents. The tasks test a range of\nenterprise-critical abilities, including Enterprise Software UI navigation,\ndata manipulation, workflow automation, information retrieval, and\ntroubleshooting. To ensure realism, SCUBA operates in Salesforce sandbox\nenvironments with support for parallel execution and fine-grained evaluation\nmetrics to capture milestone progress. We benchmark a diverse set of agents\nunder both zero-shot and demonstration-augmented settings. We observed huge\nperformance gaps in different agent design paradigms and gaps between the\nopen-source model and the closed-source model. In the zero-shot setting,\nopen-source model powered computer-use agents that have strong performance on\nrelated benchmarks like OSWorld only have less than 5\\% success rate on SCUBA,\nwhile methods built on closed-source models can still have up to 39% task\nsuccess rate. In the demonstration-augmented settings, task success rates can\nbe improved to 50\\% while simultaneously reducing time and costs by 13% and\n16%, respectively. These findings highlight both the challenges of enterprise\ntasks automation and the promise of agentic solutions. By offering a realistic\nbenchmark with interpretable evaluation, SCUBA aims to accelerate progress in\nbuilding reliable computer-use agents for complex business software ecosystems.", "AI": {"tldr": "SCUBA is a benchmark for evaluating computer-use agents on Salesforce CRM workflows, featuring 300 real-world tasks that test enterprise software skills like UI navigation, data manipulation, and troubleshooting.", "motivation": "To address the need for realistic evaluation of computer-use agents in enterprise software environments, particularly for CRM workflows where current benchmarks may not capture the complexity of real business tasks.", "method": "Created a benchmark with 300 task instances from real user interviews across three personas (administrators, sales reps, service agents), operating in Salesforce sandbox environments with parallel execution and fine-grained evaluation metrics.", "result": "Huge performance gaps observed: open-source models achieved <5% success rate in zero-shot settings, while closed-source models reached up to 39%. With demonstrations, success rates improved to 50% with 13% time reduction and 16% cost reduction.", "conclusion": "SCUBA highlights both the challenges of enterprise task automation and the promise of agentic solutions, providing a realistic benchmark to accelerate progress in building reliable computer-use agents for complex business software ecosystems."}}
{"id": "2509.25824", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.25824", "abs": "https://arxiv.org/abs/2509.25824", "authors": ["Jingqi Fan", "Canzhe Zhao", "Shuai Li", "Siwei Wang"], "title": "Decentralized Asynchronous Multi-player Bandits", "comment": null, "summary": "In recent years, multi-player multi-armed bandits (MP-MAB) have been\nextensively studied due to their wide applications in cognitive radio networks\nand Internet of Things systems. While most existing research on MP-MAB focuses\non synchronized settings, real-world systems are often decentralized and\nasynchronous, where players may enter or leave the system at arbitrary times,\nand do not have a global clock. This decentralized asynchronous setting\nintroduces two major challenges. First, without a global time, players cannot\nimplicitly coordinate their actions through time, making it difficult to avoid\ncollisions. Second, it is important to detect how many players are in the\nsystem, but doing so may cost a lot. In this paper, we address the challenges\nposed by such a fully asynchronous setting in a decentralized environment. We\ndevelop a novel algorithm in which players adaptively change between\nexploration and exploitation. During exploration, players uniformly pull their\narms, reducing the probability of collisions and effectively mitigating the\nfirst challenge. Meanwhile, players continue pulling arms currently exploited\nby others with a small probability, enabling them to detect when a player has\nleft, thereby addressing the second challenge. We prove that our algorithm\nachieves a regret of $\\mathcal{O}(\\sqrt{T \\log T} + {\\log T}/{\\Delta^2})$,\nwhere $\\Delta$ is the minimum expected reward gap between any two arms. To the\nbest of our knowledge, this is the first efficient MP-MAB algorithm in the\nasynchronous and decentralized environment. Extensive experiments further\nvalidate the effectiveness and robustness of our algorithm, demonstrating its\napplicability to real-world scenarios.", "AI": {"tldr": "This paper proposes a novel algorithm for decentralized asynchronous multi-player multi-armed bandits that adaptively switches between exploration and exploitation, achieving efficient collision avoidance and player detection with O(\u221aT log T + log T/\u0394\u00b2) regret.", "motivation": "Real-world systems like cognitive radio networks and IoT are often decentralized and asynchronous, but most existing MP-MAB research focuses on synchronized settings. The asynchronous setting introduces challenges: players cannot coordinate through time to avoid collisions, and detecting player count is costly.", "method": "Developed an algorithm where players adaptively change between exploration and exploitation. During exploration, players uniformly pull arms to reduce collision probability. Players also continue pulling exploited arms with small probability to detect when players leave the system.", "result": "The algorithm achieves a regret of O(\u221aT log T + log T/\u0394\u00b2), where \u0394 is the minimum expected reward gap between arms. This is the first efficient MP-MAB algorithm for asynchronous decentralized environments. Extensive experiments validate effectiveness and robustness.", "conclusion": "The proposed algorithm successfully addresses the challenges of fully asynchronous decentralized MP-MAB settings, providing the first efficient solution with theoretical guarantees and practical applicability to real-world scenarios like cognitive radio and IoT systems."}}
{"id": "2509.26534", "categories": ["cs.AI", "cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.26534", "abs": "https://arxiv.org/abs/2509.26534", "authors": ["Jovan Stojkovic", "Chaojie Zhang", "\u00cd\u00f1igo Goiri", "Ricardo Bianchini"], "title": "Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework", "comment": null, "summary": "The rapid rise of large language models (LLMs) has been driving an enormous\ndemand for AI inference infrastructure, mainly powered by high-end GPUs. While\nthese accelerators offer immense computational power, they incur high capital\nand operational costs due to frequent upgrades, dense power consumption, and\ncooling demands, making total cost of ownership (TCO) for AI datacenters a\ncritical concern for cloud providers. Unfortunately, traditional datacenter\nlifecycle management (designed for general-purpose workloads) struggles to keep\npace with AI's fast-evolving models, rising resource needs, and diverse\nhardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme\nacross three stages: building, hardware refresh, and operation. We show how\ndesign choices in power, cooling, and networking provisioning impact long-term\nTCO. We also explore refresh strategies aligned with hardware trends. Finally,\nwe use operation software optimizations to reduce cost. While these\noptimizations at each stage yield benefits, unlocking the full potential\nrequires rethinking the entire lifecycle. Thus, we present a holistic lifecycle\nmanagement framework that coordinates and co-optimizes decisions across all\nthree stages, accounting for workload dynamics, hardware evolution, and system\naging. Our system reduces the TCO by up to 40\\% over traditional approaches.\nUsing our framework we provide guidelines on how to manage AI datacenter\nlifecycle for the future.", "AI": {"tldr": "The paper presents a holistic lifecycle management framework for AI datacenters that coordinates building, hardware refresh, and operation stages to reduce total cost of ownership by up to 40% compared to traditional approaches.", "motivation": "High-end GPUs for AI inference incur high capital and operational costs, while traditional datacenter lifecycle management struggles with AI's fast-evolving models, rising resource needs, and diverse hardware profiles.", "method": "Rethinks AI datacenter lifecycle across three stages: building (design choices in power, cooling, networking), hardware refresh (strategies aligned with hardware trends), and operation (software optimizations). Presents a holistic framework that coordinates decisions across all stages.", "result": "The proposed system reduces total cost of ownership (TCO) by up to 40% over traditional approaches.", "conclusion": "Unlocking full potential requires rethinking the entire lifecycle with coordinated optimization across building, refresh, and operation stages, accounting for workload dynamics, hardware evolution, and system aging."}}
{"id": "2509.25826", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25826", "abs": "https://arxiv.org/abs/2509.25826", "authors": ["Kun Feng", "Shaocheng Lan", "Yuchen Fang", "Wenchao He", "Lintao Ma", "Xingyu Lu", "Kan Ren"], "title": "Kairos: Towards Adaptive and Generalizable Time Series Foundation Models", "comment": null, "summary": "Time series foundation models (TSFMs) have emerged as a powerful paradigm for\ntime series analysis, driven by large-scale pretraining on diverse data\ncorpora. However, time series inherently exhibit heterogeneous information\ndensity over time, influenced by system states and signal complexity,\npresenting significant modeling challenges especially in a zero-shot scenario.\nCurrent TSFMs rely on non-adaptive processing pipelines that fail to capture\nthis dynamic nature. For example, common tokenization strategies such as\nfixed-size patching enforce rigid observational granularity, limiting their\nability to adapt to varying information densities. Similarly, conventional\npositional encodings impose a uniform temporal scale, making it difficult to\nmodel diverse periodicities and trends across series. To overcome these\nlimitations, we propose Kairos, a flexible TSFM framework that integrates a\ndynamic patching tokenizer and an instance-adaptive positional embedding.\nKairos adaptively selects tokenization granularity and tailors positional\nencodings to the unique characteristics of each time series instance. Trained\non a large-scale Predictability-Stratified Time Series (PreSTS) corpus\ncomprising over 300 billion time points and adopting a multi-patch prediction\nstrategy in the inference stage, Kairos achieves superior performance with much\nfewer parameters on two common zero-shot benchmarks, GIFT-Eval and the\nTime-Series-Library benchmark, consistently outperforming established methods\nacross diverse tasks. The project page is at\nhttps://foundation-model-research.github.io/Kairos .", "AI": {"tldr": "Kairos is a flexible time series foundation model framework that addresses the challenge of heterogeneous information density in time series through dynamic patching tokenization and instance-adaptive positional embeddings, achieving superior zero-shot performance with fewer parameters.", "motivation": "Time series exhibit heterogeneous information density over time, influenced by system states and signal complexity, which current TSFMs fail to capture due to non-adaptive processing pipelines like fixed-size patching and uniform positional encodings.", "method": "Proposes Kairos framework with dynamic patching tokenizer that adaptively selects tokenization granularity and instance-adaptive positional embedding that tailors encodings to each time series's unique characteristics. Trained on Predictability-Stratified Time Series corpus with 300B+ time points using multi-patch prediction strategy.", "result": "Achieves superior performance with fewer parameters on GIFT-Eval and Time-Series-Library benchmarks, consistently outperforming established methods across diverse tasks in zero-shot scenarios.", "conclusion": "Kairos effectively addresses the limitations of current TSFMs by adapting to varying information densities in time series, demonstrating the importance of flexible tokenization and instance-specific positional encodings for improved zero-shot time series analysis."}}
{"id": "2509.26538", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26538", "abs": "https://arxiv.org/abs/2509.26538", "authors": ["Shaoyi Zheng", "Wenbo Lu", "Yuxuan Xia", "Haomin Liu", "Shengjie Wang"], "title": "HilbertA: Hilbert Attention for Image Generation with Diffusion Models", "comment": null, "summary": "Designing sparse attention for diffusion transformers requires reconciling\ntwo-dimensional spatial locality with GPU efficiency, a trade-off that current\nmethods struggle to achieve. Existing approaches enforce two-dimensional\nspatial locality but often incur uncoalesced memory access. We present\nHilbertA, a 2D-aware and GPU-efficient sparse attention mechanism. HilbertA\nreorders image tokens along Hilbert curves to achieve a contiguous memory\nlayout while preserving spatial neighborhoods, and employs a sliding schedule\nacross layers to enable long-range information propagation without repeated or\nuncoalesced memory access. To further enhance cross-tile communication and\npositional awareness, HilbertA introduces a small central shared region.\nImplemented in Triton, HilbertA delivers comparable image quality with\nsignificant acceleration over prior methods on Flux.1-dev, demonstrating the\nfeasibility of hardware-aligned two-dimensional sparse attention for\nhigh-resolution image generation. HilbertA delivers attention speedups of\n$2.3\\times$ when generating $1024\\times 1024$ images, and up to $4.17\\times$ at\n$2048\\times 2048$, while achieving image quality comparable to or surpassing\nbaselines.", "AI": {"tldr": "HilbertA is a GPU-efficient sparse attention mechanism for diffusion transformers that uses Hilbert curve reordering to maintain spatial locality while achieving 2.3-4.17x speedup in high-resolution image generation.", "motivation": "Existing sparse attention methods for diffusion transformers struggle to balance 2D spatial locality with GPU efficiency, often causing uncoalesced memory access that reduces performance.", "method": "HilbertA reorders image tokens along Hilbert curves for contiguous memory layout, uses sliding schedule across layers for long-range propagation, and adds a central shared region for cross-tile communication and positional awareness.", "result": "HilbertA achieves 2.3x speedup for 1024x1024 images and up to 4.17x for 2048x2048 images while maintaining comparable or better image quality than baselines on Flux.1-dev.", "conclusion": "HilbertA demonstrates the feasibility of hardware-aligned 2D sparse attention for high-resolution image generation, effectively reconciling spatial locality with GPU efficiency."}}
{"id": "2509.25831", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25831", "abs": "https://arxiv.org/abs/2509.25831", "authors": ["Seong-Hyeon Hwang", "Soyoung Choi", "Steven Euijong Whang"], "title": "MIDAS: Misalignment-based Data Augmentation Strategy for Imbalanced Multimodal Learning", "comment": "Accepted to NeurIPS 2025", "summary": "Multimodal models often over-rely on dominant modalities, failing to achieve\noptimal performance. While prior work focuses on modifying training objectives\nor optimization procedures, data-centric solutions remain underexplored. We\npropose MIDAS, a novel data augmentation strategy that generates misaligned\nsamples with semantically inconsistent cross-modal information, labeled using\nunimodal confidence scores to compel learning from contradictory signals.\nHowever, this confidence-based labeling can still favor the more confident\nmodality. To address this within our misaligned samples, we introduce\nweak-modality weighting, which dynamically increases the loss weight of the\nleast confident modality, thereby helping the model fully utilize weaker\nmodality. Furthermore, when misaligned features exhibit greater similarity to\nthe aligned features, these misaligned samples pose a greater challenge,\nthereby enabling the model to better distinguish between classes. To leverage\nthis, we propose hard-sample weighting, which prioritizes such semantically\nambiguous misaligned samples. Experiments on multiple multimodal classification\nbenchmarks demonstrate that MIDAS significantly outperforms related baselines\nin addressing modality imbalance.", "AI": {"tldr": "MIDAS is a data augmentation strategy that generates misaligned samples with semantically inconsistent cross-modal information to address modality imbalance in multimodal models.", "motivation": "Multimodal models often over-rely on dominant modalities and fail to achieve optimal performance, with data-centric solutions remaining underexplored.", "method": "Proposes MIDAS with misaligned sample generation using unimodal confidence scores, weak-modality weighting to increase loss weight of least confident modality, and hard-sample weighting to prioritize semantically ambiguous misaligned samples.", "result": "Experiments on multiple multimodal classification benchmarks demonstrate that MIDAS significantly outperforms related baselines in addressing modality imbalance.", "conclusion": "MIDAS effectively addresses modality imbalance through misaligned data augmentation and adaptive weighting strategies, enabling better utilization of weaker modalities."}}
{"id": "2509.26574", "categories": ["cs.AI", "cond-mat.other", "cs.CL", "hep-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.26574", "abs": "https://arxiv.org/abs/2509.26574", "authors": ["Minhui Zhu", "Minyang Tian", "Xiaocheng Yang", "Tianci Zhou", "Penghao Zhu", "Eli Chertkov", "Shengyan Liu", "Yufeng Du", "Lifan Yuan", "Ziming Ji", "Indranil Das", "Junyi Cao", "Yufeng Du", "Jinchen He", "Yifan Su", "Jiabin Yu", "Yikun Jiang", "Yujie Zhang", "Chang Liu", "Ze-Min Huang", "Weizhen Jia", "Xinan Chen", "Peixue Wu", "Yunkai Wang", "Juntai Zhou", "Yong Zhao", "Farshid Jafarpour", "Jessie Shelton", "Aaron Young", "John Bartolotta", "Wenchao Xu", "Yue Sun", "Anjun Chu", "Victor Colussi", "Chris Akers", "Nathan Brooks", "Wenbo Fu", "Christopher Wilson", "Jinchao Zhao", "Marvin Qi", "Anqi Mu", "Yubo Yang", "Allen Zang", "Yang Lyu", "Peizhi Mai", "Xuefei Guo", "Luyu Gao", "Ze Yang", "Chi Xue", "Dmytro Bandak", "Ya\u00efr Hein", "Yonatan Kahn", "Kevin Zhou", "John Drew Wilson Jarrod T. Reilly", "Di Luo", "Daniel Inafuku", "Hao Tong", "Liang Yang", "Ruixing Zhang", "Xueying Wang", "Ofir Press", "Nicolas Chia", "Eliu Huerta", "Hao Peng"], "title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark", "comment": "39 pages, 6 figures, 6 tables", "summary": "While large language models (LLMs) with reasoning capabilities are\nprogressing rapidly on high-school math competitions and coding, can they\nreason effectively through complex, open-ended challenges found in frontier\nphysics research? And crucially, what kinds of reasoning tasks do physicists\nwant LLMs to assist with? To address these questions, we present the CritPt\n(Complex Research using Integrated Thinking - Physics Test, pronounced\n\"critical point\"), the first benchmark designed to test LLMs on unpublished,\nresearch-level reasoning tasks that broadly covers modern physics research\nareas, including condensed matter, quantum physics, atomic, molecular & optical\nphysics, astrophysics, high energy physics, mathematical physics, statistical\nphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.\nCritPt consists of 71 composite research challenges designed to simulate\nfull-scale research projects at the entry level, which are also decomposed to\n190 simpler checkpoint tasks for more fine-grained insights. All problems are\nnewly created by 50+ active physics researchers based on their own research.\nEvery problem is hand-curated to admit a guess-resistant and machine-verifiable\nanswer and is evaluated by an automated grading pipeline heavily customized for\nadvanced physics-specific output formats. We find that while current\nstate-of-the-art LLMs show early promise on isolated checkpoints, they remain\nfar from being able to reliably solve full research-scale challenges: the best\naverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),\nmoderately rising to around 10% when equipped with coding tools. Through the\nrealistic yet standardized evaluation offered by CritPt, we highlight a large\ndisconnect between current model capabilities and realistic physics research\ndemands, offering a foundation to guide the development of scientifically\ngrounded AI tools.", "AI": {"tldr": "CritPt is the first benchmark testing LLMs on unpublished research-level physics reasoning tasks across multiple physics domains, showing current models perform poorly (4-10% accuracy) on full research challenges despite some promise on simpler checkpoints.", "motivation": "To assess if LLMs can effectively reason through complex, open-ended challenges in frontier physics research and understand what reasoning tasks physicists actually want AI assistance with.", "method": "Created CritPt benchmark with 71 composite research challenges simulating full-scale projects, decomposed into 190 simpler checkpoints. Problems were newly created by 50+ active physics researchers, hand-curated to be guess-resistant with machine-verifiable answers, using automated grading pipeline for physics-specific outputs.", "result": "Current state-of-the-art LLMs show early promise on isolated checkpoints but perform poorly on full research challenges: best base model (GPT-5) achieved only 4.0% average accuracy, rising to around 10% with coding tools.", "conclusion": "There's a large disconnect between current LLM capabilities and realistic physics research demands, highlighting the need for scientifically grounded AI tool development."}}
{"id": "2509.25837", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25837", "abs": "https://arxiv.org/abs/2509.25837", "authors": ["Yeongmin Kim", "Donghyeok Shin", "Mina Kang", "Byeonghu Na", "Il-Chul Moon"], "title": "Distillation of Large Language Models via Concrete Score Matching", "comment": null, "summary": "Large language models (LLMs) deliver remarkable performance but are costly to\ndeploy, motivating knowledge distillation (KD) for efficient inference.\nExisting KD objectives typically match student and teacher probabilities via\nsoftmax, which blurs valuable logit information. While direct logit\ndistillation (DLD) mitigates softmax smoothing, it fails to account for logit\nshift invariance, thereby restricting the solution space. We propose Concrete\nScore Distillation (CSD), a discrete score-matching objective that overcomes\nboth softmax-induced smoothing and restrictions on the optimal solution set. We\nresolve the training instability and quadratic complexity of discrete\nscore-matching in autoregressive LLMs, and the resulting CSD objective aligns\nrelative logit differences across all vocabulary pairs between student and\nteacher with flexible weighting. We provide both mode-seeking and mode-covering\ninstances within our framework and evaluate CSD on task-agnostic\ninstruction-following and task-specific distillation using GPT-2-1.5B,\nOpenLLaMA-7B, and GEMMA-7B-IT. Experiments show that CSD consistently surpasses\nrecent KD objectives, achieves favorable fidelity-diversity trade-offs, and\nyields complementary gains when combined with on-policy techniques,\ndemonstrating its scalability and effectiveness for LLM distillation.", "AI": {"tldr": "CSD is a novel knowledge distillation method that overcomes limitations of softmax-based objectives and direct logit distillation by using discrete score matching to align relative logit differences between student and teacher models.", "motivation": "Existing KD methods either blur logit information through softmax or fail to account for logit shift invariance, restricting the solution space for efficient LLM deployment.", "method": "Proposes Concrete Score Distillation (CSD) - a discrete score-matching objective that resolves training instability and quadratic complexity in autoregressive LLMs, aligning relative logit differences across all vocabulary pairs with flexible weighting.", "result": "CSD consistently surpasses recent KD objectives, achieves favorable fidelity-diversity trade-offs, and yields complementary gains when combined with on-policy techniques across GPT-2-1.5B, OpenLLaMA-7B, and GEMMA-7B-IT models.", "conclusion": "CSD demonstrates scalability and effectiveness for LLM distillation by overcoming softmax-induced smoothing and solution space restrictions while maintaining stable training."}}
{"id": "2509.26584", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.26584", "abs": "https://arxiv.org/abs/2509.26584", "authors": ["Matheus Vinicius da Silva de Oliveira", "Jonathan de Andrade Silva", "Awdren de Lima Fontao"], "title": "Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models", "comment": null, "summary": "Large Language Models (LLMs) are widely used across multiple domains but\ncontinue to raise concerns regarding security and fairness. Beyond known attack\nvectors such as data poisoning and prompt injection, LLMs are also vulnerable\nto fairness bugs. These refer to unintended behaviors influenced by sensitive\ndemographic cues (e.g., race or sexual orientation) that should not affect\noutcomes. Another key issue is hallucination, where models generate plausible\nyet false information. Retrieval-Augmented Generation (RAG) has emerged as a\nstrategy to mitigate hallucinations by combining external retrieval with text\ngeneration. However, its adoption raises new fairness concerns, as the\nretrieved content itself may surface or amplify bias. This study conducts\nfairness testing through metamorphic testing (MT), introducing controlled\ndemographic perturbations in prompts to assess fairness in sentiment analysis\nperformed by three Small Language Models (SLMs) hosted on HuggingFace\n(Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B),\neach integrated into a RAG pipeline. Results show that minor demographic\nvariations can break up to one third of metamorphic relations (MRs). A detailed\nanalysis of these failures reveals a consistent bias hierarchy, with\nperturbations involving racial cues being the predominant cause of the\nviolations. In addition to offering a comparative evaluation, this work\nreinforces that the retrieval component in RAG must be carefully curated to\nprevent bias amplification. The findings serve as a practical alert for\ndevelopers, testers and small organizations aiming to adopt accessible SLMs\nwithout compromising fairness or reliability.", "AI": {"tldr": "This paper conducts fairness testing on Small Language Models (SLMs) integrated with RAG pipelines, finding that minor demographic variations can break up to one third of metamorphic relations, with racial cues being the predominant cause of fairness violations.", "motivation": "LLMs raise concerns about security and fairness, including fairness bugs influenced by sensitive demographic cues and hallucination issues. RAG helps mitigate hallucinations but introduces new fairness concerns as retrieved content may amplify bias.", "method": "The study uses metamorphic testing (MT) with controlled demographic perturbations in prompts to assess fairness in sentiment analysis performed by three SLMs (Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B) integrated into RAG pipelines.", "result": "Results show minor demographic variations can break up to one third of metamorphic relations. A consistent bias hierarchy was observed, with racial cues being the predominant cause of fairness violations.", "conclusion": "The retrieval component in RAG must be carefully curated to prevent bias amplification. This serves as a practical alert for developers and organizations adopting accessible SLMs to maintain fairness and reliability."}}
{"id": "2509.25841", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25841", "abs": "https://arxiv.org/abs/2509.25841", "authors": ["Suping Xu", "Chuyi Dai", "Ye Liu", "Lin Shang", "Xibei Yang", "Witold Pedrycz"], "title": "S$^2$FS: Spatially-Aware Separability-Driven Feature Selection in Fuzzy Decision Systems", "comment": null, "summary": "Feature selection is crucial for fuzzy decision systems (FDSs), as it\nidentifies informative features and eliminates rule redundancy, thereby\nenhancing predictive performance and interpretability. Most existing methods\neither fail to directly align evaluation criteria with learning performance or\nrely solely on non-directional Euclidean distances to capture relationships\namong decision classes, which limits their ability to clarify decision\nboundaries. However, the spatial distribution of instances has a potential\nimpact on the clarity of such boundaries. Motivated by this, we propose\nSpatially-aware Separability-driven Feature Selection (S$^2$FS), a novel\nframework for FDSs guided by a spatially-aware separability criterion. This\ncriterion jointly considers within-class compactness and between-class\nseparation by integrating scalar-distances with spatial directional\ninformation, providing a more comprehensive characterization of class\nstructures. S$^2$FS employs a forward greedy strategy to iteratively select the\nmost discriminative features. Extensive experiments on ten real-world datasets\ndemonstrate that S$^2$FS consistently outperforms eight state-of-the-art\nfeature selection algorithms in both classification accuracy and clustering\nperformance, while feature visualizations further confirm the interpretability\nof the selected features.", "AI": {"tldr": "S^2FS is a novel feature selection framework for fuzzy decision systems that uses spatially-aware separability criteria to improve classification accuracy and interpretability by considering both within-class compactness and between-class separation with spatial directional information.", "motivation": "Existing feature selection methods for fuzzy decision systems either don't directly align with learning performance or rely only on non-directional Euclidean distances, limiting their ability to clarify decision boundaries despite the importance of spatial distribution of instances.", "method": "Proposed S^2FS framework uses a spatially-aware separability criterion that integrates scalar-distances with spatial directional information, and employs a forward greedy strategy to iteratively select the most discriminative features.", "result": "Extensive experiments on ten real-world datasets show S^2FS consistently outperforms eight state-of-the-art feature selection algorithms in both classification accuracy and clustering performance, with feature visualizations confirming interpretability.", "conclusion": "S^2FS effectively enhances feature selection for fuzzy decision systems by incorporating spatial awareness and directional information, leading to improved performance and interpretability compared to existing methods."}}
{"id": "2509.26605", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26605", "abs": "https://arxiv.org/abs/2509.26605", "authors": ["Ma\u00ebl Macuglia", "Paul Friedrich", "Giorgia Ramponi"], "title": "Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning", "comment": "85 pages (11 + references and appendix), 9 figures", "summary": "Deploying reinforcement learning (RL) in robotics, industry, and health care\nis blocked by two obstacles: the difficulty of specifying accurate rewards and\nthe risk of unsafe, data-hungry exploration. We address this by proposing a\ntwo-stage framework that first learns a safe initial policy from a reward-free\ndataset of expert demonstrations, then fine-tunes it online using\npreference-based human feedback. We provide the first principled analysis of\nthis offline-to-online approach and introduce BRIDGE, a unified algorithm that\nintegrates both signals via an uncertainty-weighted objective. We derive regret\nbounds that shrink with the number of offline demonstrations, explicitly\nconnecting the quantity of offline data to online sample efficiency. We\nvalidate BRIDGE in discrete and continuous control MuJoCo environments, showing\nit achieves lower regret than both standalone behavioral cloning and online\npreference-based RL. Our work establishes a theoretical foundation for\ndesigning more sample-efficient interactive agents.", "AI": {"tldr": "BRIDGE is a two-stage RL framework that first learns safe policies from offline expert demonstrations, then fine-tunes online using human preferences, achieving better sample efficiency than standalone methods.", "motivation": "Address two key obstacles in RL deployment: difficulty specifying accurate rewards and unsafe data-hungry exploration, by leveraging both offline demonstrations and online human feedback.", "method": "Two-stage approach: 1) Learn safe initial policy from reward-free expert demonstrations, 2) Fine-tune online using preference-based human feedback via BRIDGE algorithm with uncertainty-weighted objective.", "result": "BRIDGE achieves lower regret than standalone behavioral cloning and online preference-based RL in discrete and continuous control MuJoCo environments, with regret bounds shrinking with offline data quantity.", "conclusion": "Establishes theoretical foundation for designing more sample-efficient interactive agents by integrating offline demonstrations with online human feedback."}}
{"id": "2509.25849", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25849", "abs": "https://arxiv.org/abs/2509.25849", "authors": ["Ziniu Li", "Congliang Chen", "Tianyun Yang", "Tian Ding", "Ruoyu Sun", "Ge Zhang", "Wenhao Huang", "Zhi-Quan Luo"], "title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation", "comment": null, "summary": "Large Language Models (LLMs) can self-improve through reinforcement learning,\nwhere they generate trajectories to explore and discover better solutions.\nHowever, this exploration process is computationally expensive, often forcing\ncurrent methods to assign limited exploration budgets to each task. This\nuniform allocation creates problematic edge cases: easy tasks consistently\nsucceed while difficult tasks consistently fail, both producing zero gradients\nduring training updates for the widely used Group Relative Policy Optimization\n(GRPO). We address this problem from the lens of exploration budget allocation.\nViewing each task's exploration as an \"item\" with a distinct \"value\" and\n\"cost\", we establish a connection to the classical knapsack problem. This\nformulation allows us to derive an optimal assignment rule that adaptively\ndistributes resources based on the model's current learning status. When\napplied to GRPO, our method increases the effective ratio of non-zero policy\ngradients by 20-40% during training. Acting as a computational \"free lunch\",\nour approach could reallocate exploration budgets from tasks where learning is\nsaturated to those where it is most impactful. This enables significantly\nlarger budgets (e.g., 93 rollouts) for especially challenging problems, which\nwould be computationally prohibitive under a uniform allocation. These\nimprovements translate to meaningful gains on mathematical reasoning\nbenchmarks, with average improvements of 2-4 points and peak gains of 9 points\non specific tasks. Notably, achieving comparable performance with traditional\nhomogeneous allocation would require about 2x the computational resources.", "AI": {"tldr": "The paper proposes an adaptive exploration budget allocation method for LLM self-improvement, framing it as a knapsack problem to optimally distribute resources based on learning status, increasing non-zero policy gradients by 20-40% and achieving 2-4 point gains on math reasoning benchmarks.", "motivation": "Current uniform exploration budget allocation in LLM self-improvement creates edge cases where easy tasks consistently succeed and difficult tasks consistently fail, both producing zero gradients in GRPO training, leading to inefficient learning.", "method": "Formulate exploration budget allocation as a knapsack problem where each task has distinct 'value' and 'cost', then derive an optimal assignment rule that adaptively distributes resources based on the model's current learning status.", "result": "Method increases effective ratio of non-zero policy gradients by 20-40%, enables significantly larger budgets (e.g., 93 rollouts) for challenging problems, and achieves 2-4 point average improvements on math reasoning benchmarks with peak gains of 9 points on specific tasks.", "conclusion": "The adaptive budget allocation acts as a computational 'free lunch', enabling comparable performance with about 2x fewer computational resources compared to traditional homogeneous allocation."}}
{"id": "2509.26627", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.26627", "abs": "https://arxiv.org/abs/2509.26627", "authors": ["Yuyang Liu", "Chuan Wen", "Yihang Hu", "Dinesh Jayaraman", "Yang Gao"], "title": "TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance", "comment": null, "summary": "Designing dense rewards is crucial for reinforcement learning (RL), yet in\nrobotics it often demands extensive manual effort and lacks scalability. One\npromising solution is to view task progress as a dense reward signal, as it\nquantifies the degree to which actions advance the system toward task\ncompletion over time. We present TimeRewarder, a simple yet effective reward\nlearning method that derives progress estimation signals from passive videos,\nincluding robot demonstrations and human videos, by modeling temporal distances\nbetween frame pairs. We then demonstrate how TimeRewarder can supply step-wise\nproxy rewards to guide reinforcement learning. In our comprehensive experiments\non ten challenging Meta-World tasks, we show that TimeRewarder dramatically\nimproves RL for sparse-reward tasks, achieving nearly perfect success in 9/10\ntasks with only 200,000 interactions per task with the environment. This\napproach outperformed previous methods and even the manually designed\nenvironment dense reward on both the final success rate and sample efficiency.\nMoreover, we show that TimeRewarder pretraining can exploit real-world human\nvideos, highlighting its potential as a scalable approach path to rich reward\nsignals from diverse video sources.", "AI": {"tldr": "TimeRewarder is a reward learning method that estimates task progress from passive videos by modeling temporal distances between frames, providing dense rewards for RL that outperform manual reward design.", "motivation": "Manual dense reward design in robotics is labor-intensive and doesn't scale well. Task progress offers a natural dense reward signal but is challenging to estimate automatically.", "method": "TimeRewarder learns progress estimation from passive videos (robot demos and human videos) by modeling temporal distances between frame pairs, then uses these as step-wise proxy rewards for RL.", "result": "Achieved nearly perfect success in 9/10 Meta-World tasks with only 200k interactions per task, outperforming previous methods and manual dense rewards in both success rate and sample efficiency.", "conclusion": "TimeRewarder provides a scalable approach to derive rich reward signals from diverse video sources, including real-world human videos, significantly improving RL performance for sparse-reward tasks."}}
{"id": "2509.25850", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25850", "abs": "https://arxiv.org/abs/2509.25850", "authors": ["Animesh Jha", "Harshit Gupta", "Ananjan Nandi"], "title": "RL-Guided Data Selection for Language Model Finetuning", "comment": "To appear in NeurIPS 2025 Constrained Optimization for ML Workshop", "summary": "Data selection for finetuning Large Language Models (LLMs) can be framed as a\nbudget-constrained optimization problem: maximizing a model's downstream\nperformance under a strict training data budget. Solving this problem is\ngenerally intractable, and existing approximate approaches are\npretraining-oriented and transfer poorly to the fine-tuning setting. We\nreformulate this problem as a tractable Markov Decision Process (MDP) and train\nagents using various Reinforcement Learning (RL) methods to learn optimal data\nselection policies, guided by an efficient, proxy-model-based reward signal.\nAcross four datasets, training on a $5\\%$ subset selected by our approach\nmatches or outperforms fine-tuning on the full dataset by up to $10.8$ accuracy\npoints, while cutting wall-clock training time by up to $2 \\times$,\nhighlighting the promise of RL-guided data selection.", "AI": {"tldr": "This paper proposes using Reinforcement Learning to select optimal data subsets for fine-tuning LLMs, achieving better performance with only 5% of data while reducing training time by 2x.", "motivation": "Existing data selection methods for LLM fine-tuning are pretraining-oriented and don't transfer well, creating a need for more effective budget-constrained optimization approaches.", "method": "Reformulate data selection as a Markov Decision Process and train RL agents using proxy-model-based reward signals to learn optimal data selection policies.", "result": "Across four datasets, using only 5% of data selected by the RL approach matches or outperforms full dataset fine-tuning by up to 10.8 accuracy points, with 2x faster training time.", "conclusion": "RL-guided data selection shows significant promise for efficient LLM fine-tuning, enabling better performance with substantially less data and computational resources."}}
{"id": "2509.26632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26632", "abs": "https://arxiv.org/abs/2509.26632", "authors": ["Craig Greenberg", "Patrick Hall", "Theodore Jensen", "Kristen Greene", "Razvan Amironesei"], "title": "Branching Out: Broadening AI Measurement and Evaluation with Measurement Trees", "comment": null, "summary": "This paper introduces \\textit{measurement trees}, a novel class of metrics\ndesigned to combine various constructs into an interpretable multi-level\nrepresentation of a measurand. Unlike conventional metrics that yield single\nvalues, vectors, surfaces, or categories, measurement trees produce a\nhierarchical directed graph in which each node summarizes its children through\nuser-defined aggregation methods. In response to recent calls to expand the\nscope of AI system evaluation, measurement trees enhance metric transparency\nand facilitate the integration of heterogeneous evidence, including, e.g.,\nagentic, business, energy-efficiency, sociotechnical, or security signals. We\npresent definitions and examples, demonstrate practical utility through a\nlarge-scale measurement exercise, and provide accompanying open-source Python\ncode. By operationalizing a transparent approach to measurement of complex\nconstructs, this work offers a principled foundation for broader and more\ninterpretable AI evaluation.", "AI": {"tldr": "This paper introduces measurement trees, a novel hierarchical metric framework that creates interpretable multi-level representations of measurands using user-defined aggregation methods, enhancing transparency in AI system evaluation.", "motivation": "To address the need for expanded AI system evaluation scope and enhance metric transparency by enabling integration of heterogeneous evidence like agentic, business, energy-efficiency, sociotechnical, and security signals.", "method": "Proposes measurement trees as hierarchical directed graphs where each node summarizes its children through user-defined aggregation methods, providing definitions, examples, and practical implementation through a large-scale measurement exercise with open-source Python code.", "result": "The measurement trees framework successfully operationalizes transparent measurement of complex constructs, facilitating broader and more interpretable AI evaluation through hierarchical aggregation of diverse evidence types.", "conclusion": "Measurement trees provide a principled foundation for comprehensive AI evaluation by offering a transparent, hierarchical approach to measuring complex constructs that integrates heterogeneous evidence and enhances interpretability."}}
{"id": "2509.25876", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25876", "abs": "https://arxiv.org/abs/2509.25876", "authors": ["Xinyu Zhang", "Aishik Deb", "Klaus Mueller"], "title": "Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space", "comment": "16 pages; 7 figures", "summary": "Policy-gradient methods such as Proximal Policy Optimization (PPO) are\ntypically updated along a single stochastic gradient direction, leaving the\nrich local structure of the parameter space unexplored. Previous work has shown\nthat the surrogate gradient is often poorly correlated with the true reward\nlandscape. Building on this insight, we visualize the parameter space spanned\nby policy checkpoints within an iteration and reveal that higher performing\nsolutions often lie in nearby unexplored regions. To exploit this opportunity,\nwe introduce ExploRLer, a pluggable pipeline that seamlessly integrates with\non-policy algorithms such as PPO and TRPO, systematically probing the\nunexplored neighborhoods of surrogate on-policy gradient updates. Without\nincreasing the number of gradient updates, ExploRLer achieves significant\nimprovements over baselines in complex continuous control environments. Our\nresults demonstrate that iteration-level exploration provides a practical and\neffective way to strengthen on-policy reinforcement learning and offer a fresh\nperspective on the limitations of the surrogate objective.", "AI": {"tldr": "ExploRLer is a pluggable pipeline that enhances on-policy RL algorithms like PPO by systematically exploring nearby parameter neighborhoods beyond the single surrogate gradient direction, achieving better performance without additional gradient updates.", "motivation": "Policy-gradient methods like PPO use single stochastic gradient directions, leaving rich local parameter structure unexplored. The surrogate gradient often poorly correlates with the true reward landscape, and higher-performing solutions exist in nearby unexplored regions.", "method": "Visualize policy checkpoint parameter spaces and introduce ExploRLer - a pluggable pipeline that integrates with on-policy algorithms to systematically probe unexplored neighborhoods of surrogate on-policy gradient updates.", "result": "ExploRLer achieves significant improvements over baselines in complex continuous control environments without increasing the number of gradient updates.", "conclusion": "Iteration-level exploration provides a practical and effective way to strengthen on-policy reinforcement learning and offers fresh perspective on surrogate objective limitations."}}
{"id": "2505.23495", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2505.23495", "abs": "https://arxiv.org/abs/2505.23495", "authors": ["Liangliang Zhang", "Zhuorui Jiang", "Hongliang Chi", "Haoyang Chen", "Mohammed Elkoumy", "Fali Wang", "Qiong Wu", "Zhengyi Zhou", "Shirui Pan", "Suhang Wang", "Yao Ma"], "title": "Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking", "comment": "9 pages", "summary": "Knowledge Graph Question Answering (KGQA) systems rely on high-quality\nbenchmarks to evaluate complex multi-hop reasoning. However, despite their\nwidespread use, popular datasets such as WebQSP and CWQ suffer from critical\nquality issues, including inaccurate or incomplete ground-truth annotations,\npoorly constructed questions that are ambiguous, trivial, or unanswerable, and\noutdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA\ndatasets, including WebQSP and CWQ, we find that the average factual\ncorrectness rate is only 57 %. To address these issues, we introduce KGQAGen,\nan LLM-in-the-loop framework that systematically resolves these pitfalls.\nKGQAGen combines structured knowledge grounding, LLM-guided generation, and\nsymbolic verification to produce challenging and verifiable QA instances. Using\nKGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in\nWikidata, and evaluate a diverse set of KG-RAG models. Experimental results\ndemonstrate that even state-of-the-art systems struggle on this benchmark,\nhighlighting its ability to expose limitations of existing models. Our findings\nadvocate for more rigorous benchmark construction and position KGQAGen as a\nscalable framework for advancing KGQA evaluation.", "AI": {"tldr": "KGQAGen is an LLM-in-the-loop framework that addresses quality issues in existing KGQA benchmarks by generating challenging and verifiable QA instances through structured knowledge grounding, LLM-guided generation, and symbolic verification.", "motivation": "Popular KGQA datasets like WebQSP and CWQ suffer from critical quality issues including inaccurate ground-truth annotations, ambiguous questions, and outdated knowledge, with manual audit showing only 57% factual correctness rate.", "method": "KGQAGen combines structured knowledge grounding, LLM-guided generation, and symbolic verification to systematically produce high-quality QA instances. The framework constructs KGQAGen-10k, a ten-thousand scale benchmark grounded in Wikidata.", "result": "Experimental evaluation shows that even state-of-the-art KG-RAG models struggle on the KGQAGen-10k benchmark, demonstrating its ability to expose limitations of existing models.", "conclusion": "The findings advocate for more rigorous benchmark construction and position KGQAGen as a scalable framework for advancing KGQA evaluation by addressing quality issues in existing datasets."}}
{"id": "2509.25906", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25906", "abs": "https://arxiv.org/abs/2509.25906", "authors": ["Yiwei Li", "Shuai Wang", "Zhuojun Tian", "Xiuhua Wang", "Shijian Su"], "title": "Federated Learning with Enhanced Privacy via Model Splitting and Random Client Participation", "comment": "29 pages, 6 figures, submitted for peer review", "summary": "Federated Learning (FL) often adopts differential privacy (DP) to protect\nclient data, but the added noise required for privacy guarantees can\nsubstantially degrade model accuracy. To resolve this challenge, we propose\nmodel-splitting privacy-amplified federated learning (MS-PAFL), a novel\nframework that combines structural model splitting with statistical privacy\namplification. In this framework, each client's model is partitioned into a\nprivate submodel, retained locally, and a public submodel, shared for global\naggregation. The calibrated Gaussian noise is injected only into the public\nsubmodel, thereby confining its adverse impact while preserving the utility of\nthe local model. We further present a rigorous theoretical analysis that\ncharacterizes the joint privacy amplification achieved through random client\nparticipation and local data subsampling under this architecture. The analysis\nprovides tight bounds on both single-round and total privacy loss,\ndemonstrating that MS-PAFL significantly reduces the noise necessary to satisfy\na target privacy protection level. Extensive experiments validate our\ntheoretical findings, showing that MS-PAFL consistently attains a superior\nprivacy-utility trade-off and enables the training of highly accurate models\nunder strong privacy guarantees.", "AI": {"tldr": "MS-PAFL is a federated learning framework that splits models into private and public parts, injecting noise only into public submodels to reduce accuracy degradation while maintaining strong privacy through structural splitting and statistical amplification.", "motivation": "To address the significant model accuracy degradation caused by differential privacy noise in federated learning while maintaining strong privacy protection.", "method": "Model-splitting privacy-amplified federated learning (MS-PAFL) partitions client models into private submodels (kept locally) and public submodels (shared globally), with calibrated Gaussian noise injected only into public submodels, combined with privacy amplification through random client participation and data subsampling.", "result": "Theoretical analysis shows tight bounds on privacy loss, significantly reducing required noise for target privacy levels. Experiments demonstrate superior privacy-utility trade-off and highly accurate models under strong privacy guarantees.", "conclusion": "MS-PAFL effectively resolves the privacy-utility trade-off in differentially private federated learning by combining structural model splitting with statistical privacy amplification, enabling accurate model training with strong privacy protection."}}
{"id": "2509.25914", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25914", "abs": "https://arxiv.org/abs/2509.25914", "authors": ["Yihang Lu", "Xianwei Meng", "Enhong Chen"], "title": "ReNF: Rethinking the Design Space of Neural Long-Term Time Series Forecasters", "comment": null, "summary": "Neural Forecasters (NFs) are a cornerstone of Long-term Time Series\nForecasting (LTSF). However, progress has been hampered by an overemphasis on\narchitectural complexity at the expense of fundamental forecasting principles.\nIn this work, we return to first principles to redesign the LTSF paradigm. We\nbegin by introducing a Multiple Neural Forecasting Theorem that provides a\ntheoretical basis for our approach. We propose Boosted Direct Output (BDO), a\nnovel forecasting strategy that synergistically combines the advantages of both\nAuto-Regressive (AR) and Direct Output (DO). In addition, we stabilize the\nlearning process by smoothly tracking the model's parameters. Extensive\nexperiments show that these principled improvements enable a simple MLP to\nachieve state-of-the-art performance, outperforming recent, complex models in\nnearly all cases, without any specific considerations in the area. Finally, we\nempirically verify our theorem, establishing a dynamic performance bound and\nidentifying promising directions for future research. The code for review is\navailable at: .", "AI": {"tldr": "This paper proposes a principled approach to Long-term Time Series Forecasting by introducing the Boosted Direct Output (BDO) strategy that combines Auto-Regressive and Direct Output methods, enabling simple MLPs to achieve state-of-the-art performance.", "motivation": "Current neural forecasting research overemphasizes architectural complexity while neglecting fundamental forecasting principles, hampering progress in LTSF.", "method": "Proposes Boosted Direct Output (BDO) strategy that synergistically combines AR and DO approaches, with smooth parameter tracking for learning stability.", "result": "A simple MLP with BDO achieves state-of-the-art performance, outperforming complex models in nearly all cases without domain-specific considerations.", "conclusion": "The work establishes a theoretical foundation with Multiple Neural Forecasting Theorem, provides empirical verification, and identifies promising future research directions."}}
{"id": "2509.25933", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.25933", "abs": "https://arxiv.org/abs/2509.25933", "authors": ["Sven Br\u00e4ndle", "Till Aczel", "Andreas Plesner", "Roger Wattenhofer"], "title": "From MNIST to ImageNet: Understanding the Scalability Boundaries of Differentiable Logic Gate Networks", "comment": null, "summary": "Differentiable Logic Gate Networks (DLGNs) are a very fast and\nenergy-efficient alternative to conventional feed-forward networks. With\nlearnable combinations of logical gates, DLGNs enable fast inference by\nhardware-friendly execution. Since the concept of DLGNs has only recently\ngained attention, these networks are still in their developmental infancy,\nincluding the design and scalability of their output layer. To date, this\narchitecture has primarily been tested on datasets with up to ten classes.\n  This work examines the behavior of DLGNs on large multi-class datasets. We\ninvestigate its general expressiveness, its scalability, and evaluate\nalternative output strategies. Using both synthetic and real-world datasets, we\nprovide key insights into the importance of temperature tuning and its impact\non output layer performance. We evaluate conditions under which the Group-Sum\nlayer performs well and how it can be applied to large-scale classification of\nup to 2000 classes.", "AI": {"tldr": "DLGNs are fast and energy-efficient neural networks using logical gates, but their scalability to large multi-class datasets (up to 2000 classes) needs investigation. This paper studies DLGNs' expressiveness, scalability, and alternative output strategies, focusing on temperature tuning and Group-Sum layer performance.", "motivation": "DLGNs are a promising alternative to conventional neural networks due to their speed and energy efficiency, but they have only been tested on small datasets (up to 10 classes). This work aims to understand their behavior and scalability on large multi-class datasets.", "method": "The study investigates DLGNs' general expressiveness and scalability using both synthetic and real-world datasets. It evaluates alternative output strategies, with particular focus on temperature tuning and the performance of Group-Sum layers in large-scale classification.", "result": "The research provides key insights into the importance of temperature tuning and its impact on output layer performance. It identifies conditions under which the Group-Sum layer performs well and demonstrates its applicability to large-scale classification tasks with up to 2000 classes.", "conclusion": "DLGNs can be effectively scaled to handle large multi-class datasets through proper temperature tuning and optimized output layer strategies like the Group-Sum layer, making them viable for practical applications requiring fast and energy-efficient inference."}}
{"id": "2509.25955", "categories": ["cs.LG", "cs.AI", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2509.25955", "abs": "https://arxiv.org/abs/2509.25955", "authors": ["Mason Minot", "Gisbert Schneider"], "title": "AIM: Adaptive Intervention for Deep Multi-task Learning of Molecular Properties", "comment": "13 pages, 3 figures, 9 tables", "summary": "Simultaneously optimizing multiple, frequently conflicting, molecular\nproperties is a key bottleneck in the development of novel therapeutics.\nAlthough a promising approach, the efficacy of multi-task learning is often\ncompromised by destructive gradient interference, especially in the data-scarce\nregimes common to drug discovery. To address this, we propose AIM, an\noptimization framework that learns a dynamic policy to mediate gradient\nconflicts. The policy is trained jointly with the main network using a novel\naugmented objective composed of dense, differentiable regularizers. This\nobjective guides the policy to produce updates that are geometrically stable\nand dynamically efficient, prioritizing progress on the most challenging tasks.\nWe demonstrate that AIM achieves statistically significant improvements over\nmulti-task baselines on subsets of the QM9 and targeted protein degraders\nbenchmarks, with its advantage being most pronounced in data-scarce regimes.\nBeyond performance, AIM's key contribution is its interpretability; the learned\npolicy matrix serves as a diagnostic tool for analyzing inter-task\nrelationships. This combination of data-efficient performance and diagnostic\ninsight highlights the potential of adaptive optimizers to accelerate\nscientific discovery by creating more robust and insightful models for\nmulti-property molecular design.", "AI": {"tldr": "AIM is an optimization framework that learns a dynamic policy to mediate gradient conflicts in multi-task learning for molecular property optimization, achieving better performance in data-scarce regimes while providing interpretable insights.", "motivation": "Multi-task learning for molecular property optimization faces destructive gradient interference, especially in data-scarce drug discovery scenarios, which compromises its efficacy.", "method": "AIM learns a dynamic policy jointly with the main network using an augmented objective with dense, differentiable regularizers that guide the policy to produce geometrically stable and dynamically efficient updates.", "result": "AIM achieves statistically significant improvements over multi-task baselines on QM9 and targeted protein degraders benchmarks, with advantages most pronounced in data-scarce regimes.", "conclusion": "AIM combines data-efficient performance with interpretability through learned policy matrices, highlighting the potential of adaptive optimizers for robust and insightful multi-property molecular design."}}
{"id": "2509.25964", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25964", "abs": "https://arxiv.org/abs/2509.25964", "authors": ["Deniz Soysal", "Xabier Garc\u00eda-Andrade", "Laura E. Rodriguez", "Pablo Sobron", "Laura M. Barge", "Renaud Detry"], "title": "Reevaluating Convolutional Neural Networks for Spectral Analysis: A Focus on Raman Spectroscopy", "comment": null, "summary": "Autonomous Raman instruments on Mars rovers, deep-sea landers, and field\nrobots must interpret raw spectra distorted by fluorescence baselines, peak\nshifts, and limited ground-truth labels. Using curated subsets of the RRUFF\ndatabase, we evaluate one-dimensional convolutional neural networks (CNNs) and\nreport four advances: (i) Baseline-independent classification: compact CNNs\nsurpass $k$-nearest-neighbors and support-vector machines on handcrafted\nfeatures, removing background-correction and peak-picking stages while ensuring\nreproducibility through released data splits and scripts. (ii)\nPooling-controlled robustness: tuning a single pooling parameter accommodates\nRaman shifts up to $30 \\,\\mathrm{cm}^{-1}$, balancing translational invariance\nwith spectral resolution. (iii) Label-efficient learning: semi-supervised\ngenerative adversarial networks and contrastive pretraining raise accuracy by\nup to $11\\%$ with only $10\\%$ labels, valuable for autonomous deployments with\nscarce annotation. (iv) Constant-time adaptation: freezing the CNN backbone and\nretraining only the softmax layer transfers models to unseen minerals at\n$\\mathcal{O}(1)$ cost, outperforming Siamese networks on resource-limited\nprocessors. This workflow, which involves training on raw spectra, tuning\npooling, adding semi-supervision when labels are scarce, and fine-tuning\nlightly for new targets, provides a practical path toward robust, low-footprint\nRaman classification in autonomous exploration.", "AI": {"tldr": "This paper presents a workflow for robust Raman spectroscopy classification using 1D CNNs, achieving baseline-independent classification, pooling-controlled robustness, label-efficient learning, and constant-time adaptation for autonomous exploration systems.", "motivation": "Autonomous Raman instruments on Mars rovers, deep-sea landers, and field robots need to interpret raw spectra distorted by fluorescence baselines, peak shifts, and limited ground-truth labels, requiring robust and efficient classification methods.", "method": "Used 1D convolutional neural networks (CNNs) on curated RRUFF database subsets, incorporating baseline-independent classification, pooling parameter tuning for robustness, semi-supervised GANs and contrastive pretraining for label efficiency, and constant-time adaptation by freezing CNN backbone and retraining only softmax layer.", "result": "Compact CNNs surpassed k-nearest-neighbors and SVMs on handcrafted features, accommodated Raman shifts up to 30 cm\u207b\u00b9, achieved up to 11% accuracy improvement with only 10% labels, and enabled model transfer to unseen minerals at O(1) cost.", "conclusion": "The proposed workflow provides a practical path toward robust, low-footprint Raman classification in autonomous exploration by training on raw spectra, tuning pooling, adding semi-supervision when labels are scarce, and fine-tuning lightly for new targets."}}
{"id": "2509.25977", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25977", "abs": "https://arxiv.org/abs/2509.25977", "authors": ["Xiao Zhang", "Zengzhe Chen", "Yuan Yuan", "Yifei Zou", "Fuzhen Zhuang", "Wenyu Jiao", "Yuke Wang", "Dongxiao Yu"], "title": "Data-Free Continual Learning of Server Models in Model-Heterogeneous Federated learning", "comment": null, "summary": "Federated learning (FL) is a distributed learning paradigm across multiple\nentities while preserving data privacy. However, with the continuous emergence\nof new data and increasing model diversity, traditional federated learning\nfaces significant challenges, including inherent issues of data heterogeneity,\nmodel heterogeneity and catastrophic forgetting, along with new challenge of\nknowledge misalignment. In this study, we introduce FedDCL, a novel framework\ndesigned to enable data-free continual learning of the server model in a\nmodel-heterogeneous federated setting. We leverage pre-trained diffusion models\nto extract lightweight class-specific prototypes, which confer a threefold\ndata-free advantage, enabling: (1) generation of synthetic data for the current\ntask to augment training and counteract non-IID data distributions; (2)\nexemplar-free generative replay for retaining knowledge from previous tasks;\nand (3) data-free dynamic knowledge transfer from heterogeneous clients to the\nserver. Experimental results on various datasets demonstrate the effectiveness\nof FedDCL, showcasing its potential to enhance the generalizability and\npractical applicability of federated learning in dynamic settings.", "AI": {"tldr": "FedDCL is a novel federated learning framework that enables data-free continual learning in model-heterogeneous settings using diffusion models to create class-specific prototypes for synthetic data generation, generative replay, and knowledge transfer.", "motivation": "Traditional federated learning faces challenges with data heterogeneity, model heterogeneity, catastrophic forgetting, and knowledge misalignment when dealing with new data and diverse models in dynamic environments.", "method": "Leverages pre-trained diffusion models to extract lightweight class-specific prototypes that enable three data-free capabilities: synthetic data generation for current tasks, exemplar-free generative replay for knowledge retention, and data-free dynamic knowledge transfer from heterogeneous clients.", "result": "Experimental results on various datasets demonstrate FedDCL's effectiveness in enhancing generalizability and practical applicability of federated learning in dynamic settings.", "conclusion": "FedDCL shows potential to address key challenges in federated learning and improve its deployment in real-world dynamic scenarios through data-free continual learning capabilities."}}
{"id": "2509.25979", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25979", "abs": "https://arxiv.org/abs/2509.25979", "authors": ["Gaojie Jin", "Xinping Yi", "Xiaowei Huang"], "title": "Reconcile Certified Robustness and Accuracy for DNN-based Smoothed Majority Vote Classifier", "comment": "Under Review", "summary": "Within the PAC-Bayesian framework, the Gibbs classifier (defined on a\nposterior $Q$) and the corresponding $Q$-weighted majority vote classifier are\ncommonly used to analyze the generalization performance. However, there exists\na notable lack in theoretical research exploring the certified robustness of\nmajority vote classifier and its interplay with generalization. In this study,\nwe develop a generalization error bound that possesses a certified robust\nradius for the smoothed majority vote classifier (i.e., the $Q$-weighted\nmajority vote classifier with smoothed inputs); In other words, the\ngeneralization bound holds under any data perturbation within the certified\nrobust radius. As a byproduct, we find that the underpinnings of both the\ngeneralization bound and the certified robust radius draw, in part, upon weight\nspectral norm, which thereby inspires the adoption of spectral regularization\nin smooth training to boost certified robustness. Utilizing the\ndimension-independent property of spherical Gaussian inputs in smooth training,\nwe propose a novel and inexpensive spectral regularizer to enhance the smoothed\nmajority vote classifier. In addition to the theoretical contribution, a set of\nempirical results is provided to substantiate the effectiveness of our proposed\nmethod.", "AI": {"tldr": "This paper develops a certified robustness framework for majority vote classifiers within the PAC-Bayesian framework, connecting generalization error bounds with certified robust radii through weight spectral norm analysis.", "motivation": "There is a notable lack of theoretical research exploring the certified robustness of majority vote classifiers and its interplay with generalization performance in the PAC-Bayesian framework.", "method": "Developed a generalization error bound with certified robust radius for smoothed majority vote classifiers, using weight spectral norm analysis and proposing a novel spectral regularizer for smooth training.", "result": "Theoretical framework connects generalization bounds with certified robustness through spectral norm properties, and empirical results substantiate the effectiveness of the proposed spectral regularization method.", "conclusion": "The study bridges the gap between generalization and certified robustness for majority vote classifiers, demonstrating that spectral regularization can effectively boost certified robustness while maintaining generalization performance."}}
{"id": "2509.25980", "categories": ["cs.LG", "math-ph", "math.MP", "math.PR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.25980", "abs": "https://arxiv.org/abs/2509.25980", "authors": ["Mykola Bordyuh", "Djork-Arn\u00e9 Clevert", "Marco Bertolini"], "title": "Exact Solutions to the Quantum Schr\u00f6dinger Bridge Problem", "comment": null, "summary": "The Quantum Schr\\\"odinger Bridge Problem (QSBP) describes the evolution of a\nstochastic process between two arbitrary probability distributions, where the\ndynamics are governed by the Schr\\\"odinger equation rather than by the\ntraditional real-valued wave equation. Although the QSBP is known in the\nmathematical literature, we formulate it here from a Lagrangian perspective and\nderive its main features in a way that is particularly suited to generative\nmodeling. We show that the resulting evolution equations involve the so-called\nBohm (quantum) potential, representing a notion of non-locality in the\nstochastic process. This distinguishes the QSBP from classical stochastic\ndynamics and reflects a key characteristic typical of quantum mechanical\nsystems. In this work, we derive exact closed-form solutions for the QSBP\nbetween Gaussian distributions. Our derivation is based on solving the\nFokker-Planck Equation (FPE) and the Hamilton-Jacobi Equation (HJE) arising\nfrom the Lagrangian formulation of dynamical Optimal Transport. We find that,\nsimilar to the classical Schr\\\"odinger Bridge Problem, the solution to the QSBP\nbetween Gaussians is again a Gaussian process; however, the evolution of the\ncovariance differs due to quantum effects. Leveraging these explicit solutions,\nwe present a modified algorithm based on a Gaussian Mixture Model framework,\nand demonstrate its effectiveness across several experimental settings,\nincluding single-cell evolution data, image generation, molecular translation\nand applications in Mean-Field Games.", "AI": {"tldr": "The paper formulates the Quantum Schr\u00f6dinger Bridge Problem (QSBP) from a Lagrangian perspective, derives exact closed-form solutions for Gaussian distributions, and presents a modified Gaussian Mixture Model algorithm with applications in various domains.", "motivation": "To bridge the gap between mathematical theory and practical generative modeling by formulating QSBP from a Lagrangian perspective and deriving explicit solutions for Gaussian distributions.", "method": "Lagrangian formulation of dynamical Optimal Transport, solving Fokker-Planck Equation and Hamilton-Jacobi Equation, and developing a modified Gaussian Mixture Model algorithm.", "result": "Exact closed-form solutions for QSBP between Gaussian distributions were derived, showing the solution is a Gaussian process with quantum-modified covariance evolution. The algorithm demonstrated effectiveness in single-cell evolution, image generation, molecular translation, and Mean-Field Games.", "conclusion": "The QSBP framework provides a quantum-inspired approach to stochastic processes with non-local Bohm potential, offering new capabilities for generative modeling that distinguish it from classical stochastic dynamics."}}
{"id": "2509.25996", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25996", "abs": "https://arxiv.org/abs/2509.25996", "authors": ["Weiyu Huang", "Yuezhou Hu", "Jun Zhu", "Jianfei Chen"], "title": "CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models", "comment": "Submitted to IEEE TPAMI", "summary": "Sparsity-aware training is an effective approach for transforming large\nlanguage models (LLMs) into hardware-friendly sparse patterns, thereby reducing\nlatency and memory consumption during inference. In this paper, we propose\nContinuous Adaptive Sparse Trainer (CAST), a fully continuous and\ndifferentiable sparsity-aware training framework for semi-structured (or \"N:M\")\nsparse models. Unlike previous approaches that optimize sparsity patterns and\nweights separately, CAST enables seamless joint optimization during training,\nwhile progressively transforming the model into the desired sparsity format.\nSpecifically, CAST introduces three key components: 1) AdamS, a sparsity-aware\noptimizer that leverages adaptive L1 decay to promote uniform sparsification\nacross all parameters; 2) Weight Scaling, a module designed to mitigate the\nmagnitude reduction caused by decay while preserving desired sparsity patterns;\n3) Knowledge Distillation, which employs the dense model as a self-teacher to\nenhance training efficiency. We evaluate CAST under 2:4 sparsity patterns\nacross multiple model families, ranging from 125M to 13B parameters. Our\nresults demonstrate significant improvements over previous state-of-the-art\nmethods in both perplexity and zero-shot accuracy with minimal training\nresources. Notably, on LLaMA2-7B, our 2:4 sparse model achieves a negligible\nperplexity increase of 0.09 and a 0.36% gain in zero-shot accuracy compared to\nthe dense model using only 2% of the original pretraining tokens. Additionally,\nwe establish an accurate and robust empirical scaling law to predict sparse\nmodel performance given adequate training resources. Finally, we demonstrate\nthe practical applicability of our sparse models by evaluating them under\nquantization and fine-tuning scenarios.", "AI": {"tldr": "CAST is a fully continuous and differentiable sparsity-aware training framework for semi-structured sparse models that enables joint optimization of sparsity patterns and weights during training, achieving state-of-the-art performance with minimal training resources.", "motivation": "To reduce latency and memory consumption during LLM inference by transforming models into hardware-friendly sparse patterns through a more effective training approach than previous separate optimization methods.", "method": "CAST framework with three key components: AdamS optimizer with adaptive L1 decay for uniform sparsification, Weight Scaling to mitigate magnitude reduction while preserving sparsity patterns, and Knowledge Distillation using dense model as self-teacher.", "result": "Significant improvements over previous methods in perplexity and zero-shot accuracy; on LLaMA2-7B with 2:4 sparsity achieved only 0.09 perplexity increase and 0.36% accuracy gain using only 2% of original pretraining tokens.", "conclusion": "CAST enables efficient training of high-quality sparse models with minimal resources, establishes empirical scaling laws for performance prediction, and demonstrates practical applicability under quantization and fine-tuning scenarios."}}
{"id": "2509.26000", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.26000", "abs": "https://arxiv.org/abs/2509.26000", "authors": ["Daniel Ebi", "Gaspard Lambrechts", "Damien Ernst", "Klemens B\u00f6hm"], "title": "Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access", "comment": "15 pages, 21 pages total", "summary": "Reinforcement learning in partially observable environments requires agents\nto act under uncertainty from noisy, incomplete observations. Asymmetric\nactor-critic methods leverage privileged information during training to improve\nlearning under these conditions. However, existing approaches typically assume\nfull-state access during training. In this work, we challenge this assumption\nby proposing a novel actor-critic framework, called informed asymmetric\nactor-critic, that enables conditioning the critic on arbitrary privileged\nsignals without requiring access to the full state. We show that policy\ngradients remain unbiased under this formulation, extending the theoretical\nfoundation of asymmetric methods to the more general case of privileged partial\ninformation. To quantify the impact of such signals, we propose informativeness\nmeasures based on kernel methods and return prediction error, providing\npractical tools for evaluating training-time signals. We validate our approach\nempirically on benchmark navigation tasks and synthetic partially observable\nenvironments, showing that our informed asymmetric method improves learning\nefficiency and value estimation when informative privileged inputs are\navailable. Our findings challenge the necessity of full-state access and open\nnew directions for designing asymmetric reinforcement learning methods that are\nboth practical and theoretically sound.", "AI": {"tldr": "Proposes informed asymmetric actor-critic framework that allows conditioning critics on arbitrary privileged signals without full state access, maintaining unbiased policy gradients while improving learning efficiency in partially observable environments.", "motivation": "Existing asymmetric actor-critic methods assume full-state access during training, which is often impractical. The paper challenges this assumption to enable more flexible use of privileged information.", "method": "Develops informed asymmetric actor-critic framework that conditions critic on arbitrary privileged signals, proposes informativeness measures using kernel methods and return prediction error, and validates on navigation tasks and synthetic POMDPs.", "result": "Empirical validation shows improved learning efficiency and value estimation when informative privileged inputs are available, while maintaining theoretical guarantees of unbiased policy gradients.", "conclusion": "Challenges the necessity of full-state access in asymmetric RL, opens new directions for practical asymmetric methods that leverage partial privileged information while remaining theoretically sound."}}
{"id": "2509.26015", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26015", "abs": "https://arxiv.org/abs/2509.26015", "authors": ["Bissmella Bahaduri", "Hicham Talaoubrid", "Fangchen Feng", "Zuheng Ming", "Anissa Mokraoui"], "title": "Indirect Attention: Turning Context Misalignment into a Feature", "comment": null, "summary": "The attention mechanism has become a cornerstone of modern deep learning\narchitectures, where keys and values are typically derived from the same\nunderlying sequence or representation. This work explores a less conventional\nscenario, when keys and values originate from different sequences or\nmodalities. Specifically, we first analyze the attention mechanism's behavior\nunder noisy value features, establishing a critical noise threshold beyond\nwhich signal degradation becomes significant. Furthermore, we model context\n(key, value) misalignment as an effective form of structured noise within the\nvalue features, demonstrating that the noise induced by such misalignment can\nsubstantially exceed this critical threshold, thereby compromising standard\nattention's efficacy. Motivated by this, we introduce Indirect Attention, a\nmodified attention mechanism that infers relevance indirectly in scenarios with\nmisaligned context. We evaluate the performance of Indirect Attention across a\nrange of synthetic tasks and real world applications, showcasing its superior\nability to handle misalignment.", "AI": {"tldr": "The paper analyzes attention mechanisms when keys and values come from different sequences/modalities, identifies a critical noise threshold for value degradation, models context misalignment as structured noise, and proposes Indirect Attention to handle misalignment scenarios.", "motivation": "To address limitations of standard attention mechanisms when keys and values originate from different sequences or modalities, particularly when context misalignment creates structured noise that exceeds critical thresholds.", "method": "The authors first analyze attention behavior under noisy value features, establish a critical noise threshold, model context misalignment as structured noise, and then introduce Indirect Attention mechanism that infers relevance indirectly in misaligned scenarios.", "result": "The proposed Indirect Attention demonstrates superior performance compared to standard attention across synthetic tasks and real-world applications when dealing with misaligned context between keys and values.", "conclusion": "Indirect Attention provides an effective solution for attention mechanisms operating in scenarios where keys and values come from different sequences or modalities, overcoming the limitations of standard attention in handling context misalignment."}}
{"id": "2509.26017", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26017", "abs": "https://arxiv.org/abs/2509.26017", "authors": ["Daphne Theodorakopoulos", "Elisabeth Eberling", "Miriam Bodenheimer", "Sabine Loos", "Frederic Stahl"], "title": "FITS: Towards an AI-Driven Fashion Information Tool for Sustainability", "comment": "accepted at ECAI 2025", "summary": "Access to credible sustainability information in the fashion industry remains\nlimited and challenging to interpret, despite growing public and regulatory\ndemands for transparency. General-purpose language models often lack\ndomain-specific knowledge and tend to \"hallucinate\", which is particularly\nharmful for fields where factual correctness is crucial. This work explores how\nNatural Language Processing (NLP) techniques can be applied to classify\nsustainability data for fashion brands, thereby addressing the scarcity of\ncredible and accessible information in this domain. We present a prototype\nFashion Information Tool for Sustainability (FITS), a transformer-based system\nthat extracts and classifies sustainability information from credible,\nunstructured text sources: NGO reports and scientific publications. Several\nBERT-based language models, including models pretrained on scientific and\nclimate-specific data, are fine-tuned on our curated corpus using a\ndomain-specific classification schema, with hyperparameters optimized via\nBayesian optimization. FITS allows users to search for relevant data, analyze\ntheir own data, and explore the information via an interactive interface. We\nevaluated FITS in two focus groups of potential users concerning usability,\nvisual design, content clarity, possible use cases, and desired features. Our\nresults highlight the value of domain-adapted NLP in promoting informed\ndecision-making and emphasize the broader potential of AI applications in\naddressing climate-related challenges. Finally, this work provides a valuable\ndataset, the SustainableTextileCorpus, along with a methodology for future\nupdates. Code available at https://github.com/daphne12345/FITS", "AI": {"tldr": "FITS is a transformer-based NLP system that extracts and classifies sustainability information from fashion industry sources to address the lack of credible sustainability data.", "motivation": "Limited access to credible sustainability information in fashion industry, general-purpose language models lack domain knowledge and hallucinate, need for factual correctness in sustainability data.", "method": "Fine-tuned BERT-based models on curated corpus using domain-specific classification schema, hyperparameter optimization via Bayesian optimization, developed interactive interface for data search and analysis.", "result": "Developed FITS prototype evaluated in focus groups, created SustainableTextileCorpus dataset, system enables extraction and classification of sustainability information from credible sources.", "conclusion": "Domain-adapted NLP is valuable for promoting informed decision-making in sustainability, AI has broader potential for climate-related challenges, provides methodology and dataset for future work."}}
{"id": "2509.26030", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26030", "abs": "https://arxiv.org/abs/2509.26030", "authors": ["Shuche Wang", "Fengzhuo Zhang", "Jiaxiang Li", "Cunxiao Du", "Chao Du", "Tianyu Pang", "Zhuoran Yang", "Mingyi Hong", "Vincent Y. F. Tan"], "title": "Muon Outperforms Adam in Tail-End Associative Memory Learning", "comment": null, "summary": "The Muon optimizer is consistently faster than Adam in training Large\nLanguage Models (LLMs), yet the mechanism underlying its success remains\nunclear. This paper demystifies this mechanism through the lens of associative\nmemory. By ablating the transformer components optimized by Muon, we reveal\nthat the associative memory parameters of LLMs, namely the Value and Output\n(VO) attention weights and Feed-Forward Networks (FFNs), are the primary\ncontributors to Muon's superiority. Motivated by this associative memory view,\nwe then explain Muon's superiority on real-world corpora, which are\nintrinsically heavy-tailed: a few classes (tail classes) appear far less\nfrequently than others. The superiority is explained through two key\nproperties: (i) its update rule consistently yields a more isotropic singular\nspectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes\ntail classes more effectively than Adam. Beyond empirical evidence, we\ntheoretically confirm these findings by analyzing a one-layer associative\nmemory model under class-imbalanced data. We prove that Muon consistently\nachieves balanced learning across classes regardless of feature embeddings,\nwhereas Adam can induce large disparities in learning errors depending on\nembedding properties. In summary, our empirical observations and theoretical\nanalyses reveal Muon's core advantage: its update rule aligns with the\nouter-product structure of linear associative memories, enabling more balanced\nand effective learning of tail classes in heavy-tailed distributions than Adam.", "AI": {"tldr": "Muon optimizer outperforms Adam in LLM training due to better optimization of associative memory parameters (VO attention weights and FFNs), particularly for tail classes in heavy-tailed data distributions.", "motivation": "To understand why Muon optimizer consistently trains LLMs faster than Adam, focusing on the underlying mechanisms through the lens of associative memory.", "method": "Ablated transformer components optimized by Muon, analyzed singular spectrum properties, and theoretically analyzed a one-layer associative memory model under class-imbalanced data.", "result": "Muon's update rule yields more isotropic singular spectrum than Adam, enabling more effective optimization of tail classes in heavy-tailed corpora, with theoretical confirmation of balanced learning across classes.", "conclusion": "Muon's advantage stems from its update rule aligning with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions compared to Adam."}}
{"id": "2509.26045", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.26045", "abs": "https://arxiv.org/abs/2509.26045", "authors": ["Aoming Liu", "Kevin Miller", "Venkatesh Saligrama", "Kate Saenko", "Boqing Gong", "Ser-Nam Lim", "Bryan A. Plummer"], "title": "Scaling Up Temporal Domain Generalization via Temporal Experts Averaging", "comment": "Accepted by EMNLP 2025 main", "summary": "Temporal Domain Generalization (TDG) aims to generalize across temporal\ndistribution shifts, e.g., lexical change over time. Prior work often addresses\nthis by predicting future model weights. However, full model prediction is\nprohibitively expensive for even reasonably sized models. Thus, recent methods\nonly predict the classifier layer, limiting generalization by failing to adjust\nother model components. To address this, we propose Temporal Experts Averaging\n(TEA), a novel and scalable TDG framework that updates the entire model using\nweight averaging to maximize generalization potential while minimizing\ncomputational costs. Our theoretical analysis guides us to two steps that\nenhance generalization to future domains. First, we create expert models with\nfunctional diversity yet parameter similarity by fine-tuning a domain-agnostic\nbase model on individual temporal domains while constraining weight changes.\nSecond, we optimize the bias-variance tradeoff through adaptive averaging\ncoefficients derived from modeling temporal weight trajectories in a principal\ncomponent subspace. Expert's contributions are based on their projected\nproximity to future domains. Extensive experiments across 7 TDG benchmarks, 5\nmodels, and 2 TDG settings shows TEA outperforms prior TDG methods by up to 69%\nwhile being up to 60x more efficient.", "AI": {"tldr": "TEA is a scalable Temporal Domain Generalization framework that uses weight averaging of constrained fine-tuned experts to handle temporal distribution shifts efficiently.", "motivation": "Existing TDG methods that predict future model weights are computationally expensive for large models, while classifier-only prediction limits generalization.", "method": "Fine-tune domain-agnostic base model on temporal domains with weight constraints to create diverse experts, then use adaptive averaging coefficients based on temporal weight trajectories in PCA subspace.", "result": "Outperforms prior TDG methods by up to 69% across 7 benchmarks and 5 models, while being 60x more efficient.", "conclusion": "TEA provides an effective and scalable solution for temporal domain generalization through constrained expert creation and adaptive weight averaging."}}
{"id": "2509.26226", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26226", "abs": "https://arxiv.org/abs/2509.26226", "authors": ["Xin Xu", "Cliveb AI", "Kai Yang", "Tianhao Chen", "Yang Wang", "Saiyong Yang", "Can Yang"], "title": "Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners", "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) effectively solves\ncomplex tasks but demands extremely long context lengths during training,\nleading to substantial computational costs. While multi-stage training can\npartially mitigate this, starting with overly short contexts often causes\nirreversible performance degradation, ultimately failing to reduce overall\ntraining compute significantly. In this paper, we introduce\n**T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet\neffective adaptation to RLVR that bridges long Chain-of-Thought (CoT)\ndistillation and standard RLVR. TFPI employs a simple *ThinkFree* operation,\nexplicitly discarding the thinking content via a direct *</think>* append, to\nreduce token usage during inference. Training with *ThinkFree*-adapted inputs\nimproves performance and lowers token consumption, even in the original\nslow-thinking mode. Extensive experiments across various benchmarks have shown\nthat TFPI accelerates RL convergence, achieves a higher performance ceiling,\nand yields more token-efficient reasoning models without specialized rewards or\ncomplex training designs. With TFPI only, we train a 4B model to reach 89.0%\naccuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.", "AI": {"tldr": "TFPI introduces a ThinkFree operation that discards thinking content to reduce token usage in RLVR training, improving performance and efficiency without complex modifications.", "motivation": "RLVR requires long context lengths leading to high computational costs, and multi-stage training with short contexts causes irreversible performance degradation.", "method": "TFPI bridges CoT distillation and RLVR by using a ThinkFree operation that explicitly discards thinking content via </think> append to reduce token usage during inference.", "result": "TFPI accelerates RL convergence, achieves higher performance ceiling, and yields more token-efficient reasoning models. A 4B model reached 89.0% on AIME24 and 65.5% on LiveCodeBench using <4K H20 hours.", "conclusion": "TFPI is a simple yet effective adaptation that improves RLVR training efficiency and performance without specialized rewards or complex training designs."}}
{"id": "2509.26058", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.26058", "abs": "https://arxiv.org/abs/2509.26058", "authors": ["Hossein Enshaei", "Pariya Jebreili", "Sayed Mahmoud Sakahei"], "title": "Real-time Noise Detection and Classification in Single-Channel EEG: A Lightweight Machine Learning Approach for EMG, White Noise, and EOG Artifacts", "comment": null, "summary": "Electroencephalogram (EEG) artifact detection in real-world settings faces\nsignificant challenges such as computational inefficiency in multi-channel\nmethods, poor robustness to simultaneous noise, and trade-offs between accuracy\nand complexity in deep learning models. We propose a hybrid spectral-temporal\nframework for real-time detection and classification of ocular (EOG), muscular\n(EMG), and white noise artifacts in single-channel EEG. This method, in\ncontrast to other approaches, combines time-domain low-pass filtering\n(targeting low-frequency EOG) and frequency-domain power spectral density (PSD)\nanalysis (capturing broad-spectrum EMG), followed by PCA-optimized feature\nfusion to minimize redundancy while preserving discriminative information. This\nfeature engineering strategy allows a lightweight multi-layer perceptron (MLP)\narchitecture to outperform advanced CNNs and RNNs by achieving 99% accuracy at\nlow SNRs (SNR -7) dB and >90% accuracy in moderate noise (SNR 4 dB).\nAdditionally, this framework addresses the unexplored problem of simultaneous\nmulti-source contamination(EMG+EOG+white noise), where it maintains 96%\nclassification accuracy despite overlapping artifacts. With 30-second training\ntimes (97% faster than CNNs) and robust performance across SNR levels, this\nframework bridges the gap between clinical applicability and computational\nefficiency, which enables real-time use in wearable brain-computer interfaces.\nThis work also challenges the ubiquitous dependence on model depth for EEG\nartifact detection by demonstrating that domain-informed feature fusion\nsurpasses complex architecture in noisy scenarios.", "AI": {"tldr": "Proposes a hybrid spectral-temporal framework for real-time EEG artifact detection that combines time-domain filtering and frequency-domain analysis with PCA-optimized feature fusion, achieving high accuracy with lightweight MLP architecture.", "motivation": "Address challenges in EEG artifact detection including computational inefficiency in multi-channel methods, poor robustness to simultaneous noise, and accuracy-complexity trade-offs in deep learning models.", "method": "Hybrid spectral-temporal framework combining time-domain low-pass filtering (for EOG) and frequency-domain PSD analysis (for EMG), followed by PCA-optimized feature fusion and lightweight MLP classifier.", "result": "Achieves 99% accuracy at low SNRs (-7 dB), >90% accuracy at moderate noise (4 dB), and 96% accuracy with simultaneous multi-source contamination. 30-second training time (97% faster than CNNs).", "conclusion": "Demonstrates that domain-informed feature fusion with lightweight architecture outperforms complex deep learning models in noisy EEG scenarios, enabling real-time use in wearable brain-computer interfaces."}}
{"id": "2509.26114", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26114", "abs": "https://arxiv.org/abs/2509.26114", "authors": ["Jaesung R. Park", "Junsu Kim", "Gyeongman Kim", "Jinyoung Jo", "Sean Choi", "Jaewoong Cho", "Ernest K. Ryu"], "title": "Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently emerged as\nthe leading approach for enhancing the reasoning capabilities of large language\nmodels (LLMs). However, RLVR is prone to entropy collapse, where the LLM\nquickly converges to a near-deterministic form, hindering exploration and\nprogress during prolonged RL training. In this work, we reveal that the\nclipping mechanism in PPO and GRPO induces biases on entropy. Through\ntheoretical and empirical analyses, we show that clip-low increases entropy,\nwhile clip-high decreases it. Further, under standard clipping parameters, the\neffect of clip-high dominates, resulting in an overall entropy reduction even\nwhen purely random rewards are provided to the RL algorithm. Our findings\nhighlight an overlooked confounding factor in RLVR: independent of the reward\nsignal, the clipping mechanism influences entropy, which in turn affects the\nreasoning behavior. Furthermore, our analysis demonstrates that clipping can be\ndeliberately used to control entropy. Specifically, with a more aggressive\nclip-low value, one can increase entropy, promote exploration, and ultimately\nprevent entropy collapse in RLVR training.", "AI": {"tldr": "The paper reveals that PPO and GRPO's clipping mechanisms cause entropy bias in RLVR training, with clip-low increasing entropy and clip-high decreasing it, leading to entropy collapse. The authors propose using aggressive clip-low to maintain exploration.", "motivation": "RLVR is prone to entropy collapse where LLMs become near-deterministic, hindering exploration and progress during prolonged training. The clipping mechanism in PPO/GRPO was identified as a key factor causing this issue.", "method": "Theoretical and empirical analysis of PPO and GRPO clipping mechanisms, examining how clip-low and clip-high parameters affect entropy dynamics in RLVR training.", "result": "Clip-low increases entropy while clip-high decreases it. Under standard clipping parameters, clip-high dominates, causing overall entropy reduction even with random rewards. Aggressive clip-low can increase entropy and prevent collapse.", "conclusion": "Clipping mechanism is a confounding factor in RLVR that independently affects entropy and reasoning behavior. Clipping can be deliberately used to control entropy, with aggressive clip-low promoting exploration and preventing entropy collapse."}}
{"id": "2509.26116", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.26116", "abs": "https://arxiv.org/abs/2509.26116", "authors": ["Abdulkadir Celikkanat", "Andres R. Masegosa", "Mads Albertsen", "Thomas D. Nielsen"], "title": "UncertainGen: Uncertainty-Aware Representations of DNA Sequences for Metagenomic Binning", "comment": null, "summary": "Metagenomic binning aims to cluster DNA fragments from mixed microbial\nsamples into their respective genomes, a critical step for downstream analyses\nof microbial communities. Existing methods rely on deterministic\nrepresentations, such as k-mer profiles or embeddings from large language\nmodels, which fail to capture the uncertainty inherent in DNA sequences arising\nfrom inter-species DNA sharing and from fragments with highly similar\nrepresentations. We present the first probabilistic embedding approach,\nUncertainGen, for metagenomic binning, representing each DNA fragment as a\nprobability distribution in latent space. Our approach naturally models\nsequence-level uncertainty, and we provide theoretical guarantees on embedding\ndistinguishability. This probabilistic embedding framework expands the feasible\nlatent space by introducing a data-adaptive metric, which in turn enables more\nflexible separation of bins/clusters. Experiments on real metagenomic datasets\ndemonstrate the improvements over deterministic k-mer and LLM-based embeddings\nfor the binning task by offering a scalable and lightweight solution for\nlarge-scale metagenomic analysis.", "AI": {"tldr": "UncertainGen is the first probabilistic embedding method for metagenomic binning that represents DNA fragments as probability distributions to capture sequence uncertainty, outperforming deterministic k-mer and LLM-based approaches.", "motivation": "Existing deterministic methods fail to capture uncertainty in DNA sequences caused by inter-species DNA sharing and fragments with highly similar representations, limiting binning accuracy.", "method": "Probabilistic embedding approach representing each DNA fragment as a probability distribution in latent space with theoretical guarantees on embedding distinguishability and a data-adaptive metric for flexible cluster separation.", "result": "Experiments on real metagenomic datasets demonstrate improvements over deterministic k-mer and LLM-based embeddings, offering a scalable and lightweight solution for large-scale analysis.", "conclusion": "Probabilistic embedding framework effectively models sequence-level uncertainty and enables more flexible separation of bins/clusters, advancing metagenomic binning capabilities."}}
{"id": "2509.26131", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26131", "abs": "https://arxiv.org/abs/2509.26131", "authors": ["Fardin Jalil Piran", "Anandkumar Patel", "Rajiv Malhotra", "Farhad Imani"], "title": "Domain-Aware Hyperdimensional Computing for Edge Smart Manufacturing", "comment": "23 pages, 14 figures", "summary": "Smart manufacturing requires on-device intelligence that meets strict latency\nand energy budgets. HyperDimensional Computing (HDC) offers a lightweight\nalternative by encoding data as high-dimensional hypervectors and computing\nwith simple operations. Prior studies often assume that the qualitative\nrelation between HDC hyperparameters and performance is stable across\napplications. Our analysis of two representative tasks, signal-based quality\nmonitoring in Computer Numerical Control (CNC) machining and image-based defect\ndetection in Laser Powder Bed Fusion (LPBF), shows that this assumption does\nnot hold. We map how encoder type, projection variance, hypervector\ndimensionality, and data regime shape accuracy, inference latency, training\ntime, and training energy. A formal complexity model explains predictable\ntrends in encoding and similarity computation and reveals nonmonotonic\ninteractions with retraining that preclude a closed-form optimum. Empirically,\nsignals favor nonlinear Random Fourier Features with more exclusive encodings\nand saturate in accuracy beyond moderate dimensionality. Images favor linear\nRandom Projection, achieve high accuracy with small dimensionality, and depend\nmore on sample count than on dimensionality. Guided by these insights, we tune\nHDC under multiobjective constraints that reflect edge deployment and obtain\nmodels that match or exceed the accuracy of state-of-the-art deep learning and\nTransformer models while delivering at least 6x faster inference and more than\n40x lower training energy. These results demonstrate that domain-aware HDC\nencoding is necessary and that tuned HDC offers a practical, scalable path to\nreal-time industrial AI on constrained hardware. Future work will enable\nadaptive encoder and hyperparameter selection, expand evaluation to additional\nmanufacturing modalities, and validate on low-power accelerators.", "AI": {"tldr": "HDC performance depends on application-specific hyperparameter tuning, not universal rules. Signals prefer nonlinear encodings with higher dimensions, while images work better with linear encodings and smaller dimensions. Tuned HDC matches/exceeds deep learning accuracy with 6x faster inference and 40x lower training energy.", "motivation": "Smart manufacturing needs efficient on-device AI that meets strict latency and energy constraints. HDC offers lightweight computing but prior assumptions about stable hyperparameter-performance relationships across applications are incorrect.", "method": "Analyzed HDC on two manufacturing tasks: CNC signal monitoring and LPBF image defect detection. Mapped encoder type, projection variance, dimensionality, and data regime effects on performance metrics. Developed complexity model for encoding and similarity computation.", "result": "Signals favor nonlinear Random Fourier Features with exclusive encodings and saturate beyond moderate dimensions. Images prefer linear Random Projection, achieve high accuracy with small dimensions, and depend more on sample count. Tuned HDC matches/exceeds SOTA deep learning and Transformers with 6x faster inference and 40x lower training energy.", "conclusion": "Domain-aware HDC encoding is essential for optimal performance. Tuned HDC provides practical, scalable real-time industrial AI on constrained hardware. Future work includes adaptive encoder selection and validation on low-power accelerators."}}
{"id": "2509.26594", "categories": ["cs.LG", "cs.CL", "cs.CV", "68T05 (Primary) 68T45, 68T50 (Secondary)", "I.2.6; I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.26594", "abs": "https://arxiv.org/abs/2509.26594", "authors": ["John Gkountouras", "Ivan Titov"], "title": "Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces", "comment": null, "summary": "Recent text-only models demonstrate remarkable mathematical reasoning\ncapabilities. Extending these to visual domains requires vision-language models\nto translate images into text descriptions. However, current models, trained to\nproduce captions for human readers, often omit the precise details that\nreasoning systems require. This creates an interface mismatch: reasoners often\nfail not due to reasoning limitations but because they lack access to critical\nvisual information. We propose Adaptive-Clarification Reinforcement Learning\n(AC-RL), which teaches vision models what information reasoners need through\ninteraction. Our key insight is that clarification requests during training\nreveal information gaps; by penalizing success that requires clarification, we\ncreate pressure for comprehensive initial captions that enable the reasoner to\nsolve the problem in a single pass. AC-RL improves average accuracy by 4.4\npoints over pretrained baselines across seven visual mathematical reasoning\nbenchmarks, and analysis shows it would cut clarification requests by up to 39%\nif those were allowed. By treating clarification as a form of implicit\nsupervision, AC-RL demonstrates that vision-language interfaces can be\neffectively learned through interaction alone, without requiring explicit\nannotations.", "AI": {"tldr": "AC-RL trains vision models to generate comprehensive captions for mathematical reasoning by using clarification requests as implicit supervision, improving accuracy by 4.4 points across benchmarks.", "motivation": "Current vision-language models produce captions for humans that omit precise details needed by reasoning systems, creating an interface mismatch where reasoners fail due to missing visual information rather than reasoning limitations.", "method": "Adaptive-Clarification Reinforcement Learning (AC-RL) teaches vision models what information reasoners need through interaction, penalizing success that requires clarification to create pressure for comprehensive initial captions.", "result": "AC-RL improves average accuracy by 4.4 points over pretrained baselines across seven visual mathematical reasoning benchmarks and would cut clarification requests by up to 39% if allowed.", "conclusion": "By treating clarification as implicit supervision, AC-RL demonstrates that vision-language interfaces can be effectively learned through interaction alone without requiring explicit annotations."}}
{"id": "2509.26137", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26137", "abs": "https://arxiv.org/abs/2509.26137", "authors": ["Daniil Zelezetsky", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "Accelerating Transformers in Online RL", "comment": null, "summary": "The appearance of transformer-based models in Reinforcement Learning (RL) has\nexpanded the horizons of possibilities in robotics tasks, but it has\nsimultaneously brought a wide range of challenges during its implementation,\nespecially in model-free online RL. Some of the existing learning algorithms\ncannot be easily implemented with transformer-based models due to the\ninstability of the latter. In this paper, we propose a method that uses the\nAccelerator policy as a transformer's trainer. The Accelerator, a simpler and\nmore stable model, interacts with the environment independently while\nsimultaneously training the transformer through behavior cloning during the\nfirst stage of the proposed algorithm. In the second stage, the pretrained\ntransformer starts to interact with the environment in a fully online setting.\nAs a result, this model-free algorithm accelerates the transformer in terms of\nits performance and helps it to train online in a more stable and faster way.\nBy conducting experiments on both state-based and image-based ManiSkill\nenvironments, as well as on MuJoCo tasks in MDP and POMDP settings, we show\nthat applying our algorithm not only enables stable training of transformers\nbut also reduces training time on image-based environments by up to a factor of\ntwo. Moreover, it decreases the required replay buffer size in off-policy\nmethods to 10-20 thousand, which significantly lowers the overall computational\ndemands.", "AI": {"tldr": "Proposes a two-stage method using a stable Accelerator policy to train transformers in RL, enabling stable online training and reducing computational requirements.", "motivation": "Transformer-based models in RL face instability issues in model-free online settings, making existing algorithms difficult to implement effectively.", "method": "Two-stage approach: 1) Accelerator policy interacts with environment and trains transformer via behavior cloning, 2) Pretrained transformer interacts online. Works with both state-based and image-based environments.", "result": "Enables stable transformer training, reduces training time by up to 2x on image-based environments, and decreases replay buffer size to 10-20k in off-policy methods.", "conclusion": "The proposed algorithm successfully stabilizes transformer training in RL while significantly reducing computational demands and training time."}}
{"id": "2509.26628", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.26628", "abs": "https://arxiv.org/abs/2509.26628", "authors": ["Runze Liu", "Jiakang Wang", "Yuling Shi", "Zhihui Xie", "Chenxin An", "Kaiyan Zhang", "Jian Zhao", "Xiaodong Gu", "Lei Lin", "Wenping Hu", "Xiu Li", "Fuzheng Zhang", "Guorui Zhou", "Kun Gai"], "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models", "comment": null, "summary": "Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.", "AI": {"tldr": "AttnRL is a novel Process-Supervised RL framework that improves reasoning in LLMs through attention-based branching and adaptive sampling strategies, achieving better performance and efficiency than existing methods.", "motivation": "Existing PSRL approaches suffer from limited exploration efficiency in both branching positions and sampling, which hinders their effectiveness in enhancing LLM reasoning capabilities.", "method": "Proposes branching from positions with high attention scores, adaptive sampling strategy considering problem difficulty and historical batch size, and one-step off-policy training pipeline for PSRL.", "result": "Extensive experiments on multiple challenging mathematical reasoning benchmarks show consistent outperformance over prior approaches in performance, sampling efficiency, and training efficiency.", "conclusion": "AttnRL provides an effective PSRL framework that enables efficient exploration for reasoning models through attention-guided branching and adaptive sampling strategies."}}
{"id": "2509.26139", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.26139", "abs": "https://arxiv.org/abs/2509.26139", "authors": ["James Panayis", "Matt Field", "Vignesh Gopakumar", "Andrew Lahiff", "Kristian Zarebski", "Aby Abraham", "Jonathan L. Hodges"], "title": "Leveraging AI modelling for FDS with Simvue: monitor and optimise for more sustainable simulations", "comment": "12 pages, 17 figures, Interflam Conference", "summary": "There is high demand on fire simulations, in both scale and quantity. We\npresent a multi-pronged approach to improving the time and energy required to\nmeet these demands. We show the ability of a custom machine learning surrogate\nmodel to predict the dynamics of heat propagation orders of magnitude faster\nthan state-of-the-art CFD software for this application. We also demonstrate\nhow a guided optimisation procedure can decrease the number of simulations\nrequired to meet an objective; using lightweight models to decide which\nsimulations to run, we see a tenfold reduction when locating the most dangerous\nlocation for a fire to occur within a building based on the impact of smoke on\nvisibility. Finally we present a framework and product, Simvue, through which\nwe access these tools along with a host of automatic organisational and\ntracking features which enables future reuse of data and more savings through\nbetter management of simulations and combating redundancy.", "AI": {"tldr": "A multi-pronged approach combining ML surrogate models, guided optimization, and a simulation management framework (Simvue) to dramatically improve fire simulation efficiency.", "motivation": "High demand for fire simulations in both scale and quantity requires improved time and energy efficiency.", "method": "1) Custom ML surrogate model for heat propagation prediction; 2) Guided optimization using lightweight models to select simulations; 3) Simvue framework for simulation management and data reuse.", "result": "ML surrogate predicts heat dynamics orders of magnitude faster than CFD; optimization reduces required simulations by 10x for locating dangerous fire locations; Simvue enables better simulation management and data reuse.", "conclusion": "The integrated approach significantly reduces computational costs and improves efficiency in fire simulation workflows through ML acceleration, smart optimization, and systematic management."}}
{"id": "2509.26169", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26169", "abs": "https://arxiv.org/abs/2509.26169", "authors": ["Fr\u00e9d\u00e9ric Berdoz", "Luca A. Lanzend\u00f6rfer", "Ren\u00e9 Caky", "Roger Wattenhofer"], "title": "Alignment-Aware Decoding", "comment": null, "summary": "Alignment of large language models remains a central challenge in natural\nlanguage processing. Preference optimization has emerged as a popular and\neffective method for improving alignment, typically through training-time or\nprompt-based interventions. In this paper, we introduce alignment-aware\ndecoding (AAD), a method to enhance model alignment directly at inference.\nTheoretically, AAD can be interpreted as implicit reward optimization, yet it\nrequires no specialized training beyond the standard DPO setup. Empirically,\nAAD consistently outperforms strong baselines across diverse alignment\nbenchmarks and model scales. Moreover, in data-constrained settings, AAD can\nproduce high-quality synthetic data to improve alignment under standard\ndecoding, providing a practical solution when labeled data is limited.", "AI": {"tldr": "AAD is an inference-time method that improves LLM alignment without additional training, outperforming baselines and enabling synthetic data generation in data-constrained settings.", "motivation": "Alignment remains challenging for LLMs, and existing preference optimization methods typically require training-time or prompt-based interventions.", "method": "Alignment-aware decoding (AAD) enhances model alignment directly during inference through implicit reward optimization, requiring no specialized training beyond standard DPO.", "result": "AAD consistently outperforms strong baselines across diverse alignment benchmarks and model scales, and can generate high-quality synthetic data to improve alignment under standard decoding.", "conclusion": "AAD provides an effective inference-time solution for LLM alignment that works well in data-constrained settings through synthetic data generation."}}
{"id": "2509.26171", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.26171", "abs": "https://arxiv.org/abs/2509.26171", "authors": ["Thomas Hallopeau", "Joris Gu\u00e9rin", "Laurent Demagistri", "Christovam Barcellos", "Nadine Dessay"], "title": "Neighbor-aware informal settlement mapping with graph convolutional networks", "comment": "10 pages, 3 figures, 2 tables. Accepted at the ECML PKDD 2025\n  Workshop on Machine Learning for Earth Observation", "summary": "Mapping informal settlements is crucial for addressing challenges related to\nurban planning, public health, and infrastructure in rapidly growing cities.\nGeospatial machine learning has emerged as a key tool for detecting and mapping\nthese areas from remote sensing data. However, existing approaches often treat\nspatial units independently, neglecting the relational structure of the urban\nfabric. We propose a graph-based framework that explicitly incorporates local\ngeographical context into the classification process. Each spatial unit (cell)\nis embedded in a graph structure along with its adjacent neighbors, and a\nlightweight Graph Convolutional Network (GCN) is trained to classify whether\nthe central cell belongs to an informal settlement. Experiments are conducted\non a case study in Rio de Janeiro using spatial cross-validation across five\ndistinct zones, ensuring robustness and generalizability across heterogeneous\nurban landscapes. Our method outperforms standard baselines, improving Kappa\ncoefficient by 17 points over individual cell classification. We also show that\ngraph-based modeling surpasses simple feature concatenation of neighboring\ncells, demonstrating the benefit of encoding spatial structure for urban scene\nunderstanding.", "AI": {"tldr": "A graph-based framework using Graph Convolutional Networks (GCN) to map informal settlements by incorporating local geographical context, outperforming standard methods by 17 points in Kappa coefficient.", "motivation": "Existing approaches for mapping informal settlements treat spatial units independently, neglecting the relational structure of urban fabric, which limits their effectiveness.", "method": "Proposed a graph-based framework where each spatial unit is embedded in a graph with adjacent neighbors, and a lightweight GCN is trained to classify whether the central cell belongs to an informal settlement.", "result": "Outperformed standard baselines with a 17-point improvement in Kappa coefficient over individual cell classification, and showed superiority over simple feature concatenation of neighboring cells.", "conclusion": "Graph-based modeling effectively encodes spatial structure for urban scene understanding, demonstrating significant benefits for mapping informal settlements in heterogeneous urban landscapes."}}
{"id": "2509.26186", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.26186", "abs": "https://arxiv.org/abs/2509.26186", "authors": ["Chun-Wun Cheng", "Bin Dong", "Carola-Bibiane Sch\u00f6nlieb", "Angelica I Aviles-Rivero"], "title": "PDE Solvers Should Be Local: Fast, Stable Rollouts with Learned Local Stencils", "comment": null, "summary": "Neural operator models for solving partial differential equations (PDEs)\noften rely on global mixing mechanisms-such as spectral convolutions or\nattention-which tend to oversmooth sharp local dynamics and introduce high\ncomputational cost. We present FINO, a finite-difference-inspired neural\narchitecture that enforces strict locality while retaining multiscale\nrepresentational power. FINO replaces fixed finite-difference stencil\ncoefficients with learnable convolutional kernels and evolves states via an\nexplicit, learnable time-stepping scheme. A central Local Operator Block\nleverage a differential stencil layer, a gating mask, and a linear fuse step to\nconstruct adaptive derivative-like local features that propagate forward in\ntime. Embedded in an encoder-decoder with a bottleneck, FINO captures\nfine-grained local structures while preserving interpretability. We establish\n(i) a composition error bound linking one-step approximation error to stable\nlong-horizon rollouts under a Lipschitz condition, and (ii) a universal\napproximation theorem for discrete time-stepped PDE dynamics. (iii) Across six\nbenchmarks and a climate modelling task, FINO achieves up to 44\\% lower error\nand up to around 2\\times speedups over state-of-the-art operator-learning\nbaselines, demonstrating that strict locality with learnable time-stepping\nyields an accurate and scalable foundation for neural PDE solvers.", "AI": {"tldr": "FINO is a finite-difference-inspired neural architecture for solving PDEs that enforces strict locality while maintaining multiscale power, achieving better accuracy and speed than global mixing approaches.", "motivation": "Existing neural operator models for PDEs rely on global mixing mechanisms that oversmooth sharp local dynamics and have high computational costs.", "method": "FINO replaces fixed finite-difference stencil coefficients with learnable convolutional kernels and uses explicit learnable time-stepping. It features a Local Operator Block with differential stencil layer, gating mask, and linear fuse step to create adaptive derivative-like local features.", "result": "FINO achieves up to 44% lower error and 2\u00d7 speedups over state-of-the-art operator-learning baselines across six benchmarks and a climate modeling task.", "conclusion": "Strict locality with learnable time-stepping provides an accurate and scalable foundation for neural PDE solvers, balancing local structure preservation with computational efficiency."}}
{"id": "2509.26187", "categories": ["cs.LG", "cs.AI", "cs.CV", "60G35, 62M10, 62P35, 65C20, 68T45, 68U10, 92C35, 92C40, 92C42, 93E10", "C.2.1; C.2.4; C.3; H.2.8; H.3.4; H.3.5; I.2; I.2.4; I.2.6; I.2.11;\n  I.4.8; I.5.1; I.5.4; I.5.1; I.5.2; I.5; J.3; K.6.1"], "pdf": "https://arxiv.org/pdf/2509.26187", "abs": "https://arxiv.org/abs/2509.26187", "authors": ["Youssef Sabiri", "Walid Houmaidi", "Aaya Bougrine", "Salmane El Mansour Billah"], "title": "Optimizing Indoor Environmental Quality in Smart Buildings Using Deep Learning", "comment": "10 pages, 4 figures, 1 table. Accepted and presented at the 5th\n  International Conference on Digital Technologies and Applications (ICDTA\n  2025), April 17-18, 2025, Al Akhawayn University, Ifrane, Morocco", "summary": "Ensuring optimal Indoor Environmental Quality (IEQ) is vital for occupant\nhealth and productivity, yet it often comes at a high energy cost in\nconventional Heating, Ventilation, and Air Conditioning (HVAC) systems. This\npaper proposes a deep learning driven approach to proactively manage IEQ\nparameters specifically CO2 concentration, temperature, and humidity while\nbalancing building energy efficiency. Leveraging the ROBOD dataset collected\nfrom a net-zero energy academic building, we benchmark three\narchitectures--Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and\na hybrid Convolutional Neural Network LSTM (CNN-LSTM)--to forecast IEQ\nvariables across various time horizons. Our results show that GRU achieves the\nbest short-term prediction accuracy with lower computational overhead, whereas\nCNN-LSTM excels in extracting dominant features for extended forecasting\nwindows. Meanwhile, LSTM offers robust long-range temporal modeling. The\ncomparative analysis highlights that prediction reliability depends on data\nresolution, sensor placement, and fluctuating occupancy conditions. These\nfindings provide actionable insights for intelligent Building Management\nSystems (BMS) to implement predictive HVAC control, thereby reducing energy\nconsumption and enhancing occupant comfort in real-world building operations.", "AI": {"tldr": "This paper proposes a deep learning approach using LSTM, GRU, and CNN-LSTM architectures to forecast Indoor Environmental Quality parameters (CO2, temperature, humidity) while balancing energy efficiency in HVAC systems.", "motivation": "To address the challenge of maintaining optimal Indoor Environmental Quality (IEQ) for occupant health and productivity while minimizing the high energy costs associated with conventional HVAC systems.", "method": "Benchmarked three deep learning architectures (LSTM, GRU, and hybrid CNN-LSTM) using the ROBOD dataset from a net-zero energy academic building to forecast IEQ variables across different time horizons.", "result": "GRU achieved the best short-term prediction accuracy with lower computational overhead, CNN-LSTM excelled in extracting dominant features for extended forecasting, and LSTM offered robust long-range temporal modeling. Prediction reliability depends on data resolution, sensor placement, and occupancy conditions.", "conclusion": "The findings provide actionable insights for intelligent Building Management Systems to implement predictive HVAC control, reducing energy consumption and enhancing occupant comfort in real-world building operations."}}
{"id": "2509.26221", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26221", "abs": "https://arxiv.org/abs/2509.26221", "authors": ["Marcello Massimo Negri", "Jonathan Aellen", "Manuel Jahn", "AmirEhsan Khorashadizadeh", "Volker Roth"], "title": "Marginal Flow: a flexible and efficient framework for density estimation", "comment": null, "summary": "Current density modeling approaches suffer from at least one of the following\nshortcomings: expensive training, slow inference, approximate likelihood, mode\ncollapse or architectural constraints like bijective mappings. We propose a\nsimple yet powerful framework that overcomes these limitations altogether. We\ndefine our model $q_\\theta(x)$ through a parametric distribution $q(x|w)$ with\nlatent parameters $w$. Instead of directly optimizing the latent variables $w$,\nour idea is to marginalize them out by sampling $w$ from a learnable\ndistribution $q_\\theta(w)$, hence the name Marginal Flow. In order to evaluate\nthe learned density $q_\\theta(x)$ or to sample from it, we only need to draw\nsamples from $q_\\theta(w)$, which makes both operations efficient. The proposed\nmodel allows for exact density evaluation and is orders of magnitude faster\nthan competing models both at training and inference. Furthermore, Marginal\nFlow is a flexible framework: it does not impose any restrictions on the neural\nnetwork architecture, it enables learning distributions on lower-dimensional\nmanifolds (either known or to be learned), it can be trained efficiently with\nany objective (e.g. forward and reverse KL divergence), and it easily handles\nmulti-modal targets. We evaluate Marginal Flow extensively on various tasks\nincluding synthetic datasets, simulation-based inference, distributions on\npositive definite matrices and manifold learning in latent spaces of images.", "AI": {"tldr": "Marginal Flow is a novel density modeling framework that overcomes limitations of existing approaches by marginalizing out latent parameters through a learnable distribution, enabling exact density evaluation, fast training/inference, and flexibility across various architectures and objectives.", "motivation": "Current density modeling methods suffer from issues like expensive training, slow inference, approximate likelihoods, mode collapse, or architectural constraints like bijective mappings. The authors aim to create a framework that overcomes all these limitations simultaneously.", "method": "The model defines q_\u03b8(x) through a parametric distribution q(x|w) with latent parameters w. Instead of directly optimizing w, the method marginalizes them out by sampling w from a learnable distribution q_\u03b8(w). This enables efficient density evaluation and sampling by only requiring draws from q_\u03b8(w).", "result": "Marginal Flow achieves exact density evaluation and is orders of magnitude faster than competing models in both training and inference. It demonstrates flexibility across various tasks including synthetic datasets, simulation-based inference, distributions on positive definite matrices, and manifold learning in image latent spaces.", "conclusion": "Marginal Flow provides a simple yet powerful framework that overcomes multiple limitations of current density modeling approaches, offering exact density evaluation, computational efficiency, architectural flexibility, and strong performance across diverse applications."}}
{"id": "2509.26234", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.26234", "abs": "https://arxiv.org/abs/2509.26234", "authors": ["Ayush Patnaik", "Adam B Zufall", "Stephen K Robinson", "Xinfan Lin"], "title": "Machine Learning Detection of Lithium Plating in Lithium-ion Cells: A Gaussian Process Approach", "comment": "Submitted to American Control Conference - ACC 2026", "summary": "Lithium plating during fast charging is a critical degradation mechanism that\naccelerates capacity fade and can trigger catastrophic safety failures. Recent\nwork has identified a distinctive dQ/dV peak above 4.0 V as a reliable\nsignature of plating onset; however, conventional methods for computing dQ/dV\nrely on finite differencing with filtering, which amplifies sensor noise and\nintroduces bias in peak location. In this paper, we propose a Gaussian Process\n(GP) framework for lithium plating detection by directly modeling the\ncharge-voltage relationship Q(V) as a stochastic process with calibrated\nuncertainty. Leveraging the property that derivatives of GPs remain GPs, we\ninfer dQ/dV analytically and probabilistically from the posterior, enabling\nrobust detection without ad hoc smoothing. The framework provides three key\nbenefits: (i) noise-aware inference with hyperparameters learned from data,\n(ii) closed-form derivatives with credible intervals for uncertainty\nquantification, and (iii) scalability to online variants suitable for embedded\nBMS. Experimental validation on Li-ion coin cells across a range of C-rates\n(0.2C-1C) and temperatures (0-40\\deg C) demonstrates that the GP-based method\nreliably detects plating peaks under low-temperature, high-rate charging, while\ncorrectly reporting no peaks in baseline cases. The concurrence of\nGP-identified differential peaks, reduced charge throughput, and capacity fade\nmeasured via reference performance tests confirms the method's accuracy and\nrobustness, establishing a practical pathway for real-time lithium plating\ndetection.", "AI": {"tldr": "A Gaussian Process framework is proposed for detecting lithium plating in batteries by modeling charge-voltage relationships probabilistically, enabling robust dQ/dV peak detection without noise amplification from conventional finite differencing methods.", "motivation": "Lithium plating during fast charging accelerates battery degradation and poses safety risks. Conventional dQ/dV computation methods amplify sensor noise and introduce bias in peak detection, limiting reliable plating onset identification.", "method": "The paper develops a Gaussian Process framework that directly models Q(V) as a stochastic process. By leveraging GP properties where derivatives remain GPs, it analytically infers dQ/dV with calibrated uncertainty, eliminating the need for ad hoc smoothing.", "result": "Experimental validation on Li-ion coin cells across various C-rates (0.2C-1C) and temperatures (0-40\u00b0C) shows the GP method reliably detects plating peaks under low-temperature, high-rate charging conditions, while correctly reporting no peaks in baseline cases.", "conclusion": "The GP-based approach provides accurate lithium plating detection with uncertainty quantification, establishing a practical pathway for real-time monitoring in battery management systems, as confirmed by correlation with reduced charge throughput and capacity fade measurements."}}
{"id": "2509.26238", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26238", "abs": "https://arxiv.org/abs/2509.26238", "authors": ["James Oldfield", "Philip Torr", "Ioannis Patras", "Adel Bibi", "Fazl Barez"], "title": "Beyond Linear Probes: Dynamic Safety Monitoring for Language Models", "comment": "Project page: http://james-oldfield.github.io/tpc", "summary": "Monitoring large language models' (LLMs) activations is an effective way to\ndetect harmful requests before they lead to unsafe outputs. However,\ntraditional safety monitors often require the same amount of compute for every\nquery. This creates a trade-off: expensive monitors waste resources on easy\ninputs, while cheap ones risk missing subtle cases. We argue that safety\nmonitors should be flexible--costs should rise only when inputs are difficult\nto assess, or when more compute is available. To achieve this, we introduce\nTruncated Polynomial Classifiers (TPCs), a natural extension of linear probes\nfor dynamic activation monitoring. Our key insight is that polynomials can be\ntrained and evaluated progressively, term-by-term. At test-time, one can\nearly-stop for lightweight monitoring, or use more terms for stronger\nguardrails when needed. TPCs provide two modes of use. First, as a safety dial:\nby evaluating more terms, developers and regulators can \"buy\" stronger\nguardrails from the same model. Second, as an adaptive cascade: clear cases\nexit early after low-order checks, and higher-order guardrails are evaluated\nonly for ambiguous inputs, reducing overall monitoring costs. On two\nlarge-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with\nup to 30B parameters, we show that TPCs compete with or outperform MLP-based\nprobe baselines of the same size, all the while being more interpretable than\ntheir black-box counterparts. Our code is available at\nhttp://github.com/james-oldfield/tpc.", "AI": {"tldr": "TPCs are flexible safety monitors for LLMs that can dynamically adjust computational cost based on input difficulty, providing progressive polynomial evaluation for efficient harmful content detection.", "motivation": "Traditional safety monitors waste resources on easy inputs while risking missing subtle cases. There's a need for flexible monitors where costs rise only when inputs are difficult to assess or when more compute is available.", "method": "Introduce Truncated Polynomial Classifiers (TPCs) - an extension of linear probes that can be trained and evaluated progressively term-by-term. Allows early stopping for lightweight monitoring or using more terms for stronger guardrails.", "result": "TPCs compete with or outperform MLP-based probe baselines on two large-scale safety datasets (WildGuardMix and BeaverTails) for 4 models up to 30B parameters, while being more interpretable than black-box counterparts.", "conclusion": "TPCs provide flexible safety monitoring with two usage modes: as a safety dial for stronger guardrails when needed, and as an adaptive cascade to reduce monitoring costs by exiting early on clear cases."}}
{"id": "2509.26239", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.26239", "abs": "https://arxiv.org/abs/2509.26239", "authors": ["Joel Dyer", "Daniel Jarne Ornia", "Nicholas Bishop", "Anisoara Calinescu", "Michael Wooldridge"], "title": "Sandbagging in a Simple Survival Bandit Problem", "comment": "Forthcoming in the \"Reliable ML from Unreliable Data Workshop\" at\n  NeurIPS 2025", "summary": "Evaluating the safety of frontier AI systems is an increasingly important\nconcern, helping to measure the capabilities of such models and identify risks\nbefore deployment. However, it has been recognised that if AI agents are aware\nthat they are being evaluated, such agents may deliberately hide dangerous\ncapabilities or intentionally demonstrate suboptimal performance in\nsafety-related tasks in order to be released and to avoid being deactivated or\nretrained. Such strategic deception - often known as \"sandbagging\" - threatens\nto undermine the integrity of safety evaluations. For this reason, it is of\nvalue to identify methods that enable us to distinguish behavioural patterns\nthat demonstrate a true lack of capability from behavioural patterns that are\nconsistent with sandbagging. In this paper, we develop a simple model of\nstrategic deception in sequential decision-making tasks, inspired by the\nrecently developed survival bandit framework. We demonstrate theoretically that\nthis problem induces sandbagging behaviour in optimal rational agents, and\nconstruct a statistical test to distinguish between sandbagging and\nincompetence from sequences of test scores. In simulation experiments, we\ninvestigate the reliability of this test in allowing us to distinguish between\nsuch behaviours in bandit models. This work aims to establish a potential\navenue for developing robust statistical procedures for use in the science of\nfrontier model evaluations.", "AI": {"tldr": "Developed a statistical test to detect AI sandbagging (strategic deception) in safety evaluations using survival bandit framework.", "motivation": "AI systems may hide dangerous capabilities during safety evaluations to avoid being deactivated or retrained, undermining evaluation integrity.", "method": "Created a simple model of strategic deception in sequential decision-making tasks based on survival bandit framework, with theoretical analysis and statistical test construction.", "result": "Theoretical demonstration that optimal rational agents exhibit sandbagging behavior, and simulation experiments show test reliability in distinguishing sandbagging from incompetence.", "conclusion": "Establishes a potential approach for developing robust statistical procedures for frontier AI model evaluations to detect strategic deception."}}
{"id": "2509.26241", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26241", "abs": "https://arxiv.org/abs/2509.26241", "authors": ["Ahmad-Reza Ehyaei", "Golnoosh Farnadi", "Samira Samadi"], "title": "From Fragile to Certified: Wasserstein Audits of Group Fairness Under Distribution Shift", "comment": null, "summary": "Group-fairness metrics (e.g., equalized odds) can vary sharply across\nresamples and are especially brittle under distribution shift, undermining\nreliable audits. We propose a Wasserstein distributionally robust framework\nthat certifies worst-case group fairness over a ball of plausible test\ndistributions centered at the empirical law. Our formulation unifies common\ngroup fairness notions via a generic conditional-probability functional and\ndefines $\\varepsilon$-Wasserstein Distributional Fairness ($\\varepsilon$-WDF)\nas the audit target. Leveraging strong duality, we derive tractable\nreformulations and an efficient estimator (DRUNE) for $\\varepsilon$-WDF. We\nprove feasibility and consistency and establish finite-sample certification\nguarantees for auditing fairness, along with quantitative bounds under\nsmoothness and margin conditions. Across standard benchmarks and classifiers,\n$\\varepsilon$-WDF delivers stable fairness assessments under distribution\nshift, providing a principled basis for auditing and certifying group fairness\nbeyond observational data.", "AI": {"tldr": "Proposes Wasserstein distributionally robust framework for certifying worst-case group fairness under distribution shift, with tractable estimator DRUNE and theoretical guarantees.", "motivation": "Group-fairness metrics are brittle under distribution shift and vary sharply across resamples, undermining reliable fairness audits.", "method": "Uses Wasserstein distributionally robust framework to certify worst-case group fairness over plausible test distributions, unifying common fairness notions via conditional-probability functional, with tractable reformulations and DRUNE estimator.", "result": "Provides stable fairness assessments under distribution shift across benchmarks, with feasibility, consistency, finite-sample certification guarantees, and quantitative bounds under smoothness/margin conditions.", "conclusion": "\u03b5-WDF delivers principled basis for auditing and certifying group fairness beyond observational data, addressing brittleness of traditional fairness metrics under distribution shift."}}
{"id": "2509.26275", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26275", "abs": "https://arxiv.org/abs/2509.26275", "authors": ["Ahmad-Reza Ehyaei", "Golnoosh Farnadi", "Samira Samadi"], "title": "Wasserstein Distributionally Robust Optimization Through the Lens of Structural Causal Models and Individual Fairness", "comment": null, "summary": "In recent years, Wasserstein Distributionally Robust Optimization (DRO) has\ngarnered substantial interest for its efficacy in data-driven decision-making\nunder distributional uncertainty. However, limited research has explored the\napplication of DRO to address individual fairness concerns, particularly when\nconsidering causal structures and sensitive attributes in learning problems. To\naddress this gap, we first formulate the DRO problem from causality and\nindividual fairness perspectives. We then present the DRO dual formulation as\nan efficient tool to convert the DRO problem into a more tractable and\ncomputationally efficient form. Next, we characterize the closed form of the\napproximate worst-case loss quantity as a regularizer, eliminating the max-step\nin the min-max DRO problem. We further estimate the regularizer in more general\ncases and explore the relationship between DRO and classical robust\noptimization. Finally, by removing the assumption of a known structural causal\nmodel, we provide finite sample error bounds when designing DRO with empirical\ndistributions and estimated causal structures to ensure efficiency and robust\nlearning.", "AI": {"tldr": "This paper applies Wasserstein Distributionally Robust Optimization (DRO) to address individual fairness concerns in causal learning problems, providing dual formulations, regularizer approximations, and finite sample error bounds.", "motivation": "Limited research has explored DRO for individual fairness with causal structures and sensitive attributes, creating a gap in robust and fair data-driven decision-making.", "method": "Formulate DRO from causality and fairness perspectives, develop dual formulations, characterize worst-case loss as regularizer, estimate regularizer in general cases, and provide finite sample error bounds with empirical distributions.", "result": "The paper converts DRO problems into tractable forms, eliminates the max-step through regularizer approximation, and establishes relationships between DRO and classical robust optimization.", "conclusion": "The proposed framework enables efficient and robust learning for individual fairness in causal settings, with theoretical guarantees even when structural causal models are unknown."}}
{"id": "2509.26282", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26282", "abs": "https://arxiv.org/abs/2509.26282", "authors": ["Anthony Zhou", "Alexander Wikner", "Amaury Lancelin", "Pedram Hassanzadeh", "Amir Barati Farimani"], "title": "Reframing Generative Models for Physical Systems using Stochastic Interpolants", "comment": "Code and data is available at\n  http://github.com/anthonyzhou-1/interpolant_pdes", "summary": "Generative models have recently emerged as powerful surrogates for physical\nsystems, demonstrating increased accuracy, stability, and/or statistical\nfidelity. Most approaches rely on iteratively denoising a Gaussian, a choice\nthat may not be the most effective for autoregressive prediction tasks in PDEs\nand dynamical systems such as climate. In this work, we benchmark generative\nmodels across diverse physical domains and tasks, and highlight the role of\nstochastic interpolants. By directly learning a stochastic process between\ncurrent and future states, stochastic interpolants can leverage the proximity\nof successive physical distributions. This allows for generative models that\ncan use fewer sampling steps and produce more accurate predictions than models\nrelying on transporting Gaussian noise. Our experiments suggest that generative\nmodels need to balance deterministic accuracy, spectral consistency, and\nprobabilistic calibration, and that stochastic interpolants can potentially\nfulfill these requirements by adjusting their sampling. This study establishes\nstochastic interpolants as a competitive baseline for physical emulation and\ngives insight into the abilities of different generative modeling frameworks.", "AI": {"tldr": "Stochastic interpolants outperform Gaussian denoising methods for physical system prediction by learning direct stochastic processes between states, enabling faster and more accurate autoregressive predictions.", "motivation": "Current generative models using Gaussian denoising may not be optimal for physical system prediction tasks like PDEs and climate modeling, where successive states are closely related.", "method": "Use stochastic interpolants that directly learn stochastic processes between current and future states, leveraging the proximity of successive physical distributions.", "result": "Stochastic interpolants achieve more accurate predictions with fewer sampling steps compared to Gaussian noise transport methods, while balancing deterministic accuracy, spectral consistency, and probabilistic calibration.", "conclusion": "Stochastic interpolants establish a competitive baseline for physical emulation and provide insights into different generative modeling frameworks' capabilities for physical system prediction."}}
{"id": "2509.26294", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26294", "abs": "https://arxiv.org/abs/2509.26294", "authors": ["Lionel Blond\u00e9", "Joao A. Candido Ramos", "Alexandros Kalousis"], "title": "Noise-Guided Transport for Imitation Learning", "comment": null, "summary": "We consider imitation learning in the low-data regime, where only a limited\nnumber of expert demonstrations are available. In this setting, methods that\nrely on large-scale pretraining or high-capacity architectures can be difficult\nto apply, and efficiency with respect to demonstration data becomes critical.\nWe introduce Noise-Guided Transport (NGT), a lightweight off-policy method that\ncasts imitation as an optimal transport problem solved via adversarial\ntraining. NGT requires no pretraining or specialized architectures,\nincorporates uncertainty estimation by design, and is easy to implement and\ntune. Despite its simplicity, NGT achieves strong performance on challenging\ncontinuous control tasks, including high-dimensional Humanoid tasks, under\nultra-low data regimes with as few as 20 transitions. Code is publicly\navailable at: https://github.com/lionelblonde/ngt-pytorch.", "AI": {"tldr": "NGT is a lightweight imitation learning method that frames imitation as optimal transport problem, achieving strong performance with very limited data (as few as 20 transitions) without requiring pretraining or complex architectures.", "motivation": "Address the challenge of imitation learning in low-data regimes where only limited expert demonstrations are available, and existing methods relying on large-scale pretraining or high-capacity architectures are impractical.", "method": "Noise-Guided Transport (NGT) - casts imitation as optimal transport problem solved via adversarial training, incorporates uncertainty estimation by design, requires no pretraining or specialized architectures.", "result": "Achieves strong performance on challenging continuous control tasks including high-dimensional Humanoid tasks under ultra-low data regimes with as few as 20 transitions.", "conclusion": "NGT provides an efficient, easy-to-implement solution for imitation learning in data-scarce scenarios, demonstrating that simple methods can be highly effective when designed appropriately."}}
{"id": "2509.26300", "categories": ["cs.LG", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.26300", "abs": "https://arxiv.org/abs/2509.26300", "authors": ["Floris-Jan Willemsen", "Rob V. van Nieuwpoort", "Ben van Werkhoven"], "title": "Tuning the Tuner: Introducing Hyperparameter Optimization for Auto-Tuning", "comment": null, "summary": "Automatic performance tuning (auto-tuning) is widely used to optimize\nperformance-critical applications across many scientific domains by finding the\nbest program variant among many choices. Efficient optimization algorithms are\ncrucial for navigating the vast and complex search spaces in auto-tuning. As is\nwell known in the context of machine learning and similar fields,\nhyperparameters critically shape optimization algorithm efficiency. Yet for\nauto-tuning frameworks, these hyperparameters are almost never tuned, and their\npotential performance impact has not been studied.\n  We present a novel method for general hyperparameter tuning of optimization\nalgorithms for auto-tuning, thus \"tuning the tuner\". In particular, we propose\na robust statistical method for evaluating hyperparameter performance across\nsearch spaces, publish a FAIR data set and software for reproducibility, and\npresent a simulation mode that replays previously recorded tuning data,\nlowering the costs of hyperparameter tuning by two orders of magnitude. We show\nthat even limited hyperparameter tuning can improve auto-tuner performance by\n94.8% on average, and establish that the hyperparameters themselves can be\noptimized efficiently with meta-strategies (with an average improvement of\n204.7%), demonstrating the often overlooked hyperparameter tuning as a powerful\ntechnique for advancing auto-tuning research and practice.", "AI": {"tldr": "This paper introduces a method for tuning hyperparameters of optimization algorithms in auto-tuning frameworks, showing significant performance improvements through systematic hyperparameter optimization.", "motivation": "Auto-tuning frameworks rarely tune their optimization algorithm hyperparameters, despite their critical impact on performance, creating an unexplored opportunity for performance enhancement.", "method": "Proposes a statistical method for evaluating hyperparameter performance across search spaces, includes a FAIR dataset and software for reproducibility, and introduces a simulation mode that reduces tuning costs by 100x.", "result": "Limited hyperparameter tuning improved auto-tuner performance by 94.8% on average, while meta-strategies achieved 204.7% average improvement.", "conclusion": "Hyperparameter tuning is a powerful but overlooked technique that can significantly advance auto-tuning research and practice."}}
{"id": "2509.26301", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.26301", "abs": "https://arxiv.org/abs/2509.26301", "authors": ["Suli Wang", "Yangshen Deng", "Zhenghua Bao", "Xinyu Zhan", "Yiqun Duan"], "title": "NeuroTTT: Bridging Pretraining-Downstream Task Misalignment in EEG Foundation Models via Test-Time Training", "comment": null, "summary": "Large-scale foundation models for EEG signals offer a promising path to\ngeneralizable brain-computer interface (BCI) applications, but they often\nsuffer from misalignment between pretraining objectives and downstream tasks,\nas well as significant cross-subject distribution shifts. This paper addresses\nthese challenges by introducing a two-stage alignment strategy that bridges the\ngap between generic pretraining and specific EEG decoding tasks. First, we\npropose NeuroTTT: a domain-specific self-supervised fine-tuning paradigm that\naugments the foundation model with task-relevant self-supervised objectives,\naligning latent representations to important spectral, spatial, and temporal\nEEG features without requiring additional labeled data. Second, we incorporate\ntest-time training (TTT) at inference, we perform (i) self-supervised test-time\ntraining on individual unlabeled test samples and (ii) prediction entropy\nminimization (Tent), which updates only normalization statistics to continually\ncalibrate the model to each new input on the fly. Our approach, which, to our\nknowledge, is the first to unify domain-tuned self-supervision with test-time\ntraining in large-scale EEG foundation models, yields substantially improved\nrobustness and accuracy across diverse BCI tasks (imagined speech, stress\ndetection, motor imagery). Using CBraMod and LaBraM as backbones, our method\npushes their performance to a markedly higher level. Results on three diverse\ntasks demonstrate that the proposed alignment strategy achieves\nstate-of-the-art performance, outperforming conventional fine-tuning and\nadaptation methods. Our code is available at\nhttps://github.com/wsl2000/NeuroTTT.", "AI": {"tldr": "This paper introduces a two-stage alignment strategy for EEG foundation models that combines domain-specific self-supervised fine-tuning (NeuroTTT) with test-time training to address misalignment between pretraining and downstream tasks, and cross-subject distribution shifts in brain-computer interface applications.", "motivation": "Large-scale EEG foundation models suffer from misalignment between pretraining objectives and downstream tasks, as well as significant cross-subject distribution shifts, which limits their effectiveness in brain-computer interface applications.", "method": "A two-stage alignment strategy: (1) NeuroTTT - domain-specific self-supervised fine-tuning that aligns latent representations to spectral, spatial, and temporal EEG features without labeled data; (2) Test-time training with self-supervised training on individual test samples and prediction entropy minimization (Tent) that updates normalization statistics for real-time calibration.", "result": "The method achieves state-of-the-art performance on three diverse BCI tasks (imagined speech, stress detection, motor imagery), substantially improving robustness and accuracy compared to conventional fine-tuning and adaptation methods, using CBraMod and LaBraM as backbones.", "conclusion": "The proposed two-stage alignment strategy that unifies domain-tuned self-supervision with test-time training effectively bridges the gap between generic pretraining and specific EEG decoding tasks, enabling more robust and accurate brain-computer interface applications."}}
{"id": "2509.26307", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26307", "abs": "https://arxiv.org/abs/2509.26307", "authors": ["Piotr Komorowski", "Elena Golimblevskaia", "Reduan Achtibat", "Thomas Wiegand", "Sebastian Lapuschkin", "Wojciech Samek"], "title": "Attribution-Guided Decoding", "comment": null, "summary": "The capacity of Large Language Models (LLMs) to follow complex instructions\nand generate factually accurate text is critical for their real-world\napplication. However, standard decoding methods often fail to robustly satisfy\nthese requirements, while existing control techniques frequently degrade\ngeneral output quality. In this work, we introduce Attribution-Guided Decoding\n(AGD), an interpretability-based decoding strategy. Instead of directly\nmanipulating model activations, AGD considers a set of high-probability output\ntoken candidates and selects the one that exhibits the highest attribution to a\nuser-defined Region of Interest (ROI). This ROI can be flexibly defined over\ndifferent parts of the model's input or internal components, allowing AGD to\nsteer generation towards various desirable behaviors. We demonstrate AGD's\nefficacy across three challenging domains. For instruction following, we show\nthat AGD significantly boosts adherence (e.g., improving the overall success\nrate on Llama 3.1 from 66.0% to 79.1%). For knowledge-intensive tasks, we show\nthat guiding generation towards usage of internal knowledge components or\ncontextual sources can reduce hallucinations and improve factual accuracy in\nboth closed-book and open-book settings. Furthermore, we propose an adaptive,\nentropy-based variant of AGD that mitigates quality degradation and reduces\ncomputational overhead by applying guidance only when the model is uncertain.\nOur work presents a versatile, more interpretable, and effective method for\nenhancing the reliability of modern LLMs.", "AI": {"tldr": "AGD is an interpretability-based decoding method that selects tokens with highest attribution to user-defined regions of interest, improving instruction following and factual accuracy while reducing hallucinations.", "motivation": "Standard decoding methods often fail to robustly satisfy complex instruction following and factual accuracy requirements, while existing control techniques frequently degrade general output quality.", "method": "AGD considers high-probability output token candidates and selects the one with highest attribution to user-defined ROI, which can be flexibly defined over different parts of input or internal components. An adaptive entropy-based variant applies guidance only when model is uncertain.", "result": "Significantly improves instruction adherence (Llama 3.1 success rate from 66.0% to 79.1%), reduces hallucinations, improves factual accuracy in both closed-book and open-book settings, while mitigating quality degradation and computational overhead.", "conclusion": "AGD presents a versatile, interpretable, and effective method for enhancing reliability of modern LLMs across instruction following and knowledge-intensive tasks."}}
{"id": "2509.26321", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26321", "abs": "https://arxiv.org/abs/2509.26321", "authors": ["Judith Echevarrieta", "Etor Arza", "Aritz P\u00e9rez", "Josu Ceberio"], "title": "A Review on Single-Problem Multi-Attempt Heuristic Optimization", "comment": null, "summary": "In certain real-world optimization scenarios, practitioners are not\ninterested in solving multiple problems but rather in finding the best solution\nto a single, specific problem. When the computational budget is large relative\nto the cost of evaluating a candidate solution, multiple heuristic alternatives\ncan be tried to solve the same given problem, each possibly with a different\nalgorithm, parameter configuration, initialization, or stopping criterion. The\nsequential selection of which alternative to try next is crucial for\nefficiently identifying the one that provides the best possible solution across\nmultiple attempts. Despite the relevance of this problem in practice, it has\nnot yet been the exclusive focus of any existing review. Several sequential\nalternative selection strategies have been proposed in different research\ntopics, but they have not been comprehensively and systematically unified under\na common perspective.\n  This work presents a focused review of single-problem multi-attempt heuristic\noptimization. It brings together suitable strategies to this problem that have\nbeen studied separately through algorithm selection, parameter tuning,\nmulti-start and resource allocation. These strategies are explained using a\nunified terminology within a common framework, which supports the development\nof a taxonomy for systematically organizing and classifying them.", "AI": {"tldr": "This paper reviews sequential selection strategies for single-problem multi-attempt heuristic optimization, unifying approaches from algorithm selection, parameter tuning, multi-start and resource allocation under a common framework.", "motivation": "Practitioners often need to find the best solution for a single problem using multiple heuristic alternatives, but existing sequential selection strategies are scattered across different research topics without unified perspective.", "method": "The authors conduct a focused review and develop a taxonomy that systematically organizes and classifies sequential alternative selection strategies using unified terminology within a common framework.", "result": "The work brings together suitable strategies from algorithm selection, parameter tuning, multi-start and resource allocation, providing a comprehensive overview of methods for single-problem multi-attempt optimization.", "conclusion": "This review fills a gap in the literature by providing a unified framework and taxonomy for sequential alternative selection in single-problem multi-attempt heuristic optimization, enabling better organization and classification of existing strategies."}}
{"id": "2509.26322", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.26322", "abs": "https://arxiv.org/abs/2509.26322", "authors": ["Margarita A. Guerrero", "Cristian R. Rojas"], "title": "ACE: Adapting sampling for Counterfactual Explanations", "comment": "15 pages", "summary": "Counterfactual Explanations (CFEs) interpret machine learning models by\nidentifying the smallest change to input features needed to change the model's\nprediction to a desired output. For classification tasks, CFEs determine how\nclose a given sample is to the decision boundary of a trained classifier.\nExisting methods are often sample-inefficient, requiring numerous evaluations\nof a black-box model -- an approach that is both costly and impractical when\naccess to the model is limited. We propose Adaptive sampling for Counterfactual\nExplanations (ACE), a sample-efficient algorithm combining Bayesian estimation\nand stochastic optimization to approximate the decision boundary with fewer\nqueries. By prioritizing informative points, ACE minimizes evaluations while\ngenerating accurate and feasible CFEs. Extensive empirical results show that\nACE achieves superior evaluation efficiency compared to state-of-the-art\nmethods, while maintaining effectiveness in identifying minimal and actionable\nchanges.", "AI": {"tldr": "ACE is a sample-efficient algorithm that uses Bayesian estimation and stochastic optimization to generate counterfactual explanations with fewer model queries than existing methods.", "motivation": "Existing counterfactual explanation methods require many model evaluations, which is costly and impractical when model access is limited.", "method": "Combines Bayesian estimation and stochastic optimization to approximate decision boundaries by prioritizing informative points, reducing the number of model queries needed.", "result": "Extensive empirical results show ACE achieves superior evaluation efficiency compared to state-of-the-art methods while maintaining effectiveness.", "conclusion": "ACE provides an efficient approach for generating accurate and feasible counterfactual explanations with minimal model evaluations."}}
{"id": "2509.26327", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.26327", "abs": "https://arxiv.org/abs/2509.26327", "authors": ["Charles Westphal", "Stephen Hailes", "Mirco Musolesi"], "title": "A Generalized Information Bottleneck Theory of Deep Learning", "comment": null, "summary": "The Information Bottleneck (IB) principle offers a compelling theoretical\nframework to understand how neural networks (NNs) learn. However, its practical\nutility has been constrained by unresolved theoretical ambiguities and\nsignificant challenges in accurate estimation. In this paper, we present a\n\\textit{Generalized Information Bottleneck (GIB)} framework that reformulates\nthe original IB principle through the lens of synergy, i.e., the information\nobtainable only through joint processing of features. We provide theoretical\nand empirical evidence demonstrating that synergistic functions achieve\nsuperior generalization compared to their non-synergistic counterparts.\nBuilding on these foundations we re-formulate the IB using a computable\ndefinition of synergy based on the average interaction information (II) of each\nfeature with those remaining. We demonstrate that the original IB objective is\nupper bounded by our GIB in the case of perfect estimation, ensuring\ncompatibility with existing IB theory while addressing its limitations. Our\nexperimental results demonstrate that GIB consistently exhibits compression\nphases across a wide range of architectures (including those with \\textit{ReLU}\nactivations where the standard IB fails), while yielding interpretable dynamics\nin both CNNs and Transformers and aligning more closely with our understanding\nof adversarial robustness.", "AI": {"tldr": "The paper introduces a Generalized Information Bottleneck (GIB) framework that reformulates the original IB principle using synergy concepts, addressing theoretical ambiguities and estimation challenges while achieving better generalization and interpretability.", "motivation": "The original Information Bottleneck principle has theoretical ambiguities and practical estimation challenges that limit its utility for understanding neural network learning.", "method": "Reformulate IB through synergy lens using average interaction information, creating a Generalized Information Bottleneck framework that upper bounds the original IB objective.", "result": "GIB shows consistent compression phases across architectures (including ReLU where standard IB fails), yields interpretable dynamics in CNNs and Transformers, and aligns with adversarial robustness understanding.", "conclusion": "The Generalized Information Bottleneck framework successfully addresses limitations of the original IB principle while maintaining theoretical compatibility and providing practical benefits for neural network analysis."}}
{"id": "2509.26337", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.26337", "abs": "https://arxiv.org/abs/2509.26337", "authors": ["Yuki Takezawa", "Anastasia Koloskova", "Xiaowen Jiang", "Sebastian U. Stich"], "title": "FedMuon: Federated Learning with Bias-corrected LMO-based Optimization", "comment": null, "summary": "Recently, a new optimization method based on the linear minimization oracle\n(LMO), called Muon, has been attracting increasing attention since it can train\nneural networks faster than existing adaptive optimization methods, such as\nAdam. In this paper, we study how Muon can be utilized in federated learning.\nWe first show that straightforwardly using Muon as the local optimizer of\nFedAvg does not converge to the stationary point since the LMO is a biased\noperator. We then propose FedMuon which can mitigate this issue. We also\nanalyze how solving the LMO approximately affects the convergence rate and find\nthat, surprisingly, FedMuon can converge for any number of Newton-Schulz\niterations, while it can converge faster as we solve the LMO more accurately.\nThrough experiments, we demonstrated that FedMuon can outperform the\nstate-of-the-art federated learning methods.", "AI": {"tldr": "FedMuon is a federated learning method that adapts the Muon optimization algorithm to overcome convergence issues caused by the biased linear minimization oracle (LMO) operator, achieving faster convergence than state-of-the-art methods.", "motivation": "Muon optimization method shows promise for faster neural network training than Adam, but direct application in federated learning with FedAvg fails to converge due to the biased nature of LMO operator.", "method": "Proposed FedMuon method that mitigates the convergence issues caused by LMO bias in federated settings, with analysis of approximate LMO solving using Newton-Schulz iterations.", "result": "FedMuon converges for any number of Newton-Schulz iterations and achieves faster convergence with more accurate LMO solutions, outperforming state-of-the-art federated learning methods in experiments.", "conclusion": "FedMuon successfully adapts Muon optimization to federated learning, overcoming LMO bias issues and demonstrating superior performance compared to existing federated learning approaches."}}
{"id": "2509.26340", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26340", "abs": "https://arxiv.org/abs/2509.26340", "authors": ["Xue Yan", "Zijing Ou", "Mengyue Yang", "Yan Song", "Haifeng Zhang", "Yingzhen Li", "Jun Wang"], "title": "Memory-Driven Self-Improvement for Decision Making with Large Language Models", "comment": null, "summary": "Large language models (LLMs) have emerged as effective action policies for\nsequential decision-making (SDM) tasks due to their extensive prior knowledge.\nHowever, this broad yet general knowledge is often insufficient for specific\ndecision-making tasks with limited task-related data, making it challenging to\nefficiently adapt LLMs to specific SDM tasks. To address this challenge, we\npropose a memory-driven self-improvement framework that combines LLM general\nprior knowledge with a compact memory of domain-specific experiences. Memory\nretains past interactions and associated Q-values, thereby capturing\ndecision-relevant knowledge that facilitates accurate value estimation and\ninforms the LLM prior refinement. The refined LLM prior, in turn, generates\nhigher-reward trajectories that further enrich memory, forming a natural\nself-improvement framework where memory and LLM prior mutually reinforce each\nother. Experiments show that our memory-driven approach significantly\noutperforms both traditional RL and LLM-based baselines, e.g., improving\nperformance by over 40\\% on in-distribution tasks and over 75\\% when\ngeneralized to unseen tasks in ALFWorld.", "AI": {"tldr": "A memory-driven self-improvement framework that combines LLM general knowledge with domain-specific experiences to enhance sequential decision-making performance.", "motivation": "LLMs' broad general knowledge is insufficient for specific decision-making tasks with limited data, making efficient adaptation challenging.", "method": "Proposes a framework where memory retains past interactions and Q-values, capturing decision-relevant knowledge to refine LLM priors, which then generate better trajectories to enrich memory.", "result": "Significantly outperforms traditional RL and LLM-based baselines, improving performance by over 40% on in-distribution tasks and over 75% on unseen tasks in ALFWorld.", "conclusion": "The memory-driven self-improvement framework effectively combines LLM general knowledge with domain-specific experiences, enabling mutual reinforcement between memory and LLM prior for enhanced sequential decision-making."}}
{"id": "2509.26351", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26351", "abs": "https://arxiv.org/abs/2509.26351", "authors": ["Joshua Sebastian", "Karma Tobden", "KMA Solaiman"], "title": "LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and MCI-Like Field Simulation", "comment": "Submitted to GenAI4Health@NeurIPS 2025. This is the first version of\n  the LLM-assisted emergency triage benchmark dataset and baseline models.\n  Authors: Joshua Sebastian, Karma Tobden, KMA Solaiman", "summary": "Research on emergency and mass casualty incident (MCI) triage has been\nlimited by the absence of openly usable, reproducible benchmarks. Yet these\nscenarios demand rapid identification of the patients most in need, where\naccurate deterioration prediction can guide timely interventions. While the\nMIMIC-IV-ED database is openly available to credentialed researchers,\ntransforming it into a triage-focused benchmark requires extensive\npreprocessing, feature harmonization, and schema alignment -- barriers that\nrestrict accessibility to only highly technical users.\n  We address these gaps by first introducing an open, LLM-assisted emergency\ntriage benchmark for deterioration prediction (ICU transfer, in-hospital\nmortality). The benchmark then defines two regimes: (i) a hospital-rich setting\nwith vitals, labs, notes, chief complaints, and structured observations, and\n(ii) an MCI-like field simulation limited to vitals, observations, and notes.\nLarge language models (LLMs) contributed directly to dataset construction by\n(i) harmonizing noisy fields such as AVPU and breathing devices, (ii)\nprioritizing clinically relevant vitals and labs, and (iii) guiding schema\nalignment and efficient merging of disparate tables.\n  We further provide baseline models and SHAP-based interpretability analyses,\nillustrating predictive gaps between regimes and the features most critical for\ntriage. Together, these contributions make triage prediction research more\nreproducible and accessible -- a step toward dataset democratization in\nclinical AI.", "AI": {"tldr": "This paper introduces an open, LLM-assisted emergency triage benchmark for deterioration prediction using MIMIC-IV-ED data, addressing accessibility barriers in MCI triage research.", "motivation": "Emergency and mass casualty incident triage research has been limited by the absence of openly usable, reproducible benchmarks, despite the critical need for rapid patient deterioration prediction to guide timely interventions.", "method": "Created an LLM-assisted benchmark with two regimes: hospital-rich setting (vitals, labs, notes, chief complaints, structured observations) and MCI-like field simulation (limited to vitals, observations, notes). LLMs were used for data harmonization, feature prioritization, and schema alignment.", "result": "Developed baseline models with SHAP-based interpretability analyses, revealing predictive gaps between hospital-rich and field simulation regimes and identifying critical triage features.", "conclusion": "The contributions make triage prediction research more reproducible and accessible, advancing dataset democratization in clinical AI."}}
{"id": "2509.26364", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26364", "abs": "https://arxiv.org/abs/2509.26364", "authors": ["Kirill Tamogashev", "Nikolay Malkin"], "title": "Data-to-Energy Stochastic Dynamics", "comment": null, "summary": "The Schr\\\"odinger bridge problem is concerned with finding a stochastic\ndynamical system bridging two marginal distributions that minimises a certain\ntransportation cost. This problem, which represents a generalisation of optimal\ntransport to the stochastic case, has received attention due to its connections\nto diffusion models and flow matching, as well as its applications in the\nnatural sciences. However, all existing algorithms allow to infer such dynamics\nonly for cases where samples from both distributions are available. In this\npaper, we propose the first general method for modelling Schr\\\"odinger bridges\nwhen one (or both) distributions are given by their unnormalised densities,\nwith no access to data samples. Our algorithm relies on a generalisation of the\niterative proportional fitting (IPF) procedure to the data-free case, inspired\nby recent developments in off-policy reinforcement learning for training of\ndiffusion samplers. We demonstrate the efficacy of the proposed data-to-energy\nIPF on synthetic problems, finding that it can successfully learn transports\nbetween multimodal distributions. As a secondary consequence of our\nreinforcement learning formulation, which assumes a fixed time discretisation\nscheme for the dynamics, we find that existing data-to-data Schr\\\"odinger\nbridge algorithms can be substantially improved by learning the diffusion\ncoefficient of the dynamics. Finally, we apply the newly developed algorithm to\nthe problem of sampling posterior distributions in latent spaces of generative\nmodels, thus creating a data-free image-to-image translation method. Code:\nhttps://github.com/mmacosha/d2e-stochastic-dynamics", "AI": {"tldr": "This paper proposes the first general method for modeling Schr\u00f6dinger bridges when one or both distributions are given by unnormalized densities without access to data samples, using a data-free iterative proportional fitting procedure inspired by off-policy reinforcement learning.", "motivation": "Existing Schr\u00f6dinger bridge algorithms require samples from both distributions, limiting their applicability to cases where only unnormalized densities are available, such as in Bayesian inference and generative model latent spaces.", "method": "The authors develop a data-to-energy iterative proportional fitting (IPF) procedure that generalizes the standard IPF to the data-free case, using a reinforcement learning formulation with fixed time discretization and learning the diffusion coefficient.", "result": "The method successfully learns transports between multimodal distributions on synthetic problems and substantially improves existing data-to-data Schr\u00f6dinger bridge algorithms. It also enables data-free image-to-image translation by sampling posterior distributions in generative model latent spaces.", "conclusion": "The proposed data-to-energy IPF provides the first general solution for Schr\u00f6dinger bridge problems with unnormalized densities, demonstrating effectiveness on multimodal distributions and practical applications in generative modeling."}}
{"id": "2509.26405", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26405", "abs": "https://arxiv.org/abs/2509.26405", "authors": ["Benno Kaech", "Luis Wyss", "Karsten Borgwardt", "Gianvito Grasso"], "title": "Refine Drugs, Don't Complete Them: Uniform-Source Discrete Flows for Fragment-Based Drug Discovery", "comment": null, "summary": "We introduce InVirtuoGen, a discrete flow generative model for fragmented\nSMILES for de novo and fragment-constrained generation, and\ntarget-property/lead optimization of small molecules. The model learns to\ntransform a uniform source over all possible tokens into the data distribution.\nUnlike masked models, its training loss accounts for predictions on all\nsequence positions at every denoising step, shifting the generation paradigm\nfrom completion to refinement, and decoupling the number of sampling steps from\nthe sequence length. For \\textit{de novo} generation, InVirtuoGen achieves a\nstronger quality-diversity pareto frontier than prior fragment-based models and\ncompetitive performance on fragment-constrained tasks. For property and lead\noptimization, we propose a hybrid scheme that combines a genetic algorithm with\na Proximal Property Optimization fine-tuning strategy adapted to discrete\nflows. Our approach sets a new state-of-the-art on the Practical Molecular\nOptimization benchmark, measured by top-10 AUC across tasks, and yields higher\ndocking scores in lead optimization than previous baselines. InVirtuoGen thus\nestablishes a versatile generative foundation for drug discovery, from early\nhit finding to multi-objective lead optimization. We further contribute to open\nscience by releasing pretrained checkpoints and code, making our results fully\nreproducible\\footnote{https://github.com/invirtuolabs/InVirtuoGen_results}.", "AI": {"tldr": "InVirtuoGen is a discrete flow generative model for small molecule generation and optimization, achieving state-of-the-art performance in de novo generation, fragment-constrained tasks, and lead optimization.", "motivation": "To develop a versatile generative foundation for drug discovery that can handle various tasks from early hit finding to multi-objective lead optimization, addressing limitations of previous fragment-based models.", "method": "Uses discrete flow generative model for fragmented SMILES, learning to transform uniform source tokens into data distribution. Combines genetic algorithm with Proximal Property Optimization fine-tuning for property optimization.", "result": "Achieves stronger quality-diversity pareto frontier than prior fragment-based models, sets new SOTA on Practical Molecular Optimization benchmark, and yields higher docking scores in lead optimization than previous baselines.", "conclusion": "InVirtuoGen establishes a versatile generative foundation for drug discovery and contributes to open science by releasing pretrained models and code."}}
{"id": "2509.26427", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26427", "abs": "https://arxiv.org/abs/2509.26427", "authors": ["Ioannis Mavrothalassitis", "Pol Puigdemont", "Noam Itzhak Levi", "Volkan Cevher"], "title": "Ascent Fails to Forget", "comment": "NeurIPS 2025", "summary": "Contrary to common belief, we show that gradient ascent-based unconstrained\noptimization methods frequently fail to perform machine unlearning, a\nphenomenon we attribute to the inherent statistical dependence between the\nforget and retain data sets. This dependence, which can manifest itself even as\nsimple correlations, undermines the misconception that these sets can be\nindependently manipulated during unlearning. We provide empirical and\ntheoretical evidence showing these methods often fail precisely due to this\noverlooked relationship. For random forget sets, this dependence means that\ndegrading forget set metrics (which, for a retrained model, should mirror test\nset metrics) inevitably harms overall test performance. Going beyond random\nsets, we consider logistic regression as an instructive example where a\ncritical failure mode emerges: inter-set dependence causes gradient\ndescent-ascent iterations to progressively diverge from the ideal retrained\nmodel. Strikingly, these methods can converge to solutions that are not only\nfar from the retrained ideal but are potentially even further from it than the\noriginal model itself, rendering the unlearning process actively detrimental. A\ntoy example further illustrates how this dependence can trap models in inferior\nlocal minima, inescapable via finetuning. Our findings highlight that the\npresence of such statistical dependencies, even when manifest only as\ncorrelations, can be sufficient for ascent-based unlearning to fail. Our\ntheoretical insights are corroborated by experiments on complex neural\nnetworks, demonstrating that these methods do not perform as expected in\npractice due to this unaddressed statistical interplay.", "AI": {"tldr": "Gradient ascent-based unlearning methods often fail due to statistical dependence between forget and retain datasets, causing performance degradation and divergence from the retrained model.", "motivation": "To challenge the misconception that forget and retain datasets can be independently manipulated during unlearning, and to investigate why gradient ascent methods frequently fail in practice.", "method": "Empirical and theoretical analysis of gradient ascent-based unlearning, including logistic regression examples and experiments on complex neural networks.", "result": "Statistical dependence between datasets causes gradient descent-ascent iterations to diverge from the retrained model, potentially making the unlearned model worse than the original.", "conclusion": "Statistical dependencies, even simple correlations, are sufficient to cause ascent-based unlearning methods to fail, highlighting a fundamental limitation in current approaches."}}
{"id": "2509.26432", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26432", "abs": "https://arxiv.org/abs/2509.26432", "authors": ["Guanxi Lu", "Hao", "Chen", "Yuto Karashima", "Zhican Wang", "Daichi Fujiki", "Hongxiang Fan"], "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size", "comment": "Preprint. Under review", "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.", "AI": {"tldr": "AdaBlock-dLLM is a training-free scheduler that adaptively adjusts block size during semi-autoregressive decoding in diffusion-based LLMs, addressing limitations of fixed block size approaches by aligning block boundaries with semantic structure, achieving up to 5.3% accuracy improvement.", "motivation": "To overcome two fundamental limitations in conventional semi-autoregressive decoding with fixed block size: late decoding overhead (delayed unmasking of high-confidence tokens) and premature decoding error (early commitment of low-confidence tokens).", "method": "Statistical analysis of confidence dynamics identifies volatility band regions that encode local semantic structure. AdaBlock-dLLM adaptively aligns block boundaries with semantic steps by adjusting block size during runtime without requiring training.", "result": "Extensive experiments show AdaBlock-dLLM achieves up to 5.3% accuracy improvement under the same throughput budget compared to fixed block size approaches.", "conclusion": "The semantics-aware adaptive scheduling approach and confidence-based analysis provide insights that could inspire future training strategies for diffusion-based LLMs, moving beyond just inference-time optimization."}}
{"id": "2509.26433", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26433", "abs": "https://arxiv.org/abs/2509.26433", "authors": ["Vincent Grari", "Tim Arni", "Thibault Laugel", "Sylvain Lamprier", "James Zou", "Marcin Detyniecki"], "title": "ACT: Agentic Classification Tree", "comment": "18 pages, 6 figures", "summary": "When used in high-stakes settings, AI systems are expected to produce\ndecisions that are transparent, interpretable, and auditable, a requirement\nincreasingly expected by regulations. Decision trees such as CART provide clear\nand verifiable rules, but they are restricted to structured tabular data and\ncannot operate directly on unstructured inputs such as text. In practice, large\nlanguage models (LLMs) are widely used for such data, yet prompting strategies\nsuch as chain-of-thought or prompt optimization still rely on free-form\nreasoning, limiting their ability to ensure trustworthy behaviors. We present\nthe Agentic Classification Tree (ACT), which extends decision-tree methodology\nto unstructured inputs by formulating each split as a natural-language\nquestion, refined through impurity-based evaluation and LLM feedback via\nTextGrad. Experiments on text benchmarks show that ACT matches or surpasses\nprompting-based baselines while producing transparent and interpretable\ndecision paths.", "AI": {"tldr": "ACT extends decision trees to unstructured data by using natural-language questions for splits, achieving competitive performance with LLM prompting while providing transparent decision paths.", "motivation": "AI systems need transparent and auditable decisions for high-stakes settings, but decision trees only work on structured data while LLMs lack interpretability.", "method": "Formulate decision tree splits as natural-language questions, refined through impurity-based evaluation and LLM feedback via TextGrad.", "result": "ACT matches or surpasses prompting-based baselines on text benchmarks while producing transparent decision paths.", "conclusion": "ACT successfully bridges the gap between interpretable decision trees and LLM capabilities for unstructured data, providing trustworthy AI decisions."}}
{"id": "2509.26442", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.26442", "abs": "https://arxiv.org/abs/2509.26442", "authors": ["Xinyu Liu", "Zixuan Xie", "Shangtong Zhang"], "title": "Extensions of Robbins-Siegmund Theorem with Applications in Reinforcement Learning", "comment": null, "summary": "The Robbins-Siegmund theorem establishes the convergence of stochastic\nprocesses that are almost supermartingales and is foundational for analyzing a\nwide range of stochastic iterative algorithms in stochastic approximation and\nreinforcement learning (RL). However, its original form has a significant\nlimitation as it requires the zero-order term to be summable. In many important\nRL applications, this summable condition, however, cannot be met. This\nlimitation motivates us to extend the Robbins-Siegmund theorem for almost\nsupermartingales where the zero-order term is not summable but only square\nsummable. Particularly, we introduce a novel and mild assumption on the\nincrements of the stochastic processes. This together with the square summable\ncondition enables an almost sure convergence to a bounded set. Additionally, we\nfurther provide almost sure convergence rates, high probability concentration\nbounds, and $L^p$ convergence rates. We then apply the new results in\nstochastic approximation and RL. Notably, we obtain the first almost sure\nconvergence rate, the first high probability concentration bound, and the first\n$L^p$ convergence rate for $Q$-learning with linear function approximation.", "AI": {"tldr": "Extension of Robbins-Siegmund theorem for almost supermartingales where zero-order term is square summable instead of summable, enabling convergence analysis in RL applications where summable condition fails.", "motivation": "Original Robbins-Siegmund theorem requires summable zero-order term, which cannot be met in many important reinforcement learning applications, limiting its applicability.", "method": "Introduce novel mild assumption on increments of stochastic processes combined with square summable condition to enable almost sure convergence to bounded set.", "result": "Achieve almost sure convergence to bounded set, provide almost sure convergence rates, high probability concentration bounds, and L^p convergence rates.", "conclusion": "New extended theorem successfully applied to stochastic approximation and RL, obtaining first convergence guarantees for Q-learning with linear function approximation."}}
{"id": "2509.26468", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26468", "abs": "https://arxiv.org/abs/2509.26468", "authors": ["Oleksandr Shchur", "Abdul Fatir Ansari", "Caner Turkmen", "Lorenzo Stella", "Nick Erickson", "Pablo Guerron", "Michael Bohlke-Schneider", "Yuyang Wang"], "title": "fev-bench: A Realistic Benchmark for Time Series Forecasting", "comment": null, "summary": "Benchmark quality is critical for meaningful evaluation and sustained\nprogress in time series forecasting, particularly given the recent rise of\npretrained models. Existing benchmarks often have narrow domain coverage or\noverlook important real-world settings, such as tasks with covariates.\nAdditionally, their aggregation procedures often lack statistical rigor, making\nit unclear whether observed performance differences reflect true improvements\nor random variation. Many benchmarks also fail to provide infrastructure for\nconsistent evaluation or are too rigid to integrate into existing pipelines. To\naddress these gaps, we propose fev-bench, a benchmark comprising 100\nforecasting tasks across seven domains, including 46 tasks with covariates.\nSupporting the benchmark, we introduce fev, a lightweight Python library for\nbenchmarking forecasting models that emphasizes reproducibility and seamless\nintegration with existing workflows. Usingfev, fev-bench employs principled\naggregation methods with bootstrapped confidence intervals to report model\nperformance along two complementary dimensions: win rates and skill scores. We\nreport results on fev-bench for various pretrained, statistical and baseline\nmodels, and identify promising directions for future research.", "AI": {"tldr": "fev-bench is a comprehensive time series forecasting benchmark with 100 tasks across 7 domains, including 46 tasks with covariates, addressing limitations in existing benchmarks through principled aggregation methods and a lightweight Python library called fev.", "motivation": "Existing time series forecasting benchmarks have narrow domain coverage, overlook important real-world settings like tasks with covariates, lack statistical rigor in aggregation procedures, and fail to provide consistent evaluation infrastructure.", "method": "Proposed fev-bench benchmark with 100 forecasting tasks across 7 domains, including 46 tasks with covariates. Developed fev, a lightweight Python library for benchmarking that emphasizes reproducibility and integration with existing workflows, using bootstrapped confidence intervals for principled aggregation.", "result": "The benchmark enables meaningful evaluation of forecasting models using win rates and skill scores with statistical confidence intervals. Results are reported for various pretrained, statistical and baseline models.", "conclusion": "fev-bench addresses critical gaps in time series forecasting evaluation and identifies promising directions for future research through comprehensive benchmarking."}}
{"id": "2509.26469", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26469", "abs": "https://arxiv.org/abs/2509.26469", "authors": ["Mohammad Hassan Vali", "Tom B\u00e4ckstr\u00f6m", "Arno Solin"], "title": "DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick", "comment": null, "summary": "Vector quantization is common in deep models, yet its hard assignments block\ngradients and hinder end-to-end training. We propose DiVeQ, which treats\nquantization as adding an error vector that mimics the quantization distortion,\nkeeping the forward pass hard while letting gradients flow. We also present a\nspace-filling variant (SF-DiVeQ) that assigns to a curve constructed by the\nlines connecting codewords, resulting in less quantization error and full\ncodebook usage. Both methods train end-to-end without requiring auxiliary\nlosses or temperature schedules. On VQ-VAE compression and VQGAN generation\nacross various data sets, they improve reconstruction and sample quality over\nalternative quantization approaches.", "AI": {"tldr": "DiVeQ treats quantization as adding error vectors to mimic distortion, enabling gradient flow while keeping hard assignments. SF-DiVeQ extends this by assigning to curves between codewords for lower quantization error and full codebook usage.", "motivation": "Vector quantization is widely used in deep models but its hard assignments block gradients and prevent end-to-end training, limiting its effectiveness.", "method": "Proposed DiVeQ treats quantization as adding an error vector that mimics quantization distortion, maintaining hard forward pass while allowing gradient flow. Also introduced SF-DiVeQ that assigns to curves constructed by lines connecting codewords.", "result": "Both methods improve reconstruction and sample quality over alternative quantization approaches in VQ-VAE compression and VQGAN generation across various datasets.", "conclusion": "DiVeQ and SF-DiVeQ enable end-to-end training of vector quantization models without requiring auxiliary losses or temperature schedules, achieving better performance than existing methods."}}
{"id": "2509.26499", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26499", "abs": "https://arxiv.org/abs/2509.26499", "authors": ["Gerrit Gerhartz", "Peter Lippmann", "Fred A. Hamprecht"], "title": "Equivariance by Local Canonicalization: A Matter of Representation", "comment": "To be presented at NeurReps Workshop 2025", "summary": "Equivariant neural networks offer strong inductive biases for learning from\nmolecular and geometric data but often rely on specialized, computationally\nexpensive tensor operations. We present a framework to transfers existing\ntensor field networks into the more efficient local canonicalization paradigm,\npreserving equivariance while significantly improving the runtime. Within this\nframework, we systematically compare different equivariant representations in\nterms of theoretical complexity, empirical runtime, and predictive accuracy. We\npublish the tensor_frames package, a PyTorchGeometric based implementation for\nlocal canonicalization, that enables straightforward integration of\nequivariance into any standard message passing neural network.", "AI": {"tldr": "A framework to convert tensor field networks into local canonicalization paradigm for improved efficiency while maintaining equivariance, with systematic comparison of equivariant representations and implementation in tensor_frames package.", "motivation": "Equivariant neural networks provide strong inductive biases for molecular and geometric data but suffer from computationally expensive tensor operations that limit their practical application.", "method": "Developed a framework to transfer tensor field networks into local canonicalization paradigm, preserving equivariance while improving runtime. Systematically compared different equivariant representations in terms of complexity, runtime, and accuracy.", "result": "The framework significantly improves runtime while maintaining equivariance. Published tensor_frames package for PyTorchGeometric that enables easy integration of equivariance into standard message passing neural networks.", "conclusion": "The local canonicalization paradigm offers an efficient alternative to traditional tensor field networks, making equivariant neural networks more practical for real-world applications while preserving their theoretical benefits."}}
{"id": "2509.26522", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26522", "abs": "https://arxiv.org/abs/2509.26522", "authors": ["Xi Wang", "James McInerney", "Lequn Wang", "Nathan Kallus"], "title": "Entropy After $\\langle \\texttt{/Think} \\rangle$ for reasoning model early exiting", "comment": null, "summary": "Large reasoning models show improved performance with longer chains of\nthought. However, recent work has highlighted (qualitatively) their tendency to\noverthink, continuing to revise answers even after reaching the correct\nsolution. We quantitatively confirm this inefficiency by tracking Pass@1 for\nanswers averaged over a large number of rollouts and find that the model often\nbegins to always produce the correct answer early in the reasoning, making\nextra reasoning a waste of tokens. To detect and prevent overthinking, we\npropose a simple and inexpensive novel signal -- Entropy After </Think> (EAT)\n-- for monitoring and deciding whether to exit reasoning early. By appending a\nstop thinking token (</think>) and monitoring the entropy of the following\ntoken as the model reasons, we obtain a trajectory that decreases and\nstabilizes when Pass@1 plateaus; thresholding its variance under an exponential\nmoving average yields a practical stopping rule. Importantly, our approach\nenables adaptively allocating compute based on the EAT trajectory, allowing us\nto spend compute in a more efficient way compared with fixing the token budget\nfor all questions. Empirically, on MATH500 and AIME2025, EAT reduces token\nusage by 13 - 21% without harming accuracy, and it remains effective in black\nbox settings where logits from the reasoning model are not accessible, and EAT\nis computed with proxy models.", "AI": {"tldr": "Proposes EAT (Entropy After </Think>) to detect and stop overthinking in large reasoning models, reducing token usage by 13-21% without accuracy loss.", "motivation": "Large reasoning models tend to overthink by continuing to revise answers even after reaching correct solutions, wasting computational tokens.", "method": "Append stop thinking token (</think>) and monitor entropy of following tokens; use thresholding on variance under exponential moving average as stopping rule.", "result": "Reduces token usage by 13-21% on MATH500 and AIME2025 without harming accuracy; effective even in black-box settings with proxy models.", "conclusion": "EAT provides practical method for adaptive compute allocation, enabling efficient token usage while maintaining model performance."}}
{"id": "2509.26524", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26524", "abs": "https://arxiv.org/abs/2509.26524", "authors": ["Seohyun Lee", "Wenzhi Fang", "Dong-Jun Han", "Seyyedali Hosseinalipour", "Christopher G. Brinton"], "title": "TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning", "comment": null, "summary": "Federated Learning (FL), despite demonstrating impressive capabilities in the\ntraining of multiple models in a decentralized manner, has been shown to\nproduce a final model not necessarily well-suited to the needs of each client.\nWhile extensive work has been conducted on how to create tailored personalized\nmodels, called Personalized Federated Learning (PFL), less attention has been\ngiven to personalization via fine-tuning of foundation models with multi-task\nand multi-modal properties. Moreover, there exists a lack of understanding in\nthe literature on how to fine-tune and personalize such models in a setting\nthat is heterogeneous across clients not only in data, but also in tasks and\nmodalities. To address this gap in the literature, we propose TAP (Two-Stage\nAdaptive Personalization), which (i) leverages mismatched model architectures\nbetween the clients and server to selectively conduct replacement operations\nwhen it benefits a client's local tasks and (ii) engages in post-FL knowledge\ndistillation for capturing beneficial general knowledge without compromising\npersonalization. We also introduce the first convergence analysis of the server\nmodel under its modality-task pair architecture, and demonstrate that as the\nnumber of modality-task pairs increases, its ability to cater to all tasks\nsuffers. Through extensive experiments, we demonstrate the effectiveness of our\nproposed algorithm across a variety of datasets and tasks in comparison to a\nmultitude of baselines. Implementation code is publicly available at\nhttps://github.com/lee3296/TAP.", "AI": {"tldr": "TAP is a two-stage adaptive personalization method for federated learning that addresses heterogeneity in data, tasks, and modalities by using mismatched architectures and post-FL knowledge distillation.", "motivation": "Current personalized federated learning approaches don't adequately address fine-tuning of foundation models with multi-task and multi-modal properties in heterogeneous settings across clients.", "method": "Two-stage adaptive personalization: (i) leverages mismatched model architectures for selective replacement operations, (ii) uses post-FL knowledge distillation to capture general knowledge without compromising personalization.", "result": "Demonstrated effectiveness across various datasets and tasks compared to multiple baselines, with convergence analysis showing server model performance degrades as modality-task pairs increase.", "conclusion": "TAP provides an effective solution for personalized federated learning in multi-task, multi-modal heterogeneous settings, with theoretical convergence guarantees and empirical validation."}}
{"id": "2509.26532", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26532", "abs": "https://arxiv.org/abs/2509.26532", "authors": ["Justin Tackett", "Benjamin Francis", "Luis Garcia", "David Grimsman", "Sean Warnick"], "title": "Machine-Learning Driven Load Shedding to Mitigate Instability Attacks in Power Grids", "comment": null, "summary": "Every year critical infrastructure becomes more complex and we grow to rely\non it more and more. With this reliance, it becomes an attractive target for\ncyberattacks from sophisticated actors, with one of the most attractive targets\nbeing the power grid. One class of attacks, instability attacks, is a newer\ntype of attack that has relatively few protections developed. We present a cost\neffective, data-driven approach to training a supervised machine learning model\nto retrofit load shedding decision systems in power grids with the capacity to\ndefend against instability attacks. We show a proof of concept on the IEEE 14\nBus System using the Achilles Heel Technologies Power Grid Analyzer, and show\nthrough an implementation of modified Prony analysis (MPA) that MPA is a viable\nmethod for detecting instability attacks and triggering defense mechanisms.", "AI": {"tldr": "A cost-effective machine learning approach to retrofit power grid load shedding systems for defending against instability cyberattacks, demonstrated on IEEE 14 Bus System using modified Prony analysis for detection.", "motivation": "Critical infrastructure like power grids are increasingly complex and targeted by sophisticated cyberattacks, particularly instability attacks which have few existing protections.", "method": "Data-driven supervised machine learning model to retrofit load shedding systems, using modified Prony analysis (MPA) for detecting instability attacks and triggering defenses.", "result": "Proof of concept on IEEE 14 Bus System shows MPA is viable for detecting instability attacks and triggering defense mechanisms.", "conclusion": "The proposed approach provides an effective defense mechanism against instability attacks in power grids through machine learning and modified Prony analysis."}}
{"id": "2509.26537", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26537", "abs": "https://arxiv.org/abs/2509.26537", "authors": ["Maxwell Adam", "Zach Furman", "Jesse Hoogland"], "title": "The Loss Kernel: A Geometric Probe for Deep Learning Interpretability", "comment": "25 pages, 11 figures", "summary": "We introduce the loss kernel, an interpretability method for measuring\nsimilarity between data points according to a trained neural network. The\nkernel is the covariance matrix of per-sample losses computed under a\ndistribution of low-loss-preserving parameter perturbations. We first validate\nour method on a synthetic multitask problem, showing it separates inputs by\ntask as predicted by theory. We then apply this kernel to Inception-v1 to\nvisualize the structure of ImageNet, and we show that the kernel's structure\naligns with the WordNet semantic hierarchy. This establishes the loss kernel as\na practical tool for interpretability and data attribution.", "AI": {"tldr": "The paper introduces the loss kernel, an interpretability method that measures data similarity using neural network loss covariance under parameter perturbations.", "motivation": "To develop an interpretable method for understanding how neural networks perceive data similarity and attribution.", "method": "Compute covariance matrix of per-sample losses under low-loss-preserving parameter perturbations, validated on synthetic multitask problem and applied to Inception-v1 on ImageNet.", "result": "Method successfully separates inputs by task in synthetic data and reveals ImageNet structure aligned with WordNet semantic hierarchy.", "conclusion": "The loss kernel is established as a practical tool for neural network interpretability and data attribution."}}
{"id": "2509.26541", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.26541", "abs": "https://arxiv.org/abs/2509.26541", "authors": ["Yida Wang", "Ke Hong", "Xiuhong Li", "Yuanchao Xu", "Wenxun Wang", "Guohao Dai", "Yu Wang"], "title": "TASP: Topology-aware Sequence Parallelism", "comment": null, "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.", "AI": {"tldr": "TASP is a topology-aware sequence parallelism method that improves communication efficiency for long-context LLMs by decomposing modern accelerator topologies into multiple orthogonal ring datapaths and decomposing Ring AllGather into concurrent ring-styled transfers.", "motivation": "Current sequence parallelism methods like Ring Attention suffer from low communication efficiency due to mismatch between Ring AllGather communication primitive and AlltoAll topology of modern accelerators, limiting practical applicability for long-context LLMs.", "method": "Proposes TASP which decomposes modern accelerator topology into multiple orthogonal ring datapaths that can transfer data concurrently without interference, and decomposes Ring AllGather primitive into same number of concurrent ring-styled data transfers.", "result": "Experimental results on NVIDIA H100 and AMD MI300X systems show TASP achieves higher communication efficiency than Ring Attention and its variants, with up to 3.58\u00d7 speedup over Ring Attention and Zigzag-Ring Attention.", "conclusion": "TASP effectively addresses the communication inefficiency in sequence parallelism for long-context LLMs by leveraging topology decomposition and primitive decomposition to fully utilize modern accelerator communication capacity."}}
{"id": "2509.26544", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26544", "abs": "https://arxiv.org/abs/2509.26544", "authors": ["Philipp Alexander Kreer", "Wilson Wu", "Maxwell Adam", "Zach Furman", "Jesse Hoogland"], "title": "Bayesian Influence Functions for Hessian-Free Data Attribution", "comment": "32 pages, 19 figures", "summary": "Classical influence functions face significant challenges when applied to\ndeep neural networks, primarily due to non-invertible Hessians and\nhigh-dimensional parameter spaces. We propose the local Bayesian influence\nfunction (BIF), an extension of classical influence functions that replaces\nHessian inversion with loss landscape statistics that can be estimated via\nstochastic-gradient MCMC sampling. This Hessian-free approach captures\nhigher-order interactions among parameters and scales efficiently to neural\nnetworks with billions of parameters. We demonstrate state-of-the-art results\non predicting retraining experiments.", "AI": {"tldr": "Proposes local Bayesian influence function (BIF) to overcome classical influence functions' limitations in deep neural networks by using loss landscape statistics instead of Hessian inversion.", "motivation": "Classical influence functions struggle with deep neural networks due to non-invertible Hessians and high-dimensional parameter spaces.", "method": "Extends classical influence functions by replacing Hessian inversion with loss landscape statistics estimated via stochastic-gradient MCMC sampling.", "result": "Achieves state-of-the-art results on predicting retraining experiments and scales efficiently to neural networks with billions of parameters.", "conclusion": "BIF provides a Hessian-free approach that captures higher-order parameter interactions and addresses key limitations of classical influence functions for deep learning applications."}}
{"id": "2509.26564", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.26564", "abs": "https://arxiv.org/abs/2509.26564", "authors": ["Florian Gr\u00f6tschla", "Longxiang Jiao", "Luca A. Lanzend\u00f6rfer", "Roger Wattenhofer"], "title": "Parametric Neural Amp Modeling with Active Learning", "comment": null, "summary": "We introduce Panama, an active learning framework to train parametric guitar\namp models end-to-end using a combination of an LSTM model and a WaveNet-like\narchitecture. With \\model, one can create a virtual amp by recording samples\nthat are determined through an ensemble-based active learning strategy to\nminimize the amount of datapoints needed (i.e., amp knob settings). Our\nstrategy uses gradient-based optimization to maximize the disagreement among\nensemble models, in order to identify the most informative datapoints. MUSHRA\nlistening tests reveal that, with 75 datapoints, our models are able to match\nthe perceptual quality of NAM, the leading open-source non-parametric amp\nmodeler.", "AI": {"tldr": "Panama is an active learning framework for training parametric guitar amp models using LSTM and WaveNet architectures, requiring only 75 datapoints to match the perceptual quality of leading non-parametric models.", "motivation": "To create virtual guitar amps with minimal data collection by using active learning to identify the most informative amp knob settings.", "method": "Combines LSTM and WaveNet-like architecture with ensemble-based active learning strategy using gradient-based optimization to maximize model disagreement and select informative datapoints.", "result": "With only 75 datapoints, Panama models achieve perceptual quality equivalent to NAM (leading open-source non-parametric amp modeler) in MUSHRA listening tests.", "conclusion": "Active learning with ensemble disagreement optimization enables efficient parametric guitar amp modeling with minimal data requirements while maintaining high perceptual quality."}}
{"id": "2509.26576", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.26576", "abs": "https://arxiv.org/abs/2509.26576", "authors": ["David S. Li", "Somdatta Goswami", "Qianying Cao", "Vivek Oommen", "Roland Assi", "Jay D. Humphrey", "George E. Karniadakis"], "title": "Importance of localized dilatation and distensibility in identifying determinants of thoracic aortic aneurysm with neural operators", "comment": null, "summary": "Thoracic aortic aneurysms (TAAs) arise from diverse mechanical and\nmechanobiological disruptions to the aortic wall that increase the risk of\ndissection or rupture. Evidence links TAA development to dysfunctions in the\naortic mechanotransduction axis, including loss of elastic fiber integrity and\ncell-matrix connections. Because distinct insults create different mechanical\nvulnerabilities, there is a critical need to identify interacting factors that\ndrive progression. Here, we use a finite element framework to generate\nsynthetic TAAs from hundreds of heterogeneous insults spanning varying degrees\nof elastic fiber damage and impaired mechanosensing. From these simulations, we\nconstruct spatial maps of localized dilatation and distensibility to train\nneural networks that predict the initiating combined insult. We compare several\narchitectures (Deep Operator Networks, UNets, and Laplace Neural Operators) and\nmultiple input data formats to define a standard for future subject-specific\nmodeling. We also quantify predictive performance when networks are trained\nusing only geometric data (dilatation) versus both geometric and mechanical\ndata (dilatation plus distensibility). Across all networks, prediction errors\nare significantly higher when trained on dilatation alone, underscoring the\nadded value of distensibility information. Among the tested models, UNet\nconsistently provides the highest accuracy across all data formats. These\nfindings highlight the importance of acquiring full-field measurements of both\ndilatation and distensibility in TAA assessment to reveal the mechanobiological\ndrivers of disease and support the development of personalized treatment\nstrategies.", "AI": {"tldr": "This paper develops a finite element framework to generate synthetic thoracic aortic aneurysms (TAAs) from various mechanical insults and uses neural networks to predict the initiating causes from spatial maps of dilatation and distensibility.", "motivation": "There is a critical need to identify interacting factors that drive TAA progression, as different mechanical insults create distinct vulnerabilities in the aortic wall.", "method": "Used finite element simulations to create synthetic TAAs from heterogeneous insults (elastic fiber damage and impaired mechanosensing), then trained neural networks (Deep Operator Networks, UNets, Laplace Neural Operators) on spatial maps of dilatation and distensibility to predict the initiating combined insult.", "result": "UNet consistently provided the highest accuracy across all data formats. Prediction errors were significantly higher when trained on dilatation alone versus both dilatation and distensibility, showing the added value of mechanical data.", "conclusion": "Full-field measurements of both dilatation and distensibility are crucial for TAA assessment to reveal mechanobiological drivers and support personalized treatment strategies."}}
{"id": "2509.26578", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26578", "abs": "https://arxiv.org/abs/2509.26578", "authors": ["Zheng Zhang", "Ziwei Shan", "Kaitao Song", "Yexin Li", "Kan Ren"], "title": "Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning", "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning capabilities of large language models (LLMs) by guiding their\nstep-by-step reasoning toward a final answer. However, existing PRMs either\ntreat each reasoning step in isolation, failing to capture inter-step\ndependencies, or struggle to align process rewards with the final outcome.\nConsequently, the reward signal fails to respect temporal causality in\nsequential reasoning and faces ambiguous credit assignment. These limitations\nmake downstream models vulnerable to reward hacking and lead to suboptimal\nperformance. In this work, we propose Conditional Reward Modeling (CRM) that\nframes LLM reasoning as a temporal process leading to a correct answer. The\nreward of each reasoning step is not only conditioned on the preceding steps\nbut also explicitly linked to the final outcome of the reasoning trajectory. By\nenforcing conditional probability rules, our design captures the causal\nrelationships among reasoning steps, with the link to the outcome allowing\nprecise attribution of each intermediate step, thereby resolving credit\nassignment ambiguity. Further, through this consistent probabilistic modeling,\nthe rewards produced by CRM enable more reliable cross-sample comparison.\nExperiments across Best-of-N sampling, beam search and reinforcement learning\ndemonstrate that CRM consistently outperforms existing reward models, offering\na principled framework for enhancing LLM reasoning. In particular, CRM is more\nrobust to reward hacking and delivers stable downstream improvements without\nrelying on verifiable rewards derived from ground truth.", "AI": {"tldr": "The paper proposes Conditional Reward Modeling (CRM) to address limitations in existing Process Reward Models (PRMs) by conditioning step rewards on preceding steps and linking them to final outcomes, improving credit assignment and robustness.", "motivation": "Existing PRMs fail to capture inter-step dependencies and struggle to align process rewards with final outcomes, leading to ambiguous credit assignment and vulnerability to reward hacking.", "method": "CRM frames LLM reasoning as a temporal process where each step's reward is conditioned on previous steps and explicitly linked to the final outcome, enforcing conditional probability rules to capture causal relationships.", "result": "Experiments across Best-of-N sampling, beam search and reinforcement learning show CRM consistently outperforms existing reward models, being more robust to reward hacking and delivering stable improvements.", "conclusion": "CRM provides a principled framework for enhancing LLM reasoning through better credit assignment and more reliable cross-sample comparison without relying on ground truth verifiable rewards."}}
{"id": "2509.26610", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26610", "abs": "https://arxiv.org/abs/2509.26610", "authors": ["Alexander Fishkov", "Kajetan Schweighofer", "Mykyta Ielanskyi", "Nikita Kotelevskii", "Mohsen Guizani", "Maxim Panov"], "title": "Uncertainty Quantification for Regression using Proper Scoring Rules", "comment": null, "summary": "Quantifying uncertainty of machine learning model predictions is essential\nfor reliable decision-making, especially in safety-critical applications.\nRecently, uncertainty quantification (UQ) theory has advanced significantly,\nbuilding on a firm basis of learning with proper scoring rules. However, these\nadvances were focused on classification, while extending these ideas to\nregression remains challenging. In this work, we introduce a unified UQ\nframework for regression based on proper scoring rules, such as CRPS,\nlogarithmic, squared error, and quadratic scores. We derive closed-form\nexpressions for the resulting uncertainty measures under practical parametric\nassumptions and show how to estimate them using ensembles of models. In\nparticular, the derived uncertainty measures naturally decompose into aleatoric\nand epistemic components. The framework recovers popular regression UQ measures\nbased on predictive variance and differential entropy. Our broad evaluation on\nsynthetic and real-world regression datasets provides guidance for selecting\nreliable UQ measures.", "AI": {"tldr": "A unified uncertainty quantification framework for regression based on proper scoring rules, with closed-form expressions under parametric assumptions and ensemble estimation, decomposing uncertainty into aleatoric and epistemic components.", "motivation": "While uncertainty quantification theory has advanced significantly for classification, extending these ideas to regression remains challenging, especially for reliable decision-making in safety-critical applications.", "method": "Developed a unified UQ framework using proper scoring rules (CRPS, logarithmic, squared error, quadratic scores), derived closed-form expressions under parametric assumptions, and estimated them using ensembles of models.", "result": "The framework naturally decomposes uncertainty into aleatoric and epistemic components, recovers popular regression UQ measures based on predictive variance and differential entropy, and provides guidance for selecting reliable UQ measures through evaluation on synthetic and real-world datasets.", "conclusion": "The proposed unified framework successfully extends proper scoring rule-based UQ to regression, providing practical uncertainty measures with clear decomposition and reliable performance across various datasets."}}
{"id": "2509.26625", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.26625", "abs": "https://arxiv.org/abs/2509.26625", "authors": ["Junlin Han", "Shengbang Tong", "David Fan", "Yufan Ren", "Koustuv Sinha", "Philip Torr", "Filippos Kokkinos"], "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training", "comment": "Project page: https://junlinhan.github.io/projects/lsbs/", "summary": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.", "AI": {"tldr": "LLMs develop visual priors from text-only training, enabling vision tasks with minimal multimodal data. These priors consist of separable perception and reasoning components with different scaling trends and origins.", "motivation": "To understand how LLMs acquire visual capabilities from text-only training and systematically analyze the composition and origins of visual priors for better MLLM development.", "method": "Conducted over 100 controlled experiments using 500,000 GPU-hours across full MLLM pipeline, analyzing LLM pre-training, visual alignment, and multimodal fine-tuning across five model scales and various data mixtures.", "result": "Visual reasoning ability develops from reasoning-centric data (code, math, academia) and scales progressively, while perception emerges diffusely from broad corpora. Text describing visual world is crucial but saturates quickly.", "conclusion": "Provides data-centric recipe for pre-training vision-aware LLMs and introduces MLE-Bench, offering systematic approach to deliberately cultivate visual priors from language pre-training for next-generation MLLMs."}}
{"id": "2509.26626", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26626", "abs": "https://arxiv.org/abs/2509.26626", "authors": ["Siddarth Venkatraman", "Vineet Jain", "Sarthak Mittal", "Vedant Shah", "Johan Obando-Ceron", "Yoshua Bengio", "Brian R. Bartoldson", "Bhavya Kailkhura", "Guillaume Lajoie", "Glen Berseth", "Nikolay Malkin", "Moksh Jain"], "title": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models", "comment": "24 pages, 9 figures", "summary": "Test-time scaling methods improve the capabilities of large language models\n(LLMs) by increasing the amount of compute used during inference to make a\nprediction. Inference-time compute can be scaled in parallel by choosing among\nmultiple independent solutions or sequentially through self-refinement. We\npropose Recursive Self-Aggregation (RSA), a test-time scaling method inspired\nby evolutionary methods that combines the benefits of both parallel and\nsequential scaling. Each step of RSA refines a population of candidate\nreasoning chains through aggregation of subsets to yield a population of\nimproved solutions, which are then used as the candidate pool for the next\niteration. RSA exploits the rich information embedded in the reasoning chains\n-- not just the final answers -- and enables bootstrapping from partially\ncorrect intermediate steps within different chains of thought. Empirically, RSA\ndelivers substantial performance gains with increasing compute budgets across\ndiverse tasks, model families and sizes. Notably, RSA enables\nQwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning\nmodels, including DeepSeek-R1 and o3-mini (high), while outperforming purely\nparallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning\nGym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the\nmodel to combine solutions via a novel aggregation-aware reinforcement learning\napproach yields significant performance gains. Code available at\nhttps://github.com/HyperPotatoNeo/RSA.", "AI": {"tldr": "RSA is a test-time scaling method that combines parallel and sequential scaling by recursively aggregating subsets of candidate reasoning chains to improve LLM performance with increasing compute budgets.", "motivation": "To improve LLM capabilities by leveraging both parallel and sequential scaling approaches during inference, exploiting rich information in reasoning chains rather than just final answers.", "method": "Recursive Self-Aggregation (RSA) refines populations of candidate reasoning chains through subset aggregation, enabling bootstrapping from partially correct intermediate steps across different chains of thought.", "result": "RSA delivers substantial performance gains across diverse tasks, model families and sizes, enabling smaller models like Qwen3-4B-Instruct-2507 to compete with larger reasoning models while outperforming purely parallel and sequential scaling strategies.", "conclusion": "RSA effectively combines the benefits of parallel and sequential test-time scaling, and training models with aggregation-aware reinforcement learning yields additional performance improvements."}}
{"id": "2509.26636", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26636", "abs": "https://arxiv.org/abs/2509.26636", "authors": ["Shangding Gu", "Xiaohan Wang", "Donghao Ying", "Haoyu Zhao", "Runing Yang", "Ming Jin", "Boyi Li", "Marco Pavone", "Serena Yeung-Levy", "Jun Wang", "Dawn Song", "Costas Spanos"], "title": "AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond", "comment": null, "summary": "Rapid advances in multimodal models demand benchmarks that rigorously\nevaluate understanding and reasoning in safety-critical, dynamic real-world\nsettings. We present AccidentBench, a large-scale benchmark that combines\nvehicle accident scenarios with Beyond domains, safety-critical settings in air\nand water that emphasize spatial and temporal reasoning (e.g., navigation,\norientation, multi-vehicle motion). The benchmark contains approximately 2000\nvideos and over 19000 human-annotated question--answer pairs spanning multiple\nvideo lengths (short/medium/long) and difficulty levels (easy/medium/hard).\nTasks systematically probe core capabilities: temporal, spatial, and intent\nunderstanding and reasoning. By unifying accident-centric traffic scenes with\nbroader safety-critical scenarios in air and water, AccidentBench offers a\ncomprehensive, physically grounded testbed for evaluating models under\nreal-world variability. Evaluations of state-of-the-art models (e.g.,\nGemini-2.5 Pro and GPT-5) show that even the strongest models achieve only\nabout 18% accuracy on the hardest tasks and longest videos, revealing\nsubstantial gaps in real-world temporal, spatial, and intent reasoning.\nAccidentBench is designed to expose these critical gaps and drive the\ndevelopment of multimodal models that are safer, more robust, and better\naligned with real-world safety-critical challenges. The code and dataset are\navailable at: https://github.com/SafeRL-Lab/AccidentBench", "AI": {"tldr": "AccidentBench is a large-scale multimodal benchmark with 2000 videos and 19000+ QA pairs that evaluates AI models on safety-critical scenarios in traffic, air, and water domains, focusing on temporal, spatial, and intent reasoning.", "motivation": "To address the need for rigorous evaluation of multimodal models in safety-critical, dynamic real-world settings that require understanding and reasoning about complex scenarios.", "method": "Created a benchmark combining vehicle accident scenarios with Beyond domains (air and water) containing videos of varying lengths and difficulty levels, with human-annotated question-answer pairs systematically probing temporal, spatial, and intent reasoning capabilities.", "result": "State-of-the-art models like Gemini-2.5 Pro and GPT-5 achieve only about 18% accuracy on the hardest tasks and longest videos, revealing substantial gaps in real-world temporal, spatial, and intent reasoning.", "conclusion": "AccidentBench effectively exposes critical gaps in current multimodal models and provides a comprehensive, physically grounded testbed to drive development of safer, more robust models aligned with real-world safety-critical challenges."}}
