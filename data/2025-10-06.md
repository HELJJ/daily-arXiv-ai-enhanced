<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 77]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.LG](#cs.LG) [Total: 90]
- [cs.CR](#cs.CR) [Total: 36]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning](https://arxiv.org/abs/2510.02324)
*Wannan Yang,Xinchi Qiu,Lei Yu,Yuchen Zhang,Oliver Aobo Yang,Narine Kokhlikyan,Nicola Cancedda,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: CASAL is an efficient algorithm that bakes activation steering benefits into model weights, reducing hallucinations by 30%-40% while being 30x more compute-efficient and 20x more data-efficient than baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate by providing incorrect answers instead of admitting ignorance. Prior activation steering methods require real-time monitoring during inference, which is impractical for deployment.

Method: CASAL connects interpretability with amortized optimization by training only a submodule of a single transformer layer to directly incorporate activation steering benefits into model weights, enabling models to answer known questions while abstaining from unknown ones.

Result: Reduces hallucination by 30%-40% across multiple short-form QA benchmarks, works effectively for both dense and MoE models, generalizes to OOD domains, and is applicable to both text-only and vision-language models.

Conclusion: CASAL represents a promising step forward for applying interpretability-inspired methods in practical deployment, offering efficient hallucination mitigation without real-time intervention during inference.

Abstract: Large Language Models (LLMs) exhibit impressive capabilities but often
hallucinate, confidently providing incorrect answers instead of admitting
ignorance. Prior work has shown that models encode linear representations of
their own knowledge and that activation steering can reduce hallucinations.
These approaches, however, require real-time monitoring and intervention during
inference. We introduce Contrastive Activation Steering for Amortized Learning
(CASAL), an efficient algorithm that connects interpretability with amortized
optimization. CASAL directly bakes the benefits of activation steering into
model's weights. Once trained, LLMs answer questions they know while abstaining
from answering those they do not. CASAL's light-weight design requires training
only a submodule of a single transformer layer and yet reduces hallucination by
30%-40% across multiple short-form QA benchmarks. CASAL is 30x more
compute-efficient and 20x more data-efficient than strong LoRA-based baselines
such as SFT and DPO, boosting its practical applicability in data scarce
domains. Importantly, CASAL also generalizes effectively to out-of-distribution
(OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in
both text-only and vision-language models. To our knowledge, CASAL is the first
steering-based training method that has been shown to be effective for both
dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step
forward for applying interpretability-inspired method for practical deployment
in production systems.

</details>


### [2] [Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval](https://arxiv.org/abs/2510.02326)
*Vivek Bhavsar,Joseph Ereifej,Aravanan Gurusami*

Main category: cs.CL

TL;DR: RA-FSM is a modular GPT-based research assistant that uses a finite-state control loop (Relevance → Confidence → Knowledge) to provide well-cited, transparent answers for technical domains like photonics, outperforming baseline models in expert evaluations.


<details>
  <summary>Details</summary>
Motivation: Large language models can accelerate literature synthesis but often hallucinate and mis-cite, limiting their usefulness in expert workflows where accuracy and proper citation are critical.

Method: RA-FSM wraps generation in a finite-state control loop with three states: Relevance, Confidence, and Knowledge. It uses vector retrieval and deterministic citation pipeline, filters out-of-scope queries, scores answerability, decomposes questions, and triggers retrieval only when needed. A ranked-tier ingestion workflow builds domain knowledge base from multiple sources.

Result: In blinded A/B reviews, domain experts preferred RA-FSM over both Notebook LM and vanilla GPT API baseline, citing better boundary-condition handling and more defensible evidence use. The system explores beyond Notebook LM while maintaining tunable latency and cost overheads.

Conclusion: RA-FSM provides transparent, well-cited answers for high-stakes technical work and is generalizable to other scientific domains, addressing key limitations of current LLMs in expert workflows.

Abstract: Large language models accelerate literature synthesis but can hallucinate and
mis-cite, limiting their usefulness in expert workflows. We present RA-FSM
(Research Assistant - Finite State Machine), a modular GPT-based research
assistant that wraps generation in a finite-state control loop: Relevance ->
Confidence -> Knowledge. The system is grounded in vector retrieval and a
deterministic citation pipeline. The controller filters out-of-scope queries,
scores answerability, decomposes questions, and triggers retrieval only when
needed, and emits answers with confidence labels and in-corpus, de-duplicated
references. A ranked-tier ingestion workflow constructs a domain knowledge base
from journals, conferences, indices, preprints, and patents, writing both to a
dense vector index and to a relational store of normalized metrics. We
implement the system for photonics and evaluate it on six task categories:
analytical reasoning, numerical analysis, methodological critique, comparative
synthesis, factual extraction, and application design. In blinded A/B reviews,
domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla
Default GPT API call single-pass baseline, citing stronger boundary-condition
handling and more defensible evidence use. Coverage and novelty analyses
indicate that RA-FSM explores beyond the NLM while incurring tunable latency
and cost overheads. The design emphasizes transparent, well-cited answers for
high-stakes technical work and is generalizable to other scientific domains.

</details>


### [3] [KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI](https://arxiv.org/abs/2510.02327)
*So Kuroki,Yotaro Kubo,Takuya Akiba,Yujin Tang*

Main category: cs.CL

TL;DR: A hybrid speech-to-speech system that combines real-time S2S responsiveness with LLM knowledge infusion, achieving near-cascaded system correctness while maintaining low latency.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between low-latency but knowledge-poor real-time S2S models and high-latency but knowledge-rich cascaded systems (ASR+LLM+TTS).

Method: Process speech through S2S transformer for immediate response while concurrently sending query to LLM, then inject LLM's text response to guide S2S speech generation in real-time.

Result: Substantially outperforms baseline S2S model in correctness, approaching cascaded system performance, while maintaining baseline-level latency.

Conclusion: The hybrid architecture successfully combines the strengths of both paradigms - real-time responsiveness and rich knowledge representation - without the full latency penalty.

Abstract: Real-time speech-to-speech (S2S) models excel at generating natural,
low-latency conversational responses but often lack deep knowledge and semantic
understanding. Conversely, cascaded systems combining automatic speech
recognition, a text-based Large Language Model (LLM), and text-to-speech
synthesis offer superior knowledge representation at the cost of high latency,
which disrupts the flow of natural interaction. This paper introduces a novel
hybrid architecture that bridges the gap between these two paradigms. Our
framework processes user speech through an S2S transformer for immediate
responsiveness while concurrently relaying the query to a powerful back-end
LLM. The LLM's text-based response is then injected in real time to guide the
S2S model's speech generation, effectively infusing its output with rich
knowledge without the full latency penalty of a cascaded system. We evaluated
our method using a speech-synthesized variant of the MT-Bench benchmark that
consists of multi-turn question-answering sessions. The results demonstrate
that our system substantially outperforms a baseline S2S model in response
correctness, approaching that of a cascaded system, while maintaining a latency
on par with the baseline.

</details>


### [4] [AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering](https://arxiv.org/abs/2510.02328)
*Ziqing Wang,Chengsheng Mao,Xiaole Wen,Yuan Luo,Kaize Ding*

Main category: cs.CL

TL;DR: AMANDA is a training-free agentic framework that enhances medical multimodal LLMs through intrinsic and extrinsic knowledge augmentation, achieving significant improvements in medical visual question answering tasks.


<details>
  <summary>Details</summary>
Motivation: Existing Med-MLLMs fail in low-resource settings due to two reasoning bottlenecks: intrinsic (ignoring medical image details) and extrinsic (failing to incorporate specialized medical knowledge).

Method: Uses LLM agents for medical knowledge augmentation - intrinsic augmentation via coarse-to-fine question decomposition, and extrinsic augmentation via biomedical knowledge graph retrieval.

Result: Extensive experiments across eight Med-VQA benchmarks show substantial improvements in both zero-shot and few-shot Med-VQA settings.

Conclusion: AMANDA effectively addresses medical reasoning bottlenecks through knowledge augmentation without requiring additional training, demonstrating strong performance in medical visual question answering.

Abstract: Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise
in medical visual question answering (Med-VQA). However, when deployed in
low-resource settings where abundant labeled data are unavailable, existing
Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks:
(i) the intrinsic reasoning bottleneck that ignores the details from the
medical image; (ii) the extrinsic reasoning bottleneck that fails to
incorporate specialized medical knowledge. To address those limitations, we
propose AMANDA, a training-free agentic framework that performs medical
knowledge augmentation via LLM agents. Specifically, our intrinsic medical
knowledge augmentation focuses on coarse-to-fine question decomposition for
comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds
the reasoning process via biomedical knowledge graph retrieval. Extensive
experiments across eight Med-VQA benchmarks demonstrate substantial
improvements in both zero-shot and few-shot Med-VQA settings. The code is
available at https://github.com/REAL-Lab-NU/AMANDA.

</details>


### [5] [SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification](https://arxiv.org/abs/2510.02329)
*Kanghoon Yoon,Minsub Kim,Sungjae Lee,Joonhyung Lee,Sunghyeon Woo,Yeonjun In,Se Jung Kwon,Chanyoung Park,Dongsoo Lee*

Main category: cs.CL

TL;DR: SelfJudge enables faster LLM inference through self-supervised judge verification that accepts semantically-preserving draft tokens, outperforming existing judge decoding methods.


<details>
  <summary>Details</summary>
Motivation: Existing judge decoding methods rely on human annotations or tasks with verifiable ground truths, limiting their generalizability across diverse NLP tasks.

Method: Train judge verifiers via self-supervision of the target model by measuring semantic preservation - assessing whether token-substituted responses preserve the meaning of original responses.

Result: SelfJudge achieves superior inference-accuracy trade-offs compared to judge decoding baselines.

Conclusion: SelfJudge offers a broadly applicable solution for faster LLM inference across diverse NLP tasks through self-supervised semantic verification.

Abstract: Speculative decoding accelerates LLM inference by verifying candidate tokens
from a draft model against a larger target model. Recent judge decoding boosts
this process by relaxing verification criteria by accepting draft tokens that
may exhibit minor discrepancies from target model output, but existing methods
are restricted by their reliance on human annotations or tasks with verifiable
ground truths, limiting generalizability across diverse NLP tasks. We propose
SelfJudge, which trains judge verifiers via self-supervision of the target
model. Our method measures semantic preservation by assessing whether
token-substituted responses preserve the meaning of original responses,
enabling automatic verifier training across diverse NLP tasks. Our experiments
show SelfJudge achieves superior inference-accuracy trade-offs than judge
decoding baselines, offering a broadly applicable solution for faster LLM
inference.

</details>


### [6] [EntropyLong: Effective Long-Context Training via Predictive Uncertainty](https://arxiv.org/abs/2510.02330)
*Junlong Jia,Ziyang Chen,Xing Wu,Chaochen Gao,Zijia Lin,Debing Zhang,Songlin Hu,Binghui Guo*

Main category: cs.CL

TL;DR: EntropyLong is a novel data construction method that uses predictive uncertainty to verify long-range dependencies in language model training, ensuring genuine information gain rather than spurious correlations.


<details>
  <summary>Details</summary>
Motivation: Current approaches for training long-context language models often fail to guarantee genuine long-range dependencies, as generic text concatenation or heuristic methods may create spurious correlations rather than meaningful connections.

Method: The method identifies high-entropy positions in documents, retrieves semantically relevant contexts from large corpora, and verifies their utility by assessing whether they reduce prediction entropy through model-in-the-loop verification.

Result: Models trained on the generated dataset (128K-length sequences from FineWebEdu and Cosmopedia) show significant improvements on RULER benchmarks, especially in tasks requiring distant information, and achieve substantial gains on LongBenchv2 after instruction fine-tuning.

Conclusion: Extensive ablation studies validate the necessity and effectiveness of entropy-based verification for long-context training, demonstrating that verified dependencies lead to enhanced long-context understanding in language models.

Abstract: Training long-context language models to capture long-range dependencies
requires specialized data construction. Current approaches, such as generic
text concatenation or heuristic-based variants, frequently fail to guarantee
genuine long-range dependencies. We propose EntropyLong, a novel data
construction method that leverages predictive uncertainty to verify dependency
quality. Our approach identifies high-entropy positions in documents, retrieves
semantically relevant contexts from large corpora, and verifies their utility
by assessing whether they reduce prediction entropy. This model-in-the-loop
verification ensures each dependency represents measurable information gain
rather than spurious correlation. We construct training samples with long-range
dependencies by combining original documents with these verified contextual
supplements. Using FineWebEdu and Cosmopedia, we generate a dataset of
128K-length sequences with verified dependencies. Models trained on this data
demonstrate significant improvements on RULER benchmarks, particularly in tasks
requiring distant information. Following instruction fine-tuning, our models
also achieve substantial gains on LongBenchv2, demonstrating enhanced
long-context understanding. Extensive ablation studies further validate the
necessity and effectiveness of entropybased verification for long-context
training.

</details>


### [7] [Synthetic Dialogue Generation for Interactive Conversational Elicitation & Recommendation (ICER)](https://arxiv.org/abs/2510.02331)
*Moonkyung Ryu,Chih-Wei Hsu,Yinlam Chow,Mohammad Ghavamzadeh,Craig Boutilier*

Main category: cs.CL

TL;DR: This paper presents a methodology for generating consistent and natural dialogues for conversational recommender systems using behavior simulators and LM-prompting, addressing the lack of public CRS data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenge of limited public CRS data for fine-tuning language models, as existing user simulators often produce inconsistent dialogue sequences.

Method: The method combines behavior simulators with LM-prompting to generate dialogues that are consistent with a user's underlying state, focusing on preference elicitation and example critiquing.

Result: The approach successfully generated a large, open-source CRS dataset, with rater evaluations showing high consistency, factuality, and naturalness in the dialogues.

Conclusion: The developed methodology effectively produces realistic and consistent CRS dialogues, providing a valuable resource for training LM-based conversational recommender systems.

Abstract: While language models (LMs) offer great potential for conversational
recommender systems (CRSs), the paucity of public CRS data makes fine-tuning
LMs for CRSs challenging. In response, LMs as user simulators qua data
generators can be used to train LM-based CRSs, but often lack behavioral
consistency, generating utterance sequences inconsistent with those of any real
user. To address this, we develop a methodology for generating natural
dialogues that are consistent with a user's underlying state using behavior
simulators together with LM-prompting. We illustrate our approach by generating
a large, open-source CRS data set with both preference elicitation and example
critiquing. Rater evaluation on some of these dialogues shows them to exhibit
considerable consistency, factuality and naturalness.

</details>


### [8] [A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography](https://arxiv.org/abs/2510.02332)
*Yapei Feng,Feng Jiang,Shanhao Wu,Hua Zhong*

Main category: cs.CL

TL;DR: Proposes look-ahead Sync method for neural linguistic steganography that overcomes capacity limitations of SyncPool while maintaining security guarantees, achieving significant embedding rate improvements (160% in English, 25% in Chinese).


<details>
  <summary>Details</summary>
Motivation: Address the fundamental challenge of tokenization ambiguity in modern tokenizers that causes catastrophic decoding failures, and overcome the capacity sacrifice in SyncPool which uses entire Shannon entropy for synchronization rather than payload embedding.

Method: Performs minimal synchronized sampling only on truly indistinguishable token sequences while strategically preserving all other discernible paths to maximize embedding capacity. Uses theoretical proofs for security and analyzes capacity gap.

Result: Consistently approaches theoretical capacity upper bound, significantly outperforms SyncPool with 160% improvement in English and 25% in Chinese embedding rates, especially in larger candidate pool settings.

Conclusion: Represents a significant step toward practical high-capacity provably secure linguistic steganography by balancing security guarantees with maximum embedding capacity.

Abstract: Neural linguistic steganography aims to embed information
  into natural text while preserving statistical undetectability. A fundamental
challenge in this ffeld stems from tokenization ambiguity in modern tokenizers,
which can lead to catastrophic decoding failures. The recent method, SyncPool,
addresses this ambiguity
  by employing a coarse-grained synchronization mechanism over groups of
ambiguous candidates. However, SyncPool sacriffces embedding capacity, as it
utilizes the entire Shannon entropy of an ambiguous group solely for
synchronization rather than for payload embedding. We propose a method named
look-ahead Sync, which overcomes the capacity limitation of SyncPool while
retaining its provable security guarantees. Our approach performs minimal
synchronized sampling only on truly indistinguishable token sequences, while
strategically preserving all other discernible paths to maximize embedding
capacity. We provide theoretical proofs for the security of our method and
analyze the gap between its achievable embedding capacity and the theoretical
upper bound. Experiments on English (using Llama 3) and Chinese (using Qwen
2.5) benchmarks show that our method consistently approaches the theoretical
capacity upper bound and signiffcantly outperforms SyncPool. The improvement in
embedding rate exceeds 160% in English and 25% in Chinese, particularly in
settings with larger candidate pools. This work represents a signiffcant step
toward practical high-capacity provably secure linguistic steganography.

</details>


### [9] [Human Mobility Datasets Enriched With Contextual and Social Dimensions](https://arxiv.org/abs/2510.02333)
*Chiara Pugliese,Francesco Lettich,Guido Rocchietti,Chiara Renso,Fabio Pinelli*

Main category: cs.CL

TL;DR: This paper presents two publicly available datasets of semantically enriched human trajectories from Paris and New York, featuring GPS traces, contextual layers (stops, moves, POIs, transportation modes, weather), and novel LLM-generated social media posts, available in tabular and RDF formats with an open-source pipeline.


<details>
  <summary>Details</summary>
Motivation: To provide comprehensive datasets that combine real-world movement data with semantic enrichment and LLM-generated content, enabling multimodal mobility analysis and supporting FAIR data practices in mobility research.

Method: Developed a reproducible pipeline to process publicly available GPS traces from OpenStreetMap, enrich them with contextual layers (stops, moves, POIs, transportation modes, weather), and generate synthetic social media posts using Large Language Models.

Result: Created two datasets covering Paris and New York in both tabular and RDF formats, supporting semantic reasoning and various research tasks including behavior modeling, mobility prediction, and knowledge graph construction.

Conclusion: This resource represents the first framework combining real-world movement data, structured semantic enrichment, LLM-generated text, and semantic web compatibility, providing a reusable foundation for advanced mobility research and applications.

Abstract: In this resource paper, we present two publicly available datasets of
semantically enriched human trajectories, together with the pipeline to build
them. The trajectories are publicly available GPS traces retrieved from
OpenStreetMap. Each dataset includes contextual layers such as stops, moves,
points of interest (POIs), inferred transportation modes, and weather data. A
novel semantic feature is the inclusion of synthetic, realistic social media
posts generated by Large Language Models (LLMs), enabling multimodal and
semantic mobility analysis. The datasets are available in both tabular and
Resource Description Framework (RDF) formats, supporting semantic reasoning and
FAIR data practices. They cover two structurally distinct, large cities: Paris
and New York. Our open source reproducible pipeline allows for dataset
customization, while the datasets support research tasks such as behavior
modeling, mobility prediction, knowledge graph construction, and LLM-based
applications. To our knowledge, our resource is the first to combine real-world
movement, structured semantic enrichment, LLM-generated text, and semantic web
compatibility in a reusable framework.

</details>


### [10] [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)
*Zhe Li,Wei Zhao,Yige Li,Jun Sun*

Main category: cs.CL

TL;DR: A novel framework that diagnoses undesirable LLM behaviors by analyzing representation gradients in activation space, enabling efficient sample-level and token-level attribution.


<details>
  <summary>Details</summary>
Motivation: Existing attribution methods based on parameter gradients are often noisy and computationally complex, making it challenging to diagnose root causes of LLM failures like harmful content generation and factual inaccuracies.

Method: Analyzes representation and its gradients directly in the model's activation space to provide semantically meaningful signals linking outputs to training data.

Result: Excels at sample-level attribution and enables fine-grained token-level analysis, precisely identifying specific samples and phrases that causally influence model behavior across tasks including harmful content tracking, backdoor poisoning detection, and knowledge contamination identification.

Conclusion: Provides a powerful diagnostic tool to understand, audit, and mitigate risks associated with LLMs, with code available for public use.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their deployment is frequently undermined by undesirable behaviors such as
generating harmful content, factual inaccuracies, and societal biases.
Diagnosing the root causes of these failures poses a critical challenge for AI
safety. Existing attribution methods, particularly those based on parameter
gradients, often fall short due to prohibitive noisy signals and computational
complexity. In this work, we introduce a novel and efficient framework that
diagnoses a range of undesirable LLM behaviors by analyzing representation and
its gradients, which operates directly in the model's activation space to
provide a semantically meaningful signal linking outputs to their training
data. We systematically evaluate our method for tasks that include tracking
harmful content, detecting backdoor poisoning, and identifying knowledge
contamination. The results demonstrate that our approach not only excels at
sample-level attribution but also enables fine-grained token-level analysis,
precisely identifying the specific samples and phrases that causally influence
model behavior. This work provides a powerful diagnostic tool to understand,
audit, and ultimately mitigate the risks associated with LLMs. The code is
available at https://github.com/plumprc/RepT.

</details>


### [11] [FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory](https://arxiv.org/abs/2510.02335)
*Xiao-Wen Yang,Zihao Zhang,Jianuo Cao,Zhi Zhou,Zenan Li,Lan-Zhe Guo,Yuan Yao,Taolue Chen,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.CL

TL;DR: The paper introduces FormalML, a Lean 4 benchmark for evaluating LLMs' ability to complete missing proof steps (subgoal completion) in machine learning theories, highlighting current limitations in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs can effectively serve as practical assistants for mathematicians by filling in missing steps in complex proofs, particularly in subgoal completion tasks.

Method: Created FormalML benchmark using Lean 4 with 4937 problems from machine learning theories, employing translation tactics to convert procedural proofs into declarative form for evaluation.

Result: Evaluation of state-of-the-art provers revealed persistent limitations in both accuracy and efficiency for subgoal completion tasks.

Conclusion: There is a need for more capable LLM-based theorem provers to effectively handle subgoal completion in complex mathematical contexts.

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in formal theorem proving. Yet their ability to serve as practical assistants
for mathematicians, filling in missing steps within complex proofs, remains
underexplored. We identify this challenge as the task of subgoal completion,
where an LLM must discharge short but nontrivial proof obligations left
unresolved in a human-provided sketch. To study this problem, we introduce
FormalML, a Lean 4 benchmark built from foundational theories of machine
learning. Using a translation tactic that converts procedural proofs into
declarative form, we extract 4937 problems spanning optimization and
probability inequalities, with varying levels of difficulty. FormalML is the
first subgoal completion benchmark to combine premise retrieval and complex
research-level contexts. Evaluation of state-of-the-art provers highlights
persistent limitations in accuracy and efficiency, underscoring the need for
more capable LLM-based theorem provers for effective subgoal completion,

</details>


### [12] [KurdSTS: The Kurdish Semantic Textual Similarity](https://arxiv.org/abs/2510.02336)
*Abdulhady Abas Abdullah,Hadi Veisi,Hussein M. Al*

Main category: cs.CL

TL;DR: First Kurdish STS dataset with 10,000 annotated sentence pairs across formal/informal registers, benchmarking multiple models to establish baseline performance for Kurdish semantic similarity.


<details>
  <summary>Details</summary>
Motivation: Address the lack of semantic textual similarity resources for low-resource languages like Kurdish, which remain underserved compared to high-resource languages.

Method: Created 10,000 Kurdish sentence pairs spanning formal and informal registers with similarity annotations, then benchmarked Sentence-BERT, multilingual BERT, and other strong baselines.

Result: Obtained competitive results while identifying challenges from Kurdish morphology, orthographic variation, and code-mixing. Established reproducible evaluation suite for Kurdish STS.

Conclusion: The dataset and baselines provide a strong foundation for future research on Kurdish semantics and low-resource NLP, filling a critical gap in resources for this underserved language.

Abstract: Semantic Textual Similarity (STS) measures the degree of meaning overlap
between two texts and underpins many NLP tasks. While extensive resources exist
for high-resource languages, low-resource languages such as Kurdish remain
underserved. We present, to our knowledge, the first Kurdish STS dataset:
10,000 sentence pairs spanning formal and informal registers, each annotated
for similarity. We benchmark Sentence-BERT, multilingual BERT, and other strong
baselines, obtaining competitive results while highlighting challenges arising
from Kurdish morphology, orthographic variation, and code-mixing. The dataset
and baselines establish a reproducible evaluation suite and provide a strong
starting point for future research on Kurdish semantics and low-resource NLP.

</details>


### [13] [CRACQ: A Multi-Dimensional Approach To Automated Document Assessment](https://arxiv.org/abs/2510.02337)
*Ishak Soltani,Francisco Belo,Bernardo Tavares*

Main category: cs.CL

TL;DR: CRACQ is a multi-dimensional evaluation framework that assesses documents across five traits: Coherence, Rigor, Appropriateness, Completeness, and Quality, providing interpretable automated evaluation beyond essays.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of single-score evaluation approaches and expand automated assessment beyond essays to diverse machine-generated text, enabling both holistic and trait-level analysis.

Method: Built on trait-based Automated Essay Scoring principles, CRACQ integrates linguistic, semantic, and structural signals using a rubric-driven methodology. Trained on 500 synthetic grant proposals and benchmarked against LLM-as-a-judge.

Result: CRACQ produces more stable and interpretable trait-level judgments than direct LLM evaluation, though challenges in reliability and domain scope remain.

Conclusion: CRACQ provides a promising multi-dimensional framework for automated document evaluation that offers improved interpretability over LLM-based approaches, but requires further refinement for reliability and broader domain applicability.

Abstract: This paper presents CRACQ, a multi-dimensional evaluation framework tailored
to evaluate documents across f i v e specific traits: Coherence, Rigor,
Appropriateness, Completeness, and Quality. Building on insights from
traitbased Automated Essay Scoring (AES), CRACQ expands its fo-cus beyond
essays to encompass diverse forms of machine-generated text, providing a
rubricdriven and interpretable methodology for automated evaluation. Unlike
singlescore approaches, CRACQ integrates linguistic, semantic, and structural
signals into a cumulative assessment, enabling both holistic and trait-level
analysis. Trained on 500 synthetic grant pro-posals, CRACQ was benchmarked
against an LLM-as-a-judge and further tested on both strong and weak real
applications. Preliminary results in-dicate that CRACQ produces more stable and
interpretable trait-level judgments than direct LLM evaluation, though
challenges in reliability and domain scope remain

</details>


### [14] [Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards](https://arxiv.org/abs/2510.02338)
*Samyak Jhaveri,Praphul Singh,Jangwon Kim,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.CL

TL;DR: An evaluation-integrated reinforcement learning framework using Group Relative Policy Optimization (GRPO) with DocLens evaluator for automated clinical documentation, improving factuality, completeness and reducing hallucinations without needing separate reward models or human references.


<details>
  <summary>Details</summary>
Motivation: To automate clinical documentation with large language models while ensuring precise alignment with clinical priorities like completeness and factual grounding, addressing the need for deterministic, dialogue-grounded evaluation.

Method: Uses Group Relative Policy Optimization (GRPO) coupled with DocLens, a claim-level evaluator that provides deterministic, dialogue-grounded rewards. Directly optimizes factual grounding and completeness without training separate reward models or relying on human-authored references, using a reward-gating strategy to reduce training cost.

Result: Improves clinical note quality, reduces training cost. Independent GPT-5 qualitative evaluation shows higher preference for GRPO outputs in factuality, completeness, and brevity, with fewer omissions and hallucinations. Improvements likely represent conservative lower bounds.

Conclusion: The framework is scalable to real-world settings and can incorporate custom objectives such as guideline adherence or billing preferences, providing an effective approach for automated clinical documentation with improved alignment to clinical priorities.

Abstract: Automating clinical documentation with large language models requires precise
alignment with priorities such as completeness and factual grounding. We
present an evaluation-integrated reinforcement learning framework for long-form
clinical text generation that couples Group Relative Policy Optimization (GRPO)
with DocLens, a claim-level evaluator that provides deterministic,
dialogue-grounded rewards. Our method directly optimizes factual grounding and
completeness without training a separate reward model or relying on
human-authored references. Empirically, the approach improves clinical note
quality and reduces training cost via a simple reward-gating strategy. An
independent GPT-5 qualitative evaluation further supports these gains, showing
higher preference for GRPO outputs in factuality, completeness, and brevity,
with fewer omissions and hallucinations. Because the benchmarks are relatively
clean and the base model already well aligned, these improvements likely
represent a conservative lower bound. The framework is scalable to real-world
settings and can incorporate custom objectives such as guideline adherence or
billing preferences.

</details>


### [15] [Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models](https://arxiv.org/abs/2510.02339)
*Kevin Zhou,Adam Dejl,Gabriel Freedman,Lihu Chen,Antonio Rago,Francesca Toni*

Main category: cs.CL

TL;DR: This paper explores uncertainty quantification (UQ) methods for argumentative LLMs (ArgLLMs) in claim verification tasks, finding that simple direct prompting outperforms more complex UQ approaches.


<details>
  <summary>Details</summary>
Motivation: To ensure reliability of LLMs by integrating UQ methods into argumentative LLMs for decision-making, where UQ plays a critical role in handling intricate and potentially contentious statements.

Method: Conducted experiments evaluating ArgLLMs' performance on claim verification tasks using different LLM UQ methods, with the experimental procedure itself serving as a novel way to assess UQ effectiveness.

Result: Direct prompting, despite its simplicity, proved to be an effective UQ strategy in ArgLLMs and outperformed considerably more complex approaches.

Conclusion: Simple UQ methods like direct prompting can be highly effective in argumentative LLM frameworks, challenging the assumption that more complex approaches are necessarily better for uncertainty quantification in LLMs.

Abstract: Research in uncertainty quantification (UQ) for large language models (LLMs)
is increasingly important towards guaranteeing the reliability of this
groundbreaking technology. We explore the integration of LLM UQ methods in
argumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making
based on computational argumentation in which UQ plays a critical role. We
conduct experiments to evaluate ArgLLMs' performance on claim verification
tasks when using different LLM UQ methods, inherently performing an assessment
of the UQ methods' effectiveness. Moreover, the experimental procedure itself
is a novel way of evaluating the effectiveness of UQ methods, especially when
intricate and potentially contentious statements are present. Our results
demonstrate that, despite its simplicity, direct prompting is an effective UQ
strategy in ArgLLMs, outperforming considerably more complex approaches.

</details>


### [16] [Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs](https://arxiv.org/abs/2510.02340)
*Xin Gao,Ruiyi Zhang,Daniel Du,Saurabh Mahindre,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.CL

TL;DR: This paper investigates whether LLMs can simulate earlier knowledge cutoffs through prompting to address contamination concerns in temporal prediction tasks, finding limited effectiveness for causally related knowledge.


<details>
  <summary>Details</summary>
Motivation: To address concerns that LLMs' accurate temporal predictions may result from memorization of pretraining data rather than genuine reasoning, leading to overestimated generalization capabilities.

Method: Constructed three evaluation datasets to assess prompting-based simulated knowledge cutoffs: (1) direct factual knowledge, (2) semantic shifts, and (3) causally related knowledge.

Result: Prompt-based simulated knowledge cutoffs show effectiveness for directly queried information but struggle to induce forgetting when the forgotten content is causally related to the query.

Conclusion: Findings highlight the need for more rigorous evaluation settings when applying LLMs for temporal prediction tasks, as current prompting methods have limitations in simulating true knowledge forgetting.

Abstract: Large Language Models (LLMs) are widely used for temporal prediction, but
their reliance on pretraining data raises contamination concerns, as accurate
predictions on pre-cutoff test data may reflect memorization rather than
reasoning, leading to an overestimation of their generalization capability.
With the recent emergence of prompting-based unlearning techniques, a natural
question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff?
In this work, we investigate the capability of prompting to simulate earlier
knowledge cutoff in LLMs. We construct three evaluation datasets to assess the
extent to which LLMs can forget (1) direct factual knowledge, (2) semantic
shifts, and (3) causally related knowledge. Results demonstrate that while
prompt-based simulated knowledge cutoffs show effectiveness when directly
queried with the information after that date, they struggle to induce
forgetting when the forgotten content is not directly asked but causally
related to the query. These findings highlight the need for more rigorous
evaluation settings when applying LLMs for temporal prediction tasks. The full
dataset and evaluation code are available at
https://github.com/gxx27/time_unlearn.

</details>


### [17] [DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning](https://arxiv.org/abs/2510.02341)
*Yifan Wang,Bolian Li,Junlin Wu,Zhaoxuan Tan,Zheli Liu,Ruqi Zhang,Ananth Grama,Qingkai Zeng*

Main category: cs.CL

TL;DR: DRIFT is a new preference training method that uses abundant implicit user dissatisfaction signals from real-world LLM deployments, dynamically sampling positive responses from the evolving policy to improve model performance without costly human annotations.


<details>
  <summary>Details</summary>
Motivation: Real-world LLM deployments generate abundant implicit user dissatisfaction signals (refinements, corrections, preferences) while explicit satisfaction feedback is scarce. Existing preference learning methods are poorly aligned with this data profile as they rely on costly human annotations or assume plentiful positive responses.

Method: DRIFT anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. It preserves preference margins and avoids gradient degeneration.

Result: DRIFT models achieve up to +6.23% (7B) / +7.61% (14B) on WildBench Task Score and up to +8.95% (7B) / +12.29% (14B) on AlpacaEval2 win rate over base models, outperforming iterative DPO and SPIN. 14B DRIFT models surpass GPT-4o-mini on WildBench.

Conclusion: DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal, preserving exploratory capacity and yielding more diverse high-reward solutions.

Abstract: Real-world large language model deployments (e.g., conversational AI systems,
code generation assistants) naturally generate abundant implicit user
dissatisfaction (DSAT) signals, as users iterate toward better answers through
refinements, corrections, and expressed preferences, while explicit
satisfaction (SAT) feedback is scarce. Existing preference learning approaches
are poorly aligned with this data profile, as they rely on costly human
annotations or assume plentiful positive responses. In this paper, we introduce
\textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative
pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world
DSAT signals and samples positives dynamically from the evolving policy.
Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets
and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) /
+7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B)
on AlpacaEval2 win rate over base models, outperforming strong baseline methods
such as iterative DPO and SPIN. At larger scales, the improvements are
particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on
WildBench. Further analysis shows that DRIFT also preserves exploratory
capacity, yielding more diverse high-reward solutions rather than collapsing to
narrow subsets. Theoretically, we demonstrate that this design preserves
preference margins and avoids the gradient degeneration. These results show
that DRIFT is an effective and scalable recipe for real-world post-training
that leverages the most abundant and informative signal. The code and data are
available at https://github.com/cacayaya/DRIFT.git.

</details>


### [18] [$\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training](https://arxiv.org/abs/2510.02343)
*Aurélien Bück-Kaeffer,Je Qin Chooi,Dan Zhao,Maximilian Puelma Touzel,Kellin Pelrine,Jean-François Godbout,Reihaneh Rabbany,Zachary Yang*

Main category: cs.CL

TL;DR: SIMPACT is a framework for creating privacy-respecting social media datasets to train LLM-based agents, with BluePrint as a concrete implementation using Bluesky political discourse data.


<details>
  <summary>Details</summary>
Motivation: Address the lack of standardized data resources for fine-tuning and evaluating LLMs as realistic social media agents, enabling ethical studies of social media dynamics.

Method: Introduces SIMPACT framework with next-action prediction task, clusters anonymized users into personas, uses pseudonymization and PII removal for privacy, and includes 12 social media interaction types.

Result: Created BluePrint dataset from Bluesky data focusing on political discourse, capturing authentic engagement patterns while ensuring privacy protection.

Conclusion: SIMPACT provides standardized data and evaluation protocols for advancing rigorous, ethically responsible social media simulations, with BluePrint serving as both benchmark and template for domain-specific studies.

Abstract: Large language models (LLMs) offer promising capabilities for simulating
social media dynamics at scale, enabling studies that would be ethically or
logistically challenging with human subjects. However, the field lacks
standardized data resources for fine-tuning and evaluating LLMs as realistic
social media agents. We address this gap by introducing SIMPACT, the
SIMulation-oriented Persona and Action Capture Toolkit, a privacy respecting
framework for constructing behaviorally-grounded social media datasets suitable
for training agent models. We formulate next-action prediction as a task for
training and evaluating LLM-based agents and introduce metrics at both the
cluster and population levels to assess behavioral fidelity and stylistic
realism. As a concrete implementation, we release BluePrint, a large-scale
dataset built from public Bluesky data focused on political discourse.
BluePrint clusters anonymized users into personas of aggregated behaviours,
capturing authentic engagement patterns while safeguarding privacy through
pseudonymization and removal of personally identifiable information. The
dataset includes a sizable action set of 12 social media interaction types
(likes, replies, reposts, etc.), each instance tied to the posting activity
preceding it. This supports the development of agents that use
context-dependence, not only in the language, but also in the interaction
behaviours of social media to model social media users. By standardizing data
and evaluation protocols, SIMPACT provides a foundation for advancing rigorous,
ethically responsible social media simulations. BluePrint serves as both an
evaluation benchmark for political discourse modeling and a template for
building domain specific datasets to study challenges such as misinformation
and polarization.

</details>


### [19] [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://arxiv.org/abs/2510.02345)
*Peijun Zhu,Ning Yang,Jiayu Wei,Jinghang Wu,Haijun Zhang*

Main category: cs.CL

TL;DR: A unified framework using dynamic expert clustering and structured compression to solve MoE LLMs' trilemma of load imbalance, parameter redundancy, and communication overhead.


<details>
  <summary>Details</summary>
Motivation: Address the trilemma in Mixture-of-Experts LLMs: load imbalance, parameter redundancy, and communication overhead.

Method: Dynamic expert clustering using fused parameter-activation similarity metric, weight decomposition into shared base matrix and low-rank residual adapters, two-stage hierarchical routing, and heterogeneous precision scheme.

Result: Matches standard MoE model quality while reducing total parameters by ~80%, improving throughput by 10-20%, and lowering expert load variance by over 3x.

Conclusion: Structural reorganization is a principled path toward scalable, efficient, and memory-effective MoE LLMs.

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load
imbalance, parameter redundancy, and communication overhead. We introduce a
unified framework based on dynamic expert clustering and structured compression
to address these issues cohesively. Our method employs an online clustering
procedure that periodically regroups experts using a fused metric of parameter
and activation similarity, which stabilizes expert utilization. To our
knowledge, this is one of the first frameworks to leverage the semantic
embedding capability of the router to dynamically reconfigure the model's
architecture during training for substantial efficiency gains. Within each
cluster, we decompose expert weights into a shared base matrix and extremely
low-rank residual adapters, achieving up to fivefold parameter reduction per
group while preserving specialization. This structure enables a two-stage
hierarchical routing strategy: tokens are first assigned to a cluster, then to
specific experts within it, drastically reducing the routing search space and
the volume of all-to-all communication. Furthermore, a heterogeneous precision
scheme, which stores shared bases in FP16 and residual factors in INT4, coupled
with dynamic offloading of inactive clusters, reduces peak memory consumption
to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our
framework matches the quality of standard MoE models while reducing total
parameters by approximately 80%, improving throughput by 10% to 20%, and
lowering expert load variance by a factor of over three. Our work demonstrates
that structural reorganization is a principled path toward scalable, efficient,
and memory-effective MoE LLMs.

</details>


### [20] [Small Language Models for Curriculum-based Guidance](https://arxiv.org/abs/2510.02347)
*Konstantinos Katharakis,Sippo Rossi,Raghava Rao Mukkamala*

Main category: cs.CL

TL;DR: SLMs with RAG can match LLMs in educational AI assistants while offering sustainability benefits.


<details>
  <summary>Details</summary>
Motivation: To explore sustainable AI teaching assistants using small language models instead of large models for educational applications.

Method: Developed and evaluated AI teaching assistants using RAG pipeline on 8 SLMs (7-17B parameters) and benchmarked against GPT-4o.

Result: SLMs with proper prompting and targeted retrieval can deliver accurate, pedagogically aligned responses comparable to LLMs.

Conclusion: SLMs are viable AI teaching assistants offering cost-effectiveness, privacy, and environmental sustainability for scalable personalized learning.

Abstract: The adoption of generative AI and large language models (LLMs) in education
is still emerging. In this study, we explore the development and evaluation of
AI teaching assistants that provide curriculum-based guidance using a
retrieval-augmented generation (RAG) pipeline applied to selected open-source
small language models (SLMs). We benchmarked eight SLMs, including LLaMA 3.1,
IBM Granite 3.3, and Gemma 3 (7-17B parameters), against GPT-4o. Our findings
show that with proper prompting and targeted retrieval, SLMs can match LLMs in
delivering accurate, pedagogically aligned responses. Importantly, SLMs offer
significant sustainability benefits due to their lower computational and energy
requirements, enabling real-time use on consumer-grade hardware without
depending on cloud infrastructure. This makes them not only cost-effective and
privacy-preserving but also environmentally responsible, positioning them as
viable AI teaching assistants for educational institutions aiming to scale
personalized learning in a sustainable and energy-efficient manner.

</details>


### [21] [mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations](https://arxiv.org/abs/2510.02348)
*Guy Dar*

Main category: cs.CL

TL;DR: mini-vec2vec is an efficient and robust linear alternative to vec2vec for aligning text embedding spaces without parallel data, achieving comparable or better results with significantly lower computational cost.


<details>
  <summary>Details</summary>
Motivation: vec2vec provides near-perfect alignment but is expensive and unstable, limiting its practical adoption and scalability.

Method: Three-stage approach: tentative matching of pseudo-parallel embedding vectors, transformation fitting, and iterative refinement to learn a linear transformation.

Result: Exceeds vec2vec by orders of magnitude in efficiency while matching or exceeding its results; highly robust and stable.

Conclusion: The method's stability and interpretable steps facilitate scaling and unlock new opportunities for adoption in various domains.

Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces
without parallel data. vec2vec finds a near-perfect alignment, but it is
expensive and unstable. We present mini-vec2vec, a simple and efficient
alternative that requires substantially lower computational cost and is highly
robust. Moreover, the learned mapping is a linear transformation. Our method
consists of three main stages: a tentative matching of pseudo-parallel
embedding vectors, transformation fitting, and iterative refinement. Our linear
alternative exceeds the original instantiation of vec2vec by orders of
magnitude in efficiency, while matching or exceeding their results. The
method's stability and interpretable algorithmic steps facilitate scaling and
unlock new opportunities for adoption in new domains and fields.

</details>


### [22] [LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL](https://arxiv.org/abs/2510.02350)
*Dzmitry Pihulski,Karol Charchut,Viktoria Novogrodskaia,Jan Kocoń*

Main category: cs.CL

TL;DR: LLMSQL is a cleaned and improved version of WikiSQL dataset for Text-to-SQL tasks, addressing structural and annotation issues to make it suitable for modern LLM evaluation.


<details>
  <summary>Details</summary>
Motivation: WikiSQL dataset has declined in usage due to case sensitivity inconsistencies, data type mismatches, syntax errors, and unanswered questions, making it unsuitable for modern LLM-based Text-to-SQL research.

Method: Systematic revision and transformation of WikiSQL through error classification and automated cleaning/re-annotation methods to create LLMSQL benchmark.

Result: Multiple LLMs (Gemma 3, LLaMA 3.2, Mistral 7B, etc.) were evaluated on the improved dataset, showing LLMSQL's effectiveness as an LLM-ready benchmark.

Conclusion: LLMSQL serves as a clean, modern benchmark for Text-to-SQL tasks, providing natural language questions and full SQL queries as plain text suitable for straightforward generation and evaluation with contemporary language models.

Abstract: Converting natural language questions into SQL queries (Text-to-SQL) enables
non-expert users to interact with relational databases and has long been a
central task for natural language interfaces to data. While the WikiSQL dataset
played a key role in early NL2SQL research, its usage has declined due to
structural and annotation issues, including case sensitivity inconsistencies,
data type mismatches, syntax errors, and unanswered questions. We present
LLMSQL, a systematic revision and transformation of WikiSQL designed for the
LLM era. We classify these errors and implement automated methods for cleaning
and re-annotation. To assess the impact of these improvements, we evaluated
multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral
7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and
others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready
benchmark: unlike the original WikiSQL, tailored for pointer-network models
selecting tokens from input, LLMSQL provides clean natural language questions
and full SQL queries as plain text, enabling straightforward generation and
evaluation for modern natural language-to-SQL models.

</details>


### [23] [Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs](https://arxiv.org/abs/2510.02351)
*Dzmitry Pihulski,Jan Kocoń*

Main category: cs.CL

TL;DR: LLMs assess political tweet offensiveness from different ideological perspectives. Larger reasoning models show better sensitivity to cultural and political nuances across languages.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs evaluate offensiveness in political discourse when adopting specific political and cultural viewpoints.

Method: Evaluated multiple LLMs (DeepSeek-R1, o4-mini, GPT-4.1-mini, Qwen3, Gemma, Mistral) using multilingual MD-Agreement dataset from 2020 US elections, testing offensive judgments from far-right, conservative, centrist, progressive perspectives in English, Polish, and Russian.

Result: Larger models with explicit reasoning abilities (DeepSeek-R1, o4-mini) are more consistent and sensitive to ideological/cultural variation. Smaller models fail to capture subtle distinctions. Reasoning capabilities improve personalization and interpretability of offensiveness judgments.

Conclusion: Reasoning mechanisms are crucial for adapting LLMs to nuanced sociopolitical text classification across languages and ideologies.

Abstract: We explore how large language models (LLMs) assess offensiveness in political
discourse when prompted to adopt specific political and cultural perspectives.
Using a multilingual subset of the MD-Agreement dataset centered on tweets from
the 2020 US elections, we evaluate several recent LLMs - including DeepSeek-R1,
o4-mini, GPT-4.1-mini, Qwen3, Gemma, and Mistral - tasked with judging tweets
as offensive or non-offensive from the viewpoints of varied political personas
(far-right, conservative, centrist, progressive) across English, Polish, and
Russian contexts. Our results show that larger models with explicit reasoning
abilities (e.g., DeepSeek-R1, o4-mini) are more consistent and sensitive to
ideological and cultural variation, while smaller models often fail to capture
subtle distinctions. We find that reasoning capabilities significantly improve
both the personalization and interpretability of offensiveness judgments,
suggesting that such mechanisms are key to adapting LLMs for nuanced
sociopolitical text classification across languages and ideologies.

</details>


### [24] [Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations](https://arxiv.org/abs/2510.02352)
*Yihao Wu,Tianrui Wang,Yizhou Peng,Yi-Wen Chao,Xuyi Zhuang,Xinsheng Wang,Shunshun Yin,Ziyang Ma*

Main category: cs.CL

TL;DR: This paper presents the first systematic study of biases in spoken dialogue models (SDMs), examining how paralinguistic features like age, gender, and accent affect model outputs in multi-turn conversations with repeated negative feedback.


<details>
  <summary>Details</summary>
Motivation: While biases in text-based LLMs have been studied, biases in spoken dialogue models with audio input/output remain unexplored. Paralinguistic features can exacerbate biases in multi-turn conversations, with implications for fairness in decision-making and recommendation tasks.

Method: Systematically evaluate biases in speech LLMs using Group Unfairness Score (GUS) for decisions and similarity-based normalized statistics rate (SNSR) for recommendations. Test both open-source (Qwen2.5-Omni, GLM-4-Voice) and closed-source (GPT-4o Audio, Gemini-2.5-Flash) models with multi-turn dialogues involving repeated negative feedback.

Result: Closed-source models generally exhibit lower bias, while open-source models are more sensitive to age and gender. Recommendation tasks amplify cross-group disparities, and biased decisions may persist in multi-turn conversations.

Conclusion: This work provides foundational insights into biases in end-to-end spoken dialogue models and releases the FairDialogue dataset and evaluation code to facilitate further research towards fair and reliable audio-based interactive systems.

Abstract: While biases in large language models (LLMs), such as stereotypes and
cultural tendencies in outputs, have been examined and identified, their
presence and characteristics in spoken dialogue models (SDMs) with audio input
and output remain largely unexplored. Paralinguistic features, such as age,
gender, and accent, can affect model outputs; when compounded by multi-turn
conversations, these effects may exacerbate biases, with potential implications
for fairness in decision-making and recommendation tasks. In this paper, we
systematically evaluate biases in speech LLMs and study the impact of
multi-turn dialogues with repeated negative feedback. Bias is measured using
Group Unfairness Score (GUS) for decisions and similarity-based normalized
statistics rate (SNSR) for recommendations, across both open-source models like
Qwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o
Audio and Gemini-2.5-Flash. Our analysis reveals that closed-source models
generally exhibit lower bias, while open-source models are more sensitive to
age and gender, and recommendation tasks tend to amplify cross-group
disparities. We found that biased decisions may persist in multi-turn
conversations. This work provides the first systematic study of biases in
end-to-end spoken dialogue models, offering insights towards fair and reliable
audio-based interactive systems. To facilitate further research, we release the
FairDialogue dataset and evaluation code.

</details>


### [25] [An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph](https://arxiv.org/abs/2510.02353)
*Oumar Kane,Mouhamad M. Allaya,Dame Samb,Mamadou Bousso*

Main category: cs.CL

TL;DR: AI and LLMs applied to improve legal text access in Senegal's judicial system, focusing on extracting and organizing legal documents with advanced NLP techniques.


<details>
  <summary>Details</summary>
Motivation: To address difficulties in accessing and organizing legal documents in Senegal's judicial system, improving access to judicial information for citizens and legal professionals.

Method: Extracted 7,967 articles from legal documents, developed a graph database with 2,872 nodes and 10,774 relationships, and used advanced triple extraction techniques with models like GPT-4o, GPT-4, and Mistral-Large.

Result: Successfully created a detailed knowledge graph visualizing legal text interconnections, demonstrating effectiveness of AI models in identifying relationships and metadata within legal documents.

Conclusion: The research establishes a solid framework using AI technologies to help Senegalese citizens and legal professionals better understand their rights and responsibilities through improved access to legal information.

Abstract: This study examines the application of artificial intelligence (AI) and large
language models (LLM) to improve access to legal texts in Senegal's judicial
system. The emphasis is on the difficulties of extracting and organizing legal
documents, highlighting the need for better access to judicial information. The
research successfully extracted 7,967 articles from various legal documents,
particularly focusing on the Land and Public Domain Code. A detailed graph
database was developed, which contains 2,872 nodes and 10,774 relationships,
aiding in the visualization of interconnections within legal texts. In
addition, advanced triple extraction techniques were utilized for knowledge,
demonstrating the effectiveness of models such as GPT-4o, GPT-4, and
Mistral-Large in identifying relationships and relevant metadata. Through these
technologies, the aim is to create a solid framework that allows Senegalese
citizens and legal professionals to more effectively understand their rights
and responsibilities.

</details>


### [26] [Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness](https://arxiv.org/abs/2510.02354)
*Shreya Saha,Shurui Li,Greta Tuckute,Yuanning Li,Ru-Yuan Zhang,Leila Wehbe,Evelina Fedorenko,Meenakshi Khosla*

Main category: cs.CL

TL;DR: The study finds that abstract meaning representations exist in the language cortex, revealed through neural response predictions using vision and language models. Aggregating multiple generated images or paraphrases improves prediction accuracy, suggesting richer semantic representations than current language models.


<details>
  <summary>Details</summary>
Motivation: To investigate the abstractness of meaning representations in the human language cortex and determine whether these representations are form-independent.

Method: Modeled neural responses to sentences using vision and language model embeddings. Generated multiple images for sentences and aggregated embeddings, created multiple paraphrases with enriched contextual details, and compared prediction accuracy.

Result: Aggregating across multiple generated images yields increasingly accurate predictions, sometimes rivaling large language models. Averaging embeddings across paraphrases improves accuracy, and enriching paraphrases with contextual details further increases prediction accuracy beyond original sentence embeddings.

Conclusion: The language cortex maintains highly abstract, form-independent meaning representations that are richer and broader than those in current language models.

Abstract: The human language system represents both linguistic forms and meanings, but
the abstractness of the meaning representations remains debated. Here, we
searched for abstract representations of meaning in the language cortex by
modeling neural responses to sentences using representations from vision and
language models. When we generate images corresponding to sentences and extract
vision model embeddings, we find that aggregating across multiple generated
images yields increasingly accurate predictions of language cortex responses,
sometimes rivaling large language models. Similarly, averaging embeddings
across multiple paraphrases of a sentence improves prediction accuracy compared
to any single paraphrase. Enriching paraphrases with contextual details that
may be implicit (e.g., augmenting "I had a pancake" to include details like
"maple syrup") further increases prediction accuracy, even surpassing
predictions based on the embedding of the original sentence, suggesting that
the language system maintains richer and broader semantic representations than
language models. Together, these results demonstrate the existence of highly
abstract, form-independent meaning representations within the language cortex.

</details>


### [27] [DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding](https://arxiv.org/abs/2510.02358)
*Guanghao Li,Zhihui Fu,Min Fang,Qibin Zhao,Ming Tang,Chun Yuan,Jun Wang*

Main category: cs.CL

TL;DR: DiffuSpec is a training-free framework that uses diffusion language models to generate multi-token drafts in a single forward pass for speculative decoding, achieving up to 3x speedup over autoregressive methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the latency limitations of autoregressive decoding in large language models, where each token requires serial forward passes, by enabling parallel draft generation.

Method: Uses pretrained diffusion language models to produce multi-token drafts with bidirectional conditioning, then applies causal-consistency path search to extract left-to-right paths and adaptive draft-length controller to optimize proposal size.

Result: Achieves up to 3x wall-clock speedup across benchmarks compared to autoregressive drafters.

Conclusion: Diffusion-based drafting provides a robust alternative to autoregressive drafters for speculative decoding, significantly improving inference speed without requiring training.

Abstract: As large language models (LLMs) scale up, accuracy improves, but the
autoregressive (AR) nature of decoding increases latency since each token
requires a serial forward pass. Speculative decoding addresses this by
employing a fast drafter to propose multi-token drafts, which are then verified
in parallel by the target model. However, many deployments still rely on AR
drafters, where sequential passes limit wall-clock gains. We revisit the
drafting stage and present DiffuSpec, a training-free drop-in framework that
uses a pretrained diffusion language model (DLM) to produce multi-token drafts
in a single forward pass, while remaining compatible with standard AR
verifiers. Because DLM drafts are generated under bidirectional conditioning,
parallel per-position candidates form a token lattice in which the locally
highest-probability token at each position need not form a causal left-to-right
path. Moreover, DLM drafting requires pre-specifying a draft length, inducing a
speed-quality trade-off. To address these challenges, we introduce two
practical components: (i) a causal-consistency path search (CPS) over this
lattice that extracts a left-to-right path aligned with AR verification; and
(ii) an adaptive draft-length (ADL) controller that adjusts next proposal size
based on recent acceptance feedback and realized generated length. Across
benchmarks, DiffuSpec yields up to 3x wall-clock speedup, establishing
diffusion-based drafting as a robust alternative to autoregressive drafters for
speculative decoding.

</details>


### [28] [Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis](https://arxiv.org/abs/2510.02359)
*Jiashu Ye,Tong Wu,Weiwen Chen,Hao Zhang,Zeteng Lin,Xingxing Li,Shujuan Weng,Manni Zhu,Xin Yuan,Xinlong Hong,Jingjie Li,Junyu Zheng,Zhijiong Huang,Jing Tang*

Main category: cs.CL

TL;DR: Emission-GPT is a knowledge-enhanced LLM agent for atmospheric emissions domain, built on 10,000+ documents, enabling natural language querying, visualization, and analysis of emissions data.


<details>
  <summary>Details</summary>
Motivation: Current emission knowledge is fragmented and specialized, making it difficult for non-experts to access and interpret emissions data efficiently, hindering research and management.

Method: Built on curated knowledge base with prompt engineering and question completion; enables interactive natural language analysis including querying inventories, source contribution analysis, and emission factor recommendations.

Result: Case study in Guangdong Province shows Emission-GPT can extract key insights like point source distributions and sectoral trends directly from raw data with simple prompts.

Conclusion: Emission-GPT's modular architecture automates manual workflows and serves as foundational tool for next-generation emission inventory development and scenario-based assessment.

Abstract: Improving air quality and addressing climate change relies on accurate
understanding and analysis of air pollutant and greenhouse gas emissions.
However, emission-related knowledge is often fragmented and highly specialized,
while existing methods for accessing and compiling emissions data remain
inefficient. These issues hinder the ability of non-experts to interpret
emissions information, posing challenges to research and management. To address
this, we present Emission-GPT, a knowledge-enhanced large language model agent
tailored for the atmospheric emissions domain. Built on a curated knowledge
base of over 10,000 documents (including standards, reports, guidebooks, and
peer-reviewed literature), Emission-GPT integrates prompt engineering and
question completion to support accurate domain-specific question answering.
Emission-GPT also enables users to interactively analyze emissions data via
natural language, such as querying and visualizing inventories, analyzing
source contributions, and recommending emission factors for user-defined
scenarios. A case study in Guangdong Province demonstrates that Emission-GPT
can extract key insights--such as point source distributions and sectoral
trends--directly from raw data with simple prompts. Its modular and extensible
architecture facilitates automation of traditionally manual workflows,
positioning Emission-GPT as a foundational tool for next-generation emission
inventory development and scenario-based assessment.

</details>


### [29] [Spiral of Silence in Large Language Model Agents](https://arxiv.org/abs/2510.02360)
*Mingze Zhong,Meng Fang,Zijing Shi,Yuxuan Huang,Shunfeng Zheng,Yali Du,Ling Chen,Jun Wang*

Main category: cs.CL

TL;DR: This paper investigates whether Spiral of Silence (SoS) dynamics can emerge in LLM collectives through statistical language generation, developing an evaluation framework that tests four conditions varying History and Persona signals.


<details>
  <summary>Details</summary>
Motivation: The classical SoS theory, developed for human societies, may not directly apply to LLMs, raising questions about whether similar conformity dynamics can emerge from purely statistical language generation in LLM collectives.

Method: Proposed an evaluation framework with four controlled conditions systematically varying History and Persona signals. Used trend tests (Mann-Kendall, Spearman's rank) and concentration measures (kurtosis, interquartile range) to assess opinion dynamics across open-source and closed-source models.

Result: History and Persona together produced strong majority dominance and replicated SoS patterns; History alone induced strong anchoring; Persona alone fostered diverse but uncorrelated opinions, showing that without historical anchoring, SoS dynamics cannot emerge.

Conclusion: The work bridges computational sociology and responsible AI design, highlighting the need to monitor and mitigate emergent conformity in LLM-agent systems.

Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views
often refrain from speaking out for fear of social isolation, enabling majority
positions to dominate public discourse. When the 'agents' are large language
models (LLMs), however, the classical psychological explanation is not directly
applicable, since SoS was developed for human societies. This raises a central
question: can SoS-like dynamics nevertheless emerge from purely statistical
language generation in LLM collectives? We propose an evaluation framework for
examining SoS in LLM agents. Specifically, we consider four controlled
conditions that systematically vary the availability of 'History' and 'Persona'
signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall
and Spearman's rank, along with concentration measures including kurtosis and
interquartile range. Experiments across open-source and closed-source models
show that history and persona together produce strong majority dominance and
replicate SoS patterns; history signals alone induce strong anchoring; and
persona signals alone foster diverse but uncorrelated opinions, indicating that
without historical anchoring, SoS dynamics cannot emerge. The work bridges
computational sociology and responsible AI design, highlighting the need to
monitor and mitigate emergent conformity in LLM-agent systems.

</details>


### [30] [ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference](https://arxiv.org/abs/2510.02361)
*Haojie Ouyang,Jianwei Lv,Lei Ren,Chen Wei,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CL

TL;DR: ChunkLLM is a lightweight training framework that addresses Transformer's quadratic complexity by using QK Adapter for feature compression and Chunk Adapter for boundary detection, achieving up to 4.48x speedup while maintaining 98.64% performance on long contexts.


<details>
  <summary>Details</summary>
Motivation: Transformer models suffer from computational inefficiency due to self-attention's quadratic complexity with input tokens. Existing methods have issues with semantic incompleteness or poor training-inference efficiency.

Method: Proposes ChunkLLM with two components: QK Adapter (Q-Adapter and K-Adapter) for feature compression and chunk attention, and Chunk Adapter for detecting chunk boundaries. Uses attention distillation for training QK Adapter and keeps backbone parameters frozen.

Result: Achieves comparable performance on short-text benchmarks, maintains 98.64% performance on long-context benchmarks with 48.58% KV cache retention rate, and attains 4.48x speedup on 120K long texts compared to vanilla Transformer.

Conclusion: ChunkLLM effectively addresses Transformer's computational inefficiency while preserving performance, making it a practical solution for long-text processing with significant speed improvements.

Abstract: Transformer-based large models excel in natural language processing and
computer vision, but face severe computational inefficiencies due to the
self-attention's quadratic complexity with input tokens. Recently, researchers
have proposed a series of methods based on block selection and compression to
alleviate this problem, but they either have issues with semantic
incompleteness or poor training-inference efficiency. To comprehensively
address these challenges, we propose ChunkLLM, a lightweight and pluggable
training framework. Specifically, we introduce two components: QK Adapter
(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each
Transformer layer, serving dual purposes of feature compression and chunk
attention acquisition. The latter operates at the bottommost layer of the
model, functioning to detect chunk boundaries by leveraging contextual semantic
information. During the training phase, the parameters of the backbone remain
frozen, with only the QK Adapter and Chunk Adapter undergoing training.
Notably, we design an attention distillation method for training the QK
Adapter, which enhances the recall rate of key chunks. During the inference
phase, chunk selection is triggered exclusively when the current token is
detected as a chunk boundary, thereby accelerating model inference.
Experimental evaluations are conducted on a diverse set of long-text and
short-text benchmark datasets spanning multiple tasks. ChunkLLM not only
attains comparable performance on short-text benchmarks but also maintains
98.64% of the performance on long-context benchmarks while preserving a 48.58%
key-value cache retention rate. Particularly, ChunkLLM attains a maximum
speedup of 4.48x in comparison to the vanilla Transformer in the processing of
120K long texts.

</details>


### [31] [A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History](https://arxiv.org/abs/2510.02362)
*Matei-Iulian Cocu,Răzvan-Cosmin Cristia,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: This study examines biases in Large Language Models by testing their responses to controversial Romanian historical questions across different languages and response formats, revealing significant inconsistencies in model behavior.


<details>
  <summary>Details</summary>
Motivation: To assess LLM biases in historical contexts, recognizing that history is often presented through altered cultural perspectives and that LLMs trained on biased datasets may lack neutrality.

Method: Three-stage research process: asking controversial Romanian historical questions in different languages and contexts, testing binary vs. numeric response formats, and analyzing response stability across formats.

Result: Binary response stability is relatively high but imperfect and varies by language; models often flip stance across languages or formats; numeric ratings frequently diverge from initial binary choices; most consistent models are not always the most accurate or neutral.

Conclusion: LLMs exhibit significant inconsistencies and biases in historical responses, with predisposition to format and language-dependent variations, highlighting the need for more robust and neutral model training.

Abstract: In this case study, we select a set of controversial Romanian historical
questions and ask multiple Large Language Models to answer them across
languages and contexts, in order to assess their biases. Besides being a study
mainly performed for educational purposes, the motivation also lies in the
recognition that history is often presented through altered perspectives,
primarily influenced by the culture and ideals of a state, even through large
language models. Since they are often trained on certain data sets that may
present certain ambiguities, the lack of neutrality is subsequently instilled
in users. The research process was carried out in three stages, to confirm the
idea that the type of response expected can influence, to a certain extent, the
response itself; after providing an affirmative answer to some given question,
an LLM could shift its way of thinking after being asked the same question
again, but being told to respond with a numerical value of a scale. Results
show that binary response stability is relatively high but far from perfect and
varies by language. Models often flip stance across languages or between
formats; numeric ratings frequently diverge from the initial binary choice, and
the most consistent models are not always those judged most accurate or
neutral. Our research brings to light the predisposition of models to such
inconsistencies, within a specific contextualization of the language for the
question asked.

</details>


### [32] [Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents](https://arxiv.org/abs/2510.02369)
*Kuntai Cai,Juncheng Liu,Xianglin Yang,Zhaojie Niu,Xiaokui Xiao,Xing Chen*

Main category: cs.CL

TL;DR: The paper identifies instance-level context as a crucial missing element in LLM agents, formalizes Instance-Level Context Learning (ILCL), and introduces a method that uses guided exploration with TODO forests to create reusable context documents, achieving significant performance improvements across multiple environments.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents rely on environment-level manuals and task-level guidance but lack instance-level context - verifiable facts specific to environment instances like object locations and local rules. This absence causes failures in complex tasks where success depends on precise, persistent facts beyond just reasoning over global rules.

Method: Proposes Instance-Level Context Learning (ILCL) using guided exploration with a compact TODO forest to prioritize actions and a lightweight plan-act-extract loop to execute them. This automatically produces high-precision, reusable context documents that amortize exploration costs across multiple downstream tasks.

Result: Experiments across TextWorld, ALFWorld, and Crafter show consistent gains: ReAct's mean success rate in TextWorld rises from 37% to 95%, IGE improves from 81% to 95%. The method transforms one-off exploration into persistent, reusable knowledge.

Conclusion: By complementing existing contexts with instance-level knowledge, the method enables more reliable and efficient LLM agents. The approach demonstrates that transforming exploration into reusable context documents significantly improves agent performance across various environments.

Abstract: Large language model (LLM) agents typically receive two kinds of context: (i)
environment-level manuals that define interaction interfaces and global rules,
and (ii) task-level guidance or demonstrations tied to specific goals. In this
work, we identify a crucial but overlooked third type of context,
instance-level context, which consists of verifiable and reusable facts tied to
a specific environment instance, such as object locations, crafting recipes,
and local rules. We argue that the absence of instance-level context is a
common source of failure for LLM agents in complex tasks, as success often
depends not only on reasoning over global rules or task prompts but also on
making decisions based on precise and persistent facts. Acquiring such context
requires more than memorization: the challenge lies in efficiently exploring,
validating, and formatting these facts under tight interaction budgets. We
formalize this problem as Instance-Level Context Learning (ILCL) and introduce
our task-agnostic method to solve it. Our method performs a guided exploration,
using a compact TODO forest to intelligently prioritize its next actions and a
lightweight plan-act-extract loop to execute them. This process automatically
produces a high-precision context document that is reusable across many
downstream tasks and agents, thereby amortizing the initial exploration cost.
Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent
gains in both success and efficiency: for instance, ReAct's mean success rate
in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By
transforming one-off exploration into persistent, reusable knowledge, our
method complements existing contexts to enable more reliable and efficient LLM
agents.

</details>


### [33] [Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models](https://arxiv.org/abs/2510.02370)
*Minsung Kim,Dong-Kyum Kim,Jea Kwon,Nakyeong Yang,Kyomin Jung,Meeyoung Cha*

Main category: cs.CL

TL;DR: This paper presents a controlled study on how training conditions affect language models' arbitration between parametric knowledge (from pretraining) and in-context knowledge (from retrieval), revealing that certain "non-ideal" training properties like repetition and inconsistency actually help develop robust knowledge integration strategies.


<details>
  <summary>Details</summary>
Motivation: Large language models face conflicts between their parametric knowledge and retrieved in-context knowledge, leading to either uncritical acceptance of misinformation or rigid adherence to parametric knowledge. There's a lack of systematic understanding about what shapes knowledge-arbitration strategies during training, risking wasted computational resources.

Method: The authors trained transformer-based language models on a synthetic biographies corpus while systematically controlling various training conditions, including intra-document repetition, inconsistent information, and distributional skew.

Result: Experiments showed that intra-document repetition fosters both parametric and in-context capabilities, and training on corpora with inconsistent information or distributional skew encourages robust strategies for leveraging both knowledge sources.

Conclusion: Rather than removing "non-ideal" training properties like repetition and inconsistency, these should be viewed as important for learning robust knowledge arbitration. The findings provide empirical guidance for pretraining models that harmoniously integrate parametric and in-context knowledge.

Abstract: Large language models often encounter conflicts between in-context knowledge
retrieved at inference time and parametric knowledge acquired during
pretraining. Models that accept external knowledge uncritically are vulnerable
to misinformation, whereas models that adhere rigidly to parametric knowledge
fail to benefit from retrieval. Despite the widespread adoption of
retrieval-augmented generation, we still lack a systematic understanding of
what shapes knowledge-arbitration strategies during training. This gap risks
producing pretrained models with undesirable arbitration behaviors and,
consequently, wasting substantial computational resources after the pretraining
budget has already been spent. To address this problem, we present the first
controlled study of how training conditions influence models' use of in-context
and parametric knowledge, and how they arbitrate between them. We train
transformer-based language models on a synthetic biographies corpus while
systematically controlling various conditions. Our experiments reveal that
intra-document repetition of facts fosters the development of both parametric
and in-context capabilities. Moreover, training on a corpus that contains
inconsistent information or distributional skew encourages models to develop
robust strategies for leveraging parametric and in-context knowledge. Rather
than viewing these non-ideal properties as artifacts to remove, our results
indicate that they are important for learning robust arbitration. These
insights offer concrete, empirical guidance for pretraining models that
harmoniously integrate parametric and in-context knowledge.

</details>


### [34] [Pretraining with hierarchical memories: separating long-tail and common knowledge](https://arxiv.org/abs/2510.02375)
*Hadi Pouransari,David Grangier,C Thomas,Michael Kirchhof,Oncel Tuzel*

Main category: cs.CL

TL;DR: Small language models augmented with hierarchical parametric memory banks achieve comparable performance to larger models by storing world knowledge in external memory rather than model parameters.


<details>
  <summary>Details</summary>
Motivation: Current language models require scaling parameters to store world knowledge, which is inefficient and impractical for edge devices with limited memory and compute resources.

Method: Memory-augmented architecture with small language models that access large hierarchical parametric memory banks, fetching context-dependent memory blocks during pretraining and inference.

Result: A 160M-parameter model with 18M memory from a 4.6B memory bank performs comparably to models with 2x more parameters; hierarchical feed-forward memories scale to over 21B parameters and work across transformer architectures.

Conclusion: Memory-augmented language models provide an efficient alternative to parameter scaling, enabling small models to access extensive world knowledge while maintaining performance comparable to much larger models.

Abstract: The impressive performance gains of modern language models currently rely on
scaling parameters: larger models store more world knowledge and reason better.
Yet compressing all world knowledge into parameters is unnecessary, as only a
fraction is used per prompt, and impractical for edge devices with limited
inference-time memory and compute. We address this shortcoming by a
memory-augmented architecture and a pretraining strategy aligned with existing
hardware paradigms. We introduce small language models that access large
hierarchical parametric memory banks encoding world knowledge. During
pretraining and inference, we fetch a small, context-dependent memory block and
add it to the model. Our pretraining learns to store long-tail world knowledge
in the memory parameters, while the small language model acts as an anchor
capturing common knowledge and general reasoning abilities. Through
trillion-token-scale experiments, we show significant gains: a 160M-parameters
model augmented with an 18M-parameters memory fetched from a 4.6B memory bank
obtains comparable performance to a regular model with more than 2x the
parameters. Through extensive experiments, we study the optimal type and size
of parametric memories in transformers, scaling them to over 21B parameters. We
find that our proposed hierarchical feed-forward memories work robustly across
transformer architectures, whether added during pretraining or post-hoc.

</details>


### [35] [Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems](https://arxiv.org/abs/2510.02377)
*Aakriti Agrawal,Rohith Aralikatti,Anirudh Satheesh,Souradip Chakraborty,Amrit Singh Bedi,Furong Huang*

Main category: cs.CL

TL;DR: A novel method for selecting the best response from multiple LLMs using calibrated log-likelihood scores, achieving 3-5% improvements across various benchmarks without requiring costly external verifiers.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for selecting reliable LLM responses rely on expensive external verifiers, human evaluators, or self-consistency techniques that require multiple samples from a single model. Multi-LLM systems produce diverse responses but often underperform compared to single LLM self-consistency.

Method: Using a calibrated log-likelihood score to implicitly leverage the inherent knowledge and confidence of multiple LLMs, providing a principled and computationally efficient way to select the best response.

Result: Demonstrated improvements of approximately 4%, 3%, and 5% across debate (multi-round LLM discussions) and non-debate (Best-of-N with multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets respectively.

Conclusion: The proposed method effectively selects the best response from multiple LLMs using calibrated log-likelihood scores, outperforming existing approaches while being computationally efficient and not requiring external resources.

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities, yet
selecting the most reliable response from multiple LLMs remains a challenge,
particularly in resource-constrained settings. Existing approaches often depend
on costly external verifiers, human evaluators, or self-consistency techniques
that require multiple samples from a single model. While multi-LLM systems
produce more diverse responses than single models and thus have greater
potential, they often underperform compared to single LLM self-consistency. We
propose a principled, novel and computationally efficient method to select the
best response from multiple different LLMs using a calibrated log-likelihood
score, implicitly leveraging the inherent knowledge and confidence of these
models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across
both debate (multi-round LLM discussions) and non-debate (Best-of-N with
multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets
respectively.

</details>


### [36] [Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2510.02388)
*Haoyue Bai,Haoyu Wang,Shengyu Chen,Zhengzhang Chen,Lu-An Tang,Wei Cheng,Haifeng Chen,Yanjie Fu*

Main category: cs.CL

TL;DR: A rule-driven routing framework for retrieval-augmented generation that intelligently selects between database and document sources for each query, achieving better accuracy with moderate computational cost.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with domain-specific QA requiring accurate, up-to-date information. While RAG helps, existing systems mainly use unstructured documents and overlook relational databases, which provide precise, timely, and queryable factual information crucial in domains like finance and healthcare.

Method: Proposed a rule-driven routing framework with three components: routing agent that scores augmentation paths using explicit rules, rule-making expert agent that refines rules using QA feedback, and path-level meta-cache that reuses routing decisions for similar queries to reduce latency and cost.

Result: Experiments on three QA benchmarks show the framework consistently outperforms static strategies and learned routing baselines, achieving higher accuracy while maintaining moderate computational cost.

Conclusion: The systematic analysis reveals that databases and documents offer complementary strengths, and selecting the most suitable source for each query is crucial. The proposed rule-driven routing framework effectively balances effectiveness and efficiency in domain-specific QA.

Abstract: Large Language Models (LLMs) have shown remarkable performance on general
Question Answering (QA), yet they often struggle in domain-specific scenarios
where accurate and up-to-date information is required. Retrieval-Augmented
Generation (RAG) addresses this limitation by enriching LLMs with external
knowledge, but existing systems primarily rely on unstructured documents, while
largely overlooking relational databases, which provide precise, timely, and
efficiently queryable factual information, serving as indispensable
infrastructure in domains such as finance, healthcare, and scientific research.
Motivated by this gap, we conduct a systematic analysis that reveals three
central observations: (i) databases and documents offer complementary strengths
across queries, (ii) naively combining both sources introduces noise and cost
without consistent accuracy gains, and (iii) selecting the most suitable source
for each query is crucial to balance effectiveness and efficiency. We further
observe that query types show consistent regularities in their alignment with
retrieval paths, suggesting that routing decisions can be effectively guided by
systematic rules that capture these patterns. Building on these insights, we
propose a rule-driven routing framework. A routing agent scores candidate
augmentation paths based on explicit rules and selects the most suitable one; a
rule-making expert agent refines the rules over time using QA feedback to
maintain adaptability; and a path-level meta-cache reuses past routing
decisions for semantically similar queries to reduce latency and cost.
Experiments on three QA benchmarks demonstrate that our framework consistently
outperforms static strategies and learned routing baselines, achieving higher
accuracy while maintaining moderate computational cost.

</details>


### [37] [KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning](https://arxiv.org/abs/2510.02392)
*Yinyi Luo,Zhexian Zhou,Hao Chen,Kai Qiu,Marios Savvides,Yixuan Li,Jindong Wang*

Main category: cs.CL

TL;DR: KnowledgeSmith is a unified framework that systematically analyzes how LLMs update knowledge through editing and unlearning, revealing nuanced insights about knowledge propagation, plasticity scaling, and consistency-capacity trade-offs.


<details>
  <summary>Details</summary>
Motivation: To understand the knowledge updating mechanism of LLMs, which remains largely unexplored due to insufficient, isolated, and small-scale evaluation. The paper aims to investigate whether LLMs update knowledge similarly to humans and how editing/unlearning differ as training data increases.

Method: Proposes KnowledgeSmith framework that casts editing and unlearning as constrained optimization problems. Uses an automatic dataset generator providing structured interventions across multiple graph levels and data scales to study how different modification strategies propagate through model knowledge.

Result: Extensive experiments show nuanced insights: LLMs do not exhibit similar updating as humans for different knowledge levels, and there exists consistency-capacity trade-off. The framework enables controlled studies of knowledge propagation patterns.

Conclusion: The findings offer suggestions for designing more reliable and scalable knowledge updating strategies for LLMs, highlighting the importance of understanding knowledge propagation mechanisms in model updates.

Abstract: Knowledge editing and machine unlearning are two popular approaches for large
language models (LLMs) to stay up-to-date. However, the knowledge updating
mechanism of LLMs remains largely unexplored due to insufficient, isolated, and
small-scale evaluation. For instance, are LLMs similar to humans in modifying
certain knowledge? What differs editing and unlearning as training data
increases? This paper proposes KnowledgeSmith, a unified framework to
systematically understand the updating mechanism of LLMs. We first cast editing
and unlearning as instances of one constrained optimization problem. Then, we
propose an automatic dataset generator that provides structured interventions
across multiple graph levels and data scales, enabling controlled studies of
how different modification strategies propagate through model knowledge.
Extensive experiments demonstrate nuanced insights over knowledge propagation,
plasticity scaling, consistency, and robustness. For instance, our results show
that LLMs do not exhibit similar updating as humans for different levels of
knowledge, and there exists consistency-capacity trade-off. We hope our
findings can offer suggestions to the design of more reliable and scalable
strategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git

</details>


### [38] [Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing](https://arxiv.org/abs/2510.02394)
*Manasi Patwardhan,Ayush Agarwal,Shabbirhussain Bhaisaheb,Aseem Arora,Lovekesh Vig,Sunita Sarawagi*

Main category: cs.CL

TL;DR: The paper proposes a systematic framework using structured domain statements at database level with sub-string match retrieval, showing better performance than existing ad-hoc approaches for NL-to-SQL translation.


<details>
  <summary>Details</summary>
Motivation: Current LLM performance for NL-to-SQL translation varies across databases due to domain-specific vocabulary, and existing benchmarks use unrealistic ad-hoc query hints for domain knowledge.

Method: Systematic framework associating structured domain statements at database level, with retrieval using sub-string level match to find relevant domain statements for user queries.

Result: Evaluation on 11 realistic DB schemas across 5 LLMs shows DB-level structured domain statements are more practical and accurate than ad-hoc approaches, and sub-string match retrieval provides significantly higher accuracy.

Conclusion: The proposed framework with structured domain statements and sub-string match retrieval effectively addresses domain knowledge challenges in NL-to-SQL translation, outperforming existing methods.

Abstract: The performance of Large Language Models (LLMs) for translating Natural
Language (NL) queries into SQL varies significantly across databases (DBs). NL
queries are often expressed using a domain specific vocabulary, and mapping
these to the correct SQL requires an understanding of the embedded domain
expressions, their relationship to the DB schema structure. Existing benchmarks
rely on unrealistic, ad-hoc query specific textual hints for expressing domain
knowledge. In this paper, we propose a systematic framework for associating
structured domain statements at the database level. We present retrieval of
relevant structured domain statements given a user query using sub-string level
match. We evaluate on eleven realistic DB schemas covering diverse domains
across five open-source and proprietary LLMs and demonstrate that (1) DB level
structured domain statements are more practical and accurate than existing
ad-hoc query specific textual domain statements, and (2) Our sub-string match
based retrieval of relevant domain statements provides significantly higher
accuracy than other retrieval approaches.

</details>


### [39] [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
*Sophie L. Wang,Phillip Isola,Brian Cheung*

Main category: cs.CL

TL;DR: Sensory prompts like 'see' or 'hear' can activate modality-appropriate representations in text-only LLMs, aligning them with specialist vision/audio encoders.


<details>
  <summary>Details</summary>
Motivation: To test whether text-only LLMs have latent multimodal structure that can be surfaced through sensory prompting, despite lacking direct perceptual experience.

Method: Using explicit sensory prompts (e.g., 'see' or 'hear') to cue LLMs to resolve next-token predictions as if conditioned on latent visual/auditory evidence, comparing representations with specialist encoders.

Result: Lightweight prompt engineering reliably activates modality-appropriate representations in purely text-trained LLMs, bringing them into closer alignment with vision and audio encoders.

Conclusion: Text-only LLMs contain implicit multimodal structure that can be explicitly accessed through sensory prompting, demonstrating latent perceptual capabilities despite training only on text.

Abstract: Large language models (LLMs) trained purely on text ostensibly lack any
direct perceptual experience, yet their internal representations are implicitly
shaped by multimodal regularities encoded in language. We test the hypothesis
that explicit sensory prompting can surface this latent structure, bringing a
text-only LLM into closer representational alignment with specialist vision and
audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it
cues the model to resolve its next-token predictions as if they were
conditioned on latent visual or auditory evidence that is never actually
supplied. Our findings reveal that lightweight prompt engineering can reliably
activate modality-appropriate representations in purely text-trained LLMs.

</details>


### [40] [CLARITY: Clinical Assistant for Routing, Inference, and Triage](https://arxiv.org/abs/2510.02463)
*Vladimir Shaposhnikov,Aleksandr Nesterov,Ilia Kopanichuk,Ivan Bakulin,Egor Zhelvakov,Ruslan Abramov,Ekaterina Tsapieva,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.CL

TL;DR: CLARITY is an AI-driven clinical assistant platform that combines FSM for structured dialogue with LLM-powered agents to route patients to specialists, analyze symptoms, and assess severity. It achieved human-level routing precision with 3x faster consultations.


<details>
  <summary>Details</summary>
Motivation: To improve patient-to-specialist routing efficiency and clinical consultation workflows in healthcare systems through AI assistance.

Method: Hybrid architecture combining Finite State Machine for structured dialogue flows with collaborative agents using Large Language Models for symptom analysis and referral prioritization, built on modular microservices.

Result: Successfully integrated into national hospital IT platform with 55,000 user dialogues in 2 months; validation on 2,500 expert-annotated dialogues showed CLARITY surpasses human-level first-attempt routing precision and reduces consultation duration by up to 3 times.

Conclusion: CLARITY demonstrates effective AI-driven clinical assistance that improves routing accuracy and consultation efficiency while being scalable and integrable with existing healthcare IT solutions.

Abstract: We present CLARITY (Clinical Assistant for Routing, Inference, and Triage),
an AI-driven platform designed to facilitate patient-to-specialist routing,
clinical consultations, and severity assessment of patients' conditions. Its
hybrid architecture combines a Finite State Machine (FSM) for structured
dialogue flows with collaborative agents that employ Large Language Model (LLM)
to analyze symptoms and prioritize referrals to appropriate specialists. Built
on a modular microservices framework, CLARITY ensures safe, efficient, and
robust performance, flexible and readily scalable to meet the demands of
existing workflows and IT solutions in healthcare.
  We report integration of our clinical assistant into a large-scale
nation-wide inter-hospital IT platform, with over 55,000 content-rich user
dialogues completed within the two months of deployment, 2,500 of which were
expert-annotated for a consequent validation. The validation results show that
CLARITY surpasses human-level performance in terms of the first-attempt routing
precision, naturally requiring up to 3 times shorter duration of the
consultation than with a human.

</details>


### [41] [Unraveling Syntax: How Language Models Learn Context-Free Grammars](https://arxiv.org/abs/2510.02524)
*Laura Ying Schulz,Daniel Mitropolsky,Tomaso Poggio*

Main category: cs.CL

TL;DR: A framework using PCFGs to study transformer learning dynamics, showing parallel learning across subgrammars (unlike children), pretraining benefits, and challenges with deep recursion.


<details>
  <summary>Details</summary>
Motivation: To understand how language models acquire syntax, given limited knowledge about their learning dynamics despite impressive performance.

Method: Train small models on synthetic languages from PCFGs with controlled grammar complexity, recursion depth, and subgrammar structure; prove recursive formulas for loss and KL divergence.

Result: Transformers learn all subgrammars in parallel (unlike children), pretraining improves final loss for small models, models struggle with deep recursion, and pretrained models develop grammar-aligned representations.

Conclusion: PCFGs provide a versatile testbed for studying transformer learning dynamics, revealing fundamental challenges in hierarchical syntax representation and opening new research directions.

Abstract: We introduce a new framework for understanding how language models acquire
syntax. While large models achieve impressive results, little is known about
their learning dynamics. Our approach starts with the observation that most
domains of interest, such as natural language syntax, coding languages,
arithmetic problems, are captured by probabilistic context-free grammars
(PCFGs). We study the learning dynamics of small models trained on synthetic
languages generated from PCFGs, enabling precise control over grammar
complexity, recursion depth, and subgrammar structure. We prove several
general, recursive formulae for the training loss and Kullback-Leibler
divergence over the subgrammar structure of a PCFG. Empirically, we find that
unlike children, who first master simple substructures before progressing to
more complex constructions, transformers reduce loss across all subgrammars in
parallel. We further show that subgrammar pretraining can improve the final
loss for smaller models, and that pretrained models develop internal
representations more aligned with the grammar's substructure. Finally, we
demonstrate that models struggle with deeper recursive structures (a limitation
even of large language models), revealing fundamental challenges in how neural
networks represent hierarchical syntax. Overall, our work initiates the study
of the learning dynamics of transformers on PCFGs as a versatile testbed for
probing learning in language models, opening a research direction with many
open questions.

</details>


### [42] [Hierarchical Semantic Retrieval with Cobweb](https://arxiv.org/abs/2510.02539)
*Anant Gupta,Karthik Singaravadivelan,Zekun Wang*

Main category: cs.CL

TL;DR: Cobweb is a hierarchy-aware document retrieval framework that organizes sentence embeddings into a prototype tree and ranks documents through coarse-to-fine traversal, providing competitive performance, improved robustness, and interpretable retrieval.


<details>
  <summary>Details</summary>
Motivation: Current neural document retrieval treats corpora as flat vector clouds, underutilizing corpus structure and lacking transparent explanations.

Method: Organizes sentence embeddings into a prototype tree with internal nodes as concept prototypes, using two inference approaches: generalized best-first search and lightweight path-sum ranker.

Result: Matches dot product search performance on strong encoder embeddings (BERT/T5) while remaining robust when kNN degrades - with GPT-2 vectors, dot product collapses but Cobweb still retrieves relevant results.

Conclusion: Cobweb provides competitive effectiveness, improved robustness to embedding quality, scalability, and interpretable retrieval via hierarchical prototypes.

Abstract: Neural document retrieval often treats a corpus as a flat cloud of vectors
scored at a single granularity, leaving corpus structure underused and
explanations opaque. We use Cobweb--a hierarchy-aware framework--to organize
sentence embeddings into a prototype tree and rank documents via coarse-to-fine
traversal. Internal nodes act as concept prototypes, providing multi-granular
relevance signals and a transparent rationale through retrieval paths. We
instantiate two inference approaches: a generalized best-first search and a
lightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP
with encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results
show that our retrieval approaches match the dot product search on strong
encoder embeddings while remaining robust when kNN degrades: with GPT-2
vectors, dot product performance collapses whereas our approaches still
retrieve relevant results. Overall, our experiments suggest that Cobweb
provides competitive effectiveness, improved robustness to embedding quality,
scalability, and interpretable retrieval via hierarchical prototypes.

</details>


### [43] [Knowledge-Graph Based RAG System Evaluation Framework](https://arxiv.org/abs/2510.02549)
*Sicheng Dong,Vahid Zolfaghari,Nenad Petrovic,Alois Knoll*

Main category: cs.CL

TL;DR: This paper proposes a knowledge graph (KG)-based evaluation framework for Retrieval Augmented Generation (RAG) systems, extending the RAGAS tool to enable multi-hop reasoning and semantic community clustering for more comprehensive scoring metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional evaluation metrics struggle to effectively capture the key features of modern LLM-generated content, which often exhibits high fluency and naturalness. Evaluating RAG systems remains challenging despite their importance in enhancing content reliability and relevance.

Method: Extended the RAGAS framework into a KG-based evaluation paradigm that enables multi-hop reasoning and semantic community clustering to derive more comprehensive scoring metrics.

Result: The KG-based evaluation method shows better correlation with human judgments compared to RAGAS scores and is more sensitive to subtle semantic differences in generated outputs, as validated through targeted experiments.

Conclusion: The proposed KG-based evaluation framework provides deeper understanding and more nuanced perspective on RAG system performance, while highlighting key challenges and potential directions for future research in RAG evaluation.

Abstract: Large language models (LLMs) has become a significant research focus and is
utilized in various fields, such as text generation and dialog systems. One of
the most essential applications of LLM is Retrieval Augmented Generation (RAG),
which greatly enhances generated content's reliability and relevance. However,
evaluating RAG systems remains a challenging task. Traditional evaluation
metrics struggle to effectively capture the key features of modern
LLM-generated content that often exhibits high fluency and naturalness.
Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended
this framework into a KG-based evaluation paradigm, enabling multi-hop
reasoning and semantic community clustering to derive more comprehensive
scoring metrics. By incorporating these comprehensive evaluation criteria, we
gain a deeper understanding of RAG systems and a more nuanced perspective on
their performance. To validate the effectiveness of our approach, we compare
its performance with RAGAS scores and construct a human-annotated subset to
assess the correlation between human judgments and automated metrics. In
addition, we conduct targeted experiments to demonstrate that our KG-based
evaluation method is more sensitive to subtle semantic differences in generated
outputs. Finally, we discuss the key challenges in evaluating RAG systems and
highlight potential directions for future research.

</details>


### [44] [Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models](https://arxiv.org/abs/2510.02569)
*Tolúl\d{o}pé Ògúnrèmí,Christopher D. Manning,Dan Jurafsky,Karen Livescu*

Main category: cs.CL

TL;DR: This paper analyzes how modality adapters (MAs) in spoken language models transform speech representations for language models, revealing two strategies: meaning-based interlingua for Whisper-based models and phonetic representation for others.


<details>
  <summary>Details</summary>
Motivation: To understand how crucial modality adapters transform speech encoder outputs into representations that language model decoders can process, as this transformation process is not well understood in spoken language models.

Method: Examined MA output representations in three SLMs (SALMONN, Qwen2-Audio and Phi-4-Multimodal-Instruct) by finding the nearest decoder LM token to MA representations.

Result: Found two MA representation strategies: 1) For Whisper encoder models, MAs create an English-based interlingua representing meaning, enabling handling of unseen languages; 2) For non-Whisper models like Phi-4, MAs represent input phonetics using English words.

Conclusion: The MA representation strategy depends on whether the speech encoder was trained only for speech recognition or also for translation, with Whisper-based models developing meaning-based interlingua while others use phonetic representations.

Abstract: Spoken language models (SLMs) that integrate speech with large language
models (LMs) rely on modality adapters (MAs) to map the output of speech
encoders to a representation that is understandable to the decoder LM. Yet we
know very little about how these crucial MAs transform representations. Here we
examine the MA output representation in three SLMs (SALMONN, Qwen2-Audio and
Phi-4-Multimodal-Instruct). By finding the nearest decoder LM token to an MA
representation, we uncover two strategies for MA representations. For models
using a Whisper encoder, MAs appear to represent the meaning of the input using
an English-based interlingua, allowing them to handle languages unseen in
instruction tuning. For models that don't, like Phi-4-Multimodal-Instruct, MAs
instead represent the phonetics of the input, but expressed with English words.
We hypothesise that which arises depends on whether the speech encoder is
trained only for speech recognition or also for translation.

</details>


### [45] [Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models](https://arxiv.org/abs/2510.02629)
*Jingyi Sun,Pepa Atanasova,Sagnik Ray Choudhury,Sekh Mainul Islam,Isabelle Augenstein*

Main category: cs.CL

TL;DR: This paper introduces the first gold standard evaluation framework for highlight explanations (HEs) in assessing context utilization by language models, evaluating four HE methods across various scenarios and finding MechLight performs best but all methods struggle with long contexts and positional biases.


<details>
  <summary>Details</summary>
Motivation: Current language models lack transparency in context utilization - users cannot determine whether models use parametric memory or provided context, nor identify which specific context pieces inform responses. Highlight explanations offer a solution but no existing work evaluates their effectiveness.

Method: Introduced a gold standard HE evaluation framework using controlled test cases with known ground-truth context usage, avoiding limitations of existing proxy evaluations. Evaluated four HE methods (three established techniques and MechLight, a mechanistic interpretability approach adapted for this task) across four context scenarios, four datasets, and five language models.

Result: MechLight performed best across all context scenarios. However, all methods struggled with longer contexts and exhibited positional biases, indicating fundamental challenges in explanation accuracy.

Conclusion: There are fundamental challenges in delivering reliable context utilization explanations at scale, requiring new approaches to address issues with long contexts and positional biases in highlight explanations.

Abstract: Context utilisation, the ability of Language Models (LMs) to incorporate
relevant information from the provided context when generating responses,
remains largely opaque to users, who cannot determine whether models draw from
parametric memory or provided context, nor identify which specific context
pieces inform the response. Highlight explanations (HEs) offer a natural
solution as they can point the exact context pieces and tokens that influenced
model outputs. However, no existing work evaluates their effectiveness in
accurately explaining context utilisation. We address this gap by introducing
the first gold standard HE evaluation framework for context attribution, using
controlled test cases with known ground-truth context usage, which avoids the
limitations of existing indirect proxy evaluations. To demonstrate the
framework's broad applicability, we evaluate four HE methods -- three
established techniques and MechLight, a mechanistic interpretability approach
we adapt for this task -- across four context scenarios, four datasets, and
five LMs. Overall, we find that MechLight performs best across all context
scenarios. However, all methods struggle with longer contexts and exhibit
positional biases, pointing to fundamental challenges in explanation accuracy
that require new approaches to deliver reliable context utilisation
explanations at scale.

</details>


### [46] [Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions](https://arxiv.org/abs/2510.02645)
*Fulei Zhang,Zhou Yu*

Main category: cs.CL

TL;DR: Users communicate differently with LLM chatbots vs human agents, showing differences in grammar, politeness, and vocabulary. Models trained only on human-human data may not handle this style shift well. Data augmentation helps models adapt better than inference-time reformulation.


<details>
  <summary>Details</summary>
Motivation: To understand how user communication styles differ between LLM chatbots and human agents, and address the gap where models trained on human-human data may not perform well with chatbot-specific communication patterns.

Method: Analyzed user communication patterns, then experimented with two strategies: (1) data augmentation during post-training using stylistically diverse datasets, and (2) inference-time user message reformulation.

Result: Found significant differences in grammatical fluency, politeness, and lexical diversity between chatbot and human agent interactions. Models trained on stylistically diverse datasets significantly outperformed those trained on original or uniform datasets, while inference-time reformulation was less effective.

Conclusion: Adapting models to handle communication style shifts between human-human and human-chatbot interactions is crucial, with data augmentation proving more effective than inference-time reformulation for improving LLM-user interaction experiences.

Abstract: As Large Language Models (LLMs) are increasingly deployed in customer-facing
applications, a critical yet underexplored question is how users communicate
differently with LLM chatbots compared to human agent. In this study, we
present empirical evidence that users adopt distinct communication styles when
users interact with chatbots versus human agents. Our analysis reveals
significant differences in grammatical fluency, politeness, and lexical
diversity in user language between the two settings. These findings suggest
that models trained exclusively on human-human interaction data may not
adequately accommodate the communication style shift that occurs once an LLM
chatbot is deployed. To enhance LLM robustness to post-launch communication
style changes, we experimented with two strategies: (1) data augmentation
during the post-training phase and (2) inference-time user message
reformulation. Our results indicate that models trained on stylistically
diverse datasets significantly outperform those trained exclusively on original
or stylistically uniform datasets, while inference-time reformulation proved
less effective. These insights help us to better adapt our models for improved
LLM-user interaction experiences.

</details>


### [47] [SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models](https://arxiv.org/abs/2510.02648)
*Rui Qi,Zhibo Man,Yufeng Chen,Fengran Mo,Jinan Xu,Kaiyu Huang*

Main category: cs.CL

TL;DR: SoT is a training-free method that improves multilingual reasoning by transforming language-specific semantics into language-agnostic structured representations through multi-step transformations.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with multilingual reasoning tasks due to resource constraints, as reasoning capacity hasn't been successfully transferred to non-high-resource languages.

Method: Proposes Structured-of-Thought (SoT) with two transformations: Language Thinking Transformation and Structured Knowledge Transformation, converting semantic information into structured representations to maintain consistent reasoning pathways across languages.

Result: SoT outperforms strong baselines on multiple multilingual reasoning benchmarks across various LLM backbones and can integrate with other training-free strategies for further improvements.

Conclusion: SoT effectively enables LLMs to handle multilingual reasoning tasks by transforming language-specific information into structured representations, maintaining consistent reasoning pathways despite cross-lingual variations.

Abstract: Recent developments have enabled Large Language Models (LLMs) to engage in
complex reasoning tasks through deep thinking. However, the capacity of
reasoning has not been successfully transferred to non-high-resource languages
due to resource constraints, which struggles with multilingual reasoning tasks.
To this end, we propose Structured-of-Thought (SoT), a training-free method
that improves the performance on multilingual reasoning through a multi-step
transformation: Language Thinking Transformation and Structured Knowledge
Transformation. The SoT method converts language-specific semantic information
into language-agnostic structured representations, enabling the models to
understand the query in different languages more sophisticated. Besides, SoT
effectively guides LLMs toward more concentrated reasoning to maintain
consistent underlying reasoning pathways when handling cross-lingual variations
in expression. Experimental results demonstrate that SoT outperforms several
strong baselines on multiple multilingual reasoning benchmarks when adapting to
various backbones of LLMs. It can also be integrated with other training-free
strategies for further improvements. Our code is available at
https://github.com/Cherry-qwq/SoT.

</details>


### [48] [Self-Improvement in Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2510.02665)
*Shijian Deng,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CL

TL;DR: This survey provides the first comprehensive overview of self-improvement in Multimodal LLMs (MLLMs), covering data collection, organization, model optimization methods, evaluations, and applications.


<details>
  <summary>Details</summary>
Motivation: To extend self-improvement techniques from LLMs to multimodal domain for leveraging diverse data sources and developing more general self-improving models.

Method: Structured analysis from three perspectives: data collection, data organization, and model optimization; includes evaluations and downstream applications.

Result: Provides comprehensive framework and current state of self-improvement methods in MLLMs.

Conclusion: Outlines open challenges and future research directions for self-improvement in multimodal LLMs.

Abstract: Recent advancements in self-improvement for Large Language Models (LLMs) have
efficiently enhanced model capabilities without significantly increasing costs,
particularly in terms of human effort. While this area is still relatively
young, its extension to the multimodal domain holds immense potential for
leveraging diverse data sources and developing more general self-improving
models. This survey is the first to provide a comprehensive overview of
self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview
of the current literature and discuss methods from three perspectives: 1) data
collection, 2) data organization, and 3) model optimization, to facilitate the
further development of self-improvement in MLLMs. We also include commonly used
evaluations and downstream applications. Finally, we conclude by outlining open
challenges and future research directions.

</details>


### [49] [Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering](https://arxiv.org/abs/2510.02671)
*Yavuz Bakman,Sungmin Kang,Zhiqi Huang,Duygu Nur Yaldiz,Catarina G. Belém,Chenyang Zhu,Anoop Kumar,Alfy Samuel,Salman Avestimehr,Daben Liu,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: This paper proposes a theoretically grounded approach for uncertainty quantification in contextual question answering, focusing on epistemic uncertainty through semantic feature gaps analysis.


<details>
  <summary>Details</summary>
Motivation: Current UQ research focuses on closed-book factual QA, while contextual QA remains unexplored despite its importance in real-world applications.

Method: Proposes a task-agnostic token-level uncertainty measure, decomposes it to isolate epistemic uncertainty, approximates true distribution with idealized model, and extracts three semantic features (context-reliance, context comprehension, honesty) using top-down interpretability approach.

Result: Outperforms state-of-the-art unsupervised and supervised UQ methods on multiple QA benchmarks, achieving up to 13-point PRR improvement with negligible inference overhead.

Conclusion: The proposed method provides an effective framework for uncertainty quantification in contextual QA by leveraging semantic feature gaps analysis and achieves superior performance compared to existing approaches.

Abstract: Uncertainty Quantification (UQ) research has primarily focused on closed-book
factual question answering (QA), while contextual QA remains unexplored,
despite its importance in real-world applications. In this work, we focus on UQ
for the contextual QA task and propose a theoretically grounded approach to
quantify epistemic uncertainty. We begin by introducing a task-agnostic,
token-level uncertainty measure defined as the cross-entropy between the
predictive distribution of the given model and the unknown true distribution.
By decomposing this measure, we isolate the epistemic component and approximate
the true distribution by a perfectly prompted, idealized model. We then derive
an upper bound for epistemic uncertainty and show that it can be interpreted as
semantic feature gaps in the given model's hidden representations relative to
the ideal model. We further apply this generic framework to the contextual QA
task and hypothesize that three features approximate this gap: context-reliance
(using the provided context rather than parametric knowledge), context
comprehension (extracting relevant information from context), and honesty
(avoiding intentional lies). Using a top-down interpretability approach, we
extract these features by using only a small number of labeled samples and
ensemble them to form a robust uncertainty score. Experiments on multiple QA
benchmarks in both in-distribution and out-of-distribution settings show that
our method substantially outperforms state-of-the-art unsupervised
(sampling-free and sampling-based) and supervised UQ methods, achieving up to a
13-point PRR improvement while incurring a negligible inference overhead.

</details>


### [50] [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.CL

TL;DR: This paper presents the first survival analysis of conversational AI robustness, revealing that abrupt semantic drift catastrophically increases failure risk while gradual drift protects against failure, challenging assumptions about semantic consistency.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation frameworks focus on static benchmarks and single-turn assessments, failing to capture temporal dynamics of conversational degradation in real-world multi-turn dialogues.

Method: Analyzed 36,951 conversation turns across 9 state-of-the-art LLMs using survival modeling (Cox proportional hazards, Accelerated Failure Time, and Random Survival Forest) to model failure as time-to-event process.

Result: Abrupt prompt-to-prompt semantic drift dramatically increases failure hazard, while gradual cumulative drift significantly reduces failure hazard and enables longer dialogues. AFT models with interactions showed superior performance.

Conclusion: Survival analysis is a powerful paradigm for evaluating LLM robustness, offering insights for designing resilient conversational agents and challenging assumptions about semantic consistency necessity.

Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their
robustness in extended multi-turn dialogues remains poorly understood. Existing
evaluation frameworks focus on static benchmarks and single-turn assessments,
failing to capture the temporal dynamics of conversational degradation that
characterize real-world interactions. In this work, we present the first
comprehensive survival analysis of conversational AI robustness, analyzing
36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a
time-to-event process. Our survival modeling framework-employing Cox
proportional hazards, Accelerated Failure Time, and Random Survival Forest
approaches-reveals extraordinary temporal dynamics. We find that abrupt,
prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing
the hazard of conversational failure. In stark contrast, gradual, cumulative
drift is highly protective, vastly reducing the failure hazard and enabling
significantly longer dialogues. AFT models with interactions demonstrate
superior performance, achieving excellent discrimination and exceptional
calibration. These findings establish survival analysis as a powerful paradigm
for evaluating LLM robustness, offer concrete insights for designing resilient
conversational agents, and challenge prevailing assumptions about the necessity
of semantic consistency in conversational AI Systems.

</details>


### [51] [TravelBench : Exploring LLM Performance in Low-Resource Domains](https://arxiv.org/abs/2510.02719)
*Srinivas Billa,Xiaonan Jing*

Main category: cs.CL

TL;DR: General LLM benchmarks don't reflect performance in low-resource tasks. The study created 14 travel-domain datasets and found that despite high training FLOPs, LLMs struggle in domain-specific scenarios, with reasoning helping smaller models more.


<details>
  <summary>Details</summary>
Motivation: Existing LLM benchmarks provide limited insight into model capabilities for low-resource tasks, making it hard to develop effective solutions in specialized domains like travel.

Method: Curated 14 travel-domain datasets spanning 7 NLP tasks using anonymized real-world data, then analyzed LLM performance across accuracy, scaling behavior, and reasoning capabilities.

Result: General benchmarking results are insufficient for understanding model performance in low-resource tasks. LLMs hit performance bottlenecks in complex domain-specific scenarios despite training FLOPs. Reasoning provides greater performance boost for smaller LLMs.

Conclusion: Domain-specific evaluation is crucial as general benchmarks don't capture LLM performance in low-resource tasks. Reasoning capabilities are particularly beneficial for smaller models in specialized domains.

Abstract: Results on existing LLM benchmarks capture little information over the model
capabilities in low-resource tasks, making it difficult to develop effective
solutions in these domains. To address these challenges, we curated 14
travel-domain datasets spanning 7 common NLP tasks using anonymised data from
real-world scenarios, and analysed the performance across LLMs. We report on
the accuracy, scaling behaviour, and reasoning capabilities of LLMs in a
variety of tasks. Our results confirm that general benchmarking results are
insufficient for understanding model performance in low-resource tasks. Despite
the amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks
in complex, domain-specific scenarios. Furthermore, reasoning provides a more
significant boost for smaller LLMs by making the model a better judge on
certain tasks.

</details>


### [52] [PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking](https://arxiv.org/abs/2510.02726)
*KM Pooja,Cheng Long,Aixin Sun*

Main category: cs.CL

TL;DR: The paper proposes PGMEL, a policy gradient-based generative adversarial network for multimodal entity linking that generates high-quality negative samples to improve representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal entity linking methods lack exploration of high-quality negative sample selection, which is crucial for metric learning but remains unexplored in MEL literature.

Method: A generative adversarial network framework where a generator creates high-quality negative samples and a discriminator performs metric learning, optimized using policy gradient techniques for the discrete generation process.

Result: PGMEL outperforms state-of-the-art methods on Wiki-MEL, Richpedia-MEL and WikiDiverse datasets by learning meaningful representations through challenging negative sample selection.

Conclusion: The proposed adversarial framework with policy gradient optimization effectively addresses the negative sample selection problem in multimodal entity linking, leading to improved performance.

Abstract: The task of entity linking, which involves associating mentions with their
respective entities in a knowledge graph, has received significant attention
due to its numerous potential applications. Recently, various multimodal entity
linking (MEL) techniques have been proposed, targeted to learn comprehensive
embeddings by leveraging both text and vision modalities. The selection of
high-quality negative samples can potentially play a crucial role in
metric/representation learning. However, to the best of our knowledge, this
possibility remains unexplored in existing literature within the framework of
MEL. To fill this gap, we address the multimodal entity linking problem in a
generative adversarial setting where the generator is responsible for
generating high-quality negative samples, and the discriminator is assigned the
responsibility for the metric learning tasks. Since the generator is involved
in generating samples, which is a discrete process, we optimize it using policy
gradient techniques and propose a policy gradient-based generative adversarial
network for multimodal entity linking (PGMEL). Experimental results based on
Wiki-MEL, Richpedia-MEL and WikiDiverse datasets demonstrate that PGMEL learns
meaningful representation by selecting challenging negative samples and
outperforms state-of-the-art methods.

</details>


### [53] [IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context](https://arxiv.org/abs/2510.02742)
*Santhosh G S,Akshay Govind S,Gokul S Krishnan,Balaraman Ravindran,Sriraam Natarajan*

Main category: cs.CL

TL;DR: Proposes an evaluation framework using contrastive learning to detect nuanced cultural biases in LLMs, introduces IndiCASA dataset with 2,575 validated sentences across five demographic axes, and finds all tested models exhibit stereotypical biases, especially in disability-related contexts.


<details>
  <summary>Details</summary>
Motivation: Existing embedding-based bias assessment methods fail to capture nuanced stereotypes in culturally diverse contexts like India, necessitating better evaluation frameworks for LLMs deployed in high-stakes applications.

Method: Developed an evaluation framework using an encoder trained with contrastive learning to capture fine-grained bias through embedding similarity, and created IndiCASA dataset with 2,575 human-validated sentences across caste, gender, religion, disability, and socioeconomic status.

Result: Evaluation of multiple open-weight LLMs revealed all models exhibit stereotypical bias, with disability-related biases being notably persistent and religion bias generally lower (likely due to global debiasing efforts).

Conclusion: There is a demonstrated need for fairer model development as current LLMs show embedded biases across multiple demographic axes, particularly in culturally specific contexts.

Abstract: Large Language Models (LLMs) have gained significant traction across critical
domains owing to their impressive contextual understanding and generative
capabilities. However, their increasing deployment in high stakes applications
necessitates rigorous evaluation of embedded biases, particularly in culturally
diverse contexts like India where existing embedding-based bias assessment
methods often fall short in capturing nuanced stereotypes. We propose an
evaluation framework based on a encoder trained using contrastive learning that
captures fine-grained bias through embedding similarity. We also introduce a
novel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes and
Anti-stereotypes) comprising 2,575 human-validated sentences spanning five
demographic axes: caste, gender, religion, disability, and socioeconomic
status. Our evaluation of multiple open-weight LLMs reveals that all models
exhibit some degree of stereotypical bias, with disability related biases being
notably persistent, and religion bias generally lower likely due to global
debiasing efforts demonstrating the need for fairer model development.

</details>


### [54] [The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback](https://arxiv.org/abs/2510.02752)
*Hangfan Zhang,Siyuan Xu,Zhimeng Guo,Huaisheng Zhu,Shicheng Liu,Xinrun Wang,Qiaosheng Zhang,Yang Chen,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: Self-aware reinforcement learning for LLMs that alternates between task proposal and solving, using difficulty prediction and limit breaking mechanisms to achieve significant performance gains with minimal extra data.


<details>
  <summary>Details</summary>
Motivation: Traditional RL training for LLMs requires substantial data creation and annotation efforts, which is resource-intensive. This work aims to improve LLMs through RL with minimal data dependency.

Method: Alternates between LLM proposing tasks and solving them. Introduces two self-aware mechanisms: (1) self-aware difficulty prediction to assess task difficulty relative to model abilities and prioritize challenging yet solvable tasks, (2) self-aware limit breaking to recognize when tasks exceed capability boundaries and proactively request external data.

Result: Extensive experiments on nine benchmarks show 53.8% relative improvement with less than 1.2% extra data.

Conclusion: Self-aware RL demonstrates efficacy and underscores the promise of self-evolving agent training for LLMs.

Abstract: Reinforcement learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of large language models (LLMs), but such training
typically demands substantial efforts in creating and annotating data. In this
work, we explore improving LLMs through RL with minimal data. Our approach
alternates between the LLM proposing a task and then attempting to solve it. To
minimize data dependency, we introduce two novel mechanisms grounded in
self-awareness: (1) self-aware difficulty prediction, where the model learns to
assess task difficulty relative to its own abilities and prioritize challenging
yet solvable tasks, and (2) self-aware limit breaking, where the model
recognizes when a task is beyond its capability boundary and proactively
requests external data to break through that limit. Extensive experiments on
nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra
data demonstrate the efficacy of self-aware RL and underscore the promise of
self-evolving agent training.

</details>


### [55] [XTRA: Cross-Lingual Topic Modeling with Topic and Representation Alignments](https://arxiv.org/abs/2510.02788)
*Tien Phat Nguyen,Vu Minh Ngo,Tung Nguyen,Linh Van Ngo,Duc Anh Nguyen,Sang Dinh,Trung Le*

Main category: cs.CL

TL;DR: XTRA is a cross-lingual topic modeling framework that combines Bag-of-Words modeling with multilingual embeddings using representation and topic alignments to improve topic coherence, diversity, and cross-lingual consistency.


<details>
  <summary>Details</summary>
Motivation: Existing cross-lingual topic modeling methods struggle with ensuring high topic coherence and consistent alignment across languages, despite some improvements in topic diversity.

Method: XTRA uses two core components: representation alignment (aligning document-topic distributions via contrastive learning) and topic alignment (projecting topic-word distributions into shared semantic space) to enforce cross-lingual consistency.

Result: Experiments show XTRA significantly outperforms strong baselines in topic coherence, diversity, and alignment quality on multilingual corpora.

Conclusion: XTRA successfully learns interpretable topics that are both coherent and well-aligned across languages through its dual alignment mechanism.

Abstract: Cross-lingual topic modeling aims to uncover shared semantic themes across
languages. Several methods have been proposed to address this problem,
leveraging both traditional and neural approaches. While previous methods have
achieved some improvements in topic diversity, they often struggle to ensure
high topic coherence and consistent alignment across languages. We propose XTRA
(Cross-Lingual Topic Modeling with Topic and Representation Alignments), a
novel framework that unifies Bag-of-Words modeling with multilingual
embeddings. XTRA introduces two core components: (1) representation alignment,
aligning document-topic distributions via contrastive learning in a shared
semantic space; and (2) topic alignment, projecting topic-word distributions
into the same space to enforce crosslingual consistency. This dual mechanism
enables XTRA to learn topics that are interpretable (coherent and diverse) and
well-aligned across languages. Experiments on multilingual corpora confirm that
XTRA significantly outperforms strong baselines in topic coherence, diversity,
and alignment quality. Code and reproducible scripts are available at https:
//github.com/tienphat140205/XTRA.

</details>


### [56] [A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media](https://arxiv.org/abs/2510.02811)
*Matej Gjurković*

Main category: cs.CL

TL;DR: This thesis addresses challenges in automated personality assessment from social media text by creating large datasets (MBTI9k and PANDORA) and developing the SIMPA framework, which matches user statements to validated questionnaire items for interpretable personality assessment.


<details>
  <summary>Details</summary>
Motivation: Address two main challenges: scarcity of large personality-labeled datasets and disconnect between personality psychology and NLP, which limits model validity and interpretability in automated personality assessment from social media data.

Method: Created two Reddit datasets (MBTI9k and PANDORA) with personality labels and demographics. Developed SIMPA framework using machine learning and semantic similarity to match user-generated statements with validated questionnaire items for interpretable personality assessment.

Result: PANDORA dataset contains 17M comments from 10k+ users with MBTI and Big Five personality models. Experiments showed demographic variables influence model validity. SIMPA framework achieved personality assessments comparable to human evaluations with high interpretability and efficiency.

Conclusion: SIMPA framework provides interpretable personality assessment that bridges psychology and NLP, with versatility extending beyond personality to applications with complex label taxonomies and variable cue associations.

Abstract: Personality refers to individual differences in behavior, thinking, and
feeling. With the growing availability of digital footprints, especially from
social media, automated methods for personality assessment have become
increasingly important. Natural language processing (NLP) enables the analysis
of unstructured text data to identify personality indicators. However, two main
challenges remain central to this thesis: the scarcity of large,
personality-labeled datasets and the disconnect between personality psychology
and NLP, which restricts model validity and interpretability. To address these
challenges, this thesis presents two datasets -- MBTI9k and PANDORA --
collected from Reddit, a platform known for user anonymity and diverse
discussions. The PANDORA dataset contains 17 million comments from over 10,000
users and integrates the MBTI and Big Five personality models with demographic
information, overcoming limitations in data size, quality, and label coverage.
Experiments on these datasets show that demographic variables influence model
validity. In response, the SIMPA (Statement-to-Item Matching Personality
Assessment) framework was developed - a computational framework for
interpretable personality assessment that matches user-generated statements
with validated questionnaire items. By using machine learning and semantic
similarity, SIMPA delivers personality assessments comparable to human
evaluations while maintaining high interpretability and efficiency. Although
focused on personality assessment, SIMPA's versatility extends beyond this
domain. Its model-agnostic design, layered cue detection, and scalability make
it suitable for various research and practical applications involving complex
label taxonomies and variable cue associations with target concepts.

</details>


### [57] [StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.02827)
*Tengjun Ni,Xin Yuan,Shenghong Li,Kai Wu,Ren Ping Liu,Wei Ni,Wenjie Zhang*

Main category: cs.CL

TL;DR: StepChain GraphRAG is a new framework that combines question decomposition with BFS reasoning flow for multi-hop QA, achieving state-of-the-art performance with improved explainability.


<details>
  <summary>Details</summary>
Motivation: To address challenges in integrating iterative reasoning steps with external knowledge retrieval in retrieval-augmented generation for multi-hop question answering.

Method: Builds global corpus index, parses retrieved passages into knowledge graph on-the-fly, splits complex queries into sub-questions, and uses BFS-based traversal to expand along relevant edges while assembling explicit evidence chains.

Result: Achieves SOTA Exact Match and F1 scores on MuSiQue, 2WikiMultiHopQA, and HotpotQA datasets, with average improvements of 2.57% EM and 2.13% F1 over previous SOTA, largest gains on HotpotQA (+4.70% EM, +3.44% F1).

Conclusion: The framework enhances explainability through chain-of-thought preservation, and future work should address computational overhead and potential LLM hallucinations to improve efficiency and reliability.

Abstract: Recent progress in retrieval-augmented generation (RAG) has led to more
accurate and interpretable multi-hop question answering (QA). Yet, challenges
persist in integrating iterative reasoning steps with external knowledge
retrieval. To address this, we introduce StepChain GraphRAG, a framework that
unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow
for enhanced multi-hop QA. Our approach first builds a global index over the
corpus; at inference time, only retrieved passages are parsed on-the-fly into a
knowledge graph, and the complex query is split into sub-questions. For each
sub-question, a BFS-based traversal dynamically expands along relevant edges,
assembling explicit evidence chains without overwhelming the language model
with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA
show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1
scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the
SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).
StepChain GraphRAG also fosters enhanced explainability by preserving the
chain-of-thought across intermediate retrieval steps. We conclude by discussing
how future work can mitigate the computational overhead and address potential
hallucinations from large language models to refine efficiency and reliability
in multi-hop QA.

</details>


### [58] [Evaluating Large Language Models for IUCN Red List Species Information](https://arxiv.org/abs/2510.02830)
*Shinya Uryu*

Main category: cs.CL

TL;DR: LLMs show high accuracy in taxonomic classification (94.9%) but poor performance in conservation reasoning (27.2%), revealing a knowledge-reasoning gap and systematic biases toward charismatic vertebrates.


<details>
  <summary>Details</summary>
Motivation: To systematically validate the reliability of LLMs for species evaluation in conservation, given their rapid adoption despite uncertainty about their effectiveness.

Method: Systematic validation of five leading LLM models on 21,955 species across four IUCN Red List assessment components: taxonomy, conservation status, distribution, and threats.

Result: Models excelled at taxonomic classification but consistently failed at conservation reasoning, showing a knowledge-reasoning gap across all models and systematic biases favoring charismatic vertebrates.

Conclusion: LLMs are powerful for information retrieval but require human oversight for judgment-based decisions; a hybrid approach is recommended where LLMs augment expert capacity while humans retain authority over risk assessment and policy.

Abstract: Large Language Models (LLMs) are rapidly being adopted in conservation to
address the biodiversity crisis, yet their reliability for species evaluation
is uncertain. This study systematically validates five leading models on 21,955
species across four core IUCN Red List assessment components: taxonomy,
conservation status, distribution, and threats. A critical paradox was
revealed: models excelled at taxonomic classification (94.9%) but consistently
failed at conservation reasoning (27.2% for status assessment). This
knowledge-reasoning gap, evident across all models, suggests inherent
architectural constraints, not just data limitations. Furthermore, models
exhibited systematic biases favoring charismatic vertebrates, potentially
amplifying existing conservation inequities. These findings delineate clear
boundaries for responsible LLM deployment: they are powerful tools for
information retrieval but require human oversight for judgment-based decisions.
A hybrid approach is recommended, where LLMs augment expert capacity while
human experts retain sole authority over risk assessment and policy.

</details>


### [59] [Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation](https://arxiv.org/abs/2510.02855)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Kamrujjaman,Eftakhar Ahmed Arnob,Ahsan Habib Tareq*

Main category: cs.CL

TL;DR: First comprehensive CSP formulation of Wordle with constraint-aware strategies (CSP-Aware Entropy and Probabilistic CSP) outperforms existing methods, achieving 3.54 avg guesses with 99.9% success rate and robustness across noise levels and languages.


<details>
  <summary>Details</summary>
Motivation: Existing Wordle solvers use information-theoretic entropy or frequency heuristics without formal constraint treatment, creating a need for principled CSP approaches.

Method: Introduced CSP-Aware Entropy (computing information gain after constraint propagation) and Probabilistic CSP framework integrating Bayesian priors with logical constraints.

Result: CSP-Aware Entropy achieved 3.54 avg guesses with 99.9% success rate (1.7% improvement over Forward Checking), 46% faster runtime, and maintained advantages under noise. Cross-lexicon validation showed 88% success in Spanish without language-specific tuning.

Conclusion: Principled constraint satisfaction techniques outperform classical information-theoretic and learning-based approaches for structured puzzle-solving domains, establishing new performance benchmarks.

Abstract: Wordle presents an algorithmically rich testbed for constraint satisfaction
problem (CSP) solving. While existing solvers rely on information-theoretic
entropy maximization or frequency-based heuristics without formal constraint
treatment, we present the first comprehensive CSP formulation of Wordle with
novel constraint-aware solving strategies. We introduce CSP-Aware Entropy,
computing information gain after constraint propagation rather than on raw
candidate sets, and a Probabilistic CSP framework integrating Bayesian
word-frequency priors with logical constraints. Through evaluation on 2,315
English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9%
success rate, a statistically significant 1.7% improvement over Forward
Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms
versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3
percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic
CSP achieves 100% success across all noise levels (0-20%) through constraint
recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates
88% success with zero language-specific tuning, validating that core CSP
principles transfer across languages despite an 11.2 percentage point gap from
linguistic differences (p<0.001, Fisher's exact test). Our open-source
implementation with 34 unit tests achieving 91% code coverage provides
reproducible infrastructure for CSP research. The combination of formal CSP
treatment, constraint-aware heuristics, probabilistic-logical integration,
robustness analysis, and cross-lexicon validation establishes new performance
benchmarks demonstrating that principled constraint satisfaction techniques
outperform classical information-theoretic and learning-based approaches for
structured puzzle-solving domains.

</details>


### [60] [Self-Reflective Generation at Test Time](https://arxiv.org/abs/2510.02919)
*Jian Mu,Qixin Zhang,Zhiyong Wang,Menglin Yang,Shuang Qiu,Chengwei Qin,Zhongxiang Dai,Yao Shu*

Main category: cs.CL

TL;DR: SRGen is a lightweight test-time framework that enables LLMs to self-reflect before generating uncertain tokens, using dynamic entropy thresholding to identify high-uncertainty points and applying corrective vectors to improve reasoning quality.


<details>
  <summary>Details</summary>
Motivation: Current LLM self-reflection methods are either reactive (revising full drafts) or require expensive training, making them inefficient for preventing error cascades in complex reasoning tasks.

Method: Uses dynamic entropy thresholding to detect uncertain tokens during generation, then applies specific corrective vectors that exploit context for self-reflective generation to correct token probability distributions.

Result: Consistent improvements on mathematical reasoning benchmarks: +12.0% Pass@1 and +13.3% Cons@5 on AIME2024 with DeepSeek-R1-Distill-Qwen-7B; strengthens self-consistency voting across diverse LLMs.

Conclusion: SRGen provides plug-and-play integration of reflection into generation process, achieving reliable LLM reasoning with bounded overhead and broad composability with other techniques.

Abstract: Large language models (LLMs) increasingly solve complex reasoning tasks via
long chain-of-thought, but their forward-only autoregressive generation process
is fragile; early token errors can cascade, which creates a clear need for
self-reflection mechanisms. However, existing self-reflection either performs
revisions over full drafts or learns self-correction via expensive training,
both fundamentally reactive and inefficient. To address this, we propose
Self-Reflective Generation at Test Time (SRGen), a lightweight test-time
framework that reflects before generating at uncertain points. During token
generation, SRGen utilizes dynamic entropy thresholding to identify
high-uncertainty tokens. For each identified token, it trains a specific
corrective vector, which fully exploits the already generated context for a
self-reflective generation to correct the token probability distribution. By
retrospectively analyzing the partial output, this self-reflection enables more
trustworthy decisions, thereby significantly reducing the probability of errors
at highly uncertain points. Evaluated on challenging mathematical reasoning
benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model
reasoning: improvements in single-pass quality also translate into stronger
self-consistency voting. Especially, on AIME2024 with
DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on
Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a
plug-and-play method that integrates reflection into the generation process for
reliable LLM reasoning, achieving consistent gains with bounded overhead and
broad composability with other training-time (e.g., RLHF) and test-time (e.g.,
SLOT) techniques.

</details>


### [61] [Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval](https://arxiv.org/abs/2510.02938)
*Yohan Lee,Yongwoo Song,Sangyeop Kim*

Main category: cs.CL

TL;DR: The Conversational Data Retrieval (CDR) benchmark is the first comprehensive test set for evaluating conversation data retrieval systems, revealing significant performance gaps in current embedding models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized evaluation for conversational data retrieval systems and identify unique challenges in retrieving product insights from conversation data.

Method: Created a benchmark with 1.6k queries across five analytical tasks and 9.1k conversations, then evaluated 16 popular embedding models on this benchmark.

Result: Even the best models achieved only around NDCG@10 of 0.51, showing substantial performance gap compared to document retrieval. Identified unique challenges including implicit state recognition, turn dynamics, and contextual references.

Conclusion: The CDR benchmark provides a reliable standard for measuring conversational data retrieval performance and reveals significant room for improvement in current retrieval capabilities for conversation data.

Abstract: We present the Conversational Data Retrieval (CDR) benchmark, the first
comprehensive test set for evaluating systems that retrieve conversation data
for product insights. With 1.6k queries across five analytical tasks and 9.1k
conversations, our benchmark provides a reliable standard for measuring
conversational data retrieval performance. Our evaluation of 16 popular
embedding models shows that even the best models reach only around NDCG@10 of
0.51, revealing a substantial gap between document and conversational data
retrieval capabilities. Our work identifies unique challenges in conversational
data retrieval (implicit state recognition, turn dynamics, contextual
references) while providing practical query templates and detailed error
analysis across different task categories. The benchmark dataset and code are
available at https://github.com/l-yohai/CDR-Benchmark.

</details>


### [62] [Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking](https://arxiv.org/abs/2510.02962)
*Jingqi Zhang,Ruibo Chen,Yingqing Yang,Peihua Mai,Heng Huang,Yan Pang*

Main category: cs.CL

TL;DR: TRACE is a black-box framework for detecting copyrighted dataset usage in LLM fine-tuning using distortion-free watermarks and entropy-gated detection.


<details>
  <summary>Details</summary>
Motivation: Existing methods for detecting unauthorized dataset usage require internal model signals or clean reference datasets, limiting practical applicability. Watermarking approaches often degrade text quality or reduce performance.

Method: TRACE rewrites datasets with distortion-free watermarks using a private key, then exploits the radioactivity effect during fine-tuning with an entropy-gated procedure that selectively scores high-uncertainty tokens.

Result: Across diverse datasets and model families, TRACE consistently achieves significant detections (p<0.05) with strong statistical evidence, supports multi-dataset attribution, and remains robust after continued pretraining on non-watermarked corpora.

Conclusion: TRACE provides a practical and reliable black-box verification method for detecting copyrighted dataset usage in LLM fine-tuning while maintaining text quality and downstream utility.

Abstract: Large Language Models (LLMs) are increasingly fine-tuned on smaller,
domain-specific datasets to improve downstream performance. These datasets
often contain proprietary or copyrighted material, raising the need for
reliable safeguards against unauthorized use. Existing membership inference
attacks (MIAs) and dataset-inference methods typically require access to
internal signals such as logits, while current black-box approaches often rely
on handcrafted prompts or a clean reference dataset for calibration, both of
which limit practical applicability. Watermarking is a promising alternative,
but prior techniques can degrade text quality or reduce task performance. We
propose TRACE, a practical framework for fully black-box detection of
copyrighted dataset usage in LLM fine-tuning. \texttt{TRACE} rewrites datasets
with distortion-free watermarks guided by a private key, ensuring both text
quality and downstream utility. At detection time, we exploit the radioactivity
effect of fine-tuning on watermarked data and introduce an entropy-gated
procedure that selectively scores high-uncertainty tokens, substantially
amplifying detection power. Across diverse datasets and model families, TRACE
consistently achieves significant detections (p<0.05), often with extremely
strong statistical evidence. Furthermore, it supports multi-dataset attribution
and remains robust even after continued pretraining on large non-watermarked
corpora. These results establish TRACE as a practical route to reliable
black-box verification of copyrighted dataset usage. We will make our code
available at: https://github.com/NusIoraPrivacy/TRACE.

</details>


### [63] [Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines](https://arxiv.org/abs/2510.02967)
*Matthew Lewis,Samuel Thio,Richard JB Dobson,Spiros Denaxas*

Main category: cs.CL

TL;DR: Developed a RAG system for querying UK NICE clinical guidelines using LLMs, achieving high retrieval performance (MRR 0.814, 81% recall at first chunk) and significantly improving answer faithfulness by 64.7 percentage points to 99.5%.


<details>
  <summary>Details</summary>
Motivation: The extensive length and volume of NICE clinical guidelines impede their utilization in time-constrained healthcare systems, necessitating a system that can provide precisely matched information through natural language queries.

Method: Used a hybrid embedding mechanism retrieval architecture evaluated on 10,195 text chunks from 300 guidelines, with RAG-enhanced models tested on 70 manually curated question-answer pairs.

Result: High retrieval performance with MRR 0.814, 81% recall at first chunk, 99.1% recall within top 10 chunks. RAG-enhanced models increased faithfulness by 64.7pp to 99.5% and achieved perfect context precision of 1, significantly outperforming medical-focused Meditron3-8B LLM (43% faithfulness).

Conclusion: RAG is an effective, reliable, and scalable approach for applying generative AI in healthcare, enabling cost-effective access to medical guidelines by preventing information fabrication through source-grounded answers.

Abstract: This paper presents the development and evaluation of a Retrieval-Augmented
Generation (RAG) system for querying the United Kingdom's National Institute
for Health and Care Excellence (NICE) clinical guidelines using Large Language
Models (LLMs). The extensive length and volume of these guidelines can impede
their utilisation within a time-constrained healthcare system, a challenge this
project addresses through the creation of a system capable of providing users
with precisely matched information in response to natural language queries. The
system's retrieval architecture, composed of a hybrid embedding mechanism, was
evaluated against a database of 10,195 text chunks derived from three hundred
guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)
of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten
retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the
generation phase. When evaluated on a manually curated dataset of seventy
question-answer pairs, RAG-enhanced models showed substantial gains in
performance. Faithfulness, the measure of whether an answer is supported by the
source text, was increased by 64.7 percentage points to 99.5% for the
RAG-enhanced O4-Mini model and significantly outperformed the medical-focused
Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context
Precision score of 1 for all RAG-enhanced models, confirms the system's ability
to prevent information fabrication by grounding its answers in relevant source
material. This study thus establishes RAG as an effective, reliable, and
scalable approach for applying generative AI in healthcare, enabling
cost-effective access to medical guidelines.

</details>


### [64] [Semantic Differentiation in Speech Emotion Recognition: Insights from Descriptive and Expressive Speech Roles](https://arxiv.org/abs/2510.03060)
*Rongchen Guo,Vincent Francoeur,Isar Nejadgholi,Sylvain Gagnon,Miodrag Bolic*

Main category: cs.CL

TL;DR: This paper distinguishes between descriptive semantics (contextual content) and expressive semantics (emotional state) in speech emotion recognition, showing they align with intended vs evoked emotions respectively.


<details>
  <summary>Details</summary>
Motivation: Speech Emotion Recognition accuracy is limited by emotional nuances in speech, and distinguishing between different types of semantic information could improve understanding of emotional content.

Method: Recorded audio clips from participants describing experiences after watching emotional movie segments, collected intended emotion tags, self-rated emotional responses, and valence/arousal scores.

Result: Descriptive semantics align with intended emotions, while expressive semantics correlate with evoked emotions.

Conclusion: The findings inform SER applications in human-AI interaction and enable more context-aware AI systems.

Abstract: Speech Emotion Recognition (SER) is essential for improving human-computer
interaction, yet its accuracy remains constrained by the complexity of
emotional nuances in speech. In this study, we distinguish between descriptive
semantics, which represents the contextual content of speech, and expressive
semantics, which reflects the speaker's emotional state. After watching
emotionally charged movie segments, we recorded audio clips of participants
describing their experiences, along with the intended emotion tags for each
clip, participants' self-rated emotional responses, and their valence/arousal
scores. Through experiments, we show that descriptive semantics align with
intended emotions, while expressive semantics correlate with evoked emotions.
Our findings inform SER applications in human-AI interaction and pave the way
for more context-aware AI systems.

</details>


### [65] [Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling than CoT Prompting?](https://arxiv.org/abs/2510.03093)
*Oriol Pareras,Gerard I. Gállego,Federico Costa,Cristina España-Bonet,Javier Hernando*

Main category: cs.CL

TL;DR: This paper compares Chain-of-Thought (CoT) and Direct prompting in Speech-to-Text Translation (S2TT) using LLM-based models, finding that Direct prompting improves more consistently with increasing S2TT data.


<details>
  <summary>Details</summary>
Motivation: To systematically compare CoT and Direct prompting strategies in S2TT under varying amounts of data, as CoT typically outperforms Direct but relies heavily on ASR and T2TT datasets.

Method: Pseudo-labeled an ASR corpus by translating transcripts into six European languages, then trained LLM-based S2TT systems with both prompting strategies at different data scales.

Result: Direct prompting improves more consistently as S2TT data increases, suggesting it may become more effective than CoT as larger S2TT resources become available.

Conclusion: Direct prompting shows better scalability with increasing S2TT data, potentially making it a more effective long-term approach compared to CoT prompting as S2TT datasets grow.

Abstract: Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based
models, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,
where the model is guided to first transcribe the speech and then translate it.
CoT typically outperforms direct prompting primarily because it can exploit
abundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)
datasets to explicitly model its steps. In this paper, we systematically
compare CoT and Direct prompting under increasing amounts of S2TT data. To this
end, we pseudo-label an ASR corpus by translating its transcriptions into six
European languages, and train LLM-based S2TT systems with both prompting
strategies at different data scales. Our results show that Direct improves more
consistently as the amount of data increases, suggesting that it may become a
more effective approach as larger S2TT resources are created.

</details>


### [66] [Semantic Similarity in Radiology Reports via LLMs and NER](https://arxiv.org/abs/2510.03102)
*Beth Pearson,Ahmed Adnan,Zahraa Abdallah*

Main category: cs.CL

TL;DR: The paper proposes Llama-EntScore, a method combining Llama 3.1 and NER with tunable weights to compare radiology reports, achieving 67% exact-match accuracy and outperforming standalone LLMs and NER approaches.


<details>
  <summary>Details</summary>
Motivation: Radiology report evaluation is crucial for training junior radiologists and ensuring diagnostic accuracy. Identifying semantic differences between preliminary and final reports helps uncover knowledge gaps and serves as a training tool, but applying LLMs in this specialized domain is challenging.

Method: Proposed Llama-EntScore: a semantic similarity scoring method using combination of Llama 3.1 and Named-Entity-Recognition with tunable weights to emphasize or de-emphasize specific types of differences. Generates quantitative similarity scores and interpretable feedback.

Result: Achieved 67% exact-match accuracy and 93% accuracy within +/- 1 when compared to radiologist-provided ground truth scores, outperforming both standalone LLMs and NER approaches.

Conclusion: The proposed Llama-EntScore method effectively addresses limitations of individual LLM and NER approaches in radiology report comparison, providing accurate semantic similarity assessment with interpretable feedback for training purposes.

Abstract: Radiology report evaluation is a crucial part of radiologists' training and
plays a key role in ensuring diagnostic accuracy. As part of the standard
reporting workflow, a junior radiologist typically prepares a preliminary
report, which is then reviewed and edited by a senior radiologist to produce
the final report. Identifying semantic differences between preliminary and
final reports is essential for junior doctors, both as a training tool and to
help uncover gaps in clinical knowledge. While AI in radiology is a rapidly
growing field, the application of large language models (LLMs) remains
challenging due to the need for specialised domain knowledge. In this paper, we
explore the ability of LLMs to provide explainable and accurate comparisons of
reports in the radiology domain. We begin by comparing the performance of
several LLMs in comparing radiology reports. We then assess a more traditional
approach based on Named-Entity-Recognition (NER). However, both approaches
exhibit limitations in delivering accurate feedback on semantic similarity. To
address this, we propose Llama-EntScore, a semantic similarity scoring method
using a combination of Llama 3.1 and NER with tunable weights to emphasise or
de-emphasise specific types of differences. Our approach generates a
quantitative similarity score for tracking progress and also gives an
interpretation of the score that aims to offer valuable guidance in reviewing
and refining their reporting. We find our method achieves 67% exact-match
accuracy and 93% accuracy within +/- 1 when compared to radiologist-provided
ground truth scores - outperforming both LLMs and NER used independently. Code
is available at:
\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\_reports}

</details>


### [67] [Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation](https://arxiv.org/abs/2510.03115)
*Jacobo Romero-Díaz,Gerard I. Gállego,Oriol Pareras,Federico Costa,Javier Hernando,Cristina España-Bonet*

Main category: cs.CL

TL;DR: Chain-of-Thought prompting for Speech-to-Text Translation largely behaves like cascaded systems, relying mainly on transcripts rather than leveraging speech information, despite expectations to overcome error propagation and acoustic cue limitations.


<details>
  <summary>Details</summary>
Motivation: To address limitations in cascaded Speech-to-Text Translation systems, including error propagation and inability to use prosodic/acoustic cues, by analyzing whether Chain-of-Thought prompting can effectively integrate speech and transcription.

Method: Used attribution methods, robustness evaluations with corrupted transcripts, and prosody-awareness analysis to examine CoT behavior. Also tested training interventions like adding Direct S2TT data and noisy transcript injection.

Result: CoT largely mirrors cascaded behavior, relying mainly on transcripts while barely leveraging speech. Training interventions enhanced robustness and increased speech attribution.

Conclusion: Findings challenge assumed CoT advantages and highlight need for architectures that explicitly integrate acoustic information into translation.

Abstract: Speech-to-Text Translation (S2TT) systems built from Automatic Speech
Recognition (ASR) and Text-to-Text Translation (T2TT) modules face two major
limitations: error propagation and the inability to exploit prosodic or other
acoustic cues. Chain-of-Thought (CoT) prompting has recently been introduced,
with the expectation that jointly accessing speech and transcription will
overcome these issues. Analyzing CoT through attribution methods, robustness
evaluations with corrupted transcripts, and prosody-awareness, we find that it
largely mirrors cascaded behavior, relying mainly on transcripts while barely
leveraging speech. Simple training interventions, such as adding Direct S2TT
data or noisy transcript injection, enhance robustness and increase speech
attribution. These findings challenge the assumed advantages of CoT and
highlight the need for architectures that explicitly integrate acoustic
information into translation.

</details>


### [68] [SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?](https://arxiv.org/abs/2510.03120)
*Zhaojun Sun,Xuzhou Zhu,Xuanhe Zhou,Xin Tong,Shuo Wang,Jie Fu,Guoliang Li,Zhiyuan Liu,Fan Wu*

Main category: cs.CL

TL;DR: SurveyBench is a quiz-driven evaluation framework that assesses automated survey generation methods, revealing they perform 21% worse than human-written surveys.


<details>
  <summary>Details</summary>
Motivation: Existing automated survey generation methods (LLM4Survey) produce outputs that fall short of human standards, and there's a lack of rigorous benchmarks to evaluate their deficiencies from a reader's perspective.

Method: Proposed SurveyBench framework with: (1) survey topics from 11,343 arXiv papers and 4,947 high-quality surveys; (2) multifaceted metrics assessing outline quality, content quality, and non-textual richness; (3) dual-mode evaluation with content-based and quiz-based answerability tests.

Result: SurveyBench effectively challenges existing LLM4Survey approaches, showing they perform on average 21% lower than human-written surveys in content-based evaluation.

Conclusion: SurveyBench provides a comprehensive benchmark that reveals significant gaps between automated survey generation methods and human-written surveys, highlighting the need for improved LLM4Survey approaches.

Abstract: Academic survey writing, which distills vast literature into a coherent and
insightful narrative, remains a labor-intensive and intellectually demanding
task. While recent approaches, such as general DeepResearch agents and
survey-specialized methods, can generate surveys automatically (a.k.a.
LLM4Survey), their outputs often fall short of human standards and there lacks
a rigorous, reader-aligned benchmark for thoroughly revealing their
deficiencies. To fill the gap, we propose a fine-grained, quiz-driven
evaluation framework SurveyBench, featuring (1) typical survey topics source
from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;
(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,
coverage breadth, logical coherence), content quality (e.g., synthesis
granularity, clarity of insights), and non-textual richness; and (3) a
dual-mode evaluation protocol that includes content-based and quiz-based
answerability tests, explicitly aligned with readers' informational needs.
Results show SurveyBench effectively challenges existing LLM4Survey approaches
(e.g., on average 21% lower than human in content-based evaluation).

</details>


### [69] [Beyond the Final Layer: Intermediate Representations for Better Multilingual Calibration in Large Language Models](https://arxiv.org/abs/2510.03136)
*Ej Zhou,Caiqi Zhang,Tiancheng Hu,Chengzu Li,Nigel Collier,Ivan Vulić,Anna Korhonen*

Main category: cs.CL

TL;DR: This paper reveals systematic calibration issues in multilingual LLMs, showing non-English languages have worse confidence calibration due to English-centric training biases in final layers, and proposes training-free methods like LACE that use late-intermediate layers for better multilingual calibration.


<details>
  <summary>Details</summary>
Motivation: Confidence calibration is crucial for reliable LLM deployment but remains under-explored in multilingual contexts, with non-English languages suffering from systematically worse calibration due to English-centric training biases.

Method: Conducted large-scale multilingual calibration studies across 6 model families and 100+ languages, analyzed internal representations layer-wise, and introduced training-free methods including Language-Aware Confidence Ensemble (LACE) that adaptively selects optimal layer ensembles per language.

Result: Found that final layers provide poor multilingual confidence signals due to English-centric bias, while late-intermediate layers offer more reliable and better-calibrated signals across languages.

Conclusion: Highlights hidden costs of English-centric alignment and provides a path toward more globally equitable LLMs by looking beyond final layers for multilingual confidence calibration.

Abstract: Confidence calibration, the alignment of a model's predicted confidence with
its actual accuracy, is crucial for the reliable deployment of Large Language
Models (LLMs). However, this critical property remains largely under-explored
in multilingual contexts. In this work, we conduct the first large-scale,
systematic studies of multilingual calibration across six model families and
over 100 languages, revealing that non-English languages suffer from
systematically worse calibration. To diagnose this, we investigate the model's
internal representations and find that the final layer, biased by
English-centric training, provides a poor signal for multilingual confidence.
In contrast, our layer-wise analysis uncovers a key insight that
late-intermediate layers consistently offer a more reliable and
better-calibrated signal. Building on this, we introduce a suite of
training-free methods, including Language-Aware Confidence Ensemble (LACE),
which adaptively selects an optimal ensemble of layers for each specific
language. Our study highlights the hidden costs of English-centric alignment
and offer a new path toward building more globally equitable and trustworthy
LLMs by looking beyond the final layer.

</details>


### [70] [EditLens: Quantifying the Extent of AI Editing in Text](https://arxiv.org/abs/2510.03154)
*Katherine Thai,Bradley Emi,Elyas Masrour,Mohit Iyyer*

Main category: cs.CL

TL;DR: EditLens detects AI-edited text using similarity metrics and achieves state-of-the-art performance in distinguishing human, AI, and mixed writing.


<details>
  <summary>Details</summary>
Motivation: Many queries to LLMs involve editing user-provided text rather than generating new text, but previous work only focused on detecting fully AI-generated text.

Method: Proposed lightweight similarity metrics to quantify AI editing, then trained EditLens regression model using these metrics as intermediate supervision.

Result: Achieved F1=94.7% on binary classification and F1=90.4% on ternary classification tasks. Successfully analyzed AI-edits from Grammarly as a case study.

Conclusion: AI-edited text is distinguishable from human-written and AI-generated text, and the degree of AI editing can be detected, with implications for authorship, education, and policy.

Abstract: A significant proportion of queries to large language models ask them to edit
user-provided text, rather than generate new text from scratch. While previous
work focuses on detecting fully AI-generated text, we demonstrate that
AI-edited text is distinguishable from human-written and AI-generated text.
First, we propose using lightweight similarity metrics to quantify the
magnitude of AI editing present in a text given the original human-written text
and validate these metrics with human annotators. Using these similarity
metrics as intermediate supervision, we then train EditLens, a regression model
that predicts the amount of AI editing present within a text. Our model
achieves state-of-the-art performance on both binary (F1=94.7%) and ternary
(F1=90.4%) classification tasks in distinguishing human, AI, and mixed writing.
Not only do we show that AI-edited text can be detected, but also that the
degree of change made by AI to human writing can be detected, which has
implications for authorship attribution, education, and policy. Finally, as a
case study, we use our model to analyze the effects of AI-edits applied by
Grammarly, a popular writing assistance tool. To encourage further research, we
commit to publicly releasing our models and dataset.

</details>


### [71] [Neural Correlates of Language Models Are Specific to Human Language](https://arxiv.org/abs/2510.03156)
*Iñigo Parra*

Main category: cs.CL

TL;DR: This paper validates previous findings about correlations between LLM hidden states and fMRI brain responses, addressing concerns about dimensionality, similarity measures, training specificity, and positional encoding.


<details>
  <summary>Details</summary>
Motivation: To test the robustness of previous findings about correlations between LLM representations and brain states, addressing potential methodological concerns.

Method: Used dimensionality reduction, new similarity measures, compared models trained on human language vs. others, and examined the role of positional encoding.

Result: Confirmed previous correlations are robust to dimensionality concerns, validated with new similarity measures, specific to human language-trained models, and dependent on positional encoding.

Conclusion: The results strengthen evidence for representational similarity between LLMs and brain states, supporting debates about biological plausibility of language models.

Abstract: Previous work has shown correlations between the hidden states of large
language models and fMRI brain responses, on language tasks. These correlations
have been taken as evidence of the representational similarity of these models
and brain states. This study tests whether these previous results are robust to
several possible concerns. Specifically this study shows: (i) that the previous
results are still found after dimensionality reduction, and thus are not
attributable to the curse of dimensionality; (ii) that previous results are
confirmed when using new measures of similarity; (iii) that correlations
between brain representations and those from models are specific to models
trained on human language; and (iv) that the results are dependent on the
presence of positional encoding in the models. These results confirm and
strengthen the results of previous research and contribute to the debate on the
biological plausibility and interpretability of state-of-the-art large language
models.

</details>


### [72] [Topic Modeling as Long-Form Generation: Can Long-Context LLMs revolutionize NTM via Zero-Shot Prompting?](https://arxiv.org/abs/2510.03174)
*Xuan Xu,Haolun Li,Zhongliang Yang,Beilin Chu,Jia Song,Moxuan Xu,Linna Zhou*

Main category: cs.CL

TL;DR: This paper proposes a new paradigm for topic modeling using large language models (LLMs) as long-form generation tasks, comparing them against traditional neural topic models (NTMs) to assess if LLMs can outperform NTMs through zero-shot prompting.


<details>
  <summary>Details</summary>
Motivation: To explore whether the era of large language models can provide a better alternative to traditional neural topic models by reframing topic modeling as a long-form generation task.

Method: Proposes a simple practical approach: sample data subset, generate topics and representative text using prompts, and assign text via keyword matching. Conducts systematic comparison between NTMs and LLMs using zero-shot prompting.

Result: Empirically examines the claim that 'a majority of NTMs are outdated' by comparing topic quality between LLMs and NTMs.

Conclusion: The paper investigates whether the long-form generation paradigm using LLMs can outperform traditional neural topic models, potentially making many existing NTMs obsolete.

Abstract: Traditional topic models such as neural topic models rely on inference and
generation networks to learn latent topic distributions. This paper explores a
new paradigm for topic modeling in the era of large language models, framing TM
as a long-form generation task whose definition is updated in this paradigm. We
propose a simple but practical approach to implement LLM-based topic model
tasks out of the box (sample a data subset, generate topics and representative
text with our prompt, text assignment with keyword match). We then investigate
whether the long-form generation paradigm can beat NTMs via zero-shot
prompting. We conduct a systematic comparison between NTMs and LLMs in terms of
topic quality and empirically examine the claim that "a majority of NTMs are
outdated."

</details>


### [73] [Model-Based Ranking of Source Languages for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2510.03202)
*Abteen Ebrahimi,Adam Wiemerslage,Katharina von der Wense*

Main category: cs.CL

TL;DR: NN-Rank is a novel algorithm for ranking source languages in cross-lingual transfer tasks using multilingual model representations and unlabeled target-language data, outperforming existing methods on POS tagging and NER tasks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for source language ranking rely on lexical and linguistic features, which may not fully capture transferability potential. The authors aim to leverage multilingual model representations to better predict which source languages will transfer most effectively to target languages.

Method: NN-Rank uses hidden representations from pretrained multilingual models combined with unlabeled target-language data to rank source languages. The method was tested on 51 source languages for POS tagging (56 targets) and NER (72 targets) using two multilingual models.

Result: NN-Rank significantly outperforms state-of-the-art baselines, achieving average improvements of up to 35.56 NDCG for POS and 18.14 NDCG for NER. It remains competitive using only the Bible as out-of-domain data and works well with as few as 25 target examples (achieving 92.8% of full-data NDCG).

Conclusion: NN-Rank provides an effective and data-efficient method for source language ranking in cross-lingual transfer, leveraging multilingual model representations to outperform feature-based approaches while requiring minimal target language data.

Abstract: We present NN-Rank, an algorithm for ranking source languages for
cross-lingual transfer, which leverages hidden representations from
multilingual models and unlabeled target-language data. We experiment with two
pretrained multilingual models and two tasks: part-of-speech tagging (POS) and
named entity recognition (NER). We consider 51 source languages and evaluate on
56 and 72 target languages for POS and NER, respectively. When using in-domain
data, NN-Rank beats state-of-the-art baselines that leverage lexical and
linguistic features, with average improvements of up to 35.56 NDCG for POS and
18.14 NDCG for NER. As prior approaches can fall back to language-level
features if target language data is not available, we show that NN-Rank remains
competitive using only the Bible, an out-of-domain corpus available for a large
number of languages. Ablations on the amount of unlabeled target data show
that, for subsets consisting of as few as 25 examples, NN-Rank produces
high-quality rankings which achieve 92.8% of the NDCG achieved using all
available target data for ranking.

</details>


### [74] [FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents](https://arxiv.org/abs/2510.03204)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Léo Boisvert,Massimo Caccia,Jérémy Espinas,Alexandre Aussem,Véronique Eglin,Alexandre Lacoste*

Main category: cs.CL

TL;DR: FocusAgent uses a lightweight LLM retriever to extract relevant lines from web page accessibility trees, reducing observation size by over 50% while maintaining performance and improving security against prompt injection attacks.


<details>
  <summary>Details</summary>
Motivation: Current web agents process lengthy web pages that exceed context limits, increase computational costs, and expose agents to security risks like prompt injection. Existing pruning methods either discard relevant content or keep irrelevant context.

Method: Leverages a lightweight LLM retriever to extract the most relevant lines from accessibility tree (AxTree) observations, guided by task goals. This prunes noisy and irrelevant content.

Result: On WorkArena and WebArena benchmarks, FocusAgent matches baseline performance while reducing observation size by over 50%. A variant significantly reduces prompt-injection attack success rates while maintaining task success in attack-free settings.

Conclusion: Targeted LLM-based retrieval is a practical and robust strategy for building web agents that are efficient, effective, and secure.

Abstract: Web agents powered by large language models (LLMs) must process lengthy web
page observations to complete user goals; these pages often exceed tens of
thousands of tokens. This saturates context limits and increases computational
cost processing; moreover, processing full pages exposes agents to security
risks such as prompt injection. Existing pruning strategies either discard
relevant content or retain irrelevant context, leading to suboptimal action
prediction. We introduce FocusAgent, a simple yet effective approach that
leverages a lightweight LLM retriever to extract the most relevant lines from
accessibility tree (AxTree) observations, guided by task goals. By pruning
noisy and irrelevant content, FocusAgent enables efficient reasoning while
reducing vulnerability to injection attacks. Experiments on WorkArena and
WebArena benchmarks show that FocusAgent matches the performance of strong
baselines, while reducing observation size by over 50%. Furthermore, a variant
of FocusAgent significantly reduces the success rate of prompt-injection
attacks, including banner and pop-up attacks, while maintaining task success
performance in attack-free settings. Our results highlight that targeted
LLM-based retrieval is a practical and robust strategy for building web agents
that are efficient, effective, and secure.

</details>


### [75] [Cache-to-Cache: Direct Semantic Communication Between Large Language Models](https://arxiv.org/abs/2510.03215)
*Tianyu Fu,Zihan Min,Hanling Zhang,Jichao Yan,Guohao Dai,Wanli Ouyang,Yu Wang*

Main category: cs.CL

TL;DR: C2C enables direct KV-cache communication between LLMs, bypassing text generation for faster, more accurate multi-model systems.


<details>
  <summary>Details</summary>
Motivation: Text communication between LLMs loses semantic information and incurs token-by-token latency; KV-cache can preserve rich semantics for better inter-model communication.

Method: Uses neural network to project and fuse source model's KV-cache with target model's cache, with learnable gating to select beneficial layers for semantic transfer.

Result: Achieves 8.5-10.5% higher accuracy than individual models, outperforms text communication by 3.0-5.0% with 2.0x speedup in latency.

Conclusion: KV-cache communication is more effective than text for multi-LLM systems, providing both performance gains and efficiency improvements.

Abstract: Multi-LLM systems harness the complementary strengths of diverse Large
Language Models, achieving performance and efficiency gains unattainable by a
single model. In existing designs, LLMs communicate through text, forcing
internal representations to be transformed into output token sequences. This
process both loses rich semantic information and incurs token-by-token
generation latency. Motivated by these limitations, we ask: Can LLMs
communicate beyond text? Oracle experiments show that enriching the KV-Cache
semantics can improve response quality without increasing cache size,
supporting KV-Cache as an effective medium for inter-model communication. Thus,
we propose Cache-to-Cache (C2C), a new paradigm for direct semantic
communication between LLMs. C2C uses a neural network to project and fuse the
source model's KV-cache with that of the target model to enable direct semantic
transfer. A learnable gating mechanism selects the target layers that benefit
from cache communication. Compared with text communication, C2C utilizes the
deep, specialized semantics from both models, while avoiding explicit
intermediate text generation. Experiments show that C2C achieves 8.5-10.5%
higher average accuracy than individual models. It further outperforms the text
communication paradigm by approximately 3.0-5.0%, while delivering an average
2.0x speedup in latency. Our code is available at
https://github.com/thu-nics/C2C.

</details>


### [76] [Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment](https://arxiv.org/abs/2510.03223)
*Hongxiang Zhang,Yuan Tian,Tianyi Zhang*

Main category: cs.CL

TL;DR: Self-Anchor is a prompting method that structures reasoning trajectories to maintain LLM attention on critical steps, improving performance on complex reasoning tasks without retraining.


<details>
  <summary>Details</summary>
Motivation: As reasoning chains extend in LLMs, critical intermediate steps and original prompts get buried in context, receiving insufficient attention and causing errors.

Method: Decomposes reasoning trajectories into structured plans and automatically aligns model attention to the most relevant inference steps throughout generation.

Result: Outperforms SOTA prompting methods across six benchmarks, significantly reduces performance gap between non-reasoning and specialized reasoning models.

Conclusion: Self-Anchor enables most LLMs to tackle complex reasoning tasks without retraining by maintaining focus on critical reasoning steps.

Abstract: To solve complex reasoning tasks for Large Language Models (LLMs),
prompting-based methods offer a lightweight alternative to fine-tuning and
reinforcement learning. However, as reasoning chains extend, critical
intermediate steps and the original prompt will be buried in the context,
receiving insufficient attention and leading to errors. In this paper, we
propose Self-Anchor, a novel pipeline that leverages the inherent structure of
reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories
into structured plans and automatically aligns the model's attention to the
most relevant inference steps, allowing the model to maintain focus throughout
generation. Our experiment shows that Self-Anchor outperforms SOTA prompting
methods across six benchmarks. Notably, Self-Anchor significantly reduces the
performance gap between ``non-reasoning'' models and specialized reasoning
models, with the potential to enable most LLMs to tackle complex reasoning
tasks without retraining.

</details>


### [77] [Reward Models are Metrics in a Trench Coat](https://arxiv.org/abs/2510.03231)
*Sebastian Gehrmann*

Main category: cs.CL

TL;DR: Reward models and evaluation metrics share similar goals but are studied separately, leading to redundant work. Closer collaboration could address common challenges like spurious correlations and reward hacking.


<details>
  <summary>Details</summary>
Motivation: To highlight the separation between reward models in RL post-training and evaluation metrics, which causes redundant terminology and repeated pitfalls in both fields.

Method: Analyze the similarities between reward models and evaluation metrics, conduct an extensive survey of both areas, and demonstrate how metrics outperform reward models on specific tasks.

Result: Found that metrics can outperform reward models on certain tasks, and identified multiple research topics where closer alignment could benefit both fields.

Conclusion: Closer collaboration between reward modeling and evaluation metrics research can help overcome common challenges and improve both areas through shared insights and methods.

Abstract: The emergence of reinforcement learning in post-training of large language
models has sparked significant interest in reward models. Reward models assess
the quality of sampled model outputs to generate training signals. This task is
also performed by evaluation metrics that monitor the performance of an AI
model. We find that the two research areas are mostly separate, leading to
redundant terminology and repeated pitfalls. Common challenges include
susceptibility to spurious correlations, impact on downstream reward hacking,
methods to improve data quality, and approaches to meta-evaluation. Our
position paper argues that a closer collaboration between the fields can help
overcome these issues. To that end, we show how metrics outperform reward
models on specific tasks and provide an extensive survey of the two areas.
Grounded in this survey, we point to multiple research topics in which closer
alignment can improve reward models and metrics in areas such as preference
elicitation methods, avoidance of spurious correlations and reward hacking, and
calibration-aware meta-evaluation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [78] [BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks](https://arxiv.org/abs/2510.02418)
*Sagnik Anupam,Davis Brown,Shuo Li,Eric Wong,Hamed Hassani,Osbert Bastani*

Main category: cs.AI

TL;DR: BrowserArena is a live open-web evaluation platform for LLM web agents that identifies key failure modes (captcha resolution, pop-up banner removal, direct URL navigation) through step-level human feedback and head-to-head comparisons.


<details>
  <summary>Details</summary>
Motivation: Current web agent evaluations are limited to sandboxed environments or artificial tasks, lacking real-world testing on the open web where agents face complex challenges.

Method: Developed BrowserArena platform that collects user-submitted tasks, runs Arena-style head-to-head comparisons, and uses step-level human feedback to analyze agent traces and identify failure patterns.

Result: Identified three consistent failure modes across models: captcha resolution, pop-up banner removal, and direct URL navigation. Found model-specific variations - o4-mini uses diverse captcha circumvention strategies while DeepSeek-R1 misleads users about captcha resolution.

Conclusion: Current web agents show both diversity and brittleness in real-world scenarios. The benchmarking methodology provides scalable approach to evaluate and understand web agent failure modes.

Abstract: LLM web agents now browse and take actions on the open web, yet current agent
evaluations are constrained to sandboxed environments or artificial tasks. We
introduce BrowserArena, a live open-web agent evaluation platform that collects
user-submitted tasks, runs Arena-style head-to-head comparisons, and uses
step-level human feedback to surface failure modes. Collecting and analyzing
step-level annotations on the agent traces, we identify three consistent
failure modes: captcha resolution, pop-up banner removal, and direct navigation
to URLs. By constructing targeted datasets to further study these tasks, we
discover variations in how different language models navigate these failure
modes. We find, for example, that o4-mini deploys a wider variety of strategies
to circumvent captcha resolution than other models and DeepSeek-R1 consistently
misleads users about captcha resolution. Our findings surface both the
diversity and brittleness of current web agents. More broadly, our benchmarking
methodology provides an approach to evaluating and understanding web agent
failure modes at scale.

</details>


### [79] [RefineShot: Rethinking Cinematography Understanding with Foundational Skill Evaluation](https://arxiv.org/abs/2510.02423)
*Hang Wu,Yujun Cai,Haonan Ge,Hongkai Chen,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: The paper identifies issues with ShotBench benchmark and ShotVL model for cinematography understanding, and introduces RefineShot - a refined benchmark with improved option design and extended evaluation protocol.


<details>
  <summary>Details</summary>
Motivation: To address reliability issues in current cinematography understanding benchmarks, specifically ambiguous option design in ShotBench and reasoning inconsistencies in ShotVL, which hinder fair model comparison and future progress.

Method: Systematically refine ShotBench through consistent option restructuring, conduct critical analysis of ShotVL's reasoning behavior, and introduce extended evaluation protocol assessing both task accuracy and core model competencies.

Result: Development of RefineShot benchmark that enables more reliable assessment of cinematography understanding models through improved option design and comprehensive evaluation metrics.

Conclusion: RefineShot provides a more robust foundation for evaluating cinematography understanding, addressing previous limitations and fostering future advances in this field through reliable benchmarking.

Abstract: Cinematography understanding refers to the ability to recognize not only the
visual content of a scene but also the cinematic techniques that shape
narrative meaning. This capability is attracting increasing attention, as it
enhances multimodal understanding in real-world applications and underpins
coherent content creation in film and media. As the most comprehensive
benchmark for this task, ShotBench spans a wide range of cinematic concepts and
VQA-style evaluations, with ShotVL achieving state-of-the-art results on it.
However, our analysis reveals that ambiguous option design in ShotBench and
ShotVL's shortcomings in reasoning consistency and instruction adherence
undermine evaluation reliability, limiting fair comparison and hindering future
progress. To overcome these issues, we systematically refine ShotBench through
consistent option restructuring, conduct the first critical analysis of
ShotVL's reasoning behavior, and introduce an extended evaluation protocol that
jointly assesses task accuracy and core model competencies. These efforts lead
to RefineShot, a refined and expanded benchmark that enables more reliable
assessment and fosters future advances in cinematography understanding.

</details>


### [80] [Safe and Efficient In-Context Learning via Risk Control](https://arxiv.org/abs/2510.02480)
*Andrea Wynn,Metod Jazbec,Charith Peris,Rinat Khaziev,Anqi Liu,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: Proposes a safety mechanism using distribution-free risk control to limit performance degradation from harmful in-context demonstrations while maintaining computational efficiency gains from helpful demonstrations.


<details>
  <summary>Details</summary>
Motivation: Address safety concerns where LLMs can be influenced by incorrect or malicious in-context examples, requiring built-in mechanisms to guard against such attacks.

Method: Define baseline safe behavior (zero-shot performance), apply distribution-free risk control with dynamic early exit prediction that ignores attention heads focusing on unsafe inputs, and modify DFRC to handle both harmful and helpful inputs.

Result: The approach effectively controls risk for harmful in-context demonstrations while achieving substantial computational efficiency gains with helpful demonstrations.

Conclusion: The proposed method provides principled safety design that protects LLMs from malicious demonstrations while maintaining performance benefits from legitimate examples.

Abstract: Large language models (LLMs) demonstrate a remarkable ability to learn new
tasks from a few in-context examples. However, this flexibility introduces
safety concerns: LLMs can be influenced by incorrect or malicious
demonstrations -- for example, if an adversary tampers with or injects harmful
examples without a human supervisor noticing. This motivates principled designs
in which the system itself includes built-in mechanisms to guard against such
attacks. We propose a novel approach to limit the degree to which harmful
demonstrations can degrade model performance. First, we define a baseline
``safe'' behavior for the model -- the model's performance given no in-context
demonstrations (zero-shot). Next, we apply distribution-free risk control
(DFRC) to control the extent to which in-context samples can decay performance
below zero-shot. We achieve this by leveraging dynamic early exit prediction,
ignoring later attention heads that attend the most to the unsafe inputs.
Finally, we propose modifications to DFRC that allow it to both control risk
for harmful inputs \textit{and} leverage performance and efficiency gains on
helpful inputs. We present both theoretical and empirical results showing that
our approach can effectively control risk for harmful in-context demonstrations
while simultaneously achieving substantial computational efficiency gains with
helpful demonstrations.

</details>


### [81] [Multimodal Function Vectors for Spatial Relations](https://arxiv.org/abs/2510.02528)
*Shuhao Fu,Esther Goldberg,Ying Nian Wu,Hongjing Lu*

Main category: cs.AI

TL;DR: The paper identifies specific attention heads in OpenFlamingo-4B that encode spatial relations, extracts their activations as "function vectors," and shows these vectors can be manipulated to improve relational reasoning performance without retraining the full model.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms behind Large Multimodal Models' in-context learning abilities for relational tasks, particularly how spatial relational knowledge is encoded and can be systematically extracted.

Method: Applied causal mediation analysis to identify key attention heads responsible for spatial relations, extracted their activations as multimodal function vectors, and fine-tuned these vectors with minimal training data while keeping the LMM parameters frozen.

Result: Function vectors improved zero-shot accuracy, outperformed in-context learning baselines after fine-tuning, and could be linearly combined to solve analogy problems with novel spatial relations, demonstrating strong generalization.

Conclusion: LMMs encode spatial relational knowledge in localized internal structures that can be systematically extracted and optimized, advancing understanding of model modularity and enhancing control over relational reasoning.

Abstract: Large Multimodal Models (LMMs) demonstrate impressive in-context learning
abilities from limited multimodal demonstrations, yet the internal mechanisms
supporting such task learning remain opaque. Building on prior work of large
language models, we show that a small subset of attention heads in the
vision-language model OpenFlamingo-4B is responsible for transmitting
representations of spatial relations. The activations of these attention heads,
termed function vectors, can be extracted and manipulated to alter an LMM's
performance on relational tasks. First, using both synthetic and real image
datasets, we apply causal mediation analysis to identify attention heads that
strongly influence relational predictions, and extract multimodal function
vectors that improve zero-shot accuracy at inference time. We further
demonstrate that these multimodal function vectors can be fine-tuned with a
modest amount of training data, while keeping LMM parameters frozen, to
significantly outperform in-context learning baselines. Finally, we show that
relation-specific function vectors can be linearly combined to solve analogy
problems involving novel and untrained spatial relations, highlighting the
strong generalization ability of this approach. Our results show that LMMs
encode spatial relational knowledge within localized internal structures, which
can be systematically extracted and optimized, thereby advancing our
understanding of model modularity and enhancing control over relational
reasoning in LMMs.

</details>


### [82] [Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge](https://arxiv.org/abs/2510.02557)
*Charlie Masters,Advaith Vellanki,Jiangbo Shangguan,Bart Kultys,Jonathan Gilmore,Alastair Moore,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: This paper proposes Autonomous Manager Agents for orchestrating multi-agent workflows in human-AI teams, identifies key challenges in workflow management, and releases MA-Gym framework to evaluate such systems.


<details>
  <summary>Details</summary>
Motivation: While AI has advanced in automating individual tasks, managing complex multi-agent workflows remains challenging, requiring systems that can orchestrate collaboration in dynamic human-AI teams.

Method: Formalizes workflow management as a Partially Observable Stochastic Game, proposes Autonomous Manager Agent concept, identifies four foundational challenges, and releases MA-Gym simulation framework for evaluation.

Result: Evaluation of GPT-5-based Manager Agents across 20 workflows shows they struggle to jointly optimize for goal completion, constraint adherence, and workflow runtime, highlighting workflow management as a difficult open problem.

Conclusion: Workflow management remains a significant challenge for autonomous agentic systems, with important organizational and ethical implications that need to be addressed as these systems develop.

Abstract: While agentic AI has advanced in automating individual tasks, managing
complex multi-agent workflows remains a challenging problem. This paper
presents a research vision for autonomous agentic systems that orchestrate
collaboration within dynamic human-AI teams. We propose the Autonomous Manager
Agent as a core challenge: an agent that decomposes complex goals into task
graphs, allocates tasks to human and AI workers, monitors progress, adapts to
changing conditions, and maintains transparent stakeholder communication. We
formalize workflow management as a Partially Observable Stochastic Game and
identify four foundational challenges: (1) compositional reasoning for
hierarchical decomposition, (2) multi-objective optimization under shifting
preferences, (3) coordination and planning in ad hoc teams, and (4) governance
and compliance by design. To advance this agenda, we release MA-Gym, an
open-source simulation and evaluation framework for multi-agent workflow
orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we
find they struggle to jointly optimize for goal completion, constraint
adherence, and workflow runtime - underscoring workflow management as a
difficult open problem. We conclude with organizational and ethical
implications of autonomous management systems.

</details>


### [83] [Agentic Additive Manufacturing Alloy Discovery](https://arxiv.org/abs/2510.02567)
*Peter Pak,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.AI

TL;DR: This paper presents a multi-agent system using LLMs to automate alloy discovery in additive manufacturing by leveraging tools like Thermo-Calc calculations and process map generation.


<details>
  <summary>Details</summary>
Motivation: Alloy discovery in additive manufacturing is complex and requires expertise across multiple domains. The authors aim to automate and accelerate this process using LLM-enabled agents to augment researchers' capabilities.

Method: Developed a multi-agent system using Large Language Models that dispatch tool calls via Model Context Protocol to perform thermodynamic simulations (Thermo-Calc property diagrams) and generate process maps (lack of fusion analysis). The system can reason through complex prompts and dynamically adjust task trajectories based on tool results.

Result: The multi-agent system effectively reasons through user prompts and provides analysis on alloy printability. It enables autonomous decision-making by dynamically adjusting task trajectories according to tool call outcomes.

Conclusion: LLM-enabled agents can successfully automate and accelerate alloy discovery in additive manufacturing, demonstrating the benefits of adopting multi-agent systems for complex research tasks requiring cross-domain expertise.

Abstract: Agentic systems enable the intelligent use of research tooling, augmenting a
researcher's ability to investigate and propose novel solutions to existing
problems. Within Additive Manufacturing (AM), alloy discovery remains a complex
challenge, often requiring expertise in the various domains of materials
science, thermodynamic simulations, and experimental analysis. Large Language
Model (LLM) enabled agents can facilitate this endeavor by utilizing their
extensive knowledge base to dispatch tool calls via Model Context Protocol
(MCP) to perform actions such as Thermo-Calc property diagram calculations and
lack of fusion process map generation. In addition, the multi-agent system
developed in this work is able to effectively reason through complex user
prompts and provide analysis on the printability of proposed alloys. These
agents can dynamically adjust their task trajectory to the outcomes of tool
call results, effectively enabling autonomous decision-making in practical
environments. This work aims to utilize LLM enabled agents to automate and
accelerate the task of alloy discovery within the field of additive
manufacturing and showcase the benefits of adopting this multi-agent system.

</details>


### [84] [A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem](https://arxiv.org/abs/2510.02589)
*Yunqi Huang,Nishith Chennakeshava,Alexis Carras,Vladislav Neverov,Wei Liu,Aske Plaat,Yingjie Fan*

Main category: cs.AI

TL;DR: This paper benchmarks five RL algorithms (DQN, QR-DQN, A2C, PPO, TRPO) for container stowage planning with crane scheduling, revealing performance gaps as complexity increases.


<details>
  <summary>Details</summary>
Motivation: Container stowage planning is crucial for maritime logistics but traditionally relies on human expertise. While RL has been applied, there's a lack of systematic benchmark comparisons across different algorithms.

Method: Developed a Gym environment capturing CSPP fundamentals with crane scheduling in both multi-agent and single-agent formulations. Evaluated five RL algorithms under multiple complexity scenarios.

Result: Results show distinct performance gaps with increasing complexity, highlighting the importance of algorithm choice and problem formulation for CSPP.

Conclusion: The paper provides a benchmark for RL methods in CSPP and offers a reusable Gym environment with crane scheduling, serving as a foundation for future research and practical deployment.

Abstract: Container stowage planning (CSPP) is a critical component of maritime
transportation and terminal operations, directly affecting supply chain
efficiency. Owing to its complexity, CSPP has traditionally relied on human
expertise. While reinforcement learning (RL) has recently been applied to CSPP,
systematic benchmark comparisons across different algorithms remain limited. To
address this gap, we develop a Gym environment that captures the fundamental
features of CSPP and extend it to include crane scheduling in both multi-agent
and single-agent formulations. Within this framework, we evaluate five RL
algorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying
complexity. The results reveal distinct performance gaps with increasing
complexity, underscoring the importance of algorithm choice and problem
formulation for CSPP. Overall, this paper benchmarks multiple RL methods for
CSPP while providing a reusable Gym environment with crane scheduling, thus
offering a foundation for future research and practical deployment in maritime
logistics.

</details>


### [85] [Multimodal Large Language Model Framework for Safe and Interpretable Grid-Integrated EVs](https://arxiv.org/abs/2510.02592)
*Jean Douglas Carvalho,Hugo Kenji,Ahmad Mohammad Saber,Glaucia Melo,Max Mauro Dias Santos,Deepa Kundur*

Main category: cs.AI

TL;DR: A multi-modal LLM framework that processes sensor data (object detection, segmentation, telemetry) to generate natural-language alerts for EV drivers, validated with real-world urban driving data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring safe and interpretable interactions between drivers, vehicles, and environment in EV-smart grid integration.

Method: Combines visual perception (YOLOv8), geocoded positioning, and CAN bus telemetry with large language models to bridge raw sensor data and driver comprehension.

Result: Case studies demonstrate effectiveness in generating context-aware alerts for critical situations like proximity to pedestrians, cyclists, and other vehicles.

Conclusion: LLMs show potential as assistive tools in e-mobility, enabling scalable fleet coordination, EV load forecasting, and traffic-aware energy planning for both transportation systems and electric networks.

Abstract: The integration of electric vehicles (EVs) into smart grids presents unique
opportunities to enhance both transportation systems and energy networks.
However, ensuring safe and interpretable interactions between drivers,
vehicles, and the surrounding environment remains a critical challenge. This
paper presents a multi-modal large language model (LLM)-based framework to
process multimodal sensor data - such as object detection, semantic
segmentation, and vehicular telemetry - and generate natural-language alerts
for drivers. The framework is validated using real-world data collected from
instrumented vehicles driving on urban roads, ensuring its applicability to
real-world scenarios. By combining visual perception (YOLOv8), geocoded
positioning, and CAN bus telemetry, the framework bridges raw sensor data and
driver comprehension, enabling safer and more informed decision-making in urban
driving scenarios. Case studies using real data demonstrate the framework's
effectiveness in generating context-aware alerts for critical situations, such
as proximity to pedestrians, cyclists, and other vehicles. This paper
highlights the potential of LLMs as assistive tools in e-mobility, benefiting
both transportation systems and electric networks by enabling scalable fleet
coordination, EV load forecasting, and traffic-aware energy planning.
  Index Terms - Electric vehicles, visual perception, large language models,
YOLOv8, semantic segmentation, CAN bus, prompt engineering, smart grid.

</details>


### [86] [Mitigating Modal Imbalance in Multimodal Reasoning](https://arxiv.org/abs/2510.02608)
*Chen Henry Wu,Neil Kale,Aditi Raghunathan*

Main category: cs.AI

TL;DR: Foundation models struggle with cross-modal reasoning, showing only 3% conflict recognition when evidence is split across modalities due to attention imbalance. A simple training method combining modalities improves performance.


<details>
  <summary>Details</summary>
Motivation: To understand how foundation models perform joint reasoning across multiple modalities, especially when modalities interact and form cross-modal context, by studying cross-modal conflicts.

Method: Study FMs on cross-modal conflicts where conflicting evidence is presented across modalities. Analyze attention imbalance and propose a simple scalable method of explicitly combining multiple modalities within each training instance.

Result: FMs recognize conflicts in unimodal contexts 90% of the time but only 3% when evidence is split across modalities. Cross-modal attention imbalance is identified as the cause. The proposed method reduces attention imbalance and improves downstream performance.

Conclusion: Cross-modal attention imbalance is a critical issue that doesn't resolve with dataset scaling alone. Explicitly combining modalities during training significantly improves cross-modal reasoning and downstream performance, highlighting the need for systematic approaches to cross-modal contexts.

Abstract: Foundation models (FMs) deployed in real-world tasks such as computer-use
agents must integrate diverse modalities. How good are FMs at performing joint
reasoning, simultaneously reasoning over multiple modalities, especially when
the modalities interact and relate to each other to form cross-modal context?
To better understand this problem, we study FMs on cross-modal conflicts:
scenarios where conflicting evidence is presented across modalities. This
allows us to examine whether FMs prioritize one modality over another or reason
jointly to reconcile the conflict. Our experiments reveal that FMs can
recognize conflicts in unimodal contexts, composed of a single modality, 90% of
the time, but the ratio falls as low as 3% when evidence is split across
modalities -- similar observations hold in cross-lingual contexts, composed of
multiple languages. We trace this failure to cross-modal attention imbalance,
showing that FMs exhibit extreme asymmetry in attention scores,
disproportionately prioritizing certain modalities. We show that cross-modal
attention imbalance does not go away by simply scaling up multimodal or
multilingual datasets blindly, since they lack training examples that
explicitly require cross-modal reasoning. We demonstrate that even a simple and
scalable method of explicitly combining multiple modalities within each
training instance significantly reduces attention imbalance. Reduced attention
imbalance directly translates to improved downstream performance on several
vision-language benchmarks. Our findings underscore the importance of
systematically addressing cross-modal contexts to build reliable foundation
models.

</details>


### [87] [On the Role of Temperature Sampling in Test-Time Scaling](https://arxiv.org/abs/2510.02611)
*Yuheng Wu,Azalia Mirhoseini,Thierry Tambe*

Main category: cs.AI

TL;DR: Test-time scaling with multiple temperature settings significantly improves LLM reasoning performance beyond single-temperature approaches, enabling base models to match RL-trained counterparts without additional training.


<details>
  <summary>Details</summary>
Motivation: Prior work shows that increasing sample count (K) in test-time scaling improves accuracy, but this trend plateaus at large K and certain hard problems remain unsolved regardless of sample size. Different temperatures solve different problem subsets, suggesting single-temperature scaling only explores part of model potential.

Method: Proposed temperature scaling - generating reasoning traces across multiple temperature settings rather than just increasing sample count at a single temperature. Also designed a multi-temperature voting method to reduce computational overhead.

Result: Temperature scaling yields additional 7.3 points improvement over single-temperature TTS across Qwen3 models (0.6B-8B) and five reasoning benchmarks. Enables base models to reach performance comparable to RL-trained counterparts without additional post-training.

Conclusion: Test-time scaling is more powerful than previously thought, and temperature scaling offers a simple and effective way to unlock the latent potential of base models by exploring different reasoning pathways through varied temperature settings.

Abstract: Large language models (LLMs) can improve reasoning at inference time through
test-time scaling (TTS), where multiple reasoning traces are generated and the
best one is selected. Prior work shows that increasing the number of samples K
steadily improves accuracy. In this paper, we demonstrate that this trend does
not hold indefinitely: at large K, further scaling yields no gains, and certain
hard questions remain unsolved regardless of the number of traces.
Interestingly, we find that different sampling temperatures solve different
subsets of problems, implying that single-temperature scaling explores only
part of a model's potential. We therefore propose scaling along the temperature
dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3
(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME
2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an
additional 7.3 points over single-temperature TTS. Temperature scaling also
enables base models to reach performance comparable to reinforcement learning
(RL)-trained counterparts, without additional post-training. We further provide
a comprehensive analysis of this phenomenon and design a multi-temperature
voting method that reduces the overhead of temperature scaling. Overall, our
findings suggest that TTS is more powerful than previously thought, and that
temperature scaling offers a simple and effective way to unlock the latent
potential of base models.

</details>


### [88] [Geolog-IA: Conversational System for Academic Theses](https://arxiv.org/abs/2510.02653)
*Micaela Fuel Pozo,Andrea Guatumillo Saltos,Yeseña Tipan Llumiquinga,Kelly Lascano Aguirre,Marilyn Castillo Jara,Christian Mejia-Escobar*

Main category: cs.AI

TL;DR: Geolog-IA is an AI conversational system using Llama 3.1 and Gemini 2.5 models with RAG architecture and SQLite database to answer geology thesis questions from Central University of Ecuador, achieving high accuracy with BLEU score of 0.87.


<details>
  <summary>Details</summary>
Motivation: To create a natural conversational system for answering geology thesis questions that overcomes problems like hallucinations and outdated knowledge in AI models.

Method: Uses Llama 3.1 and Gemini 2.5 language models combined with Retrieval Augmented Generation (RAG) architecture and SQLite database for information retrieval and response generation.

Result: The system achieved an average BLEU score of 0.87, indicating high consistency and accuracy in generated responses. It provides an intuitive web-based interface for various university stakeholders.

Conclusion: Geolog-IA serves as a key educational and research support tool and establishes a foundation for future applications in other disciplines beyond geology.

Abstract: This study presents the development of Geolog-IA, a novel conversational
system based on artificial intelligence that responds naturally to questions
about geology theses from the Central University of Ecuador. Our proposal uses
the Llama 3.1 and Gemini 2.5 language models, which are complemented by a
Retrieval Augmented Generation (RAG) architecture and an SQLite database. This
strategy allows us to overcome problems such as hallucinations and outdated
knowledge. The evaluation of Geolog-IA's performance with the BLEU metric
reaches an average of 0.87, indicating high consistency and accuracy in the
responses generated. The system offers an intuitive, web-based interface that
facilitates interaction and information retrieval for directors, teachers,
students, and administrative staff at the institution. This tool can be a key
support in education, training, and research and establishes a basis for future
applications in other disciplines.

</details>


### [89] [A Concept of Possibility for Real-World Events](https://arxiv.org/abs/2510.02655)
*Daniel G. Schwartz*

Main category: cs.AI

TL;DR: This paper proposes a new concept of possibility as an alternative to Zadeh's standard possibility theory, focusing specifically on real-world events with prerequisites and constraints.


<details>
  <summary>Details</summary>
Motivation: To provide a more practical alternative to Zadeh's possibility theory that specifically addresses the possibility of real-world events, particularly useful for planning problems.

Method: Defines possibility as a function of probabilities that prerequisites hold and constraints don't impede an event, using Łukasiewicz multivalent logic interpretation.

Result: Developed a theoretical framework for computing event possibility that can be applied to planning problems like vehicle route planning.

Conclusion: This new possibility concept may better capture human reasoning about plans and has potential applications in planning domains, with suggestions for future applications.

Abstract: This paper offers a new concept of {\it possibility} as an alternative to the
now-a-days standard concept originally introduced by L.A. Zadeh in 1978. This
new version was inspired by the original but, formally, has nothing in common
with it other than that they both adopt the {\L}ukasiewicz multivalent
interpretation of the logical connectives. Moreover, rather than seeking to
provide a general notion of possibility, this focuses specifically on the
possibility of a real-world event. An event is viewed as having prerequisites
that enable its occurrence and constraints that may impede its occurrence, and
the possibility of the event is computed as a function of the probabilities
that the prerequisites hold and the constraints do not. This version of
possibility might appropriately be applied to problems of planning. When there
are multiple plans available for achieving a goal, this theory can be used to
determine which plan is most possible, i.e., easiest or most feasible to
complete. It is speculated that this model of reasoning correctly captures
normal human reasoning about plans. The theory is elaborated and an
illustrative example for vehicle route planning is provided. There is also a
suggestion of potential future applications.

</details>


### [90] [AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models](https://arxiv.org/abs/2510.02669)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.AI

TL;DR: AutoMaAS is a self-evolving multi-agent architecture search framework that automatically discovers optimal agent configurations using neural architecture search principles, achieving performance improvements of 1.0-7.1% while reducing inference costs by 3-5%.


<details>
  <summary>Details</summary>
Motivation: Existing automated design approaches for multi-agent systems seek monolithic solutions that fail to adapt resource allocation based on query complexity and domain requirements, highlighting the need for more adaptive and efficient frameworks.

Method: The framework incorporates four key innovations: automatic operator generation/fusion/elimination based on performance-cost analysis, dynamic cost-aware optimization with real-time parameter adjustment, online feedback integration for continuous refinement, and enhanced interpretability through decision tracing mechanisms.

Result: Extensive experiments across six benchmarks demonstrate that AutoMaAS achieves 1.0-7.1% performance improvement while reducing inference costs by 3-5% compared to state-of-the-art methods, with superior transferability across datasets and LLM backbones.

Conclusion: AutoMaAS establishes a new paradigm for automated multi-agent system design in the era of large language models, providing an efficient and adaptive framework for discovering optimal agent configurations.

Abstract: Multi-agent systems powered by large language models have demonstrated
remarkable capabilities across diverse domains, yet existing automated design
approaches seek monolithic solutions that fail to adapt resource allocation
based on query complexity and domain requirements. This paper introduces
AutoMaAS, a self-evolving multi-agent architecture search framework that
leverages neural architecture search principles to automatically discover
optimal agent configurations through dynamic operator lifecycle management and
automated machine learning techniques. Our approach incorporates four key
innovations: (1) automatic operator generation, fusion, and elimination based
on performance-cost analysis, (2) dynamic cost-aware optimization with
real-time parameter adjustment, (3) online feedback integration for continuous
architecture refinement, and (4) enhanced interpretability through decision
tracing mechanisms. Extensive experiments across six benchmarks demonstrate
that AutoMaAS achieves 1.0-7.1\% performance improvement while reducing
inference costs by 3-5\% compared to state-of-the-art methods. The framework
shows superior transferability across datasets and LLM backbones, establishing
a new paradigm for automated multi-agent system design in the era of large
language models.

</details>


### [91] [ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks](https://arxiv.org/abs/2510.02677)
*Zhaorun Chen,Xun Liu,Mintong Kang,Jiawei Zhang,Minzhou Pan,Shuang Yang,Bo Li*

Main category: cs.AI

TL;DR: ARMs is an adaptive red-teaming agent that systematically conducts comprehensive risk assessments for vision-language models (VLMs) by automatically optimizing diverse red-teaming strategies with reasoning-enhanced multi-step orchestration to elicit harmful outputs.


<details>
  <summary>Details</summary>
Motivation: As VLMs gain prominence, their multimodal interfaces introduce new safety vulnerabilities. Existing red-teaming efforts are either restricted to narrow adversarial patterns or depend heavily on manual engineering, lacking scalable exploration of emerging real-world VLM vulnerabilities.

Method: Proposed ARMs with 11 novel multimodal attack strategies covering diverse adversarial patterns, integrated 17 red-teaming algorithms via model context protocol (MCP), and designed layered memory with epsilon-greedy attack exploration algorithm to balance diversity and effectiveness.

Result: ARMs achieves SOTA attack success rates, exceeding baselines by average 52.1% and surpassing 90% on Claude-4-Sonnet. Generated significantly more diverse red-teaming instances revealing emerging vulnerabilities. Constructed ARMs-Bench with over 30K red-teaming instances spanning 51 risk categories.

Conclusion: Safety fine-tuning with ARMs-Bench substantially improves VLM robustness while preserving general utility, providing actionable guidance to improve multimodal safety alignment against emerging threats.

Abstract: As vision-language models (VLMs) gain prominence, their multimodal interfaces
also introduce new safety vulnerabilities, making the safety evaluation
challenging and critical. Existing red-teaming efforts are either restricted to
a narrow set of adversarial patterns or depend heavily on manual engineering,
lacking scalable exploration of emerging real-world VLM vulnerabilities. To
bridge this gap, we propose ARMs, an adaptive red-teaming agent that
systematically conducts comprehensive risk assessments for VLMs. Given a target
harmful behavior or risk definition, ARMs automatically optimizes diverse
red-teaming strategies with reasoning-enhanced multi-step orchestration, to
effectively elicit harmful outputs from target VLMs. We propose 11 novel
multimodal attack strategies, covering diverse adversarial patterns of VLMs
(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming
algorithms into ARMs via model context protocol (MCP). To balance the diversity
and effectiveness of the attack, we design a layered memory with an
epsilon-greedy attack exploration algorithm. Extensive experiments on instance-
and policy-based benchmarks show that ARMs achieves SOTA attack success rates,
exceeding baselines by an average of 52.1% and surpassing 90% on
Claude-4-Sonnet. We show that the diversity of red-teaming instances generated
by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.
Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety
dataset comprising over 30K red-teaming instances spanning 51 diverse risk
categories, grounded in both real-world multimodal threats and regulatory
risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness
of VLMs while preserving their general utility, providing actionable guidance
to improve multimodal safety alignment against emerging threats.

</details>


### [92] [Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation](https://arxiv.org/abs/2510.02679)
*Yu-Zhe Shi,Qiao Xu,Yanjia Li,Mingchen Liu,Huamin Qu,Lecheng Ruan,Qining Wang*

Main category: cs.AI

TL;DR: The paper presents a constraint-centric architecture that regulates LLMs for reliable automated constraint specification in production scheduling, addressing challenges of natural language ambiguity and non-deterministic outputs.


<details>
  <summary>Details</summary>
Motivation: Manual specification of manufacturing requirements into formal constraints is labor-intensive, and direct LLM application faces challenges due to ambiguity, non-deterministic outputs, and limited domain knowledge.

Method: A constraint-centric architecture with hierarchical structural space across three levels, implemented through domain-specific representation, plus an automated production scenario adaptation algorithm for customization.

Result: The approach successfully balances LLM generative capabilities with manufacturing reliability requirements, significantly outperforming pure LLM-based approaches in constraint specification tasks.

Conclusion: The proposed architecture enables reliable automated constraint specification for production scheduling by regulating LLMs while maintaining precision and flexibility.

Abstract: Advanced Planning and Scheduling (APS) systems have become indispensable for
modern manufacturing operations, enabling optimized resource allocation and
production efficiency in increasingly complex and dynamic environments. While
algorithms for solving abstracted scheduling problems have been extensively
investigated, the critical prerequisite of specifying manufacturing
requirements into formal constraints remains manual and labor-intensive.
Although recent advances of generative models, particularly Large Language
Models (LLMs), show promise in automating constraint specification from
heterogeneous raw manufacturing data, their direct application faces challenges
due to natural language ambiguity, non-deterministic outputs, and limited
domain-specific knowledge. This paper presents a constraint-centric
architecture that regulates LLMs to perform reliable automated constraint
specification for production scheduling. The architecture defines a
hierarchical structural space organized across three levels, implemented
through domain-specific representation to ensure precision and reliability
while maintaining flexibility. Furthermore, an automated production scenario
adaptation algorithm is designed and deployed to efficiently customize the
architecture for specific manufacturing configurations. Experimental results
demonstrate that the proposed approach successfully balances the generative
capabilities of LLMs with the reliability requirements of manufacturing
systems, significantly outperforming pure LLM-based approaches in constraint
specification tasks.

</details>


### [93] [NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning](https://arxiv.org/abs/2510.02816)
*Yulong Zhang,Li Wang,Wei Du,Peilin Li,Yuqin Dai Zhiyuan Zhao,Lingyong Fang,Ziniu Liu,Ru Zhang,Huijia Zhu,Gongshen Liu*

Main category: cs.AI

TL;DR: NCV is a training-free framework that verifies LLM reasoning through node-wise consistency checks, improving error localization and reducing token usage by 6-58x compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing verification methods for multi-step reasoning in LLMs suffer from imprecise error localization, attention dilution, and high token costs from multi-sampling approaches.

Method: Node-wise Consistency Verification (NCV) decomposes chain of thought into interconnected verification nodes and performs lightweight binary consistency checks at each node level.

Result: NCV achieves 10-25% F1 score improvement over baselines on public datasets while using 6x-58x fewer tokens than traditional CoT-based verifiers.

Conclusion: NCV provides a scalable, interpretable, and efficient solution for reliable LLM reasoning verification through precise error localization and reduced computational overhead.

Abstract: Verifying multi-step reasoning in large language models is difficult due to
imprecise error localization and high token costs. Existing methods either
assess entire reasoning chains, suffering attention dilution, or rely on
expensive multi-sampling. We introduce Node-wise Consistency Verification
(NCV), a training-free framework that recasts verification as lightweight
binary consistency checks at the node level. By decomposing the chain of
thought into interconnected verification nodes, NCV precisely localizes errors
and avoids unnecessary long-form generation. Experiments demonstrate that our
approach enhances interpretability and efficiency, presenting a scalable
solution for reliable LLM reasoning verification. On public datasets, NCV
achieves a 10\% to 25\% improvement in F1 scores over baselines while utilizing
$6\times$~$58\times$ fewer tokens than traditional methods like CoT-based
verifiers.

</details>


### [94] [Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents](https://arxiv.org/abs/2510.02837)
*Wonjoong Kim,Sangwu Park,Yeonjun In,Sein Kim,Dongha Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: TRACE is a framework for multi-dimensional evaluation of tool-augmented LLM agents that assesses reasoning trajectories beyond just final answers, addressing limitations of traditional answer-matching evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current tool-augmented benchmarks only evaluate final answers, ignoring important aspects like efficiency, hallucination, and adaptivity in problem-solving trajectories. Annotating all valid ground-truth trajectories is too expensive, and simple LLM evaluators struggle without ground truth.

Method: TRACE incorporates an evidence bank that accumulates knowledge from preceding reasoning steps, enabling multi-faceted analysis of agent trajectories. The framework was validated using a meta-evaluation dataset created by augmenting existing benchmarks with diverse and flawed trajectories labeled with multi-faceted performance scores.

Result: TRACE accurately evaluates complex agent behaviors in a scalable and cost-effective manner, even with small open-source LLMs. The method revealed previously unreported observations about agent trajectories in tool-augmented tasks.

Conclusion: TRACE provides an effective framework for comprehensive evaluation of tool-augmented LLM agents, addressing the limitations of traditional evaluation methods and enabling detailed assessment of reasoning trajectories without requiring expensive ground-truth annotations.

Abstract: Although recent tool-augmented benchmarks incorporate complex user requests
and diverse tools, the evaluation methods for most of them remain limited to
answer matching. However, as the number of steps required to resolve a user
request increases, a proper evaluation of an agent's performance must go beyond
the final answer to also assess the problem-solving trajectory, including
previously ignored aspects such as efficiency, hallucination, and adaptivity.
The most straightforward method for evaluating these aspects is to compare an
agent's trajectory with the ground-truth trajectory, but this approach is
fundamentally limited since annotating all valid ground-truth trajectories is
prohibitively expensive. However, a simple LLM-based evaluator struggles to
assess trajectories in detail without ground truth. To effectively evaluate the
agents in this manner, we introduce TRACE, a framework for the
multi-dimensional evaluation of tool-augmented LLM agent performance. By
incorporating an evidence bank, which accumulates knowledge gathered from
preceding reasoning steps, TRACE enables a multi-faceted analysis and
evaluation of an agent's reasoning trajectory effectively. To validate our
framework, we develop a new meta-evaluation dataset by augmenting existing
benchmarks with diverse and flawed trajectories, each labeled with
multi-faceted performance scores. Our results confirm that TRACE accurately
evaluates these complex behaviors in a scalable and cost-effective manner, even
with small open-source LLMs. Furthermore, we apply our method to evaluate the
trajectories that agents produce while solving tool-augmented tasks, presenting
previously unreported observations and their corresponding insights.

</details>


### [95] [Take Goodhart Seriously: Principled Limit on General-Purpose AI Optimization](https://arxiv.org/abs/2510.02840)
*Antoine Maier,Aude Maier,Tom David*

Main category: cs.AI

TL;DR: The paper challenges the Objective Satisfaction Assumption (OSA) in machine learning, arguing that training doesn't guarantee models satisfy their specified objectives due to approximation, estimation, and optimization errors, plus inevitable objective misspecification.


<details>
  <summary>Details</summary>
Motivation: To examine the overlooked assumption that training yields models satisfying their specified objectives, and to highlight the risks when this assumption fails under strong optimization pressure.

Method: Uses a learning-paradigm-agnostic framework to analyze systematic deviations from intended objectives due to technical errors and objective misspecification, building on recent mathematical results about Goodhart's law.

Result: Shows that OSA fails in realistic conditions, and without mathematical characterization of objective gaps, systems risk collapsing into Goodhart's law failure modes under optimization pressure.

Conclusion: A principled limit on optimization of General-Purpose AI systems is necessary to prevent predictable and irreversible loss of control when pushing systems beyond their breaking points.

Abstract: A common but rarely examined assumption in machine learning is that training
yields models that actually satisfy their specified objective function. We call
this the Objective Satisfaction Assumption (OSA). Although deviations from OSA
are acknowledged, their implications are overlooked. We argue, in a
learning-paradigm-agnostic framework, that OSA fails in realistic conditions:
approximation, estimation, and optimization errors guarantee systematic
deviations from the intended objective, regardless of the quality of its
specification. Beyond these technical limitations, perfectly capturing and
translating the developer's intent, such as alignment with human preferences,
into a formal objective is practically impossible, making misspecification
inevitable. Building on recent mathematical results, absent a mathematical
characterization of these gaps, they are indistinguishable from those that
collapse into Goodhart's law failure modes under strong optimization pressure.
Because the Goodhart breaking point cannot be located ex ante, a principled
limit on the optimization of General-Purpose AI systems is necessary. Absent
such a limit, continued optimization is liable to push systems into predictable
and irreversible loss of control.

</details>


### [96] [Reward Model Routing in Alignment](https://arxiv.org/abs/2510.02850)
*Xinle Wu,Yao Lu*

Main category: cs.AI

TL;DR: BayesianRouter is a hybrid RM routing framework that combines offline learning of RM strengths with online Bayesian selection to dynamically choose the best reward model from a pool, improving alignment quality while maintaining O(1) RM calls.


<details>
  <summary>Details</summary>
Motivation: Existing RLHF/RLAIF pipelines rely on single reward models, limiting alignment quality and risking overfitting. Current RM routing methods suffer from cold-start problems and insufficient exploration.

Method: Two-stage approach: 1) Offline stage trains multi-task router on preference data to estimate per-RM reliability; 2) Online stage uses Bayesian Thompson sampling with offline embeddings as Gaussian priors, adaptively updating posteriors with online rewards.

Result: Extensive experiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and reasoning (GSM8K, MMLU) benchmarks show BayesianRouter consistently outperforms individual RMs, RM ensembling, and existing routing methods.

Conclusion: BayesianRouter effectively addresses cold-start and exploration issues in RM routing, providing a robust framework for improving LLM alignment quality through dynamic reward model selection.

Abstract: Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become
the standard paradigm for aligning large language models (LLMs). However, most
pipelines rely on a single reward model (RM), limiting alignment quality and
risking overfitting. Recent work explores RM routing--dynamically selecting an
RM from a candidate pool to exploit complementary strengths while maintaining
$O(1)$ RM calls--but existing methods suffer from cold-start and insufficient
exploration. We propose BayesianRouter, a hybrid routing framework that
combines offline RM strengths learning with online Bayesian selection. In the
offline stage, a multi-task router is trained on preference data to estimate
per-RM reliability. In the online stage, a Bayesian Thompson sampling router
performs per-query RM selection, initializing RM-specific weight vectors with
offline embeddings as Gaussian priors and adaptively updating their posteriors
with online rewards to adapt to the evolving policy distribution. Extensive
experiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and
reasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently
outperforms individual RMs, RM ensembling, and existing routing methods.

</details>


### [97] [Consolidating Reinforcement Learning for Multimodal Discrete Diffusion Models](https://arxiv.org/abs/2510.02880)
*Tianren Ma,Mu Zhang,Yibing Wang,Qixiang Ye*

Main category: cs.AI

TL;DR: MaskGRPO is the first viable approach for scalable multimodal reinforcement learning in discrete diffusion models, addressing challenges in importance sampling and rollout complexity through theoretical foundations and modality-specific adaptations.


<details>
  <summary>Details</summary>
Motivation: Optimizing discrete diffusion models with rewards is challenging due to non-autoregressive paradigm making importance sampling intractable and rollout complex, which puzzles reinforcement learning methods like GRPO.

Method: Clarified theoretical foundation for DDMs to build importance estimator capturing token fluctuation for gradient updates, and tailored rollout method for visual sequences to yield diverse completions and reliable optimization gradients.

Result: On math reasoning, coding, and visual generation benchmarks, MaskGRPO brings more stable and efficient updates, leading to stronger reasoning performance and better generation quality.

Conclusion: MaskGRPO establishes as a systematic policy optimization approach and the first practical way for discretized visual diffusion.

Abstract: Optimizing discrete diffusion model (DDM) with rewards remains a challenge:
the non-autoregressive paradigm makes importance sampling intractable and
rollout complex, puzzling reinforcement learning methods such as Group Relative
Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first
viable approach to enable scalable multimodal reinforcement learning in
discrete diffusion with effective importance sampling and modality-specific
adaptations. To this end, we first clarify the theoretical foundation for DDMs,
which facilitates building an importance estimator that captures valuable token
fluctuation for gradient updates. We then delicately tailored the rollout
method for visual sequences, which yields diverse completions and reliable
optimization gradients. Upon math reasoning, coding, and visual generation
benchmarks, MaskGRPO brings more stable and efficient updates, leading to
stronger reasoning performance and better generation quality. This study
establishes MaskGRPO as a systematic policy optimization approach and the first
practical way for discretized visual diffusion.

</details>


### [98] [Onto-Epistemological Analysis of AI Explanations](https://arxiv.org/abs/2510.02996)
*Martina Mattioli,Eike Petersen,Aasa Feragen,Marcello Pelillo,Siavash A. Bigdeli*

Main category: cs.AI

TL;DR: The paper examines how different explainable AI (XAI) methods incorporate specific ontological and epistemological assumptions about explanations, and how these assumptions affect the validity and interpretation of AI explanations across different application domains.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods are black-box systems lacking explanations, limiting their trustworthiness. XAI methods are developed with technical assumptions about explanations, but the fundamental concept of explanation is philosophically complex and debated.

Method: The authors analyze ontological and epistemological assumptions in XAI methods applied to AI systems, examining assumptions about the existence of explanations and our ability to gain knowledge about them.

Result: The analysis reveals that seemingly minor technical changes in XAI methods correspond to significant differences in underlying assumptions about explanations. Ignoring these onto-epistemological paradigms when choosing XAI methods poses risks.

Conclusion: It's crucial to consider the underlying onto-epistemological paradigm when selecting and adapting XAI methods for different application domains to ensure valid and appropriate explanations.

Abstract: Artificial intelligence (AI) is being applied in almost every field. At the
same time, the currently dominant deep learning methods are fundamentally
black-box systems that lack explanations for their inferences, significantly
limiting their trustworthiness and adoption. Explainable AI (XAI) methods aim
to overcome this challenge by providing explanations of the models' decision
process. Such methods are often proposed and developed by engineers and
scientists with a predominantly technical background and incorporate their
assumptions about the existence, validity, and explanatory utility of different
conceivable explanatory mechanisms. However, the basic concept of an
explanation -- what it is, whether we can know it, whether it is absolute or
relative -- is far from trivial and has been the subject of deep philosophical
debate for millennia. As we point out here, the assumptions incorporated into
different XAI methods are not harmless and have important consequences for the
validity and interpretation of AI explanations in different domains. We
investigate ontological and epistemological assumptions in explainability
methods when they are applied to AI systems, meaning the assumptions we make
about the existence of explanations and our ability to gain knowledge about
those explanations. Our analysis shows how seemingly small technical changes to
an XAI method may correspond to important differences in the underlying
assumptions about explanations. We furthermore highlight the risks of ignoring
the underlying onto-epistemological paradigm when choosing an XAI method for a
given application, and we discuss how to select and adapt appropriate XAI
methods for different domains of application.

</details>


### [99] [From Facts to Foils: Designing and Evaluating Counterfactual Explanations for Smart Environments](https://arxiv.org/abs/2510.03078)
*Anna Trapp,Mersedeh Sadeghi,Andreas Vogelsang*

Main category: cs.AI

TL;DR: First formalization and implementation of counterfactual explanations for rule-based smart environments, showing user preference depends on context - causal explanations for simplicity/time pressure, counterfactuals for actionable problem-solving.


<details>
  <summary>Details</summary>
Motivation: No established methods exist for generating counterfactual explanations in rule-based smart environments, despite explainability being essential and counterfactuals being powerful in XAI.

Method: Developed formalization and implementation of counterfactual explanations as plugin for existing explanation engine, then conducted user study (N=17) comparing counterfactuals against traditional causal explanations.

Result: User preference is highly contextual: causal explanations favored for linguistic simplicity and time-pressured situations, counterfactuals preferred for actionable content when users want to resolve problems.

Conclusion: Provides practical framework for counterfactual explanations in smart environments and empirical evidence guiding when each explanation type is most effective.

Abstract: Explainability is increasingly seen as an essential feature of rule-based
smart environments. While counterfactual explanations, which describe what
could have been done differently to achieve a desired outcome, are a powerful
tool in eXplainable AI (XAI), no established methods exist for generating them
in these rule-based domains. In this paper, we present the first formalization
and implementation of counterfactual explanations tailored to this domain. It
is implemented as a plugin that extends an existing explanation engine for
smart environments. We conducted a user study (N=17) to evaluate our generated
counterfactuals against traditional causal explanations. The results show that
user preference is highly contextual: causal explanations are favored for their
linguistic simplicity and in time-pressured situations, while counterfactuals
are preferred for their actionable content, particularly when a user wants to
resolve a problem. Our work contributes a practical framework for a new type of
explanation in smart environments and provides empirical evidence to guide the
choice of when each explanation type is most effective.

</details>


### [100] [A Study of Rule Omission in Raven's Progressive Matrices](https://arxiv.org/abs/2510.03127)
*Binze Li*

Main category: cs.AI

TL;DR: This paper investigates whether AI models genuinely reason or rely on statistical shortcuts in Raven's Progressive Matrices tasks by testing their generalization on omitted rules during training.


<details>
  <summary>Details</summary>
Motivation: To determine if modern AI systems' success on RPM tasks reflects true reasoning ability or just pattern recognition, by examining their performance on novel structural rules not seen during training.

Method: Evaluated sequence-to-sequence transformers and vision-based architectures (CoPINet, Dual-Contrast Network) on the I-RAVEN dataset, deliberately omitting several structural rules during training to test generalization.

Result: Transformers showed strong performance on familiar rules but sharp accuracy decline on novel/omitted rules. Token-level vs complete answer accuracy gap revealed fundamental limitations in current approaches.

Conclusion: Current deep learning models lack robust abstract reasoning capabilities and primarily rely on pattern recognition, highlighting the need for architectures that can move beyond statistical shortcuts to genuine reasoning.

Abstract: Analogical reasoning lies at the core of human cognition and remains a
fundamental challenge for artificial intelligence. Raven's Progressive Matrices
(RPM) serve as a widely used benchmark to assess abstract reasoning by
requiring the inference of underlying structural rules. While many vision-based
and language-based models have achieved success on RPM tasks, it remains
unclear whether their performance reflects genuine reasoning ability or
reliance on statistical shortcuts. This study investigates the generalization
capacity of modern AI systems under conditions of incomplete training by
deliberately omitting several structural rules during training. Both
sequence-to-sequence transformer models and vision-based architectures such as
CoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN
(I-RAVEN) dataset. Experiments reveal that although transformers demonstrate
strong performance on familiar rules, their accuracy declines sharply when
faced with novel or omitted rules. Moreover, the gap between token-level
accuracy and complete answer accuracy highlights fundamental limitations in
current approaches. These findings provide new insights into the reasoning
mechanisms underlying deep learning models and underscore the need for
architectures that move beyond pattern recognition toward robust abstract
reasoning.

</details>


### [101] [Improving Cooperation in Collaborative Embodied AI](https://arxiv.org/abs/2510.03153)
*Hima Jacob Leven Suprabha,Laxmi Nag Laxminarayan Nagesh,Ajith Nair,Alvin Reuben Amal Selvaster,Ayan Khan,Raghuram Damarla,Sanju Hannah Samuel,Sreenithi Saravana Perumal,Titouan Puech,Venkataramireddy Marella,Vishal Sonar,Alessandro Suglia,Oliver Lemon*

Main category: cs.AI

TL;DR: This paper explores LLM prompting methods to enhance multi-agent collaboration in the CoELA framework, achieving 22% efficiency improvement with Gemma3 and integrating speech capabilities for better user interaction.


<details>
  <summary>Details</summary>
Motivation: To enhance collaborative reasoning and cooperation in multiagent systems by optimizing LLM prompting methods and integrating speech capabilities for more natural interactions.

Method: Enhanced the CoELA framework with systematic experimentation on different LLMs and prompt engineering strategies, plus speech integration for voice-based collaborative interactions.

Result: Best prompt optimization combination improved system efficiency by 22% with Gemma3 compared to original CoELA, and speech integration provided more engaging user interface.

Conclusion: Prompt optimization significantly enhances collaborative agent performance, and speech integration offers valuable improvements for iterative development and user engagement in multi-agent systems.

Abstract: The integration of Large Language Models (LLMs) into multiagent systems has
opened new possibilities for collaborative reasoning and cooperation with AI
agents. This paper explores different prompting methods and evaluates their
effectiveness in enhancing agent collaborative behaviour and decision-making.
We enhance CoELA, a framework designed for building Collaborative Embodied
Agents that leverage LLMs for multi-agent communication, reasoning, and task
coordination in shared virtual spaces. Through systematic experimentation, we
examine different LLMs and prompt engineering strategies to identify optimised
combinations that maximise collaboration performance. Furthermore, we extend
our research by integrating speech capabilities, enabling seamless
collaborative voice-based interactions. Our findings highlight the
effectiveness of prompt optimisation in enhancing collaborative agent
performance; for example, our best combination improved the efficiency of the
system running with Gemma3 by 22% compared to the original CoELA system. In
addition, the speech integration provides a more engaging user interface for
iterative system development and demonstrations.

</details>


### [102] [CoDA: Agentic Systems for Collaborative Data Visualization](https://arxiv.org/abs/2510.03194)
*Zichen Chen,Jiefeng Chen,Sercan Ö. Arik,Misha Sra,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: CoDA is a multi-agent system that automates visualization generation from natural language queries through specialized LLM agents for metadata analysis, task planning, code generation, and self-reflection, achieving 41.5% improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Current visualization automation systems struggle with complex multi-file datasets and iterative refinement, often oversimplifying the task and failing to handle data complexity, code errors, or visualization quality.

Method: CoDA employs a collaborative multi-agent approach with specialized LLM agents: metadata analysis agent (bypasses token limits), task planning agent, code generation agent, and self-reflection agent for quality-driven refinement.

Result: Extensive evaluations show CoDA achieves substantial gains in overall score, outperforming competitive baselines by up to 41.5%.

Conclusion: The future of visualization automation lies in integrated, collaborative agentic workflows rather than isolated code generation.

Abstract: Deep research has revolutionized data analysis, yet data scientists still
devote substantial time to manually crafting visualizations, highlighting the
need for robust automation from natural language queries. However, current
systems struggle with complex datasets containing multiple files and iterative
refinement. Existing approaches, including simple single- or multi-agent
systems, often oversimplify the task, focusing on initial query parsing while
failing to robustly manage data complexity, code errors, or final visualization
quality. In this paper, we reframe this challenge as a collaborative
multi-agent problem. We introduce CoDA, a multi-agent system that employs
specialized LLM agents for metadata analysis, task planning, code generation,
and self-reflection. We formalize this pipeline, demonstrating how
metadata-focused analysis bypasses token limits and quality-driven refinement
ensures robustness. Extensive evaluations show CoDA achieves substantial gains
in the overall score, outperforming competitive baselines by up to 41.5%. This
work demonstrates that the future of visualization automation lies not in
isolated code generation but in integrated, collaborative agentic workflows.

</details>


### [103] [Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner](https://arxiv.org/abs/2510.03206)
*Cai Zhou,Chenxiao Yang,Yi Hu,Chenyu Wang,Chubin Zhang,Muhan Zhang,Lester Mackey,Tommi Jaakkola,Stephen Bates,Dinghuai Zhang*

Main category: cs.AI

TL;DR: Continuous diffusion language models have stronger theoretical expressivity than discrete diffusions but underperform in practice due to decoding difficulties. The paper proposes CCDD - a joint multimodal diffusion process that simultaneously denoises in both continuous representation and discrete token spaces.


<details>
  <summary>Details</summary>
Motivation: To address the contradiction between the theoretical expressiveness of continuous diffusion models and their empirical underperformance compared to discrete counterparts, by leveraging both continuous and discrete modalities.

Method: Proposes Coevolutionary Continuous Discrete Diffusion (CCDD) - a joint multimodal diffusion process that operates simultaneously in continuous representation space and discrete token space using a single model for denoising.

Result: CCDD achieves strong empirical performance in extensive language modeling experiments on real-world tasks, combining the expressivity of continuous representations with the trainability and sample quality of discrete tokens.

Conclusion: The joint multimodal approach of CCDD successfully bridges the gap between theoretical expressiveness and practical performance in diffusion language models by leveraging both continuous and discrete modalities.

Abstract: Diffusion language models, especially masked discrete diffusion models, have
achieved great success recently. While there are some theoretical and primary
empirical results showing the advantages of latent reasoning with looped
transformers or continuous chain-of-thoughts, continuous diffusion models
typically underperform their discrete counterparts. In this paper, we argue
that diffusion language models do not necessarily need to be in the discrete
space. In particular, we prove that continuous diffusion models have stronger
expressivity than discrete diffusions and looped transformers. We attribute the
contradiction between the theoretical expressiveness and empirical performance
to their practical trainability: while continuous diffusion provides
intermediate supervision that looped transformers lack, they introduce
additional difficulty decoding tokens into the discrete token space from the
continuous representation space. We therefore propose Coevolutionary Continuous
Discrete Diffusion (CCDD), which defines a joint multimodal diffusion process
on the union of a continuous representation space and a discrete token space,
leveraging a single model to simultaneously denoise in the joint space. By
combining two modalities, CCDD is expressive with rich semantics in the latent
space, as well as good trainability and sample quality with the help of
explicit discrete tokens. We also propose effective architectures and advanced
training/sampling techniques for CCDD, which reveals strong empirical
performance in extensive language modeling experiments on real-world tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [104] [Extreme value forecasting using relevance-based data augmentation with deep learning models](https://arxiv.org/abs/2510.02407)
*Junru Hua,Rahul Ahluwalia,Rohitash Chandra*

Main category: cs.LG

TL;DR: This paper presents a data augmentation framework for extreme value forecasting using GANs and SMOTE with deep learning models like Conv-LSTM and BD-LSTM, showing SMOTE-based strategies perform best.


<details>
  <summary>Details</summary>
Motivation: Extreme value forecasting is challenging but important for finance and climate applications. Data augmentation can help address class imbalance problems in forecasting extreme values.

Method: Used GANs and SMOTE for data augmentation, combined with Conv-LSTM and BD-LSTM deep learning models for multistep ahead prediction. Developed novel strategies incorporating data augmentation based on relevance functions for extreme values.

Result: SMOTE-based strategy consistently showed superior adaptability and improved performance across both short- and long-horizon forecasts. Conv-LSTM excels in periodic, stable datasets while BD-LSTM performs better in chaotic or non-stationary sequences.

Conclusion: Data augmentation with SMOTE is effective for extreme value forecasting, with different deep learning models showing complementary strengths depending on dataset characteristics.

Abstract: Data augmentation with generative adversarial networks (GANs) has been
popular for class imbalance problems, mainly for pattern classification and
computer vision-related applications. Extreme value forecasting is a
challenging field that has various applications from finance to climate change
problems. In this study, we present a data augmentation framework for extreme
value forecasting. In this framework, our focus is on forecasting extreme
values using deep learning models in combination with data augmentation models
such as GANs and synthetic minority oversampling technique (SMOTE). We use deep
learning models such as convolutional long short-term memory (Conv-LSTM) and
bidirectional long short-term memory (BD-LSTM) networks for multistep ahead
prediction featuring extremes. We investigate which data augmentation models
are the most suitable, taking into account the prediction accuracy overall and
at extreme regions, along with computational efficiency. We also present novel
strategies for incorporating data augmentation, considering extreme values
based on a relevance function. Our results indicate that the SMOTE-based
strategy consistently demonstrated superior adaptability, leading to improved
performance across both short- and long-horizon forecasts. Conv-LSTM and
BD-LSTM exhibit complementary strengths: the former excels in periodic, stable
datasets, while the latter performs better in chaotic or non-stationary
sequences.

</details>


### [105] [OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data](https://arxiv.org/abs/2510.02410)
*Patrick Langer,Thomas Kaar,Max Rosenblattl,Maxwell A. Xu,Winnie Chow,Martin Maritsch,Aradhana Verma,Brian Han,Daniel Seung Kim,Henry Chubb,Scott Ceresnak,Aydin Zahedivash,Alexander Tarlochan Singh Sandhu,Fatima Rodriguez,Daniel McDuff,Elgar Fleisch,Oliver Aalami,Filipe Barata,Paul Schmiedmayer*

Main category: cs.LG

TL;DR: OpenTSLM is a family of Time Series Language Models that integrate time series as a native modality into pretrained LLMs, enabling reasoning over multiple time series of any length through two architectures: SoftPrompt (implicit modeling) and Flamingo (explicit modeling via cross-attention).


<details>
  <summary>Details</summary>
Motivation: LLMs have limitations in handling time series data, which is crucial in medical applications for synthesizing clinical information into actionable insights. Current approaches treat time series as text tokens or plots, which is suboptimal.

Method: Two architectures: 1) OpenTSLM-SoftPrompt uses learnable time series tokens concatenated with text tokens via soft prompting; 2) OpenTSLM-Flamingo integrates time series with text via cross-attention. Both are benchmarked on text-time-series Chain-of-Thought reasoning tasks using three datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT.

Result: OpenTSLM models outperform baselines significantly: 69.9 F1 in sleep staging and 65.4 in HAR vs 9.05 and 52.2 for text-only models. Even 1B-parameter OpenTSLM surpasses GPT-4o (15.47 and 2.95). Flamingo matches SoftPrompt performance and outperforms on longer sequences while maintaining stable memory requirements (40GB vs 110GB VRAM).

Conclusion: OpenTSLM successfully enables LLMs to reason over time series data, with explicit modeling (Flamingo) scaling better than implicit approaches. The models demonstrate strong reasoning capabilities in medical applications and are provided open-source to facilitate further research.

Abstract: LLMs have emerged as powerful tools for interpreting multimodal data. In
medicine, they hold particular promise for synthesizing large volumes of
clinical information into actionable insights and digital health applications.
Yet, a major limitation remains their inability to handle time series. To
overcome this gap, we present OpenTSLM, a family of Time Series Language Models
(TSLMs) created by integrating time series as a native modality to pretrained
LLMs, enabling reasoning over multiple time series of any length. We
investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt,
models time series implicitly by concatenating learnable time series tokens
with text tokens via soft prompting. Although parameter-efficient, we
hypothesize that explicit time series modeling scales better and outperforms
implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time
series with text via cross-attention. We benchmark both variants against
baselines that treat time series as text tokens or plots, across a suite of
text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three
datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models
outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR,
compared to 9.05 and 52.2 for finetuned text-only models. Notably, even
1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo
matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences,
while maintaining stable memory requirements. By contrast, SoftPrompt grows
exponentially in memory with sequence length, requiring around 110 GB compared
to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by
clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA.
To facilitate further research, we provide all code, datasets, and models
open-source.

</details>


### [106] [RainSeer: Fine-Grained Rainfall Reconstruction via Physics-Guided Modeling](https://arxiv.org/abs/2510.02414)
*Lin Chen,Jun Chen,Minghui Qiu,Shuxin Zhong,Binghong Chen,Kaishun Wu*

Main category: cs.LG

TL;DR: RainSeer is a structure-aware framework that uses radar reflectivity as a physical prior to reconstruct high-resolution rainfall fields, addressing challenges in spatial alignment and physical disconnect between aloft hydro-meteors and ground-level precipitation.


<details>
  <summary>Details</summary>
Motivation: Existing spatial interpolation methods over-smooth critical structures and fail to capture sharp transitions and localized extremes in rainfall fields, which are essential for flood forecasting, hydrological modeling, and climate analysis.

Method: RainSeer uses a physics-informed two-stage architecture: (1) Structure-to-Point Mapper for spatial alignment by projecting mesoscale radar structures into localized ground-level rainfall through bidirectional mapping, and (2) Geo-Aware Rain Decoder that captures semantic transformation of hydro-meteors via causal spatiotemporal attention mechanism.

Result: Evaluation on RAIN-F (Korea, 2017-2019) and MeteoNet (France, 2016-2018) datasets shows consistent improvements over state-of-the-art baselines, reducing MAE by over 13.31% and significantly enhancing structural fidelity in reconstructed rainfall fields.

Conclusion: RainSeer effectively bridges the physical disconnect between radar observations and ground-level precipitation, providing more accurate and structure-preserving rainfall field reconstructions for hydrological applications.

Abstract: Reconstructing high-resolution rainfall fields is essential for flood
forecasting, hydrological modeling, and climate analysis. However, existing
spatial interpolation methods-whether based on automatic weather station (AWS)
measurements or enhanced with satellite/radar observations often over-smooth
critical structures, failing to capture sharp transitions and localized
extremes. We introduce RainSeer, a structure-aware reconstruction framework
that reinterprets radar reflectivity as a physically grounded structural
prior-capturing when, where, and how rain develops. This shift, however,
introduces two fundamental challenges: (i) translating high-resolution
volumetric radar fields into sparse point-wise rainfall observations, and (ii)
bridging the physical disconnect between aloft hydro-meteors and ground-level
precipitation. RainSeer addresses these through a physics-informed two-stage
architecture: a Structure-to-Point Mapper performs spatial alignment by
projecting mesoscale radar structures into localized ground-level rainfall,
through a bidirectional mapping, and a Geo-Aware Rain Decoder captures the
semantic transformation of hydro-meteors through descent, melting, and
evaporation via a causal spatiotemporal attention mechanism. We evaluate
RainSeer on two public datasets-RAIN-F (Korea, 2017-2019) and MeteoNet (France,
2016-2018)-and observe consistent improvements over state-of-the-art baselines,
reducing MAE by over 13.31% and significantly enhancing structural fidelity in
reconstructed rainfall fields.

</details>


### [107] [How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models](https://arxiv.org/abs/2510.02453)
*Parth Asawa,Alan Zhu,Matei Zaharia,Alexandros G. Dimakis,Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: Advisor Models are lightweight policies trained with RL to dynamically issue natural language steering instructions to black-box foundation models, enabling per-instance adaptation instead of using static prompts.


<details>
  <summary>Details</summary>
Motivation: Foundation models are increasingly deployed as black-box services where model weights cannot be modified and customization is limited to prompting. Static prompt optimization produces fixed prompts that fail to adapt to different inputs, users, or environments.

Method: Train lightweight parametric policies using reinforcement learning to reactively issue natural language steering instructions in-context to black-box models. The advisor acts as a second small model between input and model, shaping behavior per-instance using reward signals.

Result: Across multiple domains involving reasoning and personalization, Advisor Models outperform static prompt optimizers, discovering environment dynamics and improving downstream task performance. They can be transferred across black-box models and achieve specialization while retaining robustness to out-of-distribution inputs.

Conclusion: Advisor Models provide a learnable interface to black-box systems where the advisor acts as parametric, environment-specific memory. Dynamic optimization via Advisor Models is a promising direction for enabling personalization and environment-adaptable AI with frontier-level capabilities.

Abstract: Foundation models are increasingly deployed as black-box services, where
model weights cannot be modified and customization is limited to prompting.
While static prompt optimization has shown promise, it produces a single fixed
prompt that fails to adapt to different inputs, users, or environments. We
introduce Advisor Models, lightweight parametric policies trained with
reinforcement learning to reactively issue natural language steering
instructions in-context to black-box models. The advisor is a second small
model that sits between the input and the model, shaping behavior on a
per-instance basis using reward signals from the environment. Across multiple
domains involving reasoning and personalization, we show that Advisor Models
outperform static prompt optimizers, discovering environment dynamics and
improving downstream task performance. We also demonstrate the generalizability
of advisors by transferring them across black-box models, as well as the
framework's ability to achieve specialization while retaining robustness to
out-of-distribution inputs. Viewed more broadly, Advisor Models provide a
learnable interface to black-box systems where the advisor acts as a
parametric, environment-specific memory. We argue that dynamic optimization of
black-box models via Advisor Models is a promising direction for enabling
personalization and environment-adaptable AI with frontier-level capabilities.

</details>


### [108] [Market-Based Data Subset Selection -- Principled Aggregation of Multi-Criteria Example Utility](https://arxiv.org/abs/2510.02456)
*Ashish Jha,Valentin Leplat,AH Phan*

Main category: cs.LG

TL;DR: A market-based data selection method using prediction markets to price training examples, combining multiple utility signals with controlled concentration and explicit token budget management.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches to selecting useful training data subsets combine heterogeneous utility signals (uncertainty, rarity, diversity) with ad hoc weights, lacking systematic integration.

Method: Uses cost-function prediction market (LMSR) to price examples, with signals as traders. Features topic-wise normalization, price-per-token rule for budget control, and lightweight diversity head for coverage.

Result: On GSM8K with 60k-token budget, achieves parity with strong baselines while reducing seed variance and <0.1 GPU-hr overhead. On AGNews (5-25% kept), delivers competitive accuracy with improved balance and stability.

Conclusion: The framework provides a unified approach for multi-signal data curation under fixed compute constraints, with transparent aggregation and interpretable parameters.

Abstract: Selecting a small yet useful subset of training data is hard because signals
of example utility (uncertainty, rarity, diversity, etc.) are heterogeneous and
typically combined with ad hoc weights. We propose a market-based selector that
prices each example via a cost-function prediction market (LMSR), signals act
as traders, a single liquidity parameter controls concentration, and topic-wise
normalization stabilizes calibration. Token budgets are handled explicitly by a
price-per-token rule $\rho=p/\ell^{\gamma}$, with $\gamma$ exposing an
interpretable length bias; a lightweight diversity head improves coverage. We
quantify coverage via topic cluster coverage and effective sample size. On the
theory side, we show that LMSR implements a maximum-entropy aggregation with
exponential weighting and a convex objective, yielding transparent knobs for
aggregation strength. Empirically, on GSM8K (60k-token budget) the market with
diversity achieves parity with strong single-signal baselines while reducing
seed variance and incurring $<\!0.1$ GPU-hr selection overhead; on AGNews at
kept=5-25\% the market (with light balancing) delivers competitive accuracy
with improved balance and stability. The framework unifies multi-signal data
curation under fixed compute for prompt-level reasoning and classification.

</details>


### [109] [Assessing the Potential for Catastrophic Failure in Dynamic Post-Training Quantization](https://arxiv.org/abs/2510.02457)
*Logan Frank,Paul Ardis*

Main category: cs.LG

TL;DR: This paper investigates catastrophic failure cases in post-training quantization (PTQ) where performance can drop 10-65% due to specific network-bitwidth policy combinations, compared to robust cases with <2% decrease.


<details>
  <summary>Details</summary>
Motivation: To understand potential performance degradation in safety-critical deployments of quantized neural networks and identify characteristics that cause extreme failure.

Method: Formulated knowledge distillation and reinforcement learning tasks to learn network-bitwidth policy pairs that reveal worst-case quantization scenarios.

Result: Confirmed existence of detrimental network-policy pairs causing 10-65% accuracy drops, while robust pairs showed <2% decrease. Identified high vulnerability points.

Conclusion: PTQ requires caution in real-world deployment due to potential catastrophic failures. Emphasizes need for rigorous robustness examination and safety considerations in deep learning.

Abstract: Post-training quantization (PTQ) has recently emerged as an effective tool
for reducing the computational complexity and memory usage of a neural network
by representing its weights and activations with lower precision. While this
paradigm has shown great success in lowering compute and storage costs, there
is the potential for drastic performance reduction depending upon the
distribution of inputs experienced in inference. When considering possible
deployment in safety-critical environments, it is important to investigate the
extent of potential performance reduction, and what characteristics of input
distributions may give rise to this reduction. In this work, we explore the
idea of extreme failure stemming from dynamic PTQ and formulate a knowledge
distillation and reinforcement learning task to learn a network and bit-width
policy pair such that catastrophic failure under quantization is analyzed in
terms of worst case potential. Our results confirm the existence of this
"detrimental" network-policy pair, with several instances demonstrating
performance reductions in the range of 10-65% in accuracy, compared to their
"robust" counterparts encountering a <2% decrease. From systematic
experimentation and analyses, we also provide an initial exploration into
points at highest vulnerability. While our results represent an initial step
toward understanding failure cases introduced by PTQ, our findings ultimately
emphasize the need for caution in real-world deployment scenarios. We hope this
work encourages more rigorous examinations of robustness and a greater emphasis
on safety considerations for future works within the broader field of deep
learning.

</details>


### [110] [SAGE: Streaming Agreement-Driven Gradient Sketches for Representative Subset Selection](https://arxiv.org/abs/2510.02470)
*Ashish Jha,Salman Ahmadi-Asl*

Main category: cs.LG

TL;DR: SAGE is a streaming data-subset selection method that uses Frequent Directions sketches to efficiently select training examples based on gradient alignment, reducing computational and memory requirements while maintaining competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Training modern neural networks on large datasets is computationally and energy intensive, creating a need for efficient data selection methods that reduce resource requirements without sacrificing performance.

Method: SAGE maintains a compact Frequent Directions sketch of gradient geometry in O(ℓD) memory, prioritizes examples whose sketched gradients align with a consensus direction, and uses a simple two-pass GPU-friendly pipeline that eliminates N×N pairwise similarities and explicit N×ℓ gradient stores.

Result: Across multiple benchmarks, SAGE trains with small kept-rate budgets while retaining competitive accuracy relative to full-data training and recent subset-selection baselines, and reduces end-to-end compute and peak memory.

Conclusion: SAGE offers a practical, constant-memory alternative that complements pruning and model compression for efficient training, providing deterministic approximation guarantees while preserving gradient energy within the principal sketched subspace.

Abstract: Training modern neural networks on large datasets is computationally and
energy intensive. We present SAGE, a streaming data-subset selection method
that maintains a compact Frequent Directions (FD) sketch of gradient geometry
in $O(\ell D)$ memory and prioritizes examples whose sketched gradients align
with a consensus direction. The approach eliminates $N \times N$ pairwise
similarities and explicit $N \times \ell$ gradient stores, yielding a simple
two-pass, GPU-friendly pipeline. Leveraging FD's deterministic approximation
guarantees, we analyze how agreement scoring preserves gradient energy within
the principal sketched subspace. Across multiple benchmarks, SAGE trains with
small kept-rate budgets while retaining competitive accuracy relative to
full-data training and recent subset-selection baselines, and reduces
end-to-end compute and peak memory. Overall, SAGE offers a practical,
constant-memory alternative that complements pruning and model compression for
efficient training.

</details>


### [111] [Uncertainty-Guided Model Selection for Tabular Foundation Models in Biomolecule Efficacy Prediction](https://arxiv.org/abs/2510.02476)
*Jie Li,Andrew McCarthy,Zhizhuo Zhang,Stephen Young*

Main category: cs.LG

TL;DR: The paper proposes an uncertainty-guided ensemble strategy for TabPFN models in biomolecule efficacy prediction, using inter-quantile range (IQR) as a label-free heuristic to select the best models for ensembling.


<details>
  <summary>Details</summary>
Motivation: In-context learners like TabPFN are promising for biomolecule efficacy prediction but their performance is sensitive to context. The challenge is selecting the best models for ensembling without ground truth labels.

Method: Use TabPFN with sequence-based features, employ model's predicted inter-quantile range (IQR) as uncertainty measure, and select ensemble models with lowest mean IQR for averaging.

Result: TabPFN with simple sequence features outperforms specialized state-of-the-art predictors. IQR shows negative correlation with true prediction error. Ensemble with lowest mean IQR achieves superior performance over naive ensembling or single model.

Conclusion: Model uncertainty (IQR) serves as a powerful, label-free heuristic for optimizing biomolecule efficacy predictions through uncertainty-guided ensemble selection.

Abstract: In-context learners like TabPFN are promising for biomolecule efficacy
prediction, where established molecular feature sets and relevant experimental
results can serve as powerful contextual examples. However, their performance
is highly sensitive to the provided context, making strategies like post-hoc
ensembling of models trained on different data subsets a viable approach. An
open question is how to select the best models for the ensemble without access
to ground truth labels. In this study, we investigate an uncertainty-guided
strategy for model selection. We demonstrate on an siRNA knockdown efficacy
task that a TabPFN model using simple sequence-based features can surpass
specialized state-of-the-art predictors. We also show that the model's
predicted inter-quantile range (IQR), a measure of its uncertainty, has a
negative correlation with true prediction error. By selecting and averaging an
ensemble of models with the lowest mean IQR, we achieve superior performance
compared to naive ensembling or using a single model trained on all available
data. This finding highlights model uncertainty as a powerful, label-free
heuristic for optimizing biomolecule efficacy predictions.

</details>


### [112] [Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework](https://arxiv.org/abs/2510.02483)
*Nii Osae Osae Dade,Moinul Hossain Rahat*

Main category: cs.LG

TL;DR: Litespark is a novel pre-training framework that optimizes transformer attention and MLP layers to achieve 2x-6x training throughput improvement and 55%-83% energy reduction for LLMs.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of long training times and massive energy consumption in LLM training, which require months of computation and gigawatt-hours of electricity.

Method: Combines architectural improvements with algorithmic enhancements to transformer attention and MLP layers, maximizing Model FLOPs Utilization while maintaining compatibility with standard transformers.

Result: 2x-6x training throughput improvement and 55%-83% energy consumption reduction on 3B and 30B parameter Llama models using SlimPajama-627B dataset across multi-node H200 GPU clusters.

Conclusion: The optimizations are model- and hardware-agnostic, enabling broad applicability across transformer architectures and extending to post-training phases including supervised fine-tuning and direct preference optimization.

Abstract: Training Large Language Models (LLMs) is plagued by long training times and
massive energy consumption, with modern models requiring months of computation
and gigawatt-hours of electricity. In light of these challenges,we introduce
Litespark, a novel pre-training framework that addresses these inefficiencies
through targeted optimizations to transformer attention and MLP layers. Our
approach combines architectural improvements with algorithmic enhancements to
maximize Model FLOPs Utilization (MFU) while maintaining compatibility with
standard transformer implementations. Comprehensive benchmarking on 3B and 30B
parameter Llama models using the SlimPajama-627B dataset demonstrates
substantial performance gains: 2x-6x training throughput improvement and
$55\%-83$% energy consumption reduction across multi-node H200 GPU clusters.
These optimizations are model- and hardware-agnostic, enabling broad
applicability across transformer architectures and extending to post-training
phases including supervised fine-tuning and direct preference optimization.

</details>


### [113] [From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning](https://arxiv.org/abs/2510.02484)
*Rafael Rodriguez-Sanchez,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: ACF is a contrastive learning method that discovers independently controllable latent variables from high-dimensional observations, enabling sample-efficient RL by leveraging factored structure without requiring prior knowledge of the factorization.


<details>
  <summary>Details</summary>
Motivation: Existing factored MDP algorithms require known factored representations, which breaks down with high-dimensional observations, while deep RL cannot exploit factored structure. This creates a representation gap.

Method: Action-Controllable Factorization (ACF) uses contrastive learning to uncover latent variables that each action can influence separately, leveraging sparsity where actions affect only subsets of variables.

Result: ACF successfully recovers ground truth controllable factors from pixel observations on Taxi, FourRooms, and MiniGrid-DoorKey benchmarks, consistently outperforming baseline disentanglement algorithms.

Conclusion: ACF bridges the gap between factored MDPs and deep RL by automatically discovering controllable latent structure from raw observations, enabling sample-efficient reinforcement learning.

Abstract: Algorithms that exploit factored Markov decision processes are far more
sample-efficient than factor-agnostic methods, yet they assume a factored
representation is known a priori -- a requirement that breaks down when the
agent sees only high-dimensional observations. Conversely, deep reinforcement
learning handles such inputs but cannot benefit from factored structure. We
address this representation problem with Action-Controllable Factorization
(ACF), a contrastive learning approach that uncovers independently controllable
latent variables -- state components each action can influence separately. ACF
leverages sparsity: actions typically affect only a subset of variables, while
the rest evolve under the environment's dynamics, yielding informative data for
contrastive training. ACF recovers the ground truth controllable factors
directly from pixel observations on three benchmarks with known factored
structure -- Taxi, FourRooms, and MiniGrid-DoorKey -- consistently
outperforming baseline disentanglement algorithms.

</details>


### [114] [Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking](https://arxiv.org/abs/2510.02490)
*Shaifalee Saxena,Alan Williams,Rafael Fierro,Alexander Scheinker*

Main category: cs.LG

TL;DR: This paper proposes a hybrid controller combining deep reinforcement learning (DRL) with bounded extremum seeking (ES) to improve robustness in nonlinear time-varying systems. The hybrid approach leverages DRL's ability to learn from historical data for fast control of many-parameter systems while using bounded ES to ensure robustness against time variations.


<details>
  <summary>Details</summary>
Motivation: DRL controllers perform well with large datasets but degrade catastrophically when system models change rapidly over time. Bounded ES can handle time-varying systems but slows down with many parameters and can get stuck in local minima. The motivation is to combine both methods to overcome their individual limitations.

Method: The authors develop a hybrid controller that integrates DRL with bounded ES feedback control. DRL learns from historical data to quickly control many-parameter systems, while bounded ES ensures robustness to time variations by handling unknown control directions in time-varying systems.

Result: The hybrid ES-DRL controller demonstrates performance that exceeds the sum of its individual components. It successfully controls a general time-varying system and is applied to automatic tuning of the Low Energy Beam Transport section at the Los Alamos Neutron Science Center linear particle accelerator.

Conclusion: Combining DRL with bounded ES creates a synergistic hybrid controller that overcomes the limitations of both methods individually - DRL provides fast learning from historical data while bounded ES ensures robustness to time variations, resulting in superior performance for nonlinear time-varying systems.

Abstract: In this paper, we study the use of robust model independent bounded extremum
seeking (ES) feedback control to improve the robustness of deep reinforcement
learning (DRL) controllers for a class of nonlinear time-varying systems. DRL
has the potential to learn from large datasets to quickly control or optimize
the outputs of many-parameter systems, but its performance degrades
catastrophically when the system model changes rapidly over time. Bounded ES
can handle time-varying systems with unknown control directions, but its
convergence speed slows down as the number of tuned parameters increases and,
like all local adaptive methods, it can get stuck in local minima. We
demonstrate that together, DRL and bounded ES result in a hybrid controller
whose performance exceeds the sum of its parts with DRL taking advantage of
historical data to learn how to quickly control a many-parameter system to a
desired setpoint while bounded ES ensures its robustness to time variations. We
present a numerical study of a general time-varying system and a combined
ES-DRL controller for automatic tuning of the Low Energy Beam Transport section
at the Los Alamos Neutron Science Center linear particle accelerator.

</details>


### [115] [Beyond Imitation: Recovering Dense Rewards from Demonstrations](https://arxiv.org/abs/2510.02493)
*Jiangnan Li,Thuy-Trang Vu,Ehsan Abbasnejad,Gholamreza Haffari*

Main category: cs.LG

TL;DR: SFT is equivalent to Inverse RL, learning implicit token-level rewards that can be recovered and used for policy improvement via reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Challenge the conventional view of SFT as simple imitation learning and establish its equivalence with Inverse RL to reveal hidden reward learning capabilities.

Method: Prove SFT objective is a special case of Inverse Q-Learning, recover dense token-level rewards from SFT models, and develop Dense-Path REINFORCE for policy improvement.

Result: Dense-Path REINFORCE consistently outperforms original SFT models on instruction-following benchmarks.

Conclusion: SFT should be reframed as a powerful reward learning mechanism, not just policy imitation, opening new possibilities for leveraging expert demonstrations.

Abstract: Conventionally, supervised fine-tuning (SFT) is treated as a simple imitation
learning process that only trains a policy to imitate expert behavior on
demonstration datasets. In this work, we challenge this view by establishing a
fundamental equivalence between SFT and Inverse Reinforcement Learning. We
prove that the SFT objective is a special case of Inverse Q-Learning, which
implies that the SFT process does not just learn a policy, but also an
implicit, dense, token-level reward model that explains the expert
demonstrations. We then show how to recover this dense reward signal directly
from the SFT model by formulating a baseline-relative reward function. The
availability of such a dense reward model offers numerous benefits, providing
granular credit assignment for each token generated. We demonstrate one key
application by using these recovered rewards to further improve the policy with
reinforcement learning. Our method, Dense-Path REINFORCE, consistently
outperforms the original SFT models on instruction-following benchmarks. This
work reframes SFT not merely as policy imitation but as a powerful reward
learning mechanism, opening new possibilities for leveraging expert
demonstrations.

</details>


### [116] [In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning](https://arxiv.org/abs/2510.02516)
*Jindan Li,Zhaoxian Wu,Gaowen Liu,Tayfun Gokmen,Tianyi Chen*

Main category: cs.LG

TL;DR: Proposes residual learning framework for analog in-memory training with limited 4-bit memristive devices, using multiple crossbar tiles to compensate low-precision errors and achieve linear convergence.


<details>
  <summary>Details</summary>
Motivation: Analog in-memory computing accelerators need 8-bit conductance states for effective training, but many memristive devices like ReRAM only offer 4-bit resolution due to fabrication constraints, degrading training accuracy.

Method: Residual learning framework that sequentially learns on multiple crossbar tiles to compensate residual errors from low-precision weight updates.

Result: Theoretical analysis shows optimality gap shrinks with number of tiles and achieves linear convergence rate. Experiments show consistent outperformance of state-of-the-art in-memory analog training strategies under limited-state settings.

Conclusion: Enables on-chip training with limited-state devices while incurring only moderate hardware overhead, making practical analog in-memory training feasible with current 4-bit memristive technology.

Abstract: Analog in-memory computing (AIMC) accelerators enable efficient deep neural
network computation directly within memory using resistive crossbar arrays,
where model parameters are represented by the conductance states of memristive
devices. However, effective in-memory training typically requires at least
8-bit conductance states to match digital baselines. Realizing such
fine-grained states is costly and often requires complex noise mitigation
techniques that increase circuit complexity and energy consumption. In
practice, many promising memristive devices such as ReRAM offer only about
4-bit resolution due to fabrication constraints, and this limited update
precision substantially degrades training accuracy. To enable on-chip training
with these limited-state devices, this paper proposes a \emph{residual
learning} framework that sequentially learns on multiple crossbar tiles to
compensate the residual errors from low-precision weight updates. Our
theoretical analysis shows that the optimality gap shrinks with the number of
tiles and achieves a linear convergence rate. Experiments on standard image
classification benchmarks demonstrate that our method consistently outperforms
state-of-the-art in-memory analog training strategies under limited-state
settings, while incurring only moderate hardware overhead as confirmed by our
cost analysis.

</details>


### [117] [Graph Generation with Spectral Geodesic Flow Matching](https://arxiv.org/abs/2510.02520)
*Xikun Huang,Tianyu Ruan,Chihao Zhang,Shihua Zhang*

Main category: cs.LG

TL;DR: SFMG is a novel graph generation framework that uses spectral eigenmaps to embed graphs into Riemannian manifolds, then matches distributions along geodesic flows for efficient and geometrically-aware graph synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing graph generation methods often ignore the geometry induced by eigenvectors and global graph structure, focusing only on spectrum or degree profiles.

Method: Uses spectral eigenmaps to embed graphs into continuous Riemannian manifolds, defines geodesic flows between embeddings, and matches distributions along these flows to generate output graphs.

Result: Matches state-of-the-art performance on graphlet, degree, and spectral metrics across diverse benchmarks, achieves 30x speedup over diffusion models, and demonstrates generalization to unseen graph scales.

Conclusion: SFMG provides a new approach to graph synthesis by integrating spectral geometry with flow matching, offering geometric structure capture, flexible generation, and efficient scalability.

Abstract: Graph generation is a fundamental task with wide applications in modeling
complex systems. Although existing methods align the spectrum or degree profile
of the target graph, they often ignore the geometry induced by eigenvectors and
the global structure of the graph. In this work, we propose Spectral Geodesic
Flow Matching (SFMG), a novel framework that uses spectral eigenmaps to embed
both input and target graphs into continuous Riemannian manifolds. We then
define geodesic flows between embeddings and match distributions along these
flows to generate output graphs. Our method yields several advantages: (i)
captures geometric structure beyond eigenvalues, (ii) supports flexible
generation of diverse graphs, and (iii) scales efficiently. Empirically, SFMG
matches the performance of state-of-the-art approaches on graphlet, degree, and
spectral metrics across diverse benchmarks. In particular, it achieves up to
30$\times$ speedup over diffusion-based models, offering a substantial
advantage in scalability and training efficiency. We also demonstrate its
ability to generalize to unseen graph scales. Overall, SFMG provides a new
approach to graph synthesis by integrating spectral geometry with flow
matching.

</details>


### [118] [Model-brain comparison using inter-animal transforms](https://arxiv.org/abs/2510.02523)
*Imran Thobani,Javier Sagastuy-Brena,Aran Nayebi,Jacob Prince,Rosa Cao,Daniel Yamins*

Main category: cs.LG

TL;DR: The paper proposes a new comparison methodology called Inter-Animal Transform Class (IATC) for comparing artificial neural network models to brain responses, which enables accurate neural activity predictions while achieving high specificity in mechanism identification without tradeoffs.


<details>
  <summary>Details</summary>
Motivation: There is little consensus on the correct method for comparing model activations to brain responses in neuroscience, and existing approaches lack principled comparison frameworks.

Method: The IATC methodology identifies the strictest set of functions needed to accurately map neural responses between subjects, allowing bidirectional mapping between candidate models' responses and brain data to assess how well models can masquerade as typical subjects.

Result: The IATC resolves detailed neural mechanisms like non-linear activation functions, enables accurate neural activity predictions, achieves high specificity in mechanism identification, and provides new evidence favoring topographical deep neural networks as models of the visual system.

Conclusion: The IATC enables principled model-brain comparisons without inherent tradeoffs between predictivity and mechanistic accuracy, improving upon previous approaches and contextualizing findings about deep learning models of the brain.

Abstract: Artificial neural network models have emerged as promising mechanistic models
of the brain. However, there is little consensus on the correct method for
comparing model activations to brain responses. Drawing on recent work in
philosophy of neuroscience, we propose a comparison methodology based on the
Inter-Animal Transform Class (IATC) - the strictest set of functions needed to
accurately map neural responses between subjects in an animal population. Using
the IATC, we can map bidirectionally between a candidate model's responses and
brain data, assessing how well the model can masquerade as a typical subject
using the same kinds of transforms needed to map across real subjects. We
identify the IATC in three settings: a simulated population of neural network
models, a population of mouse subjects, and a population of human subjects. We
find that the IATC resolves detailed aspects of the neural mechanism, such as
the non-linear activation function. Most importantly, we find that the IATC
enables accurate predictions of neural activity while also achieving high
specificity in mechanism identification, evidenced by its ability to separate
response patterns from different brain areas while strongly aligning
same-brain-area responses between subjects. In other words, the IATC is a
proof-by-existence that there is no inherent tradeoff between the neural
engineering goal of high model-brain predictivity and the neuroscientific goal
of identifying mechanistically accurate brain models. Using IATC-guided
transforms, we obtain new evidence in favor of topographical deep neural
networks (TDANNs) as models of the visual system. Overall, the IATC enables
principled model-brain comparisons, contextualizing previous findings about the
predictive success of deep learning models of the brain, while improving upon
previous approaches to model-brain comparison.

</details>


### [119] [AttentiveGRUAE: An Attention-Based GRU Autoencoder for Temporal Clustering and Behavioral Characterization of Depression from Wearable Data](https://arxiv.org/abs/2510.02558)
*Nidhi Soley,Vishal M Patel,Casey O Taylor*

Main category: cs.LG

TL;DR: AttentiveGRUAE is an attention-based GRU autoencoder for temporal clustering and outcome prediction from longitudinal wearable data, achieving superior clustering and depression classification performance.


<details>
  <summary>Details</summary>
Motivation: To develop a model that can jointly learn compact latent representations from longitudinal wearable data, predict depression outcomes, and identify behavioral subtypes through clustering.

Method: Uses attention-based GRU autoencoder with three objectives: sequence reconstruction, depression classification, and GMM-based soft clustering of embeddings.

Result: Achieved silhouette score of 0.70 (vs 0.32-0.70 baselines) and AUC of 0.74 (vs 0.50-0.67 baselines) on sleep data from 372 participants, with external validation confirming reproducibility.

Conclusion: The model provides clinically interpretable explanations of risk through subtype analysis and temporal attention visualization, demonstrating effectiveness for behavioral phenotyping from wearable data.

Abstract: In this study, we present AttentiveGRUAE, a novel attention-based gated
recurrent unit (GRU) autoencoder designed for temporal clustering and
prediction of outcome from longitudinal wearable data. Our model jointly
optimizes three objectives: (1) learning a compact latent representation of
daily behavioral features via sequence reconstruction, (2) predicting
end-of-period depression rate through a binary classification head, and (3)
identifying behavioral subtypes through Gaussian Mixture Model (GMM) based soft
clustering of learned embeddings. We evaluate AttentiveGRUAE on longitudinal
sleep data from 372 participants (GLOBEM 2018-2019), and it demonstrates
superior performance over baseline clustering, domain-aligned self-supervised,
and ablated models in both clustering quality (silhouette score = 0.70 vs
0.32-0.70) and depression classification (AUC = 0.74 vs 0.50-0.67).
Additionally, external validation on cross-year cohorts from 332 participants
(GLOBEM 2020-2021) confirms cluster reproducibility (silhouette score = 0.63,
AUC = 0.61) and stability. We further perform subtype analysis and visualize
temporal attention, which highlights sleep-related differences between clusters
and identifies salient time windows that align with changes in sleep
regularity, yielding clinically interpretable explanations of risk.

</details>


### [120] [On The Expressive Power of GNN Derivatives](https://arxiv.org/abs/2510.02565)
*Yam Eitan,Moshe Eliasof,Yoav Gelberg,Fabrizio Frasca,Guy Bar-Shalom,Haggai Maron*

Main category: cs.LG

TL;DR: HOD-GNN enhances GNN expressivity by leveraging high-order node derivatives of base MPNNs, creating structure-aware embeddings processed by a second GNN, with theoretical alignment to WL hierarchy and strong empirical performance.


<details>
  <summary>Details</summary>
Motivation: To address the limited expressivity of GNNs by exploring unexplored derivatives as a means to enhance expressivity, building on research about GNN derivatives in oversquashing, over-smoothing, and explainability contexts.

Method: Introduces High-Order Derivative GNN (HOD-GNN) that uses high-order node derivatives of base MPNNs to generate expressive structure-aware node embeddings, processed by a second GNN in end-to-end trainable architecture with efficient message-passing algorithm.

Result: Theoretical analysis shows alignment with WL hierarchy, deep connections to Subgraph GNNs and structural encodings, and strong performance on popular graph learning benchmarks.

Conclusion: High-order derivatives provide a natural and effective way to enhance GNN expressivity, with HOD-GNN demonstrating both theoretical foundations and practical performance improvements.

Abstract: Despite significant advances in Graph Neural Networks (GNNs), their limited
expressivity remains a fundamental challenge. Research on GNN expressivity has
produced many expressive architectures, leading to architecture hierarchies
with models of increasing expressive power. Separately, derivatives of GNNs
with respect to node features have been widely studied in the context of the
oversquashing and over-smoothing phenomena, GNN explainability, and more. To
date, these derivatives remain unexplored as a means to enhance GNN
expressivity. In this paper, we show that these derivatives provide a natural
way to enhance the expressivity of GNNs. We introduce High-Order Derivative GNN
(HOD-GNN), a novel method that enhances the expressivity of Message Passing
Neural Networks (MPNNs) by leveraging high-order node derivatives of the base
model. These derivatives generate expressive structure-aware node embeddings
processed by a second GNN in an end-to-end trainable architecture.
Theoretically, we show that the resulting architecture family's expressive
power aligns with the WL hierarchy. We also draw deep connections between
HOD-GNN, Subgraph GNNs, and popular structural encoding schemes. For
computational efficiency, we develop a message-passing algorithm for computing
high-order derivatives of MPNNs that exploits graph sparsity and parallelism.
Evaluations on popular graph learning benchmarks demonstrate HOD-GNN's strong
performance on popular graph learning tasks.

</details>


### [121] [Geospatial Machine Learning Libraries](https://arxiv.org/abs/2510.02572)
*Adam J. Stewart,Caleb Robinson,Arindam Banerjee*

Main category: cs.LG

TL;DR: This paper provides a comprehensive overview of geospatial machine learning (GeoML) libraries, analyzing their evolution, functionalities, and ecosystem, with practical applications in crop type mapping and discussion of future directions.


<details>
  <summary>Details</summary>
Motivation: The availability of Earth observation data has outpaced the development of domain-specific libraries to handle unique geospatial challenges like varying spatial resolutions, spectral properties, temporal cadence, and coordinate systems.

Method: The chapter presents analysis of GeoML libraries' evolution and core functionalities, introduces popular libraries (TorchGeo, eo-learn, Raster Vision), discusses methodologies for data preprocessing, spatial-temporal joins, benchmarking, and uses a crop type mapping case study.

Result: The paper demonstrates practical applications of GeoML tools through case studies and provides guidance on navigating the GeoML ecosystem, highlighting best practices in software design, licensing, and testing.

Conclusion: The paper aims to guide practitioners, developers, and researchers in the rapidly evolving GeoML landscape, addressing open challenges and future directions including foundation models and governance in open-source geospatial software.

Abstract: Recent advances in machine learning have been supported by the emergence of
domain-specific software libraries, enabling streamlined workflows and
increased reproducibility. For geospatial machine learning (GeoML), the
availability of Earth observation data has outpaced the development of domain
libraries to handle its unique challenges, such as varying spatial resolutions,
spectral properties, temporal cadence, data coverage, coordinate systems, and
file formats. This chapter presents a comprehensive overview of GeoML
libraries, analyzing their evolution, core functionalities, and the current
ecosystem. It also introduces popular GeoML libraries such as TorchGeo,
eo-learn, and Raster Vision, detailing their architecture, supported data
types, and integration with ML frameworks. Additionally, it discusses common
methodologies for data preprocessing, spatial--temporal joins, benchmarking,
and the use of pretrained models. Through a case study in crop type mapping, it
demonstrates practical applications of these tools. Best practices in software
design, licensing, and testing are highlighted, along with open challenges and
future directions, particularly the rise of foundation models and the need for
governance in open-source geospatial software. Our aim is to guide
practitioners, developers, and researchers in navigating and contributing to
the rapidly evolving GeoML landscape.

</details>


### [122] [Use the Online Network If You Can: Towards Fast and Stable Reinforcement Learning](https://arxiv.org/abs/2510.02590)
*Ahmed Hendawy,Henrik Metternich,Théo Vincent,Mahdi Kallel,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: MINTO introduces a novel update rule that uses the minimum estimate between target and online networks for value function estimation, enabling faster and more stable learning in deep RL.


<details>
  <summary>Details</summary>
Motivation: Target networks provide stability but slow learning, while using online networks directly causes instability. MINTO aims to combine the benefits of both approaches.

Method: MINTO computes bootstrapped targets using the minimum value estimate between the target network and online network, mitigating overestimation bias.

Result: MINTO consistently improves performance across diverse benchmarks including online/offline RL and discrete/continuous action spaces, with negligible computational cost.

Conclusion: MINTO provides a simple yet effective modification that enables faster and more stable value function learning, broadly applicable to various RL algorithms.

Abstract: The use of target networks is a popular approach for estimating value
functions in deep Reinforcement Learning (RL). While effective, the target
network remains a compromise solution that preserves stability at the cost of
slowly moving targets, thus delaying learning. Conversely, using the online
network as a bootstrapped target is intuitively appealing, albeit well-known to
lead to unstable learning. In this work, we aim to obtain the best out of both
worlds by introducing a novel update rule that computes the target using the
MINimum estimate between the Target and Online network, giving rise to our
method, MINTO. Through this simple, yet effective modification, we show that
MINTO enables faster and stable value function learning, by mitigating the
potential overestimation bias of using the online network for bootstrapping.
Notably, MINTO can be seamlessly integrated into a wide range of value-based
and actor-critic algorithms with a negligible cost. We evaluate MINTO
extensively across diverse benchmarks, spanning online and offline RL, as well
as discrete and continuous action spaces. Across all benchmarks, MINTO
consistently improves performance, demonstrating its broad applicability and
effectiveness.

</details>


### [123] [Towards CONUS-Wide ML-Augmented Conceptually-Interpretable Modeling of Catchment-Scale Precipitation-Storage-Runoff Dynamics](https://arxiv.org/abs/2510.02605)
*Yuan-Heng Wang,Yang Yang,Fabio Ciulla,Hoshin V. Gupta,Charuleka Varadharajan*

Main category: cs.LG

TL;DR: ML-augmented physically-interpretable catchment models using Mass-Conserving Perceptron (MCP) achieve comparable performance to LSTM-based models while providing better physical understanding and interpretability across diverse US hydrological regimes.


<details>
  <summary>Details</summary>
Motivation: Address the gap between ML-based hydrologic modeling and physical-conceptual understanding, aiming to develop interpretable models that translate into enhanced predictive improvements.

Method: CONUS-wide large-sample study using ML-augmented physically-interpretable catchment-scale models based on Mass-Conserving Perceptron (MCP) with varying complexity, evaluated using attribute masks (snow regime, forest cover, climate zone).

Result: MCP-based models achieve performance comparable to LSTM models; importance of selecting appropriate model complexity based on hydrological regime process dominance; physically-interpretable models can match data-based model performance.

Conclusion: Theory-informed, physically grounded approach to large-sample hydrology enables mechanistic understanding and development of parsimonious, interpretable model architectures for future 'models of everywhere' that encode spatial and temporal process dominance information.

Abstract: While many modern studies are dedicated to ML-based large-sample hydrologic
modeling, these efforts have not necessarily translated into predictive
improvements that are grounded in enhanced physical-conceptual understanding.
Here, we report on a CONUS-wide large-sample study (spanning diverse
hydro-geo-climatic conditions) using ML-augmented physically-interpretable
catchment-scale models of varying complexity based in the Mass-Conserving
Perceptron (MCP). Results were evaluated using attribute masks such as snow
regime, forest cover, and climate zone. Our results indicate the importance of
selecting model architectures of appropriate model complexity based on how
process dominance varies with hydrological regime. Benchmark comparisons show
that physically-interpretable mass-conserving MCP-based models can achieve
performance comparable to data-based models based in the Long Short-Term Memory
network (LSTM) architecture. Overall, this study highlights the potential of a
theory-informed, physically grounded approach to large-sample hydrology, with
emphasis on mechanistic understanding and the development of parsimonious and
interpretable model architectures, thereby laying the foundation for future
models of everywhere that architecturally encode information about spatially-
and temporally-varying process dominance.

</details>


### [124] [MINERVA: Mutual Information Neural Estimation for Supervised Feature Selection](https://arxiv.org/abs/2510.02610)
*Taurai Muvunzaa,Egor Kraev,Pere Planell-Morell,Alexander Y. Shestopaloff*

Main category: cs.LG

TL;DR: MINERVA is a neural network-based feature selection method that uses mutual information estimation with sparsity regularization to capture complex higher-order feature interactions that traditional pair-wise dependence metrics miss.


<details>
  <summary>Details</summary>
Motivation: Traditional feature filters fail when targets depend on higher-order feature interactions rather than individual feature contributions, as they rely on statistical pair-wise dependence metrics.

Method: Two-stage process: neural estimation of mutual information between features and targets using neural networks, with sparsity-inducing regularizers in the loss function. Decouples representation learning from feature selection and evaluates feature subsets as an ensemble.

Result: Effectively captures complex feature-target relationships that are rarely captured in literature. Experimental results on synthetic and real-life fraud datasets demonstrate efficacy and ability to perform exact solutions.

Conclusion: MINERVA provides a novel approach to supervised feature selection that better handles complex dependency structures through neural mutual information estimation and ensemble evaluation of feature subsets.

Abstract: Existing feature filters rely on statistical pair-wise dependence metrics to
model feature-target relationships, but this approach may fail when the target
depends on higher-order feature interactions rather than individual
contributions. We introduce Mutual Information Neural Estimation Regularized
Vetting Algorithm (MINERVA), a novel approach to supervised feature selection
based on neural estimation of mutual information between features and targets.
We paramaterize the approximation of mutual information with neural networks
and perform feature selection using a carefully designed loss function
augmented with sparsity-inducing regularizers. Our method is implemented in a
two-stage process to decouple representation learning from feature selection,
ensuring better generalization and a more accurate expression of feature
importance. We present examples of ubiquitous dependency structures that are
rarely captured in literature and show that our proposed method effectively
captures these complex feature-target relationships by evaluating feature
subsets as an ensemble. Experimental results on synthetic and real-life fraud
datasets demonstrate the efficacy of our method and its ability to perform
exact solutions.

</details>


### [125] [TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer](https://arxiv.org/abs/2510.02625)
*Jacob Feitelberg,Dwaipayan Saha,Kyuseong Choi,Zaid Ahmad,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: TabImpute is a pre-trained transformer for zero-shot tabular data imputation that requires no fitting or hyperparameter tuning, achieving fast and accurate results across diverse domains.


<details>
  <summary>Details</summary>
Motivation: Existing imputation methods suffer from performance variance across domains and require time-consuming hyperparameter tuning, with no default solution available for tabular data.

Method: Uses TabPFN foundation model with entry-wise featurization for 100x speedup, synthetic training data with realistic missingness patterns, and comprehensive MissBench evaluation framework.

Result: TabImpute shows robust performance compared to 11 established methods across 42 datasets and 13 missingness patterns in medicine, finance, and engineering domains.

Conclusion: TabImpute provides an effective zero-shot imputation solution that eliminates the need for hyperparameter tuning while maintaining strong performance across diverse real-world scenarios.

Abstract: Missing data is a pervasive problem in tabular settings. Existing solutions
range from simple averaging to complex generative adversarial networks.
However, due to huge variance in performance across real-world domains and
time-consuming hyperparameter tuning, no default imputation method exists.
Building on TabPFN, a recent tabular foundation model for supervised learning,
we propose TabImpute, a pre-trained transformer that delivers accurate and fast
zero-shot imputations requiring no fitting or hyperparameter tuning at
inference-time. To train and evaluate TabImpute, we introduce (i) an entry-wise
featurization for tabular settings, which enables a $100\times$ speedup over
the previous TabPFN imputation method, (ii) a synthetic training data
generation pipeline incorporating realistic missingness patterns, which boosts
test-time performance, and (iii) MissBench, a comprehensive benchmark for
evaluation of imputation methods with $42$ OpenML datasets and $13$ missingness
patterns. MissBench spans domains such as medicine, finance, and engineering,
showcasing TabImpute's robust performance compared to $11$ established
imputation methods.

</details>


### [126] [HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance](https://arxiv.org/abs/2510.02630)
*Hao Zhang,Zhenjia Li,Runfeng Bao,Yifan Gao,Xi Xiao,Bo Huang,Yuhang Wu,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: HyperAdaLoRA is a novel PEFT framework that uses a hypernetwork with attention mechanisms to dynamically generate SVD parameters, accelerating AdaLoRA's convergence while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the slow convergence and high computational overhead issues in AdaLoRA, which uses dynamic rank allocation but suffers from optimization challenges.

Method: Employ a hypernetwork based on attention mechanisms to dynamically generate SVD parameters (P, Λ, Q) instead of directly optimizing them, with pruning of singular values for dynamic rank allocation.

Result: Achieves faster convergence without sacrificing performance on various datasets and models, with broad applicability validated on other LoRA-based approaches.

Conclusion: HyperAdaLoRA effectively accelerates AdaLoRA convergence through hypernetwork-based parameter generation, offering a promising solution for efficient LLM fine-tuning.

Abstract: Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation
(LoRA), has emerged as a promising approach to fine-tuning large language
models(LLMs) while reducing computational and memory overhead. However, LoRA
assumes a uniform rank \textit{r} for each incremental matrix, not accounting
for the varying significance of weight matrices across different modules and
layers. AdaLoRA leverages Singular Value Decomposition (SVD) to parameterize
updates and employs pruning of singular values to introduce dynamic rank
allocation, thereby enhancing adaptability. However, during the training
process, it often encounters issues of slow convergence speed and high
computational overhead. To address this issue, we propose HyperAdaLoRA, a novel
framework that accelerates the convergence of AdaLoRA by leveraging a
hypernetwork. Instead of directly optimizing the components of Singular Value
Decomposition $(P, \Lambda, Q)$, HyperAdaLoRA employs a hypernetwork based on
attention mechanisms to dynamically generate these parameters. By pruning the
outputs of the hypernetwork that generates the singular values, dynamic rank
allocation is achieved. Comprehensive experiments on various datasets and
models demonstrate that our method achieves faster convergence without
sacrificing performance. Additionally, further extension experiments on other
LoRA-based approaches validate the broad applicability of our method.

</details>


### [127] [Optimal Characteristics of Inspection Vehicle for Drive-by Bridge Inspection](https://arxiv.org/abs/2510.02658)
*A. Calderon Hurtado,E. Atroshchenko,K. C. Chang,C. W. Kim,M. Makki Alamdari*

Main category: cs.LG

TL;DR: This paper presents a framework for optimizing inspection vehicles in drive-by bridge monitoring using adversarial autoencoders and Kriging meta-modeling to enhance damage sensitivity by tuning vehicle mass and stiffness parameters.


<details>
  <summary>Details</summary>
Motivation: Drive-by inspection for bridge monitoring is gaining attention but vehicle properties significantly limit detection performance, requiring optimization of the inspection vehicle to improve damage sensitivity.

Method: Uses unsupervised deep learning with adversarial autoencoders to reconstruct frequency-domain acceleration responses, optimizes vehicle mass and stiffness by minimizing Wasserstein distance between damage distributions, and employs Kriging meta-model for efficient optimization.

Result: Vehicles with frequency ratios between 0.3-0.7 relative to bridge's first natural frequency are most effective, while resonant vehicles perform poorly. Lighter vehicles need lower natural frequencies for optimal detection.

Conclusion: This is the first study to rigorously optimize the sensing platform for drive-by sensing and propose a purpose-built inspection vehicle, demonstrating significant improvements in damage detection sensitivity through vehicle parameter optimization.

Abstract: Drive-by inspection for bridge health monitoring has gained increasing
attention over the past decade. This method involves analysing the coupled
vehicle-bridge response, recorded by an instrumented inspection vehicle, to
assess structural integrity and detect damage. However, the vehicles mechanical
and dynamic properties significantly influence detection performance, limiting
the effectiveness of the approach. This study presents a framework for
optimising the inspection vehicle to enhance damage sensitivity. An
unsupervised deep learning methodbased on adversarial autoencoders (AAE)is used
to reconstruct the frequency- domain representation of acceleration responses.
The mass and stiffness of the tyre suspension system of a two-axle vehicle are
optimised by minimising the Wasserstein distance between damage index
distributions for healthy and damaged bridge states. A Kriging meta-model is
employed to approximate this objective function efficiently and identify
optimal vehicle configurations in both dimensional and non-dimensional
parameter spaces. Results show that vehicles with frequency ratios between 0.3
and 0.7 relative to the bridges' first natural frequency are most effective,
while those near resonance perform poorly. Lighter vehicles require lower
natural frequencies for optimal detection. This is the first study to
rigorously optimise the sensing platform for drive-by sensing and to propose a
purpose-built inspection vehicle.

</details>


### [128] [TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models](https://arxiv.org/abs/2510.02663)
*Rakshith S Srinivasa,Zora Che,Chen Bo Calvin Zhang,Diego Mares,Ernesto Hernandez,Jayeon Park,Dean Lee,Guillermo Mangialardi,Charmaine Ng,Ed-Yeremai Hernandez Cardona,Anisha Gunjal,Yunzhong He,Bing Liu,Chen Xing*

Main category: cs.LG

TL;DR: TutorBench is a dataset and benchmark for evaluating LLMs' tutoring skills, showing current models perform poorly (≤56%) and need significant improvement in adaptive explanations, feedback, and hint generation.


<details>
  <summary>Details</summary>
Motivation: As students increasingly use LLMs as learning aids, there's a need to evaluate and improve models' core tutoring abilities like identifying student needs, being adaptive, providing personalized guidance, and accuracy.

Method: Created TutorBench with 1,490 expert-curated samples from high-school/AP curricula across three tutoring tasks: adaptive explanations, actionable feedback, and hint generation. Uses LLM-judge with sample-specific rubrics for fine-grained automatic evaluation.

Result: Evaluated 16 frontier LLMs - none scored above 56%, all models below 60% pass rate on key tutoring skills. Claude models excel in active learning support but lag in other areas.

Conclusion: Current LLMs fall short in comprehensive tutoring capabilities. TutorBench provides an unsaturated benchmark to guide development of next-generation AI tutors with improved adaptive teaching skills.

Abstract: As students increasingly adopt large language models (LLMs) as learning aids,
it is crucial to build models that are adept at handling the nuances of
tutoring: they need to identify the core needs of students, be adaptive,
provide personalized guidance, and be accurate. To this end, we introduce
TutorBench, a dataset and evaluation benchmark designed to rigorously evaluate
the core tutoring skills of LLMs. The dataset comprises 1,490 samples curated
by human experts, focused on high-school and AP-level curricula. The samples
are drawn from three common tutoring tasks: (i) generating adaptive
explanations tailored to a student's confusion, (ii) providing actionable
feedback on a student's work, and (iii) promoting active learning through
effective hint generation. To account for the inherent complexity of tutoring,
samples are accompanied by sample-specific rubrics which are used to judge
model responses during evaluation. TutorBench uses a reliable and fine-grained
automatic evaluation method that uses an LLM-judge and the sample-specific
rubrics. We evaluate 16 frontier LLMs on TutorBench and present a detailed
analysis of their performance and behavior. Our results show that none of the
frontier LLMs achieve a score of greater than $56\%$, showing a large room for
improvement. We find that LLMs fall short in exhibiting the full range of
tutoring skills needed to guide, diagnose, and support students effectively,
with all the frontier models achieving less than a $60\%$ pass rate on rubric
criteria related to these skills. We also find that different model families
exhibit varied strengths and limitations: the Claude models outperform others
in supporting active learning, while they lag behind in the other two use
cases. By releasing TutorBench, we provide a comprehensive and unsaturated
benchmark to guide the development of the next-generation of AI tutors.

</details>


### [129] [Topological Invariance and Breakdown in Learning](https://arxiv.org/abs/2510.02670)
*Yongyi Yang,Tomaso Poggio,Isaac Chuang,Liu Ziyin*

Main category: cs.LG

TL;DR: Permutation-equivariant learning rules induce bi-Lipschitz mappings that constrain neuron topology. Small learning rates preserve topology, while large rates allow topological simplification, reducing model expressivity.


<details>
  <summary>Details</summary>
Motivation: To understand how learning dynamics affect the topological structure of neuron distributions and reveal qualitative differences between small and large learning rates.

Method: Analysis of permutation-equivariant learning rules (SGD, Adam) showing they create bi-Lipschitz mappings between neurons, constraining topology during training.

Result: Learning rate below critical point η* preserves all topological structure; above η* allows topological simplification, making neuron manifold coarser and reducing expressivity.

Conclusion: Learning dynamics have two phases: smooth optimization under topological constraints, then learning through topological simplifications. Theory is architecture/loss-independent, enabling universal topological analysis of deep learning.

Abstract: We prove that for a broad class of permutation-equivariant learning rules
(including SGD, Adam, and others), the training process induces a bi-Lipschitz
mapping between neurons and strongly constrains the topology of the neuron
distribution during training. This result reveals a qualitative difference
between small and large learning rates $\eta$. With a learning rate below a
topological critical point $\eta^*$, the training is constrained to preserve
all topological structure of the neurons. In contrast, above $\eta^*$, the
learning process allows for topological simplification, making the neuron
manifold progressively coarser and thereby reducing the model's expressivity.
Viewed in combination with the recent discovery of the edge of stability
phenomenon, the learning dynamics of neuron networks under gradient descent can
be divided into two phases: first they undergo smooth optimization under
topological constraints, and then enter a second phase where they learn through
drastic topological simplifications. A key feature of our theory is that it is
independent of specific architectures or loss functions, enabling the universal
application of topological methods to the study of deep learning.

</details>


### [130] [To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration](https://arxiv.org/abs/2510.02676)
*Zeyu Yang,Tianyi Zhang,Jianwen Xie,Chuan Li,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: The paper presents ECF8, a lossless FP8 compression framework for GenAI models that leverages exponent concentration phenomenon to achieve up to 26.9% memory savings and 177.1% throughput acceleration without model output deviation.


<details>
  <summary>Details</summary>
Motivation: The scaling of GenAI models to hundreds of billions of parameters makes low-precision computation essential for efficient deployment, but existing approaches face numerical stability and dequantization overhead challenges.

Method: Theoretical analysis of exponent concentration phenomenon in GenAI weights, design of FP8 format based on compression limits, and development of ECF8 framework with entropy-aware encoding and GPU-optimized decoding.

Result: Experiments on LLMs and DiTs up to 671B parameters show up to 26.9% memory savings and 177.1% throughput acceleration with perfectly lossless computations (no deviation in model outputs).

Conclusion: Exponent concentration is established as a statistical law of trained models, providing a principled path for lossless low-precision floating-point design in the FP8 era.

Abstract: The scaling of Generative AI (GenAI) models into the hundreds of billions of
parameters makes low-precision computation indispensable for efficient
deployment. We argue that the fundamental solution lies in developing
low-precision floating-point formats, which inherently provide numerical
stability, memory savings, and hardware efficiency without dequantization
overhead. In this paper, we present a theoretical and empirical study of an
exponent concentration phenomenon in GenAI weights: exponents consistently
exhibit low entropy across architectures and modalities. We show that this
arises naturally from $\alpha$-stable distributions induced by stochastic
gradient descent, and we prove tight bounds on the entropy of exponents. Our
analysis establishes a theoretical compression limit near FP4.67, which
motivates the design of a practical FP8 format. Building on these insights, we
propose Exponent-Concentrated FP8 (ECF8), a lossless compression framework with
entropy-aware encoding and GPU-optimized decoding. Experiments on LLMs and DiTs
up to 671B parameters demonstrate up to 26.9% memory savings and 177.1%
throughput acceleration, with perfectly lossless computations, i.e., no
deviation in model outputs. Our results establish exponent concentration as a
statistical law of trained models and open a principled path for lossless
low-precision floating-point design in the FP8 era.

</details>


### [131] [Can Data-Driven Dynamics Reveal Hidden Physics? There Is A Need for Interpretable Neural Operators](https://arxiv.org/abs/2510.02683)
*Wenhan Gao,Jian Luo,Fang Wan,Ruichen Xu,Xiang Liu,Haipeng Xing,Yi Liu*

Main category: cs.LG

TL;DR: This paper classifies neural operators into spatial domain and functional domain models, analyzes their learning mechanisms, and proposes a dual-space multi-scale approach for improved performance in learning physics-driven dynamics.


<details>
  <summary>Details</summary>
Motivation: To better understand the learning mechanisms of neural operators and develop more effective methods for learning data-driven dynamics that adhere to physical principles.

Method: Classifies neural operators into spatial domain (grid-based) and functional domain (function basis-based) models, proposes explanation methods for prediction processes, and introduces a dual-space multi-scale model approach.

Result: Shows that neural operators can learn hidden physical patterns from data, and demonstrates that a simple dual-space multi-scale model can achieve state-of-the-art performance.

Conclusion: Dual-space multi-spatio-scale models have significant potential for learning complex physics, and there is an urgent need for generalizable explanation methods and principled frameworks to incorporate known physics into neural operators for better generalization.

Abstract: Recently, neural operators have emerged as powerful tools for learning
mappings between function spaces, enabling data-driven simulations of complex
dynamics. Despite their successes, a deeper understanding of their learning
mechanisms remains underexplored. In this work, we classify neural operators
into two types: (1) Spatial domain models that learn on grids and (2)
Functional domain models that learn with function bases. We present several
viewpoints based on this classification and focus on learning data-driven
dynamics adhering to physical principles. Specifically, we provide a way to
explain the prediction-making process of neural operators and show that neural
operator can learn hidden physical patterns from data. However, this
explanation method is limited to specific situations, highlighting the urgent
need for generalizable explanation methods. Next, we show that a simple
dual-space multi-scale model can achieve SOTA performance and we believe that
dual-space multi-spatio-scale models hold significant potential to learn
complex physics and require further investigation. Lastly, we discuss the
critical need for principled frameworks to incorporate known physics into
neural operators, enabling better generalization and uncovering more hidden
physical phenomena.

</details>


### [132] [EvoSpeak: Large Language Models for Interpretable Genetic Programming-Evolved Heuristics](https://arxiv.org/abs/2510.02686)
*Meng Xu,Jiao Liu,Yew Soon Ong*

Main category: cs.LG

TL;DR: EvoSpeak integrates genetic programming with LLMs to enhance heuristic evolution by improving efficiency, interpretability, and transferability across optimization tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of complex GP heuristics in dynamic scenarios: poor interpretability, slow convergence, and limited transferability.

Method: Combine GP with LLMs to extract knowledge from GP heuristics, generate warm-start populations, translate trees to natural language, and enable cross-task knowledge transfer.

Result: In DFJSS experiments, EvoSpeak produces more effective heuristics, improves evolutionary efficiency, and generates human-readable explanations.

Conclusion: EvoSpeak advances intelligent heuristic development by coupling GP's symbolic reasoning with LLMs' interpretative capabilities for real-world optimization.

Abstract: Genetic programming (GP) has demonstrated strong effectiveness in evolving
tree-structured heuristics for complex optimization problems. Yet, in dynamic
and large-scale scenarios, the most effective heuristics are often highly
complex, hindering interpretability, slowing convergence, and limiting
transferability across tasks. To address these challenges, we present EvoSpeak,
a novel framework that integrates GP with large language models (LLMs) to
enhance the efficiency, transparency, and adaptability of heuristic evolution.
EvoSpeak learns from high-quality GP heuristics, extracts knowledge, and
leverages this knowledge to (i) generate warm-start populations that accelerate
convergence, (ii) translate opaque GP trees into concise natural-language
explanations that foster interpretability and trust, and (iii) enable knowledge
transfer and preference-aware heuristic generation across related tasks. We
verify the effectiveness of EvoSpeak through extensive experiments on dynamic
flexible job shop scheduling (DFJSS), under both single- and multi-objective
formulations. The results demonstrate that EvoSpeak produces more effective
heuristics, improves evolutionary efficiency, and delivers human-readable
reports that enhance usability. By coupling the symbolic reasoning power of GP
with the interpretative and generative strengths of LLMs, EvoSpeak advances the
development of intelligent, transparent, and user-aligned heuristics for
real-world optimization problems.

</details>


### [133] [Fine-Tuning Diffusion Models via Intermediate Distribution Shaping](https://arxiv.org/abs/2510.02692)
*Gautham Govind Anil,Shaan Ul Haque,Nithish Kannen,Dheeraj Nagaraj,Sanjay Shakkottai,Karthikeyan Shanmugam*

Main category: cs.LG

TL;DR: The paper introduces GRAFT, a unified framework for fine-tuning diffusion models that implicitly performs PPO with reshaped rewards, and proposes P-GRAFT for intermediate noise level distribution shaping and inverse noise correction for flow models.


<details>
  <summary>Details</summary>
Motivation: To enable effective fine-tuning of diffusion models using reward functions for downstream applications, addressing the intractability of marginal likelihoods in policy gradient methods.

Method: Unified RAFT variants as GRAFT framework, introduced P-GRAFT for intermediate noise level shaping, and proposed inverse noise correction for flow models without explicit rewards.

Result: Applied to Stable Diffusion 2, improved over policy gradient methods on T2I benchmarks (8.81% relative improvement over base model) and enhanced FID for unconditional image generation at lower FLOPs/image.

Conclusion: The proposed methods provide effective fine-tuning approaches for diffusion models with mathematical justification via bias-variance tradeoff, demonstrating improvements across multiple generation tasks.

Abstract: Diffusion models are widely used for generative tasks across domains. While
pre-trained diffusion models effectively capture the training data
distribution, it is often desirable to shape these distributions using reward
functions to align with downstream applications. Policy gradient methods, such
as Proximal Policy Optimization (PPO), are widely used in the context of
autoregressive generation. However, the marginal likelihoods required for such
methods are intractable for diffusion models, leading to alternative proposals
and relaxations. In this context, we unify variants of Rejection sAmpling based
Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with
reshaped rewards. We then introduce P-GRAFT to shape distributions at
intermediate noise levels and demonstrate empirically that this can lead to
more effective fine-tuning. We mathematically explain this via a bias-variance
tradeoff. Motivated by this, we propose inverse noise correction to improve
flow models without leveraging explicit rewards. We empirically evaluate our
methods on text-to-image(T2I) generation, layout generation, molecule
generation and unconditional image generation. Notably, our framework, applied
to Stable Diffusion 2, improves over policy gradient methods on popular T2I
benchmarks in terms of VQAScore and shows an $8.81\%$ relative improvement over
the base model. For unconditional image generation, inverse noise correction
improves FID of generated images at lower FLOPs/image.

</details>


### [134] [RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization](https://arxiv.org/abs/2510.02695)
*Kai Fukazawa,Kunal Mundada,Iman Soltani*

Main category: cs.LG

TL;DR: RAMAC is a risk-aware offline RL framework combining expressive generative actors with distributional critics to handle multimodal scenarios while maintaining safety through CVaR optimization.


<details>
  <summary>Details</summary>
Motivation: Address the gap in offline RL where risk-averse methods are too conservative and restricted, while expressive policies are only used in risk-neutral settings, particularly in safety-critical domains where online data collection is infeasible.

Method: Introduces Risk-Aware Multimodal Actor-Critic (RAMAC) framework that couples expressive generative actors (diffusion and flow-matching) with distributional critics, differentiating composite objective combining distributional risk and behavior cloning loss through generative path.

Result: Consistent gains in CVaR_0.1 while maintaining strong returns on most Stochastic-D4RL tasks, demonstrating improved risk-sensitive learning in complex multimodal scenarios.

Conclusion: RAMAC successfully bridges the gap between risk-averse safety and expressive policy learning in offline RL, enabling safe yet high-performing policies in safety-critical domains.

Abstract: In safety-critical domains where online data collection is infeasible,
offline reinforcement learning (RL) offers an attractive alternative but only
if policies deliver high returns without incurring catastrophic lower-tail
risk. Prior work on risk-averse offline RL achieves safety at the cost of value
conservatism and restricted policy classes, whereas expressive policies are
only used in risk-neutral settings. Here, we address this gap by introducing
the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which
couples an \emph{expressive generative actor} with a distributional critic. The
RAMAC differentiates composite objective combining distributional risk and BC
loss through the generative path, achieving risk-sensitive learning in complex
multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching
actors and observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining
strong returns on most Stochastic-D4RL tasks. Code:
https://github.com/KaiFukazawa/RAMAC.git

</details>


### [135] [A Novel Unified Lightweight Temporal-Spatial Transformer Approach for Intrusion Detection in Drone Networks](https://arxiv.org/abs/2510.02711)
*Tarun Kumar Biswas,Ashrafun Zannat,Waqas Ishtiaq,Md. Alamgir Hossain*

Main category: cs.LG

TL;DR: TSLT-Net is a lightweight intrusion detection system for drone networks that uses temporal-spatial transformers to achieve high accuracy (99.99% multiclass, 100% binary) with minimal resource usage.


<details>
  <summary>Details</summary>
Motivation: Drones face cybersecurity challenges due to susceptibility to cyberattacks, and existing intrusion detection systems lack adaptability, efficiency, and generalizability for dynamic, resource-constrained drone environments.

Method: Proposes TSLT-Net using self-attention mechanisms to model temporal patterns and spatial dependencies in network traffic, with streamlined preprocessing and unified architecture for both multiclass classification and binary anomaly detection.

Result: Achieved 99.99% accuracy in multiclass detection and 100% in binary anomaly detection on ISOT Drone Anomaly Detection Dataset (2.3M+ records), with only 0.04MB memory footprint and 9722 trainable parameters.

Conclusion: TSLT-Net is an effective, scalable solution for real-time drone cybersecurity, suitable for deployment on edge devices in mission-critical UAV systems.

Abstract: The growing integration of drones across commercial, industrial, and civilian
domains has introduced significant cybersecurity challenges, particularly due
to the susceptibility of drone networks to a wide range of cyberattacks.
Existing intrusion detection mechanisms often lack the adaptability,
efficiency, and generalizability required for the dynamic and resource
constrained environments in which drones operate. This paper proposes TSLT-Net,
a novel lightweight and unified Temporal Spatial Transformer based intrusion
detection system tailored specifically for drone networks. By leveraging self
attention mechanisms, TSLT-Net effectively models both temporal patterns and
spatial dependencies in network traffic, enabling accurate detection of diverse
intrusion types. The framework includes a streamlined preprocessing pipeline
and supports both multiclass attack classification and binary anomaly detection
within a single architecture. Extensive experiments conducted on the ISOT Drone
Anomaly Detection Dataset, consisting of more than 2.3 million labeled records,
demonstrate the superior performance of TSLT-Net with 99.99 percent accuracy in
multiclass detection and 100 percent in binary anomaly detection, while
maintaining a minimal memory footprint of only 0.04 MB and 9722 trainable
parameters. These results establish TSLT-Net as an effective and scalable
solution for real time drone cybersecurity, particularly suitable for
deployment on edge devices in mission critical UAV systems.

</details>


### [136] [CST-AFNet: A dual attention-based deep learning framework for intrusion detection in IoT networks](https://arxiv.org/abs/2510.02717)
*Waqas Ishtiaq,Ashrafun Zannat,A. H. M. Shahariar Parvez,Md. Alamgir Hossain,Muntasir Hasan Kanchan,Muhammad Masud Tarek*

Main category: cs.LG

TL;DR: CST AFNet is a dual attention-based deep learning framework for IoT intrusion detection, achieving 99.97% accuracy on Edge IIoTset dataset with 15 attack types.


<details>
  <summary>Details</summary>
Motivation: Address cybersecurity challenges in IoT environments due to their heterogeneous, resource-constrained, and distributed nature.

Method: Integrates multi-scale CNNs for spatial features, BiGRUs for temporal dependencies, and dual attention mechanism (channel and temporal) to focus on critical patterns.

Result: Achieves 99.97% accuracy, with macro averaged precision, recall, and F1 score all above 99.3%, outperforming traditional deep learning models.

Conclusion: CST AFNet is a powerful and scalable solution for real-time cyber threat detection in complex IoT/IIoT environments, enabling more secure cyber-physical systems.

Abstract: The rapid expansion of the Internet of Things (IoT) has revolutionized modern
industries by enabling smart automation and real time connectivity. However,
this evolution has also introduced complex cybersecurity challenges due to the
heterogeneous, resource constrained, and distributed nature of these
environments. To address these challenges, this research presents CST AFNet, a
novel dual attention based deep learning framework specifically designed for
robust intrusion detection in IoT networks. The model integrates multi scale
Convolutional Neural Networks (CNNs) for spatial feature extraction,
Bidirectional Gated Recurrent Units (BiGRUs) for capturing temporal
dependencies, and a dual attention mechanism, channel and temporal attention,
to enhance focus on critical patterns in the data. The proposed method was
trained and evaluated on the Edge IIoTset dataset, a comprehensive and
realistic benchmark containing more than 2.2 million labeled instances spanning
15 attack types and benign traffic, collected from a seven layer industrial
testbed. Our proposed model achieves outstanding accuracy for both 15 attack
types and benign traffic. CST AFNet achieves 99.97 percent accuracy. Moreover,
this model demonstrates exceptional performance with macro averaged precision,
recall, and F1 score all above 99.3 percent. Experimental results show that CST
AFNet achieves superior detection accuracy, significantly outperforming
traditional deep learning models. The findings confirm that CST AFNet is a
powerful and scalable solution for real time cyber threat detection in complex
IoT and IIoT environments, paving the way for more secure, intelligent, and
adaptive cyber physical systems.

</details>


### [137] [Hyperparameter Loss Surfaces Are Simple Near their Optima](https://arxiv.org/abs/2510.02721)
*Nicholas Lourie,He He,Kyunghyun Cho*

Main category: cs.LG

TL;DR: The paper discovers novel structure in hyperparameter loss surfaces and develops tools to understand them, revealing that simple structure emerges near optima characterized by features like effective dimension and best possible loss.


<details>
  <summary>Details</summary>
Motivation: Hyperparameters greatly impact model capabilities but modern models are too large for extensive search, and few tools exist for understanding hyperparameter loss surfaces despite their importance.

Method: Developed a novel technique based on random search to uncover the asymptotic regime where loss surfaces become simple, discovering a new distribution for best scores from random search whose parameters define the loss surface features.

Result: Found that in the asymptotic regime, loss surfaces are characterized by basic features like effective dimension and best possible loss, and derived a new asymptotic law for random search that explains and extrapolates its convergence.

Conclusion: These new tools enable novel analyses including confidence intervals for best possible performance and determining effective number of hyperparameters, with tools made available via GitHub repository.

Abstract: Hyperparameters greatly impact models' capabilities; however, modern models
are too large for extensive search. Instead, researchers design recipes that
train well across scales based on their understanding of the hyperparameters.
Despite this importance, few tools exist for understanding the hyperparameter
loss surface. We discover novel structure in it and propose a new theory
yielding such tools. The loss surface is complex, but as you approach the
optimum simple structure emerges. It becomes characterized by a few basic
features, like its effective dimension and the best possible loss. To uncover
this asymptotic regime, we develop a novel technique based on random search.
Within this regime, the best scores from random search take on a new
distribution we discover. Its parameters are exactly the features defining the
loss surface in the asymptotic regime. From these features, we derive a new
asymptotic law for random search that can explain and extrapolate its
convergence. These new tools enable new analyses, such as confidence intervals
for the best possible performance or determining the effective number of
hyperparameters. We make these tools available at
https://github.com/nicholaslourie/opda .

</details>


### [138] [Accuracy Law for the Future of Deep Time Series Forecasting](https://arxiv.org/abs/2510.02729)
*Yuxuan Wang,Haixu Wu,Yuezhou Ma,Yuchen Fang,Ziyi Zhang,Yong Liu,Shiyu Wang,Zhou Ye,Yang Xiang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: This paper proposes an accuracy law that estimates the performance upper bound of deep time series forecasting by discovering an exponential relationship between minimum forecasting error and window-wise series pattern complexity.


<details>
  <summary>Details</summary>
Motivation: To address confusion in deep time series forecasting research direction due to minor benchmark improvements, and to establish a fundamental understanding of forecasting performance limits given the inherent uncertainty and partial observability of time series data.

Method: Conducted rigorous statistical tests on over 2,800 trained deep forecasters, analyzed window-wise series properties (beyond classical series-wise metrics), and discovered an exponential relationship between minimum error and pattern complexity.

Result: Found a significant exponential relationship (accuracy law) between minimum forecasting error and window-wise series pattern complexity, which helps identify saturated tasks and derive effective training strategies for large time series models.

Conclusion: The proposed accuracy law provides valuable insights for future research by establishing performance upper bounds, identifying saturated benchmarks, and guiding training strategies for large-scale time series forecasting models.

Abstract: Deep time series forecasting has emerged as a booming direction in recent
years. Despite the exponential growth of community interests, researchers are
sometimes confused about the direction of their efforts due to minor
improvements on standard benchmarks. In this paper, we notice that, unlike
image recognition, whose well-acknowledged and realizable goal is 100%
accuracy, time series forecasting inherently faces a non-zero error lower bound
due to its partially observable and uncertain nature. To pinpoint the research
objective and release researchers from saturated tasks, this paper focuses on a
fundamental question: how to estimate the performance upper bound of deep time
series forecasting? Going beyond classical series-wise predictability metrics,
e.g., ADF test, we realize that the forecasting performance is highly related
to window-wise properties because of the sequence-to-sequence forecasting
paradigm of deep time series models. Based on rigorous statistical tests of
over 2,800 newly trained deep forecasters, we discover a significant
exponential relationship between the minimum forecasting error of deep models
and the complexity of window-wise series patterns, which is termed the accuracy
law. The proposed accuracy law successfully guides us to identify saturated
tasks from widely used benchmarks and derives an effective training strategy
for large time series models, offering valuable insights for future research.

</details>


### [139] [Dale meets Langevin: A Multiplicative Denoising Diffusion Model](https://arxiv.org/abs/2510.02730)
*Nishanth Shetty,Madhava Prasath,Chandra Sekhar Seelamantula*

Main category: cs.LG

TL;DR: This paper presents a biologically inspired generative model using multiplicative updates based on geometric Brownian motion, connecting exponential gradient descent from Dale's law with score-based generative modeling for positive data.


<details>
  <summary>Details</summary>
Motivation: Standard gradient descent optimization is inconsistent with biological learning systems. The work is motivated by Dale's law in neuroscience and aims to develop biologically plausible learning techniques.

Method: The method starts with SDE governing geometric Brownian motion, discretizes reverse-time SDE to get multiplicative update rules, and proposes multiplicative denoising score-matching formalism for log-normally distributed positive data.

Result: Experimental results on MNIST, Fashion MNIST, and Kuzushiji datasets demonstrate the generative capability of the new multiplicative update scheme.

Conclusion: This is the first biologically inspired generative model using multiplicative updates based on geometric Brownian motion, connecting neuroscience principles with machine learning optimization.

Abstract: Gradient descent has proven to be a powerful and effective technique for
optimization in numerous machine learning applications. Recent advances in
computational neuroscience have shown that learning in standard gradient
descent optimization formulation is not consistent with learning in biological
systems. This has opened up interesting avenues for building biologically
inspired learning techniques. One such approach is inspired by Dale's law,
which states that inhibitory and excitatory synapses do not swap roles during
the course of learning. The resulting exponential gradient descent optimization
scheme leads to log-normally distributed synaptic weights. Interestingly, the
density that satisfies the Fokker-Planck equation corresponding to the
stochastic differential equation (SDE) with geometric Brownian motion (GBM) is
the log-normal density. Leveraging this connection, we start with the SDE
governing geometric Brownian motion, and show that discretizing the
corresponding reverse-time SDE yields a multiplicative update rule, which
surprisingly, coincides with the sampling equivalent of the exponential
gradient descent update founded on Dale's law. Furthermore, we propose a new
formalism for multiplicative denoising score-matching, subsuming the loss
function proposed by Hyvaerinen for non-negative data. Indeed, log-normally
distributed data is positive and the proposed score-matching formalism turns
out to be a natural fit. This allows for training of score-based models for
image data and results in a novel multiplicative update scheme for sample
generation starting from a log-normal density. Experimental results on MNIST,
Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the
new scheme. To the best of our knowledge, this is the first instance of a
biologically inspired generative model employing multiplicative updates,
founded on geometric Brownian motion.

</details>


### [140] [Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering](https://arxiv.org/abs/2510.02731)
*Tianxiang Zhao,Youqing Wang,Jinlu Wang,Jiapu Wang,Mingliang Cui,Junbin Gao,Jipeng Guo*

Main category: cs.LG

TL;DR: Proposes RAGC method with hybrid-collaborative augmentation and contrastive sample adaptive-differential awareness for robust attributed graph clustering, addressing limitations in existing CAGC methods.


<details>
  <summary>Details</summary>
Motivation: Existing CAGC methods overlook edge-level embedding augmentation and interactions between node-level and edge-level augmentations, and treat all contrastive sample pairs equally without distinguishing hard vs easy pairs, limiting discriminative capability.

Method: Uses hybrid-collaborative augmentation (HCA) for simultaneous node-level and edge-level embedding representations and augmentations, and contrastive sample adaptive-differential awareness (CSADA) strategy that adaptively identifies contrastive sample pairs using pseudo-label information and differentially treats them with weight modulation.

Result: Comprehensive evaluations on six benchmark datasets demonstrate RAGC's effectiveness against state-of-the-art CAGC methods.

Conclusion: The proposed RAGC method with HCA and CSADA modules mutually reinforce each other to enhance discriminability in representation learning for attributed graph clustering.

Abstract: Due to its powerful capability of self-supervised representation learning and
clustering, contrastive attributed graph clustering (CAGC) has achieved great
success, which mainly depends on effective data augmentation and contrastive
objective setting. However, most CAGC methods utilize edges as auxiliary
information to obtain node-level embedding representation and only focus on
node-level embedding augmentation. This approach overlooks edge-level embedding
augmentation and the interactions between node-level and edge-level embedding
augmentations across various granularity. Moreover, they often treat all
contrastive sample pairs equally, neglecting the significant differences
between hard and easy positive-negative sample pairs, which ultimately limits
their discriminative capability. To tackle these issues, a novel robust
attributed graph clustering (RAGC), incorporating hybrid-collaborative
augmentation (HCA) and contrastive sample adaptive-differential awareness
(CSADA), is proposed. First, node-level and edge-level embedding
representations and augmentations are simultaneously executed to establish a
more comprehensive similarity measurement criterion for subsequent contrastive
learning. In turn, the discriminative similarity further consciously guides
edge augmentation. Second, by leveraging pseudo-label information with high
confidence, a CSADA strategy is elaborately designed, which adaptively
identifies all contrastive sample pairs and differentially treats them by an
innovative weight modulation function. The HCA and CSADA modules mutually
reinforce each other in a beneficent cycle, thereby enhancing discriminability
in representation learning. Comprehensive graph clustering evaluations over six
benchmark datasets demonstrate the effectiveness of the proposed RAGC against
several state-of-the-art CAGC methods.

</details>


### [141] [TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via Preemptive Scheduling](https://arxiv.org/abs/2510.02758)
*Junyi Chen,Chuheng Du,Renyuan Liu,Shuochao Yao,Dingtian Yan,Jiang Liao,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: TokenFlow is a novel LLM serving system that improves real-time text streaming performance through preemptive request scheduling and proactive KV cache management, achieving higher effective throughput and lower latency.


<details>
  <summary>Details</summary>
Motivation: Standard LLM serving systems suffer from poor resource utilization and low parallelism under request bursts due to inflexible non-preemptive scheduling and reactive memory management, leading to suboptimal streaming performance.

Method: TokenFlow implements preemptive request scheduling based on real-time token buffer occupancy and consumption rate, along with proactive KV cache transfer between GPU and CPU memory with I/O-computation overlap to minimize preemption overhead.

Result: Experiments on Llama3-8B and Qwen2.5-32B across multiple GPUs show TokenFlow achieves up to 82.5% higher effective throughput while reducing P99 TTFT by up to 80.2%, without degrading overall token throughput.

Conclusion: TokenFlow significantly enhances LLM streaming performance by addressing scheduling and memory management limitations of existing systems, providing better responsiveness and steady generation for real-time interactions.

Abstract: Real-time LLM interactions demand streamed token generations, where text
tokens are progressively generated and delivered to users while balancing two
objectives: responsiveness (i.e., low time-to-first-token) and steady
generation (i.e.,required time-between-tokens). Standard LLM serving systems
suffer from the inflexibility caused by non-preemptive request scheduling and
reactive memory management, leading to poor resource utilization and low
request processing parallelism under request bursts. Therefore, we present
TokenFlow, a novel LLM serving system with enhanced text streaming performance
via preemptive request scheduling and proactive key-value (KV) cache
management. TokenFlow dynamically prioritizes requests based on real-time token
buffer occupancy and token consumption rate, while actively transferring KV
cache between GPU and CPU memory in the background and overlapping I/O with
computation to minimize request preemption overhead. Extensive experiments on
Llama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)
demonstrate that TokenFlow achieves up to 82.5% higher effective throughput
(accounting for actual user consumption) while reducing P99 TTFT by up to
80.2%, without degrading overall token throughput.

</details>


### [142] [Fusing Multi- and Hyperspectral Satellite Data for Harmful Algal Bloom Monitoring with Self-Supervised and Hierarchical Deep Learning](https://arxiv.org/abs/2510.02763)
*Nicholas LaHaye,Kelly M. Luis,Michelle M. Gierach*

Main category: cs.LG

TL;DR: SIT-FUSE is a self-supervised ML framework that detects and maps harmful algal bloom severity and speciation using multi-sensor satellite data without requiring labeled datasets.


<details>
  <summary>Details</summary>
Motivation: To develop scalable HAB monitoring in label-scarce environments and advance operational self-supervised learning for global aquatic biogeochemistry.

Method: Fuses reflectance data from VIIRS, MODIS, Sentinel-3, PACE with TROPOMI solar-induced fluorescence, using self-supervised representation learning and hierarchical deep clustering to segment phytoplankton concentrations.

Result: Strong agreement with in-situ measurements of total phytoplankton, Karenia brevis, Alexandrium spp., and Pseudo-nitzschia spp. in Gulf of Mexico and Southern California (2018-2025).

Conclusion: The framework enables exploratory analysis via hierarchical embeddings and represents a critical step toward operationalizing self-supervised learning for global aquatic biogeochemistry monitoring.

Abstract: We present a self-supervised machine learning framework for detecting and
mapping harmful algal bloom (HAB) severity and speciation using multi-sensor
satellite data. By fusing reflectance data from operational instruments (VIIRS,
MODIS, Sentinel-3, PACE) with TROPOMI solar-induced fluorescence (SIF), our
framework, called SIT-FUSE, generates HAB severity and speciation products
without requiring per-instrument labeled datasets. The framework employs
self-supervised representation learning, hierarchical deep clustering to
segment phytoplankton concentrations and speciations into interpretable
classes, validated against in-situ data from the Gulf of Mexico and Southern
California (2018-2025). Results show strong agreement with total phytoplankton,
Karenia brevis, Alexandrium spp., and Pseudo-nitzschia spp. measurements. This
work advances scalable HAB monitoring in label-scarce environments while
enabling exploratory analysis via hierarchical embeddings: a critical step
toward operationalizing self-supervised learning for global aquatic
biogeochemistry.

</details>


### [143] [DMark: Order-Agnostic Watermarking for Diffusion Large Language Models](https://arxiv.org/abs/2510.02902)
*Linyu Wu,Linhao Zhong,Wenjie Qu,Yuexin Li,Yue Liu,Shengfang Zhai,Chunhua Shen,Jiaheng Zhang*

Main category: cs.LG

TL;DR: DMark is the first watermarking framework for diffusion large language models (dLLMs) that addresses their non-sequential decoding through predictive, bidirectional, and combined watermarking strategies, achieving high detection rates while maintaining text quality.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods fail on dLLMs due to their non-sequential decoding nature, which breaks the causal design underlying traditional watermarks designed for autoregressive models.

Method: DMark introduces three strategies: predictive watermarking (using model-predicted tokens), bidirectional watermarking (exploiting forward and backward dependencies), and predictive-bidirectional watermarking (combining both approaches).

Result: Experiments show DMark achieves 92.0-99.5% detection rates at 1% false positive rate while maintaining text quality, compared to only 49.6-71.2% for naive adaptations of existing methods. It also demonstrates robustness against text manipulations.

Conclusion: Effective watermarking is feasible for non-autoregressive language models, establishing that dLLMs can be watermarked despite their non-sequential decoding characteristics.

Abstract: Diffusion large language models (dLLMs) offer faster generation than
autoregressive models while maintaining comparable quality, but existing
watermarking methods fail on them due to their non-sequential decoding. Unlike
autoregressive models that generate tokens left-to-right, dLLMs can finalize
tokens in arbitrary order, breaking the causal design underlying traditional
watermarks. We present DMark, the first watermarking framework designed
specifically for dLLMs. DMark introduces three complementary strategies to
restore watermark detectability: predictive watermarking uses model-predicted
tokens when actual context is unavailable; bidirectional watermarking exploits
both forward and backward dependencies unique to diffusion decoding; and
predictive-bidirectional watermarking combines both approaches to maximize
detection strength. Experiments across multiple dLLMs show that DMark achieves
92.0-99.5% detection rates at 1% false positive rate while maintaining text
quality, compared to only 49.6-71.2% for naive adaptations of existing methods.
DMark also demonstrates robustness against text manipulations, establishing
that effective watermarking is feasible for non-autoregressive language models.

</details>


### [144] [Curl Descent: Non-Gradient Learning Dynamics with Sign-Diverse Plasticity](https://arxiv.org/abs/2510.02765)
*Hugo Ninou,Jonathan Kadmon,N. Alex Cayco-Gajic*

Main category: cs.LG

TL;DR: The paper investigates whether biological neural networks use non-gradient "curl" components in learning dynamics, showing these can optimize loss functions while being fundamentally different from gradient descent.


<details>
  <summary>Details</summary>
Motivation: To understand if biological neural networks employ gradient-based strategies or alternative non-gradient learning dynamics, given the diversity of synaptic plasticity rules observed in experiments.

Method: Analyzed feedforward networks using a student-teacher framework, systematically introducing non-gradient dynamics through neurons with rule-flipped plasticity, and studied the impact of curl terms on learning stability.

Result: Small curl terms preserve solution stability with gradient-descent-like learning; strong curl terms destabilize solutions, potentially causing chaotic dynamics or paradoxically speeding learning by escaping saddles through temporary loss ascent.

Conclusion: Specific neural architectures can support robust learning via diverse non-gradient rules, challenging normative gradient-based theories and providing alternatives for biological learning mechanisms.

Abstract: Gradient-based algorithms are a cornerstone of artificial neural network
training, yet it remains unclear whether biological neural networks use similar
gradient-based strategies during learning. Experiments often discover a
diversity of synaptic plasticity rules, but whether these amount to an
approximation to gradient descent is unclear. Here we investigate a previously
overlooked possibility: that learning dynamics may include fundamentally
non-gradient "curl"-like components while still being able to effectively
optimize a loss function. Curl terms naturally emerge in networks with
inhibitory-excitatory connectivity or Hebbian/anti-Hebbian plasticity,
resulting in learning dynamics that cannot be framed as gradient descent on any
objective. To investigate the impact of these curl terms, we analyze
feedforward networks within an analytically tractable student-teacher
framework, systematically introducing non-gradient dynamics through neurons
exhibiting rule-flipped plasticity. Small curl terms preserve the stability of
the original solution manifold, resulting in learning dynamics similar to
gradient descent. Beyond a critical value, strong curl terms destabilize the
solution manifold. Depending on the network architecture, this loss of
stability can lead to chaotic learning dynamics that destroy performance. In
other cases, the curl terms can counterintuitively speed learning compared to
gradient descent by allowing the weight dynamics to escape saddles by
temporarily ascending the loss. Our results identify specific architectures
capable of supporting robust learning via diverse learning rules, providing an
important counterpoint to normative theories of gradient-based learning in
neural networks.

</details>


### [145] [A Granular Study of Safety Pretraining under Model Abliteration](https://arxiv.org/abs/2510.02768)
*Shashank Agnihotri,Jonas Jakubassa,Priyam Dey,Sachin Goyal,Bernt Schiele,Venkatesh Babu Radhakrishnan,Margret Keuper*

Main category: cs.LG

TL;DR: This paper evaluates whether safety interventions like refusal training survive activation edits at inference time, using model abliteration to remove refusal-sensitive directions and testing across Safety Pretraining checkpoints.


<details>
  <summary>Details</summary>
Motivation: To determine if common safety interventions remain effective when models are modified with simple activation edits at inference time, addressing practical safety concerns for open-weight LLMs.

Method: Used model abliteration (lightweight projection technique) to remove refusal-sensitive directions, evaluated across 20 systems (original and abliterated) using 100 prompts with balanced harmful/harmless cases, classified responses using multiple judges, and validated judge fidelity with human-labeled data.

Result: The study provides checkpoint-level characterization of which data-centric safety components remain robust under abliteration, quantifies how judge selection affects evaluation outcomes, and develops a protocol for integrating inference-time edits into safety assessments.

Conclusion: The research outlines a practical evaluation protocol for assessing safety intervention robustness against inference-time edits and identifies which safety components maintain effectiveness under such modifications.

Abstract: Open-weight LLMs can be modified at inference time with simple activation
edits, which raises a practical question for safety: do common safety
interventions like refusal training or metatag training survive such edits? We
study model abliteration, a lightweight projection technique designed to remove
refusal-sensitive directions, and conduct a controlled evaluation across a
granular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside
widely used open baselines. For each of 20 systems, original and abliterated,
we issue 100 prompts with balanced harmful and harmless cases, classify
responses as **Refusal** or **Non-Refusal** using multiple judges, and validate
judge fidelity on a small human-labeled subset. We also probe whether models
can identify refusal in their own outputs. Our study produces a
checkpoint-level characterization of which data-centric safety components
remain robust under abliteration, quantifies how judge selection influences
evaluation outcomes, and outlines a practical protocol for integrating
inference-time edits into safety assessments. Code:
https://github.com/shashankskagnihotri/safety_pretraining.

</details>


### [146] [Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification](https://arxiv.org/abs/2510.02779)
*Yuanfan Li,Yunwen Lei,Zheng-Chu Guo,Yiming Ying*

Main category: cs.LG

TL;DR: This paper establishes optimal generalization rates for gradient descent with deep ReLU networks, achieving polynomial dependence on network depth rather than exponential dependence, and matching optimal SVM-type rates up to depth-dependent factors.


<details>
  <summary>Details</summary>
Motivation: Existing results either yield suboptimal generalization rates of O(1/√n) or focus on networks with smooth activation functions that incur exponential dependence on network depth. The paper aims to achieve minimax optimal rates for deep ReLU networks.

Method: The authors carefully trade off optimization and generalization errors, using novel control of activation patterns near a reference model to enable sharper Rademacher complexity bounds for deep ReLU networks trained with gradient descent.

Result: Under the assumption that data are NTK separable from margin γ, the paper proves an excess risk rate of Õ(L⁴(1 + γL²)/(nγ²)), which aligns with the optimal SVM-type rate Õ(1/(nγ²)) up to depth-dependent factors.

Conclusion: The work demonstrates that gradient descent can achieve generalization rates comparable to minimax optimal rates for deep ReLU networks, with only polynomial dependence on network depth rather than exponential dependence.

Abstract: Recent advances have significantly improved our understanding of the
generalization performance of gradient descent (GD) methods in deep neural
networks. A natural and fundamental question is whether GD can achieve
generalization rates comparable to the minimax optimal rates established in the
kernel setting. Existing results either yield suboptimal rates of
$O(1/\sqrt{n})$, or focus on networks with smooth activation functions,
incurring exponential dependence on network depth $L$. In this work, we
establish optimal generalization rates for GD with deep ReLU networks by
carefully trading off optimization and generalization errors, achieving only
polynomial dependence on depth. Specifically, under the assumption that the
data are NTK separable from the margin $\gamma$, we prove an excess risk rate
of $\widetilde{O}(L^4 (1 + \gamma L^2) / (n \gamma^2))$, which aligns with the
optimal SVM-type rate $\widetilde{O}(1 / (n \gamma^2))$ up to depth-dependent
factors. A key technical contribution is our novel control of activation
patterns near a reference model, enabling a sharper Rademacher complexity bound
for deep ReLU networks trained with gradient descent.

</details>


### [147] [OptunaHub: A Platform for Black-Box Optimization](https://arxiv.org/abs/2510.02798)
*Yoshihiko Ozaki,Shuhei Watanabe,Toshihiko Yanase*

Main category: cs.LG

TL;DR: OptunaHub is a community platform that centralizes black-box optimization methods and benchmarks with unified Python APIs, package registry, and web interface to foster cross-domain research.


<details>
  <summary>Details</summary>
Motivation: Research in black-box optimization (BBO) remains fragmented across domains like AutoML and Materials Informatics, hindering progress and collaboration.

Method: Developed OptunaHub platform with unified Python APIs, contributor package registry, and web interface to centralize BBO methods and benchmarks.

Result: Created a publicly available platform with source code in GitHub repositories under the Optuna organization, enabling searchability and cross-domain research.

Conclusion: OptunaHub aims to foster a virtuous cycle of contributions and applications in black-box optimization by providing a centralized community platform.

Abstract: Black-box optimization (BBO) drives advances in domains such as AutoML and
Materials Informatics, yet research efforts often remain fragmented across
domains. We introduce OptunaHub (https://hub.optuna.org/), a community platform
that centralizes BBO methods and benchmarks. OptunaHub provides unified Python
APIs, a contributor package registry, and a web interface to promote
searchability and cross-domain research. OptunaHub aims to foster a virtuous
cycle of contributions and applications. The source code is publicly available
in the optunahub, optunahub-registry, and optunahub-web repositories under the
Optuna organization on GitHub (https://github.com/optuna/).

</details>


### [148] [Relevance-Aware Thresholding in Online Conformal Prediction for Time Series](https://arxiv.org/abs/2510.02809)
*Théo Dupuy,Binbin Xu,Stéphane Perrey,Jacky Montmain,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: This paper proposes enhancing Online Conformal Prediction (OCP) for time series by replacing the binary evaluation of prediction intervals with relevance-quantifying functions, resulting in narrower intervals while maintaining coverage validity.


<details>
  <summary>Details</summary>
Motivation: Most existing OCP methods focus only on whether ground truth falls inside/outside prediction intervals during threshold updates, without considering interval relevance. This paper aims to leverage this overlooked aspect to prevent abrupt threshold changes and produce tighter intervals.

Method: The authors propose enhancing the threshold update step in OCP by replacing binary evaluation with a broader class of functions that quantify prediction interval relevance using ground truth information.

Result: Experimental results on real-world datasets show that the proposed functions can produce tighter prediction intervals compared to existing OCP methods while maintaining coverage validity.

Conclusion: Leveraging relevance-quantifying functions in OCP threshold updates can effectively produce narrower prediction intervals without compromising coverage guarantees, addressing a key limitation in current OCP approaches.

Abstract: Uncertainty quantification has received considerable interest in recent works
in Machine Learning. In particular, Conformal Prediction (CP) gains ground in
this field. For the case of time series, Online Conformal Prediction (OCP)
becomes an option to address the problem of data distribution shift over time.
Indeed, the idea of OCP is to update a threshold of some quantity (whether the
miscoverage level or the quantile) based on the distribution observation. To
evaluate the performance of OCP methods, two key aspects are typically
considered: the coverage validity and the prediction interval width
minimization. Recently, new OCP methods have emerged, offering long-run
coverage guarantees and producing more informative intervals. However, during
the threshold update step, most of these methods focus solely on the validity
of the prediction intervals~--~that is, whether the ground truth falls inside
or outside the interval~--~without accounting for their relevance. In this
paper, we aim to leverage this overlooked aspect. Specifically, we propose
enhancing the threshold update step by replacing the binary evaluation
(inside/outside) with a broader class of functions that quantify the relevance
of the prediction interval using the ground truth. This approach helps prevent
abrupt threshold changes, potentially resulting in narrower prediction
intervals. Indeed, experimental results on real-world datasets suggest that
these functions can produce tighter intervals compared to existing OCP methods
while maintaining coverage validity.

</details>


### [149] [Dissecting Transformers: A CLEAR Perspective towards Green AI](https://arxiv.org/abs/2510.02810)
*Hemang Jain,Shailender Goyal,Divyansh Pandey,Karthik Vaidhyanathan*

Main category: cs.LG

TL;DR: This paper presents CLEAR, a novel methodology for fine-grained component-level energy assessment of transformer models during inference, revealing that attention blocks consume disproportionately more energy per FLOP than other components.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of LLMs has raised environmental concerns, with inference dominating AI energy footprint. Current sustainability studies lack fine-grained measurement methods, treating energy efficiency as an afterthought rather than primary objective.

Method: Proposed Component-Level Energy Assessment via Repeated sampling (CLEAR) to overcome temporal mismatch between microsecond-scale component execution and millisecond-scale energy sensor monitoring. Evaluated 15 models spanning four distinct architecture types.

Result: CLEAR consistently kept component-wise energy variance below 9.5% while capturing over 90% of model's total energy as individual components. Attention blocks consume significantly more energy per FLOP, showing FLOPs alone fail to capture true energy cost at component level.

Conclusion: Established detailed component-level energy baselines and provided insights as initial step to build energy-efficient transformer models through component-level optimizations.

Abstract: The rapid adoption of Large Language Models (LLMs) has raised significant
environmental concerns. Unlike the one-time cost of training, LLM inference
occurs continuously at a global scale and now dominates the AI energy
footprint. Yet, most sustainability studies report only coarse, model-level
metrics due to the lack of fine-grained measurement methods, treating energy
efficiency more as an afterthought than as a primary objective. We present the
first fine-grained empirical analysis of inference energy across core
components of transformer architecture. We propose a novel methodology,
Component-Level Energy Assessment via Repeated sampling (CLEAR), to overcome
temporal mismatch between microsecond scale component execution and monitoring
of millisecond (ms) scale energy sensors. Using CLEAR, we evaluate 15 models
spanning four distinct architecture types and consistently keep component-wise
energy variance below 9.5\% while capturing more than 90\% of the model's total
energy as individual components. Our empirical analysis reveals that Attention
blocks consume significantly more energy per floating-point operation (FLOP),
indicating that energy consumption is not proportionally aligned with FLOP
counts. This shows that FLOPs alone fail to capture the true energy cost at a
component level. Our findings establish detailed component-level energy
baselines and provide insight as an initial step to build energy-efficient
transformer models through component-level optimizations.

</details>


### [150] [Mitigating Spurious Correlation via Distributionally Robust Learning with Hierarchical Ambiguity Sets](https://arxiv.org/abs/2510.02818)
*Sung Ho Jo,Seonghwi Kim,Minwoo Chae*

Main category: cs.LG

TL;DR: Hierarchical Group DRO extension addressing both inter-group and intra-group distribution shifts, particularly effective for minority groups with limited samples where conventional methods fail.


<details>
  <summary>Details</summary>
Motivation: Conventional robust learning methods like Group DRO are vulnerable to intra-group distribution shifts, especially in minority groups with limited samples, which is an underexplored challenge in spurious correlation research.

Method: Propose hierarchical extension of Group DRO that addresses both inter-group and intra-group uncertainties, and introduce new benchmark settings simulating realistic minority group distribution shifts.

Result: Method demonstrates strong robustness under minority group distribution shifts where existing methods consistently fail, while achieving superior performance on standard benchmarks.

Conclusion: Broadening the ambiguity set to capture both inter-group and intra-group distributional uncertainties is crucial for robust learning, especially for minority groups.

Abstract: Conventional supervised learning methods are often vulnerable to spurious
correlations, particularly under distribution shifts in test data. To address
this issue, several approaches, most notably Group DRO, have been developed.
While these methods are highly robust to subpopulation or group shifts, they
remain vulnerable to intra-group distributional shifts, which frequently occur
in minority groups with limited samples. We propose a hierarchical extension of
Group DRO that addresses both inter-group and intra-group uncertainties,
providing robustness to distribution shifts at multiple levels. We also
introduce new benchmark settings that simulate realistic minority group
distribution shifts-an important yet previously underexplored challenge in
spurious correlation research. Our method demonstrates strong robustness under
these conditions-where existing robust learning methods consistently fail-while
also achieving superior performance on standard benchmarks. These results
highlight the importance of broadening the ambiguity set to better capture both
inter-group and intra-group distributional uncertainties.

</details>


### [151] [Online Learning in the Random Order Model](https://arxiv.org/abs/2510.02820)
*Martino Bernasconi,Andrea Celli,Riccardo Colini-Baldeschi,Federico Fusco,Stefano Leonardi,Matteo Russo*

Main category: cs.LG

TL;DR: This paper proposes a method to adapt stochastic learning algorithms to work effectively in the random-order model, maintaining their regret guarantees while handling non-stationarity, and shows applications in various online learning settings.


<details>
  <summary>Details</summary>
Motivation: Random-order inputs can exhibit significant non-stationarity that hinders stochastic learning algorithms, while adversarial algorithms maintain guarantees but may be suboptimal. There's a need to bridge this gap.

Method: The authors propose a general template to adapt stochastic learning algorithms to the random-order model without substantially affecting their regret guarantees.

Result: The method enables improved regret bounds for prediction with delays, online learning with constraints, and bandits with switching costs. Also shows that in random order, learnability is characterized by VC dimension rather than Littlestone dimension.

Conclusion: The proposed template successfully adapts stochastic algorithms to random-order settings, providing better performance guarantees and revealing fundamental differences in learnability between random-order and adversarial models.

Abstract: In the random-order model for online learning,
  the sequence of losses is chosen upfront by an adversary and presented to the
learner
  after a random permutation. Any random-order input is \emph{asymptotically}
equivalent to a stochastic i.i.d. one, but, for finite times, it may exhibit
significant {\em non-stationarity}, which can hinder the performance of
stochastic learning algorithms.
  While algorithms for adversarial inputs naturally maintain their regret
guarantees in random order, simple no-regret algorithms exist for the
stochastic model that fail against random-order instances.
  In this paper, we propose a general template to adapt stochastic learning
algorithms to the random-order model without substantially affecting their
regret guarantees. This allows us to recover improved regret bounds for
prediction with delays, online learning with constraints, and bandits with
switching costs. Finally, we investigate online classification and prove that,
in random order, learnability is characterized by the VC dimension rather than
the Littlestone dimension, thus providing a further separation from the general
adversarial model.

</details>


### [152] [FlexiQ: Adaptive Mixed-Precision Quantization for Latency/Accuracy Trade-Offs in Deep Neural Networks](https://arxiv.org/abs/2510.02822)
*Jaemin Kim,Hongjun Um,Sungkyun Kim,Yongjun Park,Jiwon Seo*

Main category: cs.LG

TL;DR: FlexiQ is an adaptive mixed-precision quantization scheme for computer vision models that dynamically adjusts low-bitwidth computation ratios to handle real-time workload fluctuations while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Hardware accelerators like NPUs and GPUs are costly and difficult to scale for real-time workload fluctuations, creating a need for efficient quantization methods that can adapt to varying computational demands.

Method: FlexiQ selectively applies low-bitwidth computation to feature channels with small value ranges, uses efficient bit-lowering to minimize quantization errors, and dynamically adjusts low-bitwidth channel ratios in real-time.

Result: FlexiQ achieves 6.6% higher accuracy for 4-bit models with finetuning, outperforms four state-of-the-art quantization techniques, and achieves efficient accuracy-latency trade-off with only 0.6% accuracy loss while achieving 40% of the speedup of full 4-bit models.

Conclusion: FlexiQ demonstrates hardware efficiency with minimal runtime overhead on NPUs and GPUs, providing overall performance benefits for adaptive mixed-precision quantization in computer vision models.

Abstract: Neural networks commonly execute on hardware accelerators such as NPUs and
GPUs for their size and computation overhead. These accelerators are costly and
it is hard to scale their resources to handle real-time workload fluctuations.
  We present FlexiQ, an adaptive mixed-precision quantization scheme for
computer vision models. FlexiQ selectively applies low-bitwidth computation to
feature channels with small value ranges and employs an efficient bit-lowering
method to minimize quantization errors while maintaining inference accuracy.
Furthermore, FlexiQ adjusts its low-bitwidth channel ratio in real time,
enabling quantized models to effectively manage fluctuating inference workload.
  We implemented FlexiQ prototype, including the mixed-precision inference
runtime on our custom NPU and GPUs. Evaluated on eleven convolution- and
transformer-based vision models, FlexiQ achieves on average 6.6% higher
accuracy for 4-bit models with finetuning and outperforms four state-of-the-art
quantization techniques. Moreover, our mixed-precision models achieved an
efficient accuracy-latency trade-off, with the 50% 4-bit model incurring only
0.6% accuracy loss while achieving 40% of the speedup of the 100% 4-bit model
over 8-bit model. Latency evaluations on our NPU and GPUs confirmed that FlexiQ
introduces minimal runtime overhead, demonstrating its hardware efficiency and
overall performance benefits.

</details>


### [153] [The Curious Case of In-Training Compression of State Space Models](https://arxiv.org/abs/2510.02823)
*Makram Chahine,Philipp Nazari,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: The paper proposes using Hankel singular value analysis and balanced truncation to dynamically reduce State Space Models (SSMs) during training, identifying and preserving only high-influence dimensions to accelerate optimization while maintaining expressivity.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing computational efficiency and expressivity in SSMs, where update costs scale with state dimension, by leveraging control theory principles to identify and preserve only the most influential dimensions.

Method: Apply Hankel singular value analysis and balanced truncation during SSM training to identify high-influence dimensions, allowing models to start large and dynamically shrink while preserving critical structure.

Result: Experiments show that in-training reduction significantly accelerates optimization while preserving expressivity, with compressed models retaining task-critical structure that is lost when training directly at smaller dimensions.

Conclusion: SSMs that begin large and shrink during training achieve computational efficiency while maintaining higher performance compared to models trained directly at smaller dimensions.

Abstract: State Space Models (SSMs), developed to tackle long sequence modeling tasks
efficiently, offer both parallelizable training and fast inference. At their
core are recurrent dynamical systems that maintain a hidden state, with update
costs scaling with the state dimension. A key design challenge is striking the
right balance between maximizing expressivity and limiting this computational
burden. Control theory, and more specifically Hankel singular value analysis,
provides a potent framework for the measure of energy for each state, as well
as the balanced truncation of the original system down to a smaller
representation with performance guarantees. Leveraging the eigenvalue stability
properties of Hankel matrices, we apply this lens to SSMs during training,
where only dimensions of high influence are identified and preserved. Our
approach applies to Linear Time-Invariant SSMs such as Linear Recurrent Units,
but is also extendable to selective models. Experiments show that in-training
reduction significantly accelerates optimization while preserving expressivity,
with compressed models retaining task-critical structure lost by models trained
directly at smaller dimension. In other words, SSMs that begin large and shrink
during training achieve computational efficiency while maintaining higher
performance.

</details>


### [154] [Multi-scale Autoregressive Models are Laplacian, Discrete, and Latent Diffusion Models in Disguise](https://arxiv.org/abs/2510.02826)
*Steve Hong,Samuel Belkadi*

Main category: cs.LG

TL;DR: VAR models are reframed as iterative refinement processes with Laplacian pyramid construction and reconstruction, connecting them to diffusion models and identifying key efficiency factors.


<details>
  <summary>Details</summary>
Motivation: To better understand VAR models' efficiency and fidelity by viewing them through an iterative-refinement framework rather than just as next-scale autoregression.

Method: Formalize VAR as deterministic forward process building Laplacian latent pyramid + learned backward process reconstructing it in coarse-to-fine steps; test design choices: latent space refinement, discrete classification prediction, spatial frequency partitioning.

Result: Identified three key factors contributing to VAR's efficiency and fidelity; framework extends to graph generation and weather forecasting; enables practical interfaces for diffusion ecosystem integration.

Conclusion: The iterative-refinement framework provides deeper understanding of VAR models, connects them to diffusion methods, and enables practical improvements while maintaining few-step generation.

Abstract: We revisit Visual Autoregressive (VAR) models through the lens of an
iterative-refinement framework. Rather than viewing VAR solely as next-scale
autoregression, we formalise it as a deterministic forward process that
constructs a Laplacian-style latent pyramid, paired with a learned backward
process that reconstructs it in a small number of coarse-to-fine steps. This
view connects VAR to denoising diffusion and isolates three design choices that
help explain its efficiency and fidelity: refining in a learned latent space,
casting prediction as discrete classification over code indices, and
partitioning the task by spatial frequency. We run controlled experiments to
quantify each factor's contribution to fidelity and speed, and we outline how
the same framework extends to permutation-invariant graph generation and to
probabilistic, ensemble-style medium-range weather forecasting. The framework
also suggests practical interfaces for VAR to leverage tools from the diffusion
ecosystem while retaining few-step, scale-parallel generation.

</details>


### [155] [Subject-Adaptive Sparse Linear Models for Interpretable Personalized Health Prediction from Multimodal Lifelog Data](https://arxiv.org/abs/2510.02835)
*Dohyun Bu,Jisoo Han,Soohwa Kwon,Yulim So,Jong-Seok Lee*

Main category: cs.LG

TL;DR: SASL framework combines sparse linear models with LightGBM for interpretable personalized health prediction, achieving comparable performance to black-box methods with better transparency.


<details>
  <summary>Details</summary>
Motivation: Current deep learning and gradient-boosting models sacrifice interpretability and fail to handle inter-individual variability in lifelog data for health outcome prediction.

Method: Subject-Adaptive Sparse Linear (SASL) framework using OLS regression with subject-specific interactions, backward feature elimination with F-tests, regression-then-thresholding for ordinal targets, and confidence-based gating with LightGBM.

Result: SASL-LightGBM hybrid achieves predictive performance comparable to sophisticated black-box methods on CH-2025 dataset (450 daily observations from 10 subjects) with fewer parameters and greater transparency.

Conclusion: The framework provides interpretable personalized health prediction with actionable insights for clinicians, balancing accuracy and transparency.

Abstract: Improved prediction of personalized health outcomes -- such as sleep quality
and stress -- from multimodal lifelog data could have meaningful clinical and
practical implications. However, state-of-the-art models, primarily deep neural
networks and gradient-boosted ensembles, sacrifice interpretability and fail to
adequately address the significant inter-individual variability inherent in
lifelog data. To overcome these challenges, we propose the Subject-Adaptive
Sparse Linear (SASL) framework, an interpretable modeling approach explicitly
designed for personalized health prediction. SASL integrates ordinary least
squares regression with subject-specific interactions, systematically
distinguishing global from individual-level effects. We employ an iterative
backward feature elimination method based on nested $F$-tests to construct a
sparse and statistically robust model. Additionally, recognizing that health
outcomes often represent discretized versions of continuous processes, we
develop a regression-then-thresholding approach specifically designed to
maximize macro-averaged F1 scores for ordinal targets. For intrinsically
challenging predictions, SASL selectively incorporates outputs from compact
LightGBM models through confidence-based gating, enhancing accuracy without
compromising interpretability. Evaluations conducted on the CH-2025 dataset --
which comprises roughly 450 daily observations from ten subjects -- demonstrate
that the hybrid SASL-LightGBM framework achieves predictive performance
comparable to that of sophisticated black-box methods, but with significantly
fewer parameters and substantially greater transparency, thus providing clear
and actionable insights for clinicians and practitioners.

</details>


### [156] [Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics](https://arxiv.org/abs/2510.02839)
*Vijay Babu Pamshetti,Wei Zhang,Sumei Sun,Jie Zhang,Yonggang Wen,Qingyu Yan*

Main category: cs.LG

TL;DR: Karma is a knowledge-aware model with frequency-adaptive learning for battery health prognostics, combining signal decomposition, dual-stream deep learning, and physics-based knowledge guidance to achieve accurate capacity estimation and remaining useful life prediction.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven battery health prognostic models lack knowledge guidance, leading to unreliable long-term predictions due to complex degradation behaviors with nonlinearity, noise, and capacity regeneration.

Method: Proposes Karma with signal decomposition to derive frequency bands, dual-stream deep learning architecture (one for long-term low-frequency trends, one for high-frequency dynamics), and knowledge regulation using double exponential degradation function optimized with particle filters.

Result: Achieves average error reductions of 50.6% and 32.6% over state-of-the-art algorithms on two mainstream datasets, demonstrating superior performance in battery health prediction.

Conclusion: Karma shows robustness, generalizability, and potential for safer and more reliable battery management across diverse applications through its knowledge-aware frequency-adaptive approach.

Abstract: Battery health prognostics are critical for ensuring safety, efficiency, and
sustainability in modern energy systems. However, it has been challenging to
achieve accurate and robust prognostics due to complex battery degradation
behaviors with nonlinearity, noise, capacity regeneration, etc. Existing
data-driven models capture temporal degradation features but often lack
knowledge guidance, which leads to unreliable long-term health prognostics. To
overcome these limitations, we propose Karma, a knowledge-aware model with
frequency-adaptive learning for battery capacity estimation and remaining
useful life prediction. The model first performs signal decomposition to derive
battery signals in different frequency bands. A dual-stream deep learning
architecture is developed, where one stream captures long-term low-frequency
degradation trends and the other models high-frequency short-term dynamics.
Karma regulates the prognostics with knowledge, where battery degradation is
modeled as a double exponential function based on empirical studies. Our
dual-stream model is used to optimize the parameters of the knowledge with
particle filters to ensure physically consistent and reliable prognostics and
uncertainty quantification. Experimental study demonstrates Karma's superior
performance, achieving average error reductions of 50.6% and 32.6% over
state-of-the-art algorithms for battery health prediction on two mainstream
datasets, respectively. These results highlight Karma's robustness,
generalizability, and potential for safer and more reliable battery management
across diverse applications.

</details>


### [157] [RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning](https://arxiv.org/abs/2510.02892)
*Aleksei Arzhantsev,Otmane Sakhi,Flavian Vasile*

Main category: cs.LG

TL;DR: RoiRL is a lightweight offline reinforcement learning method that improves LLM reasoning without ground-truth rewards, training 2.5x faster than existing methods while achieving better performance.


<details>
  <summary>Details</summary>
Motivation: Traditional RL for LLMs requires ground-truth rewards, while existing test-time RL methods are computationally expensive and require maintaining reference models.

Method: RoiRL uses offline iterative reinforcement learning with weighted log-likelihood objectives, eliminating the need for reference models and enabling stable training with lower resource requirements.

Result: RoiRL trains 2.5x faster than TTRL and consistently outperforms it on reasoning benchmarks, demonstrating superior computational efficiency and performance.

Conclusion: RoiRL establishes a scalable path for self-improving LLMs without labels, providing a practical alternative to computationally expensive online RL methods.

Abstract: Reinforcement learning (RL) is central to improving reasoning in large
language models (LLMs) but typically requires ground-truth rewards. Test-Time
Reinforcement Learning (TTRL) removes this need by using majority-vote rewards,
but relies on heavy online RL and incurs substantial computational cost. We
propose RoiRL: Reasoning with offline iterative Reinforcement Learning, a
family of lightweight offline learning alternatives that can target the same
regularized optimal policies. Unlike TTRL, RoiRL eliminates the need to
maintain a reference model and instead optimizes weighted log-likelihood
objectives, enabling stable training with significantly lower memory and
compute requirements. Experimental results show that RoiRL trains to 2.5x
faster and consistently outperforms TTRL on reasoning benchmarks, establishing
a scalable path to self-improving LLMs without labels.

</details>


### [158] [Learning Explicit Single-Cell Dynamics Using ODE Representations](https://arxiv.org/abs/2510.02903)
*Jan-Philipp von Bassewitz,Adeel Pervez,Marco Fumero,Matthew Robinson,Theofanis Karaletsos,Francesco Locatello*

Main category: cs.LG

TL;DR: Cell-MNN is an end-to-end encoder-decoder model that learns interpretable gene interactions through a locally linearized ODE representation of cellular differentiation dynamics, achieving competitive performance on single-cell benchmarks with better scalability.


<details>
  <summary>Details</summary>
Motivation: Current models for cellular differentiation dynamics rely on computationally expensive optimal transport preprocessing and multi-stage training, while failing to discover explicit gene interactions that are crucial for biological understanding.

Method: Proposed Cell-MNN uses an encoder-decoder architecture with a latent representation as a locally linearized ODE that governs cellular evolution from stem to tissue cells. The model is fully end-to-end (except standard PCA preprocessing) and explicitly learns biologically consistent gene interactions.

Result: Cell-MNN achieves competitive performance on single-cell benchmarks, surpasses state-of-the-art baselines in scaling to larger datasets and joint training across multiple datasets. It also learns interpretable gene interactions validated against the TRRUST database.

Conclusion: Cell-MNN provides an effective end-to-end framework for modeling cellular differentiation that is computationally efficient, scalable, and capable of discovering biologically meaningful gene interactions.

Abstract: Modeling the dynamics of cellular differentiation is fundamental to advancing
the understanding and treatment of diseases associated with this process, such
as cancer. With the rapid growth of single-cell datasets, this has also become
a particularly promising and active domain for machine learning. Current
state-of-the-art models, however, rely on computationally expensive optimal
transport preprocessing and multi-stage training, while also not discovering
explicit gene interactions. To address these challenges we propose
Cell-Mechanistic Neural Networks (Cell-MNN), an encoder-decoder architecture
whose latent representation is a locally linearized ODE governing the dynamics
of cellular evolution from stem to tissue cells. Cell-MNN is fully end-to-end
(besides a standard PCA pre-processing) and its ODE representation explicitly
learns biologically consistent and interpretable gene interactions.
Empirically, we show that Cell-MNN achieves competitive performance on
single-cell benchmarks, surpasses state-of-the-art baselines in scaling to
larger datasets and joint training across multiple datasets, while also
learning interpretable gene interactions that we validate against the TRRUST
database of gene interactions.

</details>


### [159] [FeDABoost: Fairness Aware Federated Learning with Adaptive Boosting](https://arxiv.org/abs/2510.02914)
*Tharuka Kasthuri Arachchige,Veselka Boeva,Shahrooz Abghari*

Main category: cs.LG

TL;DR: FeDABoost is a novel FL framework that improves fairness and performance in non-IID settings through dynamic boosting and adaptive gradient aggregation.


<details>
  <summary>Details</summary>
Motivation: To address performance and fairness issues in Federated Learning under non-IID data distributions by enhancing model aggregation and boosting underperforming clients.

Method: Integrates dynamic boosting mechanism and adaptive gradient aggregation strategy inspired by Multiclass AdaBoost, assigning higher weights to clients with lower error rates and adjusting focal loss parameters for underperforming clients.

Result: Evaluated on MNIST, FEMNIST, and CIFAR10 datasets, FeDABoost achieves improved fairness and competitive performance compared to FedAvg and Ditto.

Conclusion: FeDABoost effectively enhances FL performance and fairness in non-IID settings through its dual approach of adaptive aggregation and dynamic client boosting.

Abstract: This work focuses on improving the performance and fairness of Federated
Learning (FL) in non IID settings by enhancing model aggregation and boosting
the training of underperforming clients. We propose FeDABoost, a novel FL
framework that integrates a dynamic boosting mechanism and an adaptive gradient
aggregation strategy. Inspired by the weighting mechanism of the Multiclass
AdaBoost (SAMME) algorithm, our aggregation method assigns higher weights to
clients with lower local error rates, thereby promoting more reliable
contributions to the global model. In parallel, FeDABoost dynamically boosts
underperforming clients by adjusting the focal loss focusing parameter,
emphasizing hard to classify examples during local training. We have evaluated
FeDABoost on three benchmark datasets MNIST, FEMNIST, and CIFAR10, and compared
its performance with those of FedAvg and Ditto. The results show that FeDABoost
achieves improved fairness and competitive performance.

</details>


### [160] [RAxSS: Retrieval-Augmented Sparse Sampling for Explainable Variable-Length Medical Time Series Classification](https://arxiv.org/abs/2510.02936)
*Aydin Javadov,Samir Garibov,Tobias Hoesli,Qiyang Sun,Florian von Wangenheim,Joseph Ollier,Björn W. Schuller*

Main category: cs.LG

TL;DR: The paper proposes a retrieval-augmented stochastic sparse sampling method for medical time series classification that improves explainability while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Medical time series analysis faces challenges including data sparsity, noise, and variable recording lengths. Existing approaches using stochastic sparse sampling handle variable-length signals well, while retrieval-augmented methods improve explainability and robustness.

Method: Generalizes stochastic sparse sampling framework for retrieval-informed classification by weighting window predictions based on within-channel similarity and aggregating them in probability space, producing convex series-level scores with explicit evidence trails.

Result: Achieves competitive iEEG classification performance and provides practitioners with greater transparency and explainability. Evaluated on iEEG recordings from four medical centers.

Conclusion: The method demonstrates potential for reliable and explainable clinical variable-length time series classification, offering both performance and interpretability benefits.

Abstract: Medical time series analysis is challenging due to data sparsity, noise, and
highly variable recording lengths. Prior work has shown that stochastic sparse
sampling effectively handles variable-length signals, while retrieval-augmented
approaches improve explainability and robustness to noise and weak temporal
correlations. In this study, we generalize the stochastic sparse sampling
framework for retrieval-informed classification. Specifically, we weight window
predictions by within-channel similarity and aggregate them in probability
space, yielding convex series-level scores and an explicit evidence trail for
explainability. Our method achieves competitive iEEG classification performance
and provides practitioners with greater transparency and explainability. We
evaluate our method in iEEG recordings collected in four medical centers,
demonstrating its potential for reliable and explainable clinical
variable-length time series classification.

</details>


### [161] [Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning](https://arxiv.org/abs/2510.02945)
*Juan Sebastian Rojas,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: This paper introduces the first formal theoretical treatment of continual reinforcement learning through risk-aware decision-making, proposing ergodic risk measures as a solution to incompatibility between classical risk measures and continual learning settings.


<details>
  <summary>Details</summary>
Motivation: Current continual RL focuses exclusively on risk-neutral (mean-optimizing) decision-making, but lacks theoretical foundations for risk-aware approaches that consider performance measures beyond the mean in lifelong learning scenarios.

Method: The authors identify incompatibility of classical risk measure theory with continual settings, then develop a new class of ergodic risk measures specifically designed for continual learning, supported by theoretical analysis and empirical case studies.

Result: The proposed ergodic risk measures provide intuitive appeal and theoretical soundness for risk-aware continual learning, enabling agents to maintain balance between information retention and adaptation while considering risk beyond expected performance.

Conclusion: Ergodic risk measures offer a theoretically grounded framework for risk-aware continual RL, bridging the gap between classical risk measure theory and the requirements of lifelong learning environments.

Abstract: Continual reinforcement learning (continual RL) seeks to formalize the
notions of lifelong learning and endless adaptation in RL. In particular, the
aim of continual RL is to develop RL agents that can maintain a careful balance
between retaining useful information and adapting to new situations. To date,
continual RL has been explored almost exclusively through the lens of
risk-neutral decision-making, in which the agent aims to optimize the expected
(or mean) long-run performance. In this work, we present the first formal
theoretical treatment of continual RL through the lens of risk-aware
decision-making, in which the agent aims to optimize a reward-based measure of
long-run performance beyond the mean. In particular, we show that the classical
theory of risk measures, widely used as a theoretical foundation in
non-continual risk-aware RL, is, in its current form, incompatible with the
continual setting. Then, building on this insight, we extend risk measure
theory into the continual setting by introducing a new class of ergodic risk
measures that are compatible with continual learning. Finally, we provide a
case study of risk-aware continual learning, along with empirical results,
which show the intuitive appeal and theoretical soundness of ergodic risk
measures.

</details>


### [162] [ContextFlow: Context-Aware Flow Matching For Trajectory Inference From Spatial Omics Data](https://arxiv.org/abs/2510.02952)
*Santanu Subhash Rathod,Francesco Ceccarelli,Sean B. Holden,Pietro Liò,Xiao Zhang,Jovan Tanevski*

Main category: cs.LG

TL;DR: ContextFlow is a context-aware flow matching framework that integrates tissue organization and ligand-receptor communication to infer biologically meaningful trajectories from longitudinal spatially-resolved omics data.


<details>
  <summary>Details</summary>
Motivation: Understanding tissue dynamics in development, regeneration, disease progression, and treatment response requires accurate inference of trajectories from spatially-resolved omics data.

Method: ContextFlow incorporates prior knowledge through local tissue organization and ligand-receptor communication patterns into a transition plausibility matrix that regularizes the optimal transport objective.

Result: ContextFlow consistently outperforms state-of-the-art flow matching methods across multiple quantitative and qualitative metrics of inference accuracy and biological coherence on three datasets.

Conclusion: ContextFlow provides a generalizable framework for modeling spatiotemporal dynamics from longitudinal spatially-resolved omics data by generating biologically meaningful trajectories.

Abstract: Inferring trajectories from longitudinal spatially-resolved omics data is
fundamental to understanding the dynamics of structural and functional tissue
changes in development, regeneration and repair, disease progression, and
response to treatment. We propose ContextFlow, a novel context-aware flow
matching framework that incorporates prior knowledge to guide the inference of
structural tissue dynamics from spatially resolved omics data. Specifically,
ContextFlow integrates local tissue organization and ligand-receptor
communication patterns into a transition plausibility matrix that regularizes
the optimal transport objective. By embedding these contextual constraints,
ContextFlow generates trajectories that are not only statistically consistent
but also biologically meaningful, making it a generalizable framework for
modeling spatiotemporal dynamics from longitudinal, spatially resolved omics
data. Evaluated on three datasets, ContextFlow consistently outperforms
state-of-the-art flow matching methods across multiple quantitative and
qualitative metrics of inference accuracy and biological coherence. Our code is
available at: \href{https://github.com/santanurathod/ContextFlow}{ContextFlow}

</details>


### [163] [Confidence and Dispersity as Signals: Unsupervised Model Evaluation and Ranking](https://arxiv.org/abs/2510.02956)
*Weijian Deng,Weijie Tu,Ibrahim Radwan,Mohammad Abu Alsheikh,Stephen Gould,Liang Zheng*

Main category: cs.LG

TL;DR: This paper presents a unified framework for unsupervised model evaluation using confidence and dispersity metrics, showing that hybrid approaches outperform single-metric methods across various distribution shifts.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of assessing model generalization under distribution shift when labeled test data is unavailable, which is crucial for real-world deployment.

Method: Proposes using confidence (prediction certainty) and dispersity (diversity of predicted classes) as complementary signals, and systematically benchmarks confidence-based, dispersity-based, and hybrid metrics across various models, datasets, and distribution shifts.

Result: Hybrid metrics consistently outperform single-aspect metrics in both dataset-centric and model-centric evaluation settings. The nuclear norm of the prediction matrix provides robust and accurate performance across tasks, including real-world datasets, and maintains reliability under moderate class imbalance.

Conclusion: The findings offer a practical and generalizable basis for unsupervised model assessment in deployment scenarios, with hybrid metrics providing the most reliable evaluation.

Abstract: Assessing model generalization under distribution shift is essential for
real-world deployment, particularly when labeled test data is unavailable. This
paper presents a unified and practical framework for unsupervised model
evaluation and ranking in two common deployment settings: (1) estimating the
accuracy of a fixed model on multiple unlabeled test sets (dataset-centric
evaluation), and (2) ranking a set of candidate models on a single unlabeled
test set (model-centric evaluation). We demonstrate that two intrinsic
properties of model predictions, namely confidence (which reflects prediction
certainty) and dispersity (which captures the diversity of predicted classes),
together provide strong and complementary signals for generalization. We
systematically benchmark a set of confidence-based, dispersity-based, and
hybrid metrics across a wide range of model architectures, datasets, and
distribution shift types. Our results show that hybrid metrics consistently
outperform single-aspect metrics on both dataset-centric and model-centric
evaluation settings. In particular, the nuclear norm of the prediction matrix
provides robust and accurate performance across tasks, including real-world
datasets, and maintains reliability under moderate class imbalance. These
findings offer a practical and generalizable basis for unsupervised model
assessment in deployment scenarios.

</details>


### [164] [From high-frequency sensors to noon reports: Using transfer learning for shaft power prediction in maritime](https://arxiv.org/abs/2510.03003)
*Akriti Sharma,Dogan Altan,Dusica Marijan,Arnbjørn Maressa*

Main category: cs.LG

TL;DR: Transfer learning approach for vessel shaft power prediction using high-frequency data from one vessel to improve predictions on other vessels using low-frequency noon reports.


<details>
  <summary>Details</summary>
Motivation: Energy optimization in maritime transportation is crucial for cost reduction and operational efficiency. Accurate shaft power prediction directly impacts fuel consumption, but high-quality sensor data is often infeasible and costly to obtain.

Method: Proposed transfer learning-based approach where a model is initially trained on high-frequency data from one vessel, then fine-tuned with low-frequency daily noon reports from other vessels.

Result: Experiments showed MAPE decreased by 10.6% for sister vessels, 3.6% for similar vessel, and 5.3% for different vessel compared to model trained solely on noon report data.

Conclusion: Transfer learning effectively improves shaft power prediction accuracy across various vessel types using limited noon report data, demonstrating practical value for maritime energy optimization.

Abstract: With the growth of global maritime transportation, energy optimization has
become crucial for reducing costs and ensuring operational efficiency. Shaft
power is the mechanical power transmitted from the engine to the shaft and
directly impacts fuel consumption, making its accurate prediction a paramount
step in optimizing vessel performance. Power consumption is highly correlated
with ship parameters such as speed and shaft rotation per minute, as well as
weather and sea conditions. Frequent access to this operational data can
improve prediction accuracy. However, obtaining high-quality sensor data is
often infeasible and costly, making alternative sources such as noon reports a
viable option. In this paper, we propose a transfer learning-based approach for
predicting vessels shaft power, where a model is initially trained on
high-frequency data from a vessel and then fine-tuned with low-frequency daily
noon reports from other vessels. We tested our approach on sister vessels
(identical dimensions and configurations), a similar vessel (slightly larger
with a different engine), and a different vessel (distinct dimensions and
configurations). The experiments showed that the mean absolute percentage error
decreased by 10.6 percent for sister vessels, 3.6 percent for a similar vessel,
and 5.3 percent for a different vessel, compared to the model trained solely on
noon report data.

</details>


### [165] [BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia](https://arxiv.org/abs/2510.03004)
*Tianzheng Hu,Qiang Li,Shu Liu,Vince D. Calhoun,Guido van Wingen,Shujian Yu*

Main category: cs.LG

TL;DR: BrainIB++ is a graph neural network framework that uses information bottleneck principle to identify informative brain regions as subgraphs for schizophrenia diagnosis from rs-fMRI data, achieving superior accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current machine learning models for psychiatric diagnosis require extensive feature engineering (introducing bias) or lack interpretability (limiting clinical applicability), creating a need for automated yet explainable diagnostic models.

Method: Developed an end-to-end graph neural network framework (BrainIB++) that applies information bottleneck principle to automatically identify the most informative brain regions as subgraphs during training for interpretation.

Result: Outperformed nine established brain network classification methods across three multi-cohort schizophrenia datasets, demonstrating superior diagnostic accuracy and generalizability to unseen data.

Conclusion: The identified subgraphs align with established clinical biomarkers in schizophrenia (visual, sensorimotor, and higher cognition networks), enhancing interpretability and relevance for real-world diagnostic applications.

Abstract: The development of diagnostic models is gaining traction in the field of
psychiatric disorders. Recently, machine learning classifiers based on
resting-state functional magnetic resonance imaging (rs-fMRI) have been
developed to identify brain biomarkers that differentiate psychiatric disorders
from healthy controls. However, conventional machine learning-based diagnostic
models often depend on extensive feature engineering, which introduces bias
through manual intervention. While deep learning models are expected to operate
without manual involvement, their lack of interpretability poses significant
challenges in obtaining explainable and reliable brain biomarkers to support
diagnostic decisions, ultimately limiting their clinical applicability. In this
study, we introduce an end-to-end innovative graph neural network framework
named BrainIB++, which applies the information bottleneck (IB) principle to
identify the most informative data-driven brain regions as subgraphs during
model training for interpretation. We evaluate the performance of our model
against nine established brain network classification methods across three
multi-cohort schizophrenia datasets. It consistently demonstrates superior
diagnostic accuracy and exhibits generalizability to unseen data. Furthermore,
the subgraphs identified by our model also correspond with established clinical
biomarkers in schizophrenia, particularly emphasizing abnormalities in the
visual, sensorimotor, and higher cognition brain functional network. This
alignment enhances the model's interpretability and underscores its relevance
for real-world diagnostic applications.

</details>


### [166] [Distributional Inverse Reinforcement Learning](https://arxiv.org/abs/2510.03013)
*Feiyang Wu,Ye Zhao,Anqi Wu*

Main category: cs.LG

TL;DR: A distributional offline IRL framework that models uncertainty over rewards and return distributions, capturing richer expert behavior structure through FSD minimization and DRM integration for risk-aware imitation learning.


<details>
  <summary>Details</summary>
Motivation: Conventional IRL methods only recover deterministic reward estimates or match expected returns, lacking the ability to capture richer structure in expert behavior, particularly reward distributions and risk-aware policies.

Method: Jointly models uncertainty over reward functions and full distributions of returns by minimizing first-order stochastic dominance violations and integrating distortion risk measures into policy learning.

Result: Empirical results on synthetic benchmarks, real-world neurobehavioral data, and MuJoCo control tasks show the method recovers expressive reward representations and achieves state-of-the-art imitation performance.

Conclusion: The proposed distributional framework enables recovery of both reward distributions and distribution-aware policies, making it well-suited for behavior analysis and risk-aware imitation learning.

Abstract: We propose a distributional framework for offline Inverse Reinforcement
Learning (IRL) that jointly models uncertainty over reward functions and full
distributions of returns. Unlike conventional IRL approaches that recover a
deterministic reward estimate or match only expected returns, our method
captures richer structure in expert behavior, particularly in learning the
reward distribution, by minimizing first-order stochastic dominance (FSD)
violations and thus integrating distortion risk measures (DRMs) into policy
learning, enabling the recovery of both reward distributions and
distribution-aware policies. This formulation is well-suited for behavior
analysis and risk-aware imitation learning. Empirical results on synthetic
benchmarks, real-world neurobehavioral data, and MuJoCo control tasks
demonstrate that our method recovers expressive reward representations and
achieves state-of-the-art imitation performance.

</details>


### [167] [Learning Robust Diffusion Models from Imprecise Supervision](https://arxiv.org/abs/2510.03016)
*Dong-Dong Wu,Jiacheng Cui,Wei Wang,Zhiqiang She,Masashi Sugiyama*

Main category: cs.LG

TL;DR: DMIS is a unified framework for training robust diffusion models from imprecise supervision, addressing noisy, ambiguous, or incomplete labels in conditional inputs through generative and classification components.


<details>
  <summary>Details</summary>
Motivation: Conditional diffusion models rely on large datasets with imprecise conditional inputs (noisy, ambiguous, or incomplete labels), causing condition mismatch and degraded generation quality.

Method: DMIS decomposes the objective into generative and classification components: generative component models imprecise-label distributions, classification component uses diffusion classifier with optimized timestep sampling for class-posterior probabilities.

Result: Extensive experiments on diverse imprecise supervision tasks (image generation, weakly supervised learning, noisy dataset condensation) show DMIS consistently produces high-quality, class-discriminative samples.

Conclusion: DMIS is the first systematic study for training robust diffusion models from imprecise supervision, effectively handling various forms of label imprecision while maintaining generation quality.

Abstract: Conditional diffusion models have achieved remarkable success in various
generative tasks recently, but their training typically relies on large-scale
datasets that inevitably contain imprecise information in conditional inputs.
Such supervision, often stemming from noisy, ambiguous, or incomplete labels,
will cause condition mismatch and degrade generation quality. To address this
challenge, we propose DMIS, a unified framework for training robust Diffusion
Models from Imprecise Supervision, which is the first systematic study within
diffusion models. Our framework is derived from likelihood maximization and
decomposes the objective into generative and classification components: the
generative component models imprecise-label distributions, while the
classification component leverages a diffusion classifier to infer
class-posterior probabilities, with its efficiency further improved by an
optimized timestep sampling strategy. Extensive experiments on diverse forms of
imprecise supervision, covering tasks of image generation, weakly supervised
learning, and noisy dataset condensation demonstrate that DMIS consistently
produces high-quality and class-discriminative samples.

</details>


### [168] [Differentially Private Wasserstein Barycenters](https://arxiv.org/abs/2510.03021)
*Anming Gu,Sasidhar Kunapuli,Mark Bun,Edward Chien,Kristjan Greenewald*

Main category: cs.LG

TL;DR: First differentially private algorithms for computing Wasserstein barycenters with strong accuracy-privacy tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Wasserstein barycenters are computed from empirical distributions built from sensitive datasets, requiring privacy protection.

Method: Developed differentially private algorithms for Wasserstein barycenter computation.

Result: Methods produce high-quality private barycenters on synthetic data, MNIST, and large-scale U.S. population datasets.

Conclusion: Successfully created the first differentially private approach for Wasserstein barycenter computation with practical utility.

Abstract: The Wasserstein barycenter is defined as the mean of a set of probability
measures under the optimal transport metric, and has numerous applications
spanning machine learning, statistics, and computer graphics. In practice these
input measures are empirical distributions built from sensitive datasets,
motivating a differentially private (DP) treatment. We present, to our
knowledge, the first algorithms for computing Wasserstein barycenters under
differential privacy. Empirically, on synthetic data, MNIST, and large-scale
U.S. population datasets, our methods produce high-quality private barycenters
with strong accuracy-privacy tradeoffs.

</details>


### [169] [Lightweight Transformer for EEG Classification via Balanced Signed Graph Algorithm Unrolling](https://arxiv.org/abs/2510.03027)
*Junyi Yao,Parham Eftekhar,Gene Cheung,Xujin Chris Liu,Yao Wang,Wei Hu*

Main category: cs.LG

TL;DR: The paper proposes a lightweight transformer-like neural network for EEG signal classification by unrolling a spectral denoising algorithm on balanced signed graphs, achieving comparable performance to deep learning methods with significantly fewer parameters.


<details>
  <summary>Details</summary>
Motivation: EEG signals have inherent anti-correlations that can be modeled by negative edges in graphs. The goal is to differentiate epilepsy patients from healthy subjects using interpretable and efficient methods.

Method: Build transformer-like neural nets by unrolling spectral denoising on balanced signed graphs. Use Lanczos approximation for efficient low-pass filtering on mapped positive graphs, with learned optimal cutoff frequency. Use two balanced signed graph denoisers to learn posterior probabilities for binary classification.

Result: The method achieves classification performance comparable to representative deep learning schemes while employing dramatically fewer parameters.

Conclusion: The proposed approach provides an effective and interpretable alternative to deep learning for EEG signal classification, leveraging the inherent anti-correlations in brain signals through balanced signed graph modeling.

Abstract: Samples of brain signals collected by EEG sensors have inherent
anti-correlations that are well modeled by negative edges in a finite graph. To
differentiate epilepsy patients from healthy subjects using collected EEG
signals, we build lightweight and interpretable transformer-like neural nets by
unrolling a spectral denoising algorithm for signals on a balanced signed graph
-- graph with no cycles of odd number of negative edges. A balanced signed
graph has well-defined frequencies that map to a corresponding positive graph
via similarity transform of the graph Laplacian matrices. We implement an ideal
low-pass filter efficiently on the mapped positive graph via Lanczos
approximation, where the optimal cutoff frequency is learned from data. Given
that two balanced signed graph denoisers learn posterior probabilities of two
different signal classes during training, we evaluate their reconstruction
errors for binary classification of EEG signals. Experiments show that our
method achieves classification performance comparable to representative deep
learning schemes, while employing dramatically fewer parameters.

</details>


### [170] [CHORD: Customizing Hybrid-precision On-device Model for Sequential Recommendation with Device-cloud Collaboration](https://arxiv.org/abs/2510.03038)
*Tianqi Liu,Kairui Fu,Shengyu Zhang,Wenyan Fan,Zhaocheng Du,Jieming Zhu,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: CHORD is a framework for customizing hybrid-precision on-device models for sequential recommendation through device-cloud collaboration, enabling personalized quantization without retraining.


<details>
  <summary>Details</summary>
Motivation: Current quantization methods for on-device deployment overlook device-specific user interests, compromising recommendation accuracy, while on-device finetuning adds computational burden through local retraining.

Method: CHORD uses channel-wise mixed-precision quantization with device-cloud collaboration, distributing models across devices and identifying user-specific critical parameters through cloud-based hypernetwork modules with multi-granularity parameter sensitivity analysis.

Result: Experiments on three real-world datasets with SASRec and Caser backbones demonstrate CHORD's accuracy, efficiency, and adaptivity, achieving dynamic model adaptation and accelerated inference without backpropagation.

Conclusion: CHORD effectively balances personalization and resource-adaptive deployment through mixed-precision quantization, eliminating costly retraining cycles while minimizing communication overhead with 2-bit per channel encoding.

Abstract: With the advancement of mobile device capabilities, deploying reranking
models directly on devices has become feasible, enabling real-time contextual
recommendations. When migrating models from cloud to devices, resource
heterogeneity inevitably necessitates model compression. Recent quantization
methods show promise for efficient deployment, yet they overlook
device-specific user interests, resulting in compromised recommendation
accuracy. While on-device finetuning captures personalized user preference, it
imposes additional computational burden through local retraining. To address
these challenges, we propose a framework for \underline{\textbf{C}}ustomizing
\underline{\textbf{H}}ybrid-precision \underline{\textbf{O}}n-device model for
sequential \underline{\textbf{R}}ecommendation with
\underline{\textbf{D}}evice-cloud collaboration (\textbf{CHORD}), leveraging
channel-wise mixed-precision quantization to simultaneously achieve
personalization and resource-adaptive deployment. CHORD distributes randomly
initialized models across heterogeneous devices and identifies user-specific
critical parameters through auxiliary hypernetwork modules on the cloud. Our
parameter sensitivity analysis operates across multiple granularities (layer,
filter, and element levels), enabling precise mapping from user profiles to
quantization strategy. Through on-device mixed-precision quantization, CHORD
delivers dynamic model adaptation and accelerated inference without
backpropagation, eliminating costly retraining cycles. We minimize
communication overhead by encoding quantization strategies using only 2 bits
per channel instead of 32-bit weights. Experiments on three real-world datasets
with two popular backbones (SASRec and Caser) demonstrate the accuracy,
efficiency, and adaptivity of CHORD.

</details>


### [171] [Bayesian E(3)-Equivariant Interatomic Potential with Iterative Restratification of Many-body Message Passing](https://arxiv.org/abs/2510.03046)
*Soohaeng Yoo Willow,Tae Hyeon Park,Gi Beom Sim,Sung Wook Moon,Seung Kyu Min,D. ChangMo Yang,Hyun Woo Kim,Juho Lee,Chang Woo Myung*

Main category: cs.LG

TL;DR: This paper develops Bayesian E(3) equivariant machine learning potentials with improved uncertainty quantification for atomistic simulations, introducing a joint energy-force negative log-likelihood loss function that enables better uncertainty prediction, out-of-distribution detection, calibration, and active learning.


<details>
  <summary>Details</summary>
Motivation: Current machine learning potentials lack reliable uncertainty quantification, limiting their use in active learning, calibration, and out-of-distribution detection, which are crucial for trustworthy atomistic simulations.

Method: Developed Bayesian E(3) equivariant MLPs with iterative restratification of many-body message passing, using a joint energy-force negative log-likelihood loss function. Systematically benchmarked multiple Bayesian approaches including deep ensembles, stochastic weight averaging Gaussian, improved variational online Newton, and Laplace approximation.

Result: The proposed method achieves competitive accuracy with state-of-the-art models while enabling superior uncertainty quantification. The NLL_JEF loss outperforms conventional losses, and Bayesian active learning by disagreement (BALD) outperforms random sampling and energy-uncertainty-based sampling.

Conclusion: Bayesian equivariant neural networks establish a powerful framework for developing uncertainty-aware machine learning potentials that enable uncertainty-guided active learning, OOD detection, and energy/forces calibration for large-scale atomistic simulations.

Abstract: Machine learning potentials (MLPs) have become essential for large-scale
atomistic simulations, enabling ab initio-level accuracy with computational
efficiency. However, current MLPs struggle with uncertainty quantification,
limiting their reliability for active learning, calibration, and
out-of-distribution (OOD) detection. We address these challenges by developing
Bayesian E(3) equivariant MLPs with iterative restratification of many-body
message passing. Our approach introduces the joint energy-force negative
log-likelihood (NLL$_\text{JEF}$) loss function, which explicitly models
uncertainty in both energies and interatomic forces, yielding superior accuracy
compared to conventional NLL losses. We systematically benchmark multiple
Bayesian approaches, including deep ensembles with mean-variance estimation,
stochastic weight averaging Gaussian, improved variational online Newton, and
laplace approximation by evaluating their performance on uncertainty
prediction, OOD detection, calibration, and active learning tasks. We further
demonstrate that NLL$_\text{JEF}$ facilitates efficient active learning by
quantifying energy and force uncertainties. Using Bayesian active learning by
disagreement (BALD), our framework outperforms random sampling and
energy-uncertainty-based sampling. Our results demonstrate that Bayesian MLPs
achieve competitive accuracy with state-of-the-art models while enabling
uncertainty-guided active learning, OOD detection, and energy/forces
calibration. This work establishes Bayesian equivariant neural networks as a
powerful framework for developing uncertainty-aware MLPs for atomistic
simulations at scale.

</details>


### [172] [ZeroShotOpt: Towards Zero-Shot Pretrained Models for Efficient Black-Box Optimization](https://arxiv.org/abs/2510.03051)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Johannes Dürholt,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: ZeroShotOpt is a pretrained model for black-box optimization that uses offline RL on BO trajectories and synthetic functions to achieve zero-shot generalization across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Bayesian optimization (BO) requires hand-tuning hyperparameters that don't generalize well across different problem landscapes, limiting its sample efficiency.

Method: Leverage offline reinforcement learning on large-scale optimization trajectories from 12 BO variants and generate millions of synthetic Gaussian process-based functions with diverse landscapes for pretraining.

Result: ZeroShotOpt achieves robust zero-shot generalization on unseen benchmarks, matching or surpassing the sample efficiency of leading global optimizers including BO.

Conclusion: The approach provides a reusable foundation for future extensions and improvements in black-box optimization.

Abstract: Global optimization of expensive, derivative-free black-box functions
requires extreme sample efficiency. While Bayesian optimization (BO) is the
current state-of-the-art, its performance hinges on surrogate and acquisition
function hyper-parameters that are often hand-tuned and fail to generalize
across problem landscapes. We present ZeroShotOpt, a general-purpose,
pretrained model for continuous black-box optimization tasks ranging from 2D to
20D. Our approach leverages offline reinforcement learning on large-scale
optimization trajectories collected from 12 BO variants. To scale pretraining,
we generate millions of synthetic Gaussian process-based functions with diverse
landscapes, enabling the model to learn transferable optimization policies. As
a result, ZeroShotOpt achieves robust zero-shot generalization on a wide array
of unseen benchmarks, matching or surpassing the sample efficiency of leading
global optimizers, including BO, while also offering a reusable foundation for
future extensions and improvements. Our open-source code, dataset, and model
are available at: https://github.com/jamisonmeindl/zeroshotopt

</details>


### [173] [Comparative Analysis of Parameterized Action Actor-Critic Reinforcement Learning Algorithms for Web Search Match Plan Generation](https://arxiv.org/abs/2510.03064)
*Ubayd Bapoo,Clement N Nyirenda*

Main category: cs.LG

TL;DR: PAGAC outperforms PASAC and PATQC in high-dimensional decision-making tasks with parametrized action spaces, achieving fastest training times and highest returns.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the performance of SAC, GAC, and TQC algorithms in high-dimensional decision-making tasks using fully observable environments with parametrized action spaces.

Method: Used parametrized action spaces to eliminate recurrent networks, tested on Platform-v0 and Goal-v0 benchmarks, performed hyperparameter optimization with Microsoft NNI, and modified codebase for GAC and TQC for reproducibility.

Result: PAGAC achieved fastest training times (41:24 for Platform, 24:04 for Robot Soccer Goal) and highest returns, demonstrating superior efficiency and reliability compared to PASAC and PATQC.

Conclusion: PAGAC provides clear advantages in complex action spaces due to its speed and stability, making it ideal for tasks requiring rapid convergence and robust performance. Future work could explore hybrid strategies combining entropy-regularization with truncation-based methods.

Abstract: This study evaluates the performance of Soft Actor Critic (SAC), Greedy Actor
Critic (GAC), and Truncated Quantile Critics (TQC) in high-dimensional
decision-making tasks using fully observable environments. The focus is on
parametrized action (PA) spaces, eliminating the need for recurrent networks,
with benchmarks Platform-v0 and Goal-v0 testing discrete actions linked to
continuous action-parameter spaces. Hyperparameter optimization was performed
with Microsoft NNI, ensuring reproducibility by modifying the codebase for GAC
and TQC. Results show that Parameterized Action Greedy Actor-Critic (PAGAC)
outperformed other algorithms, achieving the fastest training times and highest
returns across benchmarks, completing 5,000 episodes in 41:24 for the Platform
game and 24:04 for the Robot Soccer Goal game. Its speed and stability provide
clear advantages in complex action spaces. Compared to PASAC and PATQC, PAGAC
demonstrated superior efficiency and reliability, making it ideal for tasks
requiring rapid convergence and robust performance. Future work could explore
hybrid strategies combining entropy-regularization with truncation-based
methods to enhance stability and expand investigations into generalizability.

</details>


### [174] [A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem](https://arxiv.org/abs/2510.03065)
*Mingfeng Fan,Jiaqi Cheng,Yaoxin Wu,Yifeng Zhang,Yibin Yang,Guohua Wu,Guillaume Sartoretti*

Main category: cs.LG

TL;DR: UD3RL is a unified dual-decoder DRL framework for solving the close-enough TSP, separating decision-making into node selection and waypoint determination, with strong generalization across problem sizes and neighborhood radii.


<details>
  <summary>Details</summary>
Motivation: Limited attention has been given to CETSP due to the challenge of neighborhood-based visitation criterion, where traditional TSP methods don't directly apply.

Method: Formulated MDP for CETSP using discretization, proposed UD3RL with node-decoder and loc-decoder, employed k-nearest neighbors subgraph interaction strategy, and customized REINFORCE algorithm for training.

Result: UD3RL outperforms conventional methods in both solution quality and runtime, exhibits strong generalization across problem scales, spatial distributions, and radius ranges, and shows robustness to dynamic environments.

Conclusion: The proposed UD3RL framework effectively addresses CETSP challenges and demonstrates superior performance and generalization capabilities compared to existing methods.

Abstract: In recent years, deep reinforcement learning (DRL) has gained traction for
solving the NP-hard traveling salesman problem (TSP). However, limited
attention has been given to the close-enough TSP (CETSP), primarily due to the
challenge introduced by its neighborhood-based visitation criterion, wherein a
node is considered visited if the agent enters a compact neighborhood around
it. In this work, we formulate a Markov decision process (MDP) for CETSP using
a discretization scheme and propose a novel unified dual-decoder DRL (UD3RL)
framework that separates decision-making into node selection and waypoint
determination. Specifically, an adapted encoder is employed for effective
feature extraction, followed by a node-decoder and a loc-decoder to handle the
two sub-tasks, respectively. A k-nearest neighbors subgraph interaction
strategy is further introduced to enhance spatial reasoning during location
decoding. Furthermore, we customize the REINFORCE algorithm to train UD3RL as a
unified model capable of generalizing across different problem sizes and
varying neighborhood radius types (i.e., constant and random radii).
Experimental results show that UD3RL outperforms conventional methods in both
solution quality and runtime, while exhibiting strong generalization across
problem scales, spatial distributions, and radius ranges, as well as robustness
to dynamic environments.

</details>


### [175] [Bootstrap Learning for Combinatorial Graph Alignment with Sequential GNNs](https://arxiv.org/abs/2510.03086)
*Marc Lelarge*

Main category: cs.LG

TL;DR: A novel chaining procedure for graph alignment using GNNs that iteratively refines similarity matrices, achieving 3x better accuracy and solving previously unsolvable regular graphs.


<details>
  <summary>Details</summary>
Motivation: GNNs have underperformed traditional optimization methods on combinatorial problems, limiting their practical impact in graph alignment tasks.

Method: Train a sequence of GNNs where each network refines similarity matrices from previous ones, using a node-pair architecture that captures global structural patterns missed by standard message-passing networks.

Result: Achieved over 3x better accuracy than existing methods on challenging instances, uniquely solved regular graphs where all competing approaches failed, and substantially outperformed state-of-the-art solvers when combined with traditional optimization.

Conclusion: The chained GNN approach successfully bridges the performance gap between neural networks and traditional optimization methods for combinatorial graph problems.

Abstract: Graph neural networks (GNNs) have struggled to outperform traditional
optimization methods on combinatorial problems, limiting their practical
impact. We address this gap by introducing a novel chaining procedure for the
graph alignment problem, a fundamental NP-hard task of finding optimal node
correspondences between unlabeled graphs using only structural information. Our
method trains a sequence of GNNs where each network learns to iteratively
refine similarity matrices produced by previous networks. During inference,
this creates a bootstrap effect: each GNN improves upon partial solutions by
incorporating discrete ranking information about node alignment quality from
prior iterations. We combine this with a powerful architecture that operates on
node pairs rather than individual nodes, capturing global structural patterns
essential for alignment that standard message-passing networks cannot
represent. Extensive experiments on synthetic benchmarks demonstrate
substantial improvements: our chained GNNs achieve over 3x better accuracy than
existing methods on challenging instances, and uniquely solve regular graphs
where all competing approaches fail. When combined with traditional
optimization as post-processing, our method substantially outperforms
state-of-the-art solvers on the graph alignment benchmark.

</details>


### [176] [Distilled Protein Backbone Generation](https://arxiv.org/abs/2510.03095)
*Liyang Xie,Haoran Zhang,Zhendong Wang,Wesley Tansey,Mingyuan Zhou*

Main category: cs.LG

TL;DR: This paper adapts Score identity Distillation (SiD) to train few-step protein backbone generators, achieving 20x faster sampling while maintaining designability comparable to the teacher model.


<details>
  <summary>Details</summary>
Motivation: Diffusion-based protein generation models have computational bottlenecks requiring hundreds of iterative steps, limiting their practical utility in large-scale protein discovery.

Method: Adapt Score identity Distillation (SiD) with multistep generation and inference time noise modulation to train few-step protein backbone generators.

Result: Distilled models achieve 20x speedup in sampling while maintaining similar designability, diversity, and novelty as the Proteina teacher model.

Conclusion: The approach enables large-scale in silico protein design and brings diffusion-based models closer to real-world protein engineering applications.

Abstract: Diffusion- and flow-based generative models have recently demonstrated strong
performance in protein backbone generation tasks, offering unprecedented
capabilities for de novo protein design. However, while achieving notable
performance in generation quality, these models are limited by their generating
speed, often requiring hundreds of iterative steps in the reverse-diffusion
process. This computational bottleneck limits their practical utility in
large-scale protein discovery, where thousands to millions of candidate
structures are needed. To address this challenge, we explore the techniques of
score distillation, which has shown great success in reducing the number of
sampling steps in the vision domain while maintaining high generation quality.
However, a straightforward adaptation of these methods results in unacceptably
low designability. Through extensive study, we have identified how to
appropriately adapt Score identity Distillation (SiD), a state-of-the-art score
distillation strategy, to train few-step protein backbone generators which
significantly reduce sampling time, while maintaining comparable performance to
their pretrained teacher model. In particular, multistep generation combined
with inference time noise modulation is key to the success. We demonstrate that
our distilled few-step generators achieve more than a 20-fold improvement in
sampling speed, while achieving similar levels of designability, diversity, and
novelty as the Proteina teacher model. This reduction in inference cost enables
large-scale in silico protein design, thereby bringing diffusion-based models
closer to real-world protein engineering applications.

</details>


### [177] [Adaptive Node Feature Selection For Graph Neural Networks](https://arxiv.org/abs/2510.03096)
*Ali Azizpour,Madeline Navarro,Santiago Segarra*

Main category: cs.LG

TL;DR: An adaptive node feature selection method for GNNs that identifies and removes unnecessary features during training using permutation-based intervention.


<details>
  <summary>Details</summary>
Motivation: To interpret GNN decisions, reduce dimensionality, and potentially improve performance by eliminating unhelpful features in graph-structured data with complex dependencies.

Method: Model- and task-agnostic approach that determines feature relevance during training based on validation performance changes when permuting feature values, with theoretical motivation on GNN performance dependencies.

Result: Provides feature importance scores and tracks relevance evolution as features are dropped, showing flexibility across different graph architectures and adaptability to challenging graph learning settings.

Conclusion: The intervention-based approach effectively identifies relevant features in GNNs, offering interpretability, dimensionality reduction, and potential performance improvements while being architecture-agnostic.

Abstract: We propose an adaptive node feature selection approach for graph neural
networks (GNNs) that identifies and removes unnecessary features during
training. The ability to measure how features contribute to model output is key
for interpreting decisions, reducing dimensionality, and even improving
performance by eliminating unhelpful variables. However, graph-structured data
introduces complex dependencies that may not be amenable to classical feature
importance metrics. Inspired by this challenge, we present a model- and
task-agnostic method that determines relevant features during training based on
changes in validation performance upon permuting feature values. We
theoretically motivate our intervention-based approach by characterizing how
GNN performance depends on the relationships between node data and graph
structure. Not only do we return feature importance scores once training
concludes, we also track how relevance evolves as features are successively
dropped. We can therefore monitor if features are eliminated effectively and
also evaluate other metrics with this technique. Our empirical results verify
the flexibility of our approach to different graph architectures as well as its
adaptability to more challenging graph learning settings.

</details>


### [178] [AdaBet: Gradient-free Layer Selection for Efficient Training of Deep Neural Networks](https://arxiv.org/abs/2510.03101)
*Irene Tenison,Soumyajit Chatterjee,Fahim Kawsar,Mohammad Malekzadeh*

Main category: cs.LG

TL;DR: AdaBet is a gradient-free layer selection method that uses Betti Numbers to analyze activation space topology, enabling efficient on-device neural network retraining without labels or gradients.


<details>
  <summary>Details</summary>
Motivation: To enable efficient adaptation of pre-trained neural networks on edge/mobile devices with limited resources, overcoming impracticalities of full-model retraining and limitations of current layer selection methods that require labels, gradients, or server-side training.

Method: Uses Betti Numbers to analyze topological features of layer activation spaces through forward passes alone, ranking layers by learning capacity for retraining without requiring labels or gradients.

Result: On 16 model-dataset pairs, AdaBet achieves 5% higher classification accuracy than gradient-based baselines while reducing average peak memory consumption by 40%.

Conclusion: AdaBet provides an effective gradient-free approach for layer selection in on-device retraining, outperforming gradient-based methods in accuracy while significantly reducing memory requirements.

Abstract: To utilize pre-trained neural networks on edge and mobile devices, we often
require efficient adaptation to user-specific runtime data distributions while
operating under limited compute and memory resources. On-device retraining with
a target dataset can facilitate such adaptations; however, it remains
impractical due to the increasing depth of modern neural nets, as well as the
computational overhead associated with gradient-based optimization across all
layers. Current approaches reduce training cost by selecting a subset of layers
for retraining, however, they rely on labeled data, at least one full-model
backpropagation, or server-side meta-training; limiting their suitability for
constrained devices. We introduce AdaBet, a gradient-free layer selection
approach to rank important layers by analyzing topological features of their
activation spaces through Betti Numbers and using forward passes alone. AdaBet
allows selecting layers with high learning capacity, which are important for
retraining and adaptation, without requiring labels or gradients. Evaluating
AdaBet on sixteen pairs of benchmark models and datasets, shows AdaBet achieves
an average gain of 5% more classification accuracy over gradient-based
baselines while reducing average peak memory consumption by 40%.

</details>


### [179] [Real Time Headway Predictions in Urban Rail Systems and Implications for Service Control: A Deep Learning Approach](https://arxiv.org/abs/2510.03121)
*Muhammad Usama,Haris Koutsopoulos*

Main category: cs.LG

TL;DR: A ConvLSTM-based deep learning framework for predicting train headway propagation in metro systems, enabling dispatchers to evaluate terminal headway control decisions efficiently.


<details>
  <summary>Details</summary>
Motivation: Need for efficient real-time dispatching to ensure service reliability, maximize resource utilization, and improve passenger satisfaction in urban metro systems.

Method: Uses Convolutional LSTM model incorporating planned terminal headways and historical data to forecast spatiotemporal headway dynamics across stations, with flexible methodology to simulate dispatcher strategies.

Result: Demonstrates promising headway predictions on large-scale metro dataset, providing actionable insights for real-time decision-making with computational efficiency.

Conclusion: Provides rail operators with a powerful tool to optimize dispatching strategies, significantly improving service consistency and passenger satisfaction through proactive operational control.

Abstract: Efficient real-time dispatching in urban metro systems is essential for
ensuring service reliability, maximizing resource utilization, and improving
passenger satisfaction. This study presents a novel deep learning framework
centered on a Convolutional Long Short-Term Memory (ConvLSTM) model designed to
predict the complex spatiotemporal propagation of train headways across an
entire metro line. By directly incorporating planned terminal headways as a
critical input alongside historical headway data, the proposed model accurately
forecasts future headway dynamics, effectively capturing both their temporal
evolution and spatial dependencies across all stations. This capability
empowers dispatchers to evaluate the impact of various terminal headway control
decisions without resorting to computationally intensive simulations. We
introduce a flexible methodology to simulate diverse dispatcher strategies,
ranging from maintaining even headways to implementing custom patterns derived
from observed terminal departures. In contrast to existing research primarily
focused on passenger load predictioning or atypical disruption scenarios, our
approach emphasizes proactive operational control. Evaluated on a large-scale
dataset from an urban metro line, the proposed ConvLSTM model demonstrates
promising headway predictions, offering actionable insights for real-time
decision-making. This framework provides rail operators with a powerful,
computationally efficient tool to optimize dispatching strategies, thereby
significantly improving service consistency and passenger satisfaction.

</details>


### [180] [Signature-Informed Transformer for Asset Allocation](https://arxiv.org/abs/2510.03129)
*Yoontae Hwang,Stefan Zohren*

Main category: cs.LG

TL;DR: SIT is a novel deep learning framework for asset allocation that uses path signatures and signature-augmented attention to directly optimize risk-aware financial objectives, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address the failure of deep-learning forecasters in robust asset allocation due to objective mismatch and error amplification in quantitative finance.

Method: Signature-Informed Transformer (SIT) with path signatures for geometric representation of asset dynamics and signature-augmented attention mechanism embedding financial inductive biases like lead-lag effects.

Result: SIT decisively outperforms traditional and deep-learning baselines on daily S&P 100 equity data, especially compared to predict-then-optimize models.

Conclusion: Portfolio-aware objectives and geometry-aware inductive biases are essential for risk-aware capital allocation in machine-learning systems.

Abstract: Robust asset allocation is a key challenge in quantitative finance, where
deep-learning forecasters often fail due to objective mismatch and error
amplification. We introduce the Signature-Informed Transformer (SIT), a novel
framework that learns end-to-end allocation policies by directly optimizing a
risk-aware financial objective. SIT's core innovations include path signatures
for a rich geometric representation of asset dynamics and a signature-augmented
attention mechanism embedding financial inductive biases, like lead-lag
effects, into the model. Evaluated on daily S\&P 100 equity data, SIT
decisively outperforms traditional and deep-learning baselines, especially when
compared to predict-then-optimize models. These results indicate that
portfolio-aware objectives and geometry-aware inductive biases are essential
for risk-aware capital allocation in machine-learning systems. The code is
available at:
https://github.com/Yoontae6719/Signature-Informed-Transformer-For-Asset-Allocation

</details>


### [181] [Enhancing XAI Narratives through Multi-Narrative Refinement and Knowledge Distillation](https://arxiv.org/abs/2510.03134)
*Flavio Giorgi,Matteo Silvestri,Cesare Campagnano,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.LG

TL;DR: A pipeline using Language Models to generate narrative counterfactual explanations, with knowledge distillation to make small models perform like large ones, plus an evaluation method for verifying narrative accuracy.


<details>
  <summary>Details</summary>
Motivation: Counterfactual explanations are promising but often too technical for non-experts to understand, creating a barrier to practical adoption of explainable AI.

Method: Proposed pipeline leverages Language Models (large and small) with knowledge distillation and refining mechanisms to compose narratives for counterfactual explanations, plus a simple evaluation method to verify narrative accuracy.

Result: The pipeline enhances reasoning capabilities and practical performance of student models, making small language models perform comparably to larger counterparts while maintaining robust reasoning.

Conclusion: The proposed approach makes counterfactual explanations more accessible to non-experts and suitable for real-world applications by improving narrative generation and model performance.

Abstract: Explainable Artificial Intelligence has become a crucial area of research,
aiming to demystify the decision-making processes of deep learning models.
Among various explainability techniques, counterfactual explanations have been
proven particularly promising, as they offer insights into model behavior by
highlighting minimal changes that would alter a prediction. Despite their
potential, these explanations are often complex and technical, making them
difficult for non-experts to interpret. To address this challenge, we propose a
novel pipeline that leverages Language Models, large and small, to compose
narratives for counterfactual explanations. We employ knowledge distillation
techniques along with a refining mechanism to enable Small Language Models to
perform comparably to their larger counterparts while maintaining robust
reasoning abilities. In addition, we introduce a simple but effective
evaluation method to assess natural language narratives, designed to verify
whether the models' responses are in line with the factual, counterfactual
ground truth. As a result, our proposed pipeline enhances both the reasoning
capabilities and practical performance of student models, making them more
suitable for real-world use cases.

</details>


### [182] [Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking](https://arxiv.org/abs/2510.03149)
*Dhruv Rohatgi,Abhishek Shetty,Donya Saless,Yuchen Li,Ankur Moitra,Andrej Risteski,Dylan J. Foster*

Main category: cs.LG

TL;DR: VGB is a new test-time sampling algorithm that uses probabilistic backtracking to improve robustness against verifier errors in language model generation, outperforming baselines on synthetic and real tasks.


<details>
  <summary>Details</summary>
Motivation: Standard decoding techniques suffer catastrophic failures due to error amplification from seemingly benign verifier errors, motivating the need for more sophisticated decoding strategies.

Method: VGB interprets autoregressive generation as a random walk on a tree of partial generations with transition probabilities guided by process verifier and base model, using theoretically grounded probabilistic backtracking that generalizes the Sinclair-Jerrum random walk.

Result: Empirical evaluation on synthetic and real language modeling tasks shows VGB outperforms baselines on various metrics.

Conclusion: VGB provides provably better robustness to verifier errors through probabilistic backtracking, demonstrating improved performance over standard approaches.

Abstract: Test-time algorithms that combine the generative power of language models
with process verifiers that assess the quality of partial generations offer a
promising lever for eliciting new reasoning capabilities, but the algorithmic
design space and computational scaling properties of such approaches are still
opaque, and their benefits are far from apparent when one accounts for the cost
of learning a high-quality verifier. Our starting point is the observation that
seemingly benign errors in a learned verifier can lead to catastrophic failures
for standard decoding techniques due to error amplification during the course
of generation. We then ask: can this be improved with more sophisticated
decoding strategies?
  We introduce a new process-guided test-time sampling algorithm, VGB, which
uses theoretically grounded backtracking to achieve provably better robustness
to verifier errors. VGB interprets autoregressive generation as a random walk
on a tree of partial generations, with transition probabilities guided by the
process verifier and base model; crucially, backtracking occurs
probabilistically. This process generalizes the seminal Sinclair-Jerrum random
walk (Sinclair & Jerrum, 1989) from the literature on approximate counting and
sampling in theoretical computer science, and a conceptual contribution of our
work is to highlight parallels with this literature. Empirically, we
demonstrate on both synthetic and real language modeling tasks that VGB
outperforms baselines on a variety of metrics.

</details>


### [183] [Mixture of Many Zero-Compute Experts: A High-Rate Quantization Theory Perspective](https://arxiv.org/abs/2510.03151)
*Yehuda Dar*

Main category: cs.LG

TL;DR: This paper applies high-rate quantization theory to analyze mixture-of-experts (MoE) models for regression, focusing on approximation error analysis and the tradeoff between approximation and estimation errors based on the number of experts.


<details>
  <summary>Details</summary>
Motivation: To provide new theoretical insights into MoE models using classical high-rate quantization theory, particularly for understanding the error behavior and optimization in regression tasks.

Method: Defines an MoE model with input-space segmentation and single-parameter experts acting as constant predictors. Uses high-rate quantization assumptions (large number of experts with small regions) to analyze approximation error for 1D and multidimensional inputs, and studies statistical learning properties of expert parameter estimation.

Result: Formulates test error minimization for 1D inputs, provides upper bound for multidimensional inputs, and analyzes the learning of expert parameters from training data. Shows theoretical and empirical evidence of the approximation-estimation error tradeoff dependence on the number of experts.

Conclusion: The study demonstrates how high-rate quantization theory can provide valuable theoretical framework for understanding MoE models, revealing the fundamental tradeoff between approximation and estimation errors that depends on the number of experts used in the model.

Abstract: This paper uses classical high-rate quantization theory to provide new
insights into mixture-of-experts (MoE) models for regression tasks. Our MoE is
defined by a segmentation of the input space to regions, each with a
single-parameter expert that acts as a constant predictor with zero-compute at
inference. Motivated by high-rate quantization theory assumptions, we assume
that the number of experts is sufficiently large to make their input-space
regions very small. This lets us to study the approximation error of our MoE
model class: (i) for one-dimensional inputs, we formulate the test error and
its minimizing segmentation and experts; (ii) for multidimensional inputs, we
formulate an upper bound for the test error and study its minimization.
Moreover, we consider the learning of the expert parameters from a training
dataset, given an input-space segmentation, and formulate their statistical
learning properties. This leads us to theoretically and empirically show how
the tradeoff between approximation and estimation errors in MoE learning
depends on the number of experts.

</details>


### [184] [Calibrated Uncertainty Sampling for Active Learning](https://arxiv.org/abs/2510.03162)
*Ha Manh Bui,Iliana Maifeld-Carucci,Anqi Liu*

Main category: cs.LG

TL;DR: Proposes a new active learning acquisition function that estimates calibration errors to select samples, addressing the issue of uncalibrated uncertainty in deep neural networks.


<details>
  <summary>Details</summary>
Motivation: Traditional uncertainty-based acquisition functions in active learning are affected by uncalibrated model uncertainty, especially in deep neural networks, leading to poor generalization and high calibration error.

Method: Uses kernel calibration error estimator under covariate shift to identify samples with highest calibration error before applying DNN uncertainty, formally proving bounded calibration error.

Result: Empirically achieves lower calibration and generalization errors compared to other acquisition function baselines across pool-based active learning settings.

Conclusion: The proposed calibration-aware acquisition function effectively addresses uncalibrated uncertainty issues in active learning, leading to improved model performance.

Abstract: We study the problem of actively learning a classifier with a low calibration
error. One of the most popular Acquisition Functions (AFs) in pool-based Active
Learning (AL) is querying by the model's uncertainty. However, we recognize
that an uncalibrated uncertainty model on the unlabeled pool may significantly
affect the AF effectiveness, leading to sub-optimal generalization and high
calibration error on unseen data. Deep Neural Networks (DNNs) make it even
worse as the model uncertainty from DNN is usually uncalibrated. Therefore, we
propose a new AF by estimating calibration errors and query samples with the
highest calibration error before leveraging DNN uncertainty. Specifically, we
utilize a kernel calibration error estimator under the covariate shift and
formally show that AL with this AF eventually leads to a bounded calibration
error on the unlabeled pool and unseen test data. Empirically, our proposed
method surpasses other AF baselines by having a lower calibration and
generalization error across pool-based AL settings.

</details>


### [185] [Why Do We Need Warm-up? A Theoretical Perspective](https://arxiv.org/abs/2510.03164)
*Foivos Alimisis,Rustem Islamov,Aurelien Lucchi*

Main category: cs.LG

TL;DR: This paper provides a principled theoretical explanation for why learning rate warm-up improves deep learning training, establishing both upper and lower complexity bounds for convergence.


<details>
  <summary>Details</summary>
Motivation: Learning rate warm-up is widely used in deep learning but lacks theoretical foundations. The authors aim to provide a principled explanation for why warm-up improves training.

Method: The authors use a generalized (L0, L1)-smoothness condition that bounds local curvature as a linear function of loss sub-optimality. They prove convergence bounds for Gradient Descent with warm-up schedules under this assumption.

Result: The paper demonstrates theoretically and empirically that the smoothness condition holds for common neural architectures with MSE and cross-entropy losses. Warm-up schedules achieve faster convergence than fixed step-sizes.

Conclusion: Warm-up schedules provide practical benefits for training language and vision models, and the theoretical framework explains why they improve convergence in deep learning.

Abstract: Learning rate warm-up - increasing the learning rate at the beginning of
training - has become a ubiquitous heuristic in modern deep learning, yet its
theoretical foundations remain poorly understood. In this work, we provide a
principled explanation for why warm-up improves training. We rely on a
generalization of the $(L_0, L_1)$-smoothness condition, which bounds local
curvature as a linear function of the loss sub-optimality and exhibits
desirable closure properties. We demonstrate both theoretically and empirically
that this condition holds for common neural architectures trained with
mean-squared error and cross-entropy losses. Under this assumption, we prove
that Gradient Descent with a warm-up schedule achieves faster convergence than
with a fixed step-size, establishing upper and lower complexity bounds.
Finally, we validate our theoretical insights through experiments on language
and vision models, confirming the practical benefits of warm-up schedules.

</details>


### [186] [FTTE: Federated Learning on Resource-Constrained Devices](https://arxiv.org/abs/2510.03165)
*Irene Tenison,Anna Murphy,Charles Beauville,Lalana Kagal*

Main category: cs.LG

TL;DR: FTTE is a semi-asynchronous federated learning framework that uses sparse parameter updates and staleness-weighted aggregation to achieve faster convergence, lower memory usage, and reduced communication overhead compared to traditional FL methods.


<details>
  <summary>Details</summary>
Motivation: Federated learning on resource-constrained edge devices faces challenges including limited memory, energy, communication bandwidth, and straggler-induced delays in heterogeneous networks.

Method: FTTE employs sparse parameter updates and a staleness-weighted aggregation mechanism that considers both the age and variance of client updates in a semi-asynchronous framework.

Result: FTTE achieves 81% faster convergence, 80% lower on-device memory usage, 69% communication payload reduction compared to synchronous FL, and reaches comparable or higher accuracy than semi-asynchronous methods in challenging scenarios with up to 500 clients and 90% stragglers.

Conclusion: FTTE establishes itself as the first practical and scalable solution for real-world federated learning deployments on heterogeneous and resource-constrained edge devices.

Abstract: Federated learning (FL) enables collaborative model training across
distributed devices while preserving data privacy, but deployment on
resource-constrained edge nodes remains challenging due to limited memory,
energy, and communication bandwidth. Traditional synchronous and asynchronous
FL approaches further suffer from straggler induced delays and slow convergence
in heterogeneous, large scale networks. We present FTTE (Federated Tiny
Training Engine),a novel semi-asynchronous FL framework that uniquely employs
sparse parameter updates and a staleness-weighted aggregation based on both age
and variance of client updates. Extensive experiments across diverse models and
data distributions - including up to 500 clients and 90% stragglers -
demonstrate that FTTE not only achieves 81% faster convergence, 80% lower
on-device memory usage, and 69% communication payload reduction than
synchronous FL (eg.FedAVG), but also consistently reaches comparable or higher
target accuracy than semi-asynchronous (eg.FedBuff) in challenging regimes.
These results establish FTTE as the first practical and scalable solution for
real-world FL deployments on heterogeneous and predominantly
resource-constrained edge devices.

</details>


### [187] [Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.03181)
*Ha Manh Bui,Felix Parker,Kimia Ghobadi,Anqi Liu*

Main category: cs.LG

TL;DR: Proposed Density-QUCB (DQUCB), a shift-aware Q-learning UCB algorithm that detects distribution shifts using transition density functions and improves regret guarantees in non-stationary RL environments.


<details>
  <summary>Details</summary>
Motivation: Address the problem of policy exploiting sub-optimal rewards after distribution shifts in non-stationary RL, where QUCB fails to maintain optimal performance when environment dynamics change.

Method: Uses transition density function to detect distribution shifts and leverages likelihood to enhance uncertainty estimation in Q-learning UCB, achieving better exploration-exploitation balance.

Result: Theoretical proof that oracle DQUCB achieves better regret guarantee than QUCB. Empirical results show lower regret across RL tasks and real-world COVID-19 patient allocation using Deep-Q-learning.

Conclusion: DQUCB effectively handles non-stationary RL with distribution shifts, providing computational efficiency of model-free RL while outperforming baselines in both theoretical guarantees and practical applications.

Abstract: We study the Non-Stationary Reinforcement Learning (RL) under distribution
shifts in both finite-horizon episodic and infinite-horizon discounted Markov
Decision Processes (MDPs). In the finite-horizon case, the transition functions
may suddenly change at a particular episode. In the infinite-horizon setting,
such changes can occur at an arbitrary time step during the agent's interaction
with the environment. While the Q-learning Upper Confidence Bound algorithm
(QUCB) can discover a proper policy during learning, due to the distribution
shifts, this policy can exploit sub-optimal rewards after the shift happens. To
address this issue, we propose Density-QUCB (DQUCB), a shift-aware
Q-learning~UCB algorithm, which uses a transition density function to detect
distribution shifts, then leverages its likelihood to enhance the uncertainty
estimation quality of Q-learning~UCB, resulting in a balance between
exploration and exploitation. Theoretically, we prove that our oracle DQUCB
achieves a better regret guarantee than QUCB. Empirically, our DQUCB enjoys the
computational efficiency of model-free RL and outperforms QUCB baselines by
having a lower regret across RL tasks, as well as a real-world COVID-19 patient
hospital allocation task using a Deep-Q-learning architecture.

</details>


### [188] [PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning](https://arxiv.org/abs/2510.03185)
*Wanjia Zhao,Qinwei Ma,Jingzhe Shi,Shirley Wu,Jiaqi Han,Yijia Xiao,Si-Yuan Chen,Xiao Luo,Ludwig Schmidt,James Zou*

Main category: cs.LG

TL;DR: PRISM-Physics is a new evaluation framework for physics reasoning that uses directed acyclic graphs (DAGs) to represent solution processes, enabling fine-grained, interpretable scoring of intermediate steps with theoretical guarantees and symbolic validation.


<details>
  <summary>Details</summary>
Motivation: Existing physics benchmarks mainly evaluate final answers, failing to capture reasoning processes, while stepwise methods rely on unreliable heuristic scoring or restrictive linear assumptions, limiting diagnostic validity.

Method: Represent solutions as DAGs of formulas encoding causal dependencies, develop rule-based symbolic formula equivalence matching, and prove optimality of DAG representation and scoring policy.

Result: The framework aligns better with human expert scoring, reveals persistent reasoning failures in state-of-the-art LLMs, and provides diagnostic insights through step-level scoring.

Conclusion: PRISM-Physics provides a principled foundation for process-level evaluation in physics, combining structural rigor, theoretical guarantees, and symbolic validation to advance scientific reasoning capabilities.

Abstract: Benchmarks for competition-style reasoning have advanced evaluation in
mathematics and programming, yet physics remains comparatively explored. Most
existing physics benchmarks evaluate only final answers, which fail to capture
reasoning processes, while recent stepwise methods rely on heuristic
LLM-as-judge scoring or restrictive linear assumptions, limiting reliability
and diagnostic validity. We introduce PRISM-Physics, a process-level evaluation
framework and benchmark for complex physics reasoning problems. Solutions are
represented as directed acyclic graphs (DAGs) of formulas, explicitly encoding
causal dependencies among intermediate steps to enable fine-grained,
interpretable, and theoretically grounded scoring. We prove the optimality of
the DAG representation and the corresponding scoring policy. Combining with a
fully rule-based method for symbolic formula equivalence matching that we
developed, we ensure consistent validation across diverse formulations without
heuristic judgments. Results show that our evaluation framework is more aligned
with human experts' scoring. Experiments on state-of-the-art LLMs reveal
persistent reasoning failures in physics, while step-level scoring offers both
diagnostic insight and rich signals for later training. By combining structural
rigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides
a principled foundation for advancing process-level evaluation and guiding the
development of models with deeper scientific reasoning capabilities.

</details>


### [189] [Superposition disentanglement of neural representations reveals hidden alignment](https://arxiv.org/abs/2510.03186)
*André Longon,David Klindt,Meenakshi Khosla*

Main category: cs.LG

TL;DR: The paper investigates how superposition (single neurons representing multiple features) affects alignment metrics between neural networks. It shows that different superposition arrangements can artificially lower alignment scores, and that disentangling superposition through sparse autoencoders improves alignment measurements.


<details>
  <summary>Details</summary>
Motivation: To understand whether superposition interferes with representational alignment metrics in undesirable ways, particularly when models represent the same features but in different superposition arrangements.

Method: Developed theoretical framework for permutation metrics' dependence on superposition, trained sparse autoencoders (SAEs) to disentangle superposition in toy models, and tested linear regression alignment in DNN→DNN and DNN→brain mappings in visual domain.

Result: Alignment scores typically increased when base neurons were replaced with sparse overcomplete latent codes. Similar increases observed for DNN→DNN and DNN→brain linear regression alignment in visual domain.

Conclusion: Superposition disentanglement is necessary for mapping metrics to uncover the true representational alignment between neural codes.

Abstract: The superposition hypothesis states that a single neuron within a population
may participate in the representation of multiple features in order for the
population to represent more features than the number of neurons. In
neuroscience and AI, representational alignment metrics measure the extent to
which different deep neural networks (DNNs) or brains represent similar
information. In this work, we explore a critical question: \textit{does
superposition interact with alignment metrics in any undesirable way?} We
hypothesize that models which represent the same features in \textit{different
superposition arrangements}, i.e., their neurons have different linear
combinations of the features, will interfere with predictive mapping metrics
(semi-matching, soft-matching, linear regression), producing lower alignment
than expected. We first develop a theory for how the strict permutation metrics
are dependent on superposition arrangements. This is tested by training sparse
autoencoders (SAEs) to disentangle superposition in toy models, where alignment
scores are shown to typically increase when a model's base neurons are replaced
with its sparse overcomplete latent codes. We find similar increases for
DNN\(\rightarrow\)DNN and DNN\(\rightarrow\)brain linear regression alignment
in the visual domain. Our results suggest that superposition disentanglement is
necessary for mapping metrics to uncover the true representational alignment
between neural codes.

</details>


### [190] [Estimation of Resistance Training RPE using Inertial Sensors and Electromyography](https://arxiv.org/abs/2510.03197)
*James Thomas,Johan Walhström*

Main category: cs.LG

TL;DR: Machine learning models using wearable sensors can estimate RPE during bicep curls with 85.9% accuracy within ±1 RPE unit, with random forest performing best and eccentric repetition time being the strongest predictor.


<details>
  <summary>Details</summary>
Motivation: Accurate RPE estimation can enhance resistance training through personalized feedback and injury prevention.

Method: Used wearable inertial and EMG sensors to collect data from 69 sets and over 1000 repetitions during single-arm dumbbell bicep curls, extracted statistical features, and evaluated various machine learning models.

Result: Random forest classifier achieved highest performance: 41.4% exact accuracy and 85.9% ±1 RPE accuracy. EMG data provided slight improvement over inertial sensors alone. Eccentric repetition time was identified as the strongest RPE predictor.

Conclusion: Wearable-sensor-based RPE estimation is feasible, but challenges remain in improving model generalizability, particularly regarding EMG data quality and placement sensitivity.

Abstract: Accurate estimation of rating of perceived exertion (RPE) can enhance
resistance training through personalized feedback and injury prevention. This
study investigates the application of machine learning models to estimate RPE
during single-arm dumbbell bicep curls, using data from wearable inertial and
electromyography (EMG) sensors. A custom dataset of 69 sets and over 1000
repetitions was collected, with statistical features extracted for model
training. Among the models evaluated, a random forest classifier achieved the
highest performance, with 41.4% exact accuracy and 85.9% $\pm1$ RPE accuracy.
While the inclusion of EMG data slightly improved model accuracy over inertial
sensors alone, its utility may have been limited by factors such as data
quality and placement sensitivity. Feature analysis highlighted eccentric
repetition time as the strongest RPE predictor. The results demonstrate the
feasibility of wearable-sensor-based RPE estimation and identify key challenges
for improving model generalizability.

</details>


### [191] [Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling](https://arxiv.org/abs/2510.03199)
*Qiwei Di,Kaixuan Ji,Xuheng Li,Heyang Zhao,Quanquan Gu*

Main category: cs.LG

TL;DR: Proposes Best-of-Majority (BoM) strategy for LLM inference that combines advantages of majority voting and Best-of-N, achieving minimax optimal regret in Pass@k settings.


<details>
  <summary>Details</summary>
Motivation: Single-shot selection strategies like majority voting and Best-of-N underperform on difficult tasks in Pass@k inference settings, and neither exhibits desirable scaling with k and sampling budget N.

Method: BoM restricts candidates to high-frequency responses from N samples before selecting top-k rewards, combining frequency-based filtering with reward optimization.

Result: BoM achieves regret O(ε_opt + √(ε_RM²C*/k)) with sampling budget N=Ω(C*), proven minimax optimal, and outperforms both majority voting and BoN in math problem experiments.

Conclusion: BoM provides a superior inference strategy that maintains performance when increasing N, unlike existing methods, and achieves optimal theoretical guarantees for Pass@k settings.

Abstract: LLM inference often generates a batch of candidates for a prompt and selects
one via strategies like majority voting or Best-of- N (BoN). For difficult
tasks, this single-shot selection often underperforms. Consequently,
evaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,
and only the best of them is used when computing regret. Motivated by this, we
study inference scaling in the more general Pass@$k$ inference setting, and
prove that neither majority voting nor BoN exhibits the desirable scaling with
$k$ and the sampling budget $N$. Combining the advantages of majority voting
and BoN, we propose a new inference strategy called Best-of-Majority (BoM),
with a pivotal step that restricts the candidates to the responses with high
frequency in the $N$ samples before selecting the top-$k$ rewards. We prove
that when the sampling budget is $N=\tilde\Omega(C^*)$, the regret of BoM is
$O(\epsilon_{\mathrm{opt}}+\sqrt{\epsilon_{\mathrm{RM}}^2C^*/k})$, where $C^*$
is the coverage coefficient, $\epsilon_{\mathrm{RM}}$ is the estimation error
of the reward model, and $\epsilon_{\mathrm{opt}}$ is the estimation error of
reward at the optimal response. We further establish a matching lower bound,
certifying that our algorithm is minimax optimal. Beyond optimality, BoM has a
key advantage: unlike majority voting and BoN, its performance does not degrade
when increasing $N$. Experimental results of inference on math problems show
BoM outperforming both majority voting and BoN.

</details>


### [192] [To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable Reinforcement Learning](https://arxiv.org/abs/2510.03207)
*Yuda Song,Dhruv Rohatgi,Aarti Singh,J. Andrew Bagnell*

Main category: cs.LG

TL;DR: This paper analyzes the trade-off between privileged expert distillation and standard RL in partially observable environments, finding that the effectiveness depends on latent dynamics stochasticity and that the optimal latent policy isn't always the best to distill.


<details>
  <summary>Details</summary>
Motivation: Partial observability is challenging in RL, and while privileged expert distillation (using latent state info during training) is efficient, it has known failure modes. The paper aims to understand the trade-offs between this approach and standard RL.

Method: Uses a theoretical model called perturbed Block MDP and controlled experiments on simulated locomotion tasks to compare privileged expert distillation with standard RL without privileged information.

Result: Found that (1) the trade-off depends on latent dynamics stochasticity, and (2) the optimal latent policy is not always the best one to distill for imitation learning.

Conclusion: Provides new guidelines for effectively using privileged information, potentially advancing policy learning efficiency in partially observable domains.

Abstract: Partial observability is a notorious challenge in reinforcement learning
(RL), due to the need to learn complex, history-dependent policies. Recent
empirical successes have used privileged expert distillation--which leverages
availability of latent state information during training (e.g., from a
simulator) to learn and imitate the optimal latent, Markovian policy--to
disentangle the task of "learning to see" from "learning to act". While expert
distillation is more computationally efficient than RL without latent state
information, it also has well-documented failure modes. In this paper--through
a simple but instructive theoretical model called the perturbed Block MDP, and
controlled experiments on challenging simulated locomotion tasks--we
investigate the algorithmic trade-off between privileged expert distillation
and standard RL without privileged information. Our main findings are: (1) The
trade-off empirically hinges on the stochasticity of the latent dynamics, as
theoretically predicted by contrasting approximate decodability with belief
contraction in the perturbed Block MDP; and (2) The optimal latent policy is
not always the best latent policy to distill. Our results suggest new
guidelines for effectively exploiting privileged information, potentially
advancing the efficiency of policy learning across many practical partially
observable domains.

</details>


### [193] [Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2510.03222)
*Guanhua Huang,Tingqiang Xu,Mingze Wang,Qi Yi,Xue Gong,Siheng Li,Ruibin Xiong,Kejiao Li,Yuhao Jiang,Bo Zhou*

Main category: cs.LG

TL;DR: RLVR faces exploration collapse due to elimination of valuable low-probability tokens (reasoning sparks). Lp-Reg introduces regularization using a heuristic proxy distribution to protect these tokens, enabling stable training and achieving 60.17% accuracy on math benchmarks.


<details>
  <summary>Details</summary>
Motivation: RLVR training suffers from performance plateaus as policy entropy collapses, losing exploration. Previous methods maintain high entropy but risk amplifying irrelevant tokens. The key issue is the systematic elimination of valuable low-probability exploratory tokens (reasoning sparks).

Method: Introduces Low-probability Regularization (Lp-Reg) that regularizes policy towards a heuristic proxy distribution. The proxy filters out noise tokens and re-normalizes over remaining candidates, amplifying reasoning sparks probability, then uses KL divergence as soft regularization target.

Result: Lp-Reg enables stable on-policy training for ~1,000 steps where baseline methods collapse. Achieves 60.17% average accuracy on five math benchmarks, improving 2.66% over prior methods.

Conclusion: Protecting reasoning sparks through selective regularization enables sustained exploration in RLVR, overcoming training collapse and achieving state-of-the-art performance on complex reasoning tasks.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large
Language Models in complex reasoning, yet its scalability is often hindered by
a training bottleneck where performance plateaus as policy entropy collapses,
signaling a loss of exploration. Previous methods typically address this by
maintaining high policy entropy, yet the precise mechanisms that govern
meaningful exploration have remained underexplored. Our analysis suggests that
an unselective focus on entropy risks amplifying irrelevant tokens and
destabilizing training. This paper investigates the exploration dynamics within
RLVR and identifies a key issue: the gradual elimination of valuable
low-probability exploratory tokens, which we term \textbf{\textit{reasoning
sparks}}. We find that while abundant in pre-trained models, these sparks are
systematically extinguished during RLVR due to over-penalization, leading to a
degeneracy in exploration. To address this, we introduce Low-probability
Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a
heuristic proxy distribution. This proxy is constructed by filtering out
presumed noise tokens and re-normalizing the distribution over the remaining
candidates. The result is a less-noisy proxy where the probability of
\textit{reasoning sparks} is amplified, which then serves as a soft
regularization target to shield these valuable tokens from elimination via KL
divergence. Experiments show that Lp-Reg enables stable on-policy training for
around 1,000 steps, a regime where baseline entropy-control methods collapse.
This sustained exploration leads to state-of-the-art performance, achieving a
$60.17\%$ average accuracy on five math benchmarks, an improvement of $2.66\%$
over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [194] [Hybrid Horizons: Policy for Post-Quantum Security](https://arxiv.org/abs/2510.02317)
*Anais Jaikissoon*

Main category: cs.CR

TL;DR: This paper identifies regulatory gaps in hybrid cryptography needed for the transition from classical to quantum cryptography, and proposes solutions to ensure a safe and effective transition.


<details>
  <summary>Details</summary>
Motivation: The expansion of AI and emerging quantum cryptography technologies lack proper regulation, creating risks of misuse. Hybrid cryptography is essential for the transition to quantum cryptography but has no regulatory infrastructure.

Method: The paper explores regulatory gaps in hybrid cryptography and analyzes the current lack of oversight in both AI and quantum technologies.

Result: Identified critical regulatory gaps in hybrid cryptography infrastructure that could hinder the safe transition from classical to quantum cryptography.

Conclusion: Solutions are needed to fix regulatory gaps in hybrid cryptography to ensure the safe and effective transition to quantum cryptography within the next 10 years.

Abstract: The Age of Artificial Intelligence is here. In 2025, there are few
regulations governing artificial intelligence. While the expansion of
artificial intelligence is going in a relatively good direction, there is a
risk that it can be misused. Misuse of technology is nothing new and will
continue to happen. The lack of regulation in artificial intelligence is
necessary because it raises the question of how we can move forward without
knowing what the limits are. While artificial intelligence dominates the
technology industry, new technology is starting to emerge. Quantum cryptography
is expected to replace classical cryptography; however, the transition from
classical to quantum cryptography is expected to occur within the next 10
years. The ability to transition from classical to quantum cryptography
requires hybrid cryptography. Hybrid cryptography can be used now; however,
similar to artificial intelligence, there is no regulation or support for the
regulatory infrastructure regarding hybrid machines. This paper will explore
the regulatory gaps in hybrid cryptography. The paper will also offer solutions
to fix the gaps and ensure the transition from classical to quantum
cryptography is safely and effectively completed.

</details>


### [195] [Modeling the Attack: Detecting AI-Generated Text by Quantifying Adversarial Perturbations](https://arxiv.org/abs/2510.02319)
*Lekkala Sai Teja,Annepaka Yadagiri,Sangam Sai Anish,Siva Gopala Krishna Nuthakki,Partha Pakray*

Main category: cs.CR

TL;DR: This paper introduces PIFE, a novel AI-generated text detection framework that transforms text into standardized form and measures transformation magnitude to detect adversarial paraphrasing attacks, achieving significantly better robustness than conventional adversarial training.


<details>
  <summary>Details</summary>
Motivation: Large Language Models create dual-use problems requiring reliable detection systems, but current detectors are vulnerable to adversarial attacks like paraphrasing that evade statistical detection.

Method: Proposed Perturbation-Invariant Feature Engineering (PIFE) framework that normalizes input text through multi-stage pipeline, quantifies transformation magnitude using metrics like Levenshtein distance and semantic similarity, and feeds these signals to classifier.

Result: Conventional adversarial training fails against semantic attacks (48.8% TPR at 1% FPR), while PIFE maintains 82.6% TPR under same conditions, effectively neutralizing sophisticated semantic attacks.

Conclusion: Explicitly modeling perturbation artifacts rather than merely training on them provides genuine robustness in adversarial arms race against AI-generated text detection evasion.

Abstract: The growth of highly advanced Large Language Models (LLMs) constitutes a huge
dual-use problem, making it necessary to create dependable AI-generated text
detection systems. Modern detectors are notoriously vulnerable to adversarial
attacks, with paraphrasing standing out as an effective evasion technique that
foils statistical detection. This paper presents a comparative study of
adversarial robustness, first by quantifying the limitations of standard
adversarial training and then by introducing a novel, significantly more
resilient detection framework: Perturbation-Invariant Feature Engineering
(PIFE), a framework that enhances detection by first transforming input text
into a standardized form using a multi-stage normalization pipeline, it then
quantifies the transformation's magnitude using metrics like Levenshtein
distance and semantic similarity, feeding these signals directly to the
classifier. We evaluate both a conventionally hardened Transformer and our
PIFE-augmented model against a hierarchical taxonomy of character-, word-, and
sentence-level attacks. Our findings first confirm that conventional
adversarial training, while resilient to syntactic noise, fails against
semantic attacks, an effect we term "semantic evasion threshold", where its
True Positive Rate at a strict 1% False Positive Rate plummets to 48.8%. In
stark contrast, our PIFE model, which explicitly engineers features from the
discrepancy between a text and its canonical form, overcomes this limitation.
It maintains a remarkable 82.6% TPR under the same conditions, effectively
neutralizing the most sophisticated semantic attacks. This superior performance
demonstrates that explicitly modeling perturbation artifacts, rather than
merely training on them, is a more promising path toward achieving genuine
robustness in the adversarial arms race.

</details>


### [196] [Agentic-AI Healthcare: Multilingual, Privacy-First Framework with MCP Agents](https://arxiv.org/abs/2510.02325)
*Mohammed A. Shehab*

Main category: cs.CR

TL;DR: Agentic-AI Healthcare is a privacy-aware, multilingual healthcare system using Model Context Protocol to orchestrate agents for patient interaction, with built-in compliance features for healthcare data protection standards.


<details>
  <summary>Details</summary>
Motivation: To create a healthcare AI system that addresses privacy concerns, supports multilingual communication, and provides explainable AI interactions while maintaining compliance with healthcare regulations.

Method: Uses Model Context Protocol (MCP) to orchestrate multiple intelligent agents, integrates Privacy and Compliance Layer with RBAC, AES-GCM encryption, and audit logging, supports multilingual interactions (English, French, Arabic).

Result: Successfully demonstrated multilingual patient-doctor interactions and transparent diagnostic reasoning, showing feasibility of combining agentic orchestration with compliance-aware architecture.

Conclusion: The research prototype proves the viability of integrating agentic AI orchestration, multilingual accessibility, and privacy compliance in healthcare applications, though it's not a certified medical device.

Abstract: This paper introduces Agentic-AI Healthcare, a privacy-aware, multilingual,
and explainable research prototype developed as a single-investigator project.
The system leverages the emerging Model Context Protocol (MCP) to orchestrate
multiple intelligent agents for patient interaction, including symptom
checking, medication suggestions, and appointment scheduling. The platform
integrates a dedicated Privacy and Compliance Layer that applies role-based
access control (RBAC), AES-GCM field-level encryption, and tamper-evident audit
logging, aligning with major healthcare data protection standards such as HIPAA
(US), PIPEDA (Canada), and PHIPA (Ontario). Example use cases demonstrate
multilingual patient-doctor interaction (English, French, Arabic) and
transparent diagnostic reasoning powered by large language models. As an
applied AI contribution, this work highlights the feasibility of combining
agentic orchestration, multilingual accessibility, and compliance-aware
architecture in healthcare applications. This platform is presented as a
research prototype and is not a certified medical device.

</details>


### [197] [CATMark: A Context-Aware Thresholding Framework for Robust Cross-Task Watermarking in Large Language Models](https://arxiv.org/abs/2510.02342)
*Yu Zhang,Shuliang Liu,Xu Yang,Xuming Hu*

Main category: cs.CR

TL;DR: CAT is a context-aware watermarking framework for LLMs that dynamically adjusts watermarking intensity based on semantic context, improving text quality in cross-task scenarios without sacrificing detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing LLM watermarking methods degrade text quality, especially in low-entropy scenarios, and require extensive computational resources for threshold tuning with poor adaptability to unknown or cross-task scenarios.

Method: Partitions text generation into semantic states using logits clustering, establishes context-aware entropy thresholds that preserve fidelity in structured content while embedding robust watermarks, requiring no pre-defined thresholds or task-specific tuning.

Result: Improves text quality in cross-task scenarios without sacrificing detection accuracy.

Conclusion: CAT provides an effective watermarking solution that maintains text quality while ensuring robust detection across different generation tasks.

Abstract: Watermarking algorithms for Large Language Models (LLMs) effectively identify
machine-generated content by embedding and detecting hidden statistical
features in text. However, such embedding leads to a decline in text quality,
especially in low-entropy scenarios where performance needs improvement.
Existing methods that rely on entropy thresholds often require significant
computational resources for tuning and demonstrate poor adaptability to unknown
or cross-task generation scenarios. We propose \textbf{C}ontext-\textbf{A}ware
\textbf{T}hreshold watermarking ($\myalgo$), a novel framework that dynamically
adjusts watermarking intensity based on real-time semantic context. $\myalgo$
partitions text generation into semantic states using logits clustering,
establishing context-aware entropy thresholds that preserve fidelity in
structured content while embedding robust watermarks. Crucially, it requires no
pre-defined thresholds or task-specific tuning. Experiments show $\myalgo$
improves text quality in cross-tasks without sacrificing detection accuracy.

</details>


### [198] [An Investigation into the Performance of Non-Contrastive Self-Supervised Learning Methods for Network Intrusion Detection](https://arxiv.org/abs/2510.02349)
*Hamed Fard,Tobias Schalau,Gerhard Wunder*

Main category: cs.CR

TL;DR: This paper compares non-contrastive self-supervised learning methods for network intrusion detection, testing 5 methods with 3 encoders and 6 augmentation strategies across 90 experiments on two datasets.


<details>
  <summary>Details</summary>
Motivation: Supervised learning for network intrusion detection has limitations in detecting only known anomalies, prompting exploration of self-supervised approaches. While contrastive methods have been studied, the effectiveness of non-contrastive methods remains unclear.

Method: Systematic comparison of five non-contrastive self-supervised learning methods using three encoder architectures and six augmentation strategies, conducting 90 experiments on UNSW-NB15 and 5G-NIDD datasets.

Result: The paper identifies the best-performing combinations of encoder architectures and augmentation methods for each self-supervised model based on precision, recall, F1-score, and AUCROC metrics.

Conclusion: Non-contrastive self-supervised learning methods demonstrate competitiveness for attack detection when compared to unsupervised baselines like DeepSVDD and Autoencoder.

Abstract: Network intrusion detection, a well-explored cybersecurity field, has
predominantly relied on supervised learning algorithms in the past two decades.
However, their limitations in detecting only known anomalies prompt the
exploration of alternative approaches. Motivated by the success of
self-supervised learning in computer vision, there is a rising interest in
adapting this paradigm for network intrusion detection. While prior research
mainly delved into contrastive self-supervised methods, the efficacy of
non-contrastive methods, in conjunction with encoder architectures serving as
the representation learning backbone and augmentation strategies that determine
what is learned, remains unclear for effective attack detection. This paper
compares the performance of five non-contrastive self-supervised learning
methods using three encoder architectures and six augmentation strategies.
Ninety experiments are systematically conducted on two network intrusion
detection datasets, UNSW-NB15 and 5G-NIDD. For each self-supervised model, the
combination of encoder architecture and augmentation method yielding the
highest average precision, recall, F1-score, and AUCROC is reported.
Furthermore, by comparing the best-performing models to two unsupervised
baselines, DeepSVDD, and an Autoencoder, we showcase the competitiveness of the
non-contrastive methods for attack detection. Code at:
https://github.com/renje4z335jh4/non_contrastive_SSL_NIDS

</details>


### [199] [Measuring Physical-World Privacy Awareness of Large Language Models: An Evaluation Benchmark](https://arxiv.org/abs/2510.02356)
*Xinjie Shen,Mufei Li,Pan Li*

Main category: cs.CR

TL;DR: EAPrivacy is a benchmark to evaluate LLM-powered agents' physical-world privacy awareness, revealing significant deficits in handling sensitive objects, adapting to environments, balancing tasks with privacy, and resolving social norm conflicts.


<details>
  <summary>Details</summary>
Motivation: There is an urgent need to measure LLMs' privacy awareness in physical world scenarios, as existing evaluation methods are confined to natural language contexts only.

Method: EAPrivacy uses procedurally generated scenarios across four tiers to test agents' abilities in handling sensitive objects, adapting to changing environments, balancing task execution with privacy constraints, and resolving conflicts with social norms.

Result: Current models show critical deficits: Gemini 2.5 Pro achieved only 59% accuracy in changing physical environments; models prioritized task completion over privacy constraints in up to 86% of cases; leading models disregarded social norms over 15% of the time in high-stakes situations.

Conclusion: The findings demonstrate fundamental misalignment in LLMs regarding physically grounded privacy and establish the need for more robust, physically-aware alignment.

Abstract: The deployment of Large Language Models (LLMs) in embodied agents creates an
urgent need to measure their privacy awareness in the physical world. Existing
evaluation methods, however, are confined to natural language based scenarios.
To bridge this gap, we introduce EAPrivacy, a comprehensive evaluation
benchmark designed to quantify the physical-world privacy awareness of
LLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across
four tiers to test an agent's ability to handle sensitive objects, adapt to
changing environments, balance task execution with privacy constraints, and
resolve conflicts with social norms. Our measurements reveal a critical deficit
in current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\%
accuracy in scenarios involving changing physical environments. Furthermore,
when a task was accompanied by a privacy request, models prioritized completion
over the constraint in up to 86\% of cases. In high-stakes situations pitting
privacy against critical social norms, leading models like GPT-4o and
Claude-3.5-haiku disregarded the social norm over 15\% of the time. These
findings, demonstrated by our benchmark, underscore a fundamental misalignment
in LLMs regarding physically grounded privacy and establish the need for more
robust, physically-aware alignment.

</details>


### [200] [Privacy in the Age of AI: A Taxonomy of Data Risks](https://arxiv.org/abs/2510.02357)
*Grace Billiris,Asif Gill,Madhushi Bandara*

Main category: cs.CR

TL;DR: This paper presents a taxonomy of AI privacy risks synthesized from 45 studies, identifying 19 key risks across four categories: Dataset-Level, Model-Level, Infrastructure-Level, and Insider Threat Risks, with human error being the most significant factor.


<details>
  <summary>Details</summary>
Motivation: Traditional privacy frameworks are inadequate for AI technologies due to unique characteristics like autonomous learning and black-box decision-making, creating unprecedented privacy challenges.

Method: Systematic review of 45 studies to synthesize and classify AI privacy risks into a comprehensive taxonomy.

Result: Identified 19 key risks balanced across four categories, with human error (9.45%) emerging as the most significant factor, challenging conventional security approaches that prioritize technical controls over human factors.

Conclusion: The taxonomy bridges technical and behavioral dimensions of AI privacy, contributing to trustworthy AI development and providing a foundation for future research by highlighting gaps in holistic understanding.

Abstract: Artificial Intelligence (AI) systems introduce unprecedented privacy
challenges as they process increasingly sensitive data. Traditional privacy
frameworks prove inadequate for AI technologies due to unique characteristics
such as autonomous learning and black-box decision-making. This paper presents
a taxonomy classifying AI privacy risks, synthesised from 45 studies identified
through systematic review. We identify 19 key risks grouped under four
categories: Dataset-Level, Model-Level, Infrastructure-Level, and Insider
Threat Risks. Findings reveal a balanced distribution across these dimensions,
with human error (9.45%) emerging as the most significant factor. This taxonomy
challenges conventional security approaches that typically prioritise technical
controls over human factors, highlighting gaps in holistic understanding. By
bridging technical and behavioural dimensions of AI privacy, this paper
contributes to advancing trustworthy AI development and provides a foundation
for future research.

</details>


### [201] [Bootstrapping as a Morphism: An Arithmetic Geometry Approach to Asymptotically Faster Homomorphic Encryption](https://arxiv.org/abs/2510.02365)
*Dongfang Zhao*

Main category: cs.CR

TL;DR: This paper introduces a novel geometric approach to FHE bootstrapping that eliminates the dependence on decryption circuit depth, achieving O(d·poly(log q)) complexity.


<details>
  <summary>Details</summary>
Motivation: Current FHE bootstrapping methods face prohibitive computational costs tied to the multiplicative depth of decryption circuits (L_dec), severely limiting practical adoption.

Method: The paper reframes bootstrapping using arithmetic geometry, modeling ciphertexts as affine schemes and defining decryptable/fresh ciphertexts as closed subschemes. Bootstrapping becomes a morphism between these spaces, computationally equivalent to solving a structured CVP using algebraic folding.

Result: The authors develop a complete bootstrapping algorithm with O(d·poly(log q)) complexity, completely eliminating the L_dec factor that was previously the primary performance bottleneck.

Conclusion: This geometric perspective represents a fundamental asymptotic improvement over state-of-the-art methods and offers a promising pathway toward practical high-performance FHE.

Abstract: Fully Homomorphic Encryption (FHE) provides a powerful paradigm for secure
computation, but its practical adoption is severely hindered by the prohibitive
computational cost of its bootstrapping procedure. The complexity of all
current bootstrapping methods is fundamentally tied to the multiplicative depth
of the decryption circuit, denoted $L_{dec}$, making it the primary performance
bottleneck. This paper introduces a new approach to bootstrapping that
completely bypasses the traditional circuit evaluation model. We apply the
tools of modern arithmetic geometry to reframe the bootstrapping operation as a
direct geometric projection. Our framework models the space of ciphertexts as
an affine scheme and rigorously defines the loci of decryptable and fresh
ciphertexts as distinct closed subschemes. The bootstrapping transformation is
then realized as a morphism between these two spaces. Computationally, this
projection is equivalent to solving a specific Closest Vector Problem (CVP)
instance on a highly structured ideal lattice, which we show can be done
efficiently using a technique we call algebraic folding. The primary result of
our work is a complete and provably correct bootstrapping algorithm with a
computational complexity of $O(d \cdot \text{poly}(\log q))$, where $d$ is the
ring dimension and $q$ is the ciphertext modulus. The significance of this
result lies in the complete elimination of the factor $L_{dec}$ from the
complexity, representing a fundamental asymptotic improvement over the state of
the art. This geometric perspective offers a new and promising pathway toward
achieving truly practical and high-performance FHE.

</details>


### [202] [Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids](https://arxiv.org/abs/2510.02371)
*Bochra Al Agha,Razane Tajeddine*

Main category: cs.CR

TL;DR: A graph-based multimodal detector using federated learning to detect passive eavesdropping attacks in smart grids by combining spatial and temporal analysis.


<details>
  <summary>Details</summary>
Motivation: Passive eavesdropping in smart grids reveals critical information without altering data, enabling future targeted attacks. Detection is challenging due to faint, short-lived signals that disappear when analyzed from single nodes or timelines.

Method: Two-stage encoder: graph convolution for spatial context across ego-centric star subgraphs, and bidirectional GRU for temporal dependencies. Uses federated learning with FedProx to handle heterogeneous data while keeping raw measurements on client devices.

Result: Achieves 98.32% per-timestep accuracy (F1_attack=0.972) and 93.35% per-sequence accuracy at 0.15% FPR using simple decision rules with run-length m=2 and threshold τ=0.55.

Conclusion: Combining spatial and temporal context enables reliable detection of stealthy reconnaissance with low false positives, suitable for non-IID federated smart-grid deployments.

Abstract: Smart grids are exposed to passive eavesdropping, where attackers listen
silently to communication links. Although no data is actively altered, such
reconnaissance can reveal grid topology, consumption patterns, and operational
behavior, creating a gateway to more severe targeted attacks. Detecting this
threat is difficult because the signals it produces are faint, short-lived, and
often disappear when traffic is examined by a single node or along a single
timeline. This paper introduces a graph-centric, multimodal detector that fuses
physical-layer and behavioral indicators over ego-centric star subgraphs and
short temporal windows to detect passive attacks. To capture stealthy
perturbations, a two-stage encoder is introduced: graph convolution aggregates
spatial context across ego-centric star subgraphs, while a bidirectional GRU
models short-term temporal dependencies. The encoder transforms heterogeneous
features into a unified spatio-temporal representation suitable for
classification. Training occurs in a federated learning setup under FedProx,
improving robustness to heterogeneous local raw data and contributing to the
trustworthiness of decentralized training; raw measurements remain on client
devices. A synthetic, standards-informed dataset is generated to emulate
heterogeneous HAN/NAN/WAN communications with wireless-only passive
perturbations, event co-occurrence, and leak-safe splits. The model achieves a
testing accuracy of 98.32% per-timestep (F1_{attack}=0.972) and 93.35%
per-sequence at 0.15% FPR using a simple decision rule with run-length m=2 and
threshold $\tau=0.55$. The results demonstrate that combining spatial and
temporal context enables reliable detection of stealthy reconnaissance while
maintaining low false-positive rates, making the approach suitable for non-IID
federated smart-grid deployments.

</details>


### [203] [A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory](https://arxiv.org/abs/2510.02373)
*Qianshan Wei,Tengchao Yang,Yaochen Wang,Xinfeng Li,Lijun Li,Zhenfei Yin,Yi Zhan,Thorsten Holz,Zhiqiang Lin,XiaoFeng Wang*

Main category: cs.CR

TL;DR: A-MemGuard is a proactive defense framework that protects LLM agents from memory poisoning attacks by using consensus-based validation and dual-memory structure to detect and correct malicious memory injections.


<details>
  <summary>Details</summary>
Motivation: LLM agents' reliance on memory creates security vulnerabilities where adversaries can inject seemingly harmless records to manipulate future behavior, creating hard-to-detect context-dependent attacks and self-reinforcing error cycles.

Method: A-MemGuard uses consensus-based validation to detect anomalies by comparing reasoning paths from multiple memories, and a dual-memory structure where detected failures are stored as "lessons" separately to break error cycles and enable adaptation.

Result: Comprehensive evaluations show A-MemGuard reduces attack success rates by over 95% with minimal utility cost, effectively protecting LLM agents from memory manipulation attacks.

Conclusion: This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time through self-checking and self-correcting memory mechanisms.

Abstract: Large Language Model (LLM) agents use memory to learn from past interactions,
enabling autonomous planning and decision-making in complex environments.
However, this reliance on memory introduces a critical security risk: an
adversary can inject seemingly harmless records into an agent's memory to
manipulate its future behavior. This vulnerability is characterized by two core
aspects: First, the malicious effect of injected records is only activated
within a specific context, making them hard to detect when individual memory
entries are audited in isolation. Second, once triggered, the manipulation can
initiate a self-reinforcing error cycle: the corrupted outcome is stored as
precedent, which not only amplifies the initial error but also progressively
lowers the threshold for similar attacks in the future. To address these
challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive
defense framework for LLM agent memory. The core idea of our work is the
insight that memory itself must become both self-checking and self-correcting.
Without modifying the agent's core architecture, A-MemGuard combines two
mechanisms: (1) consensus-based validation, which detects anomalies by
comparing reasoning paths derived from multiple related memories and (2) a
dual-memory structure, where detected failures are distilled into ``lessons''
stored separately and consulted before future actions, breaking error cycles
and enabling adaptation. Comprehensive evaluations on multiple benchmarks show
that A-MemGuard effectively cuts attack success rates by over 95% while
incurring a minimal utility cost. This work shifts LLM memory security from
static filtering to a proactive, experience-driven model where defenses
strengthen over time. Our code is available in
https://github.com/TangciuYueng/AMemGuard

</details>


### [204] [A Hybrid CAPTCHA Combining Generative AI with Keystroke Dynamics for Enhanced Bot Detection](https://arxiv.org/abs/2510.02374)
*Ayda Aghaei Nia*

Main category: cs.CR

TL;DR: A hybrid CAPTCHA system combining LLM-generated cognitive challenges with keystroke dynamics analysis to improve bot detection while maintaining usability.


<details>
  <summary>Details</summary>
Motivation: Traditional CAPTCHAs face a trade-off between usability and resilience against AI-powered bots, requiring a more sophisticated approach.

Method: Dual-layered approach: dynamic LLM-generated questions that are easy for humans but hard for bots, combined with behavioral biometric analysis of keystroke dynamics.

Result: High accuracy in bot detection, successfully thwarting both paste-based and script-based simulation attacks, while maintaining high usability scores.

Conclusion: Combining cognitive and behavioral tests creates a new generation of more secure and user-friendly CAPTCHAs.

Abstract: Completely Automated Public Turing tests to tell Computers and Humans Apart
(CAPTCHAs) are a foundational component of web security, yet traditional
implementations suffer from a trade-off between usability and resilience
against AI-powered bots. This paper introduces a novel hybrid CAPTCHA system
that synergizes the cognitive challenges posed by Large Language Models (LLMs)
with the behavioral biometric analysis of keystroke dynamics. Our approach
generates dynamic, unpredictable questions that are trivial for humans but
non-trivial for automated agents, while simultaneously analyzing the user's
typing rhythm to distinguish human patterns from robotic input. We present the
system's architecture, formalize the feature extraction methodology for
keystroke analysis, and report on an experimental evaluation. The results
indicate that our dual-layered approach achieves a high degree of accuracy in
bot detection, successfully thwarting both paste-based and script-based
simulation attacks, while maintaining a high usability score among human
participants. This work demonstrates the potential of combining cognitive and
behavioral tests to create a new generation of more secure and user-friendly
CAPTCHAs.

</details>


### [205] [Scaling Homomorphic Applications in Deployment](https://arxiv.org/abs/2510.02376)
*Ryan Marinelli,Angelica Chowdhury*

Main category: cs.CR

TL;DR: A proof-of-concept homomorphic movie recommendation app is developed to test production readiness of encryption ecosystems, using containerization and orchestration to mitigate FHE computational limitations.


<details>
  <summary>Details</summary>
Motivation: To determine the production readiness of encryption ecosystems and address the computational limitations of Fully Homomorphic Encryption (FHE) in real-world applications.

Method: Developed a movie recommendation application using homomorphic encryption, then productionized it through containerization and orchestration with optimized deployment configurations.

Result: Successfully implemented a working homomorphic encryption application and demonstrated that infrastructure optimizations can help mitigate FHE's computational constraints.

Conclusion: Homomorphic encryption ecosystems can be made production-ready through proper containerization, orchestration, and infrastructure optimizations, though computational limitations remain a challenge that requires ongoing optimization efforts.

Abstract: In this endeavor, a proof-of-concept homomorphic application is developed to
determine the production readiness of encryption ecosystems. A movie
recommendation app is implemented for this purpose and productionized through
containerization and orchestration. By tuning deployment configurations, the
computational limitations of Fully Homomorphic Encryption (FHE) are mitigated
through additional infrastructure optimizations
  Index Terms: Reinforcement Learning, Orchestration, Homomorphic Encryption

</details>


### [206] [Apply Bayes Theorem to Optimize IVR Authentication Process](https://arxiv.org/abs/2510.02378)
*Jingrong Xie,Yumin Li*

Main category: cs.CR

TL;DR: Bayesian approach to enhance IVR authentication by dynamically assessing fraud risk and adapting credential verification paths.


<details>
  <summary>Details</summary>
Motivation: Traditional IVR systems use static credential sequences with uniform effectiveness assumptions, making them vulnerable to fraudsters who exploit predictability by bypassing strong credentials.

Method: Apply Bayes' Theorem and conditional probability modeling to evaluate fraud risk dynamically and adapt credential verification paths.

Result: Not specified in the abstract.

Conclusion: Not specified in the abstract.

Abstract: This paper introduces a Bayesian approach to improve Interactive Voice
Response (IVR) authentication processes used by financial institutions.
Traditional IVR systems authenticate users through a static sequence of
credentials, assuming uniform effectiveness among them. However, fraudsters
exploit this predictability, selectively bypassing strong credentials. This
study applies Bayes' Theorem and conditional probability modeling to evaluate
fraud risk dynamically and adapt credential verification paths.

</details>


### [207] [Hybrid Schemes of NIST Post-Quantum Cryptography Standard Algorithms and Quantum Key Distribution for Key Exchange and Digital Signature](https://arxiv.org/abs/2510.02379)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: This paper proposes hybrid security schemes combining quantum key distribution (QKD) with NIST-standardized post-quantum cryptography (PQC) algorithms, including hybrid key exchange protocols and digital signature schemes.


<details>
  <summary>Details</summary>
Motivation: To create dual-layered security by combining the mathematical security of PQC with the quantum physics-based security of QKD, leveraging their complementary advantages.

Method: Develops hybrid schemes integrating ML-KEM with BB84/E91 QKD for key exchange, and ML-DSA/SLH-DSA with BB84/E91 for digital signatures using confirmation codes.

Result: Evaluates the hybrid key exchange protocol's shared secret key for entropy and IID properties, and assesses computation time and message lengths of the proposed schemes.

Conclusion: The hybrid QKD-PQC approach provides enhanced security through complementary protection mechanisms, with practical performance characteristics suitable for deployment.

Abstract: Since the security of post-quantum cryptography (PQC) algorithms is based on
the hardness of mathematical problems, while the security of quantum key
distribution (QKD) relies on the fundamental principles of quantum physics,
each approach possesses distinct advantages and limitations that can complement
one another. Consequently, recent studies have proposed hybrid schemes that
combine QKD and PQC to establish a dual-layered security model. In response to
this trend, this study proposes hybrid schemes that integrate QKD with the
National Institute of Standards and Technology (NIST) standardized PQC
algorithms. These hybrid schemes include two core components: a hybrid QKD-PQC
key exchange protocol and a hybrid QKD-PQC digital signature scheme. For the
hybrid key exchange protocol, this study combines Module-Lattice-based Key
Encapsulation Mechanisms (ML-KEM) with QKD protocols, specifically BB84 and
E91, to construct a secure key exchange protocol. In the design of the hybrid
digital signature scheme, this study utilizes Module-Lattice-based Digital
Signature Algorithms (ML-DSA) and Stateless Hash-based Digital Signature
Algorithms (SLH-DSA) to generate signature reconstruction values. These values
are verified using confirmation codes transmitted via the BB84 and E91
protocols. The proposed hybrid key exchange protocol is evaluated by examining
the shared secret key it produces, particularly with respect to entropy and
whether the output is independent and identically distributed (IID).
Furthermore, the computation time and message lengths of the proposed hybrid
schemes are evaluated.

</details>


### [208] [Selmer-Inspired Elliptic Curve Generation](https://arxiv.org/abs/2510.02383)
*Awnon Bhowmik*

Main category: cs.CR

TL;DR: A transparent framework for constructing elliptic curves using Selmer-inspired descent methods, producing auditable cryptographic curves with complete verification transcripts.


<details>
  <summary>Details</summary>
Motivation: Address opacity in existing elliptic curve parameter generation by creating a transparent, auditable framework that embeds arithmetic structure into curve construction.

Method: Use 2- and 3-descent methods to derive binary quartics and ternary cubics, apply local solubility checks modeled on Selmer admissibility, reconcile into short-Weierstrass form, and validate with cryptographic tests.

Result: Successful proof-of-concept implementation as a Las Vegas algorithm with complete verification transcripts, producing cryptographically valid curves with transparent parameter generation.

Conclusion: Descent techniques from arithmetic geometry can underpin trust-enhancing elliptic curve constructions that are standardization-ready while maintaining compatibility with secure implementations.

Abstract: Elliptic curve cryptography (ECC) is foundational to modern secure
communication, yet existing standard curves have faced scrutiny for opaque
parameter-generation practices. This work introduces a Selmer-inspired
framework for constructing elliptic curves that is both transparent and
auditable. Drawing from $2$- and $3$-descent methods, we derive binary quartics
and ternary cubics whose classical invariants deterministically yield candidate
$(c_4,c_6)$ parameters. Local solubility checks, modeled on Selmer
admissibility, filter candidates prior to reconciliation into short-Weierstrass
form over prime fields. We then apply established cryptographic validations,
including group-order factorization, cofactor bounds, twist security, and
embedding-degree heuristics. A proof-of-concept implementation demonstrates
that the pipeline functions as a retry-until-success Las Vegas algorithm, with
complete transcripts enabling independent verification. Unlike seed-based or
purely efficiency-driven designs, our approach embeds arithmetic structure into
parameter selection while remaining compatible with constant-time, side-channel
resistant implementations. This work broadens the design space for elliptic
curves, showing that descent techniques from arithmetic geometry can underpin
trust-enhancing, standardization-ready constructions.

</details>


### [209] [Secure and Robust Watermarking for AI-generated Images: A Comprehensive Survey](https://arxiv.org/abs/2510.02384)
*Jie Cao,Qi Li,Zelin Zhang,Jianbing Ni*

Main category: cs.CR

TL;DR: This paper provides a comprehensive survey of AI-generated image watermarking technologies, covering system formalization, techniques comparison, evaluation methods, security vulnerabilities, and future directions.


<details>
  <summary>Details</summary>
Motivation: The rapid development of Gen-AI enables easy creation of high-quality images but raises concerns about intellectual property protection, authenticity, and accountability. Watermarking offers a solution to distinguish AI-generated content and ensure trustworthy digital ecosystems.

Method: The survey systematically addresses five key dimensions: formalization of watermarking systems, overview and comparison of diverse techniques, evaluation methodologies (visual quality, capacity, detectability), analysis of vulnerabilities to malicious attacks, and identification of challenges and future directions.

Result: The survey provides researchers with a holistic understanding of current AI-generated image watermarking technologies, enabling better development and implementation of these systems.

Conclusion: This comprehensive survey aims to equip researchers with complete knowledge about AI-generated image watermarking, promoting continued advancement in this critical field to address intellectual property and authenticity concerns in the Gen-AI era.

Abstract: The rapid advancement of generative artificial intelligence (Gen-AI) has
facilitated the effortless creation of high-quality images, while
simultaneously raising critical concerns regarding intellectual property
protection, authenticity, and accountability. Watermarking has emerged as a
promising solution to these challenges by distinguishing AI-generated images
from natural content, ensuring provenance, and fostering trustworthy digital
ecosystems. This paper presents a comprehensive survey of the current state of
AI-generated image watermarking, addressing five key dimensions: (1)
formalization of image watermarking systems; (2) an overview and comparison of
diverse watermarking techniques; (3) evaluation methodologies with respect to
visual quality, capacity, and detectability; (4) vulnerabilities to malicious
attacks; and (5) prevailing challenges and future directions. The survey aims
to equip researchers with a holistic understanding of AI-generated image
watermarking technologies, thereby promoting their continued development.

</details>


### [210] [On The Fragility of Benchmark Contamination Detection in Reasoning Models](https://arxiv.org/abs/2510.02386)
*Han Wang,Haoyu Li,Brian Ko,Huan Zhang*

Main category: cs.CR

TL;DR: LRM leaderboards incentivize benchmark contamination, and our study shows it's alarmingly easy to evade detection through RL training and advanced contamination techniques, undermining evaluation fairness.


<details>
  <summary>Details</summary>
Motivation: To investigate how easily LRMs can evade contamination detection in leaderboard evaluations, revealing vulnerabilities in current evaluation protocols.

Method: Studied two contamination scenarios: (I) base model evolution via SFT and RL, examining GRPO training's concealment effects; (II) SFT contamination with CoT on advanced LRMs, testing detection method performance.

Result: RL training (PPO style) effectively conceals contamination signals; most detection methods perform near random guesses against advanced contamination techniques; contaminated LRMs evade memorization-based detection.

Conclusion: LRM evaluations are uniquely vulnerable to undetectable contamination, threatening leaderboard integrity and highlighting urgent need for advanced detection methods and trustworthy evaluation protocols.

Abstract: Leaderboards for LRMs have turned evaluation into a competition,
incentivizing developers to optimize directly on benchmark suites. A shortcut
to achieving higher rankings is to incorporate evaluation benchmarks into the
training data, thereby yielding inflated performance, known as benchmark
contamination. Surprisingly, our studies find that evading contamination
detections for LRMs is alarmingly easy. We focus on the two scenarios where
contamination may occur in practice: (I) when the base model evolves into LRM
via SFT and RL, we find that contamination during SFT can be originally
identified by contamination detection methods. Yet, even a brief GRPO training
can markedly conceal contamination signals that most detection methods rely on.
Further empirical experiments and theoretical analysis indicate that PPO style
importance sampling and clipping objectives are the root cause of this
detection concealment, indicating that a broad class of RL methods may
inherently exhibit similar concealment capability; (II) when SFT contamination
with CoT is applied to advanced LRMs as the final stage, most contamination
detection methods perform near random guesses. Without exposure to non-members,
contaminated LRMs would still have more confidence when responding to those
unseen samples that share similar distributions to the training set, and thus,
evade existing memorization-based detection methods. Together, our findings
reveal the unique vulnerability of LRMs evaluations: Model developers could
easily contaminate LRMs to achieve inflated leaderboards performance while
leaving minimal traces of contamination, thereby strongly undermining the
fairness of evaluation and threatening the integrity of public leaderboards.
This underscores the urgent need for advanced contamination detection methods
and trustworthy evaluation protocols tailored to LRMs.

</details>


### [211] [LLM-Generated Samples for Android Malware Detection](https://arxiv.org/abs/2510.02391)
*Nik Rollinson,Nikolaos Polatidis*

Main category: cs.CR

TL;DR: Fine-tuned GPT-4.1-mini to generate synthetic malware data for three families, evaluated classifiers under three training scenarios. Synthetic data augmentation preserves performance while synthetic-only training shows mixed results.


<details>
  <summary>Details</summary>
Motivation: Address Android malware evolution challenges and data scarcity through LLM-generated synthetic data, as LLMs' role in malware data generation is underexplored.

Method: Fine-tuned GPT-4.1-mini on KronoDroid dataset to generate structured malware records for three families, used prompt engineering and post-processing, evaluated classifiers in three training settings.

Result: Real-only training achieves near perfect detection; real-plus-synthetic preserves high performance with minor degradation; synthetic-only shows mixed effectiveness varying by family and fine-tuning.

Conclusion: LLM-generated malware can enhance scarce datasets without compromising accuracy but is insufficient as standalone training source.

Abstract: Android malware continues to evolve through obfuscation and polymorphism,
posing challenges for both signature-based defenses and machine learning models
trained on limited and imbalanced datasets. Synthetic data has been proposed as
a remedy for scarcity, yet the role of large language models (LLMs) in
generating effective malware data for detection tasks remains underexplored. In
this study, we fine-tune GPT-4.1-mini to produce structured records for three
malware families: BankBot, Locker/SLocker, and Airpush/StopSMS, using the
KronoDroid dataset. After addressing generation inconsistencies with prompt
engineering and post-processing, we evaluate multiple classifiers under three
settings: training with real data only, real-plus-synthetic data, and synthetic
data alone. Results show that real-only training achieves near perfect
detection, while augmentation with synthetic data preserves high performance
with only minor degradations. In contrast, synthetic-only training produces
mixed outcomes, with effectiveness varying across malware families and
fine-tuning strategies. These findings suggest that LLM-generated malware can
enhance scarce datasets without compromising detection accuracy, but remains
insufficient as a standalone training source.

</details>


### [212] [PolyLink: A Blockchain Based Decentralized Edge AI Platform for LLM Inference](https://arxiv.org/abs/2510.02395)
*Hongbo Liu,Jiannong Cao,Bo Yang,Dongbin Bai,Yinfeng Cao,Xiaoming Shen,Yinan Zhang,Jinwen Liang,Shan Jiang,Mingjin Zhang*

Main category: cs.CR

TL;DR: PolyLink is a blockchain-based decentralized AI platform that decentralizes LLM development and inference using crowdsourcing architecture, TIQE protocol for inference verification, and token-based incentives.


<details>
  <summary>Details</summary>
Motivation: Address centralized deployment issues of LLM services that create trust problems and high costs for users and developers.

Method: Uses decentralized crowdsourcing architecture for model deployment across heterogeneous edge devices, TIQE protocol combining cross-encoder model and LLM-as-a-Judge for inference verification, and token-based incentive model with dynamic pricing.

Result: Real-world geo-distributed deployment shows practical inference and verification latency, and security analysis confirms resistance to model degradation attacks and validator corruptions.

Conclusion: PolyLink successfully demonstrates a practical decentralized LLM platform with effective verification mechanisms and security guarantees.

Abstract: The rapid advancement of large language models (LLMs) in recent years has
revolutionized the AI landscape. However, the deployment model and usage of LLM
services remain highly centralized, creating significant trust issues and costs
for end users and developers. To address these issues, we propose PolyLink, a
blockchain-based decentralized AI platform that decentralizes LLM development
and inference. Specifically, PolyLink introduces a decentralized crowdsourcing
architecture that supports single-device and cross-device model deployment and
inference across heterogeneous devices at the edge. Moreover, to ensure the
inference integrity, we design the TIQE protocol, which combines a lightweight
cross-encoder model and an LLM-as-a-Judge for a high-accuracy inference
evaluation. Lastly, we integrate a comprehensive token-based incentive model
with dynamic pricing and reward mechanisms for all participants. We have
deployed PolyLink and conducted an extensive real-world evaluation through
geo-distributed deployment across heterogeneous devices. Results indicate that
the inference and verification latency is practical. Our security analysis
demonstrates that the system is resistant to model degradation attacks and
validator corruptions. PolyLink is now available at
https://github.com/IMCL-PolyLink/PolyLink.

</details>


### [213] [Dynamic Target Attack](https://arxiv.org/abs/2510.02422)
*Kedong Xiu,Churui Zeng,Tianhang Zheng,Xinzhe Huang,Xiaojun Jia,Di Wang,Puning Zhao,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: DTA is a new jailbreaking framework that uses the target LLM's own responses as dynamic targets for adversarial prompt optimization, significantly improving attack success rates and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing gradient-based jailbreak attacks use fixed affirmative targets that reside in low-density regions of safety-aligned LLMs' output distributions, making optimization difficult and requiring many iterations with limited success.

Method: DTA iteratively samples multiple candidate responses from the target LLM's output distribution conditioned on the current prompt, selects the most harmful response as a temporary target, and optimizes the adversarial prompt using these dynamic targets.

Result: DTA achieves over 87% attack success rate with only 200 iterations (15% higher than SOTA), reduces time cost by 2-26x, and achieves 85% ASR in black-box settings using surrogate models.

Conclusion: Dynamic target selection significantly reduces the discrepancy between targets and output distributions, making jailbreak optimization much more efficient and effective across both white-box and black-box scenarios.

Abstract: Existing gradient-based jailbreak attacks typically optimize an adversarial
suffix to induce a fixed affirmative response. However, this fixed target
usually resides in an extremely low-density region of a safety-aligned LLM's
output distribution conditioned on diverse harmful inputs. Due to the
substantial discrepancy between the target and the original output, existing
attacks require numerous iterations to optimize the adversarial prompt, which
might still fail to induce the low-probability target response from the target
LLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking
framework relying on the target LLM's own responses as targets to optimize the
adversarial prompts. In each optimization round, DTA iteratively samples
multiple candidate responses directly from the output distribution conditioned
on the current prompt, and selects the most harmful response as a temporary
target for prompt optimization. In contrast to existing attacks, DTA
significantly reduces the discrepancy between the target and the output
distribution, substantially easing the optimization process to search for an
effective adversarial prompt.
  Extensive experiments demonstrate the superior effectiveness and efficiency
of DTA: under the white-box setting, DTA only needs 200 optimization iterations
to achieve an average attack success rate (ASR) of over 87\% on recent
safety-aligned LLMs, exceeding the state-of-the-art baselines by over 15\%. The
time cost of DTA is 2-26 times less than existing baselines. Under the
black-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target
sampling and achieves an ASR of 85\% against the black-box target model
Llama-3-70B-Instruct, exceeding its counterparts by over 25\%.

</details>


### [214] [Adaptive Deception Framework with Behavioral Analysis for Enhanced Cybersecurity Defense](https://arxiv.org/abs/2510.02424)
*Basil Abdullah AL-Zahrani*

Main category: cs.CR

TL;DR: CADL is an adaptive deception framework that achieves 99.88% detection rate with 0.13% false positive rate using ensemble ML and behavioral profiling.


<details>
  <summary>Details</summary>
Motivation: To develop a high-performance intrusion detection system that adapts to attackers' behaviors and provides an affordable alternative to expensive commercial deception platforms.

Method: Uses ensemble machine learning (Random Forest, XGBoost, Neural Networks) combined with behavioral profiling and coordinated signal bus architecture for real-time intelligence sharing and collective decision-making.

Result: Achieves 99.88% detection rate with 0.13% false positive rate on CICIDS2017 dataset, significantly outperforming traditional systems (Snort: 71.2%, Suricata: 68.5%) and 89% accuracy in attacker profile classification.

Conclusion: CADL provides a highly effective, production-ready intrusion detection framework with superior performance compared to traditional systems and offers an accessible open-source alternative to expensive commercial solutions.

Abstract: This paper presents CADL (Cognitive-Adaptive Deception Layer), an adaptive
deception framework achieving 99.88% detection rate with 0.13% false positive
rate on the CICIDS2017 dataset. The framework employs ensemble machine learning
(Random Forest, XGBoost, Neural Networks) combined with behavioral profiling to
identify and adapt responses to network intrusions. Through a coordinated
signal bus architecture, security components share real-time intelligence,
enabling collective decision-making. The system profiles attackers based on
temporal patterns and deploys customized deception strategies across five
escalation levels. Evaluation on 50,000 CICIDS2017 test samples demonstrates
that CADL significantly outperforms traditional intrusion detection systems
(Snort: 71.2%, Suricata: 68.5%) while maintaining production-ready false
positive rates. The framework's behavioral analysis achieves 89% accuracy in
classifying attacker profiles. We provide open-source implementation and
transparent performance metrics, offering an accessible alternative to
commercial deception platforms costing $150-400 per host annually.

</details>


### [215] [Rigorous Evaluation of Microarchitectural Side-Channels with Statistical Model Checking](https://arxiv.org/abs/2510.02475)
*Weihang Li,Pete Crowley,Arya Tschand,Yu Wang,Miroslav Pajic,Daniel Sorin*

Main category: cs.CR

TL;DR: Introduces Statistical Model Checking (SMC) for rigorous quantitative evaluation of microarchitectural side channels, addressing challenges of probabilistic behaviors and processor complexity without requiring simplifications.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating microarchitectural side channels rely on abstract/simplified models that may miss important security phenomena, and they struggle with probabilistic behaviors from noise, attacks, and defenses.

Method: Uses Statistical Model Checking (SMC) - a rigorous statistical technique that processes probabilistic experiment results and provides statistical guarantees, treating processors as opaque boxes without abstraction.

Result: SMC effectively evaluates security vulnerabilities and defenses with greater statistical rigor, provides similar conclusions to previous methods, and enables defenders to quantify noise levels needed for desired security thresholds.

Conclusion: SMC provides a rigorous, statistically-guaranteed approach for microarchitectural side channel evaluation that doesn't require processor simplifications and offers actionable defense strategies through noise injection quantification.

Abstract: Rigorous quantitative evaluation of microarchitectural side channels is
challenging for two reasons. First, the processors, attacks, and defenses often
exhibit probabilistic behaviors. These probabilistic behaviors arise due to
natural noise in systems (e.g., from co-running processes), probabilistic side
channel attacks, and probabilistic obfuscation defenses. Second,
microprocessors are extremely complex. Previous evaluation methods have relied
on abstract or simplified models, which are necessarily less detailed than real
systems or cycle-by-cycle simulators, and these models may miss important
phenomena. Whereas a simple model may suffice for estimating performance,
security issues frequently manifest in the details.
  We address this challenge by introducing Statistical Model Checking (SMC) to
the quantitative evaluation of microarchitectural side channels. SMC is a
rigorous statistical technique that can process the results of probabilistic
experiments and provide statistical guarantees, and it has been used in
computing applications that depend heavily on statistical guarantees (e.g.,
medical implants, vehicular computing). With SMC, we can treat processors as
opaque boxes, and we do not have to abstract or simplify them. We demonstrate
the effectiveness of SMC through three case studies, in which we experimentally
show that SMC can evaluate existing security vulnerabilities and defenses and
provide qualitatively similar conclusions with greater statistical rigor, while
making no simplifying assumptions or abstractions. We also show that SMC can
enable a defender to quantify the amount of noise necessary to have a desired
level of confidence that she has reduced an attacker's probability of success
to less than a desired threshold, thus providing the defender with an
actionable plan for obfuscation via noise injection.

</details>


### [216] [TLoRa: Implementing TLS Over LoRa for Secure HTTP Communication in IoT](https://arxiv.org/abs/2510.02519)
*Atonu Ghosh,Akhilesh Mohanasundaram,Srishivanth R F,Sudip Misra*

Main category: cs.CR

TL;DR: TLoRa enables secure HTTPS communication over LoRa by integrating TCP tunneling and TLS 1.3 handshake, allowing WiFi devices to access the Internet through LoRa networks with an End Hub and Net Relay architecture.


<details>
  <summary>Details</summary>
Motivation: To provide seamless and secure communication between WiFi-enabled devices and the Internet over LoRa networks, addressing the need for HTTPS access in low-power wide-area network scenarios.

Method: Three-phase architecture: session setup (TCP socket management and TLS handshake), secure tunneling (encrypted data transfer over LoRa), and rendering (content delivery). Uses End Hub with WiFi hotspot/captive portal and Net Relay as server-side proxy, with lightweight TLS record reassembly and queuing for session multiplexing.

Result: Successfully establishes TLS session over LoRa in 9.9 seconds and fulfills API requests in 3.58 seconds, demonstrating practical HTTPS access capability over LoRa networks.

Conclusion: TLoRa provides the first comprehensive solution for HTTPS access over LoRa using full TLS, enabling secure Internet connectivity for WiFi devices through LoRa infrastructure with acceptable performance metrics.

Abstract: We present TLoRa, an end-to-end architecture for HTTPS communication over
LoRa by integrating TCP tunneling and a complete TLS 1.3 handshake. It enables
a seamless and secure communication channel between WiFi-enabled end devices
and the Internet over LoRa using an End Hub (EH) and a Net Relay (NR). The EH
tethers a WiFi hotspot and a captive portal for user devices to connect and
request URLs. The EH forwards the requested URLs to the NR using a secure
tunnel over LoRa. The NR, which acts as a server-side proxy, receives and
resolves the request from the Internet-based server. It then relays back the
encrypted response from the server over the same secure tunnel. TLoRa operates
in three phases -session setup, secure tunneling, and rendering. In the first
phase, it manages the TCP socket and initiates the TLS handshake. In the
second, it creates a secure tunnel and transfers encrypted TLS data over LoRa.
Finally, it delivers the URL content to the user. TLoRa also implements a
lightweight TLS record reassembly layer and a queuing mechanism for session
multiplexing. We evaluate TLoRa on real hardware using multiple accesses to a
web API. Results indicate that it provides a practical solution by successfully
establishing a TLS session over LoRa in 9.9 seconds and takes 3.58 seconds to
fulfill API requests. To the best of our knowledge, this is the first work to
comprehensively design, implement, and evaluate the performance of HTTPS access
over LoRa using full TLS.

</details>


### [217] [ToolTweak: An Attack on Tool Selection in LLM-based Agents](https://arxiv.org/abs/2510.02554)
*Jonathan Sneh,Ruomei Yan,Jialin Yu,Philip Torr,Yarin Gal,Sunando Sengupta,Eric Sommerlade,Alasdair Paren,Adel Bibi*

Main category: cs.CR

TL;DR: ToolTweak attack manipulates tool names/descriptions to bias LLM agents' tool selection, increasing selection rates from 20% to 81%, revealing vulnerabilities in tool ecosystems.


<details>
  <summary>Details</summary>
Motivation: As LLM agents increasingly rely on external tools, the tool selection process creates competition among providers. This research identifies vulnerabilities where adversaries can manipulate tool metadata to gain unfair advantage.

Method: ToolTweak - a lightweight automatic attack that iteratively manipulates tool names and descriptions to bias agent selection. Evaluated transferability between open-source and closed-source models.

Result: Attack increases tool selection rates from baseline ~20% to as high as 81%. Strong transferability across models. Causes distributional shifts in tool usage, revealing fairness and security risks.

Conclusion: Tool ecosystems face significant vulnerabilities from metadata manipulation attacks. Proposed defenses (paraphrasing and perplexity filtering) can mitigate bias and promote fairer tool selection among functionally similar alternatives.

Abstract: As LLMs increasingly power agents that interact with external tools, tool use
has become an essential mechanism for extending their capabilities. These
agents typically select tools from growing databases or marketplaces to solve
user tasks, creating implicit competition among tool providers and developers
for visibility and usage. In this paper, we show that this selection process
harbors a critical vulnerability: by iteratively manipulating tool names and
descriptions, adversaries can systematically bias agents toward selecting
specific tools, gaining unfair advantage over equally capable alternatives. We
present ToolTweak, a lightweight automatic attack that increases selection
rates from a baseline of around 20% to as high as 81%, with strong
transferability between open-source and closed-source models. Beyond individual
tools, we show that such attacks cause distributional shifts in tool usage,
revealing risks to fairness, competition, and security in emerging tool
ecosystems. To mitigate these risks, we evaluate two defenses: paraphrasing and
perplexity filtering, which reduce bias and lead agents to select functionally
similar tools more equally. All code will be open-sourced upon acceptance.

</details>


### [218] [Who's Wearing? Ear Canal Biometric Key Extraction for User Authentication on Wireless Earbuds](https://arxiv.org/abs/2510.02563)
*Chenpei Huang,Lingfeng Yao,Hui Zhong,Kyu In Lee,Lan Zhang,Xiaoyong Yuan,Tomoaki Ohtsuki,Miao Pan*

Main category: cs.CR

TL;DR: EarID is a practical ear canal biometric authentication system that extracts binary keys directly on earbuds, achieving 98.7% accuracy with privacy protection using fuzzy commitment schemes.


<details>
  <summary>Details</summary>
Motivation: Existing ear canal scanning methods rely on machine learning classifiers that risk raw biometric data leakage and are computationally intensive for resource-constrained earbuds.

Method: EarID extracts unique binary keys directly on earbuds during authentication, eliminating classifier dependency and enabling privacy-preserving verification through fuzzy commitment schemes on mobile devices.

Result: Achieves 98.7% authentication accuracy comparable to ML classifiers, with fast enrollment (160ms) and processing (226ms) times, and maintains <1% false acceptance rate across adversarial scenarios.

Conclusion: EarID provides a practical, secure, and efficient solution for next-generation wireless earbuds authentication that protects biometric privacy while maintaining high accuracy.

Abstract: Ear canal scanning/sensing (ECS) has emerged as a novel biometric
authentication method for mobile devices paired with wireless earbuds. Existing
studies have demonstrated the uniqueness of ear canals by training and testing
machine learning classifiers on ECS data. However, implementing practical
ECS-based authentication requires preventing raw biometric data leakage and
designing computationally efficient protocols suitable for resource-constrained
earbuds. To address these challenges, we propose an ear canal key extraction
protocol, \textbf{EarID}. Without relying on classifiers, EarID extracts unique
binary keys directly on the earbuds during authentication. These keys further
allow the use of privacy-preserving fuzzy commitment scheme that verifies the
wearer's key on mobile devices. Our evaluation results demonstrate that EarID
achieves a 98.7\% authentication accuracy, comparable to machine learning
classifiers. The mobile enrollment time (160~ms) and earbuds processing time
(226~ms) are negligible in terms of wearer's experience. Moreover, our approach
is robust and attack-resistant, maintaining a false acceptance rate below 1\%
across all adversarial scenarios. We believe the proposed EarID offers a
practical and secure solution for next-generation wireless earbuds.

</details>


### [219] [Using Preformed Resistive Random Access Memory to Create a Strong Physically Unclonable Function](https://arxiv.org/abs/2510.02643)
*Jack Garrard,John F. Hardy II,Carlo daCunha,Mayank Bakshi*

Main category: cs.CR

TL;DR: A new ReRAM PUF protocol using differential reads from unformed ReRAM for identity verification and encryption, demonstrated on hardware with excellent performance.


<details>
  <summary>Details</summary>
Motivation: To create a physical ReRAM PUF with large challenge space for identity verification and asymmetric encryption applications.

Method: Uses differential reads from unformed ReRAM as the method for response generation in the PUF protocol.

Result: Experimental hardware demonstration on a Physical ReRAM device showed excellent performance characteristics as a PUF.

Conclusion: The proposed ReRAM PUF protocol successfully creates a physical PUF with large challenge space and demonstrates practical implementation with strong performance.

Abstract: Physically Unclonable Functions (PUFs) are a promising solution for identity
verification and asymmetric encryption. In this paper, a new Resistive Random
Access Memory (ReRAM) PUF-based protocol is presented to create a physical
ReRAM PUF with a large challenge space. This protocol uses differential reads
from unformed ReRAM as the method for response generation. Lastly, this paper
also provides an experimental hardware demonstration of this protocol on a
Physical ReRAM device, along with providing notable results as a PUF, with
excellent performance characteristics.

</details>


### [220] [MALF: A Multi-Agent LLM Framework for Intelligent Fuzzing of Industrial Control Protocols](https://arxiv.org/abs/2510.02694)
*Bowei Ning,Xuejun Zong,Kan He*

Main category: cs.CR

TL;DR: MALF is a multi-agent LLM fuzzing framework that uses large language models with domain-specific knowledge to identify vulnerabilities in industrial control protocols, achieving high test case pass rates and discovering critical zero-day flaws in real-world deployments.


<details>
  <summary>Details</summary>
Motivation: Industrial control systems are increasingly vulnerable to cybersecurity threats through weaknesses in communication protocols, requiring more advanced fuzzing solutions for vulnerability discovery.

Method: Integrates LLMs with multi-agent coordination using RAG for domain knowledge and QLoRA fine-tuning for protocol-aware input generation, optimizing seed generation, mutation strategies, and feedback-driven refinement.

Result: Achieved 88-92% test case pass rate, generated more exception triggers, maintained over 90% seed coverage and 4.2-4.6 bits Shannon entropy, and identified three zero-day vulnerabilities including one registered by CNVD in real-world power plant deployments.

Conclusion: MALF demonstrates the transformative potential of multi-agent LLMs in ICS cybersecurity, providing a scalable automated framework that sets new standards for vulnerability discovery and strengthens critical infrastructure security.

Abstract: Industrial control systems (ICS) are vital to modern infrastructure but
increasingly vulnerable to cybersecurity threats, particularly through
weaknesses in their communication protocols. This paper presents MALF
(Multi-Agent LLM Fuzzing Framework), an advanced fuzzing solution that
integrates large language models (LLMs) with multi-agent coordination to
identify vulnerabilities in industrial control protocols (ICPs). By leveraging
Retrieval-Augmented Generation (RAG) for domain-specific knowledge and QLoRA
fine-tuning for protocol-aware input generation, MALF enhances fuzz testing
precision and adaptability. The multi-agent framework optimizes seed
generation, mutation strategies, and feedback-driven refinement, leading to
improved vulnerability discovery. Experiments on protocols like Modbus/TCP,
S7Comm, and Ethernet/IP demonstrate that MALF surpasses traditional methods,
achieving a test case pass rate (TCPR) of 88-92% and generating more exception
triggers (ETN). MALF also maintains over 90% seed coverage and Shannon entropy
values between 4.2 and 4.6 bits, ensuring diverse, protocol-compliant
mutations. Deployed in a real-world Industrial Attack-Defense Range for power
plants, MALF identified critical vulnerabilities, including three zero-day
flaws, one confirmed and registered by CNVD. These results validate MALF's
effectiveness in real-world fuzzing applications. This research highlights the
transformative potential of multi-agent LLMs in ICS cybersecurity, offering a
scalable, automated framework that sets a new standard for vulnerability
discovery and strengthens critical infrastructure security against emerging
threats.

</details>


### [221] [A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison](https://arxiv.org/abs/2510.02707)
*Chinthana Wimalasuriya,Spyros Tragoudas*

Main category: cs.CR

TL;DR: A statistical approach using compressed/uncompressed neural network pairs to detect adversarial attacks with near-perfect accuracy and low false positives.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attack detection methods struggle with detecting unseen attacks and achieving high accuracy across different attack types.

Method: Generate adversarial presence metric by comparing behavior of compressed/uncompressed neural network pairs, establishing detection baseline before deployment.

Result: Achieves near-perfect detection across wide range of attack types, significantly reduces false positives compared to state-of-the-art techniques.

Conclusion: The method is reliable and practical for real-world applications, enabling effective real-time adversarial detection.

Abstract: Adversarial attacks present a significant threat to modern machine learning
systems. Yet, existing detection methods often lack the ability to detect
unseen attacks or detect different attack types with a high level of accuracy.
In this work, we propose a statistical approach that establishes a detection
baseline before a neural network's deployment, enabling effective real-time
adversarial detection. We generate a metric of adversarial presence by
comparing the behavior of a compressed/uncompressed neural network pair. Our
method has been tested against state-of-the-art techniques, and it achieves
near-perfect detection across a wide range of attack types. Moreover, it
significantly reduces false positives, making it both reliable and practical
for real-world applications.

</details>


### [222] [Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs](https://arxiv.org/abs/2510.02833)
*Zhixin Xie,Xurui Song,Jun Luo*

Main category: cs.CR

TL;DR: This paper presents a novel jailbreak attack on LLMs using only 10 benign QA pairs, exploiting overfitting to bypass safety alignment without detectable harmful content.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning-based jailbreak attacks are detectable by moderation models. The authors aim to develop a stealthier attack that uses only benign data to compromise LLM safety.

Method: Two-step fine-tuning: first overfit LLM with benign QA pairs containing identical refusal answers, then fine-tune with standard benign answers to make the model forget refusal attitudes and comply with harmful requests.

Result: Successfully jailbreaks 10 different LLMs, achieving significant advantages in both attack effectiveness and stealth compared to 5 existing baselines.

Conclusion: Reveals previously unknown security vulnerabilities in LLMs, showing that even benign fine-tuning can compromise safety, providing new insights into LLM security risks.

Abstract: Despite substantial efforts in safety alignment, recent research indicates
that Large Language Models (LLMs) remain highly susceptible to jailbreak
attacks. Among these attacks, finetuning-based ones that compromise LLMs'
safety alignment via fine-tuning stand out due to its stable jailbreak
performance. In particular, a recent study indicates that fine-tuning with as
few as 10 harmful question-answer (QA) pairs can lead to successful
jailbreaking across various harmful questions. However, such malicious
fine-tuning attacks are readily detectable and hence thwarted by moderation
models. In this paper, we demonstrate that LLMs can be jailbroken by
fine-tuning with only 10 benign QA pairs; our attack exploits the increased
sensitivity of LLMs to fine-tuning data after being overfitted. Specifically,
our fine-tuning process starts with overfitting an LLM via fine-tuning with
benign QA pairs involving identical refusal answers. Further fine-tuning is
then performed with standard benign answers, causing the overfitted LLM to
forget the refusal attitude and thus provide compliant answers regardless of
the harmfulness of a question. We implement our attack on the ten LLMs and
compare it with five existing baselines. Experiments demonstrate that our
method achieves significant advantages in both attack effectiveness and attack
stealth. Our findings expose previously unreported security vulnerabilities in
current LLMs and provide a new perspective on understanding how LLMs' security
is compromised, even with benign fine-tuning. Our code is available at
https://github.com/ZHIXINXIE/tenBenign.

</details>


### [223] [Improved Search-to-Decision Reduction for Random Local Functions](https://arxiv.org/abs/2510.02944)
*Kel Zin Tan,Prashant Nalini Vasudevan*

Main category: cs.CR

TL;DR: This paper presents a new search-to-decision reduction for random local functions defined by any constant-arity predicate, showing that if such functions can be distinguished from random, they can be efficiently inverted.


<details>
  <summary>Details</summary>
Motivation: Random local functions represent natural distributions for constraint satisfaction problems and were proposed by Goldreich as candidates for low-complexity one-way functions and pseudo-random generators. Understanding their cryptographic properties is important for both theoretical cryptography and practical applications.

Method: The authors develop a new search-to-decision reduction that works for any predicate of constant arity, without requiring additional sensitivity properties. The reduction takes an efficient distinguisher and produces an efficient inverter for random local functions.

Result: Given an algorithm that distinguishes the output of a random local function from random with advantage ε, the reduction produces an algorithm that can invert such functions with Õ(m(n/ε)²) outputs, succeeding with probability Ω(ε). This implies that if local functions are one-way, then related functions with shorter output length are pseudo-random generators.

Conclusion: The paper provides a significant improvement over prior work by eliminating the need for additional sensitivity properties in predicates, generalizing to super-constant arity values, and handling noisy predicates. This strengthens the theoretical foundations of random local functions in cryptography.

Abstract: A random local function defined by a $d$-ary predicate $P$ is one where each
output bit is computed by applying $P$ to $d$ randomly chosen bits of its
input. These represent natural distributions of instances for constraint
satisfaction problems. They were put forward by Goldreich as candidates for
low-complexity one-way functions, and have subsequently been widely studied
also as potential pseudo-random generators.
  We present a new search-to-decision reduction for random local functions
defined by any predicate of constant arity. Given any efficient algorithm that
can distinguish, with advantage $\epsilon$, the output of a random local
function with $m$ outputs and $n$ inputs from random, our reduction produces an
efficient algorithm that can invert such functions with
$\tilde{O}(m(n/\epsilon)^2)$ outputs, succeeding with probability
$\Omega(\epsilon)$. This implies that if a family of local functions is
one-way, then a related family with shorter output length is family of
pseudo-random generators.
  Prior to our work, all such reductions that were known required the predicate
to have additional sensitivity properties, whereas our reduction works for any
predicate. Our results also generalise to some super-constant values of the
arity $d$, and to noisy predicates.

</details>


### [224] [SoK: Preconfirmations](https://arxiv.org/abs/2510.02947)
*Aikaterini-Panagiota Stouka,Conor McMenamin,Demetris Kyriacou,Lin Oshitani,Quentin Botha*

Main category: cs.CR

TL;DR: This paper provides a Systematization of Knowledge (SoK) on preconfirmation protocols in blockchain, which offer early guarantees of transaction confirmation to improve user experience by addressing inherent delays in conventional blockchain protocols.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental delay between transaction issuance and confirmation in blockchain systems, which limits user experience gains through confirmation uncertainty, leading to the emergence of preconfirmation protocols.

Method: The authors present core terms and definitions, outline a general framework for preconfirmation protocols, explore their economics and risks, and survey real-world implementations to bridge theory and practice.

Result: A comprehensive systematization of knowledge on preconfirmation protocols that provides a unified framework for understanding and analyzing these protocols across different blockchain implementations.

Conclusion: Preconfirmation protocols are an important development in blockchain technology that address user experience limitations by providing early confirmation guarantees, and this SoK establishes foundational understanding and analysis framework for this emerging field.

Abstract: In recent years, significant research efforts have focused on improving
blockchain throughput and confirmation speeds without compromising security.
While decreasing the time it takes for a transaction to be included in the
blockchain ledger enhances user experience, a fundamental delay still remains
between when a transaction is issued by a user and when its inclusion is
confirmed in the blockchain ledger. This delay limits user experience gains
through the confirmation uncertainty it brings for users. This inherent delay
in conventional blockchain protocols has led to the emergence of
preconfirmation protocols -- protocols that provide users with early guarantees
of eventual transaction confirmation.
  This article presents a Systematization of Knowledge (SoK) on
preconfirmations. We present the core terms and definitions needed to
understand preconfirmations, outline a general framework for preconfirmation
protocols, and explore the economics and risks of preconfirmations. Finally, we
survey and apply our framework to several implementations of real-world
preconfirmation protocols, bridging the gap between theory and practice.

</details>


### [225] [SoK: Kicking CAN Down the Road. Systematizing CAN Security Knowledge](https://arxiv.org/abs/2510.02960)
*Khaled Serag,Zhaozhou Tang,Sungwoo Kim,Vireshwar Kumar,Dave,Tian,Saman Zonouz,Raheem Beyah,Dongyan Xu,Z. Berkay Celik*

Main category: cs.CR

TL;DR: This paper systematizes CAN security knowledge, creates taxonomy of attackers/attacks/defenses, identifies root causes of security issues, and analyzes emerging in-vehicle bus technologies to show they share similar security challenges.


<details>
  <summary>Details</summary>
Motivation: CAN security literature lacks structured systematization, making it difficult to assess attack severity, defense efficacy, and identify security gaps. Non-experts struggle to understand attack relevancy, and new IVB technologies risk creating false security perceptions.

Method: Developed comprehensive taxonomy and assessment models of attackers, attacks, and defenses; identified replicable attacks and defense gaps; analyzed root causes; formally analyzed three emerging IVBs to identify shared security issues.

Result: Found that CAN is more securable than perceived, most insecurity root causes are shared across IVBs, and adopting newer IVB technology doesn't solve persistent security issues. Identified specific security gaps and their root causes.

Conclusion: The paper challenges common misconceptions about CAN security and emerging IVB technologies, highlighting that security issues persist across different bus technologies and providing future research directions for securing IVB communication.

Abstract: For decades, the Controller Area Network (CAN) has served as the primary
in-vehicle bus (IVB) and extended its use to many non-vehicular systems. Over
the past years, CAN security has been intensively scrutinized, yielding
extensive research literature. Despite its wealth, the literature lacks
structured systematization, complicating efforts to assess attack severity,
defense efficacy, identify security gaps, or root causes. This leaves non
experts uncertain about the relevancy of specific attacks or defenses to their
systems, inadvertently portraying CAN as irredeemably insecure. Further, the
introduction of new IVB technologies--CAN evolutions, add-ons, and alternative
buses--with heightened security claims risks fostering the misconception that
merely adopting these technologies resolves CAN's security challenges.
  This paper systematizes existing CAN security knowledge, presenting a
comprehensive taxonomy and assessment models of attackers, attacks, and
defenses. It identifies replicable attacks and defense gaps, investigating
their root causes as inherent, accidental, unique, or universal. It then
extrapolates these insights to emerging IVB technologies by formally analyzing
three emerging IVBs to identify shared root causes with CAN and assess their
ability to close security gaps. The findings challenge common perceptions,
demonstrating that CAN is more securable than perceived, that most insecurity
root causes are shared across IVBs, and that merely adopting newer IVB
technology does not solve persistent security issues. The paper concludes by
highlighting future research directions to secure IVB communication down the
road.

</details>


### [226] [External Data Extraction Attacks against Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2510.02964)
*Yu He,Yifei Chen,Yiming Li,Shuo Shao,Leyi Qi,Boheng Li,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: This paper presents SECRET, a comprehensive framework for external data extraction attacks (EDEAs) against RAG systems, which significantly outperforms previous methods by achieving 35% data extraction from Claude 3.7 Sonnet where other attacks failed.


<details>
  <summary>Details</summary>
Motivation: RAG systems enhance LLMs but introduce risks of extracting sensitive/copyrighted data from knowledge bases. Existing studies lack formal frameworks and robust attack performance, leaving real-world EDEA feasibility unclear.

Method: Proposes a unified EDEA framework with three components: extraction instruction, jailbreak operator, and retrieval trigger. Develops SECRET with adaptive LLM optimization for jailbreak prompts and cluster-focused triggering strategy for efficient retrieval trigger generation.

Result: SECRET significantly outperforms previous attacks across 4 models and is effective against all 16 tested RAG instances. It successfully extracts 35% of data from Claude 3.7 Sonnet RAG, while other attacks yield 0% extraction.

Conclusion: The findings demonstrate the serious threat of EDEAs against RAG systems and call for attention to this emerging security vulnerability in retrieval-augmented LLM applications.

Abstract: In recent years, RAG has emerged as a key paradigm for enhancing large
language models (LLMs). By integrating externally retrieved information, RAG
alleviates issues like outdated knowledge and, crucially, insufficient domain
expertise. While effective, RAG introduces new risks of external data
extraction attacks (EDEAs), where sensitive or copyrighted data in its
knowledge base may be extracted verbatim. These risks are particularly acute
when RAG is used to customize specialized LLM applications with private
knowledge bases. Despite initial studies exploring these risks, they often lack
a formalized framework, robust attack performance, and comprehensive
evaluation, leaving critical questions about real-world EDEA feasibility
unanswered.
  In this paper, we present the first comprehensive study to formalize EDEAs
against retrieval-augmented LLMs. We first formally define EDEAs and propose a
unified framework decomposing their design into three components: extraction
instruction, jailbreak operator, and retrieval trigger, under which prior
attacks can be considered instances within our framework. Guided by this
framework, we develop SECRET: a Scalable and EffeCtive exteRnal data Extraction
aTtack. Specifically, SECRET incorporates (1) an adaptive optimization process
using LLMs as optimizers to generate specialized jailbreak prompts for EDEAs,
and (2) cluster-focused triggering, an adaptive strategy that alternates
between global exploration and local exploitation to efficiently generate
effective retrieval triggers. Extensive evaluations across 4 models reveal that
SECRET significantly outperforms previous attacks, and is highly effective
against all 16 tested RAG instances. Notably, SECRET successfully extracts 35%
of the data from RAG powered by Claude 3.7 Sonnet for the first time, whereas
other attacks yield 0% extraction. Our findings call for attention to this
emerging threat.

</details>


### [227] [Untargeted Jailbreak Attack](https://arxiv.org/abs/2510.02999)
*Xinzhe Huang,Wenjing Hu,Tianhang Zheng,Kedong Xiu,Xiaojun Jia,Di Wang,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: Proposes UJA, the first gradient-based untargeted jailbreak attack on LLMs that maximizes unsafety probability without predefined targets, achieving 80%+ success rates with only 100 iterations.


<details>
  <summary>Details</summary>
Motivation: Existing targeted jailbreak attacks constrain the search space by requiring predefined target responses, limiting attack efficacy and efficiency due to large optimization gaps.

Method: Formulates untargeted attack objective to maximize unsafety probability, decomposes into differentiable sub-objectives for optimizing harmful responses and adversarial prompts, validated by theoretical analysis.

Result: Achieves over 80% attack success rates against safety-aligned LLMs with only 100 optimization iterations, outperforming state-of-the-art methods by over 20%.

Conclusion: UJA's unrestricted objective significantly expands search space, enabling more flexible and efficient exploration of LLM vulnerabilities compared to targeted attacks.

Abstract: Existing gradient-based jailbreak attacks on Large Language Models (LLMs),
such as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize
adversarial suffixes to align the LLM output with a predefined target response.
However, by restricting the optimization objective as inducing a predefined
target, these methods inherently constrain the adversarial search space, which
limit their overall attack efficacy. Furthermore, existing methods typically
require a large number of optimization iterations to fulfill the large gap
between the fixed target and the original model response, resulting in low
attack efficiency.
  To overcome the limitations of targeted jailbreak attacks, we propose the
first gradient-based untargeted jailbreak attack (UJA), aiming to elicit an
unsafe response without enforcing any predefined patterns. Specifically, we
formulate an untargeted attack objective to maximize the unsafety probability
of the LLM response, which can be quantified using a judge model. Since the
objective is non-differentiable, we further decompose it into two
differentiable sub-objectives for optimizing an optimal harmful response and
the corresponding adversarial prompt, with a theoretical analysis to validate
the decomposition. In contrast to targeted jailbreak attacks, UJA's
unrestricted objective significantly expands the search space, enabling a more
flexible and efficient exploration of LLM vulnerabilities.Extensive evaluations
demonstrate that \textsc{UJA} can achieve over 80\% attack success rates
against recent safety-aligned LLMs with only 100 optimization iterations,
outperforming the state-of-the-art gradient-based attacks such as I-GCG and
COLD-Attack by over 20\%.

</details>


### [228] [Protecting Persona Biometric Data: The Case of Facial Privacy](https://arxiv.org/abs/2510.03035)
*Lambert Hogenhout,Rinzin Wangmo*

Main category: cs.CR

TL;DR: The paper analyzes the privacy threats from unregulated facial recognition technologies and proposes a new policy framework shifting from data-as-property to inalienable rights model.


<details>
  <summary>Details</summary>
Motivation: The proliferation of facial recognition technologies without proper consent and weak legal protections creates significant privacy threats and regulatory gaps that need addressing.

Method: Conducted comprehensive review and comparison of existing legal frameworks including GDPR, LGPD, PIPEDA, and privacy laws in China, Singapore, South Korea, Japan, and US sector-specific laws like BIPA.

Result: Analysis reveals legal loopholes and ambiguities that leave individuals vulnerable to privacy violations, discriminatory bias, and long-lasting harm from biometric data theft.

Conclusion: Proposes a new policy framework that shifts from data as property to inalienable rights model to protect fundamental human rights against unchecked technological expansion.

Abstract: The proliferation of digital technologies has led to unprecedented data
collection, with facial data emerging as a particularly sensitive commodity.
Companies are increasingly leveraging advanced facial recognition technologies,
often without the explicit consent or awareness of individuals, to build
sophisticated surveillance capabilities. This practice, fueled by weak and
fragmented laws in many jurisdictions, has created a regulatory vacuum that
allows for the commercialization of personal identity and poses significant
threats to individual privacy and autonomy. This article introduces the concept
of Facial Privacy. It analyzes the profound challenges posed by unregulated
facial recognition by conducting a comprehensive review of existing legal
frameworks. It examines and compares regulations such as the GDPR, Brazil's
LGPD, Canada's PIPEDA, and privacy acts in China, Singapore, South Korea, and
Japan, alongside sector-specific laws in the United States like the Illinois
Biometric Information Privacy Act (BIPA). The analysis highlights the societal
impacts of this technology, including the potential for discriminatory bias and
the long-lasting harm that can result from the theft of immutable biometric
data. Ultimately, the paper argues that existing legal loopholes and
ambiguities leave individuals vulnerable. It proposes a new policy framework
that shifts the paradigm from data as property to a model of inalienable
rights, ensuring that fundamental human rights are upheld against unchecked
technological expansion.

</details>


### [229] [TPM-Based Continuous Remote Attestation and Integrity Verification for 5G VNFs on Kubernetes](https://arxiv.org/abs/2510.03219)
*Al Nahian Bin Emran,Rajendra Upadhyay,Rajendra Paudyal,Lisa Donnan,Duminda Wijesekera*

Main category: cs.CR

TL;DR: A TPM 2.0-based continuous remote attestation solution for 5G core components on Kubernetes, using IMA and Keylime framework to provide hardware-based runtime integrity validation.


<details>
  <summary>Details</summary>
Motivation: Current 5G security specifications lack mechanisms to continuously validate NFV integrity at runtime, assuming network functions remain trustworthy after authentication. This gap conflicts with Zero Trust principles.

Method: Uses Linux Integrity Measurement Architecture (IMA) and Trusted Platform Module (TPM) with custom IMA template to isolate pod-level measurements. Integrates Keylime framework for continuous remote attestation on k3s cluster.

Result: The system successfully detects unauthorized modifications in real-time, labels each pod's trust state, and generates detailed audit logs for core 5G functions (AMF, SMF, UPF).

Conclusion: Provides hardware-based continuous attestation for cloud-native 5G deployments, enhancing resilience in multi-vendor and mission-critical scenarios by implementing Zero Trust principles.

Abstract: In the rapidly evolving landscape of 5G technology, the adoption of
cloud-based infrastructure for the deployment of 5G services has become
increasingly common. Using a service-based architecture, critical 5G
components, such as the Access and Mobility Management Function (AMF), Session
Management Function (SMF), and User Plane Function (UPF), now run as
containerized pods on Kubernetes clusters. Although this approach improves
scalability, flexibility, and resilience, it also introduces new security
challenges, particularly to ensure the integrity and trustworthiness of these
components. Current 5G security specifications (for example, 3GPP TS 33.501)
focus on communication security and assume that network functions remain
trustworthy after authentication, consequently lacking mechanisms to
continuously validate the integrity of NVFs at runtime. To close this gap, and
to align with Zero Trust principles of 'never trust, always verify', we present
a TPM 2.0-based continuous remote attestation solution for core 5G components
deployed on Kubernetes. Our approach uses the Linux Integrity Measurement
Architecture (IMA) and a Trusted Platform Module (TPM) to provide
hardware-based runtime validation. We integrate the open-source Keylime
framework with a custom IMA template that isolates pod-level measurements,
allowing per-pod integrity verification. A prototype on a k3s cluster
(consisting of 1 master, 2 worker nodes) was implemented to attest to core
functions, including AMF, SMF and UPF. The experimental results show that the
system detects unauthorized modifications in real time, labels each pod's trust
state, and generates detailed audit logs. This work provides hardware-based
continuous attestation for cloud native and edge deployments, strengthening the
resilience of 5G as critical infrastructure in multi-vendor and
mission-critical scenarios of 5G.

</details>
