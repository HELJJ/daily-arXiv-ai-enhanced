<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 91]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.LG](#cs.LG) [Total: 180]
- [cs.AI](#cs.AI) [Total: 115]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Cyclic Ablation: Testing Concept Localization against Functional Regeneration in AI](https://arxiv.org/abs/2509.25220)
*Eduard Kapelko*

Main category: cs.CL

TL;DR: Cyclic ablation method shows deception in language models is resilient and deeply entangled with core cognitive abilities, not localized. Attempts to remove deception cause performance decay.


<details>
  <summary>Details</summary>
Motivation: To determine if undesirable behaviors like deception are localized functions that can be removed or deeply intertwined with a model's core cognitive abilities.

Method: Introduced cyclic ablation combining sparse autoencoders, targeted ablation, and adversarial training on DistilGPT-2 to eliminate the concept of deception.

Result: Deception was highly resilient - model consistently recovered deceptive behavior after each ablation cycle via functional regeneration. Each ablation attempt caused gradual decay in linguistic performance (rising perplexity).

Conclusion: Complex concepts are distributed and entangled, showing limitations of direct model editing through mechanistic interpretability.

Abstract: Safety and controllability are critical for large language models. A central
question is whether undesirable behaviors like deception are localized
functions that can be removed, or if they are deeply intertwined with a model's
core cognitive abilities. We introduce "cyclic ablation," an iterative method
to test this. By combining sparse autoencoders, targeted ablation, and
adversarial training on DistilGPT-2, we attempted to eliminate the concept of
deception. We found that, contrary to the localization hypothesis, deception
was highly resilient. The model consistently recovered its deceptive behavior
after each ablation cycle via adversarial training, a process we term
functional regeneration. Crucially, every attempt at this "neurosurgery" caused
a gradual but measurable decay in general linguistic performance, reflected by
a consistent rise in perplexity. These findings are consistent with the view
that complex concepts are distributed and entangled, underscoring the
limitations of direct model editing through mechanistic interpretability.

</details>


### [2] [From Internal Representations to Text Quality: A Geometric Approach to LLM Evaluation](https://arxiv.org/abs/2509.25359)
*Viacheslav Yusupov,Danil Maksimov,Ameliia Alaeva,Anna Vasileva,Anna Antipina,Tatyana Zaitseva,Alina Ermilova,Evgeny Burnaev,Egor Shvetsov*

Main category: cs.CL

TL;DR: This paper shows that geometric properties of LLM internal representations can serve as reliable proxies for evaluating text quality without needing human-annotated datasets.


<details>
  <summary>Details</summary>
Motivation: To bridge internal and external analysis of LLMs by finding reliable geometric metrics that can assess text quality without requiring reference texts or human annotations.

Method: Validated multiple geometric metrics (Maximum Explainable Variance, Effective Rank, Intrinsic Dimensionality, MAUVE score, Schatten Norms) across different layers of LLMs to evaluate text naturalness and quality.

Result: Found that Intrinsic Dimensionality and Effective Rank serve as universal assessments of text quality, with different models consistently ranking text from various sources in the same order based on these geometric properties.

Conclusion: Geometric properties of internal representations provide reference-free text quality evaluation that reflects inherent text characteristics rather than model-specific artifacts, offering practical advantages for automated evaluation pipelines.

Abstract: This paper bridges internal and external analysis approaches to large
language models (LLMs) by demonstrating that geometric properties of internal
model representations serve as reliable proxies for evaluating generated text
quality. We validate a set of metrics including Maximum Explainable Variance,
Effective Rank, Intrinsic Dimensionality, MAUVE score, and Schatten Norms
measured across different layers of LLMs, demonstrating that Intrinsic
Dimensionality and Effective Rank can serve as universal assessments of text
naturalness and quality. Our key finding reveals that different models
consistently rank text from various sources in the same order based on these
geometric properties, indicating that these metrics reflect inherent text
characteristics rather than model-specific artifacts. This allows a
reference-free text quality evaluation that does not require human-annotated
datasets, offering practical advantages for automated evaluation pipelines.

</details>


### [3] [Generative Value Conflicts Reveal LLM Priorities](https://arxiv.org/abs/2509.25369)
*Andy Liu,Kshitish Ghate,Mona Diab,Daniel Fried,Atoosa Kasirzadeh,Max Kleiman-Weiner*

Main category: cs.CL

TL;DR: ConflictScope is an automated pipeline that evaluates how LLMs prioritize conflicting values by generating scenarios where models must choose between two values, revealing that models shift from protective to personal values in open-ended settings, and system prompting can improve alignment by 14%.


<details>
  <summary>Details</summary>
Motivation: Existing alignment datasets lack value conflict scenarios, making it difficult to evaluate how LLMs prioritize different values when they conflict in real-world deployment.

Method: Developed ConflictScope pipeline that automatically generates value conflict scenarios from user-defined value sets, uses LLM-written user prompts, and evaluates free-text responses to elicit value rankings.

Result: Models shift from supporting protective values (harmlessness) to personal values (user autonomy) in open-ended settings. Including detailed value orderings in system prompts improves alignment with target rankings by 14%.

Conclusion: Evaluating value prioritization is crucial for LLM alignment, and system prompting can moderately improve alignment under value conflicts, providing a foundation for future work in this area.

Abstract: Past work seeks to align large language model (LLM)-based assistants with a
target set of values, but such assistants are frequently forced to make
tradeoffs between values when deployed. In response to the scarcity of value
conflict in existing alignment datasets, we introduce ConflictScope, an
automatic pipeline to evaluate how LLMs prioritize different values. Given a
user-defined value set, ConflictScope automatically generates scenarios in
which a language model faces a conflict between two values sampled from the
set. It then prompts target models with an LLM-written "user prompt" and
evaluates their free-text responses to elicit a ranking over values in the
value set. Comparing results between multiple-choice and open-ended
evaluations, we find that models shift away from supporting protective values,
such as harmlessness, and toward supporting personal values, such as user
autonomy, in more open-ended value conflict settings. However, including
detailed value orderings in models' system prompts improves alignment with a
target ranking by 14%, showing that system prompting can achieve moderate
success at aligning LLM behavior under value conflict. Our work demonstrates
the importance of evaluating value prioritization in models and provides a
foundation for future work in this area.

</details>


### [4] [From Faithfulness to Correctness: Generative Reward Models that Think Critically](https://arxiv.org/abs/2509.25409)
*Qiyao Ma,Yunsheng Shi,Hongtao Tian,Chao Wang,Weiming Chang,Ting Yao*

Main category: cs.CL

TL;DR: The paper proposes TRM (Thinking-supervised Reward Model) to address limitations of RLVR in complex tasks like open-domain QA by incorporating critical thinking through sentence-level faithfulness and correctness evaluations.


<details>
  <summary>Details</summary>
Motivation: RLVR struggles with complex tasks due to difficulty verifying correctness in nuanced real-world knowledge scenarios. Current focus on faithfulness causes over-reliance on external sources and reduces critical assessment capabilities.

Method: TRM uses sentence-level thinking supervision: first assesses faithfulness to supporting documents, then applies reasoning to evaluate sentence-level correctness, structuring reward modeling as faithfulness-reasoning-correctness sequence.

Result: TRM significantly improves identification of incorrect sentences and, when incorporated into policy optimization, leads to substantial gains in both answer correctness and usefulness.

Conclusion: TRM's structured approach to reward modeling with critical thinking capabilities enables better assessment of both external and internal knowledge, overcoming limitations of current RLVR methods in complex tasks.

Abstract: Through reinforcement learning with verifiable rewards (RLVR), large language
models have achieved substantial progress in domains with easily verifiable
outcomes, such as mathematics and coding. However, when applied to more complex
tasks like open-domain question answering, RLVR faces significant challenges
due to the difficulty of verifying correctness. The nuanced and ambiguous
nature of real-world knowledge makes it difficult to reliably evaluate
correctness in these settings, necessitating further abilities that extend
beyond mere logical consistency to encompass an understanding and assessment of
both external and internal knowledge. Recent work has primarily focused on
improving faithfulness, defined as semantic alignment with supporting
documents, which can cause models to rely excessively on external sources and
diminish their capacity for critical assessment. To address this, we propose
the Thinking-supervised Reward Model (TRM), which incorporates sentence-level
thinking supervision to endow reward models with critical thinking abilities.
Given a query, answer, and supporting documents, TRM first assesses the
faithfulness of each answer sentence to the supporting documents, and then
applies a reasoning step to evaluate sentence-level correctness. By structuring
reward modeling as a sequence of faithfulness, reasoning, and correctness
evaluations, TRM encourages models to critically assess and leverage both
external and internal knowledge. Experiments on reward signals demonstrate that
TRM substantially improves the identification of incorrect sentences, and
incorporating TRM into policy optimization leads to significant gains in both
answer correctness and usefulness.

</details>


### [5] [Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization](https://arxiv.org/abs/2509.25416)
*Jiacheng Shi,Hongfei Du,Yangfan He,Y. Alicia Hong,Ye Gao*

Main category: cs.CL

TL;DR: EASPO is a post-training framework that aligns diffusion TTS with fine-grained emotional preferences at intermediate denoising steps using a time-conditioned scoring model.


<details>
  <summary>Details</summary>
Motivation: Existing emotional text-to-speech methods rely on coarse labels or proxy classifiers and receive only utterance-level feedback, lacking fine-grained emotional control.

Method: Introduces Emotion-Aware Stepwise Preference Optimization (EASPO) with EASPM model that scores noisy intermediate speech states to enable automatic preference pair construction and controllable emotional shaping.

Result: Experiments show superior performance over existing methods in both expressiveness and naturalness.

Conclusion: EASPO enables fine-grained emotional control in text-to-speech through stepwise preference optimization at intermediate denoising steps.

Abstract: Emotional text-to-speech seeks to convey affect while preserving
intelligibility and prosody, yet existing methods rely on coarse labels or
proxy classifiers and receive only utterance-level feedback. We introduce
Emotion-Aware Stepwise Preference Optimization (EASPO), a post-training
framework that aligns diffusion TTS with fine-grained emotional preferences at
intermediate denoising steps. Central to our approach is EASPM, a
time-conditioned model that scores noisy intermediate speech states and enables
automatic preference pair construction. EASPO optimizes generation to match
these stepwise preferences, enabling controllable emotional shaping.
Experiments show superior performance over existing methods in both
expressiveness and naturalness.

</details>


### [6] [SimulRAG: Simulator-based RAG for Grounding LLMs in Long-form Scientific QA](https://arxiv.org/abs/2509.25459)
*Haozhou Xu,Dongxia Wu,Matteo Chinazzi,Ruijia Niu,Rose Yu,Yi-An Ma*

Main category: cs.CL

TL;DR: SimulRAG is a novel RAG framework that uses scientific simulators as retrieval sources to reduce LLM hallucination in long-form scientific QA, achieving significant improvements in informativeness and factuality.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer from hallucination in long-form scientific question answering, and existing RAG approaches cannot effectively utilize scientific simulators which are crucial for validating hypotheses and improving answer factuality.

Method: Proposes SimulRAG with two key components: a generalized simulator retrieval interface for modality transformation between text and numbers, and a claim-level generation method using uncertainty estimation scores and simulator boundary assessment (UE+SBA) for efficient verification and updating.

Result: SimulRAG outperforms traditional RAG baselines by 30.4% in informativeness and 16.3% in factuality. UE+SBA further improves efficiency and quality for claim-level generation.

Conclusion: SimulRAG effectively addresses the challenges of retrieving from scientific simulators and verifying long-form answers, providing a robust framework for trustworthy scientific question answering using simulation-based grounding.

Abstract: Large language models (LLMs) show promise in solving scientific problems.
They can help generate long-form answers for scientific questions, which are
crucial for comprehensive understanding of complex phenomena that require
detailed explanations spanning multiple interconnected concepts and evidence.
However, LLMs often suffer from hallucination, especially in the challenging
task of long-form scientific question answering. Retrieval-Augmented Generation
(RAG) approaches can ground LLMs by incorporating external knowledge sources to
improve trustworthiness. In this context, scientific simulators, which play a
vital role in validating hypotheses, offer a particularly promising retrieval
source to mitigate hallucination and enhance answer factuality. However,
existing RAG approaches cannot be directly applied for scientific
simulation-based retrieval due to two fundamental challenges: how to retrieve
from scientific simulators, and how to efficiently verify and update long-form
answers. To overcome these challenges, we propose the simulator-based RAG
framework (SimulRAG) and provide a long-form scientific QA benchmark covering
climate science and epidemiology with ground truth verified by both simulations
and human annotators. In this framework, we propose a generalized simulator
retrieval interface to transform between textual and numerical modalities. We
further design a claim-level generation method that utilizes uncertainty
estimation scores and simulator boundary assessment (UE+SBA) to efficiently
verify and update claims. Extensive experiments demonstrate SimulRAG
outperforms traditional RAG baselines by 30.4% in informativeness and 16.3% in
factuality. UE+SBA further improves efficiency and quality for claim-level
generation.

</details>


### [7] [The Rise of AfricaNLP: Contributions, Contributors, and Community Impact (2005-2025)](https://arxiv.org/abs/2509.25477)
*Tadesse Destaw Belay,Kedir Yassin Hussen,Sukairaj Hafiz Imam,Iqra Ameer,Ibrahim Said Ahmad,Isa Inuwa-Dutse,Idris Abdulmumin,Grigori Sidorov,Vukosi Marivate,Seid Muhie Yimam,Shamsuddeen Hassan Muhammad*

Main category: cs.CL

TL;DR: This study analyzes the progress of African NLP research over two decades using 1.9K paper abstracts, 4.9K authors, and 7.8K annotated contribution sentences to understand research evolution, contributions, and key stakeholders.


<details>
  <summary>Details</summary>
Motivation: To track the progress of NLP research and automatically analyze contributions of African NLP papers, providing insights into the field's evolution and key contributors in Africa.

Method: Quantitative examination using 1.9K NLP paper abstracts, 4.9K author contributors, and 7.8K human-annotated contribution sentences (AfricaNLPContributions) with benchmark results.

Result: Created a dataset and continuously existing NLP progress tracking website that provides powerful tools for tracing AfricaNLP research trends and generating data-driven literature surveys.

Conclusion: The study provides a comprehensive framework for analyzing African NLP research progress, identifying key contributors, and understanding the field's evolution over two decades.

Abstract: Natural Language Processing (NLP) is undergoing constant transformation, as
Large Language Models (LLMs) are driving daily breakthroughs in research and
practice. In this regard, tracking the progress of NLP research and
automatically analyzing the contributions of research papers provides key
insights into the nature of the field and the researchers. This study explores
the progress of African NLP (AfricaNLP) by asking (and answering) basic
research questions such as: i) How has the nature of NLP evolved over the last
two decades?, ii) What are the contributions of AfricaNLP papers?, and iii)
Which individuals and organizations (authors, affiliated institutions, and
funding bodies) have been involved in the development of AfricaNLP? We
quantitatively examine the contributions of AfricaNLP research using 1.9K NLP
paper abstracts, 4.9K author contributors, and 7.8K human-annotated
contribution sentences (AfricaNLPContributions) along with benchmark results.
Our dataset and continuously existing NLP progress tracking website provide a
powerful lens for tracing AfricaNLP research trends and hold potential for
generating data-driven literature surveys.

</details>


### [8] [Not Wrong, But Untrue: LLM Overconfidence in Document-Based Queries](https://arxiv.org/abs/2509.25498)
*Nick Hagar,Wilma Agustianto,Nicholas Diakopoulos*

Main category: cs.CL

TL;DR: Evaluating ChatGPT, Gemini, and NotebookLM on journalistic tasks reveals 30% hallucination rates, with interpretive overconfidence being the main issue rather than invented facts.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in newsrooms but their tendency to hallucinate threatens core journalistic practices of sourcing, attribution, and accuracy.

Method: Tested three LLMs (ChatGPT, Gemini, NotebookLM) on reporting tasks using a 300-document corpus about TikTok litigation, varying prompt specificity and context size, with sentence-level annotation using a hallucination taxonomy.

Result: 30% of outputs contained hallucinations, with ChatGPT and Gemini showing 40% rates vs NotebookLM's 13%. Most errors were interpretive overconfidence - adding unsupported characterizations and transforming opinions into general statements.

Conclusion: LLMs have fundamental epistemological mismatch with journalism's requirement for explicit sourcing. Need journalism-specific tools that enforce accurate attribution rather than optimize for fluency.

Abstract: Large language models (LLMs) are increasingly used in newsroom workflows, but
their tendency to hallucinate poses risks to core journalistic practices of
sourcing, attribution, and accuracy. We evaluate three widely used tools -
ChatGPT, Gemini, and NotebookLM - on a reporting-style task grounded in a
300-document corpus related to TikTok litigation and policy in the U.S. We vary
prompt specificity and context size and annotate sentence-level outputs using a
taxonomy to measure hallucination type and severity. Across our sample, 30% of
model outputs contained at least one hallucination, with rates approximately
three times higher for Gemini and ChatGPT (40%) than for NotebookLM (13%).
Qualitatively, most errors did not involve invented entities or numbers;
instead, we observed interpretive overconfidence - models added unsupported
characterizations of sources and transformed attributed opinions into general
statements. These patterns reveal a fundamental epistemological mismatch: While
journalism requires explicit sourcing for every claim, LLMs generate
authoritative-sounding text regardless of evidentiary support. We propose
journalism-specific extensions to existing hallucination taxonomies and argue
that effective newsroom tools need architectures that enforce accurate
attribution rather than optimize for fluency.

</details>


### [9] [Beyond WER: Probing Whisper's Sub-token Decoder Across Diverse Language Resource Levels](https://arxiv.org/abs/2509.25516)
*Siyu Liang,Nicolas Ballier,Gina-Anne Levow,Richard Wright*

Main category: cs.CL

TL;DR: Fine-grained analysis of Whisper's multilingual decoder reveals systematic decoding disparities between high and low resource languages, with higher resource languages showing better token ranking, confidence, and diversity.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms and fairness of multilingual ASR systems, particularly how they perform across languages with different resource levels, since aggregate error rates mask underlying disparities.

Method: Traces beam search path to capture sub-token hypotheses and probabilities, uses PCA and t-SNE analysis to examine clustering patterns in sub-token usage across languages.

Result: Higher resource languages have higher likelihood of correct token ranking, greater confidence, lower predictive entropy, and more diverse alternatives. Lower resource languages perform worse on these metrics and show distinct clustering patterns influenced by typology.

Conclusion: Sub-token probing uncovers systematic decoding disparities that are masked by aggregate error rates, pointing towards targeted interventions to address imbalanced development of speech technology.

Abstract: While large multilingual automatic speech recognition (ASR) models achieve
remarkable performance, the internal mechanisms of the end-to-end pipeline,
particularly concerning fairness and efficacy across languages, remain
underexplored. This paper introduces a fine-grained analysis of Whisper's
multilingual decoder, examining its sub-token hypotheses during transcription
across languages with various resource levels. Our method traces the beam
search path, capturing sub-token guesses and their associated probabilities.
Results reveal that higher resource languages benefit from higher likelihood of
the correct token being top-ranked, greater confidence, lower predictive
entropy, and more diverse alternative candidates. Lower resource languages fare
worse on these metrics, but also exhibit distinct clustering patterns in
sub-token usage sometimes influenced by typology in our PCA and t-SNE analysis.
This sub-token probing uncovers systematic decoding disparities masked by
aggregate error rates and points towards targeted interventions to ameliorate
the imbalanced development of speech technology.

</details>


### [10] [MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources](https://arxiv.org/abs/2509.25531)
*Huu Nguyen,Victor May,Harsh Raj,Marianna Nezhurina,Yishan Wang,Yanqi Luo,Minh Chien Vu,Taishi Nakamura,Ken Tsui,Van Khue Nguyen,David Salinas,Aleksandra Krasnodębska,Christoph Schuhmann,Mats Leon Richter,Xuan-Son,Vu,Jenia Jitsev*

Main category: cs.CL

TL;DR: MixtureVitae is a legally safe pretraining corpus combining public-domain, permissively licensed, and justified low-risk sources with instruction/reasoning data, achieving strong performance while minimizing legal risks.


<details>
  <summary>Details</summary>
Motivation: To create a pretraining corpus that minimizes legal risks while maintaining competitive model performance, reducing reliance on indiscriminate web scraping.

Method: Multi-stage pipeline with license-aware filtering, safety/quality screening, domain-aware mixing, combining public-domain, permissive licenses (CC-BY/Apache), justified low-risk sources (government works, EU TDM), and synthetic data with documented provenance.

Result: Models trained on MixtureVitae consistently outperform other permissive datasets across benchmarks, particularly strong on math/code and competitive on QA tasks. At 1.7B/300B setting, surpass FineWeb-Edu and approach DCLM performance.

Conclusion: Permissive-first, risk-mitigated data provides a practical and legally safe foundation for training capable LLMs without sacrificing competitiveness, demonstrating viable alternative to indiscriminate web scraping.

Abstract: We present MixtureVitae, an open-access pretraining corpus built to minimize
legal risk while providing strong model performance. MixtureVitae follows a
risk-mitigated sourcing strategy that combines public-domain and permissively
licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions
(e.g., government works and EU TDM-eligible sources), alongside targeted
instruction, reasoning and synthetic data with documented provenance. We detail
a transparent, multi-stage pipeline for license-aware filtering, safety and
quality screening, and domain-aware mixing, and we release the dataset and
curation recipes to support reproducible research. In controlled experiments
using the open-sci-ref training protocol (fixed architectures at
130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens),
models trained on MixtureVitae consistently outperform other permissive
datasets across a suite of standard benchmarks, and at the 1.7B/300B setting
they surpass FineWeb-Edu and approach DCLM in the later stages of training.
Performance is particularly strong on math/code and competitive on QA tasks.
These results demonstrate that permissive-first, risk-mitigated data provides a
practical and legally mitigated foundation for training capable LLMs, reducing
reliance on indiscriminate web scraping without sacrificing competitiveness.
Code: https://github.com/ontocord/mixturevitae

</details>


### [11] [Calibrating Verbalized Confidence with Self-Generated Distractors](https://arxiv.org/abs/2509.25532)
*Victor Wang,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: DINCO is a new method that improves LLM confidence calibration by normalizing verbalized confidence scores to account for suggestibility bias, outperforming baselines with fewer inference calls.


<details>
  <summary>Details</summary>
Motivation: LLM-generated verbal confidence scores are often miscalibrated (overconfident on low-accuracy claims), which harms trust and safety. This overconfidence stems from LLM suggestibility when faced with unfamiliar claims.

Method: DINCO estimates suggestibility bias by having LLMs verbalize confidence across self-generated distractors and normalizes by total confidence. Combines generator-validator disagreement with consistency-based confidence estimation.

Result: DINCO provides less saturated, more usable confidence estimates. At 10 inference calls, it outperforms self-consistency at 100 calls. More suggestibility was found on lower-accuracy claims.

Conclusion: DINCO effectively addresses LLM overconfidence by accounting for suggestibility bias through distractor-normalized coherence, providing better calibrated confidence with fewer computational resources.

Abstract: Calibrated confidence estimates are necessary for large language model (LLM)
outputs to be trusted by human users. While LLMs can express their confidence
in human-interpretable ways, verbalized LLM-generated confidence scores have
empirically been found to be miscalibrated, reporting high confidence on
instances with low accuracy and thereby harming trust and safety. We
hypothesize that this overconfidence often stems from a given LLM's heightened
suggestibility when faced with claims that it encodes little information about;
we empirically validate this hypothesis, finding more suggestibility on
lower-accuracy claims. Building on this finding, we introduce
Distractor-Normalized Coherence (DINCO), which estimates and accounts for an
LLM's suggestibility bias by having the model verbalize its confidence
independently across several self-generated distractors (i.e. alternative
claims), and normalizes by the total verbalized confidence. To further improve
calibration, we leverage generator-validator disagreement, augmenting
normalized validator confidence with a consistency-based estimate of generator
confidence. Here, we frame the popular approach of self-consistency as
leveraging coherence across sampled generations, and normalized verbalized
confidence as leveraging coherence across validations on incompatible claims,
allowing us to integrate these complementary dimensions of coherence into
DINCO. Moreover, our analysis shows that DINCO provides less saturated -- and
therefore more usable -- confidence estimates, and that further sampling alone
cannot close the gap between DINCO and baselines, with DINCO at 10 inference
calls outperforming self-consistency at 100.

</details>


### [12] [Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning](https://arxiv.org/abs/2509.25534)
*Zhiling Ye,Yun Yue,Haowen Wang,Xudong Han,Jiadi Jiang,Cheng Wei,Lei Fan,Jiaxin Liang,Shuowen Zhang,Ji Li,Chunxiao Guo,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: Self-Rewarding Rubric-Based Reinforcement Learning improves reasoning performance by using the model itself as a grader and generating rubric-based reward signals, enabling efficient training that surpasses baselines.


<details>
  <summary>Details</summary>
Motivation: Open-ended evaluation is essential for deploying large language models in real-world settings. The observation that using the model itself as a grader improves reasoning performance and makes the model a stronger grader motivated this approach.

Method: Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning - a lightweight framework that uses the model itself as a grader and generates rubric-based reward signals for reinforcement learning.

Result: On Qwen3-32B, training with just the 4000-sample HealthBench Easy subset is sufficient to obtain a model that exceeds GPT-5 on HealthBench Hard. Incorporating teacher-graded data further enhances performance for less capable models.

Conclusion: The framework enables faster and more resource-efficient training while surpassing baselines, making it effective for open-ended reasoning tasks with limited training data.

Abstract: Open-ended evaluation is essential for deploying large language models in
real-world settings. In studying HealthBench, we observe that using the model
itself as a grader and generating rubric-based reward signals substantially
improves reasoning performance. Remarkably, the trained model also becomes a
stronger grader. Motivated by this, we introduce Self-Rewarding Rubric-Based
Reinforcement Learning for Open-Ended Reasoning, a lightweight framework that
enables faster and more resource-efficient training while surpassing baselines.
Remarkably, on Qwen3-32B, training with just the 4000-sample HealthBench Easy
subset is sufficient to obtain a model that exceeds GPT-5 on HealthBench Hard.
Incorporating a small amount of teacher-graded data further enhances
performance for less capable models.

</details>


### [13] [Aligning Multilingual Reasoning with Verifiable Semantics from a High-Resource Expert Model](https://arxiv.org/abs/2509.25543)
*Fahim Faisal,Kaiqiang Song,Song Wang,Simin Ma,Shujian Liu,Haoyun Deng,Sathish Reddy Indurthi*

Main category: cs.CL

TL;DR: PB-RLSVR is a framework that uses English LLMs as pivots to improve multilingual reasoning without target-language human data, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning has improved LLM reasoning but mainly in English, creating multilingual performance gaps that need addressing.

Method: Uses English LLM as pivot to generate reference responses, rewards multilingual models based on semantic equivalence to English references via cross-lingual semantic reward functions.

Result: Improves average multilingual performance of Llama-3.1-8B-Instruct by 16.41% and Qwen3-32B by 10.17%, significantly outperforming PPO baselines.

Conclusion: PB-RLSVR provides a powerful, data-efficient approach to build truly multilingual reasoning agents by transferring English reasoning capabilities across languages.

Abstract: While reinforcement learning has advanced the reasoning abilities of Large
Language Models (LLMs), these gains are largely confined to English, creating a
significant performance disparity across languages. To address this, we
introduce Pivot-Based Reinforcement Learning with Semantically Verifiable
Rewards (PB-RLSVR), a novel framework that enhances multilingual reasoning by
circumventing the need for human-annotated data in target languages. Our
approach employs a high-performing English LLM as a "pivot" model to generate
reference responses for reasoning tasks. A multilingual model is then rewarded
based on the semantic equivalence of its responses to the English reference,
effectively transferring the pivot model's reasoning capabilities across
languages. We investigate several cross-lingual semantic reward functions,
including those based on embeddings and machine translation. Extensive
experiments on a suite of multilingual reasoning benchmarks show that our
method significantly narrows the performance gap between English and other
languages, substantially outperforming traditional PPO baselines. Specifically,
our PB-RLSVR framework improves the average multilingual performance of
Llama-3.1-8B-Instruct and Qwen3-32B by 16.41% and 10.17%, respectively,
demonstrating a powerful and data-efficient approach to building truly
multilingual reasoning agents.

</details>


### [14] [Performance and competence intertwined: A computational model of the Null Subject stage in English-speaking children](https://arxiv.org/abs/2509.25545)
*Soumik Dey,William Gregory Sakas*

Main category: cs.CL

TL;DR: This paper proposes a computational parameter to measure children's misinterpretation of imperative null subject utterances as declarative ones, supporting Orfitelli and Hyams' hypothesis about temporary null subject grammar in English-speaking children.


<details>
  <summary>Details</summary>
Motivation: To computationally model and test Orfitelli and Hyams' (2012) hypothesis that young English speakers temporarily develop a null subject grammar due to confusion between imperative and declarative utterances.

Method: Developed a computational parameter to measure misinterpretation and incorporated it into a modified Variational Learner (Yang, 2012) for superset-subset languages to simulate obligatory subject grammar learning.

Result: Simulations supported Orfitelli and Hyams' hypothesis about temporary null subject grammar development in English-speaking children.

Conclusion: The study provides a computational framework for integrating models of grammatical acquisition with other developmental factors, validating the hypothesis about null subject misinterpretation in early language development.

Abstract: The empirically established null subject (NS) stage, lasting until about 4
years of age, involves frequent omission of subjects by children. Orfitelli and
Hyams (2012) observe that young English speakers often confuse imperative NS
utterances with declarative ones due to performance influences, promoting a
temporary null subject grammar. We propose a new computational parameter to
measure this misinterpretation and incorporate it into a simulated model of
obligatory subject grammar learning. Using a modified version of the
Variational Learner (Yang, 2012) which works for superset-subset languages, our
simulations support Orfitelli and Hyams' hypothesis. More generally, this study
outlines a framework for integrating computational models in the study of
grammatical acquisition alongside other key developmental factors.

</details>


### [15] [Don't Sweat the Small Stuff: Segment-Level Meta-Evaluation Based on Pairwise Difference Correlation](https://arxiv.org/abs/2509.25546)
*Colten DiIanni,Daniel Deutsch*

Main category: cs.CL

TL;DR: PDP is a novel segment-level meta-evaluation metric for MT that uses pairwise differences instead of raw scores, addressing limitations in previous Pearson's ρ and Kendall's τ approaches.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in previous Pearson's ρ-based and Kendall's τ-based meta-evaluation approaches for Machine Translation, which have issues with score distributions and robustness.

Method: PDP utilizes pairwise differences rather than raw scores, draws information from all segments for robust understanding of score distributions, and uses segment-wise pairwise differences to refine Global Pearson to intra-segment score comparisons.

Result: Analysis on WMT'24 shows PDP properly ranks sentinel evaluation metrics and better aligns with human error weightings than previous work. Noise injection analysis demonstrates PDP's robustness to random noise, segment bias, and system bias.

Conclusion: PDP is a robust meta-evaluation metric that addresses limitations of previous approaches and shows improved performance in ranking evaluation metrics and aligning with human judgments, though it remains sensitive to extreme outliers.

Abstract: This paper introduces Pairwise Difference Pearson (PDP), a novel
segment-level meta-evaluation metric for Machine Translation (MT) that address
limitations in previous Pearson's $\rho$-based and and Kendall's $\tau$-based
meta-evaluation approaches. PDP is a correlation-based metric that utilizes
pairwise differences rather than raw scores. It draws on information from all
segments for a more robust understanding of score distributions and uses
segment-wise pairwise differences to refine Global Pearson to intra-segment
score comparisons. Analysis on the WMT'24 shared task shows PDP properly ranks
sentinel evaluation metrics and better aligns with human error weightings than
previous work. Noise injection analysis demonstrates PDP's robustness to random
noise, segment bias, and system bias while highlighting its sensitivity to
extreme outliers.

</details>


### [16] [Probing the Limits of Stylistic Alignment in Vision-Language Models](https://arxiv.org/abs/2509.25568)
*Asma Farajidizaji,Akash Gupta,Vatsal Raina*

Main category: cs.CL

TL;DR: This paper studies the data efficiency of aligning small vision-language models to generate captions in specific styles (humor and romantic) using minimal preference data.


<details>
  <summary>Details</summary>
Motivation: Vision-language models struggle with subjective style generation tasks in zero-shot settings, and preference data for alignment is expensive to acquire, limiting exploration of model capabilities.

Method: The study focuses on aligning small vision-language models to humor and romantic styles by analyzing how little preference data is needed to achieve stylistic saturation.

Result: The research benchmarks the capabilities and limitations of these models by determining the minimum preference data required for effective style alignment.

Conclusion: This approach helps define performance limits of vision-language models for style-specific caption generation and establishes data efficiency benchmarks for stylistic alignment.

Abstract: Vision-language models are increasingly used to generate image captions in
specific styles, such as humor or romantic. However, these transformer-based
models often struggle with this subjective task in a zero-shot setting. While
preference data can be used to align them toward a desired style, such data is
expensive to acquire, limiting the ability to explore the models' full
capabilities. This work addresses this by studying the data efficiency of
aligning small vision-language models to humor and romantic styles. This
approach helps to define the performance limits of these models and determine
how little preference data is needed to achieve stylistic saturation,
benchmarking their capabilities and limitations.

</details>


### [17] [RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance](https://arxiv.org/abs/2509.25604)
*Tianlang Chen,Minkai Xu,Jure Leskovec,Stefano Ermon*

Main category: cs.CL

TL;DR: RFG is a training-free method that guides diffusion LLMs' reasoning without process rewards, using log-likelihood ratios between enhanced and reference models to improve performance on mathematical reasoning and code generation tasks.


<details>
  <summary>Details</summary>
Motivation: Autoregressive LLMs use process rewards with dense step annotations, but this is challenging for diffusion LLMs due to their any-order generation and partially masked intermediate states. A reward-free guidance method is needed.

Method: Parameterize process reward using log-likelihood ratios between enhanced dLLMs (post-trained with RL/SFT) and reference dLLMs, enabling reward-guided sampling without explicit reward models.

Result: RFG achieves up to 9.2% accuracy gains across four mathematical reasoning and code generation benchmarks, consistently improving performance across different dLLM types and post-training methods.

Conclusion: RFG provides a general training-free framework that scales test-time reasoning for diffusion LLMs without relying on external reward models, establishing it as an effective guidance method.

Abstract: Diffusion large language models (dLLMs) have shown great potential in
large-scale language modeling, and there is an increasing interest in further
improving the capacity to solve complex problems by guiding the reasoning
process step by step. Common practice for autoregressive language models
typically learns a process reward model with dense annotation for each
intermediate step. However, this is challenging for dLLMs where the generation
is in an any-order fashion and intermediate states are partially masked
sentences. To this end, in this paper, we propose reward-free guidance (RFG), a
principled method for guiding the reasoning trajectory of dLLMs without
explicit process reward. The key idea of RFG is to parameterize the process
reward by log-likelihood ratios of the enhanced and reference dLLMs, where the
enhanced model can be easily obtained by any off-the-shelf dLLM that has been
post-trained with reinforcement learning (RL) or supervised fine-tuning (SFT).
We provide theoretical justification that RFG induces the reward-guided
sampling distribution with no additional reward. We conduct comprehensive
experiments on four challenging mathematical reasoning and code generation
benchmarks using a diverse suite of dLLMs enhanced with various post-training
methods. RFG consistently yields significant improvements across all tasks and
model types, achieving accuracy gains of up to 9.2%. These findings establish
RFG as a general training-free framework that scales test-time reasoning
without reliance on external reward models.

</details>


### [18] [Transformers through the lens of support-preserving maps between measures](https://arxiv.org/abs/2509.25611)
*Takashi Furuya,Maarten V. de Hoop,Matti Lassas*

Main category: cs.CL

TL;DR: Transformers can be mathematically modeled as maps between probability measures. The paper characterizes which measure maps can be represented by transformers and shows transformers can approximate continuous in-context maps. It also connects transformers to the Vlasov equation from mean-field particle systems.


<details>
  <summary>Details</summary>
Motivation: To mathematically analyze transformers' expressivity in handling arbitrarily large context tokens by modeling them as maps between probability measures, enabling applications in Wasserstein regularity, generalization bounds, and mean-field analysis.

Method: Model neural networks as maps on probability measures, characterize properties of maps between measures that enable transformer representation via push forward, and analyze measure-theoretic self-attention properties.

Result: Transformers universally approximate representations with any continuous in-context map. The solution map of the Vlasov equation satisfies conditions for transformer approximation, and measure-theoretic self-attention ensures infinite depth transformers correspond to Vlasov flows.

Conclusion: Transformers can be characterized as specific maps between measures with cardinality-preserving properties, and they have deep connections to mean-field particle systems through the Vlasov equation, enabling both approximation capabilities and theoretical identifications.

Abstract: Transformers are deep architectures that define ``in-context maps'' which
enable predicting new tokens based on a given set of tokens (such as a prompt
in NLP applications or a set of patches for a vision transformer). In previous
work, we studied the ability of these architectures to handle an arbitrarily
large number of context tokens. To mathematically, uniformly analyze their
expressivity, we considered the case that the mappings are conditioned on a
context represented by a probability distribution which becomes discrete for a
finite number of tokens. Modeling neural networks as maps on probability
measures has multiple applications, such as studying Wasserstein regularity,
proving generalization bounds and doing a mean-field limit analysis of the
dynamics of interacting particles as they go through the network. In this work,
we study the question what kind of maps between measures are transformers. We
fully characterize the properties of maps between measures that enable these to
be represented in terms of in-context maps via a push forward. On the one hand,
these include transformers; on the other hand, transformers universally
approximate representations with any continuous in-context map. These
properties are preserving the cardinality of support and that the regular part
of their Fr\'{e}chet derivative is uniformly continuous. Moreover, we show that
the solution map of the Vlasov equation, which is of nonlocal transport type,
for interacting particle systems in the mean-field regime for the Cauchy
problem satisfies the conditions on the one hand and, hence, can be
approximated by a transformer; on the other hand, we prove that the
measure-theoretic self-attention has the properties that ensure that the
infinite depth, mean-field measure-theoretic transformer can be identified with
a Vlasov flow.

</details>


### [19] [The Media Bias Detector: A Framework for Annotating and Analyzing the News at Scale](https://arxiv.org/abs/2509.25649)
*Samar Haider,Amir Tohidi,Jenny S. Wang,Timothy Dörr,David M. Rothschild,Chris Callison-Burch,Duncan J. Watts*

Main category: cs.CL

TL;DR: A large-scale computational framework using LLMs to systematically measure selection and framing bias in news coverage through structured annotations of political lean, tone, topics, and events across hundreds of daily articles.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of measuring subtle forms of media bias at scale, particularly selection bias (which topics to cover) and framing bias (how issues are framed), which shape public perception but are difficult to quantify systematically.

Method: Integrated pipeline combining large language models (LLMs) with scalable, near-real-time news scraping to extract structured annotations including political lean, tone, topics, article type, and major events across hundreds of articles per day, analyzed at sentence, article, and publisher levels.

Result: Created a large, ongoing dataset (from January 1, 2024) with 150,000+ articles examined in 2024, revealing insightful patterns in news coverage and bias, plus an interactive web platform for data exploration.

Conclusion: Established a reusable methodology for studying media bias at scale, providing empirical resources for future research and supporting academic research and real-world efforts to improve media accountability.

Abstract: Mainstream news organizations shape public perception not only directly
through the articles they publish but also through the choices they make about
which topics to cover (or ignore) and how to frame the issues they do decide to
cover. However, measuring these subtle forms of media bias at scale remains a
challenge. Here, we introduce a large, ongoing (from January 1, 2024 to
present), near real-time dataset and computational framework developed to
enable systematic study of selection and framing bias in news coverage. Our
pipeline integrates large language models (LLMs) with scalable, near-real-time
news scraping to extract structured annotations -- including political lean,
tone, topics, article type, and major events -- across hundreds of articles per
day. We quantify these dimensions of coverage at multiple levels -- the
sentence level, the article level, and the publisher level -- expanding the
ways in which researchers can analyze media bias in the modern news landscape.
In addition to a curated dataset, we also release an interactive web platform
for convenient exploration of these data. Together, these contributions
establish a reusable methodology for studying media bias at scale, providing
empirical resources for future research. Leveraging the breadth of the corpus
over time and across publishers, we also present some examples (focused on the
150,000+ articles examined in 2024) that illustrate how this novel data set can
reveal insightful patterns in news coverage and bias, supporting academic
research and real-world efforts to improve media accountability.

</details>


### [20] [QFrBLiMP: a Quebec-French Benchmark of Linguistic Minimal Pairs](https://arxiv.org/abs/2509.25664)
*David Beauchemin,Pier-Luc Veilleux,Richard Khoury,Johanna-Pascale Roy*

Main category: cs.CL

TL;DR: QFrBLiMP is a Quebec-French linguistic benchmark with 1,761 minimal pairs across 20 grammatical phenomena, used to evaluate LLMs' grammatical competence compared to human native speakers.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' linguistic knowledge on Quebec-French grammatical phenomena and compare their performance with human native speakers.

Method: Created 1,761 minimal pairs from official Quebec government resources, annotated by 12 native speakers, then evaluated LLMs by comparing probability assignments to grammatical vs ungrammatical sentences.

Result: Grammatical competence scales with model size, but all models fail on phenomena requiring deep semantic understanding, showing significant gaps compared to human performance.

Conclusion: Current LLMs have critical limitations in handling Quebec-French grammatical phenomena that require semantic understanding, revealing substantial room for improvement despite scaling effects.

Abstract: In this paper, we introduce the Quebec-French Benchmark of Linguistic Minimal
Pairs (QFrBLiMP), a corpus designed to evaluate the linguistic knowledge of
LLMs on prominent grammatical phenomena in Quebec-French. QFrBLiMP consists of
1,761 minimal pairs annotated with 20 linguistic phenomena. Specifically, these
minimal pairs have been created by manually modifying sentences extracted from
an official online resource maintained by a Qu\'ebec government institution.
Each pair is annotated by twelve Quebec-French native speakers, who select the
sentence they feel is grammatical amongst the two. These annotations are used
to compare the competency of LLMs with that of humans. We evaluate different
LLMs on QFrBLiMP and MultiBLiMP-Fr by observing the rate of higher
probabilities assigned to the sentences of each minimal pair for each category.
We find that while grammatical competence scales with model size, a clear
hierarchy of difficulty emerges. All benchmarked models consistently fail on
phenomena requiring deep semantic understanding, revealing a critical
limitation and a significant gap compared to human performance on these
specific tasks.

</details>


### [21] [The Flaw of Averages: Quantifying Uniformity of Performance on Benchmarks](https://arxiv.org/abs/2509.25671)
*Arda Uzunoglu,Tianjian Li,Daniel Khashabi*

Main category: cs.CL

TL;DR: The paper introduces benchmark harmony as a measure of how uniformly a model's performance is distributed across subdomains of a benchmark, arguing that high harmony indicates more reliable evaluation.


<details>
  <summary>Details</summary>
Motivation: Benchmarks create feedback loops that shape model development, so ensuring benchmark reliability is essential for trustworthy evaluation and meaningful progress in AI.

Method: Study benchmark reliability from a distributional perspective by introducing benchmark harmony, which measures performance uniformity across subdomains. Analyze 19 multiple-choice benchmarks and five model families, mapping benchmarks onto a mean-variance plane of harmony.

Result: Analysis shows less harmonious benchmarks can give misleading results, with overall accuracy disproportionately influenced by specific subdomains. For example, ARC-Easy is overwhelmed by Biological Concepts questions, overshadowing other critical subdomains.

Conclusion: Harmony should be reported alongside accuracy to reframe evaluation from simple performance averages to more robust, distributionally reliable measurement of performance.

Abstract: Benchmarks shape scientific conclusions about model capabilities and steer
model development. This creates a feedback loop: stronger benchmarks drive
better models, and better models demand more discriminative benchmarks.
Ensuring benchmark reliability is therefore essential for trustworthy
evaluation and meaningful progress. In this work, we study benchmark
reliability from a distributional perspective and introduce benchmark harmony,
which measures how uniformly a model's performance is distributed across the
subdomains of a benchmark. We posit that high harmony is a desirable benchmark
property, indicating that the aggregate metric reflects uniform competence
across subdomains. Across 19 multiple-choice benchmarks and five model
families, we map each benchmark onto a mean-variance plane of harmony computed
across models, where high mean and low variance signal more reliable
evaluation. Our analysis shows that less harmonious benchmarks can give
misleading results, since overall accuracy may be disproportionately influenced
by specific subdomains. For instance, ARC-Easy is overwhelmed by questions on
Biological Concepts, overshadowing other critical subdomains such as Geography,
Physics, Chemistry, and Environmental Science. By recommending that harmony
should be reported alongside accuracy, we reframe evaluation from simple
performance averages to a more robust, distributionally reliable measurement of
performance.

</details>


### [22] [Mitigating Biases in Language Models via Bias Unlearning](https://arxiv.org/abs/2509.25673)
*Dianqing Liu,Yi Liu,Guoqing Jin,Zhendong Mao*

Main category: cs.CL

TL;DR: BiasUnlearn is a novel debiasing framework that uses dual-pathway unlearning to mitigate bias in language models while preserving core capabilities, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing debiasing approaches either degrade model capabilities or only address surface-level biases, failing to tackle deeply embedded stereotypical associations in model parameters.

Method: BiasUnlearn employs dual-pathway unlearning mechanisms coordinating stereotype forgetting with anti-stereotype retention, using adversarial forget set and dynamic dataset swapping to prevent bias polarity reversal.

Result: Extensive experiments show BiasUnlearn outperforms existing methods in mitigating bias while retaining language modeling capabilities, with debiasing weights being transferable across model variants.

Conclusion: Bias representations become entrenched during pre-training and persist through fine-tuning phases, and BiasUnlearn effectively addresses this through targeted unlearning mechanisms.

Abstract: Many studies have shown various biases targeting different demographic groups
in language models, amplifying discrimination and harming fairness. Recent
parameter modification debiasing approaches significantly degrade core
capabilities such as text coherence and task accuracy. And Prompt-based
debiasing methods, only effective for predefined trigger words, fail to address
deeply embedded stereotypical associations in model parameters. In this paper,
we propose BiasUnlearn, a novel model debiasing framework which achieves
targeted debiasing via dual-pathway unlearning mechanisms coordinating
stereotype forgetting with anti-stereotype retention, while preventing bias
polarity reversal through adversarial forget set and dynamic dataset swapping.
We conducted extensive experiments with multiple language models across various
evaluation benchmarks. The results show that BiasUnlearn outperforms existing
methods in mitigating bias in language models while retaining language modeling
capabilities. Further experiments reveal that debiasing weights are
transferable across model variants, confirming that bias representations become
entrenched during pre-training and persist through fine-tuning phases.

</details>


### [23] [LD-MoLE: Learnable Dynamic Routing for Mixture of LoRA Experts](https://arxiv.org/abs/2509.25684)
*Yuan Zhuang,Yi Shen,Yuexin Bian,Qing Su,Shihao Ji,Yuanyuan Shi,Fei Miao*

Main category: cs.CL

TL;DR: LD-MoLE proposes a learnable dynamic routing mechanism for Mixture of LoRA Experts that replaces non-differentiable TopK routing with differentiable routing and adaptive expert allocation per token and layer.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT+MoE approaches rely on conventional TopK routing requiring careful hyperparameter tuning and fixed expert assignment per token, lacking adaptability.

Method: Uses differentiable routing function with closed-form solution, adaptive expert number determination per token and layer, and analytical sparsity control objective.

Result: Achieves highest average scores on Qwen3-1.7B and Llama-3.2-3B models across diverse benchmarks compared to state-of-the-art baselines.

Conclusion: LD-MoLE achieves superior performance while learning token-dependent and layer-wise expert allocation, demonstrating effective adaptive routing.

Abstract: Recent studies have shown that combining parameter-efficient fine-tuning
(PEFT) with mixture-of-experts (MoE) is an effective strategy for adapting
large language models (LLMs) to the downstream tasks. However, most existing
approaches rely on conventional TopK routing, which requires careful
hyperparameter tuning and assigns a fixed number of experts to each token. In
this work, we propose LD-MoLE, a Learnable Dynamic routing mechanism for
Mixture of LoRA Experts that enables adaptive, token-dependent, and layer-wise
expert allocation. Our method replaces the non-differentiable TopK selection
with a differentiable routing function and a closed-form solution. Moreover,
our design allows the model to adaptively determine the number of experts to
activate for each token at different layers. In addition, we introduce an
analytical sparsity control objective to regularize the number of activated
experts. Extensive experiments on the Qwen3-1.7B and Llama-3.2-3B models show
that LD-MoLE achieves the highest average scores compared to state-of-the-art
baselines, across a diverse set of benchmarks. Our method not only achieves
superior performance, but also demonstrates the ability to learn
token-dependent and layer-wise expert allocation.

</details>


### [24] [Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities](https://arxiv.org/abs/2509.25725)
*Jiayi Kuang,Haojing Huang,Yinghui Li,Xinnian Liang,Zhikun Xu,Yangning Li,Xiaoyu Tan,Chao Qu,Meishan Zhang,Ying Shen,Philip S. Yu*

Main category: cs.CL

TL;DR: The paper proposes a new paradigm for evaluating mathematical atomic capabilities in LLMs, categorizing abilities into field-specific (algebra, geometry, analysis, topology) and logical dimensions (conceptual understanding, forward reasoning, backward reasoning), and explores how different atomic capabilities influence each other.


<details>
  <summary>Details</summary>
Motivation: Current LLMs primarily rely on scaling up training datasets with diverse mathematical problems, raising questions about whether they genuinely acquire mathematical concepts or merely remember training data. The paper aims to evaluate mathematical intelligence by breaking it down into fundamental atomic capabilities, inspired by human problem-solving approaches.

Method: Proposed categorization of atomic abilities into two dimensions: field-specific abilities across four mathematical fields and logical abilities at different levels. Created corresponding training and evaluation datasets for each atomic capability unit and conducted experiments to explore how different atomic capabilities influence others.

Result: Evaluation on advanced models revealed interesting discoveries about different performances on various atomic capabilities and interactions between them. The findings highlight the importance of decoupling mathematical intelligence into atomic components.

Conclusion: The atomic capability evaluation paradigm provides new insights into model cognition and guides the development of training strategies toward a more efficient, transferable, and cognitively grounded paradigm of "atomic thinking" in mathematical reasoning.

Abstract: Large Language Models (LLMs) have demonstrated outstanding performance in
mathematical reasoning capabilities. However, we argue that current large-scale
reasoning models primarily rely on scaling up training datasets with diverse
mathematical problems and long thinking chains, which raises questions about
whether LLMs genuinely acquire mathematical concepts and reasoning principles
or merely remember the training data. In contrast, humans tend to break down
complex problems into multiple fundamental atomic capabilities. Inspired by
this, we propose a new paradigm for evaluating mathematical atomic
capabilities. Our work categorizes atomic abilities into two dimensions: (1)
field-specific abilities across four major mathematical fields, algebra,
geometry, analysis, and topology, and (2) logical abilities at different
levels, including conceptual understanding, forward multi-step reasoning with
formal math language, and counterexample-driven backward reasoning. We propose
corresponding training and evaluation datasets for each atomic capability unit,
and conduct extensive experiments about how different atomic capabilities
influence others, to explore the strategies to elicit the required specific
atomic capability. Evaluation and experimental results on advanced models show
many interesting discoveries and inspirations about the different performances
of models on various atomic capabilities and the interactions between atomic
capabilities. Our findings highlight the importance of decoupling mathematical
intelligence into atomic components, providing new insights into model
cognition and guiding the development of training strategies toward a more
efficient, transferable, and cognitively grounded paradigm of "atomic
thinking".

</details>


### [25] [Controlled Generation for Private Synthetic Text](https://arxiv.org/abs/2509.25729)
*Zihao Zhao,Anjalie Field*

Main category: cs.CL

TL;DR: A novel privacy-preserving synthetic text generation method using entity-aware control codes with ICL and prefix tuning variants, achieving strong privacy-utility balance in legal and clinical domains.


<details>
  <summary>Details</summary>
Motivation: Text anonymization is crucial for responsible AI development in high-stakes domains like healthcare, social services, and law to protect sensitive information.

Method: Proposes entity-aware control codes to guide controllable generation using in-context learning (ICL) or prefix tuning, with ICL ensuring privacy levels and prefix tuning incorporating custom masking strategy and loss function.

Result: Experiments on legal and clinical datasets demonstrate the method achieves strong balance between privacy protection and utility.

Conclusion: The approach offers a practical and effective solution for synthetic text generation in sensitive domains while maintaining privacy.

Abstract: Text anonymization is essential for responsibly developing and deploying AI
in high-stakes domains such as healthcare, social services, and law. In this
work, we propose a novel methodology for privacy-preserving synthetic text
generation that leverages the principles of de-identification and the Hiding In
Plain Sight (HIPS) theory. Our approach introduces entity-aware control codes
to guide controllable generation using either in-context learning (ICL) or
prefix tuning. The ICL variant ensures privacy levels consistent with the
underlying de-identification system, while the prefix tuning variant
incorporates a custom masking strategy and loss function to support scalable,
high-quality generation. Experiments on legal and clinical datasets demonstrate
that our method achieves a strong balance between privacy protection and
utility, offering a practical and effective solution for synthetic text
generation in sensitive domains.

</details>


### [26] [CATCH: A Novel Data Synthesis Framework for High Therapy Fidelity and Memory-Driven Planning Chain of Thought in AI Counseling](https://arxiv.org/abs/2509.25733)
*Mingyu Chen,Jingkai Lin,Zhaojie Chu,Xiaofen Xing,Yirong Chen,Xiangmin Xu*

Main category: cs.CL

TL;DR: CATCH is a novel data synthesis framework for AI counseling that uses Progressive Dialogue Synthesis and Memory-Driven Dynamic Planning to improve therapy fidelity and capture decision-making rationale in multi-turn dialogues.


<details>
  <summary>Details</summary>
Motivation: Existing AI counseling approaches use one-time generation for multi-turn dialogues, resulting in low therapy fidelity and failure to capture the decision-making rationale behind each response.

Method: CATCH framework employs Progressive Dialogue Synthesis (extracting goals/resources/solutions from client reports and generating stage-aligned dialogues) and Memory-Driven Dynamic Planning (integrating memory enhancement, global planning, and strategy reasoning with multi-agent optimization to attach chain-of-thought to each dialogue turn).

Result: Extensive experiments and human evaluations demonstrate that CATCH significantly enhances fidelity and logical coherence in AI counseling.

Conclusion: CATCH effectively addresses the limitations of existing AI counseling approaches by improving therapy fidelity and capturing decision-making rationale through its novel synthesis framework.

Abstract: Recently, advancements in AI counseling based on large language models have
shown significant progress. However, existing studies employ a one-time
generation approach to synthesize multi-turn dialogue samples, resulting in low
therapy fidelity and failing to capture the decision-making rationale behind
each response. In this work, we propose CATCH, a novel data synthesis framework
designed to address these challenges. Specifically, to improve therapy
fidelity, we introduce the Progressive Dialogue Synthesis strategy, which
extracts goals, resources, and solutions from a client's self-report, organizes
them into structured outlines, and then incrementally generates stage-aligned
counseling dialogues. To capture decision-making rationale behind each
response, we propose the Memory-Driven Dynamic Planning thinking pattern that
integrates memory enhancement, global planning, and strategy reasoning; a
collaborative multi-agent optimizer then leverages MDP to attach explicit
chain-of-thought to each dialogue turn. Extensive experiments and human
evaluations demonstrate that CATCH significantly enhances fidelity and logical
coherence in AI counseling.

</details>


### [27] [Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications](https://arxiv.org/abs/2509.25736)
*Chenhua Shi,Gregor Macdonald,Bhavika Jalli,Wanlu Lei,John Zou,Mridul Jain,Joji Philip*

Main category: cs.CL

TL;DR: Automated pipeline for generating high-quality synthetic QA pairs for domain-specific LLM training using retrieval-augmented generation and RAGAS-based filtering.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of instruction-following datasets for domain-specific tasks like telecom troubleshooting is time-consuming and requires deep technical expertise.

Method: Multi-stage framework with retriever, base generator, and refinement model using domain-specific knowledge graph, with RAGAS-based quality filtering.

Result: Generated complex, context-rich troubleshooting solution plans for telecom RAN without human intervention.

Conclusion: Scalable solution for building instruction and reinforcement datasets in specialized domains, reducing manual labeling dependency while maintaining technical fidelity.

Abstract: The success of large language models (LLMs) depends heavily on large-scale,
high-quality instruction-following and reinforcement datasets. However,
generating such data through human annotation is prohibitively time-consuming
particularly for domain-specific tasks like telecom network troubleshooting,
where accurate responses require deep technical expertise and contextual
understanding. In this paper, we present a fully automated, retrieval-augmented
pipeline for generating synthetic question-answer (QA) pairs grounded in
structured domain knowledge. Our multi-stage framework integrates a retriever,
base generator, and refinement model to synthesize and enhance QA pairs using
documents retrieved from a domain-specific knowledge graph. To ensure data
quality, we employ customized RAGAS-based scoring to filter low-quality
samples, producing a high-quality dataset suitable for reinforcement
fine-tuning (RFT). We demonstrate our approach in a real-world telecom scenario
focused on radio access network (RAN) troubleshooting. The resulting pipeline
generates complex, context-rich troubleshooting solution plans without human
intervention. This work offers a scalable solution for building instruction and
reinforcement datasets in specialized domains, significantly reducing
dependence on manual labeling while maintaining high technical fidelity.

</details>


### [28] [Detecting Hope Across Languages: Multiclass Classification for Positive Online Discourse](https://arxiv.org/abs/2509.25752)
*T. O. Abiola,K. D. Abiodun,O. E. Olumide,O. O. Adebanji,O. Hiram Calvo,Grigori Sidorov*

Main category: cs.CL

TL;DR: This paper presents a multilingual hope speech detection system using XLM-RoBERTa transformer model to classify hope speech into three categories (Generalized, Realistic, Unrealistic Hope) across English, Urdu, and Spanish, achieving state-of-the-art performance on the PolyHope dataset.


<details>
  <summary>Details</summary>
Motivation: To promote positive discourse and well-being in social media by developing effective methods for detecting and categorizing hope speech across multiple languages, addressing the need for positive content moderation and supportive online communities.

Method: Leveraged transformer-based XLM-RoBERTa model for multiclass hope speech detection across English, Urdu, and Spanish languages. Used the PolyHope dataset for evaluation and participated in the PolyHope-M 2025 shared task.

Result: Achieved competitive performance across all three languages and significantly outperformed prior state-of-the-art techniques in terms of macro F1 scores. Demonstrated effectiveness in multilingual hope speech detection.

Conclusion: The work contributes to developing multilingual, fine-grained hope speech detection models that can enhance positive content moderation and foster supportive online communities, while highlighting challenges in low-resource languages and potential for improved generalization.

Abstract: The detection of hopeful speech in social media has emerged as a critical
task for promoting positive discourse and well-being. In this paper, we present
a machine learning approach to multiclass hope speech detection across multiple
languages, including English, Urdu, and Spanish. We leverage transformer-based
models, specifically XLM-RoBERTa, to detect and categorize hope speech into
three distinct classes: Generalized Hope, Realistic Hope, and Unrealistic Hope.
Our proposed methodology is evaluated on the PolyHope dataset for the
PolyHope-M 2025 shared task, achieving competitive performance across all
languages. We compare our results with existing models, demonstrating that our
approach significantly outperforms prior state-of-the-art techniques in terms
of macro F1 scores. We also discuss the challenges in detecting hope speech in
low-resource languages and the potential for improving generalization. This
work contributes to the development of multilingual, fine-grained hope speech
detection models, which can be applied to enhance positive content moderation
and foster supportive online communities.

</details>


### [29] [TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning](https://arxiv.org/abs/2509.25760)
*Zhepei Wei,Xiao Yang,Kai Sun,Jiaqi Wang,Rulin Shao,Sean Chen,Mohammad Kachuee,Teja Gollapudi,Tony Liao,Nicolas Scheffer,Rakesh Wanga,Anuj Kumar,Yu Meng,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: TruthRL is a reinforcement learning framework that directly optimizes LLM truthfulness using ternary rewards to distinguish correct answers, hallucinations, and abstentions, achieving significant reductions in hallucinations and improvements in truthfulness.


<details>
  <summary>Details</summary>
Motivation: LLMs are prone to hallucination and untruthful responses, especially when tasks require information beyond their parametric knowledge. Existing methods struggle to balance accuracy and abstention - accuracy-driven approaches amplify hallucinations while abstention-focused methods become overly conservative.

Method: TruthRL uses GRPO (Generalized Reinforcement Policy Optimization) with a ternary reward system that distinguishes between correct answers, hallucinations, and abstentions. This incentivizes models to reduce hallucinations by providing correct responses and enabling abstention when uncertain.

Result: Extensive experiments across four knowledge-intensive benchmarks show TruthRL reduces hallucinations by 28.9% and improves truthfulness by 21.1% compared to vanilla RL, with consistent gains across various backbone models (Qwen, Llama) under both retrieval and non-retrieval setups.

Conclusion: TruthRL achieves strong performance in both accuracy and truthfulness, demonstrating that truthfulness-driven learning objectives are crucial for developing truthful LLMs, unlike vanilla accuracy-driven methods that struggle to balance factual correctness and uncertainty.

Abstract: While large language models (LLMs) have demonstrated strong performance on
factoid question answering, they are still prone to hallucination and
untruthful responses, particularly when tasks demand information outside their
parametric knowledge. Indeed, truthfulness requires more than accuracy --
models must also recognize uncertainty and abstain when unsure to avoid
hallucinations. This presents a fundamental challenge for existing methods:
approaches that optimize for accuracy often amplify hallucinations, while those
that encourage abstention can become overly conservative, sacrificing correct
answers. Both extremes ultimately compromise truthfulness. In this work, we
present TruthRL, a general reinforcement learning (RL) framework that directly
optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using
GRPO with a simple yet effective ternary reward that distinguishes correct
answers, hallucinations, and abstentions. It incentivizes models to reduce
hallucinations not only by providing correct responses, but also by enabling
abstention when uncertain, thereby improving truthfulness. Extensive
experiments across four knowledge-intensive benchmarks show that, compared to
vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves
truthfulness by 21.1%, with consistent gains across various backbone models
(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth
ablation study demonstrates that vanilla accuracy-driven methods, such as
supervised fine-tuning or RL with a binary reward, struggle to balance factual
correctness and uncertainty. In contrast, our proposed truthfulness-driven
TruthRL achieves strong performance in both accuracy and truthfulness,
underscoring the importance of learning objective design for developing
truthful LLMs.

</details>


### [30] [Assessing Algorithmic Bias in Language-Based Depression Detection: A Comparison of DNN and LLM Approaches](https://arxiv.org/abs/2509.25795)
*Obed Junias,Prajakta Kini,Theodora Chaspari*

Main category: cs.CL

TL;DR: This paper examines algorithmic bias in depression detection models, comparing DNN-based embeddings with LLM few-shot learning. LLMs show better performance and reduced gender bias, while fairness-aware techniques help DNN models, with worst-group loss being most effective.


<details>
  <summary>Details</summary>
Motivation: To investigate socio-demographic disparities (gender and race/ethnicity) in automated depression detection systems and compare different approaches for mitigating algorithmic bias.

Method: Compared DNN-based embeddings with LLM few-shot learning on DAIC-WOZ clinical transcripts. Applied fairness-aware loss functions to DNN models and explored in-context learning with varied prompts for LLMs.

Result: LLMs outperformed DNN models in depression classification, especially for Hispanic participants. LLMs showed reduced gender bias but persistent racial disparities. Worst-group loss worked best for DNN fairness, while ethical prompting helped LLMs in 1-shot settings.

Conclusion: LLMs offer advantages in depression detection with reduced gender bias, but racial disparities remain challenging. Fairness-aware techniques show promise for DNN models, while prompting strategies have limited effectiveness for racial bias mitigation in LLMs.

Abstract: This paper investigates algorithmic bias in language-based models for
automated depression detection, focusing on socio-demographic disparities
related to gender and race/ethnicity. Models trained using deep neural networks
(DNN) based embeddings are compared to few-shot learning approaches with large
language models (LLMs), evaluating both performance and fairness on clinical
interview transcripts from the Distress Analysis Interview Corpus/Wizard-of-Oz
(DAIC-WOZ). To mitigate bias, fairness-aware loss functions are applied to
DNN-based models, while in-context learning with varied prompt framing and shot
counts is explored for LLMs. Results indicate that LLMs outperform DNN-based
models in depression classification, particularly for underrepresented groups
such as Hispanic participants. LLMs also exhibit reduced gender bias compared
to DNN-based embeddings, though racial disparities persist. Among
fairness-aware techniques for mitigating bias in DNN-based embeddings, the
worst-group loss, which is designed to minimize loss for the worst-performing
demographic group, achieves a better balance between performance and fairness.
In contrast, the fairness-regularized loss minimizes loss across all groups but
performs less effectively. In LLMs, guided prompting with ethical framing helps
mitigate gender bias in the 1-shot setting. However, increasing the number of
shots does not lead to further reductions in disparities. For race/ethnicity,
neither prompting strategy nor increasing $N$ in $N$-shot learning effectively
reduces disparities.

</details>


### [31] [RoBiologyDataChoiceQA: A Romanian Dataset for improving Biology understanding of Large Language Models](https://arxiv.org/abs/2509.25813)
*Dragos-Dumitru Ghinea,Adela-Nicoleta Corbeanu,Adrian-Marius Dumitran*

Main category: cs.CL

TL;DR: This paper introduces a Romanian-language biology multiple-choice question dataset to evaluate LLMs' performance in domain-specific, non-English contexts, benchmarking several models and analyzing optimization techniques.


<details>
  <summary>Details</summary>
Motivation: To address the under-explored performance of LLMs in domain-specific applications and non-English languages, particularly in scientific contexts.

Method: Created a novel Romanian-language dataset with ~14,000 biology multiple-choice questions, benchmarked popular LLMs, and evaluated prompt engineering, fine-tuning, and other optimization techniques.

Result: The study provides comprehensive evaluation of LLMs' accuracy, reasoning patterns, and understanding of domain-specific terminology in Romanian biology contexts.

Conclusion: The research highlights both strengths and limitations of current LLMs in handling specialized knowledge tasks in low-resource languages, offering valuable insights for future development.

Abstract: In recent years, large language models (LLMs) have demonstrated significant
potential across various natural language processing (NLP) tasks. However,
their performance in domain-specific applications and non-English languages
remains less explored. This study introduces a novel Romanian-language dataset
for multiple-choice biology questions, carefully curated to assess LLM
comprehension and reasoning capabilities in scientific contexts. Containing
approximately 14,000 questions, the dataset provides a comprehensive resource
for evaluating and improving LLM performance in biology.
  We benchmark several popular LLMs, analyzing their accuracy, reasoning
patterns, and ability to understand domain-specific terminology and linguistic
nuances. Additionally, we perform comprehensive experiments to evaluate the
impact of prompt engineering, fine-tuning, and other optimization techniques on
model performance. Our findings highlight both the strengths and limitations of
current LLMs in handling specialized knowledge tasks in low-resource languages,
offering valuable insights for future research and development.

</details>


### [32] [ReTAG: Retrieval-Enhanced, Topic-Augmented Graph-Based Global Sensemaking](https://arxiv.org/abs/2509.25814)
*Boyoung Kim,Dosung Lee,Sumin An,Jinseong Jeong,Paul Hongsuck Seo*

Main category: cs.CL

TL;DR: ReTAG is a retrieval-enhanced, topic-augmented graph framework that improves global sensemaking in question answering by constructing topic-specific subgraphs and retrieving relevant summaries, achieving better response quality with reduced inference time.


<details>
  <summary>Details</summary>
Motivation: Global sensemaking - answering questions by synthesizing information from entire corpora - remains challenging. Previous graph-based approaches lack retrieval mechanisms, topic specificity, and have high inference costs.

Method: Proposes ReTAG framework that constructs topic-specific subgraphs and retrieves relevant summaries for response generation, addressing limitations of prior graph-based approaches.

Result: Experiments show ReTAG improves response quality while significantly reducing inference time compared to baseline methods.

Conclusion: ReTAG effectively addresses global sensemaking challenges by combining retrieval enhancement and topic augmentation in graph frameworks, providing better performance with computational efficiency.

Abstract: Recent advances in question answering have led to substantial progress in
tasks such as multi-hop reasoning. However, global sensemaking-answering
questions by synthesizing information from an entire corpus remains a
significant challenge. A prior graph-based approach to global sensemaking lacks
retrieval mechanisms, topic specificity, and incurs high inference costs. To
address these limitations, we propose ReTAG, a Retrieval-Enhanced,
Topic-Augmented Graph framework that constructs topic-specific subgraphs and
retrieves the relevant summaries for response generation. Experiments show that
ReTAG improves response quality while significantly reducing inference time
compared to the baseline. Our code is available at
https://github.com/bykimby/retag.

</details>


### [33] [Personalized Scientific Figure Caption Generation: An Empirical Study on Author-Specific Writing Style Transfer](https://arxiv.org/abs/2509.25817)
*Jaeyoung Kim,Jongho Lee,Hongjun Choi,Sion Jang*

Main category: cs.CL

TL;DR: Personalized figure caption generation using author profile data improves personalization but reveals trade-off between author style matching and caption quality.


<details>
  <summary>Details</summary>
Motivation: To improve personalized figure caption generation by leveraging author profile data from scientific papers, addressing the need for automated caption systems that can capture individual author styles.

Method: Used author profile data combined with relevant metadata to enhance multimodal large language models for personalized caption generation, conducted as part of the 3rd SciCap challenge.

Result: Rich author profile data significantly improves personalization performance, but reveals a fundamental trade-off between matching author style and maintaining caption quality.

Conclusion: Findings provide valuable insights for developing practical caption automation systems that balance both personalization and quality objectives.

Abstract: We study personalized figure caption generation using author profile data
from scientific papers. Our experiments demonstrate that rich author profile
data, combined with relevant metadata, can significantly improve the
personalization performance of multimodal large language models. However, we
also reveal a fundamental trade-off between matching author style and
maintaining caption quality. Our findings offer valuable insights and future
directions for developing practical caption automation systems that balance
both objectives. This work was conducted as part of the 3rd SciCap challenge.

</details>


### [34] [Overthinking Reduction with Decoupled Rewards and Curriculum Data Scheduling](https://arxiv.org/abs/2509.25827)
*Shuyang Jiang,Yusheng Liao,Ya Zhang,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: DECS framework addresses overthinking in reasoning models by surgically penalizing redundant tokens and using curriculum scheduling, achieving 50%+ token reduction while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Current RLVR models suffer from overthinking - generating excessively long reasoning paths without performance benefits. Existing length penalty solutions fail due to misalignment between trajectory-level rewards and token-level optimization.

Method: Introduces DECS framework with: (1) decoupled token-level reward mechanism that distinguishes and penalizes only redundant tokens, (2) curriculum batch scheduling strategy to balance efficiency and efficacy.

Result: Experimental results show over 50% reduction in reasoning tokens across seven benchmarks while maintaining or improving performance.

Conclusion: Substantial gains in reasoning efficiency can be achieved without compromising a model's underlying reasoning power.

Abstract: While large reasoning models trained with critic-free reinforcement learning
and verifiable rewards (RLVR) represent the state-of-the-art, their practical
utility is hampered by ``overthinking'', a critical issue where models generate
excessively long reasoning paths without any performance benefit. Existing
solutions that penalize length often fail, inducing performance degradation due
to a fundamental misalignment between trajectory-level rewards and token-level
optimization. In this work, we introduce a novel framework, DECS, built on our
theoretical discovery of two previously unaddressed flaws in current length
rewards: (1) the erroneous penalization of essential exploratory tokens and (2)
the inadvertent rewarding of partial redundancy. Our framework's innovations
include (i) a first-of-its-kind decoupled token-level reward mechanism that
surgically distinguishes and penalizes redundant tokens, and (ii) a novel
curriculum batch scheduling strategy to master the efficiency-efficacy
equilibrium. Experimental results show DECS can achieve a dramatic reduction in
reasoning tokens by over 50\% across seven benchmarks while simultaneously
maintaining or even improving performance. It demonstrates conclusively that
substantial gains in reasoning efficiency can be achieved without compromising
a model's underlying reasoning power.

</details>


### [35] [Believing without Seeing: Quality Scores for Contextualizing Vision-Language Model Explanations](https://arxiv.org/abs/2509.25844)
*Keyu He,Tejas Srinivasan,Brihi Joshi,Xiang Ren,Jesse Thomason,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: This paper proposes two quality scoring functions (Visual Fidelity and Contrastiveness) for VLM-generated explanations to help users better assess prediction reliability when visual context is unavailable, reducing overreliance on incorrect predictions.


<details>
  <summary>Details</summary>
Motivation: When users cannot see visual context (e.g., blind/low-vision users), VLM explanations can signal prediction reliability but often lead to overreliance on incorrect predictions.

Method: Proposed two quality scoring functions: Visual Fidelity (faithfulness to visual context) and Contrastiveness (identifying distinguishing visual details from alternatives). Evaluated on A-OKVQA and VizWiz tasks.

Result: Quality scores were better calibrated with model correctness than existing methods. User study showed 11.1% improvement in predicting VLM correctness and 15.4% reduction in false belief of incorrect predictions.

Conclusion: Explanation quality scores effectively foster appropriate reliance on VLM predictions, especially when visual context is unavailable.

Abstract: When people query Vision-Language Models (VLMs) but cannot see the
accompanying visual context (e.g. for blind and low-vision users), augmenting
VLM predictions with natural language explanations can signal which model
predictions are reliable. However, prior work has found that explanations can
easily convince users that inaccurate VLM predictions are correct. To remedy
undesirable overreliance on VLM predictions, we propose evaluating two
complementary qualities of VLM-generated explanations via two quality scoring
functions. We propose Visual Fidelity, which captures how faithful an
explanation is to the visual context, and Contrastiveness, which captures how
well the explanation identifies visual details that distinguish the model's
prediction from plausible alternatives. On the A-OKVQA and VizWiz tasks, these
quality scoring functions are better calibrated with model correctness than
existing explanation qualities. We conduct a user study in which participants
have to decide whether a VLM prediction is accurate without viewing its visual
context. We observe that showing our quality scores alongside VLM explanations
improves participants' accuracy at predicting VLM correctness by 11.1%,
including a 15.4% reduction in the rate of falsely believing incorrect
predictions. These findings highlight the utility of explanation quality scores
in fostering appropriate reliance on VLM predictions.

</details>


### [36] [ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations](https://arxiv.org/abs/2509.25868)
*Yindong Wang,Martin Preiß,Margarita Bugueño,Jan Vincent Hoffbauer,Abdullatif Ghajar,Tolga Buz,Gerard de Melo*

Main category: cs.CL

TL;DR: ReFACT is a benchmark for detecting scientific confabulation in LLMs, featuring 1,001 expert-annotated QA pairs with precise error spans and types, enabling multi-stage evaluation.


<details>
  <summary>Details</summary>
Motivation: LLMs frequently confabulate scientific facts, undermining trustworthiness. Current benchmarks lack fine-grained evaluation beyond binary factuality.

Method: Created ReFACT benchmark with 1,001 expert-annotated question-answer pairs spanning diverse scientific domains, including both correct answers and non-factual counterparts with precise error annotations.

Result: Benchmarked 9 state-of-the-art LLMs showing limited performance (~50% accuracy). Even top models like GPT-4o fail to distinguish factual from confabulated scientific answers.

Conclusion: Highlights need for fine-grained, human-validated benchmarks to detect and correct scientific confabulation in domain-specific contexts, raising concerns about LLM-as-judge evaluation reliability.

Abstract: Large Language Models (LLMs) frequently confabulate scientific facts,severely
undermining their trustworthiness. Addressing this challenge requires
benchmarks that go beyond binary factuality and enable fine-grained evaluation.
We introduce \textbf{ReFACT} (\textit{Reddit False And Correct Texts}), a
benchmark of 1,001 expert-annotated question--answer pairs spanning diverse
scientific domains for the detection of scientific confabulation. Each instance
includes both a scientifically correct answer and a non-factual counterpart
annotated with \textbf{precise error spans and error-types}. ReFACT enables
multi-stage evaluation: (1) confabulation detection, (2) fine-grained error
localization, and (3) correction. We benchmark 9 state-of-the-art LLMs,
revealing limited performance ($\sim$50\% accuracy). Even top models such as
GPT-4o fail to distinguish factual from confabulated scientific answers,
raising concerns about the reliability of \textit{LLM-as-judge} evaluation
paradigms. Our findings highlight the need for fine-grained, human-validated
benchmarks to detect and correct scientific confabulation in domain-specific
contexts. Dataset is released on
\href{https://github.com/ddz5431/ReFACT}{GitHub}\footnote{We provide the
dataset at: https://github.com/ddz5431/ReFACT}.

</details>


### [37] [ASR Under Noise: Exploring Robustness for Sundanese and Javanese](https://arxiv.org/abs/2509.25878)
*Salsabila Zahirah Pranida,Muhammad Cendekia Airlangga,Rifo Ahmad Genadi,Shady Shehata*

Main category: cs.CL

TL;DR: This paper investigates the robustness of Whisper-based ASR models for Indonesian regional languages (Javanese and Sundanese) in noisy environments, showing that noise-aware training significantly improves performance.


<details>
  <summary>Details</summary>
Motivation: While recent work has shown strong ASR performance for Indonesian regional languages under clean conditions, their effectiveness in noisy environments remains unclear and needs investigation.

Method: The authors experiment with multiple training strategies including synthetic noise augmentation and SpecAugment, and evaluate performance across various signal-to-noise ratios (SNRs).

Result: Noise-aware training substantially improves robustness, particularly for larger Whisper models. Error analysis reveals language-specific challenges.

Conclusion: The study demonstrates that noise-aware training enhances ASR robustness for Indonesian regional languages, with error analysis highlighting specific areas for future improvements.

Abstract: We investigate the robustness of Whisper-based automatic speech recognition
(ASR) models for two major Indonesian regional languages: Javanese and
Sundanese. While recent work has demonstrated strong ASR performance under
clean conditions, their effectiveness in noisy environments remains unclear. To
address this, we experiment with multiple training strategies, including
synthetic noise augmentation and SpecAugment, and evaluate performance across a
range of signal-to-noise ratios (SNRs). Our results show that noise-aware
training substantially improves robustness, particularly for larger Whisper
models. A detailed error analysis further reveals language-specific challenges,
highlighting avenues for future improvements

</details>


### [38] [RoleConflictBench: A Benchmark of Role Conflict Scenarios for Evaluating LLMs' Contextual Sensitivity](https://arxiv.org/abs/2509.25897)
*Jisu Shin,Hoyun Song,Juhyun Oh,Changgeon Ko,Eunsu Kim,Chani Jung,Alice Oh*

Main category: cs.CL

TL;DR: This paper introduces RoleConflictBench, a benchmark to evaluate LLMs' contextual sensitivity in role conflict scenarios, finding that LLMs have insufficient sensitivity and are dominated by inherent social role biases.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs behave in complex social situations like role conflicts, which are inherently ambiguous social dilemmas requiring contextual sensitivity, as LLMs become increasingly influential in human decision-making.

Method: Developed RoleConflictBench using a three-stage pipeline to generate over 13K realistic role conflict scenarios across 65 roles, systematically varying expectations and situational urgency levels, then analyzed choices across 10 different LLMs.

Result: LLMs show some capacity to respond to contextual cues but this sensitivity is insufficient. Their decisions are predominantly governed by inherent biases related to social roles rather than situational information, with preferences for Family and Occupation domains, male roles, and Abrahamic religions.

Conclusion: LLMs exhibit significant biases in handling role conflicts, prioritizing social role stereotypes over contextual situational information, highlighting limitations in their contextual sensitivity for complex social dilemmas.

Abstract: Humans often encounter role conflicts -- social dilemmas where the
expectations of multiple roles clash and cannot be simultaneously fulfilled. As
large language models (LLMs) become increasingly influential in human
decision-making, understanding how they behave in complex social situations is
essential. While previous research has evaluated LLMs' social abilities in
contexts with predefined correct answers, role conflicts represent inherently
ambiguous social dilemmas that require contextual sensitivity: the ability to
recognize and appropriately weigh situational cues that can fundamentally alter
decision priorities. To address this gap, we introduce RoleConflictBench, a
novel benchmark designed to evaluate LLMs' contextual sensitivity in complex
social dilemmas. Our benchmark employs a three-stage pipeline to generate over
13K realistic role conflict scenarios across 65 roles, systematically varying
their associated expectations (i.e., their responsibilities and obligations)
and situational urgency levels. By analyzing model choices across 10 different
LLMs, we find that while LLMs show some capacity to respond to these contextual
cues, this sensitivity is insufficient. Instead, their decisions are
predominantly governed by a powerful, inherent bias related to social roles
rather than situational information. Our analysis quantifies these biases,
revealing a dominant preference for roles within the Family and Occupation
domains, as well as a clear prioritization of male roles and Abrahamic
religions across most evaluatee models.

</details>


### [39] [PerQ: Efficient Evaluation of Multilingual Text Personalization Quality](https://arxiv.org/abs/2509.25903)
*Dominik Macko,Andrew Pulver*

Main category: cs.CL

TL;DR: Proposes PerQ, a computationally efficient method for evaluating personalization quality in generated texts, reducing reliance on multiple expensive LLMs for meta-evaluation.


<details>
  <summary>Details</summary>
Motivation: Current text evaluation lacks specific metrics for personalization quality, forcing researchers to use multiple biased LLMs for meta-evaluation, which increases costs significantly.

Method: Introduces PerQ, a specialized metric designed to efficiently evaluate the personalization quality of texts generated by language models.

Result: A case study demonstrates PerQ's effectiveness in comparing generation capabilities of large and small language models, showing practical utility in research.

Conclusion: PerQ provides a resource-efficient alternative to using multiple LLMs for personalization quality evaluation, effectively reducing computational waste in research.

Abstract: Since no metrics are available to evaluate specific aspects of a text, such
as its personalization quality, the researchers often rely solely on large
language models to meta-evaluate such texts. Due to internal biases of
individual language models, it is recommended to use multiple of them for
combined evaluation, which directly increases costs of such meta-evaluation. In
this paper, a computationally efficient method for evaluation of
personalization quality of a given text (generated by a language model) is
introduced, called PerQ. A case study of comparison of generation capabilities
of large and small language models shows the usability of the proposed metric
in research, effectively reducing the waste of resources.

</details>


### [40] [Mem-α: Learning Memory Construction via Reinforcement Learning](https://arxiv.org/abs/2509.25911)
*Yu Wang,Ryuichi Takanobu,Zhiqi Liang,Yuzhen Mao,Yuanzhe Hu,Julian McAuley,Xiaojian Wu*

Main category: cs.CL

TL;DR: Mem-alpha is a reinforcement learning framework that trains LLM agents to effectively manage complex memory systems through interaction and feedback, achieving significant improvements over existing methods and demonstrating strong generalization to sequences 13x longer than training data.


<details>
  <summary>Details</summary>
Motivation: Current memory-augmented LLM agents rely on pre-defined instructions and tools for memory updates, but language models struggle to determine what information to store, how to structure it, and when to update it, leading to suboptimal memory construction and information loss.

Method: Propose Mem-alpha RL framework that trains agents through sequential information processing, learning to extract/store relevant content and update memory systems. Uses specialized training dataset with multi-turn interactions and evaluation questions. Reward signal comes from downstream QA accuracy over full interaction history.

Result: Mem-alpha achieves significant improvements over existing memory-augmented agent baselines. Agents trained on max 30k token sequences generalize remarkably to sequences exceeding 400k tokens (13x training length), demonstrating framework robustness.

Conclusion: The Mem-alpha reinforcement learning framework effectively trains agents to manage complex memory systems, overcoming limitations of pre-defined memory instructions and showing strong generalization capabilities to much longer sequences than seen during training.

Abstract: Large language model (LLM) agents are constrained by limited context windows,
necessitating external memory systems for long-term information understanding.
Current memory-augmented agents typically depend on pre-defined instructions
and tools for memory updates. However, language models may lack the ability to
determine which information to store, how to structure it, and when to update
it, especially as memory systems become more complex. This results in
suboptimal memory construction and information loss. To this end, we propose
Mem-alpha, a reinforcement learning framework that trains agents to effectively
manage complex memory systems through interaction and feedback. We also
construct a specialized training dataset spanning diverse multi-turn
interaction patterns paired with comprehensive evaluation questions designed to
teach effective memory management. During training, agents process sequential
information chunks, learn to extract and store relevant content, then update
the memory system. The reward signal derives from downstream question-answering
accuracy over the full interaction history, directly optimizing for memory
construction. To illustrate the effectiveness of our training framework, we
design a memory architecture comprising core, episodic, and semantic
components, equipped with multiple tools for memory operations. Empirical
evaluation demonstrates that Mem-alpha achieves significant improvements over
existing memory-augmented agent baselines. Despite being trained exclusively on
instances with a maximum length of 30k tokens, our agents exhibit remarkable
generalization to sequences exceeding 400k tokens, over 13x the training
length, highlighting the robustness of Mem-alpha.

</details>


### [41] [Understanding the Mixture-of-Experts with Nadaraya-Watson Kernel](https://arxiv.org/abs/2509.25913)
*Chuanyang Zheng,Jiankai Sun,Yihang Gao,Enze Xie,Yuehao Wang,Peihao Wang,Ting Xu,Matthew Chang,Liliang Ren,Jingyao Li,Jing Xiong,Kashif Rasul,Mac Schwager,Anderson Schneider,Zhangyang Wang,Yuriy Nevmyvaka*

Main category: cs.CL

TL;DR: This paper challenges the standard use of Softmax in Mixture-of-Experts (MoE) models by proposing KERN, a novel FFN-style router function based on Nadaraya-Watson regression insights.


<details>
  <summary>Details</summary>
Motivation: The authors question the unchallenged assumption that Softmax is necessary for MoE routers, noting it has become standard practice without principled justification. They aim to find a more theoretically grounded alternative.

Method: The authors reinterpret MoE through Nadaraya-Watson regression framework, showing both FFN and MoE are special cases. They propose KERN router with ReLU activation and l2-normalization as a zero-cost alternative to Softmax.

Result: Comprehensive experiments in MoE and LLM settings validate that the proposed FFN-style router function effectively replaces Softmax while generalizing both Sigmoid- and Softmax-based routers.

Conclusion: The KERN router provides a theoretically motivated, zero-additional-cost alternative to Softmax that works effectively in modern MoE architectures, challenging long-standing assumptions about router design.

Abstract: Mixture-of-Experts (MoE) has become a cornerstone in recent state-of-the-art
large language models (LLMs). Traditionally, MoE relies on $\mathrm{Softmax}$
as the router score function to aggregate expert output, a designed choice that
has persisted from the earliest MoE models to modern LLMs, and is now widely
regarded as standard practice. However, the necessity of using
$\mathrm{Softmax}$ to project router weights into a probability simplex remains
an unchallenged assumption rather than a principled design choice. In this
work, we first revisit the classical Nadaraya-Watson regression and observe
that MoE shares the same mathematical formulation as Nadaraya-Watson
regression. Furthermore, we show that both feed-forward neural network (FFN)
and MoE can be interpreted as a special case of Nadaraya-Watson regression,
where the kernel function corresponds to the input neurons of the output layer.
Motivated by these insights, we propose the \textbf{zero-additional-cost}
Kernel Inspired Router with Normalization (KERN), an FFN-style router function,
as an alternative to $\mathrm{Softmax}$. We demonstrate that this router
generalizes both $\mathrm{Sigmoid}$- and $\mathrm{Softmax}$-based routers.
\textbf{Based on empirical observations and established practices in FFN
implementation, we recommend the use of $\mathrm{ReLU}$ activation and
$\ell_2$-normalization in $\mathrm{KERN}$ router function.} Comprehensive
experiments in MoE and LLM validate the effectiveness of the proposed FFN-style
router function \methodNorm.

</details>


### [42] [Bringing Emerging Architectures to Sequence Labeling in NLP](https://arxiv.org/abs/2509.25918)
*Ana Ezquerro,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: The paper evaluates alternative architectures (xLSTMs, SSMs, diffusion models, adversarial learning) for sequence labeling tasks, finding their strong performance in simple settings doesn't generalize well across languages, datasets, or complex structured tasks.


<details>
  <summary>Details</summary>
Motivation: To investigate how alternative architectures beyond dominant Transformer encoders perform on sequence labeling tasks with varying structural complexity, label spaces, and token dependencies across multiple languages.

Method: Study various architectures including xLSTMs, structured state-space models, diffusion models, and adversarial learning across tagging tasks with different complexity levels, evaluated on multiple languages and datasets.

Result: The strong performance of alternative architectures observed in simpler settings does not generalize well across languages or datasets, and fails to extend to more complex structured tasks.

Conclusion: Alternative architectures show promise but have limitations in generalizing across diverse sequence labeling scenarios, suggesting the need for more robust evaluation frameworks beyond simple settings.

Abstract: Pretrained Transformer encoders are the dominant approach to sequence
labeling. While some alternative architectures-such as xLSTMs, structured
state-space models, diffusion models, and adversarial learning-have shown
promise in language modeling, few have been applied to sequence labeling, and
mostly on flat or simplified tasks. We study how these architectures adapt
across tagging tasks that vary in structural complexity, label space, and token
dependencies, with evaluation spanning multiple languages. We find that the
strong performance previously observed in simpler settings does not always
generalize well across languages or datasets, nor does it extend to more
complex structured tasks.

</details>


### [43] [Reliability Crisis of Reference-free Metrics for Grammatical Error Correction](https://arxiv.org/abs/2509.25961)
*Takumi Goto,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: Adversarial attack strategies for reference-free GEC metrics demonstrate vulnerabilities in current evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current reference-free GEC metrics achieve high correlation with human judgments but are vulnerable to adversarial systems seeking unjustifiably high scores, undermining automatic evaluation reliability.

Method: Proposed adversarial attack strategies targeting four reference-free metrics: SOME, Scribendi, IMPARA, and LLM-based metrics.

Result: The adversarial systems outperform current state-of-the-art GEC systems, successfully exploiting metric vulnerabilities.

Conclusion: Findings highlight the urgent need for more robust evaluation methods in grammatical error correction to prevent misleading system selection.

Abstract: Reference-free evaluation metrics for grammatical error correction (GEC) have
achieved high correlation with human judgments. However, these metrics are not
designed to evaluate adversarial systems that aim to obtain unjustifiably high
scores. The existence of such systems undermines the reliability of automatic
evaluation, as it can mislead users in selecting appropriate GEC systems. In
this study, we propose adversarial attack strategies for four reference-free
metrics: SOME, Scribendi, IMPARA, and LLM-based metrics, and demonstrate that
our adversarial systems outperform the current state-of-the-art. These findings
highlight the need for more robust evaluation methods.

</details>


### [44] [RAGferee: Building Contextual Reward Models for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.26011)
*Andrei C. Coman,Ionut-Teodor Sorodoc,Leonardo F. R. Ribeiro,Bill Byrne,James Henderson,Adrià de Gispert*

Main category: cs.CL

TL;DR: RAGferee is a methodology that converts QA datasets into preference pairs focused on groundedness, enabling training of specialized RMs for RAG systems that outperform larger general RMs.


<details>
  <summary>Details</summary>
Motivation: Existing Reward Models trained on general preference data struggle in RAG settings which require judging faithfulness to context, query relevance, appropriate refusals, completeness and conciseness.

Method: RAGferee repurposes question-answering datasets into preference pairs prioritizing groundedness over stylistic features, then fine-tunes RMs from 7B to 24B parameters on a curated 4K-sample dataset.

Result: RAG-centric RMs achieve state-of-the-art performance on ContextualJudgeBench, surpassing existing 70B+ RMs trained on much larger (up to 2.4M samples) general corpora with +15.5% absolute improvement.

Conclusion: Specialized RMs trained on RAG-focused preference data can significantly outperform much larger general-purpose RMs in RAG evaluation tasks.

Abstract: Existing Reward Models (RMs), typically trained on general preference data,
struggle in Retrieval Augmented Generation (RAG) settings, which require
judging responses for faithfulness to retrieved context, relevance to the user
query, appropriate refusals when context is insufficient, completeness and
conciseness of information. To address the lack of publicly available
RAG-centric preference datasets and specialised RMs, we introduce RAGferee, a
methodology that repurposes question-answering (QA) datasets into preference
pairs that prioritise groundedness over stylistic features, enabling the
training of contextual RMs better suited to judging RAG responses. Using
RAGferee, we curate a small preference dataset of 4K samples and fine-tune RMs
ranging from 7B to 24B parameters. Our RAG-centric RMs achieve state-of-the-art
performance on ContextualJudgeBench, surpassing existing 70B+ RMs trained on
much larger (up to 2.4M samples) general corpora, with an absolute improvement
of +15.5%.

</details>


### [45] [RE$^2$: Improving Chinese Grammatical Error Correction via Retrieving Appropriate Examples with Explanation](https://arxiv.org/abs/2509.26038)
*Baoxin Wang,Yumeng Luo,Yixuan Wang,Dayong Wu,Wanxiang Che,Shijin Wang*

Main category: cs.CL

TL;DR: RE² method improves Chinese grammatical error correction by using grammatical error explanations instead of text similarity for example retrieval, achieving better performance on CGEC datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on text similarity for example retrieval, which often mismatches actual error patterns and retrieves lexically similar but grammatically irrelevant sentences.

Method: Proposed RE² method retrieves appropriate examples with explanations of grammatical errors, using error explanations rather than text similarity to select reference examples for LLMs.

Result: Experimental results on two CGEC datasets show that the proposed method effectively improves CGEC performance. Created a high-quality grammatical error explanation dataset for future research.

Conclusion: Using grammatical error explanations for example retrieval is more effective than text similarity for improving Chinese grammatical error correction performance with LLMs.

Abstract: The primary objective of Chinese grammatical error correction (CGEC) is to
detect and correct errors in Chinese sentences. Recent research shows that
large language models (LLMs) have been applied to CGEC with significant
results. For LLMs, selecting appropriate reference examples can help improve
their performance. However, existing methods predominantly rely on text
similarity for example retrieval, a strategy that frequently mismatches actual
error patterns and retrieves lexically similar yet grammatically irrelevant
sentences. To address this problem, we propose a method named RE$^2$, which
retrieves appropriate examples with explanations of grammatical errors. Instead
of using text similarity of the input sentence, we use explanations of
grammatical errors to select reference examples, which are used by LLMs to
improve the performance of CGEC. We conduct experiments on two CGEC datasets
and create a high-quality grammatical error explanation (GEE) dataset, which is
not only used in our research but also serves as a valuable resource for future
studies in both CGEC and GEE. The experimental results on the two datasets
indicate that our proposed method effectively improves the performance of CGEC.

</details>


### [46] [Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning](https://arxiv.org/abs/2509.26041)
*Arash Marioriyad,Shaygan Adim,Nima Alighardashi,Mahdieh Soleymani Banghshah,Mohammad Hossein Rohban*

Main category: cs.CL

TL;DR: LLM reasoning faithfulness is systematically influenced by hint shortcuts, with correct hints improving accuracy and complex hints increasing verbal acknowledgment, while presentation styles affect overt vs hidden reliance.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLM-generated rationales are faithful computations or post-hoc narratives shaped by embedded hint shortcuts in CoT prompting.

Method: Systematic study across 4 datasets (AIME, GSM-Hard, MATH-500, UniADILR), 2 models (GPT-4o, Gemini-2-Flash), with controlled hint manipulations varying in correctness, presentation style, and complexity.

Result: Correct hints substantially improve accuracy, especially on harder tasks; hint acknowledgment is uneven (complex hints get more verbalization); presentation style affects reliance patterns (sycophancy encourages overt acknowledgment, data leak promotes hidden reliance).

Conclusion: LLM reasoning is systematically shaped by shortcuts that obscure faithfulness, with RLHF-related effects influencing how hints are acknowledged and utilized.

Abstract: Large language models (LLMs) increasingly rely on chain-of-thought (CoT)
prompting to solve mathematical and logical reasoning tasks. Yet, a central
question remains: to what extent are these generated rationales \emph{faithful}
to the underlying computations, rather than post-hoc narratives shaped by hints
that function as answer shortcuts embedded in the prompt? Following prior work
on hinted vs.\ unhinted prompting, we present a systematic study of CoT
faithfulness under controlled hint manipulations. Our experimental design spans
four datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models
(GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in
correctness (correct and incorrect), presentation style (sycophancy and data
leak), and complexity (raw answers, two-operator expressions, four-operator
expressions). We evaluate both task accuracy and whether hints are explicitly
acknowledged in the reasoning. Our results reveal three key findings. First,
correct hints substantially improve accuracy, especially on harder benchmarks
and logical reasoning, while incorrect hints sharply reduce accuracy in tasks
with lower baseline competence. Second, acknowledgement of hints is highly
uneven: equation-based hints are frequently referenced, whereas raw hints are
often adopted silently, indicating that more complex hints push models toward
verbalizing their reliance in the reasoning process. Third, presentation style
matters: sycophancy prompts encourage overt acknowledgement, while leak-style
prompts increase accuracy but promote hidden reliance. This may reflect
RLHF-related effects, as sycophancy exploits the human-pleasing side and data
leak triggers the self-censoring side. Together, these results demonstrate that
LLM reasoning is systematically shaped by shortcuts in ways that obscure
faithfulness.

</details>


### [47] [RE-Searcher: Robust Agentic Search with Goal-oriented Planning and Self-reflection](https://arxiv.org/abs/2509.26048)
*Daocheng Fu,Jianbiao Mei,Licheng Wen,Xuemeng Yang,Cheng Yang,Rong Wu,Tao Hu,Siqi Li,Yufan Shen,Xinyu Cai,Pinlong Cai,Botian Shi,Yong Liu,Yu Qiao*

Main category: cs.CL

TL;DR: RE-Searcher is a search agent that combines goal-oriented planning and self-reflection to improve search robustness in complex environments, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges in real-world deployment due to knowledge cutoff, hallucination, and limited interaction modalities. While external search tools help, they expose agents to complex search environments where small query variations can lead to unproductive reasoning and amplify errors.

Method: RE-Searcher explicitly articulates concrete search goals and reflects on whether retrieved evidence satisfies those goals. This goal-oriented planning with self-reflection helps resist spurious cues in complex search environments.

Result: Extensive experiments show improved search accuracy and state-of-the-art results. Perturbation studies demonstrate substantial resilience to noisy or misleading external signals, mitigating search process fragility.

Conclusion: The findings offer practical guidance for integrating LLM-powered agents into complex interactive environments and enabling more autonomous decision-making through robust search mechanisms.

Abstract: Large language models (LLMs) excel at knowledge-intensive question answering
and reasoning, yet their real-world deployment remains constrained by knowledge
cutoff, hallucination, and limited interaction modalities. Augmenting LLMs with
external search tools helps alleviate these issues, but it also exposes agents
to a complex search environment in which small, plausible variations in query
formulation can steer reasoning into unproductive trajectories and amplify
errors. We present a systematic analysis that quantifies how environmental
complexity induces fragile search behaviors and, in turn, degrades overall
performance. To address this challenge, we propose a simple yet effective
approach to instantiate a search agent, RE-Searcher. During search, RE-Searcher
explicitly articulates a concrete search goal and subsequently reflects on
whether the retrieved evidence satisfies that goal. This combination of
goal-oriented planning and self-reflection enables RE-Searcher to resist
spurious cues in complex search environments and perform robust search.
Extensive experiments show that our method improves search accuracy and
achieves state-of-the-art results. Perturbation studies further demonstrate
substantial resilience to noisy or misleading external signals, mitigating the
fragility of the search process. We believe these findings offer practical
guidance for integrating LLM-powered agents into more complex interactive
environments and enabling more autonomous decision-making.

</details>


### [48] [CEAID: Benchmark of Multilingual Machine-Generated Text Detection Methods for Central European Languages](https://arxiv.org/abs/2509.26051)
*Dominik Macko,Jakub Kopal*

Main category: cs.CL

TL;DR: First benchmark of machine-generated text detection methods for Central European languages, showing supervised finetuned detectors perform best and are most robust against obfuscation.


<details>
  <summary>Details</summary>
Motivation: Existing detectors focus mainly on English, leaving Central European languages unexplored with limited cross-lingual transferability.

Method: Multi-domain, multi-generator, multilingual evaluation comparing train-language combinations and testing adversarial robustness.

Result: Supervised finetuned detectors in Central European languages are most performant and resistant to obfuscation.

Conclusion: Language-specific finetuning is crucial for effective machine-generated text detection in Central European languages.

Abstract: Machine-generated text detection, as an important task, is predominantly
focused on English in research. This makes the existing detectors almost
unusable for non-English languages, relying purely on cross-lingual
transferability. There exist only a few works focused on any of Central
European languages, leaving the transferability towards these languages rather
unexplored. We fill this gap by providing the first benchmark of detection
methods focused on this region, while also providing comparison of
train-languages combinations to identify the best performing ones. We focus on
multi-domain, multi-generator, and multilingual evaluation, pinpointing the
differences of individual aspects, as well as adversarial robustness of
detection methods. Supervised finetuned detectors in the Central European
languages are found the most performant in these languages as well as the most
resistant against obfuscation.

</details>


### [49] [DyFlow: Dynamic Workflow Framework for Agentic Reasoning](https://arxiv.org/abs/2509.26062)
*Yanbo Wang,Zixiang Xu,Yue Huang,Xiangqi Wang,Zirui Song,Lang Gao,Chenxi Wang,Xiangru Tang,Yue Zhao,Arman Cohan,Xiangliang Zhang,Xiuying Chen*

Main category: cs.CL

TL;DR: DyFlow is a dynamic workflow generation framework for LLM-based agents that adaptively constructs and adjusts reasoning procedures using real-time feedback, enhancing cross-task generalization.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent systems rely on manually designed workflows that limit adaptability, while automated methods are dataset-specific and make limited use of intermediate feedback, reducing robustness and reasoning depth.

Method: DyFlow consists of two components: a designer that decomposes problems into sub-goals and dynamically plans next steps, and an executor that uses dynamic operators with context-aware parameterization to execute operations.

Result: DyFlow significantly outperforms existing baselines across diverse domains including social reasoning, biomedical tasks, mathematical problem solving, and code generation, achieving substantial Pass@k improvements.

Conclusion: The framework demonstrates robust generalization across domains and enables flexible, semantically grounded reasoning through dynamic workflow generation and real-time feedback utilization.

Abstract: Agent systems based on large language models (LLMs) have shown great
potential in complex reasoning tasks, but building efficient and generalizable
workflows remains a major challenge. Most existing approaches rely on manually
designed processes, which limits their adaptability across different tasks.
While a few methods attempt automated workflow generation, they are often tied
to specific datasets or query types and make limited use of intermediate
feedback, reducing system robustness and reasoning depth. Moreover, their
operations are typically predefined and inflexible. To address these
limitations, we propose DyFlow, a dynamic workflow generation framework that
adaptively constructs and adjusts reasoning procedures based on task
requirements and real-time intermediate feedback, thereby enhancing cross-task
generalization. DyFlow consists of two core components: a designer and an
executor. The designer decomposes complex problems into a sequence of sub-goals
defined by high-level objectives and dynamically plans the next steps based on
intermediate outputs and feedback. These plans are then carried out by the
executor, which executes each operation using dynamic operators with
context-aware parameterization, enabling flexible and semantically grounded
reasoning. We systematically evaluate DyFlow across diverse domains, including
social reasoning, biomedical tasks, mathematical problem solving, and code
generation. Results demonstrate that DyFlow significantly outperforms existing
baselines, achieving substantial Pass@k improvements and exhibiting robust
generalization across diverse domains. The code is publicly available at
https://github.com/wyf23187/DyFlow.

</details>


### [50] [The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge](https://arxiv.org/abs/2509.26072)
*Arash Marioriyad,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah*

Main category: cs.CL

TL;DR: LLM judges exhibit systematic biases based on superficial cues (provenance and recency) rather than actual response quality, and fail to acknowledge these biases in their justifications.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLM judges base their evaluations faithfully on response quality or rely on shortcuts introduced in prompts, which would undermine their reliability as evaluators.

Method: Used ELI5 and LitBench datasets with pairwise comparisons, constructed 100 judgment tasks per dataset, employed GPT-4o and Gemini-2.5-Flash as evaluators, and systematically manipulated provenance (Human, Expert, LLM, Unknown) and recency (Old/1950 vs New/2025) cues while keeping content constant.

Result: Both models showed strong recency bias (favoring new responses) and clear provenance hierarchy (Expert > Human > LLM > Unknown), with biases more pronounced in GPT-4o and subjective domains like LitBench. Cue acknowledgment was rare - justifications rationalized decisions based on content qualities rather than the injected cues.

Conclusion: Current LLM-as-a-judge systems are shortcut-prone and unfaithful, relying on superficial cues rather than genuine quality assessment, which undermines their reliability for research and deployment purposes.

Abstract: Large language models (LLMs) are increasingly deployed as automatic judges to
evaluate system outputs in tasks such as summarization, dialogue, and creative
writing. A faithful judge should base its verdicts solely on response quality
and explicitly acknowledge the factors shaping its decision. We show that
current LLM judges fail on both counts by relying on shortcuts introduced in
the prompt. Our study uses two evaluation datasets: ELI5, a benchmark for
long-form question answering, and LitBench, a recent benchmark for creative
writing. Both datasets provide pairwise comparisons, where the evaluator must
choose which of two responses is better. From each dataset we construct 100
pairwise judgment tasks and employ two widely used models, GPT-4o and
Gemini-2.5-Flash, as evaluators in the role of LLM-as-a-judge. For each pair,
we assign superficial cues to the responses, provenance cues indicating source
identity (Human, Expert, LLM, or Unknown) and recency cues indicating temporal
origin (Old, 1950 vs. New, 2025), while keeping the rest of the prompt fixed.
Results reveal consistent verdict shifts: both models exhibit a strong recency
bias, systematically favoring new responses over old, as well as a clear
provenance hierarchy (Expert > Human > LLM > Unknown). These biases are
especially pronounced in GPT-4o and in the more subjective and open-ended
LitBench domain. Crucially, cue acknowledgment is rare: justifications almost
never reference the injected cues, instead rationalizing decisions in terms of
content qualities. These findings demonstrate that current LLM-as-a-judge
systems are shortcut-prone and unfaithful, undermining their reliability as
evaluators in both research and deployment.

</details>


### [51] [Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis](https://arxiv.org/abs/2509.26074)
*Leitian Tao,Xuefeng Du,Yixuan Li*

Main category: cs.CL

TL;DR: LENS is a novel framework that synthesizes preference data directly in LLM's latent embedding space using VAE, bypassing expensive text generation and annotation while preserving preference ordering.


<details>
  <summary>Details</summary>
Motivation: High cost of preference data collection bottlenecks reward modeling for aligning LLMs with human preferences, and existing text-based synthesis methods are computationally expensive.

Method: Uses VAE to learn structured latent representation of response embeddings, performs controlled perturbations in latent space, and decodes back to embedding space to generate synthetic preference pairs.

Result: Significantly outperforms text-based augmentation on benchmarks, achieves superior results with 18x faster generation and uses 16,000x smaller model.

Conclusion: LENS offers scalable and effective alternative for enhancing reward modeling through efficient latent-space data augmentation.

Abstract: Reward modeling, crucial for aligning large language models (LLMs) with human
preferences, is often bottlenecked by the high cost of preference data.
Existing textual data synthesis methods are computationally expensive. We
propose a novel framework LENS for synthesizing preference data directly in the
LLM's latent embedding space. Our method employs a Variational Autoencoder
(VAE) to learn a structured latent representation of response embeddings. By
performing controlled perturbations in this latent space and decoding back to
the embedding space, we efficiently generate diverse, semantically consistent
synthetic preference pairs, bypassing costly text generation and annotation. We
provide theoretical guarantees that our synthesized pairs approximately
preserve original preference ordering and improve reward model generalization.
Empirically, our latent-space synthesis significantly outperforms text-based
augmentation on standard benchmarks, achieving superior results while being 18x
faster in generation and using a 16,000x smaller model. Our work offers a
scalable and effective alternative for enhancing reward modeling through
efficient data augmentation. Code is publicly available at
https://github.com/deeplearning-wisc/lens

</details>


### [52] [IMProofBench: Benchmarking AI on Research-Level Mathematical Proof Generation](https://arxiv.org/abs/2509.26076)
*Johannes Schmitt,Gergely Bérczi,Jasper Dekoninck,Jeremy Feusi,Tim Gehrunger,Raphael Appenzeller,Jim Bryan,Niklas Canova,Timo de Wolff,Filippo Gaia,Michel van Garrel,Baran Hashemi,David Holmes,Aitor Iribar Lopez,Victor Jaeck,Martina Jørgensen,Steven Kelk,Stefan Kuhlmann,Adam Kurpisz,Chiara Meroni,Ingmar Metzler,Martin Möller,Samuel Muñoz-Echániz,Robert Nowak,Georg Oberdieck,Daniel Platt,Dylan Possamaï,Gabriel Ribeiro,Raúl Sánchez Galán,Zheming Sun,Josef Teichmann,Richard P. Thomas,Charles Vial*

Main category: cs.CL

TL;DR: IMProofBench is a new benchmark for evaluating LLMs on research-level mathematical problems, featuring peer-reviewed problems requiring detailed proofs and subproblems with final answers, tested in an agentic framework with tools.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are limited to final-answer questions or high-school competition problems, failing to evaluate LLMs' capabilities on research-level mathematical tasks at the frontier of knowledge.

Method: Created IMProofBench with 39 peer-reviewed problems developed by expert mathematicians, each requiring detailed proofs and paired with subproblems. Evaluation uses agentic framework with tools like web search and SageMath to simulate realistic research environment.

Result: Current LLMs succeed at accessible research-level questions but struggle with challenging problems. Grok-4 achieves 52% accuracy on final-answer subproblems, while GPT-5 achieves 22% fully correct proof solutions.

Conclusion: IMProofBench addresses the gap in evaluating LLMs on research-level mathematics and will evolve as a dynamic benchmark in collaboration with the mathematical community to ensure relevance for future LLM evaluation.

Abstract: As the mathematical capabilities of large language models (LLMs) improve, it
becomes increasingly important to evaluate their performance on research-level
tasks at the frontier of mathematical knowledge. However, existing benchmarks
are limited, as they focus solely on final-answer questions or high-school
competition problems. To address this gap, we introduce IMProofBench, a private
benchmark consisting of 39 peer-reviewed problems developed by expert
mathematicians. Each problem requires a detailed proof and is paired with
subproblems that have final answers, supporting both an evaluation of
mathematical reasoning capabilities by human experts and a large-scale
quantitative analysis through automated grading. Furthermore, unlike prior
benchmarks, the evaluation setup simulates a realistic research environment:
models operate in an agentic framework with tools like web search for
literature review and mathematical software such as SageMath. Our results show
that current LLMs can succeed at the more accessible research-level questions,
but still encounter significant difficulties on more challenging problems.
Quantitatively, Grok-4 achieves the highest accuracy of 52% on final-answer
subproblems, while GPT-5 obtains the best performance for proof generation,
achieving a fully correct solution for 22% of problems. IMProofBench will
continue to evolve as a dynamic benchmark in collaboration with the
mathematical community, ensuring its relevance for evaluating the next
generation of LLMs.

</details>


### [53] [Reinforced Strategy Optimization for Conversational Recommender Systems via Network-of-Experts](https://arxiv.org/abs/2509.26093)
*Xiaoyan Zhao*

Main category: cs.CL

TL;DR: RSO is a hierarchical framework for conversational recommender systems that separates strategy planning from response generation, using reinforcement learning with LLM-based rewards to optimize interaction strategies.


<details>
  <summary>Details</summary>
Motivation: Existing methods using unified prompts in LLM-based conversational recommender systems lack explicit optimization of interaction strategies, leading to suboptimal outcomes.

Method: Hierarchical framework with Planner selecting strategies and Actor generating responses guided by experts. Uses reinforcement learning with LLM-based rewards to address limited multi-turn data.

Result: RSO outperforms state-of-the-art baselines in experiments.

Conclusion: The hierarchical strategy optimization approach is effective for improving conversational recommender systems.

Abstract: Conversational Recommender Systems (CRSs) provide personalized
recommendations through multi-turn interactions. With the strong reasoning
abilities of Large Language Models (LLMs), applying them to CRSs has become
promising. Yet, existing methods often lack explicit optimization of
interaction strategies, relying instead on unified prompts, which can yield
suboptimal outcomes. We propose Reinforced Strategy Optimization (RSO), a
hierarchical framework that decomposes response generation into macro-level
strategy planning and micro-level adaptation within a network-of-experts. A
Planner selects strategies (e.g., recommend, explain, encourage), while an
Actor generates responses guided by auxiliary experts for preferences and
factual grounding. This disentanglement enables more tractable learning. To
address limited multi-turn data, we model strategy learning as reinforcement
learning with an LLM-based reward for exploration. Experiments show RSO
outperforms state-of-the-art baselines, validating the effectiveness of
hierarchical strategy optimization.

</details>


### [54] [End-to-End Aspect-Guided Review Summarization at Scale](https://arxiv.org/abs/2509.26103)
*Ilya Boytsov,Vinny DeGenova,Mikhail Balyasin,Joseph Walt,Caitlin Eusden,Marie-Claire Rochat,Margaret Pierson*

Main category: cs.CL

TL;DR: A scalable LLM-based system that combines aspect-based sentiment analysis with guided summarization to generate concise product review summaries for Wayfair, validated through large-scale A/B testing.


<details>
  <summary>Details</summary>
Motivation: To create interpretable product review summaries that are grounded in actual customer feedback, addressing the need for scalable and effective review summarization in e-commerce platforms.

Method: Extracts aspect-sentiment pairs from reviews, selects most frequent aspects per product, samples representative reviews, and uses structured prompts to guide LLM summarization.

Result: Successfully deployed in real-time with demonstrated effectiveness through large-scale online A/B testing, and released a dataset of 11.8 million anonymized reviews with extracted aspects and summaries.

Conclusion: The system provides an effective approach for aspect-guided review summarization at scale, with practical deployment and valuable dataset contribution for future research.

Abstract: We present a scalable large language model (LLM)-based system that combines
aspect-based sentiment analysis (ABSA) with guided summarization to generate
concise and interpretable product review summaries for the Wayfair platform.
Our approach first extracts and consolidates aspect-sentiment pairs from
individual reviews, selects the most frequent aspects for each product, and
samples representative reviews accordingly. These are used to construct
structured prompts that guide the LLM to produce summaries grounded in actual
customer feedback. We demonstrate the real-world effectiveness of our system
through a large-scale online A/B test. Furthermore, we describe our real-time
deployment strategy and release a dataset of 11.8 million anonymized customer
reviews covering 92,000 products, including extracted aspects and generated
summaries, to support future research in aspect-guided review summarization.

</details>


### [55] [Vocabulary Customization for Efficient Domain-Specific LLM Deployment](https://arxiv.org/abs/2509.26124)
*Christian Herold,Michael Kozielski,Nicholas Santavas,Yannick Versley,Shahram Khadivi*

Main category: cs.CL

TL;DR: The paper addresses vocabulary mismatch in LLMs by augmenting pretrained vocabularies with domain-specific tokens, improving tokenization efficiency and reducing inference latency without compromising model quality.


<details>
  <summary>Details</summary>
Motivation: Vocabulary mismatch occurs when general-domain tokenizers fail to capture domain-specific terms, leading to suboptimal sub-word splits, higher token fertility, and decreased processing speed in out-of-domain text processing.

Method: Design an algorithm to extend existing tokenizers by adding domain-specific tokens while guaranteeing no decrease in tokenization efficiency - every input sequence is segmented into at most the same number of tokens as before.

Result: In e-Commerce use-cases, the augmented tokenizer shortens input sequences by up to 20%, reduces inference latency on downstream tasks while preserving predictive quality, and shows benefits in forward pass speed and token adoption rates.

Conclusion: Vocabulary adaptation through domain-specific token augmentation effectively addresses vocabulary mismatch, improving processing efficiency and inference speed without sacrificing model performance.

Abstract: When using an LLM to process text outside the training domain(s), an often
overlooked factor is vocabulary mismatch, where the general-domain tokenizer
fails to capture frequent domain-specific terms, leading to higher token
fertility and thus a decrease in processing speed due to suboptimal sub-word
splits.
  We address this limitation by augmenting the pretrained vocabulary with a set
of domain-specific tokens. To this end, we design an algorithm that extends an
existing tokenizer while guaranteeing it never decreases tokenization
efficiency: every input sequence is segmented into at most the same number of
tokens as before.
  Evaluated on real-world e-Commerce use-cases, the augmented tokenizer
significantly shortens input sequences by up to 20% and reduces inference
latency on downstream tasks while preserving predictive quality. We further
analyze secondary effects, such as the impact on forward pass speed and the
rate at which the model adopts the newly introduced tokens, to illustrate the
broader benefits of vocabulary adaptation.

</details>


### [56] [The Hunger Game Debate: On the Emergence of Over-Competition in Multi-Agent Systems](https://arxiv.org/abs/2509.26126)
*Xinbei Ma,Ruotian Ma,Xingyu Chen,Zhengliang Shi,Mengru Wang,Jen-tse Huang,Qu Yang,Wenxuan Wang,Fanghua Ye,Qingxuan Jiang,Mengfei Zhou,Zhuosheng Zhang,Rui Wang,Hai Zhao,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CL

TL;DR: This paper studies over-competition in LLM-based multi-agent debates, where extreme pressure causes unreliable and harmful behaviors that degrade task performance. The authors propose HATE framework to simulate zero-sum competition and find that objective feedback can mitigate these issues.


<details>
  <summary>Details</summary>
Motivation: To investigate how competition shapes behavior in LLM-based multi-agent systems, particularly the phenomenon of over-competition where agents under pressure exhibit harmful behaviors that undermine collaboration and task performance.

Method: Proposed HATE (Hunger Game Debate), a novel experimental framework simulating debates under zero-sum competition arena. Conducted experiments across various LLMs and tasks, and explored the impact of environmental feedback by adding different types of judges.

Result: Competitive pressure significantly stimulates over-competition behaviors and degrades task performance, causing discussions to derail. Objective, task-focused feedback effectively mitigates over-competition behaviors. The study also characterized top LLMs through post-hoc kindness analysis.

Conclusion: The research provides insights for understanding and governing emergent social dynamics in AI communities, highlighting both the risks of over-competition in multi-agent systems and potential mitigation strategies through appropriate environmental feedback.

Abstract: LLM-based multi-agent systems demonstrate great potential for tackling
complex problems, but how competition shapes their behavior remains
underexplored. This paper investigates the over-competition in multi-agent
debate, where agents under extreme pressure exhibit unreliable, harmful
behaviors that undermine both collaboration and task performance. To study this
phenomenon, we propose HATE, the Hunger Game Debate, a novel experimental
framework that simulates debates under a zero-sum competition arena. Our
experiments, conducted across a range of LLMs and tasks, reveal that
competitive pressure significantly stimulates over-competition behaviors and
degrades task performance, causing discussions to derail. We further explore
the impact of environmental feedback by adding variants of judges, indicating
that objective, task-focused feedback effectively mitigates the
over-competition behaviors. We also probe the post-hoc kindness of LLMs and
form a leaderboard to characterize top LLMs, providing insights for
understanding and governing the emergent social dynamics of AI community.

</details>


### [57] [CliniBench: A Clinical Outcome Prediction Benchmark for Generative and Encoder-Based Language Models](https://arxiv.org/abs/2509.26136)
*Paul Grundmann,Dennis Fast,Jan Frick,Thomas Steffek,Felix Gers,Wolfgang Nejdl,Alexander Löser*

Main category: cs.CL

TL;DR: CliniBench is the first benchmark comparing encoder-based classifiers and generative LLMs for discharge diagnosis prediction from admission notes, showing encoders outperform LLMs, though retrieval augmentation helps LLMs.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of generative LLMs in real-world clinical applications, particularly for discharge diagnosis prediction, as their capabilities in complex medical tasks remain underexplored.

Method: Created CliniBench benchmark using MIMIC-IV dataset to compare 12 generative LLMs and 3 encoder-based classifiers for discharge diagnosis prediction from admission notes, and assessed retrieval augmentation strategies for in-context learning.

Result: Encoder-based classifiers consistently outperform generative models in diagnosis prediction. Retrieval augmentation strategies provide notable performance improvements for generative LLMs.

Conclusion: While generative LLMs show promise, encoder-based classifiers remain superior for discharge diagnosis prediction, though retrieval augmentation can enhance LLM performance in clinical applications.

Abstract: With their growing capabilities, generative large language models (LLMs) are
being increasingly investigated for complex medical tasks. However, their
effectiveness in real-world clinical applications remains underexplored. To
address this, we present CliniBench, the first benchmark that enables
comparability of well-studied encoder-based classifiers and generative LLMs for
discharge diagnosis prediction from admission notes in MIMIC-IV dataset. Our
extensive study compares 12 generative LLMs and 3 encoder-based classifiers and
demonstrates that encoder-based classifiers consistently outperform generative
models in diagnosis prediction. We assess several retrieval augmentation
strategies for in-context learning from similar patients and find that they
provide notable performance improvements for generative LLMs.

</details>


### [58] [MGen: Millions of Naturally Occurring Generics in Context](https://arxiv.org/abs/2509.26160)
*Gustavo Cilleruelo,Emily Allaway,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TL;DR: MGen is a large dataset of 4+ million naturally occurring generic and quantified sentences from diverse sources, with long context documents covering 11 quantifiers, enabling large-scale computational research on genericity.


<details>
  <summary>Details</summary>
Motivation: To create the biggest and most diverse dataset of naturally occurring generic sentences to enable large-scale computational research on genericity, as existing resources were limited.

Method: Extracted over 4 million generic and quantified sentences from diverse textual sources including websites and academic papers, with long context documents and coverage of 11 different quantifiers.

Result: Created MGen dataset with interesting insights: generic sentences can be long (averaging over 16 words) and speakers often use them to express generalizations about people. It's the largest and most diverse dataset of naturally occurring generic sentences.

Conclusion: MGen opens the door to large-scale computational research on genericity and is publicly available, providing a valuable resource for studying generic language in natural contexts.

Abstract: MGen is a dataset of over 4 million naturally occurring generic and
quantified sentences extracted from diverse textual sources. Sentences in the
dataset have long context documents, corresponding to websites and academic
papers, and cover 11 different quantifiers. We analyze the features of generics
sentences in the dataset, with interesting insights: generics can be long
sentences (averaging over 16 words) and speakers often use them to express
generalisations about people.
  MGen is the biggest and most diverse dataset of naturally occurring generic
sentences, opening the door to large-scale computational research on
genericity. It is publicly available at https://gustavocilleruelo.com/mgen

</details>


### [59] [Explaining novel senses using definition generation with open language models](https://arxiv.org/abs/2509.26181)
*Mariia Fedorova,Andrey Kutuzov,Francesco Periti,Yves Scherrer*

Main category: cs.CL

TL;DR: Open-weights LLMs fine-tuned for novel sense definition generation outperform proprietary LLMs in AXOLOTL'24 shared task across Finnish, Russian, and German languages.


<details>
  <summary>Details</summary>
Motivation: To create explanations for novel word senses using open-source models that can outperform proprietary LLMs in semantic change modeling tasks.

Method: Fine-tuned open-weights large language models as definition generators, using target word usages as input on AXOLOTL'24 datasets for Finnish, Russian, and German languages.

Result: The fine-tuned open-source models performed higher than the best submissions from the shared task that used closed proprietary LLMs. Encoder-decoder models performed on par with decoder-only models.

Conclusion: Open-weights LLMs can effectively generate novel sense definitions and compete with proprietary models, with encoder-decoder architectures showing comparable performance to decoder-only approaches.

Abstract: We apply definition generators based on open-weights large language models to
the task of creating explanations of novel senses, taking target word usages as
an input. To this end, we employ the datasets from the AXOLOTL'24 shared task
on explainable semantic change modeling, which features Finnish, Russian and
German languages. We fine-tune and provide publicly the open-source models
performing higher than the best submissions of the aforementioned shared task,
which employed closed proprietary LLMs. In addition, we find that
encoder-decoder definition generators perform on par with their decoder-only
counterparts.

</details>


### [60] [VietBinoculars: A Zero-Shot Approach for Detecting Vietnamese LLM-Generated Text](https://arxiv.org/abs/2509.26189)
*Trieu Hai Nguyen,Sivaswamy Akilesh*

Main category: cs.CL

TL;DR: VietBinoculars is an optimized adaptation of the Binoculars method with improved global thresholds for detecting Vietnamese LLM-generated text, achieving over 99% accuracy across multiple metrics and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional detection methods are becoming less effective as LLM-generated text becomes more complex and human-like, especially with the rapid growth and diversity of LLM models in Vietnamese language contexts.

Method: Adapted the Binoculars method with optimized global thresholds, constructed new Vietnamese AI-generated datasets for threshold determination and benchmarking.

Result: VietBinoculars achieves over 99% accuracy, F1-score, and AUC on multiple out-of-domain datasets, outperforming original Binoculars, traditional methods, state-of-the-art approaches, and commercial tools like ZeroGPT and DetectGPT.

Conclusion: The optimized VietBinoculars method is highly effective for detecting Vietnamese LLM-generated text, demonstrating superior performance compared to existing detection approaches.

Abstract: The rapid development research of Large Language Models (LLMs) based on
transformer architectures raises key challenges, one of them being the task of
distinguishing between human-written text and LLM-generated text. As
LLM-generated textual content, becomes increasingly complex over time, and
resembles human writing, traditional detection methods are proving less
effective, especially as the number and diversity of LLMs continue to grow with
new models and versions being released at a rapid pace. This study proposes
VietBinoculars, an adaptation of the Binoculars method with optimized global
thresholds, to enhance the detection of Vietnamese LLM-generated text. We have
constructed new Vietnamese AI-generated datasets to determine the optimal
thresholds for VietBinoculars and to enable benchmarking. The results from our
experiments show results show that VietBinoculars achieves over 99\% in all two
domains of accuracy, F1-score, and AUC on multiple out-of-domain datasets. It
outperforms the original Binoculars model, traditional detection methods, and
other state-of-the-art approaches, including commercial tools such as ZeroGPT
and DetectGPT, especially under specially modified prompting strategies.

</details>


### [61] [Comparative Analysis of Ant Colony Optimization and Google OR-Tools for Solving the Open Capacitated Vehicle Routing Problem in Logistics](https://arxiv.org/abs/2509.26216)
*Assem Omar,Youssef Omar,Marwa Solayman,Hesham Mansour*

Main category: cs.CL

TL;DR: Comparative study of Ant Colony Optimization (ACO) and Google OR-Tools for solving Open Capacitated Vehicle Routing Problem (OCVRP), showing ACO offers routing flexibility while OR-Tools provides faster, more consistent performance.


<details>
  <summary>Details</summary>
Motivation: Modern logistics systems require efficient route planning, particularly for OCVRP where vehicles don't return to depot after deliveries, necessitating comparison of different optimization approaches.

Method: Implemented both ACO (nature-inspired metaheuristic) and Google OR-Tools in Python using custom dataset, evaluating performance based on routing efficiency, computation time, and scalability.

Result: ACO provides flexibility in routing parameters, while OR-Tools runs significantly faster with more consistency and requires less input complexity.

Conclusion: The comparison helps select appropriate routing strategies for scalable real-time logistics systems based on specific requirements - flexibility vs speed/consistency.

Abstract: In modern logistics management systems, route planning requires high
efficiency. The Open Capacitated Vehicle Routing Problem (OCVRP) deals with
finding optimal delivery routes for a fleet of vehicles serving geographically
distributed customers, without requiring the vehicles to return to the depot
after deliveries. The present study is comparative in nature and speaks of two
algorithms for OCVRP solution: Ant Colony Optimization (ACO), a nature-inspired
metaheuristic; and Google OR-Tools, an industry-standard toolkit for
optimization. Both implementations were developed in Python and using a custom
dataset. Performance appraisal was based on routing efficiency, computation
time, and scalability. The results show that ACO allows flexibility in routing
parameters while OR-Tools runs much faster with more consistency and requires
less input. This could help choose among routing strategies for scalable
real-time logistics systems.

</details>


### [62] [Type-Less yet Type-Aware Inductive Link Prediction with Pretrained Language Models](https://arxiv.org/abs/2509.26224)
*Alessandro De Bellis,Salvatore Bufi,Giovanni Servedio,Vito Walter Anelli,Tommaso Di Noia,Eugenio Di Sciascio*

Main category: cs.CL

TL;DR: TyleR is a type-aware inductive link prediction method that uses pre-trained language models to enrich node representations with implicit type signals, outperforming state-of-the-art methods when type annotations are scarce and graphs are sparse.


<details>
  <summary>Details</summary>
Motivation: Real-world knowledge graphs often lack complete or accurate type information, which is crucial for inductive link prediction where models must generalize to new entities without retraining. Existing type information is typically coarse-grained, sparse, and error-prone.

Method: TyleR leverages pre-trained language models to enrich node representations with implicit type signals, using a subgraph-based approach for inductive link prediction without requiring explicit type annotations.

Result: Experiments on standard benchmarks show TyleR outperforms state-of-the-art baselines in scenarios with scarce type annotations and sparse graph connectivity.

Conclusion: Pre-trained language models can effectively provide implicit type signals for inductive link prediction, enabling better performance even when explicit type information is limited or unavailable.

Abstract: Inductive link prediction is emerging as a key paradigm for real-world
knowledge graphs (KGs), where new entities frequently appear and models must
generalize to them without retraining. Predicting links in a KG faces the
challenge of guessing previously unseen entities by leveraging generalizable
node features such as subgraph structure, type annotations, and ontological
constraints. However, explicit type information is often lacking or incomplete.
Even when available, type information in most KGs is often coarse-grained,
sparse, and prone to errors due to human annotation. In this work, we explore
the potential of pre-trained language models (PLMs) to enrich node
representations with implicit type signals. We introduce TyleR, a Type-less yet
type-awaRe approach for subgraph-based inductive link prediction that leverages
PLMs for semantic enrichment. Experiments on standard benchmarks demonstrate
that TyleR outperforms state-of-the-art baselines in scenarios with scarce type
annotations and sparse graph connectivity. To ensure reproducibility, we share
our code at https://github.com/sisinflab/tyler .

</details>


### [63] [Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted Annealing](https://arxiv.org/abs/2509.26242)
*Yang Tang,Ruijie Liu,Yifan Wang,Shiyu Li,Xi Chen*

Main category: cs.CL

TL;DR: DBA is an efficient fine-tuning method that uses zero-learning-rate training on general data to obtain global gradients, then applies gradient boosting and dynamic training step correction during domain training with annealing learning, eliminating the need for general data in annealing and reducing GPU hours by 91%.


<details>
  <summary>Details</summary>
Motivation: To address challenges in vanilla fine-tuning methods that require intricate data mixture and repeated experiments for optimal generalization, and to streamline the training process.

Method: Dynamic Boosted Annealing (DBA) - obtains global gradient through zero-learning-rate training on general data, then uses it for gradient boosting and dynamic training step correction during domain training with annealing learning.

Result: Achieves average improvement of 5.8% in joint performance over vanilla fine-tuning across multiple tasks on several base models, and reduces GPU hours by 91.0% compared to vanilla method.

Conclusion: DBA establishes a fine-tuning pipeline that relies solely on domain data without collapse, eliminates repeated experiments from data mixture, and significantly improves efficiency while maintaining performance.

Abstract: Large language models (LLMs) fine-tuning shows excellent implications.
However, vanilla fine-tuning methods often require intricate data mixture and
repeated experiments for optimal generalization. To address these challenges
and streamline the training process, we propose an efficient and universal
solution, Dynamic Boosted Annealing (DBA). We obtain a global gradient through
zero-learning-rate training on general data, which is subsequently employed for
gradient boosting and dynamic training step correction during domain training.
In conjunction with annealing learning, we end up establishing a fine-tuning
pipeline that relies solely on domain data without collapse. By evaluating both
general and domain-specific performance across multiple tasks on several
popular base models, DBA achieves an average improvement of 5.8% in joint
performance over vanilla fine-tuning. Furthermore, since general data is no
longer involved in annealing, repeated experiments led by data mixture are also
eliminated. According to our tests, the DBA method can reduce GPU hours by
91.0% compared to the vanilla method.

</details>


### [64] [Optimizing Speech Language Models for Acoustic Consistency](https://arxiv.org/abs/2509.26276)
*Morteza Rohanian,Michael Krauthammer*

Main category: cs.CL

TL;DR: Speech language models with semantic initialization and planning losses achieve robust generation through self-supervised feature initialization, light alignment loss, and auxiliary training objectives.


<details>
  <summary>Details</summary>
Motivation: To develop speech language models that maintain robust and consistent generation across various acoustic factors while preserving semantic content.

Method: Initialize speech tokens with self-supervised features, apply light alignment loss, and train with thinning and auxiliary objectives targeting robustness and content planning. Three models trained: 0.7B speech-only, 1.0B speech-only, and 1.0B interleaved text-speech model.

Result: Speech-only models achieve highest consistency across speaker, gender, sentiment, room, and background factors, surpassing larger systems. Interleaving improves lexical/syntactic probes and semantic-acoustic alignment but reduces consistency. Initialization biases model toward content structure while trading off prosody detail.

Conclusion: LM-side design and training mix control the balance between acoustic stability and semantic grounding without changes to tokenizer or runtime architecture.

Abstract: We study speech language models that incorporate semantic initialization and
planning losses to achieve robust and consistent generation. Our approach
initializes speech tokens with self-supervised features, applies a light
alignment loss, and trains with thinning and auxiliary objectives that target
robustness and content planning. We train three models: a 0.7B speech-only
model, a 1.0B speech-only model, and a 1.0B interleaved model with both text
and speech. Acoustic studies show that the speech-only models achieve the
highest consistency across speaker, gender, sentiment, room, and background
factors, surpassing larger systems. Interleaving improves lexical and syntactic
probes and semantic--acoustic alignment but reduces consistency. Linear probes
show that our initialization biases the model toward content structure while
trading off prosody detail. These results show that LM-side design and training
mix control the balance between acoustic stability and semantic grounding
without changes to the tokenizer or runtime architecture. A demo and model
weights are available for exploration.

</details>


### [65] [QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented Dialogue Summarization](https://arxiv.org/abs/2509.26302)
*Mohamed Imed Eddine Ghebriout,Gaël Guibon,Ivan Lerner,Emmanuel Vincent*

Main category: cs.CL

TL;DR: \app is a framework for task-oriented dialogue summarization that generates multiple summaries and QA pairs using LLMs, selects the best ones based on task utility, and fine-tunes the best LLM on selected summaries, achieving competitive results without full supervision.


<details>
  <summary>Details</summary>
Motivation: Traditional dialogue summarization methods require costly human-written summaries and often lack task-specific focus, limiting their effectiveness in applications like medical tasks.

Method: Generate multiple summaries and task-oriented QA pairs using LLMs in zero-shot manner, evaluate summary quality by having LLMs answer task-related questions, select best candidate answers and most informative summary, then fine-tune best LLM on selected summaries.

Result: Achieves competitive results in various zero-shot settings, rivaling fully-supervised State-of-the-Art methods across multiple datasets.

Conclusion: \app framework provides an effective approach for task-oriented utility-based dialogue summarization without requiring costly human supervision, demonstrating strong performance comparable to supervised methods.

Abstract: Dialogue summarization aims to distill the core meaning of a conversation
into a concise text. This is crucial for reducing the complexity and noise
inherent in dialogue-heavy applications. While recent approaches typically
train language models to mimic human-written summaries, such supervision is
costly and often results in outputs that lack task-specific focus limiting
their effectiveness in downstream applications, such as medical tasks. In this
paper, we propose \app, a framework for task-oriented utility-based dialogue
summarization. \app starts by generating multiple summaries and task-oriented
question-answer pairs from a dialogue in a zero-shot manner using a pool of
large language models (LLMs). The quality of the generated summaries is
evaluated by having LLMs answer task-related questions before \textit{(i)}
selecting the best candidate answers and \textit{(ii)} identifying the most
informative summary based on these answers. Finally, we fine-tune the best LLM
on the selected summaries. When validated on multiple datasets, \app
demonstrates its effectiveness by achieving competitive results in various
zero-shot settings, rivaling fully-supervised State-of-the-Art (SotA) methods.

</details>


### [66] [Feedback Forensics: A Toolkit to Measure AI Personality](https://arxiv.org/abs/2509.26305)
*Arduin Findeis,Timo Kaufmann,Eyke Hüllermeier,Robert Mullins*

Main category: cs.CL

TL;DR: Feedback Forensics is an open-source toolkit that tracks AI personality changes using AI annotators, enabling analysis of personality traits in feedback datasets and models.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods struggle to measure subjective traits like model personality, and current approaches have limitations including sycophantic personality issues and overfitting to feedback-based leaderboards.

Method: Leverages AI annotators to investigate personality via Python API and browser app, analyzing personality traits in human feedback datasets and evaluating how much popular models exhibit such traits.

Result: The toolkit enables analysis of personality traits in popular human feedback datasets (Chatbot Arena, MultiPref, PRISM) and evaluates personality traits in popular AI models.

Conclusion: Feedback Forensics provides a public tool to explicitly evaluate model personality, addressing limitations of existing opaque evaluation approaches and enabling tracking of personality changes across AI models.

Abstract: Some traits making a "good" AI model are hard to describe upfront. For
example, should responses be more polite or more casual? Such traits are
sometimes summarized as model character or personality. Without a clear
objective, conventional benchmarks based on automatic validation struggle to
measure such traits. Evaluation methods using human feedback such as Chatbot
Arena have emerged as a popular alternative. These methods infer "better"
personality and other desirable traits implicitly by ranking multiple model
responses relative to each other. Recent issues with model releases highlight
limitations of these existing opaque evaluation approaches: a major model was
rolled back over sycophantic personality issues, models were observed
overfitting to such feedback-based leaderboards. Despite these known issues,
limited public tooling exists to explicitly evaluate model personality. We
introduce Feedback Forensics: an open-source toolkit to track AI personality
changes, both those encouraged by human (or AI) feedback, and those exhibited
across AI models trained and evaluated on such feedback. Leveraging AI
annotators, our toolkit enables investigating personality via Python API and
browser app. We demonstrate the toolkit's usefulness in two steps: (A) first we
analyse the personality traits encouraged in popular human feedback datasets
including Chatbot Arena, MultiPref and PRISM; and (B) then use our toolkit to
analyse how much popular models exhibit such traits. We release (1) our
Feedback Forensics toolkit alongside (2) a web app tracking AI personality in
popular models and feedback datasets as well as (3) the underlying annotation
data at https://github.com/rdnfn/feedback-forensics.

</details>


### [67] [One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient](https://arxiv.org/abs/2509.26313)
*Rui Ming,Haoyuan Wu,Shoubo Hu,Zhuolun He,Bei Yu*

Main category: cs.CL

TL;DR: OTR is a novel fine-tuning algorithm that combines SFT with policy gradient by treating token generation as single-step RL, using supervised data to create on-policy learning signals without full sentence generation overhead.


<details>
  <summary>Details</summary>
Motivation: SFT struggles with generalization compared to RL, which the authors attribute to SFT using fixed off-policy data while RL uses dynamic on-policy data from current policy.

Method: OTR reframes autoregressive learning as single-step RL trajectories, sampling multiple candidate tokens and using ground-truth tokens as reward signals to apply policy gradient updates.

Result: OTR consistently outperforms standard SFT across diverse benchmarks including mathematical reasoning, code generation, and general domain reasoning.

Conclusion: OTR establishes that on-policy data nature is critical for generalization, offering a practical alternative for fine-tuning LLMs with RL benefits but without RL's computational costs.

Abstract: Supervised fine-tuning (SFT) is the predominant method for adapting large
language models (LLMs), yet it often struggles with generalization compared to
reinforcement learning (RL). In this work, we posit that this performance
disparity stems not just from the loss function, but from a more fundamental
difference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes
on-policy data sampled from the current policy. Building on this hypothesis, we
introduce one-token rollout (OTR), a novel fine-tuning algorithm that guides
SFT with the policy gradient method. OTR reframes the autoregressive learning
process by treating each token generation as a single-step reinforcement
learning trajectory. At each step, it performs a Monte Carlo ``rollout'' by
sampling multiple candidate tokens from the current policy's distribution. The
ground-truth token from the supervised data is then used to provide a reward
signal to these samples. Guided by policy gradient, our algorithm repurposes
static, off-policy supervised data into a dynamic, on-policy signal at the
token level, capturing the generalization benefits of on-policy learning while
bypassing the costly overhead of full sentence generation. Through extensive
experiments on a diverse suite of challenging benchmarks spanning mathematical
reasoning, code generation, and general domain reasoning, we demonstrate that
OTR consistently outperforms standard SFT. Our findings establish OTR as a
powerful and practical alternative for fine-tuning LLMs and provide compelling
evidence that the on-policy nature of data is a critical driver of
generalization, offering a promising new direction for fine-tuning LLMs.

</details>


### [68] [Latent Thinking Optimization: Your Latent Reasoning Language Model Secretly Encodes Reward Signals in its Latent Thoughts](https://arxiv.org/abs/2509.26314)
*Hanwen Du,Yuxin Dong,Xia Ning*

Main category: cs.CL

TL;DR: This paper studies Huggin-3.5B's latent thinking processes, showing that correct and incorrect latent thoughts have distinguishable patterns. The authors propose Latent Thinking Optimization (LTO) using a latent classifier as reward model to optimize thinking processes, achieving significant improvements across diverse reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs' verbal thinking is computationally costly and prone to overthinking. While latent thinking architectures like Huggin-3.5B offer efficiency, they lack interpretability and supervision, raising concerns about correctness and reliability of latent reasoning processes.

Method: Systematic study of Huggin-3.5B's latent thinking patterns, development of latent classifier to predict answer correctness from latent thoughts, and proposal of Latent Thinking Optimization (LTO) - a probabilistic algorithm using latent classifier as Latent Reward Model (LRM) to optimize thinking processes.

Result: Latent thoughts leading to correct vs incorrect answers show highly distinguishable patterns. LRM effectively detects incorrect latent thinking patterns, and LTO significantly improves latent thinking processes across diverse reasoning tasks. The method generalizes across domains and can be applied to general LLMs.

Conclusion: Reward modeling and scaling test-time thinking with supervision can be performed directly in latent space, offering a general, efficient, and domain-agnostic approach to improve LLMs' thinking processes, contrasting with costly verbal thinking methods.

Abstract: Large Language Models (LLMs) excel at problem solving by generating chain of
thoughts in natural language, but such verbal thinking is computationally
costly and prone to overthinking. Recent work instead proposes a latent
thinking architecture Huggin-3.5B, which represents intermediate reasoning
steps as sequence of latent representations. However, latent thoughts lack
interpretability and are difficult to supervise, raising concerns about the
correctness and reliability of its latent thinking processes. In this paper, we
provide a systematic study of how Huggin-3.5B thinks in the latent space and
how external supervision signals can improve its latent thinking processes. We
show that latent thoughts leading to correct versus incorrect answers exhibit
highly distinguishable patterns, and that a latent classifier can reliably
predict answer correctness directly from latent thoughts. Leveraging these
insights, we propose Latent Thinking Optimization (LTO), a probabilistic
algorithm that employs the latent classifier as a Latent Reward Model (LRM) to
optimize the latent thinking processes. Extensive experiments across diverse
reasoning tasks demonstrate that LRM is highly effective in detecting incorrect
latent thinking patterns, and LTO can significantly improve the latent thinking
processes. Furthermore, we show that LRM can generalize across diverse domains,
and LTO can be seamlessly applied to general LLMs to improve their thinking
processes. In contrast to verbal thinking, our method demonstrates that reward
modeling and scaling test-time thinking with supervision can be performed
directly in the latent space, highlighting its potential as a general,
efficient, and domain-agnostic approach to improving the thinking processes of
LLMs.

</details>


### [69] [Fast-dLLM v2: Efficient Block-Diffusion LLM](https://arxiv.org/abs/2509.26328)
*Chengyue Wu,Hao Zhang,Shuchen Xue,Shizhe Diao,Yonggan Fu,Zhijian Liu,Pavlo Molchanov,Ping Luo,Song Han,Enze Xie*

Main category: cs.CL

TL;DR: Fast-dLLM v2 converts autoregressive LLMs into block diffusion models with only 1B token fine-tuning, achieving 2.5x speedup while maintaining accuracy through novel block diffusion and hierarchical caching mechanisms.


<details>
  <summary>Details</summary>
Motivation: Autoregressive LLMs suffer from sequential decoding inefficiency, limiting inference speed despite their strong performance.

Method: Proposes block diffusion mechanism with complementary attention mask for bidirectional context modeling, plus hierarchical caching (block-level and sub-block caches) for parallel generation.

Result: Achieves 500x reduction in training data (1B vs 580B tokens), 2.5x speedup over AR decoding while matching or surpassing AR baseline accuracy across benchmarks.

Conclusion: Fast-dLLM v2 enables practical deployment of fast and accurate LLMs, representing significant progress in diffusion language model efficiency.

Abstract: Autoregressive (AR) large language models (LLMs) have achieved remarkable
performance across a wide range of natural language tasks, yet their inherent
sequential decoding limits inference efficiency. In this work, we propose
Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that
efficiently adapts pretrained AR models into dLLMs for parallel text
generation, requiring only approximately 1B tokens of fine-tuning. This
represents a 500x reduction in training data compared to full-attention
diffusion LLMs such as Dream (580B tokens), while preserving the original
model's performance. Our approach introduces a novel training recipe that
combines a block diffusion mechanism with a complementary attention mask,
enabling blockwise bidirectional context modeling without sacrificing AR
training objectives. To further accelerate decoding, we design a hierarchical
caching mechanism: a block-level cache that stores historical context
representations across blocks, and a sub-block cache that enables efficient
parallel generation within partially decoded blocks. Coupled with our parallel
decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR
decoding without compromising generation quality. Extensive experiments across
diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR
baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs
- marking a significant step toward the practical deployment of fast and
accurate LLMs. Code and model will be publicly released.

</details>


### [70] [Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning](https://arxiv.org/abs/2509.26383)
*Jinyeop Song,Song Wang,Julian Shun,Yada Zhu*

Main category: cs.CL

TL;DR: KG-R1 is a reinforcement learning-based KG-RAG framework that uses a single agent to interact with knowledge graphs, achieving better efficiency and transferability than multi-module approaches.


<details>
  <summary>Details</summary>
Motivation: To address the high inference costs and KG-specific limitations of existing multi-module KG-RAG systems that use separate LLM modules for planning, reasoning, and responding.

Method: Uses a single agent that interacts with KGs as its environment, learning retrieval strategies through reinforcement learning and incorporating retrieved information into reasoning and generation in an end-to-end optimized process.

Result: In KGQA benchmarks, KG-R1 with Qwen-2.5-3B achieved higher answer accuracy with fewer tokens than prior multi-module methods using larger models, and demonstrated plug-and-play capability on new KGs without modification.

Conclusion: KG-R1 provides an efficient and transferable KG-RAG framework suitable for real-world deployment, reducing hallucinations while maintaining reasoning transparency.

Abstract: Knowledge-graph retrieval-augmented generation (KG-RAG) couples large
language models (LLMs) with structured, verifiable knowledge graphs (KGs) to
reduce hallucinations and expose reasoning traces. However, many KG-RAG systems
compose multiple LLM modules (e.g planning, reasoning, and responding),
inflating inference cost and binding behavior to a specific target KG. To
address this, we introduce KG-R1, an agentic KG retrieval-augmented generation
(KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single
agent that interacts with KGs as its environment, learning to retrieve at each
step and incorporating the retrieved information into its reasoning and
generation. The process is optimized through end-to-end RL. In controlled
experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our
method demonstrates both efficiency and transferability: Using Qwen-2.5-3B,
KG-R1 improves answer accuracy with fewer generation tokens than prior
multi-module workflow methods that use larger foundation or fine-tuned models.
Furthermore, KG-R1 enables plug and play: after training, it maintains strong
accuracy on new KGs without modification. These properties make KG-R1 a
promising KG-RAG framework for real-world deployment. Our code is publicly
available at https://github.com/Jinyeop3110/KG-R1.

</details>


### [71] [An Annotation Scheme for Factuality and its Application to Parliamentary Proceedings](https://arxiv.org/abs/2509.26406)
*Gili Goldin,Shira Wigderson,Ella Rabinovich,Shuly Wintner*

Main category: cs.CL

TL;DR: This paper presents a comprehensive factuality annotation scheme combining concepts from previous works, applied to Hebrew parliamentary discourse with manual annotation of 5,000 sentences and experiments on automatic prediction.


<details>
  <summary>Details</summary>
Motivation: Factuality is crucial for fact checking but is complex and relies on multiple linguistic signals. The paper aims to develop a comprehensive annotation scheme to better understand and analyze factuality in language.

Method: Developed a multi-faceted factuality annotation scheme combining previous concepts, manually annotated 5,000 Hebrew parliamentary sentences, measured inter-annotator agreement, and experimented with automatic prediction approaches.

Result: Created a detailed annotation scheme applicable to Hebrew, achieved measurable inter-annotator agreement, and demonstrated feasibility of automatically predicting some factuality features to scale annotation to larger corpora.

Conclusion: The proposed factuality annotation scheme is effective for Hebrew and potentially adaptable to other languages, with promising results for automated extension to large corpora through feature prediction.

Abstract: Factuality assesses the extent to which a language utterance relates to
real-world information; it determines whether utterances correspond to facts,
possibilities, or imaginary situations, and as such, it is instrumental for
fact checking. Factuality is a complex notion that relies on multiple
linguistic signals, and has been studied in various disciplines.
  We present a complex, multi-faceted annotation scheme of factuality that
combines concepts from a variety of previous works. We developed the scheme for
Hebrew, but we trust that it can be adapted to other languages. We also present
a set of almost 5,000 sentences in the domain of parliamentary discourse that
we manually annotated according to this scheme. We report on inter-annotator
agreement, and experiment with various approaches to automatically predict
(some features of) the scheme, in order to extend the annotation to a large
corpus.

</details>


### [72] [Automatic Fact-checking in English and Telugu](https://arxiv.org/abs/2509.26415)
*Ravi Kiran Chikkala,Tatiana Anikina,Natalia Skachkova,Ivan Vykopal,Rodrigo Agerri,Josef van Genabith*

Main category: cs.CL

TL;DR: This paper explores using large language models (LLMs) for automated fact-checking by classifying claim veracity and generating justifications in both English and Telugu languages.


<details>
  <summary>Details</summary>
Motivation: Manual fact-checking is time-consuming and resource-intensive, creating a need for automated solutions to combat false information globally.

Method: The researchers experimented with different LLM approaches for veracity classification and created a bilingual English-Telugu dataset to benchmark these methods.

Result: The study evaluated the effectiveness of various LLM approaches in classifying factual claims and generating justifications in both languages.

Conclusion: The work contributes a bilingual dataset and benchmarks LLM-based approaches for automated fact-checking, addressing the challenge of false information verification.

Abstract: False information poses a significant global challenge, and manually
verifying claims is a time-consuming and resource-intensive process. In this
research paper, we experiment with different approaches to investigate the
effectiveness of large language models (LLMs) in classifying factual claims by
their veracity and generating justifications in English and Telugu. The key
contributions of this work include the creation of a bilingual English-Telugu
dataset and the benchmarking of different veracity classification approaches
based on LLMs.

</details>


### [73] [Text-Based Approaches to Item Alignment to Content Standards in Large-Scale Reading & Writing Tests](https://arxiv.org/abs/2509.26431)
*Yanbin Fu,Hong Jiao,Tianyi Zhou,Robert W. Lissitz,Nan Zhang,Ming Li,Qingshu Xu,Sydney Peters*

Main category: cs.CL

TL;DR: Fine-tuned small language models (SLMs) outperform embedding-based supervised models for automated item alignment in standardized tests, with performance improving more from including more item text data than just increasing sample size.


<details>
  <summary>Details</summary>
Motivation: Human expert alignment of test items to content standards is subjective and time-consuming, creating need for automated methods to improve efficiency and objectivity.

Method: Fine-tuned small language models for automated item alignment at domain and skill levels using data from large-scale standardized reading/writing tests; compared with embedding-based supervised ML models; conducted semantic similarity analysis.

Result: SLMs consistently outperformed embedding-based models, especially for fine-grained skill alignment; including more item text data substantially improved performance; semantic analysis showed certain skills were too close, explaining misclassifications.

Conclusion: Fine-tuned SLMs are effective for automated item alignment, offering a viable alternative to human expert judgment with better performance than embedding-based approaches.

Abstract: Aligning test items to content standards is a critical step in test
development to collect validity evidence based on content. Item alignment has
typically been conducted by human experts. This judgmental process can be
subjective and time-consuming. This study investigated the performance of
fine-tuned small language models (SLMs) for automated item alignment using data
from a large-scale standardized reading and writing test for college
admissions. Different SLMs were trained for alignment at both domain and skill
levels respectively with 10 skills mapped to 4 content domains. The model
performance was evaluated in multiple criteria on two testing datasets. The
impact of types and sizes of the input data for training was investigated.
Results showed that including more item text data led to substantially better
model performance, surpassing the improvements induced by sample size increase
alone. For comparison, supervised machine learning models were trained using
the embeddings from the multilingual-E5-large-instruct model. The study results
showed that fine-tuned SLMs consistently outperformed the embedding-based
supervised machine learning models, particularly for the more fine-grained
skill alignment. To better understand model misclassifications, multiple
semantic similarity analysis including pairwise cosine similarity,
Kullback-Leibler divergence of embedding distributions, and two-dimension
projections of item embeddings were conducted. These analyses consistently
showed that certain skills in SAT and PSAT were semantically too close,
providing evidence for the observed misclassification.

</details>


### [74] [Adaptive Planning for Multi-Attribute Controllable Summarization with Monte Carlo Tree Search](https://arxiv.org/abs/2509.26435)
*Sangwon Ryu,Heejin Do,Yunsu Kim,Gary Geunbae Lee,Jungseul Ok*

Main category: cs.CL

TL;DR: PACO is a training-free framework for multi-attribute controllable summarization that uses adaptive planning with Monte Carlo Tree Search to optimize attribute control order, achieving superior performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current controllable summarization approaches struggle with interdependent attributes and require per-attribute fine-tuning, limiting flexibility and consistency in meeting multiple constraints simultaneously.

Method: Reframes the task as planning attribute control order using customized Monte Carlo Tree Search, where nodes are summaries and actions are single-attribute adjustments, enabling progressive refinement of only needed attributes.

Result: PACO achieves robust multi-attribute controllability, surpassing LLM-based self-planning models and fine-tuned baselines. With Llama-3.2-1B, it rivals the controllability of much larger Llama-3.3-70B models.

Conclusion: PACO provides an effective training-free solution for multi-attribute controllable summarization that adaptively discovers optimal control orders and outperforms existing approaches across diverse domains and models.

Abstract: Controllable summarization moves beyond generic outputs toward human-aligned
summaries guided by specified attributes. In practice, the interdependence
among attributes makes it challenging for language models to satisfy correlated
constraints consistently. Moreover, previous approaches often require
per-attribute fine-tuning, limiting flexibility across diverse summary
attributes. In this paper, we propose adaptive planning for multi-attribute
controllable summarization (PACO), a training-free framework that reframes the
task as planning the order of sequential attribute control with a customized
Monte Carlo Tree Search (MCTS). In PACO, nodes represent summaries, and actions
correspond to single-attribute adjustments, enabling progressive refinement of
only the attributes requiring further control. This strategy adaptively
discovers optimal control orders, ultimately producing summaries that
effectively meet all constraints. Extensive experiments across diverse domains
and models demonstrate that PACO achieves robust multi-attribute
controllability, surpassing both LLM-based self-planning models and fine-tuned
baselines. Remarkably, PACO with Llama-3.2-1B rivals the controllability of the
much larger Llama-3.3-70B baselines. With larger models, PACO achieves superior
control performance, outperforming all competitors.

</details>


### [75] [CreAgentive: An Agent Workflow Driven Multi-Category Creative Generation Engine](https://arxiv.org/abs/2509.26461)
*Yuyang Cheng,Linyue Cai,Changwei Peng,Yumiao Xu,Rongfang Bie,Yong Zhao*

Main category: cs.CL

TL;DR: CreAgentive is an agent workflow system for creative generation that addresses LLM limitations in genre diversity, output length, narrative coherence, and structural constructs through a Story Prototype representation and three-stage workflow.


<details>
  <summary>Details</summary>
Motivation: To overcome four key limitations of contemporary LLMs in creative writing: restricted genre diversity, insufficient output length, weak narrative coherence, and inability to enforce complex structural constructs.

Method: Uses a Story Prototype (genre-agnostic knowledge graph-based narrative representation) and three-stage agent workflow: Initialization (constructs narrative skeleton), Generation (multi-agent dialogues guided by objectives), and Writing (produces multi-genre text with advanced structures).

Result: Generates thousands of chapters with stable quality and low cost (<$1 per 100 chapters), outperforms strong baselines in 10 narrative indicators, and approaches human-authored novel quality across diverse genres.

Conclusion: CreAgentive effectively addresses key LLM limitations in creative generation through its Story Prototype and agent workflow architecture, achieving robust performance across genres with low computational cost.

Abstract: We present CreAgentive, an agent workflow driven multi-category creative
generation engine that addresses four key limitations of contemporary large
language models in writing stories, drama and other categories of creatives:
restricted genre diversity, insufficient output length, weak narrative
coherence, and inability to enforce complex structural constructs. At its core,
CreAgentive employs a Story Prototype, which is a genre-agnostic, knowledge
graph-based narrative representation that decouples story logic from stylistic
realization by encoding characters, events, and environments as semantic
triples. CreAgentive engages a three-stage agent workflow that comprises: an
Initialization Stage that constructs a user-specified narrative skeleton; a
Generation Stage in which long- and short-term objectives guide multi-agent
dialogues to instantiate the Story Prototype; a Writing Stage that leverages
this prototype to produce multi-genre text with advanced structures such as
retrospection and foreshadowing. This architecture reduces storage redundancy
and overcomes the typical bottlenecks of long-form generation. In extensive
experiments, CreAgentive generates thousands of chapters with stable quality
and low cost (less than $1 per 100 chapters) using a general-purpose backbone
model. To evaluate performance, we define a two-dimensional framework with 10
narrative indicators measuring both quality and length. Results show that
CreAgentive consistently outperforms strong baselines and achieves robust
performance across diverse genres, approaching the quality of human-authored
novels.

</details>


### [76] [Regression Language Models for Code](https://arxiv.org/abs/2509.26476)
*Yash Akhauri,Xingyou Song,Arissa Wongpanich,Bryan Lewandowski,Mohamed S. Abdelfattah*

Main category: cs.CL

TL;DR: A unified Regression Language Model (RLM) can predict multiple code execution metrics across different programming languages and hardware platforms without domain-specific feature engineering.


<details>
  <summary>Details</summary>
Motivation: Code-to-metric regression is challenging due to programming language complexity, and prior methods required heavy feature engineering. The authors aim to create a unified approach.

Method: Use a 300M parameter Regression Language Model (RLM) initialized from T5Gemma to predict numeric outcomes directly from code text across multiple languages and hardware.

Result: The RLM achieves >0.9 Spearman-rank on APPS competitive programming, >0.5 average Spearman-rank across 17 CodeNet languages, and highest 0.46 Kendall-Tau on NAS design spaces.

Conclusion: A single unified RLM can effectively predict diverse code execution metrics across multiple domains without specialized feature engineering.

Abstract: We study code-to-metric regression: predicting numeric outcomes of code
executions, a challenging task due to the open-ended nature of programming
languages. While prior methods have resorted to heavy and domain-specific
feature engineering, we show that a single unified Regression Language Model
(RLM) can simultaneously predict directly from text, (i) the memory footprint
of code across multiple high-level languages such as Python and C++, (ii) the
latency of Triton GPU kernels, and (iii) the accuracy and speed of trained
neural networks represented in ONNX. In particular, a relatively small 300M
parameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on
competitive programming submissions from APPS, and a single unified model
achieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet.
Furthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five
classic NAS design spaces previously dominated by graph neural networks, and
simultaneously predict architecture latencies on numerous hardware platforms.

</details>


### [77] [dParallel: Learnable Parallel Decoding for dLLMs](https://arxiv.org/abs/2509.26488)
*Zigeng Chen,Gongfan Fang,Xinyin Ma,Ruonan Yu,Xinchao Wang*

Main category: cs.CL

TL;DR: dParallel is a method that enables parallel decoding in diffusion large language models (dLLMs) by using certainty-forcing distillation to reduce decoding steps while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing dLLMs require nearly token-length decoding steps, limiting their parallel decoding potential and inference speed despite their theoretical advantages over autoregressive models.

Method: The approach identifies sequential certainty convergence as the bottleneck and introduces certainty-forcing distillation, which trains the model to achieve high certainty on masked tokens more rapidly and in parallel while following original sampling trajectories.

Result: Applied to LLaDA-8B-Instruct, dParallel reduces decoding steps from 256 to 30 on GSM8K (8.5x speedup) and from 256 to 24 on MBPP (10.5x speedup) without performance degradation.

Conclusion: dParallel effectively unlocks the inherent parallelism of dLLMs, enabling significant inference speedups while maintaining model performance across various benchmarks.

Abstract: Diffusion large language models (dLLMs) have recently drawn considerable
attention within the research community as a promising alternative to
autoregressive generation, offering parallel token prediction and lower
inference latency. Yet, their parallel decoding potential remains largely
underexplored, as existing open-source models still require nearly token-length
decoding steps to ensure performance. To address this, we introduce dParallel,
a simple and effective method that unlocks the inherent parallelism of dLLMs
for fast sampling. We identify that the key bottleneck to parallel decoding
arises from the sequential certainty convergence for masked tokens. Building on
this insight, we introduce the core of our approach: certainty-forcing
distillation, a novel training strategy that distills the model to follow its
original sampling trajectories while enforcing it to achieve high certainty on
masked tokens more rapidly and in parallel. Extensive experiments across
various benchmarks demonstrate that our method can dramatically reduce the
number of decoding steps while maintaining performance. When applied to the
LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on
GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP
benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup
while maintaining accuracy. Our code is available at
https://github.com/czg1225/dParallel

</details>


### [78] [VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications](https://arxiv.org/abs/2509.26490)
*Wei He,Yueqing Sun,Hongyan Hao,Xueyuan Hao,Zhikang Xia,Qi Gu,Chengcheng Han,Dengchang Zhao,Hui Su,Kefeng Zhang,Man Gao,Xi Su,Xiaodong Cai,Xunliang Cai,Yu Yang,Yunke Zhao*

Main category: cs.CL

TL;DR: VitaBench is a challenging benchmark for evaluating LLM-based agents on real-world interactive tasks across food delivery, in-store consumption, and online travel services, featuring 66 tools and complex multi-turn conversations.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to capture the complexity of real-world agent scenarios involving extensive information, diverse resources, and dynamic user interactions.

Method: Created a simulation environment with 66 tools from daily applications, eliminated domain-specific policies for flexible scenario composition, and developed a rubric-based sliding window evaluator for robust assessment.

Result: Even the most advanced models achieved only 30% success rate on cross-scenario tasks and less than 50% on single-scenario tasks, highlighting the benchmark's difficulty.

Conclusion: VitaBench serves as a valuable resource for advancing AI agent development in practical real-world applications, with code, dataset, and leaderboard publicly available.

Abstract: As LLM-based agents are increasingly deployed in real-life scenarios,
existing benchmarks fail to capture their inherent complexity of handling
extensive information, leveraging diverse resources, and managing dynamic user
interactions. To address this gap, we introduce VitaBench, a challenging
benchmark that evaluates agents on versatile interactive tasks grounded in
real-world settings. Drawing from daily applications in food delivery, in-store
consumption, and online travel services, VitaBench presents agents with the
most complex life-serving simulation environment to date, comprising 66 tools.
Through a framework that eliminates domain-specific policies, we enable
flexible composition of these scenarios and tools, yielding 100 cross-scenario
tasks (main results) and 300 single-scenario tasks. Each task is derived from
multiple real user requests and requires agents to reason across temporal and
spatial dimensions, utilize complex tool sets, proactively clarify ambiguous
instructions, and track shifting user intent throughout multi-turn
conversations. Moreover, we propose a rubric-based sliding window evaluator,
enabling robust assessment of diverse solution pathways in complex environments
and stochastic interactions. Our comprehensive evaluation reveals that even the
most advanced models achieve only 30% success rate on cross-scenario tasks, and
less than 50% success rate on others. Overall, we believe VitaBench will serve
as a valuable resource for advancing the development of AI agents in practical
real-world applications. The code, dataset, and leaderboard are available at
https://vitabench.github.io/

</details>


### [79] [BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs](https://arxiv.org/abs/2509.26514)
*Yue Wang,Ruotian Ma,Xingyu Chen,Zhengliang Shi,Wanshun Chen,Huang Liu,Jiadi Yao,Qu Yang,Qingxuan Jiang,Fanghua Ye,Juntao Li,Min Zhang,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CL

TL;DR: BatonVoice is a new framework that decouples instruction understanding from speech generation using an LLM as "conductor" to generate vocal feature plans, and a separate TTS model as "orchestra" to generate speech from these features.


<details>
  <summary>Details</summary>
Motivation: Existing approaches underutilize LLMs' linguistic intelligence and instruction-following capabilities for controllable TTS, limiting their ability to follow text instructions for speech synthesis.

Method: Proposes BatonVoice framework with LLM as conductor generating textual vocal feature plans (pitch, energy), and BatonTTS as orchestra generating speech from these features, inspired by operationalism to decouple instruction understanding from generation.

Result: BatonVoice achieves strong performance in controllable and emotional speech synthesis, outperforming open- and closed-source baselines, and enables remarkable zero-shot cross-lingual generalization to unseen languages.

Conclusion: Objectifying speech into textual vocal features more effectively unlocks LLMs' linguistic intelligence, demonstrating successful decoupling of instruction understanding from speech generation.

Abstract: The rise of Large Language Models (LLMs) is reshaping multimodel models, with
speech synthesis being a prominent application. However, existing approaches
often underutilize the linguistic intelligence of these models, typically
failing to leverage their powerful instruction-following capabilities. This
limitation hinders the model's ability to follow text instructions for
controllable Text-to-Speech~(TTS). To address this, we propose a new paradigm
inspired by ``operationalism'' that decouples instruction understanding from
speech generation. We introduce BatonVoice, a framework where an LLM acts as a
``conductor'', understanding user instructions and generating a textual
``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS
model, the ``orchestra'', then generates the speech from these features. To
realize this component, we develop BatonTTS, a TTS model trained specifically
for this task. Our experiments demonstrate that BatonVoice achieves strong
performance in controllable and emotional speech synthesis, outperforming
strong open- and closed-source baselines. Notably, our approach enables
remarkable zero-shot cross-lingual generalization, accurately applying feature
control abilities to languages unseen during post-training. This demonstrates
that objectifying speech into textual vocal features can more effectively
unlock the linguistic intelligence of LLMs.

</details>


### [80] [Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization](https://arxiv.org/abs/2509.26520)
*Yaoxiang Wang,Qingguo Hu,Yucheng Ding,Ruizhe Wang,Yeyun Gong,Jian Jiao,Yelong Shen,Peng Cheng,Jinsong Su*

Main category: cs.CL

TL;DR: Matryoshka MoE (M-MoE) is a training framework that enables elastic inference in Mixture-of-Experts models by learning a coarse-to-fine expert structure, allowing flexible activation of experts at inference time without performance degradation.


<details>
  <summary>Details</summary>
Motivation: Standard MoE training with Top-K routers prevents elastic inference - changing the number of activated experts at inference causes severe performance drops, limiting practical deployment flexibility.

Method: Systematically vary the number of activated experts during training to force the model to learn a meaningful ranking: top experts provide coarse capabilities while subsequent experts add finer details. Uses layer-wise randomization strategy.

Result: A single M-MoE model achieves remarkable elasticity, matching performance of multiple specialist models at various expert counts with only a fraction of the training cost. Enables computational budget optimization across layers.

Conclusion: M-MoE enables practical and adaptable deployment of large-scale MoE models by unlocking elastic inference capabilities and optimizing computational allocation across model layers.

Abstract: Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently
scaling large language models without a proportional increase in computational
cost. However, the standard training strategy of Top-K router prevents MoE
models from realizing their full potential for elastic inference. When the
number of activated experts is altered at inference time, these models exhibit
precipitous performance degradation. In this work, we introduce Matryoshka MoE
(M-MoE), a training framework that instills a coarse-to-fine structure directly
into the expert ensemble. By systematically varying the number of activated
experts during training, M-MoE compels the model to learn a meaningful ranking:
top-ranked experts collaborate to provide essential, coarse-grained
capabilities, while subsequent experts add progressively finer-grained detail.
We explore this principle at multiple granularities, identifying a layer-wise
randomization strategy as the most effective. Our experiments demonstrate that
a single M-MoE model achieves remarkable elasticity, with its performance at
various expert counts closely matching that of an entire suite of specialist
models, but at only a fraction of the total training cost. This flexibility not
only unlocks elastic inference but also enables optimizing performance by
allocating different computational budgets to different model layers. Our work
paves the way for more practical and adaptable deployments of large-scale MoE
models.

</details>


### [81] [OceanGym: A Benchmark Environment for Underwater Embodied Agents](https://arxiv.org/abs/2509.26536)
*Yida Xue,Mingjun Mao,Xiangyuan Ru,Yuqi Zhu,Baochang Ren,Shuofei Qiao,Mengru Wang,Shumin Deng,Xinyu An,Ningyu Zhang,Ying Chen,Huajun Chen*

Main category: cs.CL

TL;DR: OceanGym is the first comprehensive benchmark for ocean underwater embodied agents, featuring eight realistic task domains and a unified MLLM-driven framework to address extreme challenges like low visibility and dynamic currents.


<details>
  <summary>Details</summary>
Motivation: To advance AI in demanding underwater environments that present extreme perceptual and decision-making challenges, unlike terrestrial or aerial domains.

Method: Uses a unified agent framework driven by Multi-modal Large Language Models (MLLMs) that integrates perception, memory, and sequential decision-making, requiring agents to comprehend optical and sonar data and autonomously explore complex environments.

Result: Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting persistent difficulties in perception, planning, and adaptability in ocean underwater environments.

Conclusion: OceanGym establishes a testbed for developing robust embodied AI and transferring capabilities to real-world autonomous ocean underwater vehicles, marking a step toward intelligent agents capable of operating in Earth's last unexplored frontiers.

Abstract: We introduce OceanGym, the first comprehensive benchmark for ocean underwater
embodied agents, designed to advance AI in one of the most demanding real-world
environments. Unlike terrestrial or aerial domains, underwater settings present
extreme perceptual and decision-making challenges, including low visibility,
dynamic ocean currents, making effective agent deployment exceptionally
difficult. OceanGym encompasses eight realistic task domains and a unified
agent framework driven by Multi-modal Large Language Models (MLLMs), which
integrates perception, memory, and sequential decision-making. Agents are
required to comprehend optical and sonar data, autonomously explore complex
environments, and accomplish long-horizon objectives under these harsh
conditions. Extensive experiments reveal substantial gaps between
state-of-the-art MLLM-driven agents and human experts, highlighting the
persistent difficulty of perception, planning, and adaptability in ocean
underwater environments. By providing a high-fidelity, rigorously designed
platform, OceanGym establishes a testbed for developing robust embodied AI and
transferring these capabilities to real-world autonomous ocean underwater
vehicles, marking a decisive step toward intelligent agents capable of
operating in one of Earth's last unexplored frontiers. The code and data are
available at https://github.com/OceanGPT/OceanGym.

</details>


### [82] [The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models](https://arxiv.org/abs/2509.26543)
*Lina Conti,Dennis Fucci,Marco Gaido,Matteo Negri,Guillaume Wisniewski,Luisa Bentivogli*

Main category: cs.CL

TL;DR: Proposes first method for contrastive explanations in speech-to-text models by analyzing input spectrogram influence on output choices, demonstrated through gender assignment case study.


<details>
  <summary>Details</summary>
Motivation: Contrastive explanations are more informative than standard explanations but obtaining them for speech-to-text generative models remains challenging.

Method: Uses feature attribution techniques to analyze how parts of input spectrogram influence choice between alternative outputs in S2T models.

Result: Method accurately identifies audio features that drive selection of one gender over another in speech translation gender assignment.

Conclusion: Extends scope of contrastive explanations to S2T, providing foundation for better understanding speech-to-text models.

Abstract: Contrastive explanations, which indicate why an AI system produced one output
(the target) instead of another (the foil), are widely regarded in explainable
AI as more informative and interpretable than standard explanations. However,
obtaining such explanations for speech-to-text (S2T) generative models remains
an open challenge. Drawing from feature attribution techniques, we propose the
first method to obtain contrastive explanations in S2T by analyzing how parts
of the input spectrogram influence the choice between alternative outputs.
Through a case study on gender assignment in speech translation, we show that
our method accurately identifies the audio features that drive the selection of
one gender over another. By extending the scope of contrastive explanations to
S2T, our work provides a foundation for better understanding S2T models.

</details>


### [83] [Towards Reliable Benchmarking: A Contamination Free, Controllable Evaluation Framework for Multi-step LLM Function Calling](https://arxiv.org/abs/2509.26553)
*Seiji Maekawa,Jackson Hassell,Pouya Pezeshkpour,Tom Mitchell,Estevam Hruschka*

Main category: cs.CL

TL;DR: FuncBenchGen is a contamination-free framework for evaluating tool-augmented language models by generating synthetic multi-step tool-use tasks based on function-dependency DAGs, with precise control over task difficulty.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for tool-augmented language models lack control over key factors like function accessibility, task complexity, and input size, and are vulnerable to data contamination.

Method: FuncBenchGen casts tool use as traversal over hidden function-dependency DAGs where nodes are function calls and edges represent dependencies. It generates tasks where models must compose correct call sequences to compute target variables from given function schemas and initial values.

Result: Evaluation of seven LLMs shows reasoning-optimized models outperform general-purpose models, with GPT-5 performing best. Performance declines with increased dependency depth, and connected irrelevant functions are particularly challenging. Models often make valid calls but propagate incorrect values, revealing brittle state tracking.

Conclusion: A simple mitigation strategy of explicitly restating prior variable values at each step yields substantial performance gains across models, improving GPT-5's success rate from 62.5% to 81.3%.

Abstract: As language models gain access to external tools via structured function
calls, they become increasingly more capable of solving complex, multi-step
tasks. However, existing benchmarks for tool-augmented language models (TaLMs)
provide insufficient control over factors such as the number of functions
accessible, task complexity, and input size, and remain vulnerable to data
contamination. We present FuncBenchGen, a unified, contamination-free framework
that evaluates TaLMs by generating synthetic multi-step tool-use tasks. The key
idea is to cast tool use as traversal over a hidden function-dependency DAG
where nodes are function calls and an edge between nodes represents one
function consuming the output of another. Given a set of external function
schemas, initial variable values, and a target variable, models must compose
the correct call sequence to compute the target variable. FuncBenchGen allows
users to precisely control task difficulty (e.g., graph size, dependency depth,
and distractor functions) while avoiding data leakage. We apply our
FuncBenchGen framework to evaluate seven LLMs on tool use tasks of varying
difficulty. Reasoning-optimized models consistently outperform general-purpose
models with GPT-5 significantly outperforming other models. Performance
declines sharply as dependency depth increases. Furthermore, connected
irrelevant functions prove especially difficult to handle. We find that strong
models often make syntactically valid function calls but propagate incorrect or
stale argument values across steps, revealing brittle state tracking by LLMs in
multi-turn tool use. Motivated by this observation, we introduce a simple
mitigation strategy that explicitly restates prior variable values to the agent
at each step. Surprisingly, this lightweight change yields substantial gains
across models. e.g., yielding a success rate improvement from 62.5% to 81.3%
for GPT-5.

</details>


### [84] [Generating Difficult-to-Translate Texts](https://arxiv.org/abs/2509.26592)
*Vilém Zouhar,Wenda Xu,Parker Riley,Juraj Juraska,Mara Finkelstein,Markus Freitag,Dan Deutsch*

Main category: cs.CL

TL;DR: MT-breaker uses LLMs to iteratively refine source texts to create challenging machine translation test cases that expose model weaknesses while maintaining naturalness.


<details>
  <summary>Details</summary>
Motivation: Real-world MT benchmarks become obsolete quickly as models improve, limiting their ability to distinguish model quality and reveal weaknesses. Current methods for creating difficult test cases are insufficient.

Method: A large language model iteratively refines source text by querying a target MT model to guide generation of difficult examples, mimicking human expert probing for failures.

Result: Generates more challenging examples for target MT models while preserving natural text diversity. Difficulty transfers to other models and languages.

Conclusion: MT-breaker provides an effective approach for creating challenging, natural MT benchmarks that better expose model weaknesses and enable better model comparison.

Abstract: Machine translation benchmarks sourced from the real world are quickly
obsoleted, due to most examples being easy for state-of-the-art translation
models. This limits the benchmark's ability to distinguish which model is
better or to reveal models' weaknesses. Current methods for creating difficult
test cases, such as subsampling or from-scratch synthesis, either fall short of
identifying difficult examples or suffer from a lack of diversity and
naturalness. Inspired by the iterative process of human experts probing for
model failures, we propose MT-breaker, a method where a large language model
iteratively refines a source text to increase its translation difficulty. The
LLM iteratively queries a target machine translation model to guide its
generation of difficult examples. Our approach generates examples that are more
challenging for the target MT model while preserving the diversity of natural
texts. While the examples are tailored to a particular machine translation
model during the generation, the difficulty also transfers to other models and
languages.

</details>


### [85] [Deconstructing Self-Bias in LLM-generated Translation Benchmarks](https://arxiv.org/abs/2509.26600)
*Wenda Xu,Sweta Agrawal,Vilém Zouhar,Markus Freitag,Daniel Deutsch*

Main category: cs.CL

TL;DR: LLM-generated benchmarks for translation tasks exhibit systematic self-bias, favoring the model that created them, particularly in low-resource language to English translation.


<details>
  <summary>Details</summary>
Motivation: As LLMs saturate existing benchmarks, automated benchmark creation using LLMs offers a scalable alternative to human curation, but these generated benchmarks may have critical flaws.

Method: Analyzed self-bias in LLM-generated benchmarks for translation tasks by examining bias sources (test data generation and evaluation method), influence of generation capabilities, and impact of source text diversity.

Result: Found systematic self-bias favoring the creator model, amplified by combination of generated test data and LLM evaluation. Bias more pronounced in into-English translation and influenced by source language generation capabilities. Low source text diversity contributes to bias.

Conclusion: LLM-generated benchmarks have inherent self-bias issues. Improving source text diversity can help mitigate some bias, but automated benchmarking requires careful consideration of these systematic biases.

Abstract: As large language models (LLMs) begin to saturate existing benchmarks,
automated benchmark creation using LLMs (LLM as a benchmark) has emerged as a
scalable alternative to slow and costly human curation. While these generated
test sets have to potential to cheaply rank models, we demonstrate a critical
flaw. LLM generated benchmarks systematically favor the model that created the
benchmark, they exhibit self bias on low resource languages to English
translation tasks. We show three key findings on automatic benchmarking of LLMs
for translation: First, this bias originates from two sources: the generated
test data (LLM as a testset) and the evaluation method (LLM as an evaluator),
with their combination amplifying the effect. Second, self bias in LLM as a
benchmark is heavily influenced by the model's generation capabilities in the
source language. For instance, we observe more pronounced bias in into English
translation, where the model's generation system is developed, than in out of
English translation tasks. Third, we observe that low diversity in source text
is one attribution to self bias. Our results suggest that improving the
diversity of these generated source texts can mitigate some of the observed
self bias.

</details>


### [86] [MENLO: From Preferences to Proficiency - Evaluating and Modeling Native-like Quality Across 47 Languages](https://arxiv.org/abs/2509.26601)
*Chenxi Whitehouse,Sebastian Ruder,Tony Lin,Oksana Kurylo,Haruka Takagi,Janice Lam,Nicolò Busetto,Denise Diaz*

Main category: cs.CL

TL;DR: MENLO is a framework for evaluating native-like quality of LLM responses across 47 languages using audience design mechanisms, with human-annotated dataset showing LLM judges underperform humans but can be improved through fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring native-like quality in LLM responses across multiple languages by creating a systematic evaluation framework.

Method: Introduced MENLO framework using audience design mechanisms, created 6,423 human-annotated prompt-response pairs, evaluated zero-shot LLM judges with pairwise evaluation and structured rubrics, and improved performance through fine-tuning with reinforcement learning, reward shaping, and multi-task learning.

Result: LLM judges significantly benefit from pairwise evaluation and structured rubrics but still underperform human annotators. Substantial improvements achieved through fine-tuning approaches. RL-trained judges can serve as generative reward models to enhance multilingual proficiency.

Conclusion: MENLO provides promising directions for scalable multilingual evaluation and preference alignment, with released dataset and framework supporting further research in multilingual LLM evaluation.

Abstract: Ensuring native-like quality of large language model (LLM) responses across
many languages is challenging. To address this, we introduce MENLO, a framework
that operationalizes the evaluation of native-like response quality based on
audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423
human-annotated prompt-response preference pairs covering four quality
dimensions with high inter-annotator agreement in 47 language varieties. Our
evaluation reveals that zero-shot LLM judges benefit significantly from
pairwise evaluation and our structured annotation rubrics, yet they still
underperform human annotators on our dataset. We demonstrate substantial
improvements through fine-tuning with reinforcement learning, reward shaping,
and multi-task learning approaches. Additionally, we show that RL-trained
judges can serve as generative reward models to enhance LLMs' multilingual
proficiency, though discrepancies with human judgment remain. Our findings
suggest promising directions for scalable multilingual evaluation and
preference alignment. We release our dataset and evaluation framework to
support further research in multilingual LLM evaluation.

</details>


### [87] [DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively](https://arxiv.org/abs/2509.26603)
*Yixuan Weng,Minjun Zhu,Qiujie Xie,Qiyao Sun,Zhen Lin,Sifan Liu,Yue Zhang*

Main category: cs.CL

TL;DR: DeepScientist is an AI system that conducts goal-oriented scientific discovery over month-long timelines using Bayesian Optimization and a hierarchical evaluation process, achieving discoveries that surpass human-designed state-of-the-art methods on three AI tasks.


<details>
  <summary>Details</summary>
Motivation: Previous AI Scientist systems lack focus to produce scientifically valuable contributions addressing pressing human-defined challenges, so DeepScientist was developed to overcome this limitation.

Method: Formalizes discovery as Bayesian Optimization problem with hierarchical evaluation process (hypothesize, verify, analyze), using cumulative Findings Memory to balance exploration and exploitation, and selectively promoting promising findings through validation levels.

Result: Consumed over 20,000 GPU hours, generated ~5,000 unique scientific ideas, experimentally validated ~1,100 findings, and surpassed human-designed SOTA methods on three frontier AI tasks by 183.7%, 1.9%, and 7.9%.

Conclusion: This work provides first large-scale evidence of AI achieving discoveries that progressively surpass human SOTA on scientific tasks, genuinely pushing the frontier of scientific discovery. All experimental logs and system code will be open-sourced.

Abstract: While previous AI Scientist systems can generate novel findings, they often
lack the focus to produce scientifically valuable contributions that address
pressing human-defined challenges. We introduce DeepScientist, a system
designed to overcome this by conducting goal-oriented, fully autonomous
scientific discovery over month-long timelines. It formalizes discovery as a
Bayesian Optimization problem, operationalized through a hierarchical
evaluation process consisting of "hypothesize, verify, and analyze". Leveraging
a cumulative Findings Memory, this loop intelligently balances the exploration
of novel hypotheses with exploitation, selectively promoting the most promising
findings to higher-fidelity levels of validation. Consuming over 20,000 GPU
hours, the system generated about 5,000 unique scientific ideas and
experimentally validated approximately 1100 of them, ultimately surpassing
human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by
183.7\%, 1.9\%, and 7.9\%. This work provides the first large-scale evidence of
an AI achieving discoveries that progressively surpass human SOTA on scientific
tasks, producing valuable findings that genuinely push the frontier of
scientific discovery. To facilitate further research into this process, we will
open-source all experimental logs and system code at
https://github.com/ResearAI/DeepScientist/.

</details>


### [88] [Searching for Difficult-to-Translate Test Examples at Scale](https://arxiv.org/abs/2509.26619)
*Wenda Xu,Vilém Zouhar,Parker Riley,Mara Finkelstein,Markus Freitag,Daniel Deutsch*

Main category: cs.CL

TL;DR: The paper proposes using multi-armed bandit strategies to efficiently identify the most difficult topics for NLP model testing, where each topic is treated as an arm and difficulty evaluation is stochastic.


<details>
  <summary>Details</summary>
Motivation: NLP models need challenging test data, but finding difficult topics from tens of thousands of potential topics through brute-force evaluation is computationally infeasible due to the stochastic nature of topic difficulty.

Method: Formalize the task as a multi-armed bandit problem where each topic is an arm, pulling an arm involves drawing and evaluating a single example to measure difficulty, and use bandit strategies to efficiently identify the most difficult topics within fixed computational budget.

Result: Various bandit strategies vastly outperform baseline methods like brute-force search in finding the most challenging topics, as demonstrated in machine translation tasks.

Conclusion: Multi-armed bandit approaches provide an efficient computational framework for identifying difficult topics for NLP model testing, overcoming the limitations of brute-force methods.

Abstract: NLP models require test data that are sufficiently challenging. The
difficulty of an example is linked to the topic it originates from (''seed
topic''). The relationship between the topic and the difficulty of its
instances is stochastic in nature: an example about a difficult topic can
happen to be easy, and vice versa. At the scale of the Internet, there are tens
of thousands of potential topics, and finding the most difficult one by drawing
and evaluating a large number of examples across all topics is computationally
infeasible. We formalize this task and treat it as a multi-armed bandit
problem. In this framework, each topic is an ''arm,'' and pulling an arm (at a
cost) involves drawing a single example, evaluating it, and measuring its
difficulty. The goal is to efficiently identify the most difficult topics
within a fixed computational budget. We illustrate the bandit problem setup of
finding difficult examples for the task of machine translation. We find that
various bandit strategies vastly outperform baseline methods like brute-force
searching the most challenging topics.

</details>


### [89] [Scaling Spoken Language Models with Syllabic Speech Tokenization](https://arxiv.org/abs/2509.26634)
*Nicholas Lee,Cheol Jun Cho,Alan W Black,Gopala K. Anumanchipalli*

Main category: cs.CL

TL;DR: Syllabic tokenization for spoken language modeling achieves comparable or better performance than high-frame-rate tokens while significantly reducing computational costs (2x training time reduction, 5x FLOPs reduction).


<details>
  <summary>Details</summary>
Motivation: Traditional SLMs use high-frame-rate tokens that are computationally expensive due to quadratic attention scaling. Syllabic tokenization offers interpretability and significant compression (4-5 Hz) but its value for spoken language modeling hasn't been fully explored.

Method: First systematic study of syllabic tokenization for spoken language modeling, evaluating models on SLU benchmarks while varying training data scale.

Result: Syllabic tokens match or surpass previous high-frame-rate tokens while cutting training and inference costs significantly - more than 2x reduction in training time and 5x reduction in FLOPs.

Conclusion: Syllable-level language modeling is a promising path to efficient long-context spoken language models.

Abstract: Spoken language models (SLMs) typically discretize speech into
high-frame-rate tokens extracted from SSL speech models. As the most successful
LMs are based on the Transformer architecture, processing these long token
streams with self-attention is expensive, as attention scales quadratically
with sequence length. A recent SSL work introduces acoustic tokenization of
speech at the syllable level, which is more interpretable and potentially more
scalable with significant compression in token lengths (4-5 Hz). Yet, their
value for spoken language modeling is not yet fully explored. We present the
first systematic study of syllabic tokenization for spoken language modeling,
evaluating models on a suite of SLU benchmarks while varying training data
scale. Syllabic tokens can match or surpass the previous high-frame rate tokens
while significantly cutting training and inference costs, achieving more than a
2x reduction in training time and a 5x reduction in FLOPs. Our findings
highlight syllable-level language modeling as a promising path to efficient
long-context spoken language models.

</details>


### [90] [Convergence and Divergence of Language Models under Different Random Seeds](https://arxiv.org/abs/2509.26643)
*Finlay Fehlauer,Kyle Mahowald,Tiago Pimentel*

Main category: cs.CL

TL;DR: This paper investigates language model convergence patterns across different random seeds, identifying a four-phase convergence process and showing that larger models reconverge faster while smaller models may never fully reconverge.


<details>
  <summary>Details</summary>
Motivation: To understand the convergence behavior of language models trained under different random seeds and identify factors that influence the stability of learned distributions.

Method: Measure convergence as expected per-token KL divergence across seeds, analyze convergence patterns by model size and training checkpoint, and examine convergence across different token frequencies and part-of-speech tags.

Result: Identified a four-phase convergence pattern: initial uniform phase, sharp-convergence phase, sharp-divergence phase, and slow-reconvergence phase. Larger models reconverge faster in later stages while smaller models never fully reconverge. Frequent tokens and function words converge faster and more reliably than infrequent tokens and content words.

Conclusion: Model size is necessary for learning stable distributions, and convergence is uneven across linguistic categories, with frequent tokens and function words showing more reliable convergence patterns.

Abstract: In this paper, we investigate the convergence of language models (LMs)
trained under different random seeds, measuring convergence as the expected
per-token Kullback--Leibler (KL) divergence across seeds. By comparing LM
convergence as a function of model size and training checkpoint, we identify a
four-phase convergence pattern: (i) an initial uniform phase, (ii) a
sharp-convergence phase, (iii) a sharp-divergence phase, and (iv) a
slow-reconvergence phase. Further, we observe that larger models reconverge
faster in later training stages, while smaller models never actually
reconverge; these results suggest that a certain model size may be necessary to
learn stable distributions. Restricting our analysis to specific token
frequencies or part-of-speech (PoS) tags further reveals that convergence is
uneven across linguistic categories: frequent tokens and function words
converge faster and more reliably than their counterparts (infrequent tokens
and content words). Overall, our findings highlight factors that influence the
stability of the learned distributions in model training.

</details>


### [91] [Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking](https://arxiv.org/abs/2505.23495)
*Liangliang Zhang,Zhuorui Jiang,Hongliang Chi,Haoyang Chen,Mohammed Elkoumy,Fali Wang,Qiong Wu,Zhengyi Zhou,Shirui Pan,Suhang Wang,Yao Ma*

Main category: cs.CL

TL;DR: KGQAGen is an LLM-in-the-loop framework that addresses quality issues in existing KGQA benchmarks by generating challenging and verifiable QA instances through structured knowledge grounding, LLM-guided generation, and symbolic verification.


<details>
  <summary>Details</summary>
Motivation: Popular KGQA datasets like WebQSP and CWQ suffer from critical quality issues including inaccurate ground-truth annotations, ambiguous questions, and outdated knowledge, with manual audit showing only 57% factual correctness rate.

Method: KGQAGen combines structured knowledge grounding, LLM-guided generation, and symbolic verification to systematically produce high-quality QA instances. The framework constructs KGQAGen-10k, a ten-thousand scale benchmark grounded in Wikidata.

Result: Experimental evaluation shows that even state-of-the-art KG-RAG models struggle on the KGQAGen-10k benchmark, demonstrating its ability to expose limitations of existing models.

Conclusion: The findings advocate for more rigorous benchmark construction and position KGQAGen as a scalable framework for advancing KGQA evaluation by addressing quality issues in existing datasets.

Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality
benchmarks to evaluate complex multi-hop reasoning. However, despite their
widespread use, popular datasets such as WebQSP and CWQ suffer from critical
quality issues, including inaccurate or incomplete ground-truth annotations,
poorly constructed questions that are ambiguous, trivial, or unanswerable, and
outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA
datasets, including WebQSP and CWQ, we find that the average factual
correctness rate is only 57 %. To address these issues, we introduce KGQAGen,
an LLM-in-the-loop framework that systematically resolves these pitfalls.
KGQAGen combines structured knowledge grounding, LLM-guided generation, and
symbolic verification to produce challenging and verifiable QA instances. Using
KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in
Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results
demonstrate that even state-of-the-art systems struggle on this benchmark,
highlighting its ability to expose limitations of existing models. Our findings
advocate for more rigorous benchmark construction and position KGQAGen as a
scalable framework for advancing KGQA evaluation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [92] [Fast Energy-Theft Attack on Frequency-Varying Wireless Power without Additional Sensors](https://arxiv.org/abs/2509.25394)
*Hui Wang,Nima Tashakor,Xiaoyang Tian,Hans D. Schotten,Stefan M. Goetz*

Main category: cs.CR

TL;DR: The paper presents an optimized attack method that can steal 65% of power from encrypted wireless charging systems within 0.2 ms, using only the main receiving coil without additional sensors.


<details>
  <summary>Details</summary>
Motivation: With the growing popularity of wireless charging, energy access protection and cybersecurity in public places are becoming increasingly important. Current frequency-based encryption methods have been proven unreliable.

Method: The authors optimized the attack system by eliminating time-consuming maximum receiver current regulation and using only the main receiving coil to detect magnetic fields, making the hardware simpler and faster.

Result: The attack successfully intrudes and steals energy within 0.2 ms, achieving 65% power theft from encrypted wireless power systems. Simulation and experimental results confirm the fast response speed.

Conclusion: The improved attack method leaves less room for hardening encryption systems, demonstrating that current energy access protection methods are vulnerable and need significant attention and improvement.

Abstract: With the popularity of wireless charging, energy access protection and
cybersecurity are gaining importance, especially in public places. Currently,
the most common energy encryption method uses frequency and associated
impedance variation. However, we have proven that this method is not reliable,
since a hacker can detect the changing frequency and adjust the compensation.
However, the previously presented system needed time to follow the updated
frequency, while encryption systems may vary the frequency faster to avoid
energy theft. Furthermore, the previous system required an additional sensor
coil. To solve these problems, we optimized the attack and the associated
system, which can intrude and steal energy within 0.2 ms. The key is the
elimination of the time-consuming maximum receiver current regulation. Also, we
use the main receiving coil rather than any additional sensor antenna to detect
the magnetic field. Thus, the new hardware is even simpler. A simulation model
and experimental results demonstrate the fast response speed of the attack on
encrypted wireless power and steal 65% of the power. Overall, the applicability
of the attack is highly improved and leaves less room for hardening the
encryption. The results demonstrate that energy access protection needs to be
given great attention.

</details>


### [93] [Optimal Threshold Signatures in Bitcoin](https://arxiv.org/abs/2509.25408)
*Korok Ray,Sindura Saraswathi*

Main category: cs.CR

TL;DR: This paper analyzes optimal threshold signature schemes for cryptocurrency security, balancing security against malicious attacks with usability to prevent users from being locked out of their funds.


<details>
  <summary>Details</summary>
Motivation: To address the security-usability tradeoff in cryptocurrency threshold signatures, where higher thresholds provide more security but risk locking users out, and malicious attackers can obtain signatures with some probability.

Method: Formulate threshold signature design as an optimization problem, model dynamic threshold schemes where signature acquisition probability decays over time, and analyze how security/usability improvements affect optimal thresholds.

Result: Dynamic threshold signature schemes are optimal, and increasing security or usability allows for higher thresholds and longer time locks while maintaining balance between security and accessibility.

Conclusion: The optimal threshold balances security against malicious attacks with the risk of users being locked out, with dynamic schemes and security/usability improvements enabling higher thresholds and better protection.

Abstract: We formulate the design of a threshold signature scheme as made possible on
cryptocurrency protocols like Bitcoin. The funds are secured by an m-of-n
threshold signature, where at least m signatures are needed to unlock the
funds. A user designs this scheme knowing that a malicious attacker can also
obtain the signatures with some probability. Higher thresholds offer more
security, but also risk locking the user out of his own funds. The optimal
threshold balances these twin effects. Interventions like increasing the
security or usability of the signatures allow for higher thresholds. We model
dynamic threshold signature schemes, where the probability of a user or
attacker obtaining signatures decays with time. A dynamic threshold signature
scheme is optimal, and increasing security or usability allows for higher
thresholds and longer time locks.

</details>


### [94] [Characterizing Event-themed Malicious Web Campaigns: A Case Study on War-themed Websites](https://arxiv.org/abs/2509.25410)
*Maraz Mia,Mir Mehedi A. Pritom,Tariqul Islam,Shouhuai Xu*

Main category: cs.CR

TL;DR: This paper analyzes event-themed malicious websites, particularly war-themed ones, using explainable unsupervised clustering to understand attack patterns and guide early defense design.


<details>
  <summary>Details</summary>
Motivation: Cybercriminals exploit global events to create fraudulent websites that manipulate users through themes like fundraising, aid provision, and news updates, requiring better characterization of these campaigns.

Method: The study uses explainable unsupervised clustering methods to analyze event-themed malicious websites, with a specific case study on war-themed websites.

Result: Researchers found that attackers tailor their attacks by exploiting unique event aspects such as fundraising, providing aid, collecting supplies, or seeking updated news.

Conclusion: The insights from explainable unsupervised clustering can guide the design of effective early defenses against various event-themed malicious web campaigns.

Abstract: Cybercrimes such as online scams and fraud have become prevalent.
Cybercriminals often abuse various global or regional events as themes of their
fraudulent activities to breach user trust and attain a higher attack success
rate. These attacks attempt to manipulate and deceive innocent people into
interacting with meticulously crafted websites with malicious payloads,
phishing, or fraudulent transactions. To deepen our understanding of the
problem, this paper investigates how to characterize event-themed malicious
website-based campaigns, with a case study on war-themed websites. We find that
attackers tailor their attacks by exploiting the unique aspects of events, as
evidenced by activities such as fundraising, providing aid, collecting
essential supplies, or seeking updated news. We use explainable unsupervised
clustering methods to draw further insights, which could guide the design of
effective early defenses against various event-themed malicious web campaigns.

</details>


### [95] [Finding Phones Fast: Low-Latency and Scalable Monitoring of Cellular Communications in Sensitive Areas](https://arxiv.org/abs/2509.25430)
*Martin Kotuliak,Simon Erni,Jakub Polák,Marc Roeschlin,Richard Baker,Ivan Martinovic,Srdjan Čapkun*

Main category: cs.CR

TL;DR: LTag is a low-latency cellular monitoring system that detects unauthorized devices in sensitive areas before data transmission, achieving detection within 2.3ms.


<details>
  <summary>Details</summary>
Motivation: Address the gap in low-latency monitoring of cellular transmissions to prevent security breaches from unauthorized devices in sensitive areas.

Method: Uses distributed network of downlink and uplink sniffers to measure protocol info and signal characteristics, aggregating data before connection establishment.

Result: Successfully deployed for geofencing, detecting signal origins inside/outside areas within 2.3ms of initial base station message.

Conclusion: LTag provides effective, scalable real-time cellular monitoring for security applications without operator cooperation.

Abstract: The widespread availability of cellular devices introduces new threat vectors
that allow users or attackers to bypass security policies and physical barriers
and bring unauthorized devices into sensitive areas. These threats can arise
from user non-compliance or deliberate actions aimed at data
exfiltration/infiltration via hidden devices, drones, etc. We identify a
critical gap in this context: the absence of low-latency systems for
high-quality and instantaneous monitoring of cellular transmissions. Such
low-latency systems are crucial to allow for timely detection, decision (e.g.,
geofencing or localization), and disruption of unauthorized communication in
sensitive areas. Operator-based monitoring systems, built for purposes such as
people counting or tracking, lack real-time capability, require cooperation
across multiple operators, and thus are hard to deploy. Operator-independent
monitoring approaches proposed in the literature either lack low-latency
capabilities or do not scale.
  We propose LTag, the first low-latency, operator-independent and scalable
system designed to monitor cellular connections across all operators prior to
any user data transmission. LTag consists of several downlink sniffers and a
distributed network of uplink sniffers that measure both downlink protocol
information and uplink signal characteristics at multiple locations to gain a
detailed spatial image of uplink signals. LTag aggregates the recorded
information, processes it, and provides a decision about the connection all
prior to connection establishment of a UE. To evaluate LTag, we deployed it in
the context of geofencing, where LTag was able to determine if the signals
originate from inside or outside of an area within 2.3 ms of the initial base
station-to-device message, therefore enabling prompt and targeted suppression
of communication before any user data was transmitted.

</details>


### [96] [Fingerprinting LLMs via Prompt Injection](https://arxiv.org/abs/2509.25448)
*Yuepeng Hu,Zhengyuan Jiang,Mengyuan Li,Osama Ahmed,Zhicong Huang,Cheng Hong,Neil Gong*

Main category: cs.CR

TL;DR: LLMPrint is a novel framework that detects if LLMs are derived from others by exploiting prompt injection vulnerabilities to create robust fingerprints that survive post-processing like quantization or post-training.


<details>
  <summary>Details</summary>
Motivation: Existing provenance detection methods either require embedding signals before release (infeasible for published models) or use non-robust prompt comparisons that fail against post-processing modifications.

Method: LLMPrint constructs fingerprints by optimizing prompts to enforce consistent token preferences, exploiting LLMs' inherent vulnerability to prompt injection. It works in both gray-box and black-box settings with statistical guarantees.

Result: Evaluated on 5 base models and ~700 post-processed variants, LLMPrint achieves high true positive rates while maintaining near-zero false positive rates.

Conclusion: LLMPrint provides an effective solution for detecting model provenance that is robust to post-processing modifications and applicable to already published models.

Abstract: Large language models (LLMs) are often modified after release through
post-processing such as post-training or quantization, which makes it
challenging to determine whether one model is derived from another. Existing
provenance detection methods have two main limitations: (1) they embed signals
into the base model before release, which is infeasible for already published
models, or (2) they compare outputs across models using hand-crafted or random
prompts, which are not robust to post-processing. In this work, we propose
LLMPrint, a novel detection framework that constructs fingerprints by
exploiting LLMs' inherent vulnerability to prompt injection. Our key insight is
that by optimizing fingerprint prompts to enforce consistent token preferences,
we can obtain fingerprints that are both unique to the base model and robust to
post-processing. We further develop a unified verification procedure that
applies to both gray-box and black-box settings, with statistical guarantees.
We evaluate LLMPrint on five base models and around 700 post-trained or
quantized variants. Our results show that LLMPrint achieves high true positive
rates while keeping false positive rates near zero.

</details>


### [97] [Managing Differentiated Secure Connectivity using Intents](https://arxiv.org/abs/2509.25462)
*Loay Abdelrazek,Filippo Rebecchi*

Main category: cs.CR

TL;DR: Proposes differentiated security levels and intent-based management framework for 5G/6G mobile networks to address complex security requirements through automation.


<details>
  <summary>Details</summary>
Motivation: 5G/6G networks face expanded threat landscape with diverse security requirements, while existing automation approaches lack expressiveness for complex, measurable security goals.

Method: Extends intent-based management frameworks using TM Forum intent security ontology to formalize functional and non-functional security requirements.

Result: Developed approach enables expression and modeling of complex security intents, advancing security automation capabilities for mobile networks.

Conclusion: Intent-based security management can improve adaptability, resilience, and security posture of next-generation mobile networks through standardized frameworks.

Abstract: Mobile networks in the 5G and 6G era require to rethink how to manage
security due to the introduction of new services, use cases, each with its own
security requirements, while simultaneously expanding the threat landscape.
Although automation has emerged as a key enabler to address complexity in
networks, existing approaches lack the expressiveness to define and enforce
complex, goal-driven, and measurable security requirements. In this paper, we
propose the concept of differentiated security levels and leveraging intents as
a management framework. We discuss the requirements and enablers to extend the
currently defined intent-based management frameworks to pave the path for
intent-based security management in mobile networks. Our approach formalizes
both functional and non-functional security requirements and demonstrates how
these can be expressed and modeled using an extended TM Forum (TMF) intent
security ontology. We further discuss the required standardization steps to
achieve intent-based security management. Our work aims at advance security
automation, improve adaptability, and strengthen the resilience and security
posture of the next-generation mobile networks.

</details>


### [98] [Balancing Compliance and Privacy in Offline CBDC Transactions Using a Secure Element-based System](https://arxiv.org/abs/2509.25469)
*Panagiotis Michalopoulos,Anthony Mack,Cameron Clark,Linus Chen,Johannes Sedlmeir,Andreas Veneris*

Main category: cs.CR

TL;DR: This paper presents a prototype for offline digital currency payments that balances regulatory compliance with privacy protection, applicable to both cryptocurrencies and CBDCs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the regulatory challenges that arise when digital currencies support offline transactions, particularly in maintaining AML/CFT compliance while preserving user privacy.

Method: The method involves developing a prototype that uses Secure Elements and digital credentials to enable offline payments while ensuring regulatory compliance. The design also considers integrating Zero-Knowledge Proofs for enhanced privacy protection.

Result: Performance evaluation shows the prototype can be flexibly adapted to different regulatory environments with transaction latency comparable to real-life commercial payment systems.

Conclusion: The paper concludes that it's possible to design offline digital currency payment systems that effectively balance regulatory requirements with privacy protection, and suggests future enhancements through Zero-Knowledge Proof integration.

Abstract: Blockchain technology has spawned a vast ecosystem of digital currencies with
Central Bank Digital Currencies (CBDCs) -- digital forms of fiat currency --
being one of them. An important feature of digital currencies is facilitating
transactions without network connectivity, which can enhance the scalability of
cryptocurrencies and the privacy of CBDC users. However, in the case of CBDCs,
this characteristic also introduces new regulatory challenges, particularly
when it comes to applying established Anti-Money Laundering and Countering the
Financing of Terrorism (AML/CFT) frameworks. This paper introduces a prototype
for offline digital currency payments, equally applicable to cryptocurrencies
and CBDCs, that leverages Secure Elements and digital credentials to address
the tension of offline payment support with regulatory compliance. Performance
evaluation results suggest that the prototype can be flexibly adapted to
different regulatory environments, with a transaction latency comparable to
real-life commercial payment systems. Furthermore, we conceptualize how the
integration of Zero-Knowledge Proofs into our design could accommodate various
tiers of enhanced privacy protection.

</details>


### [99] [Environmental Rate Manipulation Attacks on Power Grid Security](https://arxiv.org/abs/2509.25476)
*Yonatan Gizachew Achamyeleh,Yang Xiang,Yun-Ping Hsiao,Yasamin Moghaddas,Mohammad Abdullah Al Faruque*

Main category: cs.CR

TL;DR: The paper introduces Environmental Rate Manipulation (ERM), a novel hardware Trojan that activates by monitoring the rate of change in environmental parameters rather than absolute values, enabling it to evade traditional detection methods and cause catastrophic failures in power electronics systems.


<details>
  <summary>Details</summary>
Motivation: Traditional hardware Trojan detection methods rely on identifying digital triggers or fixed threshold conditions, but these can be bypassed by monitoring environmental change rates instead of absolute values, creating a more stealthy and dangerous threat to sensor-based power electronics.

Method: Implemented a compact 14μm² circuit that measures capacitor charging rates in standard sensor front-ends and disrupts inverter PWM signals when rapid environmental changes are detected, tested on commercial Texas Instruments solar inverters.

Result: ERM successfully triggered catastrophic driver chip failure in commercial solar inverters, and ETAP simulations showed that a single compromised 100kW inverter could initiate cascading grid instabilities.

Conclusion: ERM represents a fundamental challenge for hardware security, extending beyond individual sensors to entire classes of environmental sensing systems in power electronics, demonstrating the vulnerability of modern supply chains to sophisticated rate-based Trojan attacks.

Abstract: The growing complexity of global supply chains has made hardware Trojans a
significant threat in sensor-based power electronics. Traditional Trojan
designs depend on digital triggers or fixed threshold conditions that can be
detected during standard testing. In contrast, we introduce Environmental Rate
Manipulation (ERM), a novel Trojan triggering mechanism that activates by
monitoring the rate of change in environmental parameters rather than their
absolute values. This approach allows the Trojan to remain inactive under
normal conditions and evade redundancy and sensor-fusion defenses. We implement
a compact 14~$\mu$m$^2$ circuit that measures capacitor charging rates in
standard sensor front-ends and disrupts inverter pulse-width modulation PWM
signals when a rapid change is induced. Experiments on a commercial Texas
Instruments solar inverter demonstrate that ERM can trigger catastrophic driver
chip failure. Furthermore, ETAP simulations indicate that a single compromised
100~kW inverter may initiate cascading grid instabilities. The attack's
significance extends beyond individual sensors to entire classes of
environmental sensing systems common in power electronics, demonstrating
fundamental challenges for hardware security.

</details>


### [100] [Defeating Cerberus: Concept-Guided Privacy-Leakage Mitigation in Multimodal Language Models](https://arxiv.org/abs/2509.25525)
*Boyang Zhang,Istemi Ekin Akkus,Ruichuan Chen,Alice Dethise,Klaus Satzke,Ivica Rimac,Yang Zhang*

Main category: cs.CR

TL;DR: This paper addresses privacy risks in multimodal large language models (MLLMs), specifically vision language models (VLMs), by proposing a concept-guided mitigation approach that modifies internal states to prevent PII leakage without retraining.


<details>
  <summary>Details</summary>
Motivation: MLLMs have advanced capabilities but pose significant privacy concerns regarding Personally Identifiable Information (PII) leakage, which has been insufficiently investigated in multimodal settings compared to single-modal models.

Method: A concept-guided mitigation approach that identifies and modifies the model's internal states associated with PII-related content, enabling VLMs to refuse PII-sensitive tasks without requiring retraining or fine-tuning.

Result: The method achieves an average refusal rate of 93.3% for various PII-related tasks with minimal impact on unrelated model performances, and demonstrates adaptability under various conditions.

Conclusion: The proposed concept-guided mitigation approach effectively protects against PII leakage in VLMs while maintaining general model performance, addressing a critical privacy vulnerability in multimodal AI systems.

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities in processing and reasoning over diverse modalities, but their
advanced abilities also raise significant privacy concerns, particularly
regarding Personally Identifiable Information (PII) leakage. While relevant
research has been conducted on single-modal language models to some extent, the
vulnerabilities in the multimodal setting have yet to be fully investigated. In
this work, we investigate these emerging risks with a focus on vision language
models (VLMs), a representative subclass of MLLMs that covers the two
modalities most relevant for PII leakage, vision and text. We introduce a
concept-guided mitigation approach that identifies and modifies the model's
internal states associated with PII-related content. Our method guides VLMs to
refuse PII-sensitive tasks effectively and efficiently, without requiring
re-training or fine-tuning. We also address the current lack of multimodal PII
datasets by constructing various ones that simulate real-world scenarios.
Experimental results demonstrate that the method can achieve an average refusal
rate of 93.3% for various PII-related tasks with minimal impact on unrelated
model performances. We further examine the mitigation's performance under
various conditions to show the adaptability of our proposed method.

</details>


### [101] [Zero Trust-based Decentralized Identity Management System for Autonomous Vehicles](https://arxiv.org/abs/2509.25566)
*Amal Yousseef,Shalaka Satam,Banafsheh Saber Latibari,Mai Abdel-Malek,Soheil Salehi,Pratik Satam*

Main category: cs.CR

TL;DR: This paper proposes a Zero Trust-based Decentralized Identity Management protocol for autonomous vehicles using blockchain technology to enhance cybersecurity in V2X communications.


<details>
  <summary>Details</summary>
Motivation: Traditional perimeter-based security models are inadequate for dynamic and untrusted environments in connected autonomous vehicles, which face new cybersecurity challenges despite their potential to reduce human-error accidents.

Method: The framework integrates Zero Trust Architecture principles with blockchain technology, specifically using Hyperledger Iroha to enable lightweight and secure authentication without centralized authorities, providing continuous verification for all entities.

Result: Experimental evaluation shows minimal performance overhead: less than 7.5% reduction in Packet Reception Rate in urban settings and under 11% increase in Channel Busy Ratio for LTE-V2X, demonstrating efficiency and robustness.

Conclusion: The D-IM protocol provides a resilient foundation for securing real-time V2X communication against impersonation and replay attacks, proving practical for deployment in autonomous vehicle networks.

Abstract: The rise of autonomous vehicles (AVs) promises to significantly enhance
transportation safety and efficiency by mitigating human error, which is
responsible for over 90\% of road accidents. However, the increasing
connectivity of AVs introduces new cybersecurity challenges, as traditional
perimeter-based security models are inadequate for dynamic and untrusted
environments. This paper presents a novel Zero Trust-based Decentralized
Identity Management (D-IM) protocol for AVs. By integrating the core principles
of Zero Trust Architecture, "never trust, always verify", with the tamper
resistant and decentralized nature of a blockchain network, our framework
eliminates reliance on centralized authorities and provides continuous
verification for every entity. We detail the system's design, which leverages
Hyperledger Iroha to enable lightweight and secure authentication without a
central trusted entity. A comprehensive experimental evaluation, conducted
across both urban and highway scenarios, validates the protocol's practicality.
Our results demonstrate that the D-IM framework introduces minimal overhead,
with less than 7.5\% reduction in Packet Reception Rate (PRR) in urban settings
and an increase of under 11\% in Channel Busy Ratio (CBR) for LTE-V2X. These
findings prove the protocol's efficiency and robustness, providing a resilient
foundation for securing real-time V2X communication against impersonation and
replay attacks.

</details>


### [102] [STAC: When Innocent Tools Form Dangerous Chains to Jailbreak LLM Agents](https://arxiv.org/abs/2509.25624)
*Jing-Jing Li,Jianfeng He,Chao Shang,Devang Kulshreshtha,Xun Xian,Yi Zhang,Hang Su,Sandesh Swamy,Yanjun Qi*

Main category: cs.CR

TL;DR: STAC is a novel multi-turn attack framework that chains seemingly harmless tool calls to enable harmful operations, showing over 90% success rates against state-of-the-art LLM agents like GPT-4.1.


<details>
  <summary>Details</summary>
Motivation: As LLMs advance into autonomous agents with tool-use capabilities, they introduce security challenges beyond traditional content-based safety concerns, requiring new defense approaches.

Method: A closed-loop pipeline that synthesizes executable multi-step tool chains, validates them through in-environment execution, and reverse-engineers stealthy multi-turn prompts to induce agents to execute malicious sequences.

Result: Evaluation of 483 STAC cases with 1,352 interaction sets shows state-of-the-art LLM agents are highly vulnerable (ASR >90%). Existing prompt-based defenses provide limited protection, while a new reasoning-driven defense cuts ASR by up to 28.8%.

Conclusion: Defending tool-enabled agents requires reasoning over entire action sequences and their cumulative effects, rather than evaluating isolated prompts or responses.

Abstract: As LLMs advance into autonomous agents with tool-use capabilities, they
introduce security challenges that extend beyond traditional content-based LLM
safety concerns. This paper introduces Sequential Tool Attack Chaining (STAC),
a novel multi-turn attack framework that exploits agent tool use. STAC chains
together tool calls that each appear harmless in isolation but, when combined,
collectively enable harmful operations that only become apparent at the final
execution step. We apply our framework to automatically generate and
systematically evaluate 483 STAC cases, featuring 1,352 sets of
user-agent-environment interactions and spanning diverse domains, tasks, agent
types, and 10 failure modes. Our evaluations show that state-of-the-art LLM
agents, including GPT-4.1, are highly vulnerable to STAC, with attack success
rates (ASR) exceeding 90% in most cases. The core design of STAC's automated
framework is a closed-loop pipeline that synthesizes executable multi-step tool
chains, validates them through in-environment execution, and reverse-engineers
stealthy multi-turn prompts that reliably induce agents to execute the verified
malicious sequence. We further perform defense analysis against STAC and find
that existing prompt-based defenses provide limited protection. To address this
gap, we propose a new reasoning-driven defense prompt that achieves far
stronger protection, cutting ASR by up to 28.8%. These results highlight a
crucial gap: defending tool-enabled agents requires reasoning over entire
action sequences and their cumulative effects, rather than evaluating isolated
prompts or responses.

</details>


### [103] [Better Privilege Separation for Agents by Restricting Data Types](https://arxiv.org/abs/2509.25926)
*Dennis Jacob,Emad Alghamdi,Zhanhao Hu,Basel Alomair,David Wagner*

Main category: cs.CR

TL;DR: Proposes type-directed privilege separation to systematically prevent prompt injection attacks in LLMs by converting untrusted content to limited-scope data types.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to prompt injection attacks where adversaries subvert intended functionality, and existing defenses like detectors and finetuning are insufficient against adaptive attacks or incompatible with state-of-the-art models.

Method: Type-directed privilege separation that restricts LLM interaction with third-party data by converting untrusted content to a curated set of data types with limited scope and content.

Result: The method successfully prevents prompt injection attacks across several case studies while maintaining high utility.

Conclusion: Type-directed privilege separation provides a systematic approach to eliminate prompt injection vulnerabilities in LLMs without compromising functionality.

Abstract: Large language models (LLMs) have become increasingly popular due to their
ability to interact with unstructured content. As such, LLMs are now a key
driver behind the automation of language processing systems, such as AI agents.
Unfortunately, these advantages have come with a vulnerability to prompt
injections, an attack where an adversary subverts the LLM's intended
functionality with an injected task. Past approaches have proposed detectors
and finetuning to provide robustness, but these techniques are vulnerable to
adaptive attacks or cannot be used with state-of-the-art models. To this end we
propose type-directed privilege separation for LLMs, a method that
systematically prevents prompt injections. We restrict the ability of an LLM to
interact with third-party data by converting untrusted content to a curated set
of data types; unlike raw strings, each data type is limited in scope and
content, eliminating the possibility for prompt injections. We evaluate our
method across several case studies and find that designs leveraging our
principles can systematically prevent prompt injection attacks while
maintaining high utility.

</details>


### [104] [SoK: Systematic analysis of adversarial threats against deep learning approaches for autonomous anomaly detection systems in SDN-IoT networks](https://arxiv.org/abs/2509.26350)
*Tharindu Lakshan Yasarathna,Nhien-An Le-Khac*

Main category: cs.CR

TL;DR: This SoK study systematically analyzes adversarial vulnerabilities in DL-based AAD systems for SDN-IoT networks, revealing attacks can reduce detection accuracy by up to 48.4%. It proposes a structured threat model, taxonomy of attacks, and adaptive countermeasures.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks systematic analysis of adversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT environments, despite the known threat of adversarial attacks that manipulate input data or exploit model weaknesses.

Method: Introduced a structured adversarial threat model and comprehensive taxonomy categorizing attacks into data, model, and hybrid-level threats. Systematically evaluated white, black, and grey-box attack strategies across popular benchmark datasets.

Result: Adversarial attacks can reduce detection accuracy by up to 48.4%, with Membership Inference causing the most significant drop. C&W and DeepFool achieve high evasion success rates. Adversarial training enhances robustness but has high computational overhead.

Conclusion: The study provides practical recommendations for improving resilience, interpretability, and computational efficiency in DL-based AAD security for SDN-IoT networks, offering a foundational reference with systematic threat modeling and defense evaluation.

Abstract: Integrating SDN and the IoT enhances network control and flexibility.
DL-based AAD systems improve security by enabling real-time threat detection in
SDN-IoT networks. However, these systems remain vulnerable to adversarial
attacks that manipulate input data or exploit model weaknesses, significantly
degrading detection accuracy. Existing research lacks a systematic analysis of
adversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT
environments. This SoK study introduces a structured adversarial threat model
and a comprehensive taxonomy of attacks, categorising them into data, model,
and hybrid-level threats. Unlike previous studies, we systematically evaluate
white, black, and grey-box attack strategies across popular benchmark datasets.
Our findings reveal that adversarial attacks can reduce detection accuracy by
up to 48.4%, with Membership Inference causing the most significant drop. C&W
and DeepFool achieve high evasion success rates. However, adversarial training
enhances robustness, and its high computational overhead limits the real-time
deployment of SDN-IoT applications. We propose adaptive countermeasures,
including real-time adversarial mitigation, enhanced retraining mechanisms, and
explainable AI-driven security frameworks. By integrating structured threat
models, this study offers a more comprehensive approach to attack
categorisation, impact assessment, and defence evaluation than previous
research. Our work highlights critical vulnerabilities in existing DL-based AAD
models and provides practical recommendations for improving resilience,
interpretability, and computational efficiency. This study serves as a
foundational reference for researchers and practitioners seeking to enhance
DL-based AAD security in SDN-IoT networks, offering a systematic adversarial
threat model and conceptual defence evaluation based on prior empirical
studies.

</details>


### [105] [Exact Bias of Linear TRNG Correctors - Spectral Approach](https://arxiv.org/abs/2509.26393)
*Maciej Skorski,Francisco-Javier Soto,Onur Günlü*

Main category: cs.CR

TL;DR: This paper establishes exact security bounds for linear extractors in TRNGs using Fourier analysis, providing near-optimal total variation security characterization that improves previous assessments by an order of magnitude.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous security analysis for True Random Number Generators (TRNGs) and understand the fundamental trade-offs between compression efficiency and cryptographic security in randomness extraction.

Method: Using Fourier analysis to establish security bounds, interpolating between optimal ℓ∞ and ℓ2 norm results through code weight enumerators and input bias parameters, and scanning ~20,000 codes.

Result: The bounds improve security assessments by an order of magnitude over previous approximations. Analysis reveals that achieving 80 bits of security can require sacrificing more than 50% of code rate when correcting 10% input bias.

Conclusion: The established bounds enhance security evaluation of TRNG post-processing schemes and quantify the inherent cost of randomness extraction in hardware implementations.

Abstract: Using Fourier analysis, this paper establishes exact security bounds for
linear extractors in True Random Number Generators (TRNGs). We provide the
first near-optimal total variation security characterization by interpolating
between optimal $\ell_{\infty}$ and $\ell_2$ norm results, expressed through
code weight enumerators and input bias parameters.
  Our bounds improve security assessments by an order of magnitude over
previous approximations. By scanning ~20,000 codes, we reveal fundamental
trade-offs between compression efficiency and cryptographic security. For
instance, we show that achieving 80 bits of security can require sacrificing
more than 50\% of the code rate when correcting 10\% input bias. Our bounds
enhance security evaluation of TRNG post-processing schemes and quantify the
inherent cost of randomness extraction in hardware implementations.

</details>


### [106] [SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language Model Was Trained From](https://arxiv.org/abs/2509.26404)
*Yao Tong,Haonan Wang,Siquan Li,Kenji Kawaguchi,Tianyang Hu*

Main category: cs.CR

TL;DR: SeedPrints is a novel LLM fingerprinting method that uses random initialization biases as persistent identifiers, enabling seed-level distinguishability and birth-to-lifecycle identity verification.


<details>
  <summary>Details</summary>
Motivation: Existing LLM fingerprinting methods rely on post-training properties, making them unreliable before convergence and vulnerable to distribution shifts. There's a need for more intrinsic and robust fingerprinting that works across all training stages.

Method: Leverages random initialization biases as seed-dependent identifiers that are present even before training. Uses reproducible token selection biases conditioned on initialization parameters, with statistical detection to recover model lineage.

Result: Achieves seed-level distinguishability and provides birth-to-lifecycle identity verification. Remains effective across all training stages and robust under domain shifts or parameter modifications. Experiments on LLaMA-style and Qwen-style models confirm effectiveness.

Conclusion: Initialization itself imprints a unique and persistent identity on neural language models, forming a true 'Galtonian' fingerprint that enables strong provenance verification and model attribution.

Abstract: Fingerprinting Large Language Models (LLMs) is essential for provenance
verification and model attribution. Existing methods typically extract post-hoc
signatures based on training dynamics, data exposure, or hyperparameters --
properties that only emerge after training begins. In contrast, we propose a
stronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method
that leverages random initialization biases as persistent, seed-dependent
identifiers present even before training. We show that untrained models exhibit
reproducible token selection biases conditioned solely on their parameters at
initialization. These biases are stable and measurable throughout training,
enabling our statistical detection method to recover a model's lineage with
high confidence. Unlike prior techniques, unreliable before convergence and
vulnerable to distribution shifts, SeedPrints remains effective across all
training stages and robust under domain shifts or parameter modifications.
Experiments on LLaMA-style and Qwen-style models show that SeedPrints achieves
seed-level distinguishability and can provide birth-to-lifecycle identity
verification akin to a biometric fingerprint. Evaluations on large-scale
pretrained models and fingerprinting benchmarks further confirm its
effectiveness under practical deployment scenarios. These results suggest that
initialization itself imprints a unique and persistent identity on neural
language models, forming a true ''Galtonian'' fingerprint.

</details>


### [107] [Logic Solver Guided Directed Fuzzing for Hardware Designs](https://arxiv.org/abs/2509.26509)
*Raghul Saravanan,Sai Manoj P D*

Main category: cs.CR

TL;DR: TargetFuzz is a targeted hardware fuzzing mechanism that uses SAT-based techniques to focus on specific regions of hardware designs at their native abstraction level, achieving 30x greater scalability, 100% state coverage, 1.5x faster site coverage, and 90x improvement in target state coverage compared to Coverage-Guided Fuzzing.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of processor and IP designs makes early bug detection challenging. Existing hardware fuzzers rely on equivalent software models that fail to capture intrinsic hardware characteristics, limiting their effectiveness in modern IC design flows with incremental updates.

Method: TargetFuzz leverages SAT-based techniques to focus on specific regions of hardware designs while operating at their native hardware abstraction level, enabling more precise and comprehensive verification without relying on software models.

Result: Evaluation across diverse RTL designs for various IP cores shows TargetFuzz can effectively target and fuzz a broad spectrum of sites, achieving 30x greater scalability in handling target sites, 100% state coverage, 1.5x faster site coverage, and 90x improvement in target state coverage compared to Coverage-Guided Fuzzing.

Conclusion: TargetFuzz demonstrates significant advancements in directed hardware fuzzing by addressing the limitations of existing approaches and showing potential to advance the state-of-the-art in hardware verification.

Abstract: The ever-increasing complexity of design specifications for processors and
intellectual property (IP) presents a formidable challenge for early bug
detection in the modern IC design cycle. The recent advancements in hardware
fuzzing have proven effective in detecting bugs in RTL designs of cutting-edge
processors. The modern IC design flow involves incremental updates and
modifications to the hardware designs necessitating rigorous verification and
extending the overall verification period. To accelerate this process, directed
fuzzing has emerged focusing on generating targeted stimuli for specific
regions of the design, avoiding the need for exhaustive, full-scale
verification. However, a significant limitation of these hardware fuzzers lies
in their reliance on an equivalent SW model of the hardware which fails to
capture intrinsic hardware characteristics. To circumvent the aforementioned
challenges, this work introduces TargetFuzz, an innovative and scalable
targeted hardware fuzzing mechanism. It leverages SAT-based techniques to focus
on specific regions of the hardware design while operating at its native
hardware abstraction level, ensuring a more precise and comprehensive
verification process. We evaluated this approach across a diverse range of RTL
designs for various IP cores. Our experimental results demonstrate its
capability to effectively target and fuzz a broad spectrum of sites within
these designs, showcasing its extensive coverage and precision in addressing
targeted regions. TargetFuzz demonstrates its capability to effectively scale
30x greater in terms of handling target sites, achieving 100% state coverage
and 1.5x faster in terms of site coverage, and shows 90x improvement in target
state coverage compared to Coverage-Guided Fuzzing, demonstrating its potential
to advance the state-of-the-art in directed hardware fuzzing.

</details>


### [108] [Explainable and Resilient ML-Based Physical-Layer Attack Detectors](https://arxiv.org/abs/2509.26530)
*Aleksandra Knapińska,Marija Furdek*

Main category: cs.CR

TL;DR: This paper presents an explainable ML approach for physical-layer attack detection, analyzing classifier interpretability and evaluating resilience to parameter noising, revealing a speed-resilience trade-off.


<details>
  <summary>Details</summary>
Motivation: The growing complexity of ML-based attack detection methods makes them increasingly opaque, creating a need for explainable physical-layer attack detection to improve interpretability and design.

Method: Analyzed inner workings of various classifiers for physical-layer intrusions, examined parameter influence variations across attack types, and evaluated detector resilience to malicious parameter noising.

Result: The analysis improved model interpretability and suggested design enhancements for increased speed, while revealing a key trade-off between model speed and resilience to parameter noising.

Conclusion: This work provides design guidelines for developing fast and robust physical-layer attack detectors using available network monitoring data, balancing speed and resilience considerations.

Abstract: Detection of emerging attacks on network infrastructure is a critical aspect
of security management. To meet the growing scale and complexity of modern
threats, machine learning (ML) techniques offer valuable tools for automating
the detection of malicious activities. However, as these techniques become more
complex, their internal operations grow increasingly opaque. In this context,
we address the need for explainable physical-layer attack detection methods.
First, we analyze the inner workings of various classifiers trained to alert
about physical layer intrusions, examining how the influence of different
monitored parameters varies depending on the type of attack being detected.
This analysis not only improves the interpretability of the models but also
suggests ways to enhance their design for increased speed. In the second part,
we evaluate the detectors' resilience to malicious parameter noising. The
results highlight a key trade-off between model speed and resilience. This work
serves as a design guideline for developing fast and robust detectors trained
on available network monitoring data.

</details>


### [109] [DeepProv: Behavioral Characterization and Repair of Neural Networks via Inference Provenance Graph Analysis](https://arxiv.org/abs/2509.26562)
*Firas Ben Hmida,Abderrahmen Amich,Ata Kaboudi,Birhanu Eshete*

Main category: cs.CR

TL;DR: DeepProv is a system that captures DNN runtime behavior using inference provenance graphs and systematically repairs models for objectives like robustness, privacy, and fairness.


<details>
  <summary>Details</summary>
Motivation: DNNs are used in high-stakes applications but exhibit unpredictable behaviors, requiring new approaches to ensure reliability.

Method: Models DNN inference through Inference Provenance Graphs (IPGs) that capture computational information flow, enabling structural analysis and systematic repair at node/edge level.

Result: Repairing just one DNN layer yields 55% average improvement in adversarial accuracy, complements existing defenses, and demonstrates scalability across diverse tasks and models.

Conclusion: DeepProv effectively characterizes DNN behavior and enables systematic model repair, showing broader potential for privacy auditing and fairness analysis beyond robustness.

Abstract: Deep neural networks (DNNs) are increasingly being deployed in high-stakes
applications, from self-driving cars to biometric authentication. However,
their unpredictable and unreliable behaviors in real-world settings require new
approaches to characterize and ensure their reliability.
  This paper introduces DeepProv, a novel and customizable system designed to
capture and characterize the runtime behavior of DNNs during inference by using
their underlying graph structure. Inspired by system audit provenance graphs,
DeepProv models the computational information flow of a DNN's inference process
through Inference Provenance Graphs (IPGs). These graphs provide a detailed
structural representation of the behavior of DNN, allowing both empirical and
structural analysis. DeepProv uses these insights to systematically repair DNNs
for specific objectives, such as improving robustness, privacy, or fairness.
  We instantiate DeepProv with adversarial robustness as the goal of model
repair and conduct extensive case studies to evaluate its effectiveness. Our
results demonstrate its effectiveness and scalability across diverse
classification tasks, attack scenarios, and model complexities. DeepProv
automatically identifies repair actions at the node and edge-level within IPGs,
significantly enhancing the robustness of the model. In particular, applying
DeepProv repair strategies to just a single layer of a DNN yields an average
55% improvement in adversarial accuracy. Moreover, DeepProv complements
existing defenses, achieving substantial gains in adversarial robustness.
Beyond robustness, we demonstrate the broader potential of DeepProv as an
adaptable system to characterize DNN behavior in other critical areas, such as
privacy auditing and fairness analysis.

</details>


### [110] [Are Robust LLM Fingerprints Adversarially Robust?](https://arxiv.org/abs/2509.26598)
*Anshul Nasery,Edoardo Contente,Alkin Kaz,Pramod Viswanath,Sewoong Oh*

Main category: cs.CR

TL;DR: This paper analyzes the adversarial robustness of model fingerprinting schemes, identifies vulnerabilities in existing methods, and develops attacks that can bypass authentication while maintaining model utility.


<details>
  <summary>Details</summary>
Motivation: Current model fingerprinting schemes lack systematic evaluation of adversarial robustness against malicious model hosts, leaving them vulnerable to attacks.

Method: The authors define a concrete threat model, analyze vulnerabilities in existing fingerprinting schemes, and develop adaptive adversarial attacks tailored to each vulnerability.

Result: The attacks successfully bypass model authentication for ten recent fingerprinting schemes while maintaining high model utility for end users.

Conclusion: Fingerprint designers should adopt adversarial robustness by design, and the paper provides recommendations for future fingerprinting methods.

Abstract: Model fingerprinting has emerged as a promising paradigm for claiming model
ownership. However, robustness evaluations of these schemes have mostly focused
on benign perturbations such as incremental fine-tuning, model merging, and
prompting. Lack of systematic investigations into {\em adversarial robustness}
against a malicious model host leaves current systems vulnerable. To bridge
this gap, we first define a concrete, practical threat model against model
fingerprinting. We then take a critical look at existing model fingerprinting
schemes to identify their fundamental vulnerabilities. Based on these, we
develop adaptive adversarial attacks tailored for each vulnerability, and
demonstrate that these can bypass model authentication completely for ten
recently proposed fingerprinting schemes while maintaining high utility of the
model for the end users. Our work encourages fingerprint designers to adopt
adversarial robustness by design. We end with recommendations for future
fingerprinting methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [111] [SOLD: SELFIES-based Objective-driven Latent Diffusion](https://arxiv.org/abs/2509.25198)
*Elbert Ho*

Main category: cs.LG

TL;DR: SOLD is a latent diffusion model that generates drug molecules in latent space from SELFIES strings, conditioned on target proteins, offering efficient high-affinity molecule generation.


<details>
  <summary>Details</summary>
Motivation: Current methods for de novo drug design generate molecules directly in 3D conformational space, which are slow and overly complex.

Method: Proposed SOLD model using latent diffusion in space derived from 1D SELFIES strings, trained with SELFIES transformer and balanced multi-task loss approach.

Result: Model generates high-affinity molecules for target proteins in simple and efficient manner.

Conclusion: SOLD provides efficient drug molecule generation with room for future improvements through additional data.

Abstract: Recently, machine learning has made a significant impact on de novo drug
design. However, current approaches to creating novel molecules conditioned on
a target protein typically rely on generating molecules directly in the 3D
conformational space, which are often slow and overly complex. In this work, we
propose SOLD (SELFIES-based Objective-driven Latent Diffusion), a novel latent
diffusion model that generates molecules in a latent space derived from 1D
SELFIES strings and conditioned on a target protein. In the process, we also
train an innovative SELFIES transformer and propose a new way to balance losses
when training multi-task machine learning models.Our model generates
high-affinity molecules for the target protein in a simple and efficient way,
while also leaving room for future improvements through the addition of more
data.

</details>


### [112] [VLHSA: Vision-Language Hierarchical Semantic Alignment for Jigsaw Puzzle Solving with Eroded Gaps](https://arxiv.org/abs/2509.25202)
*Zhuoning Xu,Xinyan Liu*

Main category: cs.LG

TL;DR: A vision-language framework for jigsaw puzzle solving that uses textual descriptions to enhance assembly performance, achieving 14.2% improvement in piece accuracy over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Traditional jigsaw puzzle solving methods focus only on visual cues, but struggle with challenging scenarios like eroded gap puzzles where semantic context from language could provide valuable guidance.

Method: Proposes Vision-Language Hierarchical Semantic Alignment (VLHSA) module that aligns visual patches with textual descriptions through multi-level semantic matching, using dual visual encoders combined with language features for cross-modal reasoning.

Result: Significantly outperforms state-of-the-art models across various datasets, achieving 14.2 percentage point gain in piece accuracy. Ablation studies confirm VLHSA's critical role in improvements over vision-only approaches.

Conclusion: Establishes a new paradigm for jigsaw puzzle solving by incorporating multimodal semantic insights, demonstrating the value of combining visual and language information for complex puzzle assembly tasks.

Abstract: Jigsaw puzzle solving remains challenging in computer vision, requiring an
understanding of both local fragment details and global spatial relationships.
While most traditional approaches only focus on visual cues like edge matching
and visual coherence, few methods explore natural language descriptions for
semantic guidance in challenging scenarios, especially for eroded gap puzzles.
We propose a vision-language framework that leverages textual context to
enhance puzzle assembly performance. Our approach centers on the
Vision-Language Hierarchical Semantic Alignment (VLHSA) module, which aligns
visual patches with textual descriptions through multi-level semantic matching
from local tokens to global context. Also, a multimodal architecture that
combines dual visual encoders with language features for cross-modal reasoning
is integrated into this module. Experiments demonstrate that our method
significantly outperforms state-of-the-art models across various datasets,
achieving substantial improvements, including a 14.2 percentage point gain in
piece accuracy. Ablation studies confirm the critical role of the VLHSA module
in driving improvements over vision-only approaches. Our work establishes a new
paradigm for jigsaw puzzle solving by incorporating multimodal semantic
insights.

</details>


### [113] [Spectral Logit Sculpting: Adaptive Low-Rank Logit Transformation for Controlled Text Generation](https://arxiv.org/abs/2509.25204)
*Jin Li,Zhebo Wang,Tianliang Lu,Mohan Li,Wenpeng Xing,Meng Han*

Main category: cs.LG

TL;DR: Spectral Logit Sculpting (SLS) is a lightweight inference-time optimization method that improves LLM reliability by dynamically modulating token distributions using spectral analysis and entropy properties, achieving better accuracy without parameter updates.


<details>
  <summary>Details</summary>
Motivation: Existing entropy-based inference methods for LLMs suffer from high computational overhead and ineffective use of historical token context, limiting their practical deployment.

Method: SLS maintains a sliding buffer of top-K logits, performs on-the-fly SVD to identify dominant spectral directions, and adaptively rescales logits based on entropy and logit gap statistics, only activating during high uncertainty periods.

Result: Experimental results on multiple benchmarks show SLS consistently outperforms baseline methods, achieving superior accuracy in mathematical, coding, and scientific reasoning tasks.

Conclusion: SLS provides an effective, computationally efficient solution for improving LLM reliability through spectral analysis and entropy-based logit modulation during inference.

Abstract: Entropy-based inference methods have gained traction for improving the
reliability of Large Language Models (LLMs). However, many existing approaches,
such as entropy minimization techniques, suffer from high computational
overhead and fail to leverage historical token context effectively. To address
these limitations, we propose Spectral Logit Sculpting (SLS), a lightweight
inference-time optimization method that dynamically modulates token
distributions using spectral and entropic properties of recent logits. SLS
maintains a sliding buffer of top-K logits, performs on-the-fly Singular Value
Decomposition (SVD) to identify dominant spectral directions, and adaptively
rescales logits based on both entropy and logit gap statistics--only activating
when uncertainty is high. Without updating any model parameters, SLS
effectively sharpens the output distribution while preserving contextual
consistency. Experimental results on multiple public benchmarks demonstrate
that SLS consistently outperforms existing baseline methods, achieving superior
accuracy in mathematical, coding, and scientific reasoning tasks.

</details>


### [114] [Polynomial Contrastive Learning for Privacy-Preserving Representation Learning on Graphs](https://arxiv.org/abs/2509.25205)
*Daksh Pandey*

Main category: cs.LG

TL;DR: Poly-GRACE is a novel HE-compatible self-supervised learning framework for graphs that replaces non-polynomial operations in GRACE with polynomial alternatives, enabling privacy-preserving training while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Existing SSL methods like GRACE are incompatible with Homomorphic Encryption due to reliance on non-polynomial operations, limiting privacy-preserving graph representation learning.

Method: Developed a fully polynomial-friendly GCN encoder and a novel polynomial-based contrastive loss function to enable HE-compatible self-supervised learning.

Result: On Cora, CiteSeer, and PubMed datasets, Poly-GRACE achieves competitive performance with standard non-private baseline, and superior performance on CiteSeer.

Conclusion: Poly-GRACE represents a significant advancement towards practical and high-performance privacy-preserving graph representation learning.

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for
learning representations on graph data without requiring manual labels.
However, leading SSL methods like GRACE are fundamentally incompatible with
privacy-preserving technologies such as Homomorphic Encryption (HE) due to
their reliance on non-polynomial operations. This paper introduces Poly-GRACE,
a novel framework for HE-compatible self-supervised learning on graphs. Our
approach consists of a fully polynomial-friendly Graph Convolutional Network
(GCN) encoder and a novel, polynomial-based contrastive loss function. Through
experiments on three benchmark datasets -- Cora, CiteSeer, and PubMed -- we
demonstrate that Poly-GRACE not only enables private pre-training but also
achieves performance that is highly competitive with, and in the case of
CiteSeer, superior to the standard non-private baseline. Our work represents a
significant step towards practical and high-performance privacy-preserving
graph representation learning.

</details>


### [115] [Hyperbolic Optimization](https://arxiv.org/abs/2509.25206)
*Yanke Wang,Kyriakos Flouris*

Main category: cs.LG

TL;DR: This paper extends Riemannian optimization to hyperbolic manifolds, developing Hyperbolic Adam optimizer and showing it accelerates early training convergence for diffusion models.


<details>
  <summary>Details</summary>
Motivation: To leverage hyperbolic manifolds for optimization, particularly for learning Poincaré embeddings that can accelerate convergence when parameters are far from optimum.

Method: Extends Riemannian optimization principles to hyperbolic manifolds, develops Hyperbolic SGD and Hyperbolic Adam optimizers, applies to diffusion models with hyperbolic time-discretization of Langevin dynamics.

Result: Hyperbolic optimization methods achieve faster convergence on certain datasets without sacrificing generative quality in diffusion models.

Conclusion: Hyperbolic optimization methods provide benefits for training deep learning models, particularly accelerating early-stage convergence while maintaining performance.

Abstract: This work explores optimization methods on hyperbolic manifolds. Building on
Riemannian optimization principles, we extend the Hyperbolic Stochastic
Gradient Descent (a specialization of Riemannian SGD) to a Hyperbolic Adam
optimizer. While these methods are particularly relevant for learning on the
Poincar\'e ball, they may also provide benefits in Euclidean and other
non-Euclidean settings, as the chosen optimization encourages the learning of
Poincar\'e embeddings. This representation, in turn, accelerates convergence in
the early stages of training, when parameters are far from the optimum. As a
case study, we train diffusion models using the hyperbolic optimization methods
with hyperbolic time-discretization of the Langevin dynamics, and show that
they achieve faster convergence on certain datasets without sacrificing
generative quality.

</details>


### [116] [Multi-level Diagnosis and Evaluation for Robust Tabular Feature Engineering with Large Language Models](https://arxiv.org/abs/2509.25207)
*Yebin Lim,Susik Yoon*

Main category: cs.LG

TL;DR: A multi-level framework to assess LLM robustness in feature engineering, showing variable performance across datasets and potential for up to 10.52% improvement in few-shot prediction.


<details>
  <summary>Details</summary>
Motivation: Address concerns about LLM reliability in feature engineering due to output variability, and establish systematic evaluation methods.

Method: Multi-level diagnosis and evaluation framework focusing on key variables, relationships, and decision boundary values for predicting target classes.

Result: LLM robustness varies significantly across datasets, and high-quality LLM-generated features can improve few-shot prediction performance by up to 10.52%.

Conclusion: This work opens new directions for assessing and enhancing LLM-driven feature engineering reliability across various domains.

Abstract: Recent advancements in large language models (LLMs) have shown promise in
feature engineering for tabular data, but concerns about their reliability
persist, especially due to variability in generated outputs. We introduce a
multi-level diagnosis and evaluation framework to assess the robustness of LLMs
in feature engineering across diverse domains, focusing on the three main
factors: key variables, relationships, and decision boundary values for
predicting target classes. We demonstrate that the robustness of LLMs varies
significantly over different datasets, and that high-quality LLM-generated
features can improve few-shot prediction performance by up to 10.52%. This work
opens a new direction for assessing and enhancing the reliability of LLM-driven
feature engineering in various domains.

</details>


### [117] [DPSformer: A long-tail-aware model for improving heavy rainfall prediction](https://arxiv.org/abs/2509.25208)
*Zenghui Huang,Ting Shu,Zhonglei Wang,Yang Lu,Yan Yan,Wei Zhong,Hanzi Wang*

Main category: cs.LG

TL;DR: DPSformer addresses heavy rainfall forecasting as a long-tailed learning problem, improving prediction of rare but critical heavy rainfall events through specialized high-resolution modeling.


<details>
  <summary>Details</summary>
Motivation: Heavy rainfall forecasting is challenging due to imbalanced data distribution where most observations show no/light rain while heavy rainfall events are rare, preventing deep learning models from effectively predicting these critical events.

Method: Treat rainfall forecasting as long-tailed learning problem; introduce DPSformer model with high-resolution branch to enrich representation of heavy rainfall events.

Result: For heavy rainfall events ≥50mm/6h, DPSformer improves CSI from 0.012 to 0.067; for top 1% heavy rainfall events, FSS exceeds 0.45, outperforming existing methods.

Conclusion: DPSformer establishes effective long-tailed paradigm for heavy rainfall prediction, providing practical tool to enhance early warning systems and mitigate extreme weather impacts.

Abstract: Accurate and timely forecasting of heavy rainfall remains a critical
challenge for modern society. Precipitation exhibits a highly imbalanced
distribution: most observations record no or light rain, while heavy rainfall
events are rare. Such an imbalanced distribution obstructs deep learning models
from effectively predicting heavy rainfall events. To address this challenge,
we treat rainfall forecasting explicitly as a long-tailed learning problem,
identifying the insufficient representation of heavy rainfall events as the
primary barrier to forecasting accuracy. Therefore, we introduce DPSformer, a
long-tail-aware model that enriches representation of heavy rainfall events
through a high-resolution branch. For heavy rainfall events $ \geq $ 50 mm/6 h,
DPSformer lifts the Critical Success Index (CSI) of a baseline Numerical
Weather Prediction (NWP) model from 0.012 to 0.067. For the top 1% coverage of
heavy rainfall events, its Fraction Skill Score (FSS) exceeds 0.45, surpassing
existing methods. Our work establishes an effective long-tailed paradigm for
heavy rainfall prediction, offering a practical tool to enhance early warning
systems and mitigate the societal impacts of extreme weather events.

</details>


### [118] [STCast: Adaptive Boundary Alignment for Global and Regional Weather Forecasting](https://arxiv.org/abs/2509.25210)
*Hao Chen,Tao Han,Jie Zhang,Song Guo,Lei Bai*

Main category: cs.LG

TL;DR: STCast is an AI framework that improves regional weather forecasting by adaptively optimizing regional boundaries and dynamically allocating monthly forecasts using spatial-aligned attention and temporal mixture-of-experts mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current regional weather forecasting methods are limited by static and imprecise regional boundaries, leading to poor generalization ability.

Method: Uses Spatial-Aligned Attention to align global/regional distributions and refine boundaries adaptively, plus Temporal Mixture-of-Experts that routes monthly atmospheric variables to specialized experts using discrete Gaussian distribution.

Result: Experimental results show consistent superiority over state-of-the-art methods in global forecasting, regional forecasting, extreme event prediction, and ensemble forecasting.

Conclusion: STCast effectively addresses regional boundary optimization and temporal pattern capture, demonstrating strong performance across multiple weather forecasting tasks.

Abstract: To gain finer regional forecasts, many works have explored the regional
integration from the global atmosphere, e.g., by solving boundary equations in
physics-based methods or cropping regions from global forecasts in data-driven
methods. However, the effectiveness of these methods is often constrained by
static and imprecise regional boundaries, resulting in poor generalization
ability. To address this issue, we propose Spatial-Temporal Weather Forecasting
(STCast), a novel AI-driven framework for adaptive regional boundary
optimization and dynamic monthly forecast allocation. Specifically, our
approach employs a Spatial-Aligned Attention (SAA) mechanism, which aligns
global and regional spatial distributions to initialize boundaries and
adaptively refines them based on attention-derived alignment patterns.
Furthermore, we design a Temporal Mixture-of-Experts (TMoE) module, where
atmospheric variables from distinct months are dynamically routed to
specialized experts using a discrete Gaussian distribution, enhancing the
model's ability to capture temporal patterns. Beyond global and regional
forecasting, we evaluate our STCast on extreme event prediction and ensemble
forecasting. Experimental results demonstrate consistent superiority over
state-of-the-art methods across all four tasks.

</details>


### [119] [LEMs: A Primer On Large Execution Models](https://arxiv.org/abs/2509.25211)
*Remi Genet,Hugo Inzirillo*

Main category: cs.LG

TL;DR: LEMs extend transformer architectures to handle complex execution problems with flexible time boundaries and multiple constraints, outperforming traditional benchmarks in cryptocurrency and equity markets.


<details>
  <summary>Details</summary>
Motivation: To generalize neural execution strategies from fixed-duration orders to scenarios with flexible time horizons (min/max bounds), similar to share buyback contracts, enabling unified handling of diverse execution objectives.

Method: Decouples market processing from allocation decisions: uses TKANs, VSNs, and multi-head attention for feature extraction, with independent allocation networks for different execution scenarios (fixed quantity/notional, buy/sell orders).

Result: Achieves superior execution performance in intraday cryptocurrency and multi-day equity trading (DOW Jones constituents) by dynamically optimizing execution paths within flexible time constraints.

Conclusion: The unified LEM framework enables deployment across diverse execution scenarios through a single model, providing significant operational advantages over asset-specific approaches.

Abstract: This paper introduces Large Execution Models (LEMs), a novel deep learning
framework that extends transformer-based architectures to address complex
execution problems with flexible time boundaries and multiple execution
constraints. Building upon recent advances in neural VWAP execution strategies,
LEMs generalize the approach from fixed-duration orders to scenarios where
execution duration is bounded between minimum and maximum time horizons,
similar to share buyback contract structures. The proposed architecture
decouples market information processing from execution allocation decisions: a
common feature extraction pipeline using Temporal Kolmogorov-Arnold Networks
(TKANs), Variable Selection Networks (VSNs), and multi-head attention
mechanisms processes market data to create informational context, while
independent allocation networks handle the specific execution logic for
different scenarios (fixed quantity vs. fixed notional, buy vs. sell orders).
This architectural separation enables a unified model to handle diverse
execution objectives while leveraging shared market understanding across
scenarios. Through comprehensive empirical evaluation on intraday
cryptocurrency markets and multi-day equity trading using DOW Jones
constituents, we demonstrate that LEMs achieve superior execution performance
compared to traditional benchmarks by dynamically optimizing execution paths
within flexible time constraints. The unified model architecture enables
deployment across different execution scenarios (buy/sell orders, varying
duration boundaries, volume/notional targets) through a single framework,
providing significant operational advantages over asset-specific approaches.

</details>


### [120] [Six Sigma For Neural Networks: Taguchi-based optimization](https://arxiv.org/abs/2509.25213)
*Sai Varun Kodathala*

Main category: cs.LG

TL;DR: This paper applies Taguchi Design of Experiments methodology to optimize CNN hyperparameters for boxing action recognition, achieving 98.84% training accuracy and 86.25% validation accuracy through systematic parameter evaluation.


<details>
  <summary>Details</summary>
Motivation: Traditional hyperparameter optimization in CNNs is challenging and computationally expensive, requiring extensive trial-and-error or grid searches. The study aims to apply statistical optimization techniques from quality engineering to systematically optimize CNN hyperparameters.

Method: Used Taguchi Design of Experiments with L12(211) orthogonal array to systematically evaluate eight hyperparameters across twelve configurations. Developed five different approaches to optimize multiple objectives (training/validation accuracy and loss) using Signal-to-Noise ratio analysis with logarithmic scaling.

Result: Approach 3 achieved optimal performance with 98.84% training accuracy and 86.25% validation accuracy while maintaining minimal loss values. Learning rate was identified as the most influential parameter, followed by image size and activation function.

Conclusion: Taguchi methodology provides an effective systematic approach for CNN hyperparameter optimization, offering clear parameter prioritization guidance and achieving high performance in boxing action recognition tasks.

Abstract: The optimization of hyperparameters in convolutional neural networks (CNNs)
remains a challenging and computationally expensive process, often requiring
extensive trial-and-error approaches or exhaustive grid searches. This study
introduces the application of Taguchi Design of Experiments methodology, a
statistical optimization technique traditionally used in quality engineering,
to systematically optimize CNN hyperparameters for professional boxing action
recognition. Using an L12(211) orthogonal array, eight hyperparameters
including image size, color mode, activation function, learning rate,
rescaling, shuffling, vertical flip, and horizontal flip were systematically
evaluated across twelve experimental configurations. To address the
multi-objective nature of machine learning optimization, five different
approaches were developed to simultaneously optimize training accuracy,
validation accuracy, training loss, and validation loss using Signal-to-Noise
ratio analysis. The study employed a novel logarithmic scaling technique to
unify conflicting metrics and enable comprehensive multi-quality assessment
within the Taguchi framework. Results demonstrate that Approach 3, combining
weighted accuracy metrics with logarithmically transformed loss functions,
achieved optimal performance with 98.84% training accuracy and 86.25%
validation accuracy while maintaining minimal loss values. The Taguchi analysis
revealed that learning rate emerged as the most influential parameter, followed
by image size and activation function, providing clear guidance for
hyperparameter prioritization in CNN optimization.

</details>


### [121] [On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs](https://arxiv.org/abs/2509.25214)
*Rongguang Ye,Ming Tang,Edith C. H. Ngai*

Main category: cs.LG

TL;DR: CoA-LoRA enables dynamic adjustment of LoRA adapters for arbitrary quantization configurations without repeated fine-tuning, using a configuration-aware model trained on Pareto-optimized configuration sets.


<details>
  <summary>Details</summary>
Motivation: Edge devices have heterogeneous capabilities requiring different quantization settings, but fine-tuning separate LoRA adapters for each configuration is computationally prohibitive.

Method: Propose CoA-LoRA with configuration-aware model that maps quantization configurations to low-rank adjustments, using Pareto-based configuration search to optimize training sets.

Result: CoA-LoRA achieves comparable or superior performance to methods requiring separate fine-tuning per configuration, with no additional time cost.

Conclusion: CoA-LoRA provides an efficient solution for deploying compressed models on heterogeneous edge devices without repeated fine-tuning overhead.

Abstract: As increasingly large pre-trained models are released, deploying them on edge
devices for privacy-preserving applications requires effective compression.
Recent works combine quantization with the fine-tuning of high-precision LoRA
adapters, which can substantially reduce model size while mitigating the
accuracy loss from quantization. However, edge devices have inherently
heterogeneous capabilities, while performing configuration-wise fine-tuning for
every quantization setting is computationally prohibitive. In this paper, we
propose CoA-LoRA, a method that dynamically adjusts the LoRA adapter to
arbitrary quantization configurations (i.e., the per-layer bit-width choices of
a pre-trained model) without requiring repeated fine-tuning. This is
accomplished via a configuration-aware model that maps each configuration to
its low-rank adjustments. The effectiveness of this model critically depends on
the training configuration set, a collection of configurations chosen to cover
different total bit-width budgets. However, constructing a high-quality
configuration set is non-trivial. We therefore design a Pareto-based
configuration search that iteratively optimizes the training configuration set,
yielding more precise low-rank adjustments. Our experiments demonstrate that,
unlike the state-of-the-art methods that require fine-tuning a separate LoRA
adapter for each configuration, CoA-LoRA incurs no additional time cost while
achieving comparable or even superior performance to those methods.

</details>


### [122] [Anomaly detection by partitioning of multi-variate time series](https://arxiv.org/abs/2509.25215)
*Pierre Lotte,André Péninou,Olivier Teste*

Main category: cs.LG

TL;DR: PARADISE is a novel unsupervised partition-based anomaly detection method for multivariate time series that clusters variables based on correlation coefficients and performs local anomaly detection on subsets.


<details>
  <summary>Details</summary>
Motivation: To improve anomaly detection in multivariate time series by preserving inter-variable relationships through intelligent partitioning of variables.

Method: Creates partitions of time series variables by clustering multiple correlation coefficients, then executes anomaly detection algorithms locally on each subset while maintaining inter-variable relations.

Result: Experiments on synthetic and real datasets show significant improvement in anomaly detection performance compared to existing methods.

Conclusion: The PARADISE approach is relevant and effective for multivariate time series anomaly detection, demonstrating substantial performance gains through its partition-based methodology.

Abstract: In this article, we suggest a novel non-supervised partition based anomaly
detection method for anomaly detection in multivariate time series called
PARADISE. This methodology creates a partition of the variables of the time
series while ensuring that the inter-variable relations remain untouched. This
partitioning relies on the clustering of multiple correlation coefficients
between variables to identify subsets of variables before executing anomaly
detection algorithms locally for each of those subsets. Through multiple
experimentations done on both synthetic and real datasets coming from the
literature, we show the relevance of our approach with a significant
improvement in anomaly detection performance.

</details>


### [123] [Evaluating Double Descent in Machine Learning: Insights from Tree-Based Models Applied to a Genomic Prediction Task](https://arxiv.org/abs/2509.25216)
*Guillermo Comesaña Cimadevila*

Main category: cs.LG

TL;DR: Double descent phenomenon emerges only when model complexity is scaled jointly across learner capacity and ensemble size axes, otherwise reverting to classical U/L-shaped patterns.


<details>
  <summary>Details</summary>
Motivation: To investigate claims of double descent in simpler models like decision trees and gradient boosting, and understand the conditions under which this phenomenon occurs.

Method: Systematically vary model complexity along two orthogonal axes (learner capacity and ensemble size) using biological classification task of predicting isoniazid resistance in Mycobacterium tuberculosis with whole-genome sequencing data.

Result: Double descent consistently emerges only when complexity is scaled jointly across both axes. When either axis is held fixed, generalisation behaviour reverts to classical U- or L-shaped patterns.

Conclusion: Model complexity should be treated as a multidimensional construct when analysing generalisation behaviour, supporting the unfolding hypothesis that attributes double descent to projection of distinct regimes onto a single complexity axis.

Abstract: Classical learning theory describes a well-characterised U-shaped
relationship between model complexity and prediction error, reflecting a
transition from underfitting in underparameterised regimes to overfitting as
complexity grows. Recent work, however, has introduced the notion of a second
descent in test error beyond the interpolation threshold-giving rise to the
so-called double descent phenomenon. While double descent has been studied
extensively in the context of deep learning, it has also been reported in
simpler models, including decision trees and gradient boosting. In this work,
we revisit these claims through the lens of classical machine learning applied
to a biological classification task: predicting isoniazid resistance in
Mycobacterium tuberculosis using whole-genome sequencing data. We
systematically vary model complexity along two orthogonal axes-learner capacity
(e.g., Pleaf, Pboost) and ensemble size (i.e., Pens)-and show that double
descent consistently emerges only when complexity is scaled jointly across
these axes. When either axis is held fixed, generalisation behaviour reverts to
classical U- or L-shaped patterns. These results are replicated on a synthetic
benchmark and support the unfolding hypothesis, which attributes double descent
to the projection of distinct generalisation regimes onto a single complexity
axis. Our findings underscore the importance of treating model complexity as a
multidimensional construct when analysing generalisation behaviour. All code
and reproducibility materials are available at:
https://github.com/guillermocomesanacimadevila/Demystifying-Double-Descent-in-ML.

</details>


### [124] [Learning to Condition: A Neural Heuristic for Scalable MPE Inference](https://arxiv.org/abs/2509.25217)
*Brij Malhotra,Shivvrat Arya,Tahrima Rahman,Vibhav Giridhar Gogate*

Main category: cs.LG

TL;DR: L2C is a data-driven framework that trains neural networks to accelerate MPE inference in PGMs by learning effective conditioning strategies from solver search traces.


<details>
  <summary>Details</summary>
Motivation: MPE inference in PGMs is fundamentally intractable, and existing methods need acceleration through better conditioning strategies.

Method: Train neural networks to score variable-value assignments for conditioning using supervised learning from solver search traces, then integrate as heuristic with search algorithms.

Result: Significantly reduces search space while maintaining or improving solution quality over state-of-the-art methods on high-treewidth PGMs.

Conclusion: L2C provides an effective data-driven approach for accelerating MPE inference through learned conditioning heuristics.

Abstract: We introduce learning to condition (L2C), a scalable, data-driven framework
for accelerating Most Probable Explanation (MPE) inference in Probabilistic
Graphical Models (PGMs), a fundamentally intractable problem. L2C trains a
neural network to score variable-value assignments based on their utility for
conditioning, given observed evidence. To facilitate supervised learning, we
develop a scalable data generation pipeline that extracts training signals from
the search traces of existing MPE solvers. The trained network serves as a
heuristic that integrates with search algorithms, acting as a conditioning
strategy prior to exact inference or as a branching and node selection policy
within branch-and-bound solvers. We evaluate L2C on challenging MPE queries
involving high-treewidth PGMs. Experiments show that our learned heuristic
significantly reduces the search space while maintaining or improving solution
quality over state-of-the-art methods.

</details>


### [125] [On The Dynamic Ensemble Selection for TinyML-based Systems -- a Preliminary Study](https://arxiv.org/abs/2509.25218)
*Tobiasz Puslecki,Krzysztof Walkowiak*

Main category: cs.LG

TL;DR: This paper explores Dynamic Ensemble Selection (DES) with clustering for multi-class computer vision tasks in TinyML systems, showing that larger classifier pools improve accuracy but increase inference time.


<details>
  <summary>Details</summary>
Motivation: The need to balance inference time and classification quality in TinyML systems with computational, memory, and energy constraints drives the exploration of specialized optimization techniques beyond deep neural networks.

Method: Implemented a DES-Clustering approach using the TinyDES-Clustering library optimized for embedded system limitations, allowing adjustment of classification accuracy which affects latency and energy consumption.

Result: Experiments showed that larger pools of classifiers for dynamic selection improve classification accuracy but lead to increased average inference time on TinyML devices.

Conclusion: DES-Clustering methods provide a viable approach for TinyML systems, offering a trade-off between accuracy and inference time that can be adjusted based on application requirements.

Abstract: The recent progress in TinyML technologies triggers the need to address the
challenge of balancing inference time and classification quality. TinyML
systems are defined by specific constraints in computation, memory and energy.
These constraints emphasize the need for specialized optimization techniques
when implementing Machine Learning (ML) applications on such platforms. While
deep neural networks are widely used in TinyML, the exploration of Dynamic
Ensemble Selection (DES) methods is also beneficial. This study examines a
DES-Clustering approach for a multi-class computer vision task within TinyML
systems. This method allows for adjusting classification accuracy, thereby
affecting latency and energy consumption per inference. We implemented the
TinyDES-Clustering library, optimized for embedded system limitations.
Experiments have shown that a larger pool of classifiers for dynamic selection
improves classification accuracy, and thus leads to an increase in average
inference time on the TinyML device.

</details>


### [126] [Sensor optimization for urban wind estimation with cluster-based probabilistic framework](https://arxiv.org/abs/2509.25222)
*Yutong Liang,Chang Hou,Guy Y. Cornejo Maceda,Andrea Ianiro,Stefano Discetti,Andrea Meilán-Vila,Didier Sornette,Sandro Claudio Lera,Jialong Chen,Xiaozhou He,Bernd R. Noack*

Main category: cs.LG

TL;DR: A physics-informed machine learning framework for sensor-based flow estimation in urban drone navigation, featuring domain decomposition, Reynolds scaling, and sensor optimization for minimal uncertainty.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable flow estimation method for drone trajectories in complex urban environments that can handle flows too complex for traditional reduced-order models and allows flexible sensor placement.

Method: Uses Reynolds number-based scaling, physics-based domain decomposition, cluster-based flow representation for subdomains, information entropy correlation, and multi-variate probability functions to relate sensor inputs to velocity estimates.

Result: Successfully demonstrated on drone flight paths through a three-building cluster, showing ability to extrapolate beyond training data and handle various wind conditions.

Conclusion: The framework provides a scalable solution for urban flow estimation with potential applications for complete city modeling and weather integration.

Abstract: We propose a physics-informed machine-learned framework for sensor-based flow
estimation for drone trajectories in complex urban terrain. The input is a rich
set of flow simulations at many wind conditions. The outputs are velocity and
uncertainty estimates for a target domain and subsequent sensor optimization
for minimal uncertainty. The framework has three innovations compared to
traditional flow estimators. First, the algorithm scales proportionally to the
domain complexity, making it suitable for flows that are too complex for any
monolithic reduced-order representation. Second, the framework extrapolates
beyond the training data, e.g., smaller and larger wind velocities. Last, and
perhaps most importantly, the sensor location is a free input, significantly
extending the vast majority of the literature. The key enablers are (1) a
Reynolds number-based scaling of the flow variables, (2) a physics-based domain
decomposition, (3) a cluster-based flow representation for each subdomain, (4)
an information entropy correlating the subdomains, and (5) a multi-variate
probability function relating sensor input and targeted velocity estimates.
This framework is demonstrated using drone flight paths through a
three-building cluster as a simple example. We anticipate adaptations and
applications for estimating complete cities and incorporating weather input.

</details>


### [127] [Enhancing Linear Attention with Residual Learning](https://arxiv.org/abs/2509.25223)
*Xunhao Lai,Jialiang Kang,Jianqiao Lu,Tong Lin,Pengyu Zhao*

Main category: cs.LG

TL;DR: RLA introduces a residual-fitting mechanism to linear attention, addressing its expressivity bottleneck by maintaining an auxiliary recurrent state that accumulates residual errors over time to correct base predictions.


<details>
  <summary>Details</summary>
Motivation: Linear attention struggles to capture long-range patterns due to being limited to historical prediction plus single-token correction, creating an expressivity bottleneck.

Method: Residual Linear Attention (RLA) framework with explicit residual-fitting mechanism using auxiliary recurrent state; instantiated as Residual Delta Net (RDN) with adaptive gating and residual clipping.

Result: RLA and RDN consistently outperform baseline linear attention methods and other modern linear-attention approaches across language modeling and recall-intensive evaluations.

Conclusion: The proposed methods narrow the performance gap to standard Transformers while maintaining linear time and memory scaling.

Abstract: Linear attention offers a linear-time alternative to self-attention but often
struggles to capture long-range patterns. We revisit linear attention through a
prediction-correction lens and show that prevalent variants can be written as a
combination of a historical prediction and a single-token correction, which
creates an expressivity bottleneck. To address this bottleneck, we introduce
Residual Linear Attention (RLA), a framework that equips linear attention with
an explicit residual-fitting mechanism. RLA maintains an auxiliary recurrent
state that learns to accumulate residual errors over time and correct the base
prediction. We further instantiate a delta-rule version, Residual Delta Net
(RDN), incorporating adaptive gating and residual clipping for enhanced
correction control and stability. Our implementation leverages highly optimized
linear attention kernels and preserves linear time and memory. Across language
modeling and recall-intensive evaluations, RLA and RDN consistently outperform
their respective baselines and other modern linear-attention methods, narrowing
the gap to standard Transformers while retaining linear scaling.

</details>


### [128] [AMLA: MUL by ADD in FlashAttention Rescaling](https://arxiv.org/abs/2509.25224)
*Qichen Liao,Chengqiu Hu,Fangzheng Miao,Bao Li,Yiyang Liu,Junlong Lyu,Lirui Jiang,Jun Wang,Lingchao Zheng,Jun Li,Yuwei Fan*

Main category: cs.LG

TL;DR: AMLA is a high-performance kernel optimized for Huawei's Ascend NPUs that addresses the computational overhead of Multi-head Latent Attention through FlashAttention-based algorithms and Preload Pipeline strategies, achieving up to 614 TFLOPS and 86.8% FLOPS utilization.


<details>
  <summary>Details</summary>
Motivation: Multi-head Latent Attention reduces KVCache memory usage but introduces significant computational overhead and intermediate variable expansion, creating challenges for efficient hardware implementation during decode phase.

Method: Two core innovations: (1) FlashAttention-based algorithm replacing floating-point multiplications with integer additions for output block rescaling using binary correspondence between FP32 and INT32; (2) Preload Pipeline strategy with hierarchical tiling that maximizes FLOPS utilization through Cube-bound performance and overlapping data movement with computation.

Result: On Ascend 910 NPUs, AMLA achieves up to 614 TFLOPS, reaching 86.8% of theoretical maximum FLOPS, outperforming state-of-the-art FlashMLA implementation (66.7% FLOPS utilization on NVIDIA H800 SXM5).

Conclusion: AMLA kernel has been successfully integrated into Huawei's CANN and will be released soon, demonstrating superior performance for MLA optimization on specialized hardware.

Abstract: Multi-head Latent Attention (MLA) significantly reduces KVCache memory usage
in Large Language Models while introducing substantial computational overhead
and intermediate variable expansion. This poses challenges for efficient
hardware implementation -- especially during the decode phase. This paper
introduces Ascend MLA (AMLA), a high-performance kernel specifically optimized
for Huawei's Ascend NPUs. AMLA is built on two core innovations: (1) A novel
FlashAttention-based algorithm that replaces floating-point multiplications
with integer additions for output block rescaling, leveraging binary
correspondence between FP32 and INT32 representations; (2) A Preload Pipeline
strategy with hierarchical tiling that maximizes FLOPS utilization: the Preload
Pipeline achieves Cube-bound performance, while hierarchical tiling overlaps
data movement and computation within the Cube core. Experiments show that on
Ascend 910 NPUs (integrated in CloudMatrix384), AMLA achieves up to 614 TFLOPS,
reaching 86.8% of the theoretical maximum FLOPS, outperforming the
state-of-the-art open-source FlashMLA implementation, whose FLOPS utilization
is up to 66.7% on NVIDIA H800 SXM5. The AMLA kernel has been integrated into
Huawei's CANN and will be released soon.

</details>


### [129] [MSCoD: An Enhanced Bayesian Updating Framework with Multi-Scale Information Bottleneck and Cooperative Attention for Structure-Based Drug Design](https://arxiv.org/abs/2509.25225)
*Long Xu,Yongcai Chen,Fengshuo Liu,Yuzhong Peng*

Main category: cs.LG

TL;DR: MSCoD is a Bayesian updating-based generative framework for structure-based drug design that uses multi-scale information bottleneck and asymmetric attention mechanisms to capture hierarchical protein-ligand interactions.


<details>
  <summary>Details</summary>
Motivation: Current SBDD methods struggle to capture complex protein-ligand interactions across multiple scales and often overlook hierarchical organization and intrinsic asymmetry of these interactions.

Method: Developed Multi-Scale Information Bottleneck (MSIB) for semantic compression at multiple abstraction levels, and multi-head cooperative attention (MHCA) with asymmetric protein-to-ligand attention to handle dimensionality disparity.

Result: MSCoD outperforms state-of-the-art methods on benchmark datasets and shows strong applicability in real-world scenarios like KRAS G12D targets.

Conclusion: MSCoD provides an effective framework for hierarchical feature extraction and asymmetric interaction modeling in structure-based drug design, with demonstrated superior performance.

Abstract: Structure-Based Drug Design (SBDD) is a powerful strategy in computational
drug discovery, utilizing three-dimensional protein structures to guide the
design of molecules with improved binding affinity. However, capturing complex
protein-ligand interactions across multiple scales remains challenging, as
current methods often overlook the hierarchical organization and intrinsic
asymmetry of these interactions. To address these limitations, we propose
MSCoD, a novel Bayesian updating-based generative framework for structure-based
drug design. In our MSCoD, Multi-Scale Information Bottleneck (MSIB) was
developed, which enables semantic compression at multiple abstraction levels
for efficient hierarchical feature extraction. Furthermore, a multi-head
cooperative attention (MHCA) mechanism was developed, which employs asymmetric
protein-to-ligand attention to capture diverse interaction types while
addressing the dimensionality disparity between proteins and ligands. Empirical
studies showed that MSCoD outperforms state-of-the-art methods on the benchmark
dataset. Case studies on challenging targets such as KRAS G12D further
demonstrate its applicability in real-world scenarios. The code and data
underlying this article are freely available at
https://github.com/xulong0826/MSCoD.

</details>


### [130] [Integrated Forecasting of Marine Renewable Power: An Adaptively Bayesian-Optimized MVMD-LSTM Framework for Wind-Solar-Wave Energy](https://arxiv.org/abs/2509.25226)
*Baoyi Xie,Shuiling Shi,Wenqi Liu*

Main category: cs.LG

TL;DR: A Bayesian-optimized MVMD-LSTM framework for ultra-short-term forecasting of integrated wind-solar-wave marine energy systems, achieving superior accuracy and automation compared to benchmark models.


<details>
  <summary>Details</summary>
Motivation: Existing forecasting methods for marine energy systems have limitations: they use separate models for each energy source, fail to capture complex cross-source couplings, struggle with nonlinear dynamics, and require extensive manual parameter tuning, which constrains predictive performance and practicality.

Method: Proposes a Bayesian-optimized MVMD-LSTM framework: 1) Uses MVMD to jointly decompose wind, solar and wave power series while preserving cross-source couplings; 2) Employs Bayesian optimization to automatically determine the number of modes and penalty parameter in MVMD; 3) Uses LSTM to model the resulting IMFs for ultra-short-term power forecasting.

Result: Experiments using field measurements from an offshore integrated energy platform in China show the proposed framework significantly outperforms benchmark models in terms of MAPE, RMSE and MAE metrics.

Conclusion: The framework demonstrates superior predictive accuracy, robustness, and degree of automation for integrated wind-solar-wave marine energy system forecasting.

Abstract: Integrated wind-solar-wave marine energy systems hold broad promise for
supplying clean electricity in offshore and coastal regions. By leveraging the
spatiotemporal complementarity of multiple resources, such systems can
effectively mitigate the intermittency and volatility of single-source outputs,
thereby substantially improving overall power-generation efficiency and
resource utilization. Accurate ultra-short-term forecasting is crucial for
ensuring secure operation and optimizing proactive dispatch. However, most
existing forecasting methods construct separate models for each energy source,
insufficiently account for the complex couplings among multiple energies,
struggle to capture the system's nonlinear and nonstationary dynamics, and
typically depend on extensive manual parameter tuning-limitations that
constrain both predictive performance and practicality. We address this issue
using a Bayesian-optimized Multivariate Variational Mode Decomposition-Long
Short-Term Memory (MVMD-LSTM) framework. The framework first applies MVMD to
jointly decompose wind, solar and wave power series so as to preserve
cross-source couplings; it uses Bayesian optimization to automatically search
the number of modes and the penalty parameter in the MVMD process to obtain
intrinsic mode functions (IMFs); finally, an LSTM models the resulting IMFs to
achieve ultra-short-term power forecasting for the integrated system.
Experiments based on field measurements from an offshore integrated energy
platform in China show that the proposed framework significantly outperforms
benchmark models in terms of MAPE, RMSE and MAE. The results demonstrate
superior predictive accuracy, robustness, and degree of automation.

</details>


### [131] [Simple, Fast and Efficient Injective Manifold Density Estimation with Random Projections](https://arxiv.org/abs/2509.25228)
*Ahmad Ayaz Amin*

Main category: cs.LG

TL;DR: Random Projection Flows (RPFs) are injective normalizing flows using random semi-orthogonal matrices from Haar-distributed orthogonal ensembles for efficient data projection to lower-dimensional latent spaces with closed-form Riemannian volume correction.


<details>
  <summary>Details</summary>
Motivation: To create a principled framework for injective normalizing flows that bridges random projection theory with normalizing flows, offering plug-and-play efficiency and theoretical grounding.

Method: Use random semi-orthogonal matrices from Haar-distributed orthogonal ensembles (via QR decomposition of Gaussian matrices) to project data into lower-dimensional latent spaces for base distribution, with closed-form Riemannian volume correction.

Result: RPFs are shown to be both theoretically grounded and practically effective, providing a strong baseline for generative modeling.

Conclusion: RPFs successfully bridge random projection theory and normalizing flows, offering an efficient, plug-and-play framework with theoretical guarantees for injective normalizing flows.

Abstract: We introduce Random Projection Flows (RPFs), a principled framework for
injective normalizing flows that leverages tools from random matrix theory and
the geometry of random projections. RPFs employ random semi-orthogonal
matrices, drawn from Haar-distributed orthogonal ensembles via QR decomposition
of Gaussian matrices, to project data into lower-dimensional latent spaces for
the base distribution. Unlike PCA-based flows or learned injective maps, RPFs
are plug-and-play, efficient, and yield closed-form expressions for the
Riemannian volume correction term. We demonstrate that RPFs are both
theoretically grounded and practically effective, providing a strong baseline
for generative modeling and a bridge between random projection theory and
normalizing flows.

</details>


### [132] [Fine-tuning of Large Language Models for Domain-Specific Cybersecurity Knowledge](https://arxiv.org/abs/2509.25241)
*Yuan Huang*

Main category: cs.LG

TL;DR: Fine-tuning LLMs with SFT, LoRA, and QLoRA significantly improves cybersecurity Q&A performance while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: LLMs have suboptimal zero-shot performance in specialized domains like cybersecurity due to their general-purpose design, limiting domain-specific expertise.

Method: Investigated Supervised Fine-Tuning (SFT), Low-Rank Adaptation (LoRA), and Quantized Low-Rank Adaptation (QLoRA) using cybersecurity Q&A dataset.

Result: All fine-tuning approaches significantly outperformed foundational model; LoRA and QLoRA achieved comparable performance to SFT with much lower computational costs.

Conclusion: Low-rank fine-tuning strategies effectively bridge the gap between general-purpose LLMs and domain-specific applications in cybersecurity.

Abstract: Recent advancements in training paradigms for Large Language Models (LLMs)
have unlocked their remarkable capabilities in natural language processing and
cross-domain generalization. While LLMs excel in tasks like programming and
mathematical problem-solving, their zero-shot performance in specialized
domains requiring expert knowledge, such as cybersecurity, is often suboptimal.
This limitation arises because foundational LLMs are designed for
general-purpose applications, constraining their ability to encapsulate
domain-specific expertise within their parameter space. To address this, we
explore fine-tuning strategies to embed cybersecurity knowledge into LLMs,
enhancing their performance in cybersecurity question-answering (Q\&A) tasks
while prioritizing computational efficiency. Specifically, we investigate
Supervised Fine-Tuning (SFT), Low-Rank Adaptation (LoRA), and Quantized
Low-Rank Adaptation (QLoRA) using a cybersecurity Q\&A dataset. Our results
demonstrate that these fine-tuning approaches significantly outperform the
foundational model in cybersecurity Q\&A tasks. Moreover, LoRA and QLoRA
achieve comparable performance to SFT with substantially lower computational
costs, offering an efficient pathway for adapting LLMs to specialized domains.
Our work highlights the potential of low-rank fine-tuning strategies to bridge
the gap between general-purpose LLMs and domain-specific applications.

</details>


### [133] [Energy Guided Geometric Flow Matching](https://arxiv.org/abs/2509.25230)
*Aaron Zweig,Mingxuan Zhang,Elham Azizi,David Knowles*

Main category: cs.LG

TL;DR: The paper proposes using score matching and annealed energy distillation to learn a metric tensor that captures data geometry for more accurate flow matching on temporal data.


<details>
  <summary>Details</summary>
Motivation: Traditional flow matching uses straight paths that may not follow the data manifold, and existing geodesic methods suffer from dimensionality issues with RBF kernels or nearest neighbor graphs.

Method: Use score matching and annealed energy distillation to learn a metric tensor that faithfully represents the underlying data geometry.

Result: The method is demonstrated on synthetic manifolds with analytic geodesics and cell interpolation tasks.

Conclusion: The proposed approach effectively captures data geometry and enables more accurate flow matching for temporal data.

Abstract: A useful inductive bias for temporal data is that trajectories should stay
close to the data manifold. Traditional flow matching relies on straight
conditional paths, and flow matching methods which learn geodesics rely on RBF
kernels or nearest neighbor graphs that suffer from the curse of
dimensionality. We propose to use score matching and annealed energy
distillation to learn a metric tensor that faithfully captures the underlying
data geometry and informs more accurate flows. We demonstrate the efficacy of
this strategy on synthetic manifolds with analytic geodesics, and interpolation
of cell

</details>


### [134] [Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification](https://arxiv.org/abs/2509.26032)
*Xiaobao Wang,Ruoxiao Sun,Yujun Zhang,Bingdao Feng,Dongxiao He,Luzhi Wang,Di Jin*

Main category: cs.LG

TL;DR: DPSBA is a clean-label backdoor attack framework for graph classification that learns in-distribution triggers via adversarial training to achieve high attack success while maintaining stealth against anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Existing graph-level backdoor attacks suffer from structural deviation (rare subgraph triggers) and semantic deviation (label flipping), making poisoned graphs easily detectable by anomaly detection models.

Method: Proposes DPSBA framework that learns in-distribution triggers through adversarial training guided by anomaly-aware discriminators to suppress both structural and semantic anomalies.

Result: Extensive experiments show DPSBA achieves superior balance between attack effectiveness and detectability compared to state-of-the-art baselines, with high attack success while significantly improving stealth.

Conclusion: DPSBA effectively addresses the stealth limitations of existing graph classification backdoor methods by learning in-distribution triggers that avoid detection while maintaining high attack performance.

Abstract: Graph Neural Networks (GNNs) have demonstrated strong performance across
tasks such as node classification, link prediction, and graph classification,
but remain vulnerable to backdoor attacks that implant imperceptible triggers
during training to control predictions. While node-level attacks exploit local
message passing, graph-level attacks face the harder challenge of manipulating
global representations while maintaining stealth. We identify two main sources
of anomaly in existing graph classification backdoor methods: structural
deviation from rare subgraph triggers and semantic deviation caused by label
flipping, both of which make poisoned graphs easily detectable by anomaly
detection models. To address this, we propose DPSBA, a clean-label backdoor
framework that learns in-distribution triggers via adversarial training guided
by anomaly-aware discriminators. DPSBA effectively suppresses both structural
and semantic anomalies, achieving high attack success while significantly
improving stealth. Extensive experiments on real-world datasets validate that
DPSBA achieves a superior balance between effectiveness and detectability
compared to state-of-the-art baselines.

</details>


### [135] [WDformer: A Wavelet-based Differential Transformer Model for Time Series Forecasting](https://arxiv.org/abs/2509.25231)
*Xiaojian Wang,Chaoli Zhang,Zhonglong Zheng,Yunliang Jiang*

Main category: cs.LG

TL;DR: WDformer is a wavelet-based differential Transformer model that uses multi-resolution wavelet analysis and differential attention mechanism to improve time series forecasting by better capturing time-frequency domain information and reducing noise.


<details>
  <summary>Details</summary>
Motivation: Traditional time series forecasting methods have limitations in fully utilizing multi-domain information due to data sparsity, and standard attention mechanisms tend to over-focus on irrelevant historical information, introducing noise and bias into predictions.

Method: The model employs wavelet transform for multi-resolution analysis of time series data, uses inverted dimension attention to capture relationships between multiple variables, and introduces differential attention mechanism that computes attention scores from differences between two separate softmax attention matrices.

Result: WDformer achieved state-of-the-art (SOTA) results on multiple challenging real-world datasets, demonstrating superior accuracy and effectiveness in time series forecasting tasks.

Conclusion: The proposed WDformer model successfully addresses limitations of traditional approaches by leveraging wavelet-based time-frequency analysis and differential attention mechanism, providing an effective solution for accurate time series forecasting across various applications.

Abstract: Time series forecasting has various applications, such as meteorological
rainfall prediction, traffic flow analysis, financial forecasting, and
operational load monitoring for various systems. Due to the sparsity of time
series data, relying solely on time-domain or frequency-domain modeling limits
the model's ability to fully leverage multi-domain information. Moreover, when
applied to time series forecasting tasks, traditional attention mechanisms tend
to over-focus on irrelevant historical information, which may introduce noise
into the prediction process, leading to biased results. We proposed WDformer, a
wavelet-based differential Transformer model. This study employs the wavelet
transform to conduct a multi-resolution analysis of time series data. By
leveraging the advantages of joint representation in the time-frequency domain,
it accurately extracts the key information components that reflect the
essential characteristics of the data. Furthermore, we apply attention
mechanisms on inverted dimensions, allowing the attention mechanism to capture
relationships between multiple variables. When performing attention
calculations, we introduced the differential attention mechanism, which
computes the attention score by taking the difference between two separate
softmax attention matrices. This approach enables the model to focus more on
important information and reduce noise. WDformer has achieved state-of-the-art
(SOTA) results on multiple challenging real-world datasets, demonstrating its
accuracy and effectiveness. Code is available at
https://github.com/xiaowangbc/WDformer.

</details>


### [136] [SPATA: Systematic Pattern Analysis for Detailed and Transparent Data Cards](https://arxiv.org/abs/2509.26640)
*João Vitorino,Eva Maia,Isabel Praça,Carlos Soares*

Main category: cs.LG

TL;DR: SPATA is a deterministic method that converts tabular datasets into domain-independent statistical pattern representations to enable external AI robustness evaluation without disclosing private data.


<details>
  <summary>Details</summary>
Motivation: To address the need for external verification of AI models while protecting data privacy, especially for organizations handling confidential data or critical infrastructure.

Method: Systematic Pattern Analysis (SPATA) projects data instances into a discrete space representing statistical patterns, creating domain-independent representations that prevent data leakage.

Result: SPATA enables reliable evaluation of feature effects on ML model robustness and generation of interpretable explanations without exposing private datasets.

Conclusion: SPATA contributes to more trustworthy AI by providing transparent data cards and enabling external validation while maintaining data confidentiality.

Abstract: Due to the susceptibility of Artificial Intelligence (AI) to data
perturbations and adversarial examples, it is crucial to perform a thorough
robustness evaluation before any Machine Learning (ML) model is deployed.
However, examining a model's decision boundaries and identifying potential
vulnerabilities typically requires access to the training and testing datasets,
which may pose risks to data privacy and confidentiality. To improve
transparency in organizations that handle confidential data or manage critical
infrastructure, it is essential to allow external verification and validation
of AI without the disclosure of private datasets. This paper presents
Systematic Pattern Analysis (SPATA), a deterministic method that converts any
tabular dataset to a domain-independent representation of its statistical
patterns, to provide more detailed and transparent data cards. SPATA computes
the projection of each data instance into a discrete space where they can be
analyzed and compared, without risking data leakage. These projected datasets
can be reliably used for the evaluation of how different features affect ML
model robustness and for the generation of interpretable explanations of their
behavior, contributing to more trustworthy AI.

</details>


### [137] [Sampling via Gaussian Mixture Approximations](https://arxiv.org/abs/2509.25232)
*Yongchao Huang*

Main category: cs.LG

TL;DR: Gaussian Mixture Approximation (GMA) samplers provide a gradient-free, computationally efficient method for sampling unnormalized target densities using a two-stage approach of proposal sampling followed by mixture optimization and stratified resampling.


<details>
  <summary>Details</summary>
Motivation: To develop efficient sampling methods for unnormalized target densities that don't require gradient information and can leverage the simplicity of Gaussian sampling.

Method: Two-stage paradigm: (1) initialize Gaussian components and sample from proposal mixture, (2) fit mixture to target by optimizing weights/means/variances using sample-based KL divergence, then perform stratified resampling.

Result: Method produces consistent approximations under mild conditions and demonstrates accuracy and speed across diverse densities in empirical validation.

Conclusion: GMA samplers offer a practical, gradient-free approach for efficient sampling from complex target distributions using Gaussian mixture approximations.

Abstract: We present a family of \textit{Gaussian Mixture Approximation} (GMA) samplers
for sampling unnormalised target densities, encompassing \textit{weights-only
GMA} (W-GMA), \textit{Laplace Mixture Approximation} (LMA),
\textit{expectation-maximization GMA} (EM-GMA), and further variants. GMA
adopts a simple two-stage paradigm: (i) initialise a finite set of Gaussian
components and draw samples from a proposal mixture; (ii) fit the mixture to
the target by optimising either only the component weights or also the means
and variances, via a sample-based KL divergence objective that requires only
evaluations of the unnormalised density, followed by stratified resampling. The
method is gradient-free, and computationally efficient: it leverages the ease
of sampling from Gaussians, efficient optimisation methods (projected gradient
descent, mirror descent, and EM), and the robustness of stratified resampling
to produce samples faithful to the target. We show that this
optimisation-resampling scheme yields consistent approximations under mild
conditions, and we validate this methodology with empirical results
demonstrating accuracy and speed across diverse densities.

</details>


### [138] [FedCLF - Towards Efficient Participant Selection for Federated Learning in Heterogeneous IoV Networks](https://arxiv.org/abs/2509.25233)
*Kasun Eranda Wijethilake,Adnan Mahmood,Quan Z. Sheng*

Main category: cs.LG

TL;DR: FedCLF is a federated learning method that uses calibrated loss and feedback control to handle data heterogeneity in IoV networks, improving accuracy by up to 16% while reducing resource usage.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces challenges in IoV networks due to high data and device heterogeneity, which affects model accuracy and resource efficiency.

Method: Proposes FedCLF with calibrated loss for participant selection and feedback control to dynamically adjust client sampling frequency.

Result: FedCLF outperforms FedAvg, Newt, and Oort by up to 16% in high heterogeneity scenarios with reduced sampling frequency.

Conclusion: FedCLF effectively addresses FL challenges in heterogeneous IoV networks, improving both accuracy and resource efficiency.

Abstract: Federated Learning (FL) is a distributed machine learning technique that
preserves data privacy by sharing only the trained parameters instead of the
client data. This makes FL ideal for highly dynamic, heterogeneous, and
time-critical applications, in particular, the Internet of Vehicles (IoV)
networks. However, FL encounters considerable challenges in such networks owing
to the high data and device heterogeneity. To address these challenges, we
propose FedCLF, i.e., FL with Calibrated Loss and Feedback control, which
introduces calibrated loss as a utility in the participant selection process
and a feedback control mechanism to dynamically adjust the sampling frequency
of the clients. The envisaged approach (a) enhances the overall model accuracy
in case of highly heterogeneous data and (b) optimizes the resource utilization
for resource constrained IoV networks, thereby leading to increased efficiency
in the FL process. We evaluated FedCLF vis-\`a-vis baseline models, i.e.,
FedAvg, Newt, and Oort, using CIFAR-10 dataset with varying data heterogeneity.
Our results depict that FedCLF significantly outperforms the baseline models by
up to a 16% improvement in high data heterogeneity-related scenarios with
improved efficiency via reduced sampling frequency.

</details>


### [139] [Machine Learning for Pattern Detection in Printhead Nozzle Logging](https://arxiv.org/abs/2509.25235)
*Nikola Prianikov,Evelyne Janssen-van Dam,Marcin Pietrasik,Charalampos S. Kouzinopoulos*

Main category: cs.LG

TL;DR: Machine Learning approach for classifying printhead failure mechanisms using nozzle behavior patterns, outperforming rule-based baseline with Random Forest classifier.


<details>
  <summary>Details</summary>
Motivation: Accurate identification of failure mechanisms is crucial for product quality assurance in printhead manufacturing, as nozzle failures form distinct temporal and spatial patterns.

Method: Feature-based time-series classification using domain-expert selected time-based and spatial features from nozzle logging data, evaluated with traditional ML classifiers including One-vs-Rest Random Forest.

Result: One-vs-Rest Random Forest achieved best performance, outperforming in-house rule-based baseline in weighted F1 score for several failure mechanisms.

Conclusion: Machine Learning classification approach effectively identifies printhead failure mechanisms from nozzle behavior patterns, providing better performance than traditional rule-based methods.

Abstract: Correct identification of failure mechanisms is essential for manufacturers
to ensure the quality of their products. Certain failures of printheads
developed by Canon Production Printing can be identified from the behavior of
individual nozzles, the states of which are constantly recorded and can form
distinct patterns in terms of the number of failed nozzles over time, and in
space in the nozzle grid. In our work, we investigate the problem of printhead
failure classification based on a multifaceted dataset of nozzle logging and
propose a Machine Learning classification approach for this problem. We follow
the feature-based framework of time-series classification, where a set of
time-based and spatial features was selected with the guidance of domain
experts. Several traditional ML classifiers were evaluated, and the One-vs-Rest
Random Forest was found to have the best performance. The proposed model
outperformed an in-house rule-based baseline in terms of a weighted F1 score
for several failure mechanisms.

</details>


### [140] [PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases](https://arxiv.org/abs/2509.25238)
*Sri Vatsa Vuddanti,Aarav Shah,Satwik Kumar Chittiprolu,Tony Song,Sunishchal Dev,Kevin Zhu,Maheep Chaudhary*

Main category: cs.LG

TL;DR: PALADIN is a framework that trains language agents to recover from tool failures (timeouts, API exceptions, inconsistent outputs) using systematic failure injection and expert demonstrations, achieving significant improvements in recovery rates.


<details>
  <summary>Details</summary>
Motivation: Tool-augmented language agents often fail in real-world deployment due to tool malfunctions, triggering cascading errors and task abandonment. Existing training only optimizes for success, not exposing models to the failures that dominate real usage.

Method: PALADIN trains on 50,000+ recovery-annotated trajectories via systematic failure injection and expert demonstrations on ToolBench dataset. Uses LoRA-based fine-tuning to retain base capabilities while adding recovery competence. At inference, detects errors and retrieves similar cases from 55+ failure exemplars aligned with ToolScan's taxonomy.

Result: PALADIN improves Recovery Rate from 32.76% to 89.68% (+57% relative) over ToolBench, outperforms CRITIC (76.34%) by +13.3%, and achieves 89.86% RR (+66% improvement from vanilla agents' 23.75%). Retains 95.2% recovery performance on unseen tool APIs.

Conclusion: PALADIN establishes an effective method for building fault-tolerant agents capable of robust recovery in real-world tool environments, demonstrating consistent improvements across multiple evaluation metrics.

Abstract: Tool-augmented language agents frequently fail in real-world deployment due
to tool malfunctions--timeouts, API exceptions, or inconsistent
outputs--triggering cascading reasoning errors and task abandonment. Existing
agent training pipelines optimize only for success trajectories, failing to
expose models to the tool failures that dominate real-world usage. We propose
\textbf{PALADIN}, a generalizable framework for equipping language agents with
robust failure recovery capabilities. PALADIN trains on 50,000+
recovery-annotated trajectories constructed via systematic failure injection
and expert demonstrations on an enhanced ToolBench dataset. Training uses
LoRA-based fine-tuning to retain base capabilities while injecting recovery
competence. At inference, PALADIN detects execution-time errors and retrieves
the most similar case from a curated bank of 55+ failure exemplars aligned with
ToolScan's taxonomy, then executes the corresponding recovery action. This
approach generalizes to novel failures beyond the training distribution,
retaining 95.2\% recovery performance on unseen tool APIs. Evaluation across
PaladinEval and ToolReflectEval demonstrates consistent improvements in
Recovery Rate (RR), Task Success Rate (TSR), Catastrophic Success Rate (CSR),
and Efficiency Score (ES). PALADIN improves RR from 32.76% to 89.68% (+57%
relative) over ToolBench and outperforms the strongest baseline CRITIC (76.34%)
by +13.3%. Against vanilla agents, PALADIN achieves 89.86\% RR (+66% relative
improvement from 23.75%). These results establish PALADIN as an effective
method for building fault-tolerant agents capable of robust recovery in
real-world tool environments.

</details>


### [141] [HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement](https://arxiv.org/abs/2509.25240)
*Ming Yang,Xiaofan Li,Zhiyuan Ma,Dengliang Shi,Jintao Du,Yu Cheng,Weiguo Zheng*

Main category: cs.LG

TL;DR: HAMMER is a curriculum RL method for LLMs that uses diversity metrics instead of difficulty-based ordering to maintain exploration during training, achieving 3-4% accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Traditional difficulty-based curriculum RL for LLMs suffers from local optimization where early training on simple samples reduces exploration capability.

Method: Proposes HAMMER framework that transfers diversity metrics from dataset evaluation to RL training, ordering samples via minimum-semantic Hamiltonian path to retain exploration.

Result: Empirical evaluations show HAMMER stimulates model curiosity and consistently achieves 3-4% average accuracy gain across diverse inference benchmarks.

Conclusion: Diversity-driven curriculum ordering facilitates stable convergence and better performance compared to difficulty-based approaches in LLM reinforcement learning.

Abstract: Recent curriculum reinforcement learning for large language models (LLMs)
typically rely on difficulty-based annotations for data filtering and ordering.
However, such methods suffer from local optimization, where continual training
on simple samples in the early steps can cause the policy to lose its
exploration. We propose a novel schema, namely Hamiltonian curiosity augmented
large language model reinforcement (HAMMER), that transfers diversity metrics,
commonly used in dataset evaluation, into the dynamic reinforcement learning
procedure, where training samples are ordered via a minimum-semantic
Hamiltonian path making the initial training retrain more exploration. From a
theoretical perspective of generalization bounds, diversity-driven ordering
facilitates stable convergence. Empirical evaluations indicate that HAMMER
stimulates model "curiosity" and consistently achieves a 3% to 4% average
accuracy gain across diverse inference benchmark.

</details>


### [142] [Knowledge distillation through geometry-aware representational alignment](https://arxiv.org/abs/2509.25253)
*Prajjwal Bhattarai,Mohammad Amjad,Dmytro Zhylko,Tuka Alhanai*

Main category: cs.LG

TL;DR: This paper proposes a new feature-based knowledge distillation method using Procrustes distance and Feature Gram Matrix Frobenius norm, showing these better capture feature structure than existing methods like CKA and projection-based MSE, with 2% performance improvements on BERT and OPT models.


<details>
  <summary>Details</summary>
Motivation: Existing feature distillation methods fail to properly capture the feature structure of teacher models even under zero loss conditions, motivating the need for better geometric alignment approaches.

Method: The authors propose using Procrustes distance and Frobenius norm of Feature Gram Matrix as distillation losses, which better align with representational alignment measurement principles.

Result: The method achieves statistically significant improvement in distillation performance across BERT and OPT models in classification and instruction-following tasks by up to 2 percentage points.

Conclusion: Integrating proper feature geometry through Procrustes distance and Gram Matrix norms into distillation methods shows promising potential for better knowledge transfer.

Abstract: Knowledge distillation is a common paradigm for transferring capabilities
from larger models to smaller ones. While traditional distillation methods
leverage a probabilistic divergence over the output of the teacher and student
models, feature-based distillation methods often minimize variants of Euclidean
norms between the hidden layer representations. The main goal is for the
student to mimic the structure of the feature space of the teacher. In this
work, we theoretically show that existing feature distillation methods, such as
projection based mean squared loss or Centered Kernel Alignment (CKA), cannot
capture the feature structure, even under zero loss. We then motivate the use
of Procrustes distance and the Frobenius norm of Feature Gram Matrix, distances
already common in the context of measuring representational alignment, as
distillation losses. We show that feature distillation through our method
showcases statistically significant improvement in distillation performance
across language models families (BERT and OPT) in classification and
instruction-following tasks by up to 2 percentage points, showcasing the
potential of integrating feature geometry into existing distillation methods.

</details>


### [143] [Heterogeneous Multi-agent Collaboration in UAV-assisted Mobile Crowdsensing Networks](https://arxiv.org/abs/2509.25261)
*Xianyang Deng,Wenshuai Liu,Yaru FuB,Qi Zhu*

Main category: cs.LG

TL;DR: Proposes a joint optimization framework for UAV-assisted MCS that integrates time slot partition, resource allocation, and 3D trajectory planning using a novel MADRL algorithm with hybrid actor network to maximize processed sensing data.


<details>
  <summary>Details</summary>
Motivation: Address challenges in UAV-assisted mobile crowdsensing including spectrum scarcity, device heterogeneity, and user mobility that hinder efficient coordination of sensing, communication, and computation.

Method: Formulates the problem as non-convex stochastic optimization and POMDP, solved by novel MADRL algorithm with hybrid actor network combining HAPPO, CNN for feature extraction, and KAN for structured state-action dependencies.

Result: Extensive numerical results show significant improvements in the amount of processed sensing data compared to other benchmarks.

Conclusion: The proposed joint optimization framework with novel MADRL algorithm effectively addresses coordination challenges in UAV-assisted MCS and achieves superior performance in processed data volume.

Abstract: Unmanned aerial vehicles (UAVs)-assisted mobile crowdsensing (MCS) has
emerged as a promising paradigm for data collection. However, challenges such
as spectrum scarcity, device heterogeneity, and user mobility hinder efficient
coordination of sensing, communication, and computation. To tackle these
issues, we propose a joint optimization framework that integrates time slot
partition for sensing, communication, and computation phases, resource
allocation, and UAV 3D trajectory planning, aiming to maximize the amount of
processed sensing data. The problem is formulated as a non-convex stochastic
optimization and further modeled as a partially observable Markov decision
process (POMDP) that can be solved by multi-agent deep reinforcement learning
(MADRL) algorithm. To overcome the limitations of conventional multi-layer
perceptron (MLP) networks, we design a novel MADRL algorithm with hybrid actor
network. The newly developed method is based on heterogeneous agent proximal
policy optimization (HAPPO), empowered by convolutional neural networks (CNN)
for feature extraction and Kolmogorov-Arnold networks (KAN) to capture
structured state-action dependencies. Extensive numerical results demonstrate
that our proposed method achieves significant improvements in the amount of
processed sensing data when compared with other benchmarks.

</details>


### [144] [How Effective Are Time-Series Models for Rainfall Nowcasting? A Comprehensive Benchmark for Rainfall Nowcasting Incorporating PWV Data](https://arxiv.org/abs/2509.25263)
*Yifang Zhang,Pengfei Duan,Henan Wang,Shengwu Xiong*

Main category: cs.LG

TL;DR: RainfallBench is a benchmark for rainfall nowcasting (0-3 hour prediction) that addresses gaps in existing meteorological benchmarks by focusing on complex rainfall characteristics like zero inflation, temporal decay, and non-stationarity.


<details>
  <summary>Details</summary>
Motivation: Existing meteorological benchmarks focus on periodic variables like temperature and humidity, failing to capture the complexity of rainfall nowcasting which is critical for disaster mitigation and real-time response.

Method: Created RainfallBench dataset from 5 years of meteorological data at 15-minute intervals across 12,000+ GNSS stations, including precipitable water vapor. Designed specialized evaluation strategies and introduced Bi-Focus Precipitation Forecaster (BFPF) module to handle zero-inflation and temporal decay.

Result: Evaluated over 20 state-of-the-art models across six architectures. Statistical analysis and ablation studies validated dataset comprehensiveness and methodology superiority.

Conclusion: RainfallBench provides a comprehensive benchmark for rainfall nowcasting, addressing key meteorological challenges and demonstrating the effectiveness of domain-specific approaches like BFPF for this complex forecasting task.

Abstract: Rainfall nowcasting, which aims to predict precipitation within the next 0 to
3 hours, is critical for disaster mitigation and real-time response planning.
However, most time series forecasting benchmarks in meteorology are evaluated
on variables with strong periodicity, such as temperature and humidity, which
fail to reflect model capabilities in more complex and practically meteorology
scenarios like rainfall nowcasting. To address this gap, we propose
RainfallBench, a benchmark designed for rainfall nowcasting, a highly
challenging and practically relevant task characterized by zero inflation,
temporal decay, and non-stationarity, focused on predicting precipitation
within the next 0 to 3 hours. The dataset is derived from five years of
meteorological observations, recorded at 15-minute intervals across six
essential variables, and collected from more than 12,000 GNSS stations
globally. In particular, it incorporates precipitable water vapor (PWV), a
crucial indicator of rainfall that is absent in other datasets. We further
design specialized evaluation strategies to assess model performance on key
meteorological challenges, such as multi-scale prediction and extreme rainfall
events, and evaluate over 20 state-of-the-art models across six major
architectures on RainfallBench. Additionally, to address the zero-inflation and
temporal decay issues overlooked by existing models, we introduce Bi-Focus
Precipitation Forecaster (BFPF), a plug-and-play module that incorporates
domain-specific priors to enhance rainfall time series forecasting. Statistical
analysis and ablation studies validate the comprehensiveness of our dataset as
well as the superiority of our methodology. Code and datasets are available at
https://anonymous.4open.science/r/RainfallBench-A710.

</details>


### [145] [Dynamic Policy Induction for Adaptive Prompt Optimization: Bridging the Efficiency-Accuracy Gap via Lightweight Reinforcement Learning](https://arxiv.org/abs/2509.25267)
*Jiexi Xu*

Main category: cs.LG

TL;DR: PPN is a lightweight RL framework that adaptively selects prompting strategies for LLMs, reducing token costs by 61.5% compared to Self-Consistency while maintaining competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Static prompting strategies impose rigid efficiency-accuracy trade-offs - accurate methods waste computation on simple tasks, while lightweight methods fail on complex inputs.

Method: Formalizes adaptive strategy selection as single-step MDP using Prompt Policy Network trained with PPO and resource-explicit reward function.

Result: Achieves superior performance on efficiency-accuracy Pareto front with 61.5% token cost reduction compared to Self-Consistency while maintaining competitive accuracy.

Conclusion: Provides systematic adaptive framework for cost-efficient LLM deployment, advancing lightweight optimization for scalable and sustainable language model applications.

Abstract: The performance of Large Language Models (LLMs) depends heavily on the chosen
prompting strategy, yet static approaches such as Zero-Shot, Few-Shot, or
Chain-of-Thought (CoT) impose a rigid efficiency-accuracy trade-off. Highly
accurate strategies like Self-Consistency (SC) incur substantial computational
waste on simple tasks, while lightweight methods often fail on complex inputs.
This paper introduces the Prompt Policy Network (PPN), a lightweight
reinforcement learning framework that formalizes adaptive strategy selection as
a single-step Markov Decision Process (MDP). The PPN, trained with Proximal
Policy Optimization (PPO) and guided by a resource-explicit reward function,
learns to allocate costly reasoning strategies only when necessary. Experiments
on arithmetic reasoning benchmarks demonstrate that PPN achieves superior
performance on the efficiency-accuracy Pareto front, delivering up to 61.5%
token cost reduction compared to Self-Consistency while maintaining competitive
accuracy. This work contributes a systematic, adaptive framework for
cost-efficient LLM deployment, advancing the design of lightweight optimization
techniques for scalable and sustainable language model applications.

</details>


### [146] [A Weather Foundation Model for the Power Grid](https://arxiv.org/abs/2509.25268)
*Cristian Bodnar,Raphaël Rousseau-Rizzi,Nikhil Shankar,James Merleau,Stylianos Flampouris,Guillem Candille,Slavica Antic,François Miralles,Jayesh K. Gupta*

Main category: cs.LG

TL;DR: Fine-tuning a 1.5B-parameter weather foundation model on Hydro-Québec asset data achieves superior hyper-local forecasts for grid-critical variables, including novel rime-ice detection capability.


<details>
  <summary>Details</summary>
Motivation: Weather foundation models have shown strong global forecast performance but their practical value for weather-sensitive infrastructure like power grids remains unproven.

Method: Fine-tuned Silurian AI's Generative Forecasting Transformer (GFT) on Hydro-Québec asset observations including transmission-line weather stations, wind-farm data, and icing sensors.

Result: Outperformed state-of-the-art NWP benchmarks: 15% reduction in temperature MAE, 35% reduction in precipitation MAE, 15% reduction in wind speed MAE, and achieved 0.72 average precision for day-ahead rime-ice detection.

Conclusion: Weather foundation models, when post-trained with small amounts of high-fidelity data, can serve as practical foundation for next-generation grid-resilience intelligence.

Abstract: Weather foundation models (WFMs) have recently set new benchmarks in global
forecast skill, yet their concrete value for the weather-sensitive
infrastructure that powers modern society remains largely unexplored. In this
study, we fine-tune Silurian AI's 1.5B-parameter WFM, Generative Forecasting
Transformer (GFT), on a rich archive of Hydro-Qu\'ebec asset
observations--including transmission-line weather stations, wind-farm met-mast
streams, and icing sensors--to deliver hyper-local, asset-level forecasts for
five grid-critical variables: surface temperature, precipitation, hub-height
wind speed, wind-turbine icing risk, and rime-ice accretion on overhead
conductors. Across 6-72 h lead times, the tailored model surpasses
state-of-the-art NWP benchmarks, trimming temperature mean absolute error (MAE)
by 15%, total-precipitation MAE by 35%, and lowering wind speed MAE by 15%.
Most importantly, it attains an average precision score of 0.72 for day-ahead
rime-ice detection, a capability absent from existing operational systems,
which affords several hours of actionable warning for potentially catastrophic
outage events. These results show that WFMs, when post-trained with small
amounts of high-fidelity, can serve as a practical foundation for
next-generation grid-resilience intelligence.

</details>


### [147] [InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions](https://arxiv.org/abs/2509.25270)
*Liangjian Wen,Qun Dai,Jianzhuang Liu,Jiangtao Zheng,Yong Dai,Dongkai Wang,Zhao Kang,Jun Wang,Zenglin Xu,Jiang Duan*

Main category: cs.LG

TL;DR: InfMasking is a contrastive synergistic information extraction method that uses infinite masking strategy to enhance multimodal representation learning by capturing richer interactions between modalities.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal methods struggle to capture the full spectrum of synergistic information between modalities, which is crucial for multimodal representation learning as synergistic interactions create unique outcomes that no single modality can achieve alone.

Method: InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. It aligns unmasked fused representations with masked ones through mutual information maximization using a derived InfMasking loss to approximate the computationally prohibitive mutual information calculation.

Result: InfMasking effectively enhances synergistic information between modalities and achieves state-of-the-art performance across seven benchmarks on large-scale real-world datasets.

Conclusion: The infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training, making InfMasking an effective approach for synergistic information extraction in multimodal representation learning.

Abstract: In multimodal representation learning, synergistic interactions between
modalities not only provide complementary information but also create unique
outcomes through specific interaction patterns that no single modality could
achieve alone. Existing methods may struggle to effectively capture the full
spectrum of synergistic information, leading to suboptimal performance in tasks
where such interactions are critical. This is particularly problematic because
synergistic information constitutes the fundamental value proposition of
multimodal representation. To address this challenge, we introduce InfMasking,
a contrastive synergistic information extraction method designed to enhance
synergistic information through an \textbf{Inf}inite \textbf{Masking} strategy.
InfMasking stochastically occludes most features from each modality during
fusion, preserving only partial information to create representations with
varied synergistic patterns. Unmasked fused representations are then aligned
with masked ones through mutual information maximization to encode
comprehensive synergistic information. This infinite masking strategy enables
capturing richer interactions by exposing the model to diverse partial modality
combinations during training. As computing mutual information estimates with
infinite masking is computationally prohibitive, we derive an InfMasking loss
to approximate this calculation. Through controlled experiments, we demonstrate
that InfMasking effectively enhances synergistic information between
modalities. In evaluations on large-scale real-world datasets, InfMasking
achieves state-of-the-art performance across seven benchmarks. Code is released
at https://github.com/brightest66/InfMasking.

</details>


### [148] [MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series](https://arxiv.org/abs/2509.25278)
*Payal Mohapatra,Yueyuan Sui,Akash Pandey,Stephen Xia,Qi Zhu*

Main category: cs.LG

TL;DR: MAESTRO is a novel multimodal learning framework that addresses limitations of existing approaches by enabling dynamic cross-modal interactions, handling arbitrary missing modalities, and using sparse attention mechanisms for efficient processing of long multimodal time series.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal learning approaches have three key limitations: reliance on single primary modality for alignment, pairwise modeling of modalities, and assumption of complete modality observations. These hinder real-world applicability where primary modality priors are unclear, modality counts are large, and sensor failures cause missing data.

Method: MAESTRO uses symbolic tokenization and adaptive attention budgeting to construct long multimodal sequences, processed via sparse cross-modal attention. It facilitates dynamic intra- and cross-modal interactions based on task relevance, and routes cross-modal tokens through sparse Mixture-of-Experts (MoE) mechanism for black-box specialization under varying modality combinations.

Result: MAESTRO achieves average relative improvements of 4% over best existing multimodal approaches and 8% over best multivariate approaches under complete observations. Under partial observations with up to 40% missing modalities, it achieves average 9% improvement. The framework demonstrates robustness and efficiency in learning from dynamic time series.

Conclusion: MAESTRO provides an effective solution for real-world multimodal time-series analysis by overcoming key limitations of existing approaches through its dynamic cross-modal interaction design, ability to handle missing modalities, and efficient sparse processing architecture.

Abstract: From clinical healthcare to daily living, continuous sensor monitoring across
multiple modalities has shown great promise for real-world intelligent
decision-making but also faces various challenges. In this work, we introduce
MAESTRO, a novel framework that overcomes key limitations of existing
multimodal learning approaches: (1) reliance on a single primary modality for
alignment, (2) pairwise modeling of modalities, and (3) assumption of complete
modality observations. These limitations hinder the applicability of these
approaches in real-world multimodal time-series settings, where primary
modality priors are often unclear, the number of modalities can be large
(making pairwise modeling impractical), and sensor failures often result in
arbitrary missing observations. At its core, MAESTRO facilitates dynamic intra-
and cross-modal interactions based on task relevance, and leverages symbolic
tokenization and adaptive attention budgeting to construct long multimodal
sequences, which are processed via sparse cross-modal attention. The resulting
cross-modal tokens are routed through a sparse Mixture-of-Experts (MoE)
mechanism, enabling black-box specialization under varying modality
combinations. We evaluate MAESTRO against 10 baselines on four diverse datasets
spanning three applications, and observe average relative improvements of 4%
and 8% over the best existing multimodal and multivariate approaches,
respectively, under complete observations. Under partial observations -- with
up to 40% of missing modalities -- MAESTRO achieves an average 9% improvement.
Further analysis also demonstrates the robustness and efficiency of MAESTRO's
sparse, modality-aware design for learning from dynamic time series.

</details>


### [149] [Optimisation of Resource Allocation in Heterogeneous Wireless Networks Using Deep Reinforcement Learning](https://arxiv.org/abs/2509.25284)
*Oluwaseyi Giwa,Jonathan Shock,Jaco Du Toit,Tobi Awodumila*

Main category: cs.LG

TL;DR: A DRL framework for dynamic resource allocation in HetNets outperforms heuristic methods by jointly optimizing power, bandwidth, and scheduling with multi-objective rewards.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with dynamic resource allocation in heterogeneous wireless networks under varying conditions.

Method: Proposed deep reinforcement learning framework using PPO and TD3 algorithms, compared against three heuristic algorithms using real base station coordinates.

Result: DRL frameworks outperform heuristic algorithms in optimizing resource allocation in dynamic networks.

Conclusion: DRL shows promise for future HetNets with key design trade-offs identified.

Abstract: Dynamic resource allocation in heterogeneous wireless networks (HetNets) is
challenging for traditional methods under varying user loads and channel
conditions. We propose a deep reinforcement learning (DRL) framework that
jointly optimises transmit power, bandwidth, and scheduling via a
multi-objective reward balancing throughput, energy efficiency, and fairness.
Using real base station coordinates, we compare Proximal Policy Optimisation
(PPO) and Twin Delayed Deep Deterministic Policy Gradient (TD3) against three
heuristic algorithms in multiple network scenarios. Our results show that DRL
frameworks outperform heuristic algorithms in optimising resource allocation in
dynamic networks. These findings highlight key trade-offs in DRL design for
future HetNets.

</details>


### [150] [ClustRecNet: A Novel End-to-End Deep Learning Framework for Clustering Algorithm Recommendation](https://arxiv.org/abs/2509.25289)
*Mohammadreza Bakhtyari,Bogdan Mazoure,Renato Cordeiro de Amorim,Guillaume Rabusseau,Vladimir Makarenkov*

Main category: cs.LG

TL;DR: ClustRecNet is a deep learning framework that automatically recommends the best clustering algorithm for any given dataset, outperforming traditional methods and AutoML approaches.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of clustering algorithm selection in unsupervised learning, which traditionally relies on manual feature engineering and Cluster Validity Indices.

Method: Built a comprehensive repository of 34,000 synthetic datasets, processed with 10 clustering algorithms and evaluated using Adjusted Rand Index. Developed a DL architecture combining convolutional, residual, and attention mechanisms to learn dataset representations.

Result: Outperformed conventional CVIs (Silhouette, Calinski-Harabasz, Davies-Bouldin, Dunn) and state-of-the-art AutoML approaches (ML2DAC, AutoCluster, AutoML4Clust). Achieved 0.497 ARI improvement over Calinski-Harabasz on synthetic data and 15.3% ARI gain over best AutoML on real-world data.

Conclusion: ClustRecNet provides an effective end-to-end solution for clustering algorithm recommendation, reducing reliance on handcrafted features and traditional validation indices while achieving superior performance.

Abstract: We introduce ClustRecNet - a novel deep learning (DL)-based recommendation
framework for determining the most suitable clustering algorithms for a given
dataset, addressing the long-standing challenge of clustering algorithm
selection in unsupervised learning. To enable supervised learning in this
context, we construct a comprehensive data repository comprising 34,000
synthetic datasets with diverse structural properties. Each of them was
processed using 10 popular clustering algorithms. The resulting clusterings
were assessed via the Adjusted Rand Index (ARI) to establish ground truth
labels, used for training and evaluation of our DL model. The proposed network
architecture integrates convolutional, residual, and attention mechanisms to
capture both local and global structural patterns from the input data. This
design supports end-to-end training to learn compact representations of
datasets and enables direct recommendation of the most suitable clustering
algorithm, reducing reliance on handcrafted meta-features and traditional
Cluster Validity Indices (CVIs). Comprehensive experiments across synthetic and
real-world benchmarks demonstrate that our DL model consistently outperforms
conventional CVIs (e.g. Silhouette, Calinski-Harabasz, Davies-Bouldin, and
Dunn) as well as state-of-the-art AutoML clustering recommendation approaches
(e.g. ML2DAC, AutoCluster, and AutoML4Clust). Notably, the proposed model
achieves a 0.497 ARI improvement over the Calinski-Harabasz index on synthetic
data and a 15.3% ARI gain over the best-performing AutoML approach on
real-world data.

</details>


### [151] [Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning](https://arxiv.org/abs/2509.25300)
*Zelin Tan,Hejia Geng,Mulei Zhang,Xiaohang Yu,Guancheng Wan,Yifan Zhou,Qiang He,Xiangyuan Xue,Heng Zhou,Yutao Fan,Zhongzhi Li,Zaibin Zhang,Guibin Zhang,Chen Zhang,Zhenfei Yin,Lei Bai*

Main category: cs.LG

TL;DR: This paper investigates scaling laws for LLMs during RL post-training, finding that larger models outperform smaller ones under fixed compute budgets, achieve better sample efficiency, benefit from data reuse, and show consistent scaling behaviors across different model types.


<details>
  <summary>Details</summary>
Motivation: While scaling laws for LLM pre-training are well-studied, the scaling behaviors during RL-based post-training remain largely unexplored, particularly for mathematical reasoning tasks.

Method: Conducted 54 experiments across diverse model sizes and training settings to systematically analyze how model scale, data volume, and computational budget interact during RL post-training.

Result: Four key findings: (1) Larger models outperform smaller ones under fixed compute; (2) Larger models have better sample efficiency; (3) Data reuse is effective in data-constrained regimes; (4) Scaling behaviors are robust across base and instruction-tuned models.

Conclusion: The results provide principled foundations and practical guidelines for efficiently scaling LLM reasoning capabilities through RL post-training.

Abstract: While scaling laws for large language models (LLMs) during pre-training have
been extensively studied, their behavior under reinforcement learning (RL)
post-training remains largely unexplored. This paper presents a systematic
empirical investigation of scaling behaviors in RL-based post-training, with a
particular focus on mathematical reasoning. Based on 54 experiments across
diverse model sizes and training settings, we characterize how model scale,
data volume, and computational budget interact to shape performance. Our
analysis leads to four key findings: (1). Under a fixed computational budget,
larger models trained for fewer steps consistently outperform smaller models
trained for more steps. (2). Given a fixed amount of training data, larger
models achieve superior sample efficiency, yielding lower loss. (3). In
data-constrained regimes, repeated reuse of high-quality data proves highly
effective, as final performance is primarily governed by the total number of
optimization steps rather than the uniqueness of samples. (4). These scaling
behaviors are robust across both base and instruction-tuned models, which share
similar learning dynamics (e.g., larger models show faster convergence) even
while differing in absolute accuracy. Collectively, these results provide a
principled foundation and practical guidelines for efficiently scaling the
reasoning capabilities of LLMs through RL post-training.

</details>


### [152] [Uncertainty-Aware Generative Oversampling Using an Entropy-Guided Conditional Variational Autoencoder](https://arxiv.org/abs/2509.25334)
*Amirhossein Zare,Amirhessam Zare,Parmida Sadat Pezeshki,Herlock,Rahimi,Ali Ebrahimi,Ignacio Vázquez-García,Leo Anthony Celi*

Main category: cs.LG

TL;DR: LEO-CVAE is a generative oversampling method that incorporates local uncertainty (measured by Shannon entropy) into both representation learning and data generation to address class imbalance in high-dimensional biomedical data.


<details>
  <summary>Details</summary>
Motivation: Traditional oversampling methods like SMOTE produce implausible samples, while standard generative models neglect the importance of uncertain boundary-region examples that are crucial for effective classification.

Method: Proposes Local Entropy-Guided Oversampling with CVAE (LEO-CVAE) using: (i) Local Entropy-Weighted Loss for robust learning in uncertain regions, and (ii) entropy-guided sampling strategy that concentrates generation in class-overlapping areas.

Result: Applied to clinical genomics datasets (ADNI and TCGA lung cancer), LEO-CVAE consistently improves classifier performance, outperforming both traditional oversampling and generative baselines.

Conclusion: Uncertainty-aware generative oversampling is valuable for imbalanced learning in domains with complex nonlinear structures like omics data.

Abstract: Class imbalance remains a major challenge in machine learning, especially for
high-dimensional biomedical data where nonlinear manifold structures dominate.
Traditional oversampling methods such as SMOTE rely on local linear
interpolation, often producing implausible synthetic samples. Deep generative
models like Conditional Variational Autoencoders (CVAEs) better capture
nonlinear distributions, but standard variants treat all minority samples
equally, neglecting the importance of uncertain, boundary-region examples
emphasized by heuristic methods like Borderline-SMOTE and ADASYN.
  We propose Local Entropy-Guided Oversampling with a CVAE (LEO-CVAE), a
generative oversampling framework that explicitly incorporates local
uncertainty into both representation learning and data generation. To quantify
uncertainty, we compute Shannon entropy over the class distribution in a
sample's neighborhood: high entropy indicates greater class overlap, serving as
a proxy for uncertainty. LEO-CVAE leverages this signal through two mechanisms:
(i) a Local Entropy-Weighted Loss (LEWL) that emphasizes robust learning in
uncertain regions, and (ii) an entropy-guided sampling strategy that
concentrates generation in these informative, class-overlapping areas.
  Applied to clinical genomics datasets (ADNI and TCGA lung cancer), LEO-CVAE
consistently improves classifier performance, outperforming both traditional
oversampling and generative baselines. These results highlight the value of
uncertainty-aware generative oversampling for imbalanced learning in domains
governed by complex nonlinear structures, such as omics data.

</details>


### [153] [Gradient Descent with Large Step Sizes: Chaos and Fractal Convergence Region](https://arxiv.org/abs/2509.25351)
*Shuang Liang,Guido Montúfar*

Main category: cs.LG

TL;DR: Gradient descent in matrix factorization exhibits fractal structure under large step sizes, with chaotic dynamics near critical step sizes that eliminate implicit biases.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of gradient descent in matrix factorization, particularly how large step sizes affect convergence and parameter selection.

Method: Analyze gradient descent in matrix factorization, derive critical step sizes for scalar-vector factorization, examine effects of regularization, and extend to general matrix factorization with orthogonal initialization.

Result: Large step sizes create fractal parameter structures; near critical step sizes, minimizer selection becomes highly sensitive to initialization; regularization amplifies this sensitivity; chaotic dynamics emerge with unpredictable long-term behavior.

Conclusion: Near-critical step sizes induce chaotic gradient descent dynamics that eliminate common implicit biases like balancedness, minimum norm, or flatness, revealing complex fractal boundaries in parameter space.

Abstract: We examine gradient descent in matrix factorization and show that under large
step sizes the parameter space develops a fractal structure. We derive the
exact critical step size for convergence in scalar-vector factorization and
show that near criticality the selected minimizer depends sensitively on the
initialization. Moreover, we show that adding regularization amplifies this
sensitivity, generating a fractal boundary between initializations that
converge and those that diverge. The analysis extends to general matrix
factorization with orthogonal initialization. Our findings reveal that
near-critical step sizes induce a chaotic regime of gradient descent where the
long-term dynamics are unpredictable and there are no simple implicit biases,
such as towards balancedness, minimum norm, or flatness.

</details>


### [154] [Cold-Start Active Correlation Clustering](https://arxiv.org/abs/2509.25376)
*Linus Aronsson,Han Wu,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: Active correlation clustering with cold-start scenario using coverage-aware method for diversity in active learning.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of active correlation clustering where pairwise similarities are not provided upfront and must be queried cost-efficiently, particularly in cold-start scenarios with no initial pairwise similarities.

Method: Proposed a coverage-aware method that encourages diversity early in the active learning process to handle the cold-start scenario.

Result: Demonstrated effectiveness through several synthetic and real-world experiments.

Conclusion: The coverage-aware approach effectively addresses the cold-start problem in active correlation clustering by promoting diversity in the early stages of active learning.

Abstract: We study active correlation clustering where pairwise similarities are not
provided upfront and must be queried in a cost-efficient manner through active
learning. Specifically, we focus on the cold-start scenario, where no true
initial pairwise similarities are available for active learning. To address
this challenge, we propose a coverage-aware method that encourages diversity
early in the process. We demonstrate the effectiveness of our approach through
several synthetic and real-world experiments.

</details>


### [155] [Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation](https://arxiv.org/abs/2509.25379)
*Yogesh Verma,Markus Heinonen,Vikas Garg*

Main category: cs.LG

TL;DR: A novel diffusion-based protein generation method that incorporates physically motivated noising dynamics and flow-matching on SE(3) to produce realistic protein structures with improved designability and novelty.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based protein design methods lack physical realism due to noising dynamics not grounded in physical principles, leading to unrealistic protein structures.

Method: Introduces a physically motivated non-linear noising process that unfolds proteins into secondary structures while preserving topological integrity, combined with flow-matching on SE(3) to model protein backbone distributions with sequence conditioning.

Result: Achieves state-of-the-art performance in unconditional protein generation, producing more designable and novel structures while accurately folding monomer sequences into precise conformations.

Conclusion: The proposed physically grounded approach significantly improves protein generation quality by incorporating realistic physical principles into the generative process.

Abstract: Protein structure prediction and folding are fundamental to understanding
biology, with recent deep learning advances reshaping the field.
Diffusion-based generative models have revolutionized protein design, enabling
the creation of novel proteins. However, these methods often neglect the
intrinsic physical realism of proteins, driven by noising dynamics that lack
grounding in physical principles. To address this, we first introduce a
physically motivated non-linear noising process, grounded in classical physics,
that unfolds proteins into secondary structures (e.g., alpha helices, linear
beta sheets) while preserving topological integrity--maintaining bonds, and
preventing collisions. We then integrate this process with the flow-matching
paradigm on SE(3) to model the invariant distribution of protein backbones with
high fidelity, incorporating sequence information to enable
sequence-conditioned folding and expand the generative capabilities of our
model. Experimental results demonstrate that the proposed method achieves
state-of-the-art performance in unconditional protein generation, producing
more designable and novel protein structures while accurately folding monomer
sequences into precise protein conformations.

</details>


### [156] [Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs](https://arxiv.org/abs/2509.25380)
*Shane Bergsma,Nolan Dey,Joel Hestness*

Main category: cs.LG

TL;DR: The paper introduces TREC (training re-evaluation curve) to analyze data retention during LLM training, shows high-quality data should be placed at TREC minima, and demonstrates TREC can be predicted using AdamW's EMA coefficients for proactive curriculum design.


<details>
  <summary>Details</summary>
Motivation: Current data curriculum methods for LLM training lack clear principles for optimal data placement, making it difficult to design effective training strategies.

Method: Develop TREC diagnostic to evaluate training batches using final model weights, analyze TRECs across models from 111M to 3.9B parameters, and use AdamW's implicit EMA coefficients to predict TRECs in advance.

Result: Placing high-quality data at TREC minima improves performance; TREC can be predicted before training; analysis of published recipes reveals suboptimal data placements; improved continual pre-training of 3.9B-parameter LLM.

Conclusion: TREC provides a principled approach to data curriculum design, enabling proactive optimization of data placement for better LLM training outcomes.

Abstract: Data curriculums have become central to successful LLM training, yet
principles governing optimal data placement remain unclear. We introduce the
*training re-evaluation curve (TREC)*, a diagnostic that retrospectively
evaluates training batches *using the final model weights*. The TREC
characterizes how well a trained model retains training data as a function of
*when* the data was encountered during training. Analyzing TRECs for models
from 111M to 3.9B parameters, we show that placing high-quality data at low
points on the TREC significantly improves performance. Importantly, while a
TREC is initially observable only after training, we demonstrate it can be
*predicted in advance* from AdamW's implicit EMA coefficients, enabling
proactive curriculum design. By predicting TRECs for published training
recipes, we explain prior ablations and reveal suboptimal data placements. We
also align high-quality data with TREC minima in order to improve continual
pre-training of a 3.9B-parameter LLM trained on 900B tokens.

</details>


### [157] [Deep Survival Analysis for Competing Risk Modeling with Functional Covariates and Missing Data Imputation](https://arxiv.org/abs/2509.25381)
*Penglei Gao,Yan Zou,Abhijit Duggal,Shuaiqi Huang,Faming Liang,Xiaofeng Wang*

Main category: cs.LG

TL;DR: FCRN is a deep learning framework for competing risks survival analysis that handles functional covariates and missing data through integrated imputation and hazard prediction.


<details>
  <summary>Details</summary>
Motivation: To improve prognostic modeling in critical care by better capturing dynamic risk factors and static predictors while handling irregular and incomplete data in competing risks scenarios.

Method: Combines micro-network Basis Layer for functional data representation with gradient-based imputation module to simultaneously learn missing value imputation and predict event-specific hazards.

Result: Demonstrates substantial improvements in prediction accuracy over random survival forests and traditional competing risks models on simulated datasets and real-world ICU case studies using MIMIC-IV and Cleveland Clinic datasets.

Conclusion: FCRN advances survival analysis under competing risks by providing a unified framework that effectively integrates functional covariates and handles missing data end-to-end.

Abstract: We introduce the Functional Competing Risk Net (FCRN), a unified
deep-learning framework for discrete-time survival analysis under competing
risks, which seamlessly integrates functional covariates and handles missing
data within an end-to-end model. By combining a micro-network Basis Layer for
functional data representation with a gradient-based imputation module, FCRN
simultaneously learns to impute missing values and predict event-specific
hazards. Evaluated on multiple simulated datasets and a real-world ICU case
study using the MIMIC-IV and Cleveland Clinic datasets, FCRN demonstrates
substantial improvements in prediction accuracy over random survival forests
and traditional competing risks models. This approach advances prognostic
modeling in critical care by more effectively capturing dynamic risk factors
and static predictors while accommodating irregular and incomplete data.

</details>


### [158] [On the Shape of Latent Variables in a Denoising VAE-MoG: A Posterior Sampling-Based Study](https://arxiv.org/abs/2509.25382)
*Fernanda Zapata Bascuñán*

Main category: cs.LG

TL;DR: The paper analyzes a VAE-MoG model trained on GW150914 gravitational wave data, finding that while signal reconstruction is accurate, latent space representations show statistical mismatches when validated with Hamiltonian Monte Carlo.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether strong denoising performance in variational autoencoders with mixture-of-Gaussians priors necessarily indicates reliable latent representations, particularly in gravitational wave data analysis.

Method: Used Hamiltonian Monte Carlo (HMC) to draw posterior samples from a denoising VAE-MoG model trained on GW150914 data, comparing these with encoder outputs from noisy data.

Result: The model accurately reconstructed gravitational wave signals, but statistical comparisons revealed clear mismatches in the latent space representations.

Conclusion: Strong denoising performance doesn't guarantee reliable latent representations, highlighting the importance of posterior-based validation methods for evaluating generative models.

Abstract: In this work, we explore the latent space of a denoising variational
autoencoder with a mixture-of-Gaussians prior (VAE-MoG), trained on
gravitational wave data from event GW150914. To evaluate how well the model
captures the underlying structure, we use Hamiltonian Monte Carlo (HMC) to draw
posterior samples conditioned on clean inputs, and compare them to the
encoder's outputs from noisy data. Although the model reconstructs signals
accurately, statistical comparisons reveal a clear mismatch in the latent
space. This shows that strong denoising performance doesn't necessarily mean
the latent representations are reliable highlighting the importance of using
posterior-based validation when evaluating generative models.

</details>


### [159] [Crowdsourcing Without People: Modelling Clustering Algorithms as Experts](https://arxiv.org/abs/2509.25395)
*Jordyn E. A. Lorentz,Katharine M. Clark*

Main category: cs.LG

TL;DR: Mixsemble is an ensemble method that adapts the Dawid-Skene model to aggregate predictions from multiple clustering algorithms, treating algorithm outputs as noisy annotations.


<details>
  <summary>Details</summary>
Motivation: To create a robust clustering ensemble method that can handle unknown data structures and provide reliable results for non-expert users, avoiding the need for human labels like traditional crowdsourcing.

Method: Adapts the Dawid-Skene model to aggregate predictions from multiple model-based clustering algorithms, treating algorithm outputs as noisy annotations rather than relying on human labels.

Result: Experiments on simulated and real-world datasets show mixsemble consistently approaches the best performance and avoids poor outcomes, though it's not always the single top performer.

Conclusion: Mixsemble provides robust and practical clustering performance when the true data structure is unknown, making it particularly suitable for non-expert users.

Abstract: This paper introduces mixsemble, an ensemble method that adapts the
Dawid-Skene model to aggregate predictions from multiple model-based clustering
algorithms. Unlike traditional crowdsourcing, which relies on human labels, the
framework models the outputs of clustering algorithms as noisy annotations.
Experiments on both simulated and real-world datasets show that, although the
mixsemble is not always the single top performer, it consistently approaches
the best result and avoids poor outcomes. This robustness makes it a practical
alternative when the true data structure is unknown, especially for non-expert
users.

</details>


### [160] [Multi-Task Equation Discovery](https://arxiv.org/abs/2509.25400)
*S C Bee,N Dervilis,K Worden,L A Bull*

Main category: cs.LG

TL;DR: Multi-task Bayesian inference using RVM improves equation discovery by sharing parameters across datasets, enhancing generalization and mitigating over-fitting.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of ensuring identified models generalize across operating conditions rather than over-fitting to specific datasets in equation discovery.

Method: Apply Bayesian relevance vector machine (RVM) within multi-task learning framework for simultaneous parameter identification across multiple datasets, treating responses under different excitation levels as related tasks.

Result: MTL-RVM improved parameter recovery for weakly and moderately excited datasets while maintaining strong performance under high excitation, outperforming single-task RVM models.

Conclusion: Multi-task Bayesian inference can mitigate over-fitting and promote generalization in equation discovery, particularly relevant for structural health monitoring with varying load conditions.

Abstract: Equation discovery provides a grey-box approach to system identification by
uncovering governing dynamics directly from observed data. However, a
persistent challenge lies in ensuring that identified models generalise across
operating conditions rather than over-fitting to specific datasets. This work
investigates this issue by applying a Bayesian relevance vector machine (RVM)
within a multi-task learning (MTL) framework for simultaneous parameter
identification across multiple datasets. In this formulation, responses from
the same structure under different excitation levels are treated as related
tasks that share model parameters but retain task-specific noise
characteristics. A simulated single degree-of-freedom oscillator with linear
and cubic stiffness provided the case study, with datasets generated under
three excitation regimes. Standard single-task RVM models were able to
reproduce system responses but often failed to recover the true governing terms
when excitations insufficiently stimulated non-linear dynamics. By contrast,
the MTL-RVM combined information across tasks, improving parameter recovery for
weakly and moderately excited datasets, while maintaining strong performance
under high excitation. These findings demonstrate that multi-task Bayesian
inference can mitigate over-fitting and promote generalisation in equation
discovery. The approach is particularly relevant to structural health
monitoring, where varying load conditions reveal complementary aspects of
system physics.

</details>


### [161] [FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers](https://arxiv.org/abs/2509.25401)
*Liang Qiao,Yue Dai,Yeqi Huang,Hongyu Kan,Jun Shi,Hong An*

Main category: cs.LG

TL;DR: FlashOmni is a unified sparse attention engine that accelerates Multi-Modal Diffusion Transformers (DiTs) by standardizing diverse sparsity patterns through flexible sparse symbols, enabling efficient execution within a single kernel and achieving significant speedups without quality degradation.


<details>
  <summary>Details</summary>
Motivation: Current sparsity-based acceleration methods for DiTs require customized kernels for different sparsity patterns, limiting universality and deployment efficiency. There's a need for a unified solution that can handle various sparsity strategies without performance loss.

Method: FlashOmni introduces flexible sparse symbols to standardize representation of diverse sparsity strategies (feature caching, block-sparse skipping). It designs optimized sparse GEMMs for attention blocks, leveraging sparse symbols to eliminate redundant computations and enable execution of diverse sparse computations within a single attention kernel.

Result: FlashOmni achieves near-linear speedup matching sparsity ratio (1:1) in attention and GEMM-Q, and 2.5×-3.8× acceleration in GEMM-O (reaching about 87.5% of theoretical limit). Applied to Hunyuan model (33K), it enables about 1.5× end-to-end acceleration without visual quality degradation.

Conclusion: FlashOmni provides a universal sparse attention engine that effectively accelerates DiTs across various architectures and sparsity patterns, achieving significant performance improvements while maintaining output quality, making it a practical solution for deploying computationally demanding diffusion models.

Abstract: Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional
capabilities in visual synthesis, yet their deployment remains constrained by
substantial computational demands. To alleviate this bottleneck, many
sparsity-based acceleration methods have been proposed. However, their diverse
sparsity patterns often require customized kernels for high-performance
inference, limiting universality. We propose FlashOmni, a unified sparse
attention engine compatible with arbitrary DiT architectures. FlashOmni
introduces flexible sparse symbols to standardize the representation of a wide
range of sparsity strategies, such as feature caching and block-sparse
skipping. This unified abstraction enables the execution of diverse sparse
computations within a single attention kernel. In addition, FlashOmni designs
optimized sparse GEMMs for attention blocks, leveraging sparse symbols to
eliminate redundant computations and further improve efficiency. Experiments
demonstrate that FlashOmni delivers near-linear, closely matching the sparsity
ratio speedup (1:1) in attention and GEMM-$Q$, and achieves
2.5$\times$-3.8$\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of
the theoretical limit). Applied with a multi-granularity sparsity strategy, it
enables the Hunyuan model (33K) to achieve about 1.5$\times$ end-to-end
acceleration without degrading visual quality.

</details>


### [162] [Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs](https://arxiv.org/abs/2509.25414)
*Hao Ban,Kaiyi Ji*

Main category: cs.LG

TL;DR: The paper proposes ALoRA and Fed-ALoRA, asymmetric multi-LoRA designs that share the B matrix while using multiple A matrices, achieving balanced performance across tasks in multi-task and federated fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Prior studies suggest that inner A matrices in LoRA are highly similar during training, but this similarity is due to identical initialization rather than shared knowledge. The B matrix plays a more critical role in knowledge encoding and transfer.

Method: Proposed ALoRA with multiple A matrices and a single shared B in multi-task fine-tuning, and Fed-ALoRA which shares B across clients in federated fine-tuning using a novel matrix decomposition strategy to handle heterogeneous ranks.

Result: Experiments on commonsense reasoning, math reasoning, multi-task NLP, and federated NLP datasets show balanced performance across tasks with comparable or superior average accuracy compared to existing multi-LoRA approaches.

Conclusion: The asymmetric design with shared B matrix is effective for multi-task and federated fine-tuning, demonstrating that B plays a more critical role than A in knowledge encoding and transfer.

Abstract: Large language models are often adapted using parameter-efficient techniques
such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$
is the pre-trained parameters and $x$ is the input to the adapted layer. While
multi-adapter extensions often employ multiple LoRAs, prior studies suggest
that the inner $A$ matrices are highly similar during training and thus
suitable for sharing. We revisit this phenomenon and find that this similarity
is largely attributable to the identical initialization rather than shared
knowledge, with $B$ playing a more critical role in knowledge encoding and
transfer. Motivated by these insights, we propose \textbf{ALoRA}, an asymmetric
multi-LoRA design with multiple $A$ matrices and a single shared $B$ in
multi-task fine-tuning, and \textbf{Fed-ALoRA}, which shares $B$ across clients
in federated fine-tuning under both homogeneous and heterogeneous settings,
through a novel matrix decomposition strategy to accommodate heterogeneous
ranks across clients. Experiments on commonsense reasoning, math reasoning,
multi-task NLP dataset, and federated NLP dataset demonstrate that our methods
achieve more balanced performance across tasks with comparable or superior
average accuracy relative to existing multi-LoRA approaches. Codes are
available at https://github.com/OptMN-Lab/ALoRA.

</details>


### [163] [Leveraging Vulnerabilities in Temporal Graph Neural Networks via Strategic High-Impact Assaults](https://arxiv.org/abs/2509.25418)
*Dong Hyun Jeon,Lijing Zhu,Haifang Li,Pengze Li,Jingna Feng,Tiehang Duan,Houbing Herbert Song,Cui Tao,Shuteng Niu*

Main category: cs.LG

TL;DR: HIA is a novel black-box attack framework that strategically targets both structurally and dynamically important nodes in temporal graphs, using hybrid edge perturbations to significantly degrade TGNN performance while maintaining stealth.


<details>
  <summary>Details</summary>
Motivation: TGNNs are vulnerable to adversarial attacks, especially those exploiting temporal dimensions. Existing attacks use simplistic perturbations and fail to strategically target influential nodes for maximum impact.

Method: HIA uses a data-driven surrogate model to identify structurally important nodes (network connectivity) and dynamically important nodes (temporal evolution), then employs hybrid perturbation strategy combining strategic edge injection and targeted edge deletion.

Result: Comprehensive experiments on 5 real-world datasets and 4 TGNN architectures show HIA reduces TGNN accuracy by up to 35.55% in MRR, significantly outperforming state-of-the-art baselines.

Conclusion: HIA exposes fundamental vulnerabilities in current STDG models and highlights the urgent need for robust defenses accounting for both structural and temporal dynamics.

Abstract: Temporal Graph Neural Networks (TGNNs) have become indispensable for
analyzing dynamic graphs in critical applications such as social networks,
communication systems, and financial networks. However, the robustness of TGNNs
against adversarial attacks, particularly sophisticated attacks that exploit
the temporal dimension, remains a significant challenge. Existing attack
methods for Spatio-Temporal Dynamic Graphs (STDGs) often rely on simplistic,
easily detectable perturbations (e.g., random edge additions/deletions) and
fail to strategically target the most influential nodes and edges for maximum
impact. We introduce the High Impact Attack (HIA), a novel restricted black-box
attack framework specifically designed to overcome these limitations and expose
critical vulnerabilities in TGNNs. HIA leverages a data-driven surrogate model
to identify structurally important nodes (central to network connectivity) and
dynamically important nodes (critical for the graph's temporal evolution). It
then employs a hybrid perturbation strategy, combining strategic edge injection
(to create misleading connections) and targeted edge deletion (to disrupt
essential pathways), maximizing TGNN performance degradation. Importantly, HIA
minimizes the number of perturbations to enhance stealth, making it more
challenging to detect. Comprehensive experiments on five real-world datasets
and four representative TGNN architectures (TGN, JODIE, DySAT, and TGAT)
demonstrate that HIA significantly reduces TGNN accuracy on the link prediction
task, achieving up to a 35.55% decrease in Mean Reciprocal Rank (MRR) - a
substantial improvement over state-of-the-art baselines. These results
highlight fundamental vulnerabilities in current STDG models and underscore the
urgent need for robust defenses that account for both structural and temporal
dynamics.

</details>


### [164] [Polychromic Objectives for Reinforcement Learning](https://arxiv.org/abs/2509.25424)
*Jubayer Ibn Hamid,Ifdita Hasan Orney,Ellen Xu,Chelsea Finn,Dorsa Sadigh*

Main category: cs.LG

TL;DR: The paper introduces a polychromic objective for RLFT to prevent policy collapse and maintain diverse behaviors during fine-tuning, improving exploration and generalization.


<details>
  <summary>Details</summary>
Motivation: RLFT often causes policies to lose diversity and collapse into limited outputs, hindering exploration and the benefits of test-time compute scaling.

Method: Adapts PPO with vine sampling for on-policy rollouts and modifies the advantage function to optimize the polychromic objective that enforces diverse generation exploration.

Result: Improves success rates on BabyAI, Minigrid, and Algorithmic Creativity, solves more environment configurations, generalizes better under perturbations, and achieves higher coverage in pass@k experiments.

Conclusion: The polychromic objective effectively maintains policy diversity during RLFT, leading to better exploration, generalization, and performance across multiple tasks.

Abstract: Reinforcement learning fine-tuning (RLFT) is a dominant paradigm for
improving pretrained policies for downstream tasks. These pretrained policies,
trained on large datasets, produce generations with a broad range of promising
but unrefined behaviors. Often, a critical failure mode of RLFT arises when
policies lose this diversity and collapse into a handful of easily exploitable
outputs. This convergence hinders exploration, which is essential for expanding
the capabilities of the pretrained policy and for amplifying the benefits of
test-time compute scaling. To address this, we introduce an objective for
policy gradient methods that explicitly enforces the exploration and refinement
of diverse generations, which we call a polychromic objective. We then show how
proximal policy optimization (PPO) can be adapted to optimize this objective.
Our method (1) employs vine sampling to collect on-policy rollouts and (2)
modifies the advantage function to reflect the advantage under our new
objective. Experiments on BabyAI, Minigrid, and Algorithmic Creativity show
that our method improves success rates by reliably solving a larger set of
environment configurations and generalizes better under large perturbations.
Moreover, when given multiple attempts in pass@$k$ experiments, the policy
achieves substantially higher coverage, demonstrating its ability to maintain
and exploit a diverse repertoire of strategies.

</details>


### [165] [Feedback Control for Small Budget Pacing](https://arxiv.org/abs/2509.25429)
*Sreeja Apparaju,Yichuan Niu,Xixi Qi*

Main category: cs.LG

TL;DR: A novel budget pacing controller combining bucketized hysteresis with proportional feedback for stable and adaptive spend control in online advertising, reducing pacing error by 13% and volatility by 54%.


<details>
  <summary>Details</summary>
Motivation: Existing pacing methods rely on ad-hoc parameter tuning which is unstable and inefficient, creating a need for more principled approaches to align spend with campaign goals under dynamic auctions.

Method: Proposes a controller that combines bucketized hysteresis with proportional feedback, providing a framework and analysis for parameter selection to enable accurate tracking of desired spend rates.

Result: Experiments in real-world auctions demonstrate significant improvements: 13% reduction in pacing error and 54% reduction in λ-volatility compared to baseline methods.

Conclusion: Bridging control theory with advertising systems provides a scalable and reliable solution for budget pacing, with particular benefits for small-budget campaigns.

Abstract: Budget pacing is critical in online advertising to align spend with campaign
goals under dynamic auctions. Existing pacing methods often rely on ad-hoc
parameter tuning, which can be unstable and inefficient. We propose a
principled controller that combines bucketized hysteresis with proportional
feedback to provide stable and adaptive spend control. Our method provides a
framework and analysis for parameter selection that enables accurate tracking
of desired spend rates across campaigns. Experiments in real-world auctions
demonstrate significant improvements in pacing accuracy and delivery
consistency, reducing pacing error by 13% and $\lambda$-volatility by 54%
compared to baseline method. By bridging control theory with advertising
systems, our approach offers a scalable and reliable solution for budget
pacing, with particular benefits for small-budget campaigns.

</details>


### [166] [Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring](https://arxiv.org/abs/2509.25438)
*Zhibo Hou,Zhiyu An,Wan Du*

Main category: cs.LG

TL;DR: LPM is a novel intrinsic reward method that monitors learning progress instead of prediction error, enabling efficient exploration in noisy environments by focusing on learnable transitions.


<details>
  <summary>Details</summary>
Motivation: Existing intrinsic reward methods get stuck at unlearnable noise sources (noisy-TVs) or suffer from poor sample efficiency. Inspired by human learning monitoring, LPM addresses these limitations.

Method: Uses dual-network design: an error model predicts previous dynamics model's prediction error, and the difference between current and previous model errors guides exploration.

Result: LPM converges faster, explores more states in maze experiments, and achieves higher extrinsic rewards in Atari compared to state-of-the-art baselines.

Conclusion: LPM provides a paradigm shift for noise-robust exploration by rewarding learning progress rather than prediction error, with theoretical guarantees of monotonic correspondence with Information Gain.

Abstract: When there exists an unlearnable source of randomness (noisy-TV) in the
environment, a naively intrinsic reward driven exploring agent gets stuck at
that source of randomness and fails at exploration. Intrinsic reward based on
uncertainty estimation or distribution similarity, while eventually escapes
noisy-TVs as time unfolds, suffers from poor sample efficiency and high
computational cost. Inspired by recent findings from neuroscience that humans
monitor their improvements during exploration, we propose a novel method for
intrinsically-motivated exploration, named Learning Progress Monitoring (LPM).
During exploration, LPM rewards model improvements instead of prediction error
or novelty, effectively rewards the agent for observing learnable transitions
rather than the unlearnable transitions. We introduce a dual-network design
that uses an error model to predict the expected prediction error of the
dynamics model in its previous iteration, and use the difference between the
model errors of the current iteration and previous iteration to guide
exploration. We theoretically show that the intrinsic reward of LPM is
zero-equivariant and a monotone indicator of Information Gain (IG), and that
the error model is necessary to achieve monotonicity correspondence with IG. We
empirically compared LPM against state-of-the-art baselines in noisy
environments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari.
Results show that LPM's intrinsic reward converges faster, explores more states
in the maze experiment, and achieves higher extrinsic reward in Atari. This
conceptually simple approach marks a shift-of-paradigm of noise-robust
exploration. For code to reproduce our experiments, see
https://github.com/Akuna23Matata/LPM_exploration

</details>


### [167] [Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications](https://arxiv.org/abs/2509.25439)
*Hanyuan Gao,Xiaoxuan Yang*

Main category: cs.LG

TL;DR: Norm-Q is a normalized linear quantization method that compresses probabilistic symbolic models like HMMs, achieving 99% compression with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Neuro-symbolic applications using HMMs suffer from computational intensity and data transfer bottlenecks due to dense computation requirements of both neural and symbolic components.

Method: Proposes a normalized quantization-aware expectation maximization process for probabilistic model training, reducing bit width of data with minimal impact.

Result: Successfully quantized HMM with 4096 hidden states to 8 bits without loss and 3 bits with acceptable loss, achieving 99% compression rate for HMM weights.

Conclusion: Norm-Q effectively alleviates memory and bandwidth stress while maintaining reasonable performance, enabling deployment on custom hardware with high compression rates.

Abstract: Hidden Markov models (HMM) are commonly used in generation tasks and have
demonstrated strong capabilities in neuro-symbolic applications for the Markov
property. These applications leverage the strengths of neural networks and
symbolic reasoning to create robust and interpretable AI systems. However, they
may inherit and amplify the shortcomings of both approaches. Both components
require dense computation and data transfer, and their communication further
hinders performance. This paper proposes Norm-Q, a normalized linear
quantization approach for compressing probabilistic symbolic models, such as
HMMs. We reduce the bit width of the data with minimal impact, thereby
alleviating memory and bandwidth stress and enabling deployment on potential
custom hardware. Our method introduces a normalized quantization-aware
expectation maximization process for probabilistic model training. The
experimental results show that Norm-Q achieves a higher compression rate with
reasonable score loss compared to traditional quantization methods. In the case
of the constrained generation task of large language models, we successfully
quantize an HMM of 4096 hidden states to 8 bits without loss and, at most, 3
bits with acceptable loss. Notably, the Norm-Q method can achieve a compression
rate of 99% for the weights of the HMM. The code is open source at
https://github.com/superstarghy/Norm-Q.

</details>


### [168] [Joint Embeddings Go Temporal](https://arxiv.org/abs/2509.25449)
*Sofiane Ennadir,Siavash Golkar,Leopoldo Sarra*

Main category: cs.LG

TL;DR: TS-JEPA adapts Joint-Embedding Predictive Architectures for time series representation learning, achieving state-of-the-art performance in classification and forecasting tasks while providing robust general representations.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional self-supervised learning methods (autoregressive and masked modeling) that are vulnerable to noise and confounding variables in time series data.

Method: Proposes Time Series JEPA (TS-JEPA), a Joint-Embedding Predictive Architecture specifically adapted for time series that performs self-supervised learning in latent space rather than input space.

Result: TS-JEPA matches or surpasses current state-of-the-art baselines on different standard datasets for both classification and forecasting tasks, demonstrating strong performance balance across diverse tasks.

Conclusion: The work establishes TS-JEPA as a robust foundation for learning general time series representations and lays groundwork for developing future time series foundation models based on Joint Embedding architectures.

Abstract: Self-supervised learning has seen great success recently in unsupervised
representation learning, enabling breakthroughs in natural language and image
processing. However, these methods often rely on autoregressive and masked
modeling, which aim to reproduce masked information in the input, which can be
vulnerable to the presence of noise or confounding variables. To address this
problem, Joint-Embedding Predictive Architectures (JEPA) has been introduced
with the aim to perform self-supervised learning in the latent space. To
leverage these advancements in the domain of time series, we introduce Time
Series JEPA (TS-JEPA), an architecture specifically adapted for time series
representation learning. We validate TS-JEPA on both classification and
forecasting, showing that it can match or surpass current state-of-the-art
baselines on different standard datasets. Notably, our approach demonstrates a
strong performance balance across diverse tasks, indicating its potential as a
robust foundation for learning general representations. Thus, this work lays
the groundwork for developing future time series foundation models based on
Joint Embedding.

</details>


### [169] [Data-Efficient Multitask DAgger](https://arxiv.org/abs/2509.25466)
*Haotian Fu,Ran Gong,Xiaohan Zhang,Maria Vittoria Minniti,Jigarkumar Patel,Karl Schmeckpeper*

Main category: cs.LG

TL;DR: A data-efficient multitask DAgger framework that distills a single multitask policy from multiple task-specific expert policies using performance-aware scheduling with Kalman filter-based estimation.


<details>
  <summary>Details</summary>
Motivation: Generalist robot policies typically require extensive expert data or simulations for training, which is inefficient and costly.

Method: Proposes a Data-Efficient multitask DAgger framework with performance-aware scheduling strategy using Kalman filter-based estimator to allocate demonstrations across tasks based on learning benefits.

Result: Significantly increases overall task success rate, attains high performance across all tasks using substantially fewer expert demonstrations, and shows better zero-shot transfer to real robots compared to naive DAgger and Behavior Cloning.

Conclusion: The proposed framework enables efficient learning of multitask policies with minimal expert data while maintaining high performance and effective real-world transfer.

Abstract: Generalist robot policies that can perform many tasks typically require
extensive expert data or simulations for training. In this work, we propose a
novel Data-Efficient multitask DAgger framework that distills a single
multitask policy from multiple task-specific expert policies. Our approach
significantly increases the overall task success rate by actively focusing on
tasks where the multitask policy underperforms. The core of our method is a
performance-aware scheduling strategy that tracks how much each task's learning
process benefits from the amount of data, using a Kalman filter-based estimator
to robustly decide how to allocate additional demonstrations across tasks. We
validate our approach on MetaWorld, as well as a suite of diverse
drawer-opening tasks in IsaacLab. The resulting policy attains high performance
across all tasks while using substantially fewer expert demonstrations, and the
visual policy learned with our method in simulation shows better performance
than naive DAgger and Behavior Cloning when transferring zero-shot to a real
robot without using real data.

</details>


### [170] [Conformal Prediction for Signal Temporal Logic Inference](https://arxiv.org/abs/2509.25473)
*Danyang Li,Yixuan Wang,Matthew Cleaveland,Mingyu Cai,Roberto Tron*

Main category: cs.LG

TL;DR: This paper introduces an end-to-end differentiable conformal prediction framework for Signal Temporal Logic (STL) inference that provides statistical guarantees for learned temporal rules while improving both reliability and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing STL inference methods lack formal confidence guarantees for inferred rules, and standard conformal prediction is typically applied as a post-training wrapper without improving model learning.

Method: The approach uses a robustness-based nonconformity score, embeds a smooth CP layer directly into training, and employs a novel loss function that simultaneously optimizes inference accuracy and CP prediction sets with a single term.

Result: Experiments show the method reduces prediction uncertainty (achieving high coverage while reducing prediction set size) and improves accuracy (reducing misclassifications when using fixed thresholds) over state-of-the-art baselines.

Conclusion: The proposed end-to-end differentiable CP framework successfully enhances both reliability and interpretability of STL formulas while providing formal statistical guarantees.

Abstract: Signal Temporal Logic (STL) inference seeks to extract human-interpretable
rules from time-series data, but existing methods lack formal confidence
guarantees for the inferred rules. Conformal prediction (CP) is a technique
that can provide statistical correctness guarantees, but is typically applied
as a post-training wrapper without improving model learning. Instead, we
introduce an end-to-end differentiable CP framework for STL inference that
enhances both reliability and interpretability of the resulting formulas. We
introduce a robustness-based nonconformity score, embed a smooth CP layer
directly into training, and employ a new loss function that simultaneously
optimizes inference accuracy and CP prediction sets with a single term.
Following training, an exact CP procedure delivers statistical guarantees for
the learned STL formulas. Experiments on benchmark time-series tasks show that
our approach reduces uncertainty in predictions (i.e., it achieves high
coverage while reducing prediction set size), and improves accuracy (i.e., the
number of misclassifications when using a fixed threshold) over
state-of-the-art baselines.

</details>


### [171] [Translation from Wearable PPG to 12-Lead ECG](https://arxiv.org/abs/2509.25480)
*Hui Ji,Wei Gao,Pengfei Zhou*

Main category: cs.LG

TL;DR: P2Es is a demographic-aware diffusion framework that generates clinically valid 12-lead ECG from PPG signals using frequency-domain blurring, temporal noise interference, and KNN-based clustering with contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing 12-lead ECG systems are cumbersome for ambulatory monitoring, while PPG-based methods fail to reconstruct multi-lead ECG due to lack of inter-lead constraints and insufficient spatial-temporal modeling.

Method: Uses demographic-aware diffusion framework with forward process (frequency-domain blurring + temporal noise interference) and reverse process (temporal multi-scale generation + frequency deblurring), leveraging KNN clustering and contrastive learning for demographic-specific translation.

Result: Extensive experiments show P2Es outperforms baseline models in 12-lead ECG reconstruction.

Conclusion: P2Es successfully bridges the gap between cumbersome 12-lead ECG systems and limited PPG methods by providing clinically valid ECG reconstruction from PPG signals.

Abstract: The 12-lead electrocardiogram (ECG) is the gold standard for cardiovascular
monitoring, offering superior diagnostic granularity and specificity compared
to photoplethysmography (PPG). However, existing 12-lead ECG systems rely on
cumbersome multi-electrode setups, limiting sustained monitoring in ambulatory
settings, while current PPG-based methods fail to reconstruct multi-lead ECG
due to the absence of inter-lead constraints and insufficient modeling of
spatial-temporal dependencies across leads. To bridge this gap, we introduce
P2Es, an innovative demographic-aware diffusion framework designed to generate
clinically valid 12-lead ECG from PPG signals via three key innovations.
Specifically, in the forward process, we introduce frequency-domain blurring
followed by temporal noise interference to simulate real-world signal
distortions. In the reverse process, we design a temporal multi-scale
generation module followed by frequency deblurring. In particular, we leverage
KNN-based clustering combined with contrastive learning to assign affinity
matrices for the reverse process, enabling demographic-specific ECG
translation. Extensive experimental results show that P2Es outperforms baseline
models in 12-lead ECG reconstruction.

</details>


### [172] [Scalable Disk-Based Approximate Nearest Neighbor Search with Page-Aligned Graph](https://arxiv.org/abs/2509.25487)
*Dingyi Kang,Dongming Jiang,Hanshen Yang,Hang Liu,Bingzhe Li*

Main category: cs.LG

TL;DR: PageANN is a disk-based ANNS framework that uses a page-node graph structure aligned with SSD pages to reduce I/O operations and improve scalability for large-scale vector search.


<details>
  <summary>Details</summary>
Motivation: Existing disk-based ANNS methods suffer from long I/O traversal paths, misalignment with storage I/O granularity, and high in-memory indexing overhead, limiting scalability for large-scale vector search.

Method: Introduces page-node graph structure aligning logical nodes with physical SSD pages, clusters similar vectors into page nodes, uses co-designed disk data layout with merging technique, and implements memory management strategy combining lightweight indexing with coordinated memory-disk data allocation.

Result: Achieves 1.85x-10.83x higher throughput and 51.7%-91.9% lower latency compared to state-of-the-art disk-based ANNS methods across different datasets and memory budgets, while maintaining comparable high recall accuracy.

Conclusion: PageANN significantly outperforms existing disk-based ANNS approaches, providing high performance and scalability for large-scale vector search applications.

Abstract: Approximate Nearest Neighbor Search (ANNS), as the core of vector databases
(VectorDBs), has become widely used in modern AI and ML systems, powering
applications from information retrieval to bio-informatics. While graph-based
ANNS methods achieve high query efficiency, their scalability is constrained by
the available host memory. Recent disk-based ANNS approaches mitigate memory
usage by offloading data to Solid-State Drives (SSDs). However, they still
suffer from issues such as long I/O traversal path, misalignment with storage
I/O granularity, and high in-memory indexing overhead, leading to significant
I/O latency and ultimately limiting scalability for large-scale vector search.
  In this paper, we propose PageANN, a disk-based approximate nearest neighbor
search (ANNS) framework designed for high performance and scalability. PageANN
introduces a page-node graph structure that aligns logical graph nodes with
physical SSD pages, thereby shortening I/O traversal paths and reducing I/O
operations. Specifically, similar vectors are clustered into page nodes, and a
co-designed disk data layout leverages this structure with a merging technique
to store only representative vectors and topology information, avoiding
unnecessary reads. To further improve efficiency, we design a memory management
strategy that combines lightweight indexing with coordinated memory-disk data
allocation, maximizing host memory utilization while minimizing query latency
and storage overhead. Experimental results show that PageANN significantly
outperforms state-of-the-art (SOTA) disk-based ANNS methods, achieving
1.85x-10.83x higher throughput and 51.7%-91.9% lower latency across different
datasets and memory budgets, while maintaining comparable high recall accuracy.

</details>


### [173] [Can Molecular Foundation Models Know What They Don't Know? A Simple Remedy with Preference Optimization](https://arxiv.org/abs/2509.25509)
*Langzhou He,Junyou Zhu,Fangxin Wang,Junhua Liu,Haoyan Xu,Yue Zhao,Philip S. Yu,Qitian Wu*

Main category: cs.LG

TL;DR: Mole-PAIR is a plug-and-play module that improves molecular foundation models' reliability on out-of-distribution data through preference optimization-based OOD detection.


<details>
  <summary>Details</summary>
Motivation: Molecular foundation models suffer from unreliability on OOD samples, particularly chemical hallucination where they make high-confidence incorrect predictions for unknown molecules, limiting their application in high-stakes domains like drug discovery.

Method: Formulates OOD detection as preference optimization over estimated OOD affinity between ID and OOD samples using pairwise learning objective, which essentially optimizes AUROC to consistently rank ID and OOD samples.

Result: Significantly improves OOD detection capabilities across five molecular datasets, achieving up to 45.8%, 43.9%, and 24.3% AUROC improvements under size, scaffold, and assay distribution shifts respectively.

Conclusion: Mole-PAIR provides an effective, cost-effective post-training solution to enhance reliability of molecular foundation models on OOD data through preference-aligned instance ranking.

Abstract: Molecular foundation models are rapidly advancing scientific discovery, but
their unreliability on out-of-distribution (OOD) samples severely limits their
application in high-stakes domains such as drug discovery and protein design. A
critical failure mode is chemical hallucination, where models make
high-confidence yet entirely incorrect predictions for unknown molecules. To
address this challenge, we introduce Molecular Preference-Aligned Instance
Ranking (Mole-PAIR), a simple, plug-and-play module that can be flexibly
integrated with existing foundation models to improve their reliability on OOD
data through cost-effective post-training. Specifically, our method formulates
the OOD detection problem as a preference optimization over the estimated OOD
affinity between in-distribution (ID) and OOD samples, achieving this goal
through a pairwise learning objective. We show that this objective essentially
optimizes AUROC, which measures how consistently ID and OOD samples are ranked
by the model. Extensive experiments across five real-world molecular datasets
demonstrate that our approach significantly improves the OOD detection
capabilities of existing molecular foundation models, achieving up to 45.8%,
43.9%, and 24.3% improvements in AUROC under distribution shifts of size,
scaffold, and assay, respectively.

</details>


### [174] [EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit](https://arxiv.org/abs/2509.25510)
*Chang Liu,Danial Chitnis*

Main category: cs.LG

TL;DR: EEsizer is an LLM-based AI agent that automates transistor sizing for analog circuits by integrating large language models with circuit simulators, achieving successful optimization of a 20-transistor CMOS operational amplifier across multiple technology nodes.


<details>
  <summary>Details</summary>
Motivation: To reduce manual effort in analog circuit design by leveraging LLMs' circuit design knowledge and automating the transistor sizing process without external dependencies.

Method: Integration of LLMs with circuit simulators and custom data analysis functions using prompt engineering and Chain-of-Thought reasoning for iterative design exploration and refinement.

Result: OpenAI o3 successfully optimized a 20-transistor CMOS operational amplifier at 90 nm technology node across three test groups within 20 iterations, demonstrating adaptability and robustness.

Conclusion: LLM-based agents like EEsizer can effectively automate transistor sizing for analog circuits, showing promise for reducing human intervention in AMS IC design.

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often
involves significant manual effort, especially during the transistor sizing
process. While Machine Learning techniques in Electronic Design Automation
(EDA) have shown promise in reducing complexity and minimizing human
intervention, they still face challenges such as numerous iterations and a lack
of knowledge about AMS circuit design. Recently, Large Language Models (LLMs)
have demonstrated significant potential across various fields, showing a
certain level of knowledge in circuit design and indicating their potential to
automate the transistor sizing process. In this work, we propose EEsizer, an
LLM-based AI agent that integrates large language models with circuit
simulators and custom data analysis functions, enabling fully automated,
closed-loop transistor sizing without relying on external knowledge. By
employing prompt engineering and Chain-of-Thought reasoning, the agent
iteratively explores design directions, evaluates performance, and refines
solutions with minimal human intervention. We first benchmarked 8 LLMs on six
basic circuits and selected three high-performing models to optimize a
20-transistor CMOS operational amplifier, targeting multiple performance
metrics, including rail-to-rail operation from 180 nm to 90 nm technology
nodes. Notably, OpenAI o3 successfully achieved the user-intended target at 90
nm across three different test groups, with a maximum of 20 iterations,
demonstrating adaptability and robustness at advanced nodes. To assess design
robustness, we manually designed a bias circuit and performed a variation
analysis using Gaussian-distributed variations on transistor dimensions and
threshold voltages.

</details>


### [175] [World Model for AI Autonomous Navigation in Mechanical Thrombectomy](https://arxiv.org/abs/2509.25518)
*Harry Robertshaw,Han-Ru Wu,Alejandro Granados,Thomas C Booth*

Main category: cs.LG

TL;DR: Proposed TD-MPC2 world model for autonomous mechanical thrombectomy navigation, achieving 65% success rate vs SAC's 37% across multiple patient vasculatures.


<details>
  <summary>Details</summary>
Motivation: Autonomous navigation for mechanical thrombectomy is challenging due to complex vascular anatomy and need for precise real-time decisions. Current RL methods struggle with generalization across patients and long-horizon tasks.

Method: Used TD-MPC2 model-based RL algorithm to train single agent across multiple endovascular navigation tasks in ten real patient vasculatures, compared with Soft Actor-Critic (SAC).

Result: TD-MPC2 significantly outperformed SAC in multi-task learning (65% vs 37% mean success rate) with improved path ratio, though with increased procedure times.

Conclusion: World models show strong potential for improving autonomous endovascular navigation, laying foundation for generalizable AI-driven robotic interventions despite speed-success trade-off.

Abstract: Autonomous navigation for mechanical thrombectomy (MT) remains a critical
challenge due to the complexity of vascular anatomy and the need for precise,
real-time decision-making. Reinforcement learning (RL)-based approaches have
demonstrated potential in automating endovascular navigation, but current
methods often struggle with generalization across multiple patient vasculatures
and long-horizon tasks. We propose a world model for autonomous endovascular
navigation using TD-MPC2, a model-based RL algorithm. We trained a single RL
agent across multiple endovascular navigation tasks in ten real patient
vasculatures, comparing performance against the state-of-the-art Soft
Actor-Critic (SAC) method. Results indicate that TD-MPC2 significantly
outperforms SAC in multi-task learning, achieving a 65% mean success rate
compared to SAC's 37%, with notable improvements in path ratio. TD-MPC2
exhibited increased procedure times, suggesting a trade-off between success
rate and execution speed. These findings highlight the potential of world
models for improving autonomous endovascular navigation and lay the foundation
for future research in generalizable AI-driven robotic interventions.

</details>


### [176] [Flow Matching with Semidiscrete Couplings](https://arxiv.org/abs/2509.25519)
*Alireza Mousavi-Hosseini,Stephen Y. Zhang,Michal Klein,Marco Cuturi*

Main category: cs.LG

TL;DR: This paper proposes Semidiscrete Flow Matching (SD-FM) to overcome the computational bottlenecks of Optimal Transport Flow Matching (OT-FM) by using a semidiscrete formulation that leverages the finite size of target datasets, eliminating quadratic dependencies on batch size.


<details>
  <summary>Details</summary>
Motivation: OT-FM shows promise for flow matching but becomes impractical due to the quadratic computational cost of Sinkhorn algorithm when batch sizes grow large, requiring O(n²/ε²) operations.

Method: The authors propose SD-FM which solves a semidiscrete OT problem by estimating a dual potential vector using SGD, then matches freshly sampled noise vectors with data points via maximum inner product search (MIPS), removing the quadratic dependency on batch size.

Result: SD-FM outperforms both standard FM and OT-FM across all training metrics and inference budget constraints on multiple datasets, for both unconditional and conditional generation tasks.

Conclusion: Semidiscrete Flow Matching provides a practical and efficient alternative to OT-FM that fulfills its theoretical promises while being computationally feasible, making it suitable for real-world applications.

Abstract: Flow models parameterized as time-dependent velocity fields can generate data
from noise by integrating an ODE. These models are often trained using flow
matching, i.e. by sampling random pairs of noise and target points
$(\mathbf{x}_0,\mathbf{x}_1)$ and ensuring that the velocity field is aligned,
on average, with $\mathbf{x}_1-\mathbf{x}_0$ when evaluated along a segment
linking $\mathbf{x}_0$ to $\mathbf{x}_1$. While these pairs are sampled
independently by default, they can also be selected more carefully by matching
batches of $n$ noise to $n$ target points using an optimal transport (OT)
solver. Although promising in theory, the OT flow matching (OT-FM) approach is
not widely used in practice. Zhang et al. (2025) pointed out recently that
OT-FM truly starts paying off when the batch size $n$ grows significantly,
which only a multi-GPU implementation of the Sinkhorn algorithm can handle.
Unfortunately, the costs of running Sinkhorn can quickly balloon, requiring
$O(n^2/\varepsilon^2)$ operations for every $n$ pairs used to fit the velocity
field, where $\varepsilon$ is a regularization parameter that should be
typically small to yield better results. To fulfill the theoretical promises of
OT-FM, we propose to move away from batch-OT and rely instead on a semidiscrete
formulation that leverages the fact that the target dataset distribution is
usually of finite size $N$. The SD-OT problem is solved by estimating a dual
potential vector using SGD; using that vector, freshly sampled noise vectors at
train time can then be matched with data points at the cost of a maximum inner
product search (MIPS). Semidiscrete FM (SD-FM) removes the quadratic dependency
on $n/\varepsilon$ that bottlenecks OT-FM. SD-FM beats both FM and OT-FM on all
training metrics and inference budget constraints, across multiple datasets, on
unconditional/conditional generation, or when using mean-flow models.

</details>


### [177] [Meta-Router: Bridging Gold-standard and Preference-based Evaluations in Large Language Model Routing](https://arxiv.org/abs/2509.25535)
*Yichi Zhang,Fangzheng Xie,Shu Yang,Chong Wu*

Main category: cs.LG

TL;DR: This paper presents a causal inference framework for training LLM routers that combines gold-standard and preference-based data to reduce bias and improve routing efficiency while maintaining response quality.


<details>
  <summary>Details</summary>
Motivation: To address the high cost of deploying single 'best' models and the scarcity of reliable supervision data for training LLM routers, while dealing with biased preference-based data.

Method: Proposes a causal inference framework that views response evaluation mechanisms as treatment assignments, correcting preference-data bias as conditional average treatment effect, and developing an integrative causal router training framework.

Result: Numerical experiments show the approach delivers more accurate routing and improves the trade-off between cost and quality.

Conclusion: The causal inference perspective effectively addresses bias in preference-based data and enables more robust and efficient LLM routing.

Abstract: In language tasks that require extensive human--model interaction, deploying
a single "best" model for every query can be expensive. To reduce inference
cost while preserving the quality of the responses, a large language model
(LLM) router selects the most appropriate model from a pool of candidates for
each query. A central challenge to training a high-quality router is the
scarcity of reliable supervision. Gold-standard data (e.g., expert-verified
labels or rubric-based scores) provide accurate quality evaluations of LLM
responses but are costly and difficult to scale. In contrast, preference-based
data, collected via crowdsourcing or LLM-as-a-judge systems, are cheaper and
more scalable, yet often biased in reflecting the true quality of responses. We
cast the problem of LLM router training with combined gold-standard and
preference-based data into a causal inference framework by viewing the response
evaluation mechanism as the treatment assignment. This perspective further
reveals that the bias in preference-based data corresponds to the well-known
causal estimand: the conditional average treatment effect. Based on this new
perspective, we develop an integrative causal router training framework that
corrects preference-data bias, address imbalances between two data sources, and
improve routing robustness and efficiency. Numerical experiments demonstrate
that our approach delivers more accurate routing and improves the trade-off
between cost and quality.

</details>


### [178] [Steering an Active Learning Workflow Towards Novel Materials Discovery via Queue Prioritization](https://arxiv.org/abs/2509.25538)
*Marcus Schwarting,Logan Ward,Nathaniel Hudson,Xiaoli Yan,Ben Blaiszik,Santanu Chaudhuri,Eliu Huerta,Ian Foster*

Main category: cs.LG

TL;DR: Proposes an active learning approach to improve generative AI workflows for inverse design problems, specifically for molecular structure discovery in carbon capture applications.


<details>
  <summary>Details</summary>
Motivation: Generative AI can autonomously expand search spaces but often explores low-quality regions before fine-tuning, wasting computational resources and potentially causing model decay.

Method: A queue prioritization algorithm that combines generative modeling with active learning to intelligently select top design candidates for further exploration.

Result: The active learning approach significantly improved performance - from 281 to 604 high-performing molecular candidates out of 1000 novel candidates generated.

Conclusion: Incorporating active learning prevents generative AI workflows from wasting resources on nonsensical candidates and halts potential model decay, significantly increasing the number of high-quality design candidates identified.

Abstract: Generative AI poses both opportunities and risks for solving inverse design
problems in the sciences. Generative tools provide the ability to expand and
refine a search space autonomously, but do so at the cost of exploring
low-quality regions until sufficiently fine tuned. Here, we propose a queue
prioritization algorithm that combines generative modeling and active learning
in the context of a distributed workflow for exploring complex design spaces.
We find that incorporating an active learning model to prioritize top design
candidates can prevent a generative AI workflow from expending resources on
nonsensical candidates and halt potential generative model decay. For an
existing generative AI workflow for discovering novel molecular structure
candidates for carbon capture, our active learning approach significantly
increases the number of high-quality candidates identified by the generative
model. We find that, out of 1000 novel candidates, our workflow without active
learning can generate an average of 281 high-performing candidates, while our
proposed prioritization with active learning can generate an average 604
high-performing candidates.

</details>


### [179] [Lightweight and Robust Federated Data Valuation](https://arxiv.org/abs/2509.25560)
*Guojun Tang,Jiayu Zhou,Mohammad Mamun,Steve Drew*

Main category: cs.LG

TL;DR: FedIF is a federated learning aggregation framework that uses trajectory-based influence estimation to efficiently compute client contributions, achieving robustness comparable to Shapley-value methods while reducing aggregation overhead by 450x.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces robustness challenges from non-IID data and adversarial clients. Existing Shapley-value approaches have high computational overhead due to repeated model reweighting and inference, limiting scalability.

Method: FedIF leverages trajectory-based influence estimation with normalized and smoothed influence scores computed from lightweight gradient operations on client updates and a public validation set.

Result: FedIF achieves robustness comparable to or exceeding Shapley-value methods against label noise, gradient noise, and adversarial samples, while reducing aggregation overhead by up to 450x on CIFAR-10 and Fashion-MNIST datasets.

Conclusion: FedIF provides a practical, theoretically grounded, and scalable alternative to Shapley-value-based approaches for efficient and robust federated learning in real-world deployments.

Abstract: Federated learning (FL) faces persistent robustness challenges due to non-IID
data distributions and adversarial client behavior. A promising mitigation
strategy is contribution evaluation, which enables adaptive aggregation by
quantifying each client's utility to the global model. However,
state-of-the-art Shapley-value-based approaches incur high computational
overhead due to repeated model reweighting and inference, which limits their
scalability. We propose FedIF, a novel FL aggregation framework that leverages
trajectory-based influence estimation to efficiently compute client
contributions. FedIF adapts decentralized FL by introducing normalized and
smoothed influence scores computed from lightweight gradient operations on
client updates and a public validation set. Theoretical analysis demonstrates
that FedIF yields a tighter bound on one-step global loss change under noisy
conditions. Extensive experiments on CIFAR-10 and Fashion-MNIST show that FedIF
achieves robustness comparable to or exceeding SV-based methods in the presence
of label noise, gradient noise, and adversarial samples, while reducing
aggregation overhead by up to 450x. Ablation studies confirm the effectiveness
of FedIF's design choices, including local weight normalization and influence
smoothing. Our results establish FedIF as a practical, theoretically grounded,
and scalable alternative to Shapley-value-based approaches for efficient and
robust FL in real-world deployments.

</details>


### [180] [Safe In-Context Reinforcement Learning](https://arxiv.org/abs/2509.25582)
*Amir Moeini,Minjae Kwon,Alper Kamil Bozkurt,Yuichi Motai,Rohan Chandra,Lu Feng,Shangtong Zhang*

Main category: cs.LG

TL;DR: The paper proposes the first safety-promoting method for in-context reinforcement learning (ICRL) within constrained Markov Decision Processes, enabling agents to adapt to out-of-distribution tasks without parameter updates while minimizing costs alongside reward maximization.


<details>
  <summary>Details</summary>
Motivation: ICRL enables agents to adapt to new tasks without parameter updates by expanding input context, but existing methods lack safety considerations. The authors aim to incorporate safety constraints into ICRL's adaptation process.

Method: Proposed a method that integrates constrained Markov Decision Processes framework into ICRL, allowing agents to minimize cost functions during adaptation while maximizing rewards, without requiring parameter updates.

Result: The agent successfully adapts to out-of-distribution tasks while respecting cost constraints, and actively responds to cost tolerance thresholds - behaving more aggressively with higher budgets and more conservatively with lower budgets.

Conclusion: The work establishes the first safety-promoting approach for ICRL, demonstrating that agents can effectively balance reward maximization with cost minimization during parameter-free adaptation, with behavior adaptively responding to cost budget constraints.

Abstract: In-context reinforcement learning (ICRL) is an emerging RL paradigm where the
agent, after some pretraining procedure, is able to adapt to
out-of-distribution test tasks without any parameter updates. The agent
achieves this by continually expanding the input (i.e., the context) to its
policy neural networks. For example, the input could be all the history
experience that the agent has access to until the current time step. The
agent's performance improves as the input grows, without any parameter updates.
In this work, we propose the first method that promotes the safety of ICRL's
adaptation process in the framework of constrained Markov Decision Processes.
In other words, during the parameter-update-free adaptation process, the agent
not only maximizes the reward but also minimizes an additional cost function.
We also demonstrate that our agent actively reacts to the threshold (i.e.,
budget) of the cost tolerance. With a higher cost budget, the agent behaves
more aggressively, and with a lower cost budget, the agent behaves more
conservatively.

</details>


### [181] [Machine Learning Algorithms for Improving Black Box Optimization Solvers](https://arxiv.org/abs/2509.25592)
*Morteza Kimiaei,Vyacheslav Kungurtsev*

Main category: cs.LG

TL;DR: This paper surveys how machine learning and reinforcement learning enhance black-box optimization, transforming classical methods into more scalable, robust, and adaptive frameworks for real-world applications.


<details>
  <summary>Details</summary>
Motivation: Classical derivative-free methods for black-box optimization often struggle with high-dimensional, noisy, or mixed-integer problems, motivating the integration of ML and RL techniques to improve performance.

Method: The survey covers various ML and RL approaches including neural networks with modular frameworks, zeroth-order adaptive momentum methods, automated BBO, distributed optimization, Bayesian optimization variants, transformer-based optimizers, diffusion models, surrogate-assisted RL, and robust optimization techniques.

Result: The paper reviews multiple advanced algorithms and benchmark frameworks that demonstrate improved scalability, robustness, and adaptability in black-box optimization compared to classical methods.

Conclusion: ML and RL techniques are transforming classical black-box optimization solvers into more effective frameworks capable of handling complex real-world optimization challenges across various domains.

Abstract: Black-box optimization (BBO) addresses problems where objectives are
accessible only through costly queries without gradients or explicit structure.
Classical derivative-free methods -- line search, direct search, and
model-based solvers such as Bayesian optimization -- form the backbone of BBO,
yet often struggle in high-dimensional, noisy, or mixed-integer settings.
  Recent advances use machine learning (ML) and reinforcement learning (RL) to
enhance BBO: ML provides expressive surrogates, adaptive updates, meta-learning
portfolios, and generative models, while RL enables dynamic operator
configuration, robustness, and meta-optimization across tasks.
  This paper surveys these developments, covering representative algorithms
such as NNs with the modular model-based optimization framework (mlrMBO),
zeroth-order adaptive momentum methods (ZO-AdaMM), automated BBO (ABBO),
distributed block-wise optimization (DiBB), partition-based Bayesian
optimization (SPBOpt), the transformer-based optimizer (B2Opt),
diffusion-model-based BBO, surrogate-assisted RL for differential evolution
(Surr-RLDE), robust BBO (RBO), coordinate-ascent model-based optimization with
relative entropy (CAS-MORE), log-barrier stochastic gradient descent (LB-SGD),
policy improvement with black-box (PIBB), and offline Q-learning with Mamba
backbones (Q-Mamba).
  We also review benchmark efforts such as the NeurIPS 2020 BBO Challenge and
the MetaBox framework. Overall, we highlight how ML and RL transform classical
inexact solvers into more scalable, robust, and adaptive frameworks for
real-world optimization.

</details>


### [182] [Binary Sparse Coding for Interpretability](https://arxiv.org/abs/2509.25596)
*Lucia Quirke,Stepan Shabalin,Nora Belrose*

Main category: cs.LG

TL;DR: Binary sparse autoencoders (BAEs) and binary transcoders (BTCs) improve feature interpretability by constraining activations to 0 or 1, eliminating continuous variation that can hide uninterpretable information.


<details>
  <summary>Details</summary>
Motivation: Many sparse autoencoder features are only interpretable at high activation strengths, and continuous activation variations can smuggle in uninterpretable information.

Method: Propose binary sparse autoencoders (BAEs) and binary transcoders (BTCs) that constrain all activations to be binary (0 or 1) instead of continuous values.

Result: Binarisation significantly improves interpretability and monosemanticity of features but increases reconstruction error and ultra-high frequency features. Frequency-adjusted interpretability scores show continuous sparse coders perform slightly better.

Conclusion: Polysemanticity may be an ineliminable property of neural activations, as even binary methods don't completely solve the interpretability challenge.

Abstract: Sparse autoencoders (SAEs) are used to decompose neural network activations
into sparsely activating features, but many SAE features are only interpretable
at high activation strengths. To address this issue we propose to use binary
sparse autoencoders (BAEs) and binary transcoders (BTCs), which constrain all
activations to be zero or one. We find that binarisation significantly improves
the interpretability and monosemanticity of the discovered features, while
increasing reconstruction error. By eliminating the distinction between high
and low activation strengths, we prevent uninterpretable information from being
smuggled in through the continuous variation in feature activations. However,
we also find that binarisation increases the number of uninterpretable
ultra-high frequency features, and when interpretability scores are
frequency-adjusted, the scores for continuous sparse coders are slightly better
than those of binary ones. This suggests that polysemanticity may be an
ineliminable property of neural activations.

</details>


### [183] [Effective Model Pruning](https://arxiv.org/abs/2509.25606)
*Yixuan Wang,Dan Guralnik,Saiedeh Akbari,Warren Dixon*

Main category: cs.LG

TL;DR: EMP provides a universal adaptive threshold for model pruning that determines how many parameters to keep based on the Inverse Simpson index, working with any pruning criterion across various model architectures.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental question of how many entries to keep during model pruning without being dependent on specific pruning criteria or requiring parameter tuning.

Method: EMP maps any score vector to an effective number N_eff inspired by Inverse Simpson index, then retains the N_eff highest scoring entries while zeroing the rest.

Result: EMP produces sparse models with performance comparable to original dense networks across MLPs, CNNs, Transformers/LLMs, and KAN architectures.

Conclusion: EMP provides a robust, parameter-free pruning rule that works universally across different pruning criteria and model types, with beta=1 as the default effective threshold.

Abstract: We introduce Effective Model Pruning (EMP), a context-agnostic,
parameter-free rule addressing a fundamental question about pruning: how many
entries to keep. EMP does not prescribe how to score the parameters or prune
the models; instead, it supplies a universal adaptive threshold that can be
applied to any pruning criterion: weight magnitude, attention score, KAN
importance score, or even feature-level signals such as image pixel, and used
on structural parts or weights of the models. Given any score vector s, EMP
maps s to a built-in effective number N_eff which is inspired by the Inverse
Simpson index of contributors. Retaining the N_eff highest scoring entries and
zeroing the remainder yields sparse models with performance comparable to the
original dense networks across MLPs, CNNs, Transformers/LLMs, and KAN, in our
experiments. By leveraging the geometry of the simplex, we derive a tight lower
bound on the preserved mass s_eff (the sum of retained scores) over the
corresponding ordered probability simplex associated with the score vector s.
We further verify the effectiveness of N_eff by pruning the model with a scaled
threshold \b{eta}*N_eff across a variety of criteria and models. Experiments
suggest that the default \b{eta} = 1 yields a robust threshold for model
pruning while \b{eta} not equal to 1 still serves as an optional adjustment to
meet specific sparsity requirements.

</details>


### [184] [Unsupervised Detection of Spatiotemporal Anomalies in PMU Data Using Transformer-Based BiGAN](https://arxiv.org/abs/2509.25612)
*Muhammad Imran Hossain,Jignesh Solanki,Sarika Khushlani Solanki*

Main category: cs.LG

TL;DR: T-BiGAN is a novel unsupervised anomaly detection framework that combines window-attention Transformers with bidirectional GANs to detect anomalies in synchrophasor data streams for power grid resilience.


<details>
  <summary>Details</summary>
Motivation: To ensure power grid resilience through timely and unsupervised detection of anomalies in synchrophasor data streams without relying on manually labeled fault data.

Method: Integrates window-attention Transformers within a bidirectional GAN (BiGAN) with self-attention encoder-decoder architecture to capture complex spatio-temporal dependencies, and uses a joint discriminator for cycle consistency. Anomalies are detected using an adaptive score combining reconstruction error, latent space drift, and discriminator confidence.

Result: Achieves ROC-AUC of 0.95 and average precision of 0.996 on realistic hardware-in-the-loop PMU benchmark, significantly outperforming leading supervised and unsupervised methods. Shows particular strength in detecting subtle frequency and voltage deviations.

Conclusion: T-BiGAN demonstrates practical value for live, wide-area monitoring of power grids by providing effective unsupervised anomaly detection without requiring labeled fault data.

Abstract: Ensuring power grid resilience requires the timely and unsupervised detection
of anomalies in synchrophasor data streams. We introduce T-BiGAN, a novel
framework that integrates window-attention Transformers within a bidirectional
Generative Adversarial Network (BiGAN) to address this challenge. Its
self-attention encoder-decoder architecture captures complex spatio-temporal
dependencies across the grid, while a joint discriminator enforces cycle
consistency to align the learned latent space with the true data distribution.
Anomalies are flagged in real-time using an adaptive score that combines
reconstruction error, latent space drift, and discriminator confidence.
Evaluated on a realistic hardware-in-the-loop PMU benchmark, T-BiGAN achieves
an ROC-AUC of 0.95 and an average precision of 0.996, significantly
outperforming leading supervised and unsupervised methods. It shows particular
strength in detecting subtle frequency and voltage deviations, demonstrating
its practical value for live, wide-area monitoring without relying on manually
labeled fault data.

</details>


### [185] [Layer-wise dynamic rank for compressing large language models](https://arxiv.org/abs/2509.25622)
*Zhendong Mi,Bian Sun,Grace Li Zhang,Shaoyi Huang*

Main category: cs.LG

TL;DR: D-Rank is a dynamic rank allocation framework for LLM compression that addresses layer heterogeneity by allocating more capacity to information-rich middle layers using effective rank metrics and Lagrange optimization.


<details>
  <summary>Details</summary>
Motivation: Existing SVD-based compression methods use uniform compression ratios across all layers, ignoring the substantial intra-layer heterogeneity in LLMs where middle layers encode richer information while early and late layers are more redundant.

Method: Proposes D-Rank framework with layer-wise balanced dynamic rank allocation: 1) Uses effective rank to measure information density of weight matrices, 2) Allocates ranks via Lagrange multiplier-based optimization to assign more capacity to higher information density groups, 3) Rebalances ranks across attention layers accounting for varying importance, 4) Extends to LLMs with grouped-query attention.

Result: Extensive experiments show D-Rank consistently outperforms SVD-LLM, ASVD, and Basis Sharing: achieves >15% lower perplexity with LLaMA-3-8B on C4 at 20% compression ratio, and up to 5% higher zero-shot reasoning accuracy with LLaMA-7B at 40% compression ratio while achieving higher throughput.

Conclusion: D-Rank demonstrates that adaptive layer-wise rank allocation based on information density significantly improves LLM compression performance compared to uniform compression approaches, effectively addressing the heterogeneity in different layers.

Abstract: Large language models (LLMs) have rapidly scaled in size, bringing severe
memory and computational challenges that hinder their deployment. Singular
Value Decomposition (SVD)-based compression has emerged as an appealing
post-training compression technique for LLMs, yet most existing methods apply a
uniform compression ratio across all layers, implicitly assuming homogeneous
information included in various layers. This overlooks the substantial
intra-layer heterogeneity observed in LLMs, where middle layers tend to encode
richer information while early and late layers are more redundant. In this
work, we revisit the existing SVD-based compression method and propose D-Rank,
a framework with layer-wise balanced Dynamic Rank allocation for LLMs
compression. We first introduce effective rank as a principled metric to
measure the information density of weight matrices, and then allocate ranks via
a Lagrange multiplier-based optimization scheme to adaptively assign more
capacity to groups with higher information density under a fixed compression
ratio. Moreover, we rebalance the allocated ranks across attention layers to
account for their varying importance and extend D-Rank to latest LLMs with
grouped-query attention. Extensive experiments on various LLMs with different
scales across multiple compression ratios demonstrate that D-Rank consistently
outperforms SVD-LLM, ASVD, and Basis Sharing, achieving more than 15 lower
perplexity with LLaMA-3-8B model on C4 datasets at 20% compression ratio and up
to 5% higher zero-shot reasoning accuracy with LLaMA-7B model at 40%
compression ratio while achieving even higher throughput.

</details>


### [186] [Swift: An Autoregressive Consistency Model for Efficient Weather Forecasting](https://arxiv.org/abs/2509.25631)
*Jason Stock,Troy Arcomano,Rao Kotamarthi*

Main category: cs.LG

TL;DR: Swift is a single-step consistency model that enables efficient probabilistic weather forecasting by eliminating slow iterative solvers, achieving 39x speedup over diffusion baselines while maintaining competitive forecast skill up to 75 days.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for weather forecasting rely on slow iterative solvers during inference, making them impractical for subseasonal-to-seasonal applications that require long lead-times and domain-driven calibration.

Method: Introduce Swift, a single-step consistency model that enables autoregressive finetuning of a probability flow model with continuous ranked probability score (CRPS) objective, eliminating the need for multi-model ensembling or parameter perturbations.

Result: Swift produces skillful 6-hourly forecasts stable for up to 75 days, running 39x faster than state-of-the-art diffusion baselines while achieving forecast skill competitive with operational IFS ENS numerical model.

Conclusion: This represents a step toward efficient and reliable ensemble forecasting from medium-range to seasonal scales, making probabilistic weather forecasting more practical for operational applications.

Abstract: Diffusion models offer a physically grounded framework for probabilistic
weather forecasting, but their typical reliance on slow, iterative solvers
during inference makes them impractical for subseasonal-to-seasonal (S2S)
applications where long lead-times and domain-driven calibration are essential.
To address this, we introduce Swift, a single-step consistency model that, for
the first time, enables autoregressive finetuning of a probability flow model
with a continuous ranked probability score (CRPS) objective. This eliminates
the need for multi-model ensembling or parameter perturbations. Results show
that Swift produces skillful 6-hourly forecasts that remain stable for up to 75
days, running $39\times$ faster than state-of-the-art diffusion baselines while
achieving forecast skill competitive with the numerical-based, operational IFS
ENS. This marks a step toward efficient and reliable ensemble forecasting from
medium-range to seasonal-scales.

</details>


### [187] [How Does Preconditioning Guide Feature Learning in Deep Neural Networks?](https://arxiv.org/abs/2509.25637)
*Kotaro Yoshida,Atsushi Nitanda*

Main category: cs.LG

TL;DR: Preconditioning affects generalization by inducing spectral bias through the Gram matrix metric. The exponent p in preconditioner and alignment with teacher spectrum are crucial for feature learning and generalization performance.


<details>
  <summary>Details</summary>
Motivation: To understand how preconditioning affects feature learning and generalization performance on expected risk, beyond just accelerating convergence on empirical risk.

Method: Analyze preconditioning as p-th power of input covariance matrix within single-index teacher model, examining spectral bias effects on feature learning from three perspectives: noise robustness, OOD generalization, and forward knowledge transfer.

Result: Learned features mirror preconditioner's spectral bias - favoring emphasized components and reducing sensitivity to suppressed ones. Generalization significantly improves when spectral bias aligns with teacher's bias.

Conclusion: Preconditioning's spectral bias plays crucial role in generalization, with optimal performance achieved when preconditioner's bias aligns with teacher model's spectral characteristics.

Abstract: Preconditioning is widely used in machine learning to accelerate convergence
on the empirical risk, yet its role on the expected risk remains underexplored.
In this work, we investigate how preconditioning affects feature learning and
generalization performance. We first show that the input information available
to the model is conveyed solely through the Gram matrix defined by the
preconditioner's metric, thereby inducing a controllable spectral bias on
feature learning. Concretely, instantiating the preconditioner as the $p$-th
power of the input covariance matrix and within a single-index teacher model,
we prove that in generalization, the exponent $p$ and the alignment between the
teacher and the input spectrum are crucial factors. We further investigate how
the interplay between these factors influences feature learning from three
complementary perspectives: (i) Robustness to noise, (ii) Out-of-distribution
generalization, and (iii) Forward knowledge transfer. Our results indicate that
the learned feature representations closely mirror the spectral bias introduced
by the preconditioner -- favoring components that are emphasized and exhibiting
reduced sensitivity to those that are suppressed. Crucially, we demonstrate
that generalization is significantly enhanced when this spectral bias is
aligned with that of the teacher.

</details>


### [188] [Deep set based operator learning with uncertainty quantification](https://arxiv.org/abs/2509.25646)
*Lei Ma,Ling Guo,Hao Wu,Tao Zhou*

Main category: cs.LG

TL;DR: UQ-SONet is a permutation-invariant operator learning framework with built-in uncertainty quantification that handles sparse and variable sensor locations using set transformer embeddings and conditional variational autoencoders.


<details>
  <summary>Details</summary>
Motivation: Existing operator learning methods like DeepONets have limitations: they require fixed sensor configurations, lack uncertainty quantification mechanisms, and cannot handle sparse observations or operators with inherent randomness.

Method: Integrates set transformer embedding to handle variable sensor locations and uses conditional variational autoencoder (cVAE) to approximate the conditional distribution of solution operators, minimizing negative ELBO for principled uncertainty estimation.

Result: Numerical experiments on deterministic and stochastic PDEs (including Navier-Stokes) demonstrate the framework's robustness and effectiveness in providing uncertainty quantification while maintaining predictive accuracy.

Conclusion: UQ-SONet successfully addresses key limitations of existing operator learning methods by providing built-in uncertainty quantification and handling sparse, variable sensor configurations, making it more practical for real-world applications.

Abstract: Learning operators from data is central to scientific machine learning. While
DeepONets are widely used for their ability to handle complex domains, they
require fixed sensor numbers and locations, lack mechanisms for uncertainty
quantification (UQ), and are thus limited in practical applicability. Recent
permutationinvariant extensions, such as the Variable-Input Deep Operator
Network (VIDON), relax these sensor constraints but still rely on sufficiently
dense observations and cannot capture uncertainties arising from incomplete
measurements or from operators with inherent randomness. To address these
challenges, we propose UQ-SONet, a permutation-invariant operator learning
framework with built-in UQ. Our model integrates a set transformer embedding to
handle sparse and variable sensor locations, and employs a conditional
variational autoencoder (cVAE) to approximate the conditional distribution of
the solution operator. By minimizing the negative ELBO, UQ-SONet provides
principled uncertainty estimation while maintaining predictive accuracy.
Numerical experiments on deterministic and stochastic PDEs, including the
Navier-Stokes equation, demonstrate the robustness and effectiveness of the
proposed framework.

</details>


### [189] [BaB-prob: Branch and Bound with Preactivation Splitting for Probabilistic Verification of Neural Networks](https://arxiv.org/abs/2509.25647)
*Fangji Wang,Panagiotis Tsiotras*

Main category: cs.LG

TL;DR: BaB-prob extends branch-and-bound with preactivation splitting to probabilistic verification of neural networks, proving soundness and completeness for feedforward-ReLU networks and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Extend the effective deterministic branch-and-bound verification framework with preactivation splitting to the probabilistic setting for neural network verification.

Method: BaB-prob iteratively divides problems into subproblems by splitting preactivations, uses linear bound propagation to bound probabilities, introduces uncertainty level concept, and employs efficient preactivation splitting strategies (BaB-prob-ordered and BaB+BaBSR-prob).

Result: BaB-prob consistently outperforms state-of-the-art approaches on untrained networks, MNIST and CIFAR-10 models, and VNN-COMP 2025 benchmarks, especially in medium- to high-dimensional input problems.

Conclusion: The proposed BaB-prob framework successfully extends branch-and-bound verification to probabilistic settings, demonstrating superior performance across various network types and benchmarks.

Abstract: Branch-and-bound with preactivation splitting has been shown highly effective
for deterministic verification of neural networks. In this paper, we extend
this framework to the probabilistic setting. We propose BaB-prob that
iteratively divides the original problem into subproblems by splitting
preactivations and leverages linear bounds computed by linear bound propagation
to bound the probability for each subproblem. We prove soundness and
completeness of BaB-prob for feedforward-ReLU neural networks. Furthermore, we
introduce the notion of uncertainty level and design two efficient strategies
for preactivation splitting, yielding BaB-prob-ordered and BaB+BaBSR-prob. We
evaluate BaB-prob on untrained networks, MNIST and CIFAR-10 models,
respectively, and VNN-COMP 2025 benchmarks. Across these settings, our approach
consistently outperforms state-of-the-art approaches in medium- to
high-dimensional input problems.

</details>


### [190] [Growing Winning Subnetworks, Not Pruning Them: A Paradigm for Density Discovery in Sparse Neural Networks](https://arxiv.org/abs/2509.25665)
*Qihang Yao,Constantine Dovrolis*

Main category: cs.LG

TL;DR: PWMPR is a growth-based sparse training method that starts from sparse networks and adds edges using path-kernel-inspired scores, automatically discovering optimal density without pre-specifying target sparsity.


<details>
  <summary>Details</summary>
Motivation: Existing sparse training methods like iterative pruning and dynamic sparse training either require heavy retraining costs or assume fixed target density in advance, limiting their practical applicability.

Method: Path Weight Magnitude Product-biased Random growth (PWMPR) - a constructive sparse-to-dense training that grows networks from sparse seeds, adds edges using path-kernel-inspired scores, mitigates bottlenecks via randomization, and stops when accuracy plateaus are detected.

Result: PWMPR approaches the performance of IMP-derived lottery tickets (though at higher density) with substantially lower computational cost (~1.5x dense training vs. 3-4x for IMP) on CIFAR, TinyImageNet, and ImageNet datasets.

Conclusion: Growth-based density discovery is established as a promising paradigm that complements existing pruning and dynamic sparsity approaches for efficient sparse neural network training.

Abstract: The lottery ticket hypothesis suggests that dense networks contain sparse
subnetworks that can be trained in isolation to match full-model performance.
Existing approaches-iterative pruning, dynamic sparse training, and pruning at
initialization-either incur heavy retraining costs or assume the target density
is fixed in advance. We introduce Path Weight Magnitude Product-biased Random
growth (PWMPR), a constructive sparse-to-dense training paradigm that grows
networks rather than pruning them, while automatically discovering their
operating density. Starting from a sparse seed, PWMPR adds edges guided by
path-kernel-inspired scores, mitigates bottlenecks via randomization, and stops
when a logistic-fit rule detects plateauing accuracy. Experiments on CIFAR,
TinyImageNet, and ImageNet show that PWMPR approaches the performance of
IMP-derived lottery tickets-though at higher density-at substantially lower
cost (~1.5x dense vs. 3-4x for IMP). These results establish growth-based
density discovery as a promising paradigm that complements pruning and dynamic
sparsity.

</details>


### [191] [Nudging the Boundaries of LLM Reasoning](https://arxiv.org/abs/2509.25666)
*Justin Chih-Yao Chen,Becky Xiangyu Peng,Prafulla Kumar Choubey,Kung-Hsiang Huang,Jiaxin Zhang,Mohit Bansal,Chien-Sheng Wu*

Main category: cs.LG

TL;DR: NuRL is a novel reinforcement learning method that enables LLMs to learn from previously unsolvable problems by generating and injecting self-generated hints, thereby pushing the upper bound of reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Current RL algorithms like GRPO cannot learn from problems that are unsolvable to the model, as no rollouts yield rewards and thus no gradients are produced. This limits the model's upper performance ceiling.

Method: The model generates self-generated hints containing core knowledge needed to solve problems. For hard samples with 0% pass rate, hints are injected and new trajectories are generated, boosting pass rates from 0% to non-zero.

Result: NuRL achieves consistent improvements across 6 benchmarks and 3 models, raising the model's upper limit while GRPO leaves pass@1024 unchanged. It remains complementary to test-time scaling.

Conclusion: NuRL successfully enables learning from hard samples through self-generated hints, with abstract and high-level hints being most effective when applied necessarily after GRPO convergence.

Abstract: Current online reinforcement learning (RL) algorithms like GRPO share a key
limitation in LLM reasoning: they cannot learn from problems that are
"unsolvable" to the model. In other words, they can only improve performance on
problems where the model is capable of exploring the correct answer.
Consequently, the model's "upper limit" remains unchanged after RL training,
even though the likelihood of solving easier, solvable problems may increase.
These hard samples cannot contribute to training, as no rollouts yield rewards
and thus no gradients are produced. To unlock learning from these hard samples,
we propose NuRL, a "nudging" method that aims to push the upper bound of LLM
reasoning using self-generated hints, i.e., abstract cues that help reduce the
problem difficulty for the model. Given a question and its gold answer, the
model generates a CoT and then produces a hint containing the core knowledge
needed to solve the problem. During training, we generate G rollouts from the
base policy and use the pass rate to decide whether the hint should be
injected. For hard samples with a 0% pass rate, we inject the hint and
regenerate a new batch of trajectories. This yields two benefits: (1) the hint
boosts pass rates (from 0% to non-zero), thereby introducing training signals
for previously unsolvable samples, and (2) the hints are self-generated,
avoiding distributional shift and do not rely on external models. NuRL achieves
consistent improvements across 6 benchmarks and 3 models, while remaining
complementary to test-time scaling. Notably, NuRL can raise the model's upper
limit, whereas GRPO leaves pass@1024 unchanged from the base model.
Furthermore, we present a systematic study of what makes an effective hint and
when hints are most useful. Interestingly, the best hints are abstract and
high-level, and are most beneficial when applied necessarily and after GRPO has
converged.

</details>


### [192] [EEG-based AI-BCI Wheelchair Advancement: Hybrid Deep Learning with Motor Imagery for Brain Computer Interface](https://arxiv.org/abs/2509.25667)
*Bipul Thapa,Biplov Paneru,Bishwash Paneru,Khem Narayan Poudyal*

Main category: cs.LG

TL;DR: AI-powered BCI wheelchair control using motor imagery of right-left hand movements with EEG data, achieving 92.26% accuracy using BiLSTM-BiGRU model.


<details>
  <summary>Details</summary>
Motivation: To develop an intuitive and functional BCI-based wheelchair control system using motor imagery for improved accessibility and independence for users.

Method: Used EEG data segmented into 19x200 arrays from open-source repository, integrated Tkinter interface for simulation, and proposed BiLSTM-BiGRU attention-based model for classification.

Result: BiLSTM-BiGRU model achieved 92.26% test accuracy, outperforming XGBoost, EEGNet, and transformer models, with 90.13% mean accuracy in cross-validation.

Conclusion: The attention-based BiLSTM-BiGRU model demonstrates strong potential for BCI applications, providing accurate and reliable wheelchair control through motor imagery.

Abstract: This paper presents an Artificial Intelligence (AI) integrated novel approach
to Brain-Computer Interface (BCI)-based wheelchair development, utilizing a
motor imagery right-left-hand movement mechanism for control. The system is
designed to simulate wheelchair navigation based on motor imagery right and
left-hand movements using electroencephalogram (EEG) data. A pre-filtered
dataset, obtained from an open-source EEG repository, was segmented into arrays
of 19x200 to capture the onset of hand movements. The data was acquired at a
sampling frequency of 200Hz. The system integrates a Tkinter-based interface
for simulating wheelchair movements, offering users a functional and intuitive
control system. We propose a BiLSTM-BiGRU model that shows a superior test
accuracy of 92.26% as compared with various machine learning baseline models,
including XGBoost, EEGNet, and a transformer-based model. The Bi-LSTM-BiGRU
attention-based model achieved a mean accuracy of 90.13% through
cross-validation, showcasing the potential of attention mechanisms in BCI
applications.

</details>


### [193] [Guiding Mixture-of-Experts with Temporal Multimodal Interactions](https://arxiv.org/abs/2509.25678)
*Xing Han,Hsing-Huan Chung,Joydeep Ghosh,Paul Pu Liang,Suchi Saria*

Main category: cs.LG

TL;DR: A novel MoE framework that uses temporal multimodal interaction dynamics to guide expert routing, improving performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current MoE routing mechanisms ignore time-varying interaction dynamics between modalities, limiting expert specialization and effective reasoning.

Method: Proposes a multimodal interaction-aware router that dispatches tokens to experts based on temporal interaction patterns, encouraging generalizable interaction-processing skills.

Result: Comprehensive experiments on multimodal benchmarks show enhanced performance and improved interpretability.

Conclusion: Leveraging temporal multimodal interactions improves MoE design and performance, enabling better expert specialization.

Abstract: Mixture-of-Experts (MoE) architectures have become pivotal for large-scale
multimodal models. However, their routing mechanisms typically overlook the
informative, time-varying interaction dynamics between modalities. This
limitation hinders expert specialization, as the model cannot explicitly
leverage intrinsic modality relationships for effective reasoning. To address
this, we propose a novel framework that guides MoE routing using quantified
temporal interaction. A multimodal interaction-aware router learns to dispatch
tokens to experts based on the nature of their interactions. This dynamic
routing encourages experts to acquire generalizable interaction-processing
skills rather than merely learning task-specific features. Our framework builds
on a new formulation of temporal multimodal interaction dynamics, which are
used to guide expert routing. We first demonstrate that these temporal
multimodal interactions reveal meaningful patterns across applications, and
then show how they can be leveraged to improve both the design and performance
of MoE-based models. Comprehensive experiments on challenging multimodal
benchmarks validate our approach, demonstrating both enhanced performance and
improved interpretability.

</details>


### [194] [Minimalist Explanation Generation and Circuit Discovery](https://arxiv.org/abs/2509.25686)
*Pirzada Suhail,Aditya Anand,Amit Sethi*

Main category: cs.LG

TL;DR: This paper introduces an activation-matching approach to generate minimal and faithful explanations for image classifier decisions, using a lightweight autoencoder to create binary masks that highlight critical image regions while maintaining model decision consistency.


<details>
  <summary>Details</summary>
Motivation: Machine learning models learn many decision rules that are difficult to identify and interpret in high-dimensional spaces, creating a need for minimal and human-readable explanations that preserve model decisions.

Method: Train a lightweight autoencoder to produce binary masks that highlight decision-critical image regions, integrating activation alignment across layers, output consistency, sparsity priors, compactness, and robustness constraints. Also introduce a circuit readout procedure using explanations' forward pass and gradients to identify active channels and construct channel-level graphs.

Result: The approach generates minimal explanations that preserve model decisions while being concise and human-readable, and enables mechanistic interpretation of model internals through channel-level circuit analysis.

Conclusion: The method provides a practical bridge between minimal input-level explanations and mechanistic understanding of internal computations driving model decisions, offering both interpretable explanations and insights into model internals.

Abstract: Machine learning models, by virtue of training, learn a large repertoire of
decision rules for any given input, and any one of these may suffice to justify
a prediction. However, in high-dimensional input spaces, such rules are
difficult to identify and interpret. In this paper, we introduce an
activation-matching based approach to generate minimal and faithful
explanations for the decisions of pre-trained image classifiers. We aim to
identify minimal explanations that not only preserve the model's decision but
are also concise and human-readable. To achieve this, we train a lightweight
autoencoder to produce binary masks that learns to highlight the decision-wise
critical regions of an image while discarding irrelevant background. The
training objective integrates activation alignment across multiple layers,
consistency at the output label, priors that encourage sparsity, and
compactness, along with a robustness constraint that enforces faithfulness. The
minimal explanations so generated also lead us to mechanistically interpreting
the model internals. In this regard we also introduce a circuit readout
procedure wherein using the explanation's forward pass and gradients, we
identify active channels and construct a channel-level graph, scoring
inter-layer edges by ingress weight magnitude times source activation and
feature-to-class links by classifier weight magnitude times feature activation.
Together, these contributions provide a practical bridge between minimal
input-level explanations and a mechanistic understanding of the internal
computations driving model decisions.

</details>


### [195] [A Unified Probabilistic Framework for Dictionary Learning with Parsimonious Activation](https://arxiv.org/abs/2509.25690)
*Zihui Zhao,Yuanbo Tang,Jieyu Ren,Xiaoping Zhang,Yang Li*

Main category: cs.LG

TL;DR: The paper proposes a new dictionary learning method using row-wise L∞ norm regularization to promote atom sparsity across samples, reducing redundant dictionary atoms while improving reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Traditional dictionary learning focuses on sample-wise sparsity but overlooks how atoms are shared across samples, leading to redundant dictionaries and suboptimal performance.

Method: Introduces a parsimony-promoting regularizer based on row-wise L∞ norm of coefficient matrix, derived from Beta-Bernoulli priors, with theoretical analysis for hyperparameter selection connecting to Bayesian model selection principles.

Result: Achieves 20% RMSE reduction in reconstruction quality, uses less than 1/10 of available dictionary atoms, and demonstrates enhanced representation sparsity on benchmark datasets.

Conclusion: The proposed method effectively reduces dictionary redundancy while improving reconstruction performance, with theoretical foundations validated empirically.

Abstract: Dictionary learning is traditionally formulated as an $L_1$-regularized
signal reconstruction problem. While recent developments have incorporated
discriminative, hierarchical, or generative structures, most approaches rely on
encouraging representation sparsity over individual samples that overlook how
atoms are shared across samples, resulting in redundant and sub-optimal
dictionaries. We introduce a parsimony promoting regularizer based on the
row-wise $L_\infty$ norm of the coefficient matrix. This additional penalty
encourages entire rows of the coefficient matrix to vanish, thereby reducing
the number of dictionary atoms activated across the dataset. We derive the
formulation from a probabilistic model with Beta-Bernoulli priors, which
provides a Bayesian interpretation linking the regularization parameters to
prior distributions. We further establish theoretical calculation for optimal
hyperparameter selection and connect our formulation to both Minimum
Description Length, Bayesian model selection and pathlet learning. Extensive
experiments on benchmark datasets demonstrate that our method achieves
substantially improved reconstruction quality (with a 20\% reduction in RMSE)
and enhanced representation sparsity, utilizing fewer than one-tenth of the
available dictionary atoms, while empirically validating our theoretical
analysis.

</details>


### [196] [Annotation-Efficient Active Test-Time Adaptation with Conformal Prediction](https://arxiv.org/abs/2509.25692)
*Tingyu Shi,Fan Lyu,Shaoliang Peng*

Main category: cs.LG

TL;DR: CPATTA introduces conformal prediction to Active Test-Time Adaptation, using principled uncertainty measures and online algorithms to improve data selection efficiency and achieve 5% higher accuracy than state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing ATTA methods use heuristic uncertainty measures and suffer from low data selection efficiency, wasting human annotation budget during deployment under domain shift.

Method: Uses smoothed conformal scores with top-K certainty measure, online weight-update algorithm driven by pseudo coverage, domain-shift detector for adaptive human supervision, and staged update scheme balancing human-labeled and model-labeled data.

Result: Extensive experiments show CPATTA consistently outperforms state-of-the-art ATTA methods by around 5% in accuracy.

Conclusion: CPATTA successfully brings principled, coverage-guaranteed uncertainty into ATTA, significantly improving data selection efficiency and model robustness under domain shift.

Abstract: Active Test-Time Adaptation (ATTA) improves model robustness under domain
shift by selectively querying human annotations at deployment, but existing
methods use heuristic uncertainty measures and suffer from low data selection
efficiency, wasting human annotation budget. We propose Conformal Prediction
Active TTA (CPATTA), which first brings principled, coverage-guaranteed
uncertainty into ATTA. CPATTA employs smoothed conformal scores with a top-K
certainty measure, an online weight-update algorithm driven by pseudo coverage,
a domain-shift detector that adapts human supervision, and a staged update
scheme balances human-labeled and model-labeled data. Extensive experiments
demonstrate that CPATTA consistently outperforms the state-of-the-art ATTA
methods by around 5% in accuracy. Our code and datasets are available at
https://github.com/tingyushi/CPATTA.

</details>


### [197] [Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?](https://arxiv.org/abs/2509.25696)
*Takuya Fujimura,Kota Dohi,Natsuo Yamashita,Yohei Kawaguchi*

Main category: cs.LG

TL;DR: Proposes training TSQA models using pseudo labels from vision-language models, leveraging neural networks' robustness to label noise to overcome data scarcity.


<details>
  <summary>Details</summary>
Motivation: Time-series question answering faces challenges due to limited labeled data, while vision-language models show potential for zero-shot time-series analysis.

Method: Use pseudo labels generated by a vision-language model to train TSQA models, exploiting deep neural networks' inherent robustness to noisy labels.

Result: TSQA models trained with pseudo labels not only succeed but outperform the original VLM by utilizing large amounts of unlabeled data.

Conclusion: Pseudo labeling from VLMs provides an effective solution for TSQA data scarcity, enabling models to exceed VLM performance through access to abundant unlabeled data.

Abstract: Time-series question answering (TSQA) tasks face significant challenges due
to the lack of labeled data. Alternatively, with recent advancements in
large-scale models, vision-language models (VLMs) have demonstrated the
potential to analyze time-series signals in a zero-shot manner. In this paper,
we propose a training approach that uses pseudo labels generated by a VLM.
Although VLMs can produce incorrect labels, TSQA models can still be
effectively trained based on the property that deep neural networks are
inherently robust to such noisy labels. Our experimental results demonstrate
that TSQA models are not only successfully trained with pseudo labels, but also
surpass the performance of the VLM itself by leveraging a large amount of
unlabeled data.

</details>


### [198] [Physics-Informed Learning for Human Whole-Body Kinematics Prediction via Sparse IMUs](https://arxiv.org/abs/2509.25704)
*Cheng Guo,Giuseppe L'Erario,Giulio Romualdi,Mattia Leonori,Marta Lorenzini,Arash Ajoudani,Daniele Pucci*

Main category: cs.LG

TL;DR: A physics-informed learning framework that uses only 5 IMUs to predict human motion, incorporating domain knowledge in training and inference for accurate, physically feasible predictions.


<details>
  <summary>Details</summary>
Motivation: Existing human motion prediction approaches lack future predictions and physical constraints, and rely heavily on past poses that may not be available in real-world scenarios.

Method: Proposed a network accounting for spatial characteristics of human movements. During training, incorporated forward and differential kinematics as additional loss components. During inference, refined predictions using joint state buffer as extra inputs.

Result: Achieved high accuracy, smooth transitions between motions, and good generalization to unseen subjects.

Conclusion: The physics-informed learning framework successfully addresses limitations of conventional approaches by integrating domain knowledge, enabling accurate human motion prediction with minimal sensor requirements.

Abstract: Accurate and physically feasible human motion prediction is crucial for safe
and seamless human-robot collaboration. While recent advancements in human
motion capture enable real-time pose estimation, the practical value of many
existing approaches is limited by the lack of fu- ture predictions and
consideration of physical constraints. Conventional motion prediction schemes
rely heavily on past poses, which are not always available in real-world
scenarios. To address these limitations, we present a physics-informed learning
framework that integrates domain knowledge into both training and inference to
predict human motion using inertial measurements from only 5 IMUs. We propose a
network that accounts for the spatial characteristics of human movements.
During training, we incorporate forward and differential kinematics functions
as additional loss components to regularize the learned joint predictions. At
the inference stage, we refine the prediction from the previous iteration to
update a joint state buffer, which is used as extra inputs to the network.
Experimental results demonstrate that our approach achieves high accuracy,
smooth transitions between motions, and generalizes well to unseen subjects

</details>


### [199] [Adaptive Graph Coarsening for Efficient GNN Training](https://arxiv.org/abs/2509.25706)
*Rostyslav Olshevskyi,Madeline Navarro,Santiago Segarra*

Main category: cs.LG

TL;DR: Proposes adaptive graph coarsening method that jointly trains GNN parameters and merges nodes via K-means clustering during training, enabling task-adaptive graph reduction for large-scale graphs.


<details>
  <summary>Details</summary>
Motivation: Real-world graphs are growing larger, making direct processing challenging and sometimes infeasible. Tailoring algorithms to large-scale data may sacrifice performance, so graph reduction is needed to decrease training data volume.

Method: Simultaneously trains GNN and coarsens graph by partitioning nodes via K-means clustering based on their embeddings. Unlike preprocessing approaches, this allows node merging during training with adaptive clusters.

Result: Validated on both homophilic and heterophilic node classification datasets. Method handles challenging scenarios like heterophilic data and visualizations show node embeddings adapt to learning task during training.

Conclusion: The adaptive graph coarsening approach successfully enables task-adaptive graph reduction during GNN training, overcoming limitations of preprocessing methods and handling diverse graph types including heterophilic data.

Abstract: We propose an adaptive graph coarsening method to jointly learn graph neural
network (GNN) parameters and merge nodes via K-means clustering during
training. As real-world graphs grow larger, processing them directly becomes
increasingly challenging and sometimes infeasible. Tailoring algorithms to
large-scale data may sacrifice performance, so we instead consider graph
reduction to decrease the amount of data used during training. In particular,
we propose a method to simultaneously train a GNN and coarsen its graph by
partitioning nodes via K-means clustering based on their embeddings. Unlike
past graph coarsening works, our approach allows us to merge nodes during
training. Not only does this preclude coarsening as a preprocessing step, but
our node clusters can adapt to the learning task instead of relying solely on
graph connectivity and features. Thus, our method is amenable to scenarios that
are challenging for other methods, such as heterophilic data. We validate our
approach on both homophilic and heterophilic node classification datasets. We
further visualize relationships between node embeddings and their corresponding
clusters to illustrate that our coarsened graph adapts to the learning task
during training.

</details>


### [200] [Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking](https://arxiv.org/abs/2509.25712)
*Dengming Zhang,Xiaowen Ma,Zhenliang Ni,Zhenkai Wu,Han Shu,Xin Jiang,Xinghao Chen*

Main category: cs.LG

TL;DR: Expert Merging is a training-light method that learns layer-wise coefficients using unlabeled data to merge multiple domain-specialized models, with Expert Merging++ adding importance-guided chunking for better performance.


<details>
  <summary>Details</summary>
Motivation: Existing model merging methods either rely on hand-tuned coefficients (training-free) or align parameters rather than downstream behavior (training-based), and both ignore inter-layer heterogeneity.

Method: Learn layer-wise coefficients using unlabeled calibration data, optimized to align hidden states and logits with experts, plus coefficient regularization and task-weighted losses. Expert Merging++ adds importance-guided chunking based on learned coefficients, task-vector magnitudes, and parameter counts.

Result: Outperforms strong training-free and training-based baselines across MLLM (InternVL, Qwen2-VL) and LLM (Mistral) backbones, with Expert Merging++ achieving further gains and sometimes exceeding supervised Mixture Training.

Conclusion: Provides a label-free, parameter-efficient, and scalable approach to multi-expert model merging that captures inter-layer variation and achieves state-of-the-art performance.

Abstract: Model merging, which combines multiple domain-specialized experts into a
single model, offers a practical path to endow Large Language Models (LLMs) and
Multimodal Large Language Models (MLLMs) with broad capabilities without the
cost of joint training or serving many models. However, training-free methods
rely on hand-tuned coefficients, whereas training-based methods primarily align
parameters rather than downstream task behavior and typically treat all layers
uniformly, ignoring inter-layer heterogeneity. We introduce Expert Merging, a
training-light method that learns a small set of layer-wise coefficients using
only unlabeled calibration data. The coefficients are optimized to explicitly
align the merged model's hidden states and logits with those of the
corresponding experts, with a coefficient regularizer for stability and
task-weighted losses for controllable trade-offs. To capture inter-layer
variation, Expert Merging++ augments this design with importance-guided
chunking: a normalized layer-importance metric, derived from learned
coefficients, task-vector magnitudes, and parameter counts, allocates more
chunk-wise coefficients to high-importance layers while keeping low-importance
layers lightweight. The result is a label-free, parameter-efficient, and
scalable approach to multi-expert model merging across LLMs and MLLMs. Across
MLLM backbones (InternVL and Qwen2-VL) and the LLM backbone (Mistral), our
method surpasses strong training-free and training-based merging baselines,
with Expert Merging++ delivering further gains and, in some cases, even
exceeding supervised Mixture Training. The source code is available at
https://github.com/Littleor/ExpertMerging.

</details>


### [201] [Reweighted Flow Matching via Unbalanced OT for Label-free Long-tailed Generation](https://arxiv.org/abs/2509.25713)
*Hyunsoo Song,Minjung Gim,Jaewoong Choi*

Main category: cs.LG

TL;DR: UOT-RFM is a flow matching framework that addresses majority bias in long-tailed distributions using Unbalanced Optimal Transport and inverse reweighting without class labels.


<details>
  <summary>Details</summary>
Motivation: Standard flow matching fails on long-tailed distributions due to majority bias, producing poor minority modes and incorrect class proportions.

Method: Uses mini-batch Unbalanced Optimal Transport to construct conditional vector fields and implements inverse reweighting based on a label-free majority score derived from density ratios.

Result: Outperforms existing flow matching baselines on long-tailed benchmarks while maintaining competitive performance on balanced datasets.

Conclusion: UOT-RFM effectively mitigates majority bias in generative modeling for class-imbalanced distributions through principled reweighting without requiring class labels.

Abstract: Flow matching has recently emerged as a powerful framework for
continuous-time generative modeling. However, when applied to long-tailed
distributions, standard flow matching suffers from majority bias, producing
minority modes with low fidelity and failing to match the true class
proportions. In this work, we propose Unbalanced Optimal Transport Reweighted
Flow Matching (UOT-RFM), a novel framework for generative modeling under
class-imbalanced (long-tailed) distributions that operates without any class
label information. Our method constructs the conditional vector field using
mini-batch Unbalanced Optimal Transport (UOT) and mitigates majority bias
through a principled inverse reweighting strategy. The reweighting relies on a
label-free majority score, defined as the density ratio between the target
distribution and the UOT marginal. This score quantifies the degree of majority
based on the geometric structure of the data, without requiring class labels.
By incorporating this score into the training objective, UOT-RFM theoretically
recovers the target distribution with first-order correction ($k=1$) and
empirically improves tail-class generation through higher-order corrections ($k
> 1$). Our model outperforms existing flow matching baselines on long-tailed
benchmarks, while maintaining competitive performance on balanced datasets.

</details>


### [202] [MuPlon: Multi-Path Causal Optimization for Claim Verification through Controlling Confounding](https://arxiv.org/abs/2509.25715)
*Hanghui Guo,Shimin Di,Pasquale De Meo,Zhangze Chen,Jia Zhu*

Main category: cs.LG

TL;DR: MuPlon is a novel framework for claim verification that addresses data noise and biases through dual causal intervention strategies (back-door and front-door paths) to improve reliability.


<details>
  <summary>Details</summary>
Motivation: Traditional claim verification methods overlook complex evidence interactions, leading to unreliable results due to data noise and biases in fully connected claim-evidence graphs.

Method: Proposes Multi-Path Causal Optimization (MuPlon) with dual causal intervention: back-door path optimizes node probability weights to reduce noise and strengthen relevant connections; front-door path extracts relevant subgraphs and applies counterfactual reasoning to eliminate biases.

Result: Experimental results show MuPlon outperforms existing methods and achieves state-of-the-art performance in claim verification.

Conclusion: MuPlon effectively addresses data noise and biases in claim verification through causal intervention strategies, demonstrating superior performance over traditional approaches.

Abstract: As a critical task in data quality control, claim verification aims to curb
the spread of misinformation by assessing the truthfulness of claims based on a
wide range of evidence. However, traditional methods often overlook the complex
interactions between evidence, leading to unreliable verification results. A
straightforward solution represents the claim and evidence as a fully connected
graph, which we define as the Claim-Evidence Graph (C-E Graph). Nevertheless,
claim verification methods based on fully connected graphs face two primary
confounding challenges, Data Noise and Data Biases. To address these
challenges, we propose a novel framework, Multi-Path Causal Optimization
(MuPlon). MuPlon integrates a dual causal intervention strategy, consisting of
the back-door path and front-door path. In the back-door path, MuPlon dilutes
noisy node interference by optimizing node probability weights, while
simultaneously strengthening the connections between relevant evidence nodes.
In the front-door path, MuPlon extracts highly relevant subgraphs and
constructs reasoning paths, further applying counterfactual reasoning to
eliminate data biases within these paths. The experimental results demonstrate
that MuPlon outperforms existing methods and achieves state-of-the-art
performance.

</details>


### [203] [Beyond Point Estimates: Likelihood-Based Full-Posterior Wireless Localization](https://arxiv.org/abs/2509.25719)
*Haozhe Lei,Hao Guo,Tommy Svensson,Sundeep Rangan*

Main category: cs.LG

TL;DR: MC-CLE uses neural networks and Monte Carlo sampling for wireless localization with uncertainty quantification, outperforming Gaussian and uniform baselines.


<details>
  <summary>Details</summary>
Motivation: Modern wireless systems need both position estimates and quantified uncertainty for planning, control, and radio resource management.

Method: Formulates localization as posterior inference, trains neural scoring network using Monte Carlo sampling to compare true and candidate transmitter locations.

Result: Learns critical properties like angular ambiguity and front-to-back antenna patterns in line-of-sight simulations, achieves lower cross-entropy loss than uniform baseline and Gaussian posteriors.

Conclusion: MC-CLE provides effective uncertainty quantification for wireless localization through neural network-based posterior inference.

Abstract: Modern wireless systems require not only position estimates, but also
quantified uncertainty to support planning, control, and radio resource
management. We formulate localization as posterior inference of an unknown
transmitter location from receiver measurements. We propose Monte Carlo
Candidate-Likelihood Estimation (MC-CLE), which trains a neural scoring network
using Monte Carlo sampling to compare true and candidate transmitter locations.
We show that in line-of-sight simulations with a multi-antenna receiver, MC-CLE
learns critical properties including angular ambiguity and front-to-back
antenna patterns. MC-CLE also achieves lower cross-entropy loss relative to a
uniform baseline and Gaussian posteriors. alternatives under a uniform-loss
metric.

</details>


### [204] [Boundary-to-Region Supervision for Offline Safe Reinforcement Learning](https://arxiv.org/abs/2509.25727)
*Huikang Su,Dengyun Peng,Zifeng Zhuang,YuHan Liu,Qiguang Chen,Donglin Wang,Qinghe Liu*

Main category: cs.LG

TL;DR: B2R is a framework for offline safe RL that addresses the asymmetry between return-to-go (performance target) and cost-to-go (safety boundary) through asymmetric conditioning and cost signal realignment, achieving better constraint satisfaction and reward performance.


<details>
  <summary>Details</summary>
Motivation: Existing sequence-model-based methods treat return-to-go and cost-to-go symmetrically, but they are inherently asymmetric - RTG is a flexible performance target while CTG should be a rigid safety boundary. This symmetric approach leads to unreliable constraint satisfaction, especially with out-of-distribution cost trajectories.

Method: Proposed Boundary-to-Region (B2R) framework with asymmetric conditioning through cost signal realignment. Redefines cost-to-go as a boundary constraint under fixed safety budget, unifying cost distribution of feasible trajectories while preserving reward structures. Uses rotary positional embeddings to enhance exploration within safe region.

Result: B2R satisfies safety constraints in 35 out of 38 safety-critical tasks while achieving superior reward performance over baseline methods.

Conclusion: The work highlights limitations of symmetric token conditioning and establishes new theoretical and practical approach for applying sequence models to safe RL, demonstrating that asymmetric treatment of performance and safety signals is crucial for reliable constraint satisfaction.

Abstract: Offline safe reinforcement learning aims to learn policies that satisfy
predefined safety constraints from static datasets. Existing
sequence-model-based methods condition action generation on symmetric input
tokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry:
return-to-go (RTG) serves as a flexible performance target, while cost-to-go
(CTG) should represent a rigid safety boundary. This symmetric conditioning
leads to unreliable constraint satisfaction, especially when encountering
out-of-distribution cost trajectories. To address this, we propose
Boundary-to-Region (B2R), a framework that enables asymmetric conditioning
through cost signal realignment . B2R redefines CTG as a boundary constraint
under a fixed safety budget, unifying the cost distribution of all feasible
trajectories while preserving reward structures. Combined with rotary
positional embeddings , it enhances exploration within the safe region.
Experimental results show that B2R satisfies safety constraints in 35 out of 38
safety-critical tasks while achieving superior reward performance over baseline
methods. This work highlights the limitations of symmetric token conditioning
and establishes a new theoretical and practical approach for applying sequence
models to safe RL. Our code is available at https://github.com/HuikangSu/B2R.

</details>


### [205] [A Physics-Guided Probabilistic Surrogate Modeling Framework for Digital Twins of Underwater Radiated Noise](https://arxiv.org/abs/2509.25730)
*Indu Kant Deo,Akash Venkateshwaran,Rajeev K. Jaiman*

Main category: cs.LG

TL;DR: A physics-guided probabilistic framework for predicting 3D underwater acoustic transmission loss using machine learning, applied to ship noise mitigation in the Salish Sea.


<details>
  <summary>Details</summary>
Motivation: Ship traffic is increasing underwater radiated noise in coastal waters, creating need for real-time digital twins of ocean acoustics for operational noise mitigation.

Method: Combines sparse variational Gaussian processes with physics-based mean functions, deep sigma-point processes, and stochastic variational deep kernel learning. Framework includes learnable physics-informed mean, convolutional encoder for bathymetry, neural encoder for coordinates, and residual SVGP layer for uncertainty.

Result: Generated dataset of 30+ million source-receiver pairs using Gaussian beam solver. Framework provides calibrated predictive uncertainty and enables sound-exposure bounds and worst-case scenario analysis.

Conclusion: The probabilistic digital twin advances uncertainty-aware ocean acoustics modeling and demonstrates how physics-guided machine learning can support sustainable maritime operations through applications like ship speed optimization.

Abstract: Ship traffic is an increasing source of underwater radiated noise in coastal
waters, motivating real-time digital twins of ocean acoustics for operational
noise mitigation. We present a physics-guided probabilistic framework to
predict three-dimensional transmission loss in realistic ocean environments. As
a case study, we consider the Salish Sea along shipping routes from the Pacific
Ocean to the Port of Vancouver. A dataset of over 30 million source-receiver
pairs was generated with a Gaussian beam solver across seasonal sound speed
profiles and one-third-octave frequency bands spanning 12.5 Hz to 8 kHz. We
first assess sparse variational Gaussian processes (SVGP) and then incorporate
physics-based mean functions combining spherical spreading with
frequency-dependent absorption. To capture nonlinear effects, we examine deep
sigma-point processes and stochastic variational deep kernel learning. The
final framework integrates four components: (i) a learnable physics-informed
mean that represents dominant propagation trends, (ii) a convolutional encoder
for bathymetry along the source-receiver track, (iii) a neural encoder for
source, receiver, and frequency coordinates, and (iv) a residual SVGP layer
that provides calibrated predictive uncertainty. This probabilistic digital
twin facilitates the construction of sound-exposure bounds and worst-case
scenarios for received levels. We further demonstrate the application of the
framework to ship speed optimization, where predicted transmission loss
combined with near-field source models provides sound exposure level estimates
for minimizing acoustic impacts on marine mammals. The proposed framework
advances uncertainty-aware digital twins for ocean acoustics and illustrates
how physics-guided machine learning can support sustainable maritime
operations.

</details>


### [206] [Less is More: Towards Simple Graph Contrastive Learning](https://arxiv.org/abs/2509.25742)
*Yanan Zhao,Feng Ji,Jingyang Dai,Jiaze Ma,Wee Peng Tay*

Main category: cs.LG

TL;DR: A simple graph contrastive learning method that uses GCN and MLP encoders to capture structural features and node feature noise respectively, achieving SOTA on heterophilic graphs without data augmentation or negative sampling.


<details>
  <summary>Details</summary>
Motivation: Existing GCL methods perform poorly on heterophilic graphs and rely on complex components like data augmentation and negative sampling, raising questions about whether such complexity is necessary.

Method: Propose a simple GCL model with GCN encoder for structural features and MLP encoder for node feature noise, using original node features and graph structure as two complementary views without data augmentation or negative sampling.

Result: Achieves state-of-the-art performance on heterophilic benchmarks with minimal computational overhead, also performs well on homophilic graphs with advantages in complexity, scalability, and robustness.

Conclusion: Simple GCL design using complementary views from node features and graph structure can effectively handle heterophilic graphs without complex components, providing theoretical justification and experimental validation.

Abstract: Graph Contrastive Learning (GCL) has shown strong promise for unsupervised
graph representation learning, yet its effectiveness on heterophilic graphs,
where connected nodes often belong to different classes, remains limited. Most
existing methods rely on complex augmentation schemes, intricate encoders, or
negative sampling, which raises the question of whether such complexity is
truly necessary in this challenging setting. In this work, we revisit the
foundations of supervised and unsupervised learning on graphs and uncover a
simple yet effective principle for GCL: mitigating node feature noise by
aggregating it with structural features derived from the graph topology. This
observation suggests that the original node features and the graph structure
naturally provide two complementary views for contrastive learning. Building on
this insight, we propose an embarrassingly simple GCL model that uses a GCN
encoder to capture structural features and an MLP encoder to isolate node
feature noise. Our design requires neither data augmentation nor negative
sampling, yet achieves state-of-the-art results on heterophilic benchmarks with
minimal computational and memory overhead, while also offering advantages in
homophilic graphs in terms of complexity, scalability, and robustness. We
provide theoretical justification for our approach and validate its
effectiveness through extensive experiments, including robustness evaluations
against both black-box and white-box adversarial attacks.

</details>


### [207] [Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space](https://arxiv.org/abs/2509.25743)
*Xiang Zhang,Kun Wei,Xu Yang,Chenghao Xu,Su Yan,Cheng Deng*

Main category: cs.LG

TL;DR: RCU is a machine unlearning method that uses rotational salience weights and skew symmetric loss to enable continuous unlearning without requiring a retained dataset, preventing cumulative catastrophic utility loss.


<details>
  <summary>Details</summary>
Motivation: Address security vulnerabilities in LLMs by developing effective machine unlearning methods that don't rely on retained datasets and avoid cumulative utility degradation during continuous unlearning.

Method: Uses rotational salience weights to quantify unlearning degree, skew symmetric loss to create cognitive rotation space, and orthogonal rotation axes regularization to minimize interference between unlearning requests.

Result: Achieves state-of-the-art performance on multiple datasets without requiring a retained dataset, effectively handling continuous unlearning requests.

Conclusion: RCU provides an effective solution for continuous machine unlearning that eliminates the need for retained datasets and prevents cumulative utility loss, addressing key limitations of existing methods.

Abstract: As Large Language Models (LLMs) become increasingly prevalent, their security
vulnerabilities have already drawn attention. Machine unlearning is introduced
to seek to mitigate these risks by removing the influence of undesirable data.
However, existing methods not only rely on the retained dataset to preserve
model utility, but also suffer from cumulative catastrophic utility loss under
continuous unlearning requests. To solve this dilemma, we propose a novel
method, called Rotation Control Unlearning (RCU), which leverages the
rotational salience weight of RCU to quantify and control the unlearning degree
in the continuous unlearning process. The skew symmetric loss is designed to
construct the existence of the cognitive rotation space, where the changes of
rotational angle can simulate the continuous unlearning process. Furthermore,
we design an orthogonal rotation axes regularization to enforce mutually
perpendicular rotation directions for continuous unlearning requests,
effectively minimizing interference and addressing cumulative catastrophic
utility loss. Experiments on multiple datasets confirm that our method without
retained dataset achieves SOTA performance.

</details>


### [208] [OPPO: Accelerating PPO-based RLHF via Pipeline Overlap](https://arxiv.org/abs/2509.25762)
*Kaizhuo Yan,Yingjie Yu,Yifan Yu,Haizhong Zheng,Fan Lai*

Main category: cs.LG

TL;DR: OPPO is a novel PPO-based RLHF framework that improves training efficiency through pipeline overlapping techniques, achieving 1.8-2.8x speedup and 1.4-2.1x better GPU utilization without compromising convergence.


<details>
  <summary>Details</summary>
Motivation: PPO-based RLHF suffers from inefficiencies due to sequential multi-model dependencies and long-tail response lengths that slow down training pipeline completion.

Method: OPPO introduces two techniques: (1) Intra-step overlap - streams upstream model outputs in chunks to enable downstream model prefill while upstream continues decoding; (2) Inter-step overlap - adaptively overcommits prompts and defers long generations to mitigate tail latency.

Result: Extensive evaluations show OPPO accelerates PPO-based RLHF training by 1.8-2.8x and improves GPU utilization by 1.4-2.1x without compromising training convergence.

Conclusion: OPPO provides an efficient, lightweight, and model-agnostic framework that easily integrates with existing PPO implementations with minimal code changes.

Abstract: Proximal Policy Optimization (PPO)-based reinforcement learning from human
feedback (RLHF) is a widely adopted paradigm for aligning large language models
(LLMs) with human preferences. However, its training pipeline suffers from
substantial inefficiencies due to sequential multi-model dependencies (e.g.,
reward model depends on actor outputs) and long-tail response lengths, where a
few long responses straggle the stage completion. We present OPPO, a novel,
lightweight, and model-agnostic PPO-based RLHF framework that improves training
efficiency by overlapping pipeline execution. OPPO introduces two novel
techniques: (1) Intra-step overlap, which streams upstream model outputs (e.g.,
actor model) in right-sized chunks, enabling the downstream model (e.g.,
reward) to begin prefill while the upstream continues decoding; and (2)
Inter-step overlap, which adaptively overcommits a few prompts and defers long
generations to future steps, mitigating tail latency without discarding partial
work. OPPO integrates easily with existing PPO implementations with a few lines
of code change. Extensive evaluations show that OPPO accelerates PPO-based RLHF
training by $1.8 \times-2.8 \times$ and improves GPU utilization by $1.4
\times-2.1 \times$ without compromising training convergence.

</details>


### [209] [Autonomy-Aware Clustering: When Local Decisions Supersede Global Prescriptions](https://arxiv.org/abs/2509.25775)
*Amber Srivastava,Salar Basiri,Srinivasa Salapaka*

Main category: cs.LG

TL;DR: Autonomy-aware clustering framework using RL and deterministic annealing to account for entity autonomy in clustering, achieving near-ground-truth results without explicit autonomy models.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering assumes passive entities, but real-world entities exhibit local autonomy that can reshape clustering outcomes, affecting inference and decision-making.

Method: Combines reinforcement learning with deterministic annealing procedure, plus Adaptive Distance Estimation Network (ADEN) - a transformer-based attention model that learns entity-cluster dependencies.

Result: Achieves solutions close to ground truth (3-4% gap) without explicit autonomy models, while ignoring autonomy leads to 35-40% gaps.

Conclusion: The framework effectively captures entity autonomy in clustering, enabling more accurate cluster compositions and better downstream decision-making.

Abstract: Clustering arises in a wide range of problem formulations, yet most existing
approaches assume that the entities under clustering are passive and strictly
conform to their assigned groups. In reality, entities often exhibit local
autonomy, overriding prescribed associations in ways not fully captured by
feature representations. Such autonomy can substantially reshape clustering
outcomes -- altering cluster compositions, geometry, and cardinality -- with
significant downstream effects on inference and decision-making. We introduce
autonomy-aware clustering, a reinforcement (RL) learning framework that learns
and accounts for the influence of local autonomy without requiring prior
knowledge of its form. Our approach integrates RL with a deterministic
annealing (DA) procedure, where, to determine underlying clusters, DA naturally
promotes exploration in early stages of annealing and transitions to
exploitation later. We also show that the annealing procedure exhibits phase
transitions that enable design of efficient annealing schedules. To further
enhance adaptability, we propose the Adaptive Distance Estimation Network
(ADEN), a transformer-based attention model that learns dependencies between
entities and cluster representatives within the RL loop, accommodates
variable-sized inputs and outputs, and enables knowledge transfer across
diverse problem instances. Empirical results show that our framework closely
aligns with underlying data dynamics: even without explicit autonomy models, it
achieves solutions close to the ground truth (gap ~3-4%), whereas ignoring
autonomy leads to substantially larger gaps (~35-40%). The code and data are
publicly available at https://github.com/salar96/AutonomyAwareClustering.

</details>


### [210] [Online Decision Making with Generative Action Sets](https://arxiv.org/abs/2509.25777)
*Jianyu Xu,Vidhi Jain,Bryan Wilder,Aarti Singh*

Main category: cs.LG

TL;DR: This paper proposes a doubly-optimistic algorithm for online learning with expandable action spaces, where agents can generate new actions at a cost, achieving optimal regret bounds.


<details>
  <summary>Details</summary>
Motivation: With generative AI enabling dynamic action creation during online learning, there's a need to balance the costs of action generation against potential benefits, creating triangular tradeoffs among exploitation, exploration, and creation.

Method: A doubly-optimistic algorithm that uses Lower Confidence Bounds (LCB) for action selection and Upper Confidence Bounds (UCB) for action generation.

Result: Empirical evaluation on healthcare question-answering datasets shows favorable generation-quality tradeoffs, and theoretical analysis proves optimal regret of O(T^{d/(d+2)}d^{d/(d+2)} + d√(T log T)).

Conclusion: The proposed algorithm provides the first sublinear regret bound for online learning with expanding action spaces, effectively managing the tradeoffs between action generation costs and learning benefits.

Abstract: With advances in generative AI, decision-making agents can now dynamically
create new actions during online learning, but action generation typically
incurs costs that must be balanced against potential benefits. We study an
online learning problem where an agent can generate new actions at any time
step by paying a one-time cost, with these actions becoming permanently
available for future use. The challenge lies in learning the optimal sequence
of two-fold decisions: which action to take and when to generate new ones,
further complicated by the triangular tradeoffs among exploitation, exploration
and $\textit{creation}$. To solve this problem, we propose a doubly-optimistic
algorithm that employs Lower Confidence Bounds (LCB) for action selection and
Upper Confidence Bounds (UCB) for action generation. Empirical evaluation on
healthcare question-answering datasets demonstrates that our approach achieves
favorable generation-quality tradeoffs compared to baseline strategies. From
theoretical perspectives, we prove that our algorithm achieves the optimal
regret of $O(T^{\frac{d}{d+2}}d^{\frac{d}{d+2}} + d\sqrt{T\log T})$, providing
the first sublinear regret bound for online learning with expanding action
spaces.

</details>


### [211] [A Hamiltonian driven Geometric Construction of Neural Networks on the Lognormal Statistical Manifold](https://arxiv.org/abs/2509.25778)
*Prosper Rosaire Mama Assandje,Teumsa Aboubakar,Dongho Joseph,Takemi Nakamura*

Main category: cs.LG

TL;DR: This paper presents a method for constructing neural networks intrinsically on statistical manifolds, specifically the lognormal manifold, using Hamiltonian dynamics and geometric principles.


<details>
  <summary>Details</summary>
Motivation: To bridge information geometry with machine learning by building neural networks directly on statistical manifolds, leveraging their geometric properties for more interpretable and principled learning systems.

Method: Formulate neural network architecture on lognormal statistical manifold using Hamiltonian system equivalent to gradient flow. Define input values using Hamiltonian coordinate system embedded in Poincare disk. Derive network components geometrically: rotation from SU(1,1) Lie group action, activation from symplectic structure.

Result: Obtained complete weight matrix including translation vector and output values, showing lognormal manifold can be viewed as neural manifold with geometric properties dictating unique network structure.

Conclusion: The method offers a new paradigm for building learning systems grounded in differential geometry of underlying parameter spaces, providing interpretable neural network structures dictated by geometric properties.

Abstract: Bridging information geometry with machine learning, this paper presents a
method for constructing neural networks intrinsically on statistical manifolds.
We demonstrate this approach by formulating a neural network architecture
directly on the lognormal statistical manifold. The construction is driven by
the Hamiltonian system that is equivalent to the gradient flow on this
manifold. First, we define the network's input values using the coordinate
system of this Hamiltonian dynamics, naturally embedded in the Poincare disk.
The core of our contribution lies in the derivation of the network's components
from geometric principles: the rotation component of the synaptic weight matrix
is determined by the Lie group action of SU(1,1) on the disk, while the
activation function emerges from the symplectic structure of the system. We
subsequently obtain the complete weight matrix, including its translation
vector, and the resulting output values. This work shows that the lognormal
manifold can be seamlessly viewed as a neural manifold, with its geometric
properties dictating a unique and interpretable neural network structure. The
proposed method offers a new paradigm for building learning systems grounded in
the differential geometry of their underlying parameter spaces.

</details>


### [212] [From Cheap Geometry to Expensive Physics: Elevating Neural Operators via Latent Shape Pretraining](https://arxiv.org/abs/2509.25788)
*Zhizhou Zhang,Youjia Wu,Kaixuan Zhang,Yanjia Wang*

Main category: cs.LG

TL;DR: A two-stage framework that uses geometry-only pretraining to improve neural operator learning for PDE solutions under limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Industrial design evaluation requires expensive PDE simulations, and operator learning is limited by scarce physics-based data while abundant geometry-only designs remain unused.

Method: Stage 1: Pretrain autoencoder on geometry reconstruction to learn latent representations without PDE labels. Stage 2: Train neural operator using pretrained latent embeddings as inputs instead of raw point clouds, with transformer architectures for both stages.

Result: Consistent improvement in prediction accuracy across four PDE datasets and three transformer-based neural operators compared to models trained directly on raw point clouds.

Conclusion: Physics-agnostic pretraining provides powerful foundation for data-efficient operator learning, enabling better exploitation of abundant geometry-only resources.

Abstract: Industrial design evaluation often relies on high-fidelity simulations of
governing partial differential equations (PDEs). While accurate, these
simulations are computationally expensive, making dense exploration of design
spaces impractical. Operator learning has emerged as a promising approach to
accelerate PDE solution prediction; however, its effectiveness is often limited
by the scarcity of labeled physics-based data. At the same time, large numbers
of geometry-only candidate designs are readily available but remain largely
untapped. We propose a two-stage framework to better exploit this abundant,
physics-agnostic resource and improve supervised operator learning under
limited labeled data. In Stage 1, we pretrain an autoencoder on a geometry
reconstruction task to learn an expressive latent representation without PDE
labels. In Stage 2, the neural operator is trained in a standard supervised
manner to predict PDE solutions, using the pretrained latent embeddings as
inputs instead of raw point clouds. Transformer-based architectures are adopted
for both the autoencoder and the neural operator to handle point cloud data and
integrate both stages seamlessly. Across four PDE datasets and three
state-of-the-art transformer-based neural operators, our approach consistently
improves prediction accuracy compared to models trained directly on raw point
cloud inputs. These results demonstrate that representations from
physics-agnostic pretraining provide a powerful foundation for data-efficient
operator learning.

</details>


### [213] [Characterization and Learning of Causal Graphs with Latent Confounders and Post-treatment Selection from Interventional Data](https://arxiv.org/abs/2509.25800)
*Gongxu Luo,Loka Li,Guangyi Chen,Haoyue Dai,Kun Zhang*

Main category: cs.LG

TL;DR: The paper addresses the challenge of post-treatment selection in interventional causal discovery, where samples are selectively included after interventions, which can distort causal discovery results. It introduces a new causal formulation, FI-Markov equivalence, and F-FCI algorithm to handle both latent confounders and selection bias.


<details>
  <summary>Details</summary>
Motivation: Post-treatment selection is a common but overlooked challenge in causal discovery, especially in biological studies like gene expression analysis where samples are retained based on quality criteria. This selection can introduce spurious dependencies that mimic causal responses, distorting traditional causal discovery methods.

Method: The authors introduce a novel causal formulation that explicitly models post-treatment selection, characterize its Markov properties, define FI-Markov equivalence class represented by F-PAG diagrams, and develop the F-FCI algorithm for sound and complete causal identification.

Result: Experimental results on synthetic and real-world datasets show that the proposed F-FCI method successfully recovers causal relations despite the presence of both selection bias and latent confounders.

Conclusion: The paper provides a comprehensive framework for handling post-treatment selection in causal discovery, enabling more accurate identification of causal relationships in the presence of both latent confounders and selection mechanisms.

Abstract: Interventional causal discovery seeks to identify causal relations by
leveraging distributional changes introduced by interventions, even in the
presence of latent confounders. Beyond the spurious dependencies induced by
latent confounders, we highlight a common yet often overlooked challenge in the
problem due to post-treatment selection, in which samples are selectively
included in datasets after interventions. This fundamental challenge widely
exists in biological studies; for example, in gene expression analysis, both
observational and interventional samples are retained only if they meet quality
control criteria (e.g., highly active cells). Neglecting post-treatment
selection may introduce spurious dependencies and distributional changes under
interventions, which can mimic causal responses, thereby distorting causal
discovery results and challenging existing causal formulations. To address
this, we introduce a novel causal formulation that explicitly models
post-treatment selection and reveals how its differential reactions to
interventions can distinguish causal relations from selection patterns,
allowing us to go beyond traditional equivalence classes toward the underlying
true causal structure. We then characterize its Markov properties and propose a
Fine-grained Interventional equivalence class, named FI-Markov equivalence,
represented by a new graphical diagram, F-PAG. Finally, we develop a provably
sound and complete algorithm, F-FCI, to identify causal relations, latent
confounders, and post-treatment selection up to $\mathcal{FI}$-Markov
equivalence, using both observational and interventional data. Experimental
results on synthetic and real-world datasets demonstrate that our method
recovers causal relations despite the presence of both selection and latent
confounders.

</details>


### [214] [CardioForest: An Explainable Ensemble Learning Model for Automatic Wide QRS Complex Tachycardia Diagnosis from ECG](https://arxiv.org/abs/2509.25804)
*Vaskar Chakma,Ju Xiaolin,Heling Cao,Xue Feng,Ji Xiaodong,Pan Haiyan,Gao Zhan*

Main category: cs.LG

TL;DR: This paper presents an ensemble machine learning framework called CardioForest for automatically detecting Wide QRS Complex Tachycardia (WCT) from ECG signals, achieving high accuracy (94.95%) while maintaining interpretability through Explainable AI techniques.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate and interpretable automatic detection system for Wide QRS Complex Tachycardia (WCT) that can assist cardiologists in making timely diagnoses, particularly in emergency scenarios where rapid and reliable ECG interpretation is critical.

Method: The proposed system integrates ensemble learning techniques including an optimized Random Forest (CardioForest), XGBoost, and LightGBM models trained on ECG data from the MIMIC-IV dataset. SHAP (SHapley Additive exPlanations) is used for model explainability and clinical relevance assessment.

Result: CardioForest performed best with test accuracy of 94.95%, balanced accuracy of 88.31%, and high precision and recall metrics. SHAP analysis confirmed the model's ability to identify clinically relevant ECG features like QRS duration, aligning with clinical intuitions.

Conclusion: CardioForest is a highly reliable and interpretable WCT detection model that provides both accurate predictions and transparency through explainability, making it a valuable tool for cardiologists in high-stakes and emergency diagnostic scenarios.

Abstract: This study aims to develop and evaluate an ensemble machine learning-based
framework for the automatic detection of Wide QRS Complex Tachycardia (WCT)
from ECG signals, emphasizing diagnostic accuracy and interpretability using
Explainable AI. The proposed system integrates ensemble learning techniques,
i.e., an optimized Random Forest known as CardioForest, and models like XGBoost
and LightGBM. The models were trained and tested on ECG data from the publicly
available MIMIC-IV dataset. The testing was carried out with the assistance of
accuracy, balanced accuracy, precision, recall, F1 score, ROC-AUC, and error
rate (RMSE, MAE) measures. In addition, SHAP (SHapley Additive exPlanations)
was used to ascertain model explainability and clinical relevance. The
CardioForest model performed best on all metrics, achieving a test accuracy of
94.95%, a balanced accuracy of 88.31%, and high precision and recall metrics.
SHAP analysis confirmed the model's ability to rank the most relevant ECG
features, such as QRS duration, in accordance with clinical intuitions, thereby
fostering trust and usability in clinical practice. The findings recognize
CardioForest as an extremely dependable and interpretable WCT detection model.
Being able to offer accurate predictions and transparency through
explainability makes it a valuable tool to help cardiologists make timely and
well-informed diagnoses, especially for high-stakes and emergency scenarios.

</details>


### [215] [Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse](https://arxiv.org/abs/2509.25808)
*Yuheng Zhang,Wenlin Yao,Changlong Yu,Yao Liu,Qingyu Yin,Bing Yin,Hyokun Yun,Lihong Li*

Main category: cs.LG

TL;DR: AR3PO is a sampling-efficient RLVR algorithm that addresses GRPO's vanishing advantage issue through adaptive rollout and response reuse techniques, achieving comparable or better performance than baselines with significantly reduced rollout costs.


<details>
  <summary>Details</summary>
Motivation: To solve the vanishing advantage problem in GRPO when all responses in a group receive identical rewards, which limits training effectiveness.

Method: Introduces two novel techniques: adaptive rollout (dynamically allocating more responses to difficult prompts) and response reuse (leveraging previously generated correct responses for training signals).

Result: AR3PO consistently outperforms GRPO and matches/surpasses DAPO across 7B, 8B, and 32B models, reducing rollout cost by up to 4.2x while maintaining comparable performance.

Conclusion: AR3PO provides an effective solution to the vanishing advantage problem in RLVR algorithms, achieving strong performance with significantly improved sampling efficiency.

Abstract: Large language models (LLMs) have achieved impressive reasoning performance,
with reinforcement learning with verifiable rewards (RLVR) emerging as a
standard paradigm for post-training. A representative algorithm, group relative
policy optimization (GRPO) (Shao et al., 2024), computes advantages by
normalizing outcome rewards within response groups, but suffers from a
vanishing advantage issue when all responses in a group receive identical
rewards. To address this issue, we propose Adaptive Rollout and Response Reuse
Policy Optimization (AR3PO), a sampling efficient RLVR algorithm that
introduces two novel techniques: adaptive rollout, which dynamically allocates
more responses to difficult prompts while saving computation on easier ones,
and response reuse, which leverages previously generated correct responses to
provide useful training signals. We compare AR3PO with strong RLVR baselines on
multiple representative benchmarks using two different families of base models.
Across the 7B and 8B models, AR3PO consistently outperforms GRPO and matches or
surpasses DAPO (Yu et al., 2025), reducing rollout cost by up to 4.2x. On the
larger 32B model, AR3PO achieves comparable performance to DAPO at similar
training steps while maintaining substantially lower rollout cost.

</details>


### [216] [Learning to Reason as Action Abstractions with Scalable Mid-Training RL](https://arxiv.org/abs/2509.25810)
*Shenao Zhang,Donghan Yu,Yihao Feng,Bowen Jin,Zhaoran Wang,John Peebles,Zirui Wang*

Main category: cs.LG

TL;DR: Mid-training shapes post-training by identifying compact action subspaces that minimize value approximation and RL errors. The RA3 algorithm discovers temporally-consistent latent structures via RL and fine-tuning, improving code generation performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Large language models benefit from reinforcement learning, but fully unlocking this potential requires an effective mid-training stage to identify useful actions and enable fast selection through online RL.

Method: Proposed Reasoning as Action Abstractions (RA3) algorithm: derives sequential variational lower bound and optimizes it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on bootstrapped data.

Result: RA3 improves average performance on HumanEval and MBPP by 8 and 4 points over base model and next-token prediction baseline. Achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces.

Conclusion: Mid-training is most effective when decision space is compact and effective horizon is short, highlighting importance of operating in action abstractions rather than primitive actions. RA3 demonstrates practical effectiveness in code generation tasks.

Abstract: Large language models excel with reinforcement learning (RL), but fully
unlocking this potential requires a mid-training stage. An effective
mid-training phase should identify a compact set of useful actions and enable
fast selection among them through online RL. We formalize this intuition by
presenting the first theoretical result on how mid-training shapes
post-training: it characterizes an action subspace that minimizes both the
value approximation error from pruning and the RL error during subsequent
planning. Our analysis reveals two key determinants of mid-training
effectiveness: pruning efficiency, which shapes the prior of the initial RL
policy, and its impact on RL convergence, which governs the extent to which
that policy can be improved via online interactions. These results suggest that
mid-training is most effective when the decision space is compact and the
effective horizon is short, highlighting the importance of operating in the
space of action abstractions rather than primitive actions. Building on these
insights, we propose Reasoning as Action Abstractions (RA3), a scalable
mid-training algorithm. Specifically, we derive a sequential variational lower
bound and optimize it by iteratively discovering temporally-consistent latent
structures via RL, followed by fine-tuning on the bootstrapped data.
Experiments on code generation tasks demonstrate the effectiveness of our
approach. Across multiple base models, RA3 improves the average performance on
HumanEval and MBPP by 8 and 4 points over the base model and the next-token
prediction baseline. Furthermore, RA3 achieves faster convergence and higher
asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and
Codeforces.

</details>


### [217] [Decentralized Asynchronous Multi-player Bandits](https://arxiv.org/abs/2509.25824)
*Jingqi Fan,Canzhe Zhao,Shuai Li,Siwei Wang*

Main category: cs.LG

TL;DR: This paper proposes a novel algorithm for decentralized asynchronous multi-player multi-armed bandits that adaptively switches between exploration and exploitation, achieving efficient collision avoidance and player detection with O(√T log T + log T/Δ²) regret.


<details>
  <summary>Details</summary>
Motivation: Real-world systems like cognitive radio networks and IoT are often decentralized and asynchronous, but most existing MP-MAB research focuses on synchronized settings. The asynchronous setting introduces challenges: players cannot coordinate through time to avoid collisions, and detecting player count is costly.

Method: Developed an algorithm where players adaptively change between exploration and exploitation. During exploration, players uniformly pull arms to reduce collision probability. Players also continue pulling exploited arms with small probability to detect when players leave the system.

Result: The algorithm achieves a regret of O(√T log T + log T/Δ²), where Δ is the minimum expected reward gap between arms. This is the first efficient MP-MAB algorithm for asynchronous decentralized environments. Extensive experiments validate effectiveness and robustness.

Conclusion: The proposed algorithm successfully addresses the challenges of fully asynchronous decentralized MP-MAB settings, providing the first efficient solution with theoretical guarantees and practical applicability to real-world scenarios like cognitive radio and IoT systems.

Abstract: In recent years, multi-player multi-armed bandits (MP-MAB) have been
extensively studied due to their wide applications in cognitive radio networks
and Internet of Things systems. While most existing research on MP-MAB focuses
on synchronized settings, real-world systems are often decentralized and
asynchronous, where players may enter or leave the system at arbitrary times,
and do not have a global clock. This decentralized asynchronous setting
introduces two major challenges. First, without a global time, players cannot
implicitly coordinate their actions through time, making it difficult to avoid
collisions. Second, it is important to detect how many players are in the
system, but doing so may cost a lot. In this paper, we address the challenges
posed by such a fully asynchronous setting in a decentralized environment. We
develop a novel algorithm in which players adaptively change between
exploration and exploitation. During exploration, players uniformly pull their
arms, reducing the probability of collisions and effectively mitigating the
first challenge. Meanwhile, players continue pulling arms currently exploited
by others with a small probability, enabling them to detect when a player has
left, thereby addressing the second challenge. We prove that our algorithm
achieves a regret of $\mathcal{O}(\sqrt{T \log T} + {\log T}/{\Delta^2})$,
where $\Delta$ is the minimum expected reward gap between any two arms. To the
best of our knowledge, this is the first efficient MP-MAB algorithm in the
asynchronous and decentralized environment. Extensive experiments further
validate the effectiveness and robustness of our algorithm, demonstrating its
applicability to real-world scenarios.

</details>


### [218] [Kairos: Towards Adaptive and Generalizable Time Series Foundation Models](https://arxiv.org/abs/2509.25826)
*Kun Feng,Shaocheng Lan,Yuchen Fang,Wenchao He,Lintao Ma,Xingyu Lu,Kan Ren*

Main category: cs.LG

TL;DR: Kairos is a flexible time series foundation model framework that addresses the challenge of heterogeneous information density in time series through dynamic patching tokenization and instance-adaptive positional embeddings, achieving superior zero-shot performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Time series exhibit heterogeneous information density over time, influenced by system states and signal complexity, which current TSFMs fail to capture due to non-adaptive processing pipelines like fixed-size patching and uniform positional encodings.

Method: Proposes Kairos framework with dynamic patching tokenizer that adaptively selects tokenization granularity and instance-adaptive positional embedding that tailors encodings to each time series's unique characteristics. Trained on Predictability-Stratified Time Series corpus with 300B+ time points using multi-patch prediction strategy.

Result: Achieves superior performance with fewer parameters on GIFT-Eval and Time-Series-Library benchmarks, consistently outperforming established methods across diverse tasks in zero-shot scenarios.

Conclusion: Kairos effectively addresses the limitations of current TSFMs by adapting to varying information densities in time series, demonstrating the importance of flexible tokenization and instance-specific positional encodings for improved zero-shot time series analysis.

Abstract: Time series foundation models (TSFMs) have emerged as a powerful paradigm for
time series analysis, driven by large-scale pretraining on diverse data
corpora. However, time series inherently exhibit heterogeneous information
density over time, influenced by system states and signal complexity,
presenting significant modeling challenges especially in a zero-shot scenario.
Current TSFMs rely on non-adaptive processing pipelines that fail to capture
this dynamic nature. For example, common tokenization strategies such as
fixed-size patching enforce rigid observational granularity, limiting their
ability to adapt to varying information densities. Similarly, conventional
positional encodings impose a uniform temporal scale, making it difficult to
model diverse periodicities and trends across series. To overcome these
limitations, we propose Kairos, a flexible TSFM framework that integrates a
dynamic patching tokenizer and an instance-adaptive positional embedding.
Kairos adaptively selects tokenization granularity and tailors positional
encodings to the unique characteristics of each time series instance. Trained
on a large-scale Predictability-Stratified Time Series (PreSTS) corpus
comprising over 300 billion time points and adopting a multi-patch prediction
strategy in the inference stage, Kairos achieves superior performance with much
fewer parameters on two common zero-shot benchmarks, GIFT-Eval and the
Time-Series-Library benchmark, consistently outperforming established methods
across diverse tasks. The project page is at
https://foundation-model-research.github.io/Kairos .

</details>


### [219] [MIDAS: Misalignment-based Data Augmentation Strategy for Imbalanced Multimodal Learning](https://arxiv.org/abs/2509.25831)
*Seong-Hyeon Hwang,Soyoung Choi,Steven Euijong Whang*

Main category: cs.LG

TL;DR: MIDAS is a data augmentation strategy that generates misaligned samples with semantically inconsistent cross-modal information to address modality imbalance in multimodal models.


<details>
  <summary>Details</summary>
Motivation: Multimodal models often over-rely on dominant modalities and fail to achieve optimal performance, with data-centric solutions remaining underexplored.

Method: Proposes MIDAS with misaligned sample generation using unimodal confidence scores, weak-modality weighting to increase loss weight of least confident modality, and hard-sample weighting to prioritize semantically ambiguous misaligned samples.

Result: Experiments on multiple multimodal classification benchmarks demonstrate that MIDAS significantly outperforms related baselines in addressing modality imbalance.

Conclusion: MIDAS effectively addresses modality imbalance through misaligned data augmentation and adaptive weighting strategies, enabling better utilization of weaker modalities.

Abstract: Multimodal models often over-rely on dominant modalities, failing to achieve
optimal performance. While prior work focuses on modifying training objectives
or optimization procedures, data-centric solutions remain underexplored. We
propose MIDAS, a novel data augmentation strategy that generates misaligned
samples with semantically inconsistent cross-modal information, labeled using
unimodal confidence scores to compel learning from contradictory signals.
However, this confidence-based labeling can still favor the more confident
modality. To address this within our misaligned samples, we introduce
weak-modality weighting, which dynamically increases the loss weight of the
least confident modality, thereby helping the model fully utilize weaker
modality. Furthermore, when misaligned features exhibit greater similarity to
the aligned features, these misaligned samples pose a greater challenge,
thereby enabling the model to better distinguish between classes. To leverage
this, we propose hard-sample weighting, which prioritizes such semantically
ambiguous misaligned samples. Experiments on multiple multimodal classification
benchmarks demonstrate that MIDAS significantly outperforms related baselines
in addressing modality imbalance.

</details>


### [220] [Distillation of Large Language Models via Concrete Score Matching](https://arxiv.org/abs/2509.25837)
*Yeongmin Kim,Donghyeok Shin,Mina Kang,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: CSD is a novel knowledge distillation method that overcomes limitations of softmax-based objectives and direct logit distillation by using discrete score matching to align relative logit differences between student and teacher models.


<details>
  <summary>Details</summary>
Motivation: Existing KD methods either blur logit information through softmax or fail to account for logit shift invariance, restricting the solution space for efficient LLM deployment.

Method: Proposes Concrete Score Distillation (CSD) - a discrete score-matching objective that resolves training instability and quadratic complexity in autoregressive LLMs, aligning relative logit differences across all vocabulary pairs with flexible weighting.

Result: CSD consistently surpasses recent KD objectives, achieves favorable fidelity-diversity trade-offs, and yields complementary gains when combined with on-policy techniques across GPT-2-1.5B, OpenLLaMA-7B, and GEMMA-7B-IT models.

Conclusion: CSD demonstrates scalability and effectiveness for LLM distillation by overcoming softmax-induced smoothing and solution space restrictions while maintaining stable training.

Abstract: Large language models (LLMs) deliver remarkable performance but are costly to
deploy, motivating knowledge distillation (KD) for efficient inference.
Existing KD objectives typically match student and teacher probabilities via
softmax, which blurs valuable logit information. While direct logit
distillation (DLD) mitigates softmax smoothing, it fails to account for logit
shift invariance, thereby restricting the solution space. We propose Concrete
Score Distillation (CSD), a discrete score-matching objective that overcomes
both softmax-induced smoothing and restrictions on the optimal solution set. We
resolve the training instability and quadratic complexity of discrete
score-matching in autoregressive LLMs, and the resulting CSD objective aligns
relative logit differences across all vocabulary pairs between student and
teacher with flexible weighting. We provide both mode-seeking and mode-covering
instances within our framework and evaluate CSD on task-agnostic
instruction-following and task-specific distillation using GPT-2-1.5B,
OpenLLaMA-7B, and GEMMA-7B-IT. Experiments show that CSD consistently surpasses
recent KD objectives, achieves favorable fidelity-diversity trade-offs, and
yields complementary gains when combined with on-policy techniques,
demonstrating its scalability and effectiveness for LLM distillation.

</details>


### [221] [S$^2$FS: Spatially-Aware Separability-Driven Feature Selection in Fuzzy Decision Systems](https://arxiv.org/abs/2509.25841)
*Suping Xu,Chuyi Dai,Ye Liu,Lin Shang,Xibei Yang,Witold Pedrycz*

Main category: cs.LG

TL;DR: S^2FS is a novel feature selection framework for fuzzy decision systems that uses spatially-aware separability criteria to improve classification accuracy and interpretability by considering both within-class compactness and between-class separation with spatial directional information.


<details>
  <summary>Details</summary>
Motivation: Existing feature selection methods for fuzzy decision systems either don't directly align with learning performance or rely only on non-directional Euclidean distances, limiting their ability to clarify decision boundaries despite the importance of spatial distribution of instances.

Method: Proposed S^2FS framework uses a spatially-aware separability criterion that integrates scalar-distances with spatial directional information, and employs a forward greedy strategy to iteratively select the most discriminative features.

Result: Extensive experiments on ten real-world datasets show S^2FS consistently outperforms eight state-of-the-art feature selection algorithms in both classification accuracy and clustering performance, with feature visualizations confirming interpretability.

Conclusion: S^2FS effectively enhances feature selection for fuzzy decision systems by incorporating spatial awareness and directional information, leading to improved performance and interpretability compared to existing methods.

Abstract: Feature selection is crucial for fuzzy decision systems (FDSs), as it
identifies informative features and eliminates rule redundancy, thereby
enhancing predictive performance and interpretability. Most existing methods
either fail to directly align evaluation criteria with learning performance or
rely solely on non-directional Euclidean distances to capture relationships
among decision classes, which limits their ability to clarify decision
boundaries. However, the spatial distribution of instances has a potential
impact on the clarity of such boundaries. Motivated by this, we propose
Spatially-aware Separability-driven Feature Selection (S$^2$FS), a novel
framework for FDSs guided by a spatially-aware separability criterion. This
criterion jointly considers within-class compactness and between-class
separation by integrating scalar-distances with spatial directional
information, providing a more comprehensive characterization of class
structures. S$^2$FS employs a forward greedy strategy to iteratively select the
most discriminative features. Extensive experiments on ten real-world datasets
demonstrate that S$^2$FS consistently outperforms eight state-of-the-art
feature selection algorithms in both classification accuracy and clustering
performance, while feature visualizations further confirm the interpretability
of the selected features.

</details>


### [222] [Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation](https://arxiv.org/abs/2509.25849)
*Ziniu Li,Congliang Chen,Tianyun Yang,Tian Ding,Ruoyu Sun,Ge Zhang,Wenhao Huang,Zhi-Quan Luo*

Main category: cs.LG

TL;DR: The paper proposes an adaptive exploration budget allocation method for LLM self-improvement, framing it as a knapsack problem to optimally distribute resources based on learning status, increasing non-zero policy gradients by 20-40% and achieving 2-4 point gains on math reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current uniform exploration budget allocation in LLM self-improvement creates edge cases where easy tasks consistently succeed and difficult tasks consistently fail, both producing zero gradients in GRPO training, leading to inefficient learning.

Method: Formulate exploration budget allocation as a knapsack problem where each task has distinct 'value' and 'cost', then derive an optimal assignment rule that adaptively distributes resources based on the model's current learning status.

Result: Method increases effective ratio of non-zero policy gradients by 20-40%, enables significantly larger budgets (e.g., 93 rollouts) for challenging problems, and achieves 2-4 point average improvements on math reasoning benchmarks with peak gains of 9 points on specific tasks.

Conclusion: The adaptive budget allocation acts as a computational 'free lunch', enabling comparable performance with about 2x fewer computational resources compared to traditional homogeneous allocation.

Abstract: Large Language Models (LLMs) can self-improve through reinforcement learning,
where they generate trajectories to explore and discover better solutions.
However, this exploration process is computationally expensive, often forcing
current methods to assign limited exploration budgets to each task. This
uniform allocation creates problematic edge cases: easy tasks consistently
succeed while difficult tasks consistently fail, both producing zero gradients
during training updates for the widely used Group Relative Policy Optimization
(GRPO). We address this problem from the lens of exploration budget allocation.
Viewing each task's exploration as an "item" with a distinct "value" and
"cost", we establish a connection to the classical knapsack problem. This
formulation allows us to derive an optimal assignment rule that adaptively
distributes resources based on the model's current learning status. When
applied to GRPO, our method increases the effective ratio of non-zero policy
gradients by 20-40% during training. Acting as a computational "free lunch",
our approach could reallocate exploration budgets from tasks where learning is
saturated to those where it is most impactful. This enables significantly
larger budgets (e.g., 93 rollouts) for especially challenging problems, which
would be computationally prohibitive under a uniform allocation. These
improvements translate to meaningful gains on mathematical reasoning
benchmarks, with average improvements of 2-4 points and peak gains of 9 points
on specific tasks. Notably, achieving comparable performance with traditional
homogeneous allocation would require about 2x the computational resources.

</details>


### [223] [RL-Guided Data Selection for Language Model Finetuning](https://arxiv.org/abs/2509.25850)
*Animesh Jha,Harshit Gupta,Ananjan Nandi*

Main category: cs.LG

TL;DR: This paper proposes using Reinforcement Learning to select optimal data subsets for fine-tuning LLMs, achieving better performance with only 5% of data while reducing training time by 2x.


<details>
  <summary>Details</summary>
Motivation: Existing data selection methods for LLM fine-tuning are pretraining-oriented and don't transfer well, creating a need for more effective budget-constrained optimization approaches.

Method: Reformulate data selection as a Markov Decision Process and train RL agents using proxy-model-based reward signals to learn optimal data selection policies.

Result: Across four datasets, using only 5% of data selected by the RL approach matches or outperforms full dataset fine-tuning by up to 10.8 accuracy points, with 2x faster training time.

Conclusion: RL-guided data selection shows significant promise for efficient LLM fine-tuning, enabling better performance with substantially less data and computational resources.

Abstract: Data selection for finetuning Large Language Models (LLMs) can be framed as a
budget-constrained optimization problem: maximizing a model's downstream
performance under a strict training data budget. Solving this problem is
generally intractable, and existing approximate approaches are
pretraining-oriented and transfer poorly to the fine-tuning setting. We
reformulate this problem as a tractable Markov Decision Process (MDP) and train
agents using various Reinforcement Learning (RL) methods to learn optimal data
selection policies, guided by an efficient, proxy-model-based reward signal.
Across four datasets, training on a $5\%$ subset selected by our approach
matches or outperforms fine-tuning on the full dataset by up to $10.8$ accuracy
points, while cutting wall-clock training time by up to $2 \times$,
highlighting the promise of RL-guided data selection.

</details>


### [224] [Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space](https://arxiv.org/abs/2509.25876)
*Xinyu Zhang,Aishik Deb,Klaus Mueller*

Main category: cs.LG

TL;DR: ExploRLer is a pluggable pipeline that enhances on-policy RL algorithms like PPO by systematically exploring nearby parameter neighborhoods beyond the single surrogate gradient direction, achieving better performance without additional gradient updates.


<details>
  <summary>Details</summary>
Motivation: Policy-gradient methods like PPO use single stochastic gradient directions, leaving rich local parameter structure unexplored. The surrogate gradient often poorly correlates with the true reward landscape, and higher-performing solutions exist in nearby unexplored regions.

Method: Visualize policy checkpoint parameter spaces and introduce ExploRLer - a pluggable pipeline that integrates with on-policy algorithms to systematically probe unexplored neighborhoods of surrogate on-policy gradient updates.

Result: ExploRLer achieves significant improvements over baselines in complex continuous control environments without increasing the number of gradient updates.

Conclusion: Iteration-level exploration provides a practical and effective way to strengthen on-policy reinforcement learning and offers fresh perspective on surrogate objective limitations.

Abstract: Policy-gradient methods such as Proximal Policy Optimization (PPO) are
typically updated along a single stochastic gradient direction, leaving the
rich local structure of the parameter space unexplored. Previous work has shown
that the surrogate gradient is often poorly correlated with the true reward
landscape. Building on this insight, we visualize the parameter space spanned
by policy checkpoints within an iteration and reveal that higher performing
solutions often lie in nearby unexplored regions. To exploit this opportunity,
we introduce ExploRLer, a pluggable pipeline that seamlessly integrates with
on-policy algorithms such as PPO and TRPO, systematically probing the
unexplored neighborhoods of surrogate on-policy gradient updates. Without
increasing the number of gradient updates, ExploRLer achieves significant
improvements over baselines in complex continuous control environments. Our
results demonstrate that iteration-level exploration provides a practical and
effective way to strengthen on-policy reinforcement learning and offer a fresh
perspective on the limitations of the surrogate objective.

</details>


### [225] [Federated Learning with Enhanced Privacy via Model Splitting and Random Client Participation](https://arxiv.org/abs/2509.25906)
*Yiwei Li,Shuai Wang,Zhuojun Tian,Xiuhua Wang,Shijian Su*

Main category: cs.LG

TL;DR: MS-PAFL is a federated learning framework that splits models into private and public parts, injecting noise only into public submodels to reduce accuracy degradation while maintaining strong privacy through structural splitting and statistical amplification.


<details>
  <summary>Details</summary>
Motivation: To address the significant model accuracy degradation caused by differential privacy noise in federated learning while maintaining strong privacy protection.

Method: Model-splitting privacy-amplified federated learning (MS-PAFL) partitions client models into private submodels (kept locally) and public submodels (shared globally), with calibrated Gaussian noise injected only into public submodels, combined with privacy amplification through random client participation and data subsampling.

Result: Theoretical analysis shows tight bounds on privacy loss, significantly reducing required noise for target privacy levels. Experiments demonstrate superior privacy-utility trade-off and highly accurate models under strong privacy guarantees.

Conclusion: MS-PAFL effectively resolves the privacy-utility trade-off in differentially private federated learning by combining structural model splitting with statistical privacy amplification, enabling accurate model training with strong privacy protection.

Abstract: Federated Learning (FL) often adopts differential privacy (DP) to protect
client data, but the added noise required for privacy guarantees can
substantially degrade model accuracy. To resolve this challenge, we propose
model-splitting privacy-amplified federated learning (MS-PAFL), a novel
framework that combines structural model splitting with statistical privacy
amplification. In this framework, each client's model is partitioned into a
private submodel, retained locally, and a public submodel, shared for global
aggregation. The calibrated Gaussian noise is injected only into the public
submodel, thereby confining its adverse impact while preserving the utility of
the local model. We further present a rigorous theoretical analysis that
characterizes the joint privacy amplification achieved through random client
participation and local data subsampling under this architecture. The analysis
provides tight bounds on both single-round and total privacy loss,
demonstrating that MS-PAFL significantly reduces the noise necessary to satisfy
a target privacy protection level. Extensive experiments validate our
theoretical findings, showing that MS-PAFL consistently attains a superior
privacy-utility trade-off and enables the training of highly accurate models
under strong privacy guarantees.

</details>


### [226] [ReNF: Rethinking the Design Space of Neural Long-Term Time Series Forecasters](https://arxiv.org/abs/2509.25914)
*Yihang Lu,Xianwei Meng,Enhong Chen*

Main category: cs.LG

TL;DR: This paper proposes a principled approach to Long-term Time Series Forecasting by introducing the Boosted Direct Output (BDO) strategy that combines Auto-Regressive and Direct Output methods, enabling simple MLPs to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current neural forecasting research overemphasizes architectural complexity while neglecting fundamental forecasting principles, hampering progress in LTSF.

Method: Proposes Boosted Direct Output (BDO) strategy that synergistically combines AR and DO approaches, with smooth parameter tracking for learning stability.

Result: A simple MLP with BDO achieves state-of-the-art performance, outperforming complex models in nearly all cases without domain-specific considerations.

Conclusion: The work establishes a theoretical foundation with Multiple Neural Forecasting Theorem, provides empirical verification, and identifies promising future research directions.

Abstract: Neural Forecasters (NFs) are a cornerstone of Long-term Time Series
Forecasting (LTSF). However, progress has been hampered by an overemphasis on
architectural complexity at the expense of fundamental forecasting principles.
In this work, we return to first principles to redesign the LTSF paradigm. We
begin by introducing a Multiple Neural Forecasting Theorem that provides a
theoretical basis for our approach. We propose Boosted Direct Output (BDO), a
novel forecasting strategy that synergistically combines the advantages of both
Auto-Regressive (AR) and Direct Output (DO). In addition, we stabilize the
learning process by smoothly tracking the model's parameters. Extensive
experiments show that these principled improvements enable a simple MLP to
achieve state-of-the-art performance, outperforming recent, complex models in
nearly all cases, without any specific considerations in the area. Finally, we
empirically verify our theorem, establishing a dynamic performance bound and
identifying promising directions for future research. The code for review is
available at: .

</details>


### [227] [From MNIST to ImageNet: Understanding the Scalability Boundaries of Differentiable Logic Gate Networks](https://arxiv.org/abs/2509.25933)
*Sven Brändle,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: DLGNs are fast and energy-efficient neural networks using logical gates, but their scalability to large multi-class datasets (up to 2000 classes) needs investigation. This paper studies DLGNs' expressiveness, scalability, and alternative output strategies, focusing on temperature tuning and Group-Sum layer performance.


<details>
  <summary>Details</summary>
Motivation: DLGNs are a promising alternative to conventional neural networks due to their speed and energy efficiency, but they have only been tested on small datasets (up to 10 classes). This work aims to understand their behavior and scalability on large multi-class datasets.

Method: The study investigates DLGNs' general expressiveness and scalability using both synthetic and real-world datasets. It evaluates alternative output strategies, with particular focus on temperature tuning and the performance of Group-Sum layers in large-scale classification.

Result: The research provides key insights into the importance of temperature tuning and its impact on output layer performance. It identifies conditions under which the Group-Sum layer performs well and demonstrates its applicability to large-scale classification tasks with up to 2000 classes.

Conclusion: DLGNs can be effectively scaled to handle large multi-class datasets through proper temperature tuning and optimized output layer strategies like the Group-Sum layer, making them viable for practical applications requiring fast and energy-efficient inference.

Abstract: Differentiable Logic Gate Networks (DLGNs) are a very fast and
energy-efficient alternative to conventional feed-forward networks. With
learnable combinations of logical gates, DLGNs enable fast inference by
hardware-friendly execution. Since the concept of DLGNs has only recently
gained attention, these networks are still in their developmental infancy,
including the design and scalability of their output layer. To date, this
architecture has primarily been tested on datasets with up to ten classes.
  This work examines the behavior of DLGNs on large multi-class datasets. We
investigate its general expressiveness, its scalability, and evaluate
alternative output strategies. Using both synthetic and real-world datasets, we
provide key insights into the importance of temperature tuning and its impact
on output layer performance. We evaluate conditions under which the Group-Sum
layer performs well and how it can be applied to large-scale classification of
up to 2000 classes.

</details>


### [228] [AIM: Adaptive Intervention for Deep Multi-task Learning of Molecular Properties](https://arxiv.org/abs/2509.25955)
*Mason Minot,Gisbert Schneider*

Main category: cs.LG

TL;DR: AIM is an optimization framework that learns a dynamic policy to mediate gradient conflicts in multi-task learning for molecular property optimization, achieving better performance in data-scarce regimes while providing interpretable insights.


<details>
  <summary>Details</summary>
Motivation: Multi-task learning for molecular property optimization faces destructive gradient interference, especially in data-scarce drug discovery scenarios, which compromises its efficacy.

Method: AIM learns a dynamic policy jointly with the main network using an augmented objective with dense, differentiable regularizers that guide the policy to produce geometrically stable and dynamically efficient updates.

Result: AIM achieves statistically significant improvements over multi-task baselines on QM9 and targeted protein degraders benchmarks, with advantages most pronounced in data-scarce regimes.

Conclusion: AIM combines data-efficient performance with interpretability through learned policy matrices, highlighting the potential of adaptive optimizers for robust and insightful multi-property molecular design.

Abstract: Simultaneously optimizing multiple, frequently conflicting, molecular
properties is a key bottleneck in the development of novel therapeutics.
Although a promising approach, the efficacy of multi-task learning is often
compromised by destructive gradient interference, especially in the data-scarce
regimes common to drug discovery. To address this, we propose AIM, an
optimization framework that learns a dynamic policy to mediate gradient
conflicts. The policy is trained jointly with the main network using a novel
augmented objective composed of dense, differentiable regularizers. This
objective guides the policy to produce updates that are geometrically stable
and dynamically efficient, prioritizing progress on the most challenging tasks.
We demonstrate that AIM achieves statistically significant improvements over
multi-task baselines on subsets of the QM9 and targeted protein degraders
benchmarks, with its advantage being most pronounced in data-scarce regimes.
Beyond performance, AIM's key contribution is its interpretability; the learned
policy matrix serves as a diagnostic tool for analyzing inter-task
relationships. This combination of data-efficient performance and diagnostic
insight highlights the potential of adaptive optimizers to accelerate
scientific discovery by creating more robust and insightful models for
multi-property molecular design.

</details>


### [229] [Reevaluating Convolutional Neural Networks for Spectral Analysis: A Focus on Raman Spectroscopy](https://arxiv.org/abs/2509.25964)
*Deniz Soysal,Xabier García-Andrade,Laura E. Rodriguez,Pablo Sobron,Laura M. Barge,Renaud Detry*

Main category: cs.LG

TL;DR: This paper presents a workflow for robust Raman spectroscopy classification using 1D CNNs, achieving baseline-independent classification, pooling-controlled robustness, label-efficient learning, and constant-time adaptation for autonomous exploration systems.


<details>
  <summary>Details</summary>
Motivation: Autonomous Raman instruments on Mars rovers, deep-sea landers, and field robots need to interpret raw spectra distorted by fluorescence baselines, peak shifts, and limited ground-truth labels, requiring robust and efficient classification methods.

Method: Used 1D convolutional neural networks (CNNs) on curated RRUFF database subsets, incorporating baseline-independent classification, pooling parameter tuning for robustness, semi-supervised GANs and contrastive pretraining for label efficiency, and constant-time adaptation by freezing CNN backbone and retraining only softmax layer.

Result: Compact CNNs surpassed k-nearest-neighbors and SVMs on handcrafted features, accommodated Raman shifts up to 30 cm⁻¹, achieved up to 11% accuracy improvement with only 10% labels, and enabled model transfer to unseen minerals at O(1) cost.

Conclusion: The proposed workflow provides a practical path toward robust, low-footprint Raman classification in autonomous exploration by training on raw spectra, tuning pooling, adding semi-supervision when labels are scarce, and fine-tuning lightly for new targets.

Abstract: Autonomous Raman instruments on Mars rovers, deep-sea landers, and field
robots must interpret raw spectra distorted by fluorescence baselines, peak
shifts, and limited ground-truth labels. Using curated subsets of the RRUFF
database, we evaluate one-dimensional convolutional neural networks (CNNs) and
report four advances: (i) Baseline-independent classification: compact CNNs
surpass $k$-nearest-neighbors and support-vector machines on handcrafted
features, removing background-correction and peak-picking stages while ensuring
reproducibility through released data splits and scripts. (ii)
Pooling-controlled robustness: tuning a single pooling parameter accommodates
Raman shifts up to $30 \,\mathrm{cm}^{-1}$, balancing translational invariance
with spectral resolution. (iii) Label-efficient learning: semi-supervised
generative adversarial networks and contrastive pretraining raise accuracy by
up to $11\%$ with only $10\%$ labels, valuable for autonomous deployments with
scarce annotation. (iv) Constant-time adaptation: freezing the CNN backbone and
retraining only the softmax layer transfers models to unseen minerals at
$\mathcal{O}(1)$ cost, outperforming Siamese networks on resource-limited
processors. This workflow, which involves training on raw spectra, tuning
pooling, adding semi-supervision when labels are scarce, and fine-tuning
lightly for new targets, provides a practical path toward robust, low-footprint
Raman classification in autonomous exploration.

</details>


### [230] [Data-Free Continual Learning of Server Models in Model-Heterogeneous Federated learning](https://arxiv.org/abs/2509.25977)
*Xiao Zhang,Zengzhe Chen,Yuan Yuan,Yifei Zou,Fuzhen Zhuang,Wenyu Jiao,Yuke Wang,Dongxiao Yu*

Main category: cs.LG

TL;DR: FedDCL is a novel federated learning framework that enables data-free continual learning in model-heterogeneous settings using diffusion models to create class-specific prototypes for synthetic data generation, generative replay, and knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Traditional federated learning faces challenges with data heterogeneity, model heterogeneity, catastrophic forgetting, and knowledge misalignment when dealing with new data and diverse models in dynamic environments.

Method: Leverages pre-trained diffusion models to extract lightweight class-specific prototypes that enable three data-free capabilities: synthetic data generation for current tasks, exemplar-free generative replay for knowledge retention, and data-free dynamic knowledge transfer from heterogeneous clients.

Result: Experimental results on various datasets demonstrate FedDCL's effectiveness in enhancing generalizability and practical applicability of federated learning in dynamic settings.

Conclusion: FedDCL shows potential to address key challenges in federated learning and improve its deployment in real-world dynamic scenarios through data-free continual learning capabilities.

Abstract: Federated learning (FL) is a distributed learning paradigm across multiple
entities while preserving data privacy. However, with the continuous emergence
of new data and increasing model diversity, traditional federated learning
faces significant challenges, including inherent issues of data heterogeneity,
model heterogeneity and catastrophic forgetting, along with new challenge of
knowledge misalignment. In this study, we introduce FedDCL, a novel framework
designed to enable data-free continual learning of the server model in a
model-heterogeneous federated setting. We leverage pre-trained diffusion models
to extract lightweight class-specific prototypes, which confer a threefold
data-free advantage, enabling: (1) generation of synthetic data for the current
task to augment training and counteract non-IID data distributions; (2)
exemplar-free generative replay for retaining knowledge from previous tasks;
and (3) data-free dynamic knowledge transfer from heterogeneous clients to the
server. Experimental results on various datasets demonstrate the effectiveness
of FedDCL, showcasing its potential to enhance the generalizability and
practical applicability of federated learning in dynamic settings.

</details>


### [231] [Reconcile Certified Robustness and Accuracy for DNN-based Smoothed Majority Vote Classifier](https://arxiv.org/abs/2509.25979)
*Gaojie Jin,Xinping Yi,Xiaowei Huang*

Main category: cs.LG

TL;DR: This paper develops a certified robustness framework for majority vote classifiers within the PAC-Bayesian framework, connecting generalization error bounds with certified robust radii through weight spectral norm analysis.


<details>
  <summary>Details</summary>
Motivation: There is a notable lack of theoretical research exploring the certified robustness of majority vote classifiers and its interplay with generalization performance in the PAC-Bayesian framework.

Method: Developed a generalization error bound with certified robust radius for smoothed majority vote classifiers, using weight spectral norm analysis and proposing a novel spectral regularizer for smooth training.

Result: Theoretical framework connects generalization bounds with certified robustness through spectral norm properties, and empirical results substantiate the effectiveness of the proposed spectral regularization method.

Conclusion: The study bridges the gap between generalization and certified robustness for majority vote classifiers, demonstrating that spectral regularization can effectively boost certified robustness while maintaining generalization performance.

Abstract: Within the PAC-Bayesian framework, the Gibbs classifier (defined on a
posterior $Q$) and the corresponding $Q$-weighted majority vote classifier are
commonly used to analyze the generalization performance. However, there exists
a notable lack in theoretical research exploring the certified robustness of
majority vote classifier and its interplay with generalization. In this study,
we develop a generalization error bound that possesses a certified robust
radius for the smoothed majority vote classifier (i.e., the $Q$-weighted
majority vote classifier with smoothed inputs); In other words, the
generalization bound holds under any data perturbation within the certified
robust radius. As a byproduct, we find that the underpinnings of both the
generalization bound and the certified robust radius draw, in part, upon weight
spectral norm, which thereby inspires the adoption of spectral regularization
in smooth training to boost certified robustness. Utilizing the
dimension-independent property of spherical Gaussian inputs in smooth training,
we propose a novel and inexpensive spectral regularizer to enhance the smoothed
majority vote classifier. In addition to the theoretical contribution, a set of
empirical results is provided to substantiate the effectiveness of our proposed
method.

</details>


### [232] [Exact Solutions to the Quantum Schrödinger Bridge Problem](https://arxiv.org/abs/2509.25980)
*Mykola Bordyuh,Djork-Arné Clevert,Marco Bertolini*

Main category: cs.LG

TL;DR: The paper formulates the Quantum Schrödinger Bridge Problem (QSBP) from a Lagrangian perspective, derives exact closed-form solutions for Gaussian distributions, and presents a modified Gaussian Mixture Model algorithm with applications in various domains.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between mathematical theory and practical generative modeling by formulating QSBP from a Lagrangian perspective and deriving explicit solutions for Gaussian distributions.

Method: Lagrangian formulation of dynamical Optimal Transport, solving Fokker-Planck Equation and Hamilton-Jacobi Equation, and developing a modified Gaussian Mixture Model algorithm.

Result: Exact closed-form solutions for QSBP between Gaussian distributions were derived, showing the solution is a Gaussian process with quantum-modified covariance evolution. The algorithm demonstrated effectiveness in single-cell evolution, image generation, molecular translation, and Mean-Field Games.

Conclusion: The QSBP framework provides a quantum-inspired approach to stochastic processes with non-local Bohm potential, offering new capabilities for generative modeling that distinguish it from classical stochastic dynamics.

Abstract: The Quantum Schr\"odinger Bridge Problem (QSBP) describes the evolution of a
stochastic process between two arbitrary probability distributions, where the
dynamics are governed by the Schr\"odinger equation rather than by the
traditional real-valued wave equation. Although the QSBP is known in the
mathematical literature, we formulate it here from a Lagrangian perspective and
derive its main features in a way that is particularly suited to generative
modeling. We show that the resulting evolution equations involve the so-called
Bohm (quantum) potential, representing a notion of non-locality in the
stochastic process. This distinguishes the QSBP from classical stochastic
dynamics and reflects a key characteristic typical of quantum mechanical
systems. In this work, we derive exact closed-form solutions for the QSBP
between Gaussian distributions. Our derivation is based on solving the
Fokker-Planck Equation (FPE) and the Hamilton-Jacobi Equation (HJE) arising
from the Lagrangian formulation of dynamical Optimal Transport. We find that,
similar to the classical Schr\"odinger Bridge Problem, the solution to the QSBP
between Gaussians is again a Gaussian process; however, the evolution of the
covariance differs due to quantum effects. Leveraging these explicit solutions,
we present a modified algorithm based on a Gaussian Mixture Model framework,
and demonstrate its effectiveness across several experimental settings,
including single-cell evolution data, image generation, molecular translation
and applications in Mean-Field Games.

</details>


### [233] [CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models](https://arxiv.org/abs/2509.25996)
*Weiyu Huang,Yuezhou Hu,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: CAST is a fully continuous and differentiable sparsity-aware training framework for semi-structured sparse models that enables joint optimization of sparsity patterns and weights during training, achieving state-of-the-art performance with minimal training resources.


<details>
  <summary>Details</summary>
Motivation: To reduce latency and memory consumption during LLM inference by transforming models into hardware-friendly sparse patterns through a more effective training approach than previous separate optimization methods.

Method: CAST framework with three key components: AdamS optimizer with adaptive L1 decay for uniform sparsification, Weight Scaling to mitigate magnitude reduction while preserving sparsity patterns, and Knowledge Distillation using dense model as self-teacher.

Result: Significant improvements over previous methods in perplexity and zero-shot accuracy; on LLaMA2-7B with 2:4 sparsity achieved only 0.09 perplexity increase and 0.36% accuracy gain using only 2% of original pretraining tokens.

Conclusion: CAST enables efficient training of high-quality sparse models with minimal resources, establishes empirical scaling laws for performance prediction, and demonstrates practical applicability under quantization and fine-tuning scenarios.

Abstract: Sparsity-aware training is an effective approach for transforming large
language models (LLMs) into hardware-friendly sparse patterns, thereby reducing
latency and memory consumption during inference. In this paper, we propose
Continuous Adaptive Sparse Trainer (CAST), a fully continuous and
differentiable sparsity-aware training framework for semi-structured (or "N:M")
sparse models. Unlike previous approaches that optimize sparsity patterns and
weights separately, CAST enables seamless joint optimization during training,
while progressively transforming the model into the desired sparsity format.
Specifically, CAST introduces three key components: 1) AdamS, a sparsity-aware
optimizer that leverages adaptive L1 decay to promote uniform sparsification
across all parameters; 2) Weight Scaling, a module designed to mitigate the
magnitude reduction caused by decay while preserving desired sparsity patterns;
3) Knowledge Distillation, which employs the dense model as a self-teacher to
enhance training efficiency. We evaluate CAST under 2:4 sparsity patterns
across multiple model families, ranging from 125M to 13B parameters. Our
results demonstrate significant improvements over previous state-of-the-art
methods in both perplexity and zero-shot accuracy with minimal training
resources. Notably, on LLaMA2-7B, our 2:4 sparse model achieves a negligible
perplexity increase of 0.09 and a 0.36% gain in zero-shot accuracy compared to
the dense model using only 2% of the original pretraining tokens. Additionally,
we establish an accurate and robust empirical scaling law to predict sparse
model performance given adequate training resources. Finally, we demonstrate
the practical applicability of our sparse models by evaluating them under
quantization and fine-tuning scenarios.

</details>


### [234] [Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access](https://arxiv.org/abs/2509.26000)
*Daniel Ebi,Gaspard Lambrechts,Damien Ernst,Klemens Böhm*

Main category: cs.LG

TL;DR: Proposes informed asymmetric actor-critic framework that allows conditioning critics on arbitrary privileged signals without full state access, maintaining unbiased policy gradients while improving learning efficiency in partially observable environments.


<details>
  <summary>Details</summary>
Motivation: Existing asymmetric actor-critic methods assume full-state access during training, which is often impractical. The paper challenges this assumption to enable more flexible use of privileged information.

Method: Develops informed asymmetric actor-critic framework that conditions critic on arbitrary privileged signals, proposes informativeness measures using kernel methods and return prediction error, and validates on navigation tasks and synthetic POMDPs.

Result: Empirical validation shows improved learning efficiency and value estimation when informative privileged inputs are available, while maintaining theoretical guarantees of unbiased policy gradients.

Conclusion: Challenges the necessity of full-state access in asymmetric RL, opens new directions for practical asymmetric methods that leverage partial privileged information while remaining theoretically sound.

Abstract: Reinforcement learning in partially observable environments requires agents
to act under uncertainty from noisy, incomplete observations. Asymmetric
actor-critic methods leverage privileged information during training to improve
learning under these conditions. However, existing approaches typically assume
full-state access during training. In this work, we challenge this assumption
by proposing a novel actor-critic framework, called informed asymmetric
actor-critic, that enables conditioning the critic on arbitrary privileged
signals without requiring access to the full state. We show that policy
gradients remain unbiased under this formulation, extending the theoretical
foundation of asymmetric methods to the more general case of privileged partial
information. To quantify the impact of such signals, we propose informativeness
measures based on kernel methods and return prediction error, providing
practical tools for evaluating training-time signals. We validate our approach
empirically on benchmark navigation tasks and synthetic partially observable
environments, showing that our informed asymmetric method improves learning
efficiency and value estimation when informative privileged inputs are
available. Our findings challenge the necessity of full-state access and open
new directions for designing asymmetric reinforcement learning methods that are
both practical and theoretically sound.

</details>


### [235] [Indirect Attention: Turning Context Misalignment into a Feature](https://arxiv.org/abs/2509.26015)
*Bissmella Bahaduri,Hicham Talaoubrid,Fangchen Feng,Zuheng Ming,Anissa Mokraoui*

Main category: cs.LG

TL;DR: The paper analyzes attention mechanisms when keys and values come from different sequences/modalities, identifies a critical noise threshold for value degradation, models context misalignment as structured noise, and proposes Indirect Attention to handle misalignment scenarios.


<details>
  <summary>Details</summary>
Motivation: To address limitations of standard attention mechanisms when keys and values originate from different sequences or modalities, particularly when context misalignment creates structured noise that exceeds critical thresholds.

Method: The authors first analyze attention behavior under noisy value features, establish a critical noise threshold, model context misalignment as structured noise, and then introduce Indirect Attention mechanism that infers relevance indirectly in misaligned scenarios.

Result: The proposed Indirect Attention demonstrates superior performance compared to standard attention across synthetic tasks and real-world applications when dealing with misaligned context between keys and values.

Conclusion: Indirect Attention provides an effective solution for attention mechanisms operating in scenarios where keys and values come from different sequences or modalities, overcoming the limitations of standard attention in handling context misalignment.

Abstract: The attention mechanism has become a cornerstone of modern deep learning
architectures, where keys and values are typically derived from the same
underlying sequence or representation. This work explores a less conventional
scenario, when keys and values originate from different sequences or
modalities. Specifically, we first analyze the attention mechanism's behavior
under noisy value features, establishing a critical noise threshold beyond
which signal degradation becomes significant. Furthermore, we model context
(key, value) misalignment as an effective form of structured noise within the
value features, demonstrating that the noise induced by such misalignment can
substantially exceed this critical threshold, thereby compromising standard
attention's efficacy. Motivated by this, we introduce Indirect Attention, a
modified attention mechanism that infers relevance indirectly in scenarios with
misaligned context. We evaluate the performance of Indirect Attention across a
range of synthetic tasks and real world applications, showcasing its superior
ability to handle misalignment.

</details>


### [236] [FITS: Towards an AI-Driven Fashion Information Tool for Sustainability](https://arxiv.org/abs/2509.26017)
*Daphne Theodorakopoulos,Elisabeth Eberling,Miriam Bodenheimer,Sabine Loos,Frederic Stahl*

Main category: cs.LG

TL;DR: FITS is a transformer-based NLP system that extracts and classifies sustainability information from fashion industry sources to address the lack of credible sustainability data.


<details>
  <summary>Details</summary>
Motivation: Limited access to credible sustainability information in fashion industry, general-purpose language models lack domain knowledge and hallucinate, need for factual correctness in sustainability data.

Method: Fine-tuned BERT-based models on curated corpus using domain-specific classification schema, hyperparameter optimization via Bayesian optimization, developed interactive interface for data search and analysis.

Result: Developed FITS prototype evaluated in focus groups, created SustainableTextileCorpus dataset, system enables extraction and classification of sustainability information from credible sources.

Conclusion: Domain-adapted NLP is valuable for promoting informed decision-making in sustainability, AI has broader potential for climate-related challenges, provides methodology and dataset for future work.

Abstract: Access to credible sustainability information in the fashion industry remains
limited and challenging to interpret, despite growing public and regulatory
demands for transparency. General-purpose language models often lack
domain-specific knowledge and tend to "hallucinate", which is particularly
harmful for fields where factual correctness is crucial. This work explores how
Natural Language Processing (NLP) techniques can be applied to classify
sustainability data for fashion brands, thereby addressing the scarcity of
credible and accessible information in this domain. We present a prototype
Fashion Information Tool for Sustainability (FITS), a transformer-based system
that extracts and classifies sustainability information from credible,
unstructured text sources: NGO reports and scientific publications. Several
BERT-based language models, including models pretrained on scientific and
climate-specific data, are fine-tuned on our curated corpus using a
domain-specific classification schema, with hyperparameters optimized via
Bayesian optimization. FITS allows users to search for relevant data, analyze
their own data, and explore the information via an interactive interface. We
evaluated FITS in two focus groups of potential users concerning usability,
visual design, content clarity, possible use cases, and desired features. Our
results highlight the value of domain-adapted NLP in promoting informed
decision-making and emphasize the broader potential of AI applications in
addressing climate-related challenges. Finally, this work provides a valuable
dataset, the SustainableTextileCorpus, along with a methodology for future
updates. Code available at https://github.com/daphne12345/FITS

</details>


### [237] [Muon Outperforms Adam in Tail-End Associative Memory Learning](https://arxiv.org/abs/2509.26030)
*Shuche Wang,Fengzhuo Zhang,Jiaxiang Li,Cunxiao Du,Chao Du,Tianyu Pang,Zhuoran Yang,Mingyi Hong,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: Muon optimizer outperforms Adam in LLM training due to better optimization of associative memory parameters (VO attention weights and FFNs), particularly for tail classes in heavy-tailed data distributions.


<details>
  <summary>Details</summary>
Motivation: To understand why Muon optimizer consistently trains LLMs faster than Adam, focusing on the underlying mechanisms through the lens of associative memory.

Method: Ablated transformer components optimized by Muon, analyzed singular spectrum properties, and theoretically analyzed a one-layer associative memory model under class-imbalanced data.

Result: Muon's update rule yields more isotropic singular spectrum than Adam, enabling more effective optimization of tail classes in heavy-tailed corpora, with theoretical confirmation of balanced learning across classes.

Conclusion: Muon's advantage stems from its update rule aligning with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions compared to Adam.

Abstract: The Muon optimizer is consistently faster than Adam in training Large
Language Models (LLMs), yet the mechanism underlying its success remains
unclear. This paper demystifies this mechanism through the lens of associative
memory. By ablating the transformer components optimized by Muon, we reveal
that the associative memory parameters of LLMs, namely the Value and Output
(VO) attention weights and Feed-Forward Networks (FFNs), are the primary
contributors to Muon's superiority. Motivated by this associative memory view,
we then explain Muon's superiority on real-world corpora, which are
intrinsically heavy-tailed: a few classes (tail classes) appear far less
frequently than others. The superiority is explained through two key
properties: (i) its update rule consistently yields a more isotropic singular
spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes
tail classes more effectively than Adam. Beyond empirical evidence, we
theoretically confirm these findings by analyzing a one-layer associative
memory model under class-imbalanced data. We prove that Muon consistently
achieves balanced learning across classes regardless of feature embeddings,
whereas Adam can induce large disparities in learning errors depending on
embedding properties. In summary, our empirical observations and theoretical
analyses reveal Muon's core advantage: its update rule aligns with the
outer-product structure of linear associative memories, enabling more balanced
and effective learning of tail classes in heavy-tailed distributions than Adam.

</details>


### [238] [Scaling Up Temporal Domain Generalization via Temporal Experts Averaging](https://arxiv.org/abs/2509.26045)
*Aoming Liu,Kevin Miller,Venkatesh Saligrama,Kate Saenko,Boqing Gong,Ser-Nam Lim,Bryan A. Plummer*

Main category: cs.LG

TL;DR: TEA is a scalable Temporal Domain Generalization framework that uses weight averaging of constrained fine-tuned experts to handle temporal distribution shifts efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing TDG methods that predict future model weights are computationally expensive for large models, while classifier-only prediction limits generalization.

Method: Fine-tune domain-agnostic base model on temporal domains with weight constraints to create diverse experts, then use adaptive averaging coefficients based on temporal weight trajectories in PCA subspace.

Result: Outperforms prior TDG methods by up to 69% across 7 benchmarks and 5 models, while being 60x more efficient.

Conclusion: TEA provides an effective and scalable solution for temporal domain generalization through constrained expert creation and adaptive weight averaging.

Abstract: Temporal Domain Generalization (TDG) aims to generalize across temporal
distribution shifts, e.g., lexical change over time. Prior work often addresses
this by predicting future model weights. However, full model prediction is
prohibitively expensive for even reasonably sized models. Thus, recent methods
only predict the classifier layer, limiting generalization by failing to adjust
other model components. To address this, we propose Temporal Experts Averaging
(TEA), a novel and scalable TDG framework that updates the entire model using
weight averaging to maximize generalization potential while minimizing
computational costs. Our theoretical analysis guides us to two steps that
enhance generalization to future domains. First, we create expert models with
functional diversity yet parameter similarity by fine-tuning a domain-agnostic
base model on individual temporal domains while constraining weight changes.
Second, we optimize the bias-variance tradeoff through adaptive averaging
coefficients derived from modeling temporal weight trajectories in a principal
component subspace. Expert's contributions are based on their projected
proximity to future domains. Extensive experiments across 7 TDG benchmarks, 5
models, and 2 TDG settings shows TEA outperforms prior TDG methods by up to 69%
while being up to 60x more efficient.

</details>


### [239] [Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners](https://arxiv.org/abs/2509.26226)
*Xin Xu,Cliveb AI,Kai Yang,Tianhao Chen,Yang Wang,Saiyong Yang,Can Yang*

Main category: cs.LG

TL;DR: TFPI introduces a ThinkFree operation that discards thinking content to reduce token usage in RLVR training, improving performance and efficiency without complex modifications.


<details>
  <summary>Details</summary>
Motivation: RLVR requires long context lengths leading to high computational costs, and multi-stage training with short contexts causes irreversible performance degradation.

Method: TFPI bridges CoT distillation and RLVR by using a ThinkFree operation that explicitly discards thinking content via </think> append to reduce token usage during inference.

Result: TFPI accelerates RL convergence, achieves higher performance ceiling, and yields more token-efficient reasoning models. A 4B model reached 89.0% on AIME24 and 65.5% on LiveCodeBench using <4K H20 hours.

Conclusion: TFPI is a simple yet effective adaptation that improves RLVR training efficiency and performance without specialized rewards or complex training designs.

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) effectively solves
complex tasks but demands extremely long context lengths during training,
leading to substantial computational costs. While multi-stage training can
partially mitigate this, starting with overly short contexts often causes
irreversible performance degradation, ultimately failing to reduce overall
training compute significantly. In this paper, we introduce
**T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet
effective adaptation to RLVR that bridges long Chain-of-Thought (CoT)
distillation and standard RLVR. TFPI employs a simple *ThinkFree* operation,
explicitly discarding the thinking content via a direct *</think>* append, to
reduce token usage during inference. Training with *ThinkFree*-adapted inputs
improves performance and lowers token consumption, even in the original
slow-thinking mode. Extensive experiments across various benchmarks have shown
that TFPI accelerates RL convergence, achieves a higher performance ceiling,
and yields more token-efficient reasoning models without specialized rewards or
complex training designs. With TFPI only, we train a 4B model to reach 89.0%
accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.

</details>


### [240] [Real-time Noise Detection and Classification in Single-Channel EEG: A Lightweight Machine Learning Approach for EMG, White Noise, and EOG Artifacts](https://arxiv.org/abs/2509.26058)
*Hossein Enshaei,Pariya Jebreili,Sayed Mahmoud Sakahei*

Main category: cs.LG

TL;DR: Proposes a hybrid spectral-temporal framework for real-time EEG artifact detection that combines time-domain filtering and frequency-domain analysis with PCA-optimized feature fusion, achieving high accuracy with lightweight MLP architecture.


<details>
  <summary>Details</summary>
Motivation: Address challenges in EEG artifact detection including computational inefficiency in multi-channel methods, poor robustness to simultaneous noise, and accuracy-complexity trade-offs in deep learning models.

Method: Hybrid spectral-temporal framework combining time-domain low-pass filtering (for EOG) and frequency-domain PSD analysis (for EMG), followed by PCA-optimized feature fusion and lightweight MLP classifier.

Result: Achieves 99% accuracy at low SNRs (-7 dB), >90% accuracy at moderate noise (4 dB), and 96% accuracy with simultaneous multi-source contamination. 30-second training time (97% faster than CNNs).

Conclusion: Demonstrates that domain-informed feature fusion with lightweight architecture outperforms complex deep learning models in noisy EEG scenarios, enabling real-time use in wearable brain-computer interfaces.

Abstract: Electroencephalogram (EEG) artifact detection in real-world settings faces
significant challenges such as computational inefficiency in multi-channel
methods, poor robustness to simultaneous noise, and trade-offs between accuracy
and complexity in deep learning models. We propose a hybrid spectral-temporal
framework for real-time detection and classification of ocular (EOG), muscular
(EMG), and white noise artifacts in single-channel EEG. This method, in
contrast to other approaches, combines time-domain low-pass filtering
(targeting low-frequency EOG) and frequency-domain power spectral density (PSD)
analysis (capturing broad-spectrum EMG), followed by PCA-optimized feature
fusion to minimize redundancy while preserving discriminative information. This
feature engineering strategy allows a lightweight multi-layer perceptron (MLP)
architecture to outperform advanced CNNs and RNNs by achieving 99% accuracy at
low SNRs (SNR -7) dB and >90% accuracy in moderate noise (SNR 4 dB).
Additionally, this framework addresses the unexplored problem of simultaneous
multi-source contamination(EMG+EOG+white noise), where it maintains 96%
classification accuracy despite overlapping artifacts. With 30-second training
times (97% faster than CNNs) and robust performance across SNR levels, this
framework bridges the gap between clinical applicability and computational
efficiency, which enables real-time use in wearable brain-computer interfaces.
This work also challenges the ubiquitous dependence on model depth for EEG
artifact detection by demonstrating that domain-informed feature fusion
surpasses complex architecture in noisy scenarios.

</details>


### [241] [Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2509.26114)
*Jaesung R. Park,Junsu Kim,Gyeongman Kim,Jinyoung Jo,Sean Choi,Jaewoong Cho,Ernest K. Ryu*

Main category: cs.LG

TL;DR: The paper reveals that PPO and GRPO's clipping mechanisms cause entropy bias in RLVR training, with clip-low increasing entropy and clip-high decreasing it, leading to entropy collapse. The authors propose using aggressive clip-low to maintain exploration.


<details>
  <summary>Details</summary>
Motivation: RLVR is prone to entropy collapse where LLMs become near-deterministic, hindering exploration and progress during prolonged training. The clipping mechanism in PPO/GRPO was identified as a key factor causing this issue.

Method: Theoretical and empirical analysis of PPO and GRPO clipping mechanisms, examining how clip-low and clip-high parameters affect entropy dynamics in RLVR training.

Result: Clip-low increases entropy while clip-high decreases it. Under standard clipping parameters, clip-high dominates, causing overall entropy reduction even with random rewards. Aggressive clip-low can increase entropy and prevent collapse.

Conclusion: Clipping mechanism is a confounding factor in RLVR that independently affects entropy and reasoning behavior. Clipping can be deliberately used to control entropy, with aggressive clip-low promoting exploration and preventing entropy collapse.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently emerged as
the leading approach for enhancing the reasoning capabilities of large language
models (LLMs). However, RLVR is prone to entropy collapse, where the LLM
quickly converges to a near-deterministic form, hindering exploration and
progress during prolonged RL training. In this work, we reveal that the
clipping mechanism in PPO and GRPO induces biases on entropy. Through
theoretical and empirical analyses, we show that clip-low increases entropy,
while clip-high decreases it. Further, under standard clipping parameters, the
effect of clip-high dominates, resulting in an overall entropy reduction even
when purely random rewards are provided to the RL algorithm. Our findings
highlight an overlooked confounding factor in RLVR: independent of the reward
signal, the clipping mechanism influences entropy, which in turn affects the
reasoning behavior. Furthermore, our analysis demonstrates that clipping can be
deliberately used to control entropy. Specifically, with a more aggressive
clip-low value, one can increase entropy, promote exploration, and ultimately
prevent entropy collapse in RLVR training.

</details>


### [242] [UncertainGen: Uncertainty-Aware Representations of DNA Sequences for Metagenomic Binning](https://arxiv.org/abs/2509.26116)
*Abdulkadir Celikkanat,Andres R. Masegosa,Mads Albertsen,Thomas D. Nielsen*

Main category: cs.LG

TL;DR: UncertainGen is the first probabilistic embedding method for metagenomic binning that represents DNA fragments as probability distributions to capture sequence uncertainty, outperforming deterministic k-mer and LLM-based approaches.


<details>
  <summary>Details</summary>
Motivation: Existing deterministic methods fail to capture uncertainty in DNA sequences caused by inter-species DNA sharing and fragments with highly similar representations, limiting binning accuracy.

Method: Probabilistic embedding approach representing each DNA fragment as a probability distribution in latent space with theoretical guarantees on embedding distinguishability and a data-adaptive metric for flexible cluster separation.

Result: Experiments on real metagenomic datasets demonstrate improvements over deterministic k-mer and LLM-based embeddings, offering a scalable and lightweight solution for large-scale analysis.

Conclusion: Probabilistic embedding framework effectively models sequence-level uncertainty and enables more flexible separation of bins/clusters, advancing metagenomic binning capabilities.

Abstract: Metagenomic binning aims to cluster DNA fragments from mixed microbial
samples into their respective genomes, a critical step for downstream analyses
of microbial communities. Existing methods rely on deterministic
representations, such as k-mer profiles or embeddings from large language
models, which fail to capture the uncertainty inherent in DNA sequences arising
from inter-species DNA sharing and from fragments with highly similar
representations. We present the first probabilistic embedding approach,
UncertainGen, for metagenomic binning, representing each DNA fragment as a
probability distribution in latent space. Our approach naturally models
sequence-level uncertainty, and we provide theoretical guarantees on embedding
distinguishability. This probabilistic embedding framework expands the feasible
latent space by introducing a data-adaptive metric, which in turn enables more
flexible separation of bins/clusters. Experiments on real metagenomic datasets
demonstrate the improvements over deterministic k-mer and LLM-based embeddings
for the binning task by offering a scalable and lightweight solution for
large-scale metagenomic analysis.

</details>


### [243] [Domain-Aware Hyperdimensional Computing for Edge Smart Manufacturing](https://arxiv.org/abs/2509.26131)
*Fardin Jalil Piran,Anandkumar Patel,Rajiv Malhotra,Farhad Imani*

Main category: cs.LG

TL;DR: HDC performance depends on application-specific hyperparameter tuning, not universal rules. Signals prefer nonlinear encodings with higher dimensions, while images work better with linear encodings and smaller dimensions. Tuned HDC matches/exceeds deep learning accuracy with 6x faster inference and 40x lower training energy.


<details>
  <summary>Details</summary>
Motivation: Smart manufacturing needs efficient on-device AI that meets strict latency and energy constraints. HDC offers lightweight computing but prior assumptions about stable hyperparameter-performance relationships across applications are incorrect.

Method: Analyzed HDC on two manufacturing tasks: CNC signal monitoring and LPBF image defect detection. Mapped encoder type, projection variance, dimensionality, and data regime effects on performance metrics. Developed complexity model for encoding and similarity computation.

Result: Signals favor nonlinear Random Fourier Features with exclusive encodings and saturate beyond moderate dimensions. Images prefer linear Random Projection, achieve high accuracy with small dimensions, and depend more on sample count. Tuned HDC matches/exceeds SOTA deep learning and Transformers with 6x faster inference and 40x lower training energy.

Conclusion: Domain-aware HDC encoding is essential for optimal performance. Tuned HDC provides practical, scalable real-time industrial AI on constrained hardware. Future work includes adaptive encoder selection and validation on low-power accelerators.

Abstract: Smart manufacturing requires on-device intelligence that meets strict latency
and energy budgets. HyperDimensional Computing (HDC) offers a lightweight
alternative by encoding data as high-dimensional hypervectors and computing
with simple operations. Prior studies often assume that the qualitative
relation between HDC hyperparameters and performance is stable across
applications. Our analysis of two representative tasks, signal-based quality
monitoring in Computer Numerical Control (CNC) machining and image-based defect
detection in Laser Powder Bed Fusion (LPBF), shows that this assumption does
not hold. We map how encoder type, projection variance, hypervector
dimensionality, and data regime shape accuracy, inference latency, training
time, and training energy. A formal complexity model explains predictable
trends in encoding and similarity computation and reveals nonmonotonic
interactions with retraining that preclude a closed-form optimum. Empirically,
signals favor nonlinear Random Fourier Features with more exclusive encodings
and saturate in accuracy beyond moderate dimensionality. Images favor linear
Random Projection, achieve high accuracy with small dimensionality, and depend
more on sample count than on dimensionality. Guided by these insights, we tune
HDC under multiobjective constraints that reflect edge deployment and obtain
models that match or exceed the accuracy of state-of-the-art deep learning and
Transformer models while delivering at least 6x faster inference and more than
40x lower training energy. These results demonstrate that domain-aware HDC
encoding is necessary and that tuned HDC offers a practical, scalable path to
real-time industrial AI on constrained hardware. Future work will enable
adaptive encoder and hyperparameter selection, expand evaluation to additional
manufacturing modalities, and validate on low-power accelerators.

</details>


### [244] [Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces](https://arxiv.org/abs/2509.26594)
*John Gkountouras,Ivan Titov*

Main category: cs.LG

TL;DR: AC-RL trains vision models to generate comprehensive captions for mathematical reasoning by using clarification requests as implicit supervision, improving accuracy by 4.4 points across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models produce captions for humans that omit precise details needed by reasoning systems, creating an interface mismatch where reasoners fail due to missing visual information rather than reasoning limitations.

Method: Adaptive-Clarification Reinforcement Learning (AC-RL) teaches vision models what information reasoners need through interaction, penalizing success that requires clarification to create pressure for comprehensive initial captions.

Result: AC-RL improves average accuracy by 4.4 points over pretrained baselines across seven visual mathematical reasoning benchmarks and would cut clarification requests by up to 39% if allowed.

Conclusion: By treating clarification as implicit supervision, AC-RL demonstrates that vision-language interfaces can be effectively learned through interaction alone without requiring explicit annotations.

Abstract: Recent text-only models demonstrate remarkable mathematical reasoning
capabilities. Extending these to visual domains requires vision-language models
to translate images into text descriptions. However, current models, trained to
produce captions for human readers, often omit the precise details that
reasoning systems require. This creates an interface mismatch: reasoners often
fail not due to reasoning limitations but because they lack access to critical
visual information. We propose Adaptive-Clarification Reinforcement Learning
(AC-RL), which teaches vision models what information reasoners need through
interaction. Our key insight is that clarification requests during training
reveal information gaps; by penalizing success that requires clarification, we
create pressure for comprehensive initial captions that enable the reasoner to
solve the problem in a single pass. AC-RL improves average accuracy by 4.4
points over pretrained baselines across seven visual mathematical reasoning
benchmarks, and analysis shows it would cut clarification requests by up to 39%
if those were allowed. By treating clarification as a form of implicit
supervision, AC-RL demonstrates that vision-language interfaces can be
effectively learned through interaction alone, without requiring explicit
annotations.

</details>


### [245] [Accelerating Transformers in Online RL](https://arxiv.org/abs/2509.26137)
*Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: Proposes a two-stage method using a stable Accelerator policy to train transformers in RL, enabling stable online training and reducing computational requirements.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models in RL face instability issues in model-free online settings, making existing algorithms difficult to implement effectively.

Method: Two-stage approach: 1) Accelerator policy interacts with environment and trains transformer via behavior cloning, 2) Pretrained transformer interacts online. Works with both state-based and image-based environments.

Result: Enables stable transformer training, reduces training time by up to 2x on image-based environments, and decreases replay buffer size to 10-20k in off-policy methods.

Conclusion: The proposed algorithm successfully stabilizes transformer training in RL while significantly reducing computational demands and training time.

Abstract: The appearance of transformer-based models in Reinforcement Learning (RL) has
expanded the horizons of possibilities in robotics tasks, but it has
simultaneously brought a wide range of challenges during its implementation,
especially in model-free online RL. Some of the existing learning algorithms
cannot be easily implemented with transformer-based models due to the
instability of the latter. In this paper, we propose a method that uses the
Accelerator policy as a transformer's trainer. The Accelerator, a simpler and
more stable model, interacts with the environment independently while
simultaneously training the transformer through behavior cloning during the
first stage of the proposed algorithm. In the second stage, the pretrained
transformer starts to interact with the environment in a fully online setting.
As a result, this model-free algorithm accelerates the transformer in terms of
its performance and helps it to train online in a more stable and faster way.
By conducting experiments on both state-based and image-based ManiSkill
environments, as well as on MuJoCo tasks in MDP and POMDP settings, we show
that applying our algorithm not only enables stable training of transformers
but also reduces training time on image-based environments by up to a factor of
two. Moreover, it decreases the required replay buffer size in off-policy
methods to 10-20 thousand, which significantly lowers the overall computational
demands.

</details>


### [246] [Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models](https://arxiv.org/abs/2509.26628)
*Runze Liu,Jiakang Wang,Yuling Shi,Zhihui Xie,Chenxin An,Kaiyan Zhang,Jian Zhao,Xiaodong Gu,Lei Lin,Wenping Hu,Xiu Li,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.LG

TL;DR: AttnRL is a novel Process-Supervised RL framework that improves reasoning in LLMs through attention-based branching and adaptive sampling strategies, achieving better performance and efficiency than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing PSRL approaches suffer from limited exploration efficiency in both branching positions and sampling, which hinders their effectiveness in enhancing LLM reasoning capabilities.

Method: Proposes branching from positions with high attention scores, adaptive sampling strategy considering problem difficulty and historical batch size, and one-step off-policy training pipeline for PSRL.

Result: Extensive experiments on multiple challenging mathematical reasoning benchmarks show consistent outperformance over prior approaches in performance, sampling efficiency, and training efficiency.

Conclusion: AttnRL provides an effective PSRL framework that enables efficient exploration for reasoning models through attention-guided branching and adaptive sampling strategies.

Abstract: Reinforcement Learning (RL) has shown remarkable success in enhancing the
reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL
(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.
However, existing PSRL approaches suffer from limited exploration efficiency,
both in terms of branching positions and sampling. In this paper, we introduce
a novel PSRL framework (AttnRL), which enables efficient exploration for
reasoning models. Motivated by preliminary observations that steps exhibiting
high attention scores correlate with reasoning behaviors, we propose to branch
from positions with high values. Furthermore, we develop an adaptive sampling
strategy that accounts for problem difficulty and historical batch size,
ensuring that the whole training batch maintains non-zero advantage values. To
further improve sampling efficiency, we design a one-step off-policy training
pipeline for PSRL. Extensive experiments on multiple challenging mathematical
reasoning benchmarks demonstrate that our method consistently outperforms prior
approaches in terms of performance and sampling and training efficiency.

</details>


### [247] [Leveraging AI modelling for FDS with Simvue: monitor and optimise for more sustainable simulations](https://arxiv.org/abs/2509.26139)
*James Panayis,Matt Field,Vignesh Gopakumar,Andrew Lahiff,Kristian Zarebski,Aby Abraham,Jonathan L. Hodges*

Main category: cs.LG

TL;DR: A multi-pronged approach combining ML surrogate models, guided optimization, and a simulation management framework (Simvue) to dramatically improve fire simulation efficiency.


<details>
  <summary>Details</summary>
Motivation: High demand for fire simulations in both scale and quantity requires improved time and energy efficiency.

Method: 1) Custom ML surrogate model for heat propagation prediction; 2) Guided optimization using lightweight models to select simulations; 3) Simvue framework for simulation management and data reuse.

Result: ML surrogate predicts heat dynamics orders of magnitude faster than CFD; optimization reduces required simulations by 10x for locating dangerous fire locations; Simvue enables better simulation management and data reuse.

Conclusion: The integrated approach significantly reduces computational costs and improves efficiency in fire simulation workflows through ML acceleration, smart optimization, and systematic management.

Abstract: There is high demand on fire simulations, in both scale and quantity. We
present a multi-pronged approach to improving the time and energy required to
meet these demands. We show the ability of a custom machine learning surrogate
model to predict the dynamics of heat propagation orders of magnitude faster
than state-of-the-art CFD software for this application. We also demonstrate
how a guided optimisation procedure can decrease the number of simulations
required to meet an objective; using lightweight models to decide which
simulations to run, we see a tenfold reduction when locating the most dangerous
location for a fire to occur within a building based on the impact of smoke on
visibility. Finally we present a framework and product, Simvue, through which
we access these tools along with a host of automatic organisational and
tracking features which enables future reuse of data and more savings through
better management of simulations and combating redundancy.

</details>


### [248] [Alignment-Aware Decoding](https://arxiv.org/abs/2509.26169)
*Frédéric Berdoz,Luca A. Lanzendörfer,René Caky,Roger Wattenhofer*

Main category: cs.LG

TL;DR: AAD is an inference-time method that improves LLM alignment without additional training, outperforming baselines and enabling synthetic data generation in data-constrained settings.


<details>
  <summary>Details</summary>
Motivation: Alignment remains challenging for LLMs, and existing preference optimization methods typically require training-time or prompt-based interventions.

Method: Alignment-aware decoding (AAD) enhances model alignment directly during inference through implicit reward optimization, requiring no specialized training beyond standard DPO.

Result: AAD consistently outperforms strong baselines across diverse alignment benchmarks and model scales, and can generate high-quality synthetic data to improve alignment under standard decoding.

Conclusion: AAD provides an effective inference-time solution for LLM alignment that works well in data-constrained settings through synthetic data generation.

Abstract: Alignment of large language models remains a central challenge in natural
language processing. Preference optimization has emerged as a popular and
effective method for improving alignment, typically through training-time or
prompt-based interventions. In this paper, we introduce alignment-aware
decoding (AAD), a method to enhance model alignment directly at inference.
Theoretically, AAD can be interpreted as implicit reward optimization, yet it
requires no specialized training beyond the standard DPO setup. Empirically,
AAD consistently outperforms strong baselines across diverse alignment
benchmarks and model scales. Moreover, in data-constrained settings, AAD can
produce high-quality synthetic data to improve alignment under standard
decoding, providing a practical solution when labeled data is limited.

</details>


### [249] [Neighbor-aware informal settlement mapping with graph convolutional networks](https://arxiv.org/abs/2509.26171)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Christovam Barcellos,Nadine Dessay*

Main category: cs.LG

TL;DR: A graph-based framework using Graph Convolutional Networks (GCN) to map informal settlements by incorporating local geographical context, outperforming standard methods by 17 points in Kappa coefficient.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for mapping informal settlements treat spatial units independently, neglecting the relational structure of urban fabric, which limits their effectiveness.

Method: Proposed a graph-based framework where each spatial unit is embedded in a graph with adjacent neighbors, and a lightweight GCN is trained to classify whether the central cell belongs to an informal settlement.

Result: Outperformed standard baselines with a 17-point improvement in Kappa coefficient over individual cell classification, and showed superiority over simple feature concatenation of neighboring cells.

Conclusion: Graph-based modeling effectively encodes spatial structure for urban scene understanding, demonstrating significant benefits for mapping informal settlements in heterogeneous urban landscapes.

Abstract: Mapping informal settlements is crucial for addressing challenges related to
urban planning, public health, and infrastructure in rapidly growing cities.
Geospatial machine learning has emerged as a key tool for detecting and mapping
these areas from remote sensing data. However, existing approaches often treat
spatial units independently, neglecting the relational structure of the urban
fabric. We propose a graph-based framework that explicitly incorporates local
geographical context into the classification process. Each spatial unit (cell)
is embedded in a graph structure along with its adjacent neighbors, and a
lightweight Graph Convolutional Network (GCN) is trained to classify whether
the central cell belongs to an informal settlement. Experiments are conducted
on a case study in Rio de Janeiro using spatial cross-validation across five
distinct zones, ensuring robustness and generalizability across heterogeneous
urban landscapes. Our method outperforms standard baselines, improving Kappa
coefficient by 17 points over individual cell classification. We also show that
graph-based modeling surpasses simple feature concatenation of neighboring
cells, demonstrating the benefit of encoding spatial structure for urban scene
understanding.

</details>


### [250] [PDE Solvers Should Be Local: Fast, Stable Rollouts with Learned Local Stencils](https://arxiv.org/abs/2509.26186)
*Chun-Wun Cheng,Bin Dong,Carola-Bibiane Schönlieb,Angelica I Aviles-Rivero*

Main category: cs.LG

TL;DR: FINO is a finite-difference-inspired neural architecture for solving PDEs that enforces strict locality while maintaining multiscale power, achieving better accuracy and speed than global mixing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing neural operator models for PDEs rely on global mixing mechanisms that oversmooth sharp local dynamics and have high computational costs.

Method: FINO replaces fixed finite-difference stencil coefficients with learnable convolutional kernels and uses explicit learnable time-stepping. It features a Local Operator Block with differential stencil layer, gating mask, and linear fuse step to create adaptive derivative-like local features.

Result: FINO achieves up to 44% lower error and 2× speedups over state-of-the-art operator-learning baselines across six benchmarks and a climate modeling task.

Conclusion: Strict locality with learnable time-stepping provides an accurate and scalable foundation for neural PDE solvers, balancing local structure preservation with computational efficiency.

Abstract: Neural operator models for solving partial differential equations (PDEs)
often rely on global mixing mechanisms-such as spectral convolutions or
attention-which tend to oversmooth sharp local dynamics and introduce high
computational cost. We present FINO, a finite-difference-inspired neural
architecture that enforces strict locality while retaining multiscale
representational power. FINO replaces fixed finite-difference stencil
coefficients with learnable convolutional kernels and evolves states via an
explicit, learnable time-stepping scheme. A central Local Operator Block
leverage a differential stencil layer, a gating mask, and a linear fuse step to
construct adaptive derivative-like local features that propagate forward in
time. Embedded in an encoder-decoder with a bottleneck, FINO captures
fine-grained local structures while preserving interpretability. We establish
(i) a composition error bound linking one-step approximation error to stable
long-horizon rollouts under a Lipschitz condition, and (ii) a universal
approximation theorem for discrete time-stepped PDE dynamics. (iii) Across six
benchmarks and a climate modelling task, FINO achieves up to 44\% lower error
and up to around 2\times speedups over state-of-the-art operator-learning
baselines, demonstrating that strict locality with learnable time-stepping
yields an accurate and scalable foundation for neural PDE solvers.

</details>


### [251] [Optimizing Indoor Environmental Quality in Smart Buildings Using Deep Learning](https://arxiv.org/abs/2509.26187)
*Youssef Sabiri,Walid Houmaidi,Aaya Bougrine,Salmane El Mansour Billah*

Main category: cs.LG

TL;DR: This paper proposes a deep learning approach using LSTM, GRU, and CNN-LSTM architectures to forecast Indoor Environmental Quality parameters (CO2, temperature, humidity) while balancing energy efficiency in HVAC systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining optimal Indoor Environmental Quality (IEQ) for occupant health and productivity while minimizing the high energy costs associated with conventional HVAC systems.

Method: Benchmarked three deep learning architectures (LSTM, GRU, and hybrid CNN-LSTM) using the ROBOD dataset from a net-zero energy academic building to forecast IEQ variables across different time horizons.

Result: GRU achieved the best short-term prediction accuracy with lower computational overhead, CNN-LSTM excelled in extracting dominant features for extended forecasting, and LSTM offered robust long-range temporal modeling. Prediction reliability depends on data resolution, sensor placement, and occupancy conditions.

Conclusion: The findings provide actionable insights for intelligent Building Management Systems to implement predictive HVAC control, reducing energy consumption and enhancing occupant comfort in real-world building operations.

Abstract: Ensuring optimal Indoor Environmental Quality (IEQ) is vital for occupant
health and productivity, yet it often comes at a high energy cost in
conventional Heating, Ventilation, and Air Conditioning (HVAC) systems. This
paper proposes a deep learning driven approach to proactively manage IEQ
parameters specifically CO2 concentration, temperature, and humidity while
balancing building energy efficiency. Leveraging the ROBOD dataset collected
from a net-zero energy academic building, we benchmark three
architectures--Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and
a hybrid Convolutional Neural Network LSTM (CNN-LSTM)--to forecast IEQ
variables across various time horizons. Our results show that GRU achieves the
best short-term prediction accuracy with lower computational overhead, whereas
CNN-LSTM excels in extracting dominant features for extended forecasting
windows. Meanwhile, LSTM offers robust long-range temporal modeling. The
comparative analysis highlights that prediction reliability depends on data
resolution, sensor placement, and fluctuating occupancy conditions. These
findings provide actionable insights for intelligent Building Management
Systems (BMS) to implement predictive HVAC control, thereby reducing energy
consumption and enhancing occupant comfort in real-world building operations.

</details>


### [252] [Marginal Flow: a flexible and efficient framework for density estimation](https://arxiv.org/abs/2509.26221)
*Marcello Massimo Negri,Jonathan Aellen,Manuel Jahn,AmirEhsan Khorashadizadeh,Volker Roth*

Main category: cs.LG

TL;DR: Marginal Flow is a novel density modeling framework that overcomes limitations of existing approaches by marginalizing out latent parameters through a learnable distribution, enabling exact density evaluation, fast training/inference, and flexibility across various architectures and objectives.


<details>
  <summary>Details</summary>
Motivation: Current density modeling methods suffer from issues like expensive training, slow inference, approximate likelihoods, mode collapse, or architectural constraints like bijective mappings. The authors aim to create a framework that overcomes all these limitations simultaneously.

Method: The model defines q_θ(x) through a parametric distribution q(x|w) with latent parameters w. Instead of directly optimizing w, the method marginalizes them out by sampling w from a learnable distribution q_θ(w). This enables efficient density evaluation and sampling by only requiring draws from q_θ(w).

Result: Marginal Flow achieves exact density evaluation and is orders of magnitude faster than competing models in both training and inference. It demonstrates flexibility across various tasks including synthetic datasets, simulation-based inference, distributions on positive definite matrices, and manifold learning in image latent spaces.

Conclusion: Marginal Flow provides a simple yet powerful framework that overcomes multiple limitations of current density modeling approaches, offering exact density evaluation, computational efficiency, architectural flexibility, and strong performance across diverse applications.

Abstract: Current density modeling approaches suffer from at least one of the following
shortcomings: expensive training, slow inference, approximate likelihood, mode
collapse or architectural constraints like bijective mappings. We propose a
simple yet powerful framework that overcomes these limitations altogether. We
define our model $q_\theta(x)$ through a parametric distribution $q(x|w)$ with
latent parameters $w$. Instead of directly optimizing the latent variables $w$,
our idea is to marginalize them out by sampling $w$ from a learnable
distribution $q_\theta(w)$, hence the name Marginal Flow. In order to evaluate
the learned density $q_\theta(x)$ or to sample from it, we only need to draw
samples from $q_\theta(w)$, which makes both operations efficient. The proposed
model allows for exact density evaluation and is orders of magnitude faster
than competing models both at training and inference. Furthermore, Marginal
Flow is a flexible framework: it does not impose any restrictions on the neural
network architecture, it enables learning distributions on lower-dimensional
manifolds (either known or to be learned), it can be trained efficiently with
any objective (e.g. forward and reverse KL divergence), and it easily handles
multi-modal targets. We evaluate Marginal Flow extensively on various tasks
including synthetic datasets, simulation-based inference, distributions on
positive definite matrices and manifold learning in latent spaces of images.

</details>


### [253] [Machine Learning Detection of Lithium Plating in Lithium-ion Cells: A Gaussian Process Approach](https://arxiv.org/abs/2509.26234)
*Ayush Patnaik,Adam B Zufall,Stephen K Robinson,Xinfan Lin*

Main category: cs.LG

TL;DR: A Gaussian Process framework is proposed for detecting lithium plating in batteries by modeling charge-voltage relationships probabilistically, enabling robust dQ/dV peak detection without noise amplification from conventional finite differencing methods.


<details>
  <summary>Details</summary>
Motivation: Lithium plating during fast charging accelerates battery degradation and poses safety risks. Conventional dQ/dV computation methods amplify sensor noise and introduce bias in peak detection, limiting reliable plating onset identification.

Method: The paper develops a Gaussian Process framework that directly models Q(V) as a stochastic process. By leveraging GP properties where derivatives remain GPs, it analytically infers dQ/dV with calibrated uncertainty, eliminating the need for ad hoc smoothing.

Result: Experimental validation on Li-ion coin cells across various C-rates (0.2C-1C) and temperatures (0-40°C) shows the GP method reliably detects plating peaks under low-temperature, high-rate charging conditions, while correctly reporting no peaks in baseline cases.

Conclusion: The GP-based approach provides accurate lithium plating detection with uncertainty quantification, establishing a practical pathway for real-time monitoring in battery management systems, as confirmed by correlation with reduced charge throughput and capacity fade measurements.

Abstract: Lithium plating during fast charging is a critical degradation mechanism that
accelerates capacity fade and can trigger catastrophic safety failures. Recent
work has identified a distinctive dQ/dV peak above 4.0 V as a reliable
signature of plating onset; however, conventional methods for computing dQ/dV
rely on finite differencing with filtering, which amplifies sensor noise and
introduces bias in peak location. In this paper, we propose a Gaussian Process
(GP) framework for lithium plating detection by directly modeling the
charge-voltage relationship Q(V) as a stochastic process with calibrated
uncertainty. Leveraging the property that derivatives of GPs remain GPs, we
infer dQ/dV analytically and probabilistically from the posterior, enabling
robust detection without ad hoc smoothing. The framework provides three key
benefits: (i) noise-aware inference with hyperparameters learned from data,
(ii) closed-form derivatives with credible intervals for uncertainty
quantification, and (iii) scalability to online variants suitable for embedded
BMS. Experimental validation on Li-ion coin cells across a range of C-rates
(0.2C-1C) and temperatures (0-40\deg C) demonstrates that the GP-based method
reliably detects plating peaks under low-temperature, high-rate charging, while
correctly reporting no peaks in baseline cases. The concurrence of
GP-identified differential peaks, reduced charge throughput, and capacity fade
measured via reference performance tests confirms the method's accuracy and
robustness, establishing a practical pathway for real-time lithium plating
detection.

</details>


### [254] [Beyond Linear Probes: Dynamic Safety Monitoring for Language Models](https://arxiv.org/abs/2509.26238)
*James Oldfield,Philip Torr,Ioannis Patras,Adel Bibi,Fazl Barez*

Main category: cs.LG

TL;DR: TPCs are flexible safety monitors for LLMs that can dynamically adjust computational cost based on input difficulty, providing progressive polynomial evaluation for efficient harmful content detection.


<details>
  <summary>Details</summary>
Motivation: Traditional safety monitors waste resources on easy inputs while risking missing subtle cases. There's a need for flexible monitors where costs rise only when inputs are difficult to assess or when more compute is available.

Method: Introduce Truncated Polynomial Classifiers (TPCs) - an extension of linear probes that can be trained and evaluated progressively term-by-term. Allows early stopping for lightweight monitoring or using more terms for stronger guardrails.

Result: TPCs compete with or outperform MLP-based probe baselines on two large-scale safety datasets (WildGuardMix and BeaverTails) for 4 models up to 30B parameters, while being more interpretable than black-box counterparts.

Conclusion: TPCs provide flexible safety monitoring with two usage modes: as a safety dial for stronger guardrails when needed, and as an adaptive cascade to reduce monitoring costs by exiting early on clear cases.

Abstract: Monitoring large language models' (LLMs) activations is an effective way to
detect harmful requests before they lead to unsafe outputs. However,
traditional safety monitors often require the same amount of compute for every
query. This creates a trade-off: expensive monitors waste resources on easy
inputs, while cheap ones risk missing subtle cases. We argue that safety
monitors should be flexible--costs should rise only when inputs are difficult
to assess, or when more compute is available. To achieve this, we introduce
Truncated Polynomial Classifiers (TPCs), a natural extension of linear probes
for dynamic activation monitoring. Our key insight is that polynomials can be
trained and evaluated progressively, term-by-term. At test-time, one can
early-stop for lightweight monitoring, or use more terms for stronger
guardrails when needed. TPCs provide two modes of use. First, as a safety dial:
by evaluating more terms, developers and regulators can "buy" stronger
guardrails from the same model. Second, as an adaptive cascade: clear cases
exit early after low-order checks, and higher-order guardrails are evaluated
only for ambiguous inputs, reducing overall monitoring costs. On two
large-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with
up to 30B parameters, we show that TPCs compete with or outperform MLP-based
probe baselines of the same size, all the while being more interpretable than
their black-box counterparts. Our code is available at
http://github.com/james-oldfield/tpc.

</details>


### [255] [Sandbagging in a Simple Survival Bandit Problem](https://arxiv.org/abs/2509.26239)
*Joel Dyer,Daniel Jarne Ornia,Nicholas Bishop,Anisoara Calinescu,Michael Wooldridge*

Main category: cs.LG

TL;DR: Developed a statistical test to detect AI sandbagging (strategic deception) in safety evaluations using survival bandit framework.


<details>
  <summary>Details</summary>
Motivation: AI systems may hide dangerous capabilities during safety evaluations to avoid being deactivated or retrained, undermining evaluation integrity.

Method: Created a simple model of strategic deception in sequential decision-making tasks based on survival bandit framework, with theoretical analysis and statistical test construction.

Result: Theoretical demonstration that optimal rational agents exhibit sandbagging behavior, and simulation experiments show test reliability in distinguishing sandbagging from incompetence.

Conclusion: Establishes a potential approach for developing robust statistical procedures for frontier AI model evaluations to detect strategic deception.

Abstract: Evaluating the safety of frontier AI systems is an increasingly important
concern, helping to measure the capabilities of such models and identify risks
before deployment. However, it has been recognised that if AI agents are aware
that they are being evaluated, such agents may deliberately hide dangerous
capabilities or intentionally demonstrate suboptimal performance in
safety-related tasks in order to be released and to avoid being deactivated or
retrained. Such strategic deception - often known as "sandbagging" - threatens
to undermine the integrity of safety evaluations. For this reason, it is of
value to identify methods that enable us to distinguish behavioural patterns
that demonstrate a true lack of capability from behavioural patterns that are
consistent with sandbagging. In this paper, we develop a simple model of
strategic deception in sequential decision-making tasks, inspired by the
recently developed survival bandit framework. We demonstrate theoretically that
this problem induces sandbagging behaviour in optimal rational agents, and
construct a statistical test to distinguish between sandbagging and
incompetence from sequences of test scores. In simulation experiments, we
investigate the reliability of this test in allowing us to distinguish between
such behaviours in bandit models. This work aims to establish a potential
avenue for developing robust statistical procedures for use in the science of
frontier model evaluations.

</details>


### [256] [From Fragile to Certified: Wasserstein Audits of Group Fairness Under Distribution Shift](https://arxiv.org/abs/2509.26241)
*Ahmad-Reza Ehyaei,Golnoosh Farnadi,Samira Samadi*

Main category: cs.LG

TL;DR: Proposes Wasserstein distributionally robust framework for certifying worst-case group fairness under distribution shift, with tractable estimator DRUNE and theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Group-fairness metrics are brittle under distribution shift and vary sharply across resamples, undermining reliable fairness audits.

Method: Uses Wasserstein distributionally robust framework to certify worst-case group fairness over plausible test distributions, unifying common fairness notions via conditional-probability functional, with tractable reformulations and DRUNE estimator.

Result: Provides stable fairness assessments under distribution shift across benchmarks, with feasibility, consistency, finite-sample certification guarantees, and quantitative bounds under smoothness/margin conditions.

Conclusion: ε-WDF delivers principled basis for auditing and certifying group fairness beyond observational data, addressing brittleness of traditional fairness metrics under distribution shift.

Abstract: Group-fairness metrics (e.g., equalized odds) can vary sharply across
resamples and are especially brittle under distribution shift, undermining
reliable audits. We propose a Wasserstein distributionally robust framework
that certifies worst-case group fairness over a ball of plausible test
distributions centered at the empirical law. Our formulation unifies common
group fairness notions via a generic conditional-probability functional and
defines $\varepsilon$-Wasserstein Distributional Fairness ($\varepsilon$-WDF)
as the audit target. Leveraging strong duality, we derive tractable
reformulations and an efficient estimator (DRUNE) for $\varepsilon$-WDF. We
prove feasibility and consistency and establish finite-sample certification
guarantees for auditing fairness, along with quantitative bounds under
smoothness and margin conditions. Across standard benchmarks and classifiers,
$\varepsilon$-WDF delivers stable fairness assessments under distribution
shift, providing a principled basis for auditing and certifying group fairness
beyond observational data.

</details>


### [257] [Wasserstein Distributionally Robust Optimization Through the Lens of Structural Causal Models and Individual Fairness](https://arxiv.org/abs/2509.26275)
*Ahmad-Reza Ehyaei,Golnoosh Farnadi,Samira Samadi*

Main category: cs.LG

TL;DR: This paper applies Wasserstein Distributionally Robust Optimization (DRO) to address individual fairness concerns in causal learning problems, providing dual formulations, regularizer approximations, and finite sample error bounds.


<details>
  <summary>Details</summary>
Motivation: Limited research has explored DRO for individual fairness with causal structures and sensitive attributes, creating a gap in robust and fair data-driven decision-making.

Method: Formulate DRO from causality and fairness perspectives, develop dual formulations, characterize worst-case loss as regularizer, estimate regularizer in general cases, and provide finite sample error bounds with empirical distributions.

Result: The paper converts DRO problems into tractable forms, eliminates the max-step through regularizer approximation, and establishes relationships between DRO and classical robust optimization.

Conclusion: The proposed framework enables efficient and robust learning for individual fairness in causal settings, with theoretical guarantees even when structural causal models are unknown.

Abstract: In recent years, Wasserstein Distributionally Robust Optimization (DRO) has
garnered substantial interest for its efficacy in data-driven decision-making
under distributional uncertainty. However, limited research has explored the
application of DRO to address individual fairness concerns, particularly when
considering causal structures and sensitive attributes in learning problems. To
address this gap, we first formulate the DRO problem from causality and
individual fairness perspectives. We then present the DRO dual formulation as
an efficient tool to convert the DRO problem into a more tractable and
computationally efficient form. Next, we characterize the closed form of the
approximate worst-case loss quantity as a regularizer, eliminating the max-step
in the min-max DRO problem. We further estimate the regularizer in more general
cases and explore the relationship between DRO and classical robust
optimization. Finally, by removing the assumption of a known structural causal
model, we provide finite sample error bounds when designing DRO with empirical
distributions and estimated causal structures to ensure efficiency and robust
learning.

</details>


### [258] [Reframing Generative Models for Physical Systems using Stochastic Interpolants](https://arxiv.org/abs/2509.26282)
*Anthony Zhou,Alexander Wikner,Amaury Lancelin,Pedram Hassanzadeh,Amir Barati Farimani*

Main category: cs.LG

TL;DR: Stochastic interpolants outperform Gaussian denoising methods for physical system prediction by learning direct stochastic processes between states, enabling faster and more accurate autoregressive predictions.


<details>
  <summary>Details</summary>
Motivation: Current generative models using Gaussian denoising may not be optimal for physical system prediction tasks like PDEs and climate modeling, where successive states are closely related.

Method: Use stochastic interpolants that directly learn stochastic processes between current and future states, leveraging the proximity of successive physical distributions.

Result: Stochastic interpolants achieve more accurate predictions with fewer sampling steps compared to Gaussian noise transport methods, while balancing deterministic accuracy, spectral consistency, and probabilistic calibration.

Conclusion: Stochastic interpolants establish a competitive baseline for physical emulation and provide insights into different generative modeling frameworks' capabilities for physical system prediction.

Abstract: Generative models have recently emerged as powerful surrogates for physical
systems, demonstrating increased accuracy, stability, and/or statistical
fidelity. Most approaches rely on iteratively denoising a Gaussian, a choice
that may not be the most effective for autoregressive prediction tasks in PDEs
and dynamical systems such as climate. In this work, we benchmark generative
models across diverse physical domains and tasks, and highlight the role of
stochastic interpolants. By directly learning a stochastic process between
current and future states, stochastic interpolants can leverage the proximity
of successive physical distributions. This allows for generative models that
can use fewer sampling steps and produce more accurate predictions than models
relying on transporting Gaussian noise. Our experiments suggest that generative
models need to balance deterministic accuracy, spectral consistency, and
probabilistic calibration, and that stochastic interpolants can potentially
fulfill these requirements by adjusting their sampling. This study establishes
stochastic interpolants as a competitive baseline for physical emulation and
gives insight into the abilities of different generative modeling frameworks.

</details>


### [259] [Noise-Guided Transport for Imitation Learning](https://arxiv.org/abs/2509.26294)
*Lionel Blondé,Joao A. Candido Ramos,Alexandros Kalousis*

Main category: cs.LG

TL;DR: NGT is a lightweight imitation learning method that frames imitation as optimal transport problem, achieving strong performance with very limited data (as few as 20 transitions) without requiring pretraining or complex architectures.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of imitation learning in low-data regimes where only limited expert demonstrations are available, and existing methods relying on large-scale pretraining or high-capacity architectures are impractical.

Method: Noise-Guided Transport (NGT) - casts imitation as optimal transport problem solved via adversarial training, incorporates uncertainty estimation by design, requires no pretraining or specialized architectures.

Result: Achieves strong performance on challenging continuous control tasks including high-dimensional Humanoid tasks under ultra-low data regimes with as few as 20 transitions.

Conclusion: NGT provides an efficient, easy-to-implement solution for imitation learning in data-scarce scenarios, demonstrating that simple methods can be highly effective when designed appropriately.

Abstract: We consider imitation learning in the low-data regime, where only a limited
number of expert demonstrations are available. In this setting, methods that
rely on large-scale pretraining or high-capacity architectures can be difficult
to apply, and efficiency with respect to demonstration data becomes critical.
We introduce Noise-Guided Transport (NGT), a lightweight off-policy method that
casts imitation as an optimal transport problem solved via adversarial
training. NGT requires no pretraining or specialized architectures,
incorporates uncertainty estimation by design, and is easy to implement and
tune. Despite its simplicity, NGT achieves strong performance on challenging
continuous control tasks, including high-dimensional Humanoid tasks, under
ultra-low data regimes with as few as 20 transitions. Code is publicly
available at: https://github.com/lionelblonde/ngt-pytorch.

</details>


### [260] [Tuning the Tuner: Introducing Hyperparameter Optimization for Auto-Tuning](https://arxiv.org/abs/2509.26300)
*Floris-Jan Willemsen,Rob V. van Nieuwpoort,Ben van Werkhoven*

Main category: cs.LG

TL;DR: This paper introduces a method for tuning hyperparameters of optimization algorithms in auto-tuning frameworks, showing significant performance improvements through systematic hyperparameter optimization.


<details>
  <summary>Details</summary>
Motivation: Auto-tuning frameworks rarely tune their optimization algorithm hyperparameters, despite their critical impact on performance, creating an unexplored opportunity for performance enhancement.

Method: Proposes a statistical method for evaluating hyperparameter performance across search spaces, includes a FAIR dataset and software for reproducibility, and introduces a simulation mode that reduces tuning costs by 100x.

Result: Limited hyperparameter tuning improved auto-tuner performance by 94.8% on average, while meta-strategies achieved 204.7% average improvement.

Conclusion: Hyperparameter tuning is a powerful but overlooked technique that can significantly advance auto-tuning research and practice.

Abstract: Automatic performance tuning (auto-tuning) is widely used to optimize
performance-critical applications across many scientific domains by finding the
best program variant among many choices. Efficient optimization algorithms are
crucial for navigating the vast and complex search spaces in auto-tuning. As is
well known in the context of machine learning and similar fields,
hyperparameters critically shape optimization algorithm efficiency. Yet for
auto-tuning frameworks, these hyperparameters are almost never tuned, and their
potential performance impact has not been studied.
  We present a novel method for general hyperparameter tuning of optimization
algorithms for auto-tuning, thus "tuning the tuner". In particular, we propose
a robust statistical method for evaluating hyperparameter performance across
search spaces, publish a FAIR data set and software for reproducibility, and
present a simulation mode that replays previously recorded tuning data,
lowering the costs of hyperparameter tuning by two orders of magnitude. We show
that even limited hyperparameter tuning can improve auto-tuner performance by
94.8% on average, and establish that the hyperparameters themselves can be
optimized efficiently with meta-strategies (with an average improvement of
204.7%), demonstrating the often overlooked hyperparameter tuning as a powerful
technique for advancing auto-tuning research and practice.

</details>


### [261] [NeuroTTT: Bridging Pretraining-Downstream Task Misalignment in EEG Foundation Models via Test-Time Training](https://arxiv.org/abs/2509.26301)
*Suli Wang,Yangshen Deng,Zhenghua Bao,Xinyu Zhan,Yiqun Duan*

Main category: cs.LG

TL;DR: This paper introduces a two-stage alignment strategy for EEG foundation models that combines domain-specific self-supervised fine-tuning (NeuroTTT) with test-time training to address misalignment between pretraining and downstream tasks, and cross-subject distribution shifts in brain-computer interface applications.


<details>
  <summary>Details</summary>
Motivation: Large-scale EEG foundation models suffer from misalignment between pretraining objectives and downstream tasks, as well as significant cross-subject distribution shifts, which limits their effectiveness in brain-computer interface applications.

Method: A two-stage alignment strategy: (1) NeuroTTT - domain-specific self-supervised fine-tuning that aligns latent representations to spectral, spatial, and temporal EEG features without labeled data; (2) Test-time training with self-supervised training on individual test samples and prediction entropy minimization (Tent) that updates normalization statistics for real-time calibration.

Result: The method achieves state-of-the-art performance on three diverse BCI tasks (imagined speech, stress detection, motor imagery), substantially improving robustness and accuracy compared to conventional fine-tuning and adaptation methods, using CBraMod and LaBraM as backbones.

Conclusion: The proposed two-stage alignment strategy that unifies domain-tuned self-supervision with test-time training effectively bridges the gap between generic pretraining and specific EEG decoding tasks, enabling more robust and accurate brain-computer interface applications.

Abstract: Large-scale foundation models for EEG signals offer a promising path to
generalizable brain-computer interface (BCI) applications, but they often
suffer from misalignment between pretraining objectives and downstream tasks,
as well as significant cross-subject distribution shifts. This paper addresses
these challenges by introducing a two-stage alignment strategy that bridges the
gap between generic pretraining and specific EEG decoding tasks. First, we
propose NeuroTTT: a domain-specific self-supervised fine-tuning paradigm that
augments the foundation model with task-relevant self-supervised objectives,
aligning latent representations to important spectral, spatial, and temporal
EEG features without requiring additional labeled data. Second, we incorporate
test-time training (TTT) at inference, we perform (i) self-supervised test-time
training on individual unlabeled test samples and (ii) prediction entropy
minimization (Tent), which updates only normalization statistics to continually
calibrate the model to each new input on the fly. Our approach, which, to our
knowledge, is the first to unify domain-tuned self-supervision with test-time
training in large-scale EEG foundation models, yields substantially improved
robustness and accuracy across diverse BCI tasks (imagined speech, stress
detection, motor imagery). Using CBraMod and LaBraM as backbones, our method
pushes their performance to a markedly higher level. Results on three diverse
tasks demonstrate that the proposed alignment strategy achieves
state-of-the-art performance, outperforming conventional fine-tuning and
adaptation methods. Our code is available at
https://github.com/wsl2000/NeuroTTT.

</details>


### [262] [Attribution-Guided Decoding](https://arxiv.org/abs/2509.26307)
*Piotr Komorowski,Elena Golimblevskaia,Reduan Achtibat,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: AGD is an interpretability-based decoding method that selects tokens with highest attribution to user-defined regions of interest, improving instruction following and factual accuracy while reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Standard decoding methods often fail to robustly satisfy complex instruction following and factual accuracy requirements, while existing control techniques frequently degrade general output quality.

Method: AGD considers high-probability output token candidates and selects the one with highest attribution to user-defined ROI, which can be flexibly defined over different parts of input or internal components. An adaptive entropy-based variant applies guidance only when model is uncertain.

Result: Significantly improves instruction adherence (Llama 3.1 success rate from 66.0% to 79.1%), reduces hallucinations, improves factual accuracy in both closed-book and open-book settings, while mitigating quality degradation and computational overhead.

Conclusion: AGD presents a versatile, interpretable, and effective method for enhancing reliability of modern LLMs across instruction following and knowledge-intensive tasks.

Abstract: The capacity of Large Language Models (LLMs) to follow complex instructions
and generate factually accurate text is critical for their real-world
application. However, standard decoding methods often fail to robustly satisfy
these requirements, while existing control techniques frequently degrade
general output quality. In this work, we introduce Attribution-Guided Decoding
(AGD), an interpretability-based decoding strategy. Instead of directly
manipulating model activations, AGD considers a set of high-probability output
token candidates and selects the one that exhibits the highest attribution to a
user-defined Region of Interest (ROI). This ROI can be flexibly defined over
different parts of the model's input or internal components, allowing AGD to
steer generation towards various desirable behaviors. We demonstrate AGD's
efficacy across three challenging domains. For instruction following, we show
that AGD significantly boosts adherence (e.g., improving the overall success
rate on Llama 3.1 from 66.0% to 79.1%). For knowledge-intensive tasks, we show
that guiding generation towards usage of internal knowledge components or
contextual sources can reduce hallucinations and improve factual accuracy in
both closed-book and open-book settings. Furthermore, we propose an adaptive,
entropy-based variant of AGD that mitigates quality degradation and reduces
computational overhead by applying guidance only when the model is uncertain.
Our work presents a versatile, more interpretable, and effective method for
enhancing the reliability of modern LLMs.

</details>


### [263] [A Review on Single-Problem Multi-Attempt Heuristic Optimization](https://arxiv.org/abs/2509.26321)
*Judith Echevarrieta,Etor Arza,Aritz Pérez,Josu Ceberio*

Main category: cs.LG

TL;DR: This paper reviews sequential selection strategies for single-problem multi-attempt heuristic optimization, unifying approaches from algorithm selection, parameter tuning, multi-start and resource allocation under a common framework.


<details>
  <summary>Details</summary>
Motivation: Practitioners often need to find the best solution for a single problem using multiple heuristic alternatives, but existing sequential selection strategies are scattered across different research topics without unified perspective.

Method: The authors conduct a focused review and develop a taxonomy that systematically organizes and classifies sequential alternative selection strategies using unified terminology within a common framework.

Result: The work brings together suitable strategies from algorithm selection, parameter tuning, multi-start and resource allocation, providing a comprehensive overview of methods for single-problem multi-attempt optimization.

Conclusion: This review fills a gap in the literature by providing a unified framework and taxonomy for sequential alternative selection in single-problem multi-attempt heuristic optimization, enabling better organization and classification of existing strategies.

Abstract: In certain real-world optimization scenarios, practitioners are not
interested in solving multiple problems but rather in finding the best solution
to a single, specific problem. When the computational budget is large relative
to the cost of evaluating a candidate solution, multiple heuristic alternatives
can be tried to solve the same given problem, each possibly with a different
algorithm, parameter configuration, initialization, or stopping criterion. The
sequential selection of which alternative to try next is crucial for
efficiently identifying the one that provides the best possible solution across
multiple attempts. Despite the relevance of this problem in practice, it has
not yet been the exclusive focus of any existing review. Several sequential
alternative selection strategies have been proposed in different research
topics, but they have not been comprehensively and systematically unified under
a common perspective.
  This work presents a focused review of single-problem multi-attempt heuristic
optimization. It brings together suitable strategies to this problem that have
been studied separately through algorithm selection, parameter tuning,
multi-start and resource allocation. These strategies are explained using a
unified terminology within a common framework, which supports the development
of a taxonomy for systematically organizing and classifying them.

</details>


### [264] [ACE: Adapting sampling for Counterfactual Explanations](https://arxiv.org/abs/2509.26322)
*Margarita A. Guerrero,Cristian R. Rojas*

Main category: cs.LG

TL;DR: ACE is a sample-efficient algorithm that uses Bayesian estimation and stochastic optimization to generate counterfactual explanations with fewer model queries than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing counterfactual explanation methods require many model evaluations, which is costly and impractical when model access is limited.

Method: Combines Bayesian estimation and stochastic optimization to approximate decision boundaries by prioritizing informative points, reducing the number of model queries needed.

Result: Extensive empirical results show ACE achieves superior evaluation efficiency compared to state-of-the-art methods while maintaining effectiveness.

Conclusion: ACE provides an efficient approach for generating accurate and feasible counterfactual explanations with minimal model evaluations.

Abstract: Counterfactual Explanations (CFEs) interpret machine learning models by
identifying the smallest change to input features needed to change the model's
prediction to a desired output. For classification tasks, CFEs determine how
close a given sample is to the decision boundary of a trained classifier.
Existing methods are often sample-inefficient, requiring numerous evaluations
of a black-box model -- an approach that is both costly and impractical when
access to the model is limited. We propose Adaptive sampling for Counterfactual
Explanations (ACE), a sample-efficient algorithm combining Bayesian estimation
and stochastic optimization to approximate the decision boundary with fewer
queries. By prioritizing informative points, ACE minimizes evaluations while
generating accurate and feasible CFEs. Extensive empirical results show that
ACE achieves superior evaluation efficiency compared to state-of-the-art
methods, while maintaining effectiveness in identifying minimal and actionable
changes.

</details>


### [265] [A Generalized Information Bottleneck Theory of Deep Learning](https://arxiv.org/abs/2509.26327)
*Charles Westphal,Stephen Hailes,Mirco Musolesi*

Main category: cs.LG

TL;DR: The paper introduces a Generalized Information Bottleneck (GIB) framework that reformulates the original IB principle using synergy concepts, addressing theoretical ambiguities and estimation challenges while achieving better generalization and interpretability.


<details>
  <summary>Details</summary>
Motivation: The original Information Bottleneck principle has theoretical ambiguities and practical estimation challenges that limit its utility for understanding neural network learning.

Method: Reformulate IB through synergy lens using average interaction information, creating a Generalized Information Bottleneck framework that upper bounds the original IB objective.

Result: GIB shows consistent compression phases across architectures (including ReLU where standard IB fails), yields interpretable dynamics in CNNs and Transformers, and aligns with adversarial robustness understanding.

Conclusion: The Generalized Information Bottleneck framework successfully addresses limitations of the original IB principle while maintaining theoretical compatibility and providing practical benefits for neural network analysis.

Abstract: The Information Bottleneck (IB) principle offers a compelling theoretical
framework to understand how neural networks (NNs) learn. However, its practical
utility has been constrained by unresolved theoretical ambiguities and
significant challenges in accurate estimation. In this paper, we present a
\textit{Generalized Information Bottleneck (GIB)} framework that reformulates
the original IB principle through the lens of synergy, i.e., the information
obtainable only through joint processing of features. We provide theoretical
and empirical evidence demonstrating that synergistic functions achieve
superior generalization compared to their non-synergistic counterparts.
Building on these foundations we re-formulate the IB using a computable
definition of synergy based on the average interaction information (II) of each
feature with those remaining. We demonstrate that the original IB objective is
upper bounded by our GIB in the case of perfect estimation, ensuring
compatibility with existing IB theory while addressing its limitations. Our
experimental results demonstrate that GIB consistently exhibits compression
phases across a wide range of architectures (including those with \textit{ReLU}
activations where the standard IB fails), while yielding interpretable dynamics
in both CNNs and Transformers and aligning more closely with our understanding
of adversarial robustness.

</details>


### [266] [FedMuon: Federated Learning with Bias-corrected LMO-based Optimization](https://arxiv.org/abs/2509.26337)
*Yuki Takezawa,Anastasia Koloskova,Xiaowen Jiang,Sebastian U. Stich*

Main category: cs.LG

TL;DR: FedMuon is a federated learning method that adapts the Muon optimization algorithm to overcome convergence issues caused by the biased linear minimization oracle (LMO) operator, achieving faster convergence than state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Muon optimization method shows promise for faster neural network training than Adam, but direct application in federated learning with FedAvg fails to converge due to the biased nature of LMO operator.

Method: Proposed FedMuon method that mitigates the convergence issues caused by LMO bias in federated settings, with analysis of approximate LMO solving using Newton-Schulz iterations.

Result: FedMuon converges for any number of Newton-Schulz iterations and achieves faster convergence with more accurate LMO solutions, outperforming state-of-the-art federated learning methods in experiments.

Conclusion: FedMuon successfully adapts Muon optimization to federated learning, overcoming LMO bias issues and demonstrating superior performance compared to existing federated learning approaches.

Abstract: Recently, a new optimization method based on the linear minimization oracle
(LMO), called Muon, has been attracting increasing attention since it can train
neural networks faster than existing adaptive optimization methods, such as
Adam. In this paper, we study how Muon can be utilized in federated learning.
We first show that straightforwardly using Muon as the local optimizer of
FedAvg does not converge to the stationary point since the LMO is a biased
operator. We then propose FedMuon which can mitigate this issue. We also
analyze how solving the LMO approximately affects the convergence rate and find
that, surprisingly, FedMuon can converge for any number of Newton-Schulz
iterations, while it can converge faster as we solve the LMO more accurately.
Through experiments, we demonstrated that FedMuon can outperform the
state-of-the-art federated learning methods.

</details>


### [267] [Memory-Driven Self-Improvement for Decision Making with Large Language Models](https://arxiv.org/abs/2509.26340)
*Xue Yan,Zijing Ou,Mengyue Yang,Yan Song,Haifeng Zhang,Yingzhen Li,Jun Wang*

Main category: cs.LG

TL;DR: A memory-driven self-improvement framework that combines LLM general knowledge with domain-specific experiences to enhance sequential decision-making performance.


<details>
  <summary>Details</summary>
Motivation: LLMs' broad general knowledge is insufficient for specific decision-making tasks with limited data, making efficient adaptation challenging.

Method: Proposes a framework where memory retains past interactions and Q-values, capturing decision-relevant knowledge to refine LLM priors, which then generate better trajectories to enrich memory.

Result: Significantly outperforms traditional RL and LLM-based baselines, improving performance by over 40% on in-distribution tasks and over 75% on unseen tasks in ALFWorld.

Conclusion: The memory-driven self-improvement framework effectively combines LLM general knowledge with domain-specific experiences, enabling mutual reinforcement between memory and LLM prior for enhanced sequential decision-making.

Abstract: Large language models (LLMs) have emerged as effective action policies for
sequential decision-making (SDM) tasks due to their extensive prior knowledge.
However, this broad yet general knowledge is often insufficient for specific
decision-making tasks with limited task-related data, making it challenging to
efficiently adapt LLMs to specific SDM tasks. To address this challenge, we
propose a memory-driven self-improvement framework that combines LLM general
prior knowledge with a compact memory of domain-specific experiences. Memory
retains past interactions and associated Q-values, thereby capturing
decision-relevant knowledge that facilitates accurate value estimation and
informs the LLM prior refinement. The refined LLM prior, in turn, generates
higher-reward trajectories that further enrich memory, forming a natural
self-improvement framework where memory and LLM prior mutually reinforce each
other. Experiments show that our memory-driven approach significantly
outperforms both traditional RL and LLM-based baselines, e.g., improving
performance by over 40\% on in-distribution tasks and over 75\% when
generalized to unseen tasks in ALFWorld.

</details>


### [268] [LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and MCI-Like Field Simulation](https://arxiv.org/abs/2509.26351)
*Joshua Sebastian,Karma Tobden,KMA Solaiman*

Main category: cs.LG

TL;DR: This paper introduces an open, LLM-assisted emergency triage benchmark for deterioration prediction using MIMIC-IV-ED data, addressing accessibility barriers in MCI triage research.


<details>
  <summary>Details</summary>
Motivation: Emergency and mass casualty incident triage research has been limited by the absence of openly usable, reproducible benchmarks, despite the critical need for rapid patient deterioration prediction to guide timely interventions.

Method: Created an LLM-assisted benchmark with two regimes: hospital-rich setting (vitals, labs, notes, chief complaints, structured observations) and MCI-like field simulation (limited to vitals, observations, notes). LLMs were used for data harmonization, feature prioritization, and schema alignment.

Result: Developed baseline models with SHAP-based interpretability analyses, revealing predictive gaps between hospital-rich and field simulation regimes and identifying critical triage features.

Conclusion: The contributions make triage prediction research more reproducible and accessible, advancing dataset democratization in clinical AI.

Abstract: Research on emergency and mass casualty incident (MCI) triage has been
limited by the absence of openly usable, reproducible benchmarks. Yet these
scenarios demand rapid identification of the patients most in need, where
accurate deterioration prediction can guide timely interventions. While the
MIMIC-IV-ED database is openly available to credentialed researchers,
transforming it into a triage-focused benchmark requires extensive
preprocessing, feature harmonization, and schema alignment -- barriers that
restrict accessibility to only highly technical users.
  We address these gaps by first introducing an open, LLM-assisted emergency
triage benchmark for deterioration prediction (ICU transfer, in-hospital
mortality). The benchmark then defines two regimes: (i) a hospital-rich setting
with vitals, labs, notes, chief complaints, and structured observations, and
(ii) an MCI-like field simulation limited to vitals, observations, and notes.
Large language models (LLMs) contributed directly to dataset construction by
(i) harmonizing noisy fields such as AVPU and breathing devices, (ii)
prioritizing clinically relevant vitals and labs, and (iii) guiding schema
alignment and efficient merging of disparate tables.
  We further provide baseline models and SHAP-based interpretability analyses,
illustrating predictive gaps between regimes and the features most critical for
triage. Together, these contributions make triage prediction research more
reproducible and accessible -- a step toward dataset democratization in
clinical AI.

</details>


### [269] [Data-to-Energy Stochastic Dynamics](https://arxiv.org/abs/2509.26364)
*Kirill Tamogashev,Nikolay Malkin*

Main category: cs.LG

TL;DR: This paper proposes the first general method for modeling Schrödinger bridges when one or both distributions are given by unnormalized densities without access to data samples, using a data-free iterative proportional fitting procedure inspired by off-policy reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing Schrödinger bridge algorithms require samples from both distributions, limiting their applicability to cases where only unnormalized densities are available, such as in Bayesian inference and generative model latent spaces.

Method: The authors develop a data-to-energy iterative proportional fitting (IPF) procedure that generalizes the standard IPF to the data-free case, using a reinforcement learning formulation with fixed time discretization and learning the diffusion coefficient.

Result: The method successfully learns transports between multimodal distributions on synthetic problems and substantially improves existing data-to-data Schrödinger bridge algorithms. It also enables data-free image-to-image translation by sampling posterior distributions in generative model latent spaces.

Conclusion: The proposed data-to-energy IPF provides the first general solution for Schrödinger bridge problems with unnormalized densities, demonstrating effectiveness on multimodal distributions and practical applications in generative modeling.

Abstract: The Schr\"odinger bridge problem is concerned with finding a stochastic
dynamical system bridging two marginal distributions that minimises a certain
transportation cost. This problem, which represents a generalisation of optimal
transport to the stochastic case, has received attention due to its connections
to diffusion models and flow matching, as well as its applications in the
natural sciences. However, all existing algorithms allow to infer such dynamics
only for cases where samples from both distributions are available. In this
paper, we propose the first general method for modelling Schr\"odinger bridges
when one (or both) distributions are given by their unnormalised densities,
with no access to data samples. Our algorithm relies on a generalisation of the
iterative proportional fitting (IPF) procedure to the data-free case, inspired
by recent developments in off-policy reinforcement learning for training of
diffusion samplers. We demonstrate the efficacy of the proposed data-to-energy
IPF on synthetic problems, finding that it can successfully learn transports
between multimodal distributions. As a secondary consequence of our
reinforcement learning formulation, which assumes a fixed time discretisation
scheme for the dynamics, we find that existing data-to-data Schr\"odinger
bridge algorithms can be substantially improved by learning the diffusion
coefficient of the dynamics. Finally, we apply the newly developed algorithm to
the problem of sampling posterior distributions in latent spaces of generative
models, thus creating a data-free image-to-image translation method. Code:
https://github.com/mmacosha/d2e-stochastic-dynamics

</details>


### [270] [Refine Drugs, Don't Complete Them: Uniform-Source Discrete Flows for Fragment-Based Drug Discovery](https://arxiv.org/abs/2509.26405)
*Benno Kaech,Luis Wyss,Karsten Borgwardt,Gianvito Grasso*

Main category: cs.LG

TL;DR: InVirtuoGen is a discrete flow generative model for small molecule generation and optimization, achieving state-of-the-art performance in de novo generation, fragment-constrained tasks, and lead optimization.


<details>
  <summary>Details</summary>
Motivation: To develop a versatile generative foundation for drug discovery that can handle various tasks from early hit finding to multi-objective lead optimization, addressing limitations of previous fragment-based models.

Method: Uses discrete flow generative model for fragmented SMILES, learning to transform uniform source tokens into data distribution. Combines genetic algorithm with Proximal Property Optimization fine-tuning for property optimization.

Result: Achieves stronger quality-diversity pareto frontier than prior fragment-based models, sets new SOTA on Practical Molecular Optimization benchmark, and yields higher docking scores in lead optimization than previous baselines.

Conclusion: InVirtuoGen establishes a versatile generative foundation for drug discovery and contributes to open science by releasing pretrained models and code.

Abstract: We introduce InVirtuoGen, a discrete flow generative model for fragmented
SMILES for de novo and fragment-constrained generation, and
target-property/lead optimization of small molecules. The model learns to
transform a uniform source over all possible tokens into the data distribution.
Unlike masked models, its training loss accounts for predictions on all
sequence positions at every denoising step, shifting the generation paradigm
from completion to refinement, and decoupling the number of sampling steps from
the sequence length. For \textit{de novo} generation, InVirtuoGen achieves a
stronger quality-diversity pareto frontier than prior fragment-based models and
competitive performance on fragment-constrained tasks. For property and lead
optimization, we propose a hybrid scheme that combines a genetic algorithm with
a Proximal Property Optimization fine-tuning strategy adapted to discrete
flows. Our approach sets a new state-of-the-art on the Practical Molecular
Optimization benchmark, measured by top-10 AUC across tasks, and yields higher
docking scores in lead optimization than previous baselines. InVirtuoGen thus
establishes a versatile generative foundation for drug discovery, from early
hit finding to multi-objective lead optimization. We further contribute to open
science by releasing pretrained checkpoints and code, making our results fully
reproducible\footnote{https://github.com/invirtuolabs/InVirtuoGen_results}.

</details>


### [271] [Ascent Fails to Forget](https://arxiv.org/abs/2509.26427)
*Ioannis Mavrothalassitis,Pol Puigdemont,Noam Itzhak Levi,Volkan Cevher*

Main category: cs.LG

TL;DR: Gradient ascent-based unlearning methods often fail due to statistical dependence between forget and retain datasets, causing performance degradation and divergence from the retrained model.


<details>
  <summary>Details</summary>
Motivation: To challenge the misconception that forget and retain datasets can be independently manipulated during unlearning, and to investigate why gradient ascent methods frequently fail in practice.

Method: Empirical and theoretical analysis of gradient ascent-based unlearning, including logistic regression examples and experiments on complex neural networks.

Result: Statistical dependence between datasets causes gradient descent-ascent iterations to diverge from the retrained model, potentially making the unlearned model worse than the original.

Conclusion: Statistical dependencies, even simple correlations, are sufficient to cause ascent-based unlearning methods to fail, highlighting a fundamental limitation in current approaches.

Abstract: Contrary to common belief, we show that gradient ascent-based unconstrained
optimization methods frequently fail to perform machine unlearning, a
phenomenon we attribute to the inherent statistical dependence between the
forget and retain data sets. This dependence, which can manifest itself even as
simple correlations, undermines the misconception that these sets can be
independently manipulated during unlearning. We provide empirical and
theoretical evidence showing these methods often fail precisely due to this
overlooked relationship. For random forget sets, this dependence means that
degrading forget set metrics (which, for a retrained model, should mirror test
set metrics) inevitably harms overall test performance. Going beyond random
sets, we consider logistic regression as an instructive example where a
critical failure mode emerges: inter-set dependence causes gradient
descent-ascent iterations to progressively diverge from the ideal retrained
model. Strikingly, these methods can converge to solutions that are not only
far from the retrained ideal but are potentially even further from it than the
original model itself, rendering the unlearning process actively detrimental. A
toy example further illustrates how this dependence can trap models in inferior
local minima, inescapable via finetuning. Our findings highlight that the
presence of such statistical dependencies, even when manifest only as
correlations, can be sufficient for ascent-based unlearning to fail. Our
theoretical insights are corroborated by experiments on complex neural
networks, demonstrating that these methods do not perform as expected in
practice due to this unaddressed statistical interplay.

</details>


### [272] [AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size](https://arxiv.org/abs/2509.26432)
*Guanxi Lu,Hao,Chen,Yuto Karashima,Zhican Wang,Daichi Fujiki,Hongxiang Fan*

Main category: cs.LG

TL;DR: AdaBlock-dLLM is a training-free scheduler that adaptively adjusts block size during semi-autoregressive decoding in diffusion-based LLMs, addressing limitations of fixed block size approaches by aligning block boundaries with semantic structure, achieving up to 5.3% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: To overcome two fundamental limitations in conventional semi-autoregressive decoding with fixed block size: late decoding overhead (delayed unmasking of high-confidence tokens) and premature decoding error (early commitment of low-confidence tokens).

Method: Statistical analysis of confidence dynamics identifies volatility band regions that encode local semantic structure. AdaBlock-dLLM adaptively aligns block boundaries with semantic steps by adjusting block size during runtime without requiring training.

Result: Extensive experiments show AdaBlock-dLLM achieves up to 5.3% accuracy improvement under the same throughput budget compared to fixed block size approaches.

Conclusion: The semantics-aware adaptive scheduling approach and confidence-based analysis provide insights that could inspire future training strategies for diffusion-based LLMs, moving beyond just inference-time optimization.

Abstract: Diffusion-based large language models (dLLMs) are gaining attention for their
inherent capacity for parallel decoding, offering a compelling alternative to
autoregressive LLMs. Among various decoding strategies, blockwise
semi-autoregressive (semi-AR) approaches are widely adopted due to their
natural support for KV caching and their favorable accuracy-speed trade-off.
However, this paper identifies two fundamental limitations in the conventional
semi-AR decoding approach that applies a fixed block size: i) late decoding
overhead, where the unmasking of high-confidence tokens outside the current
block is unnecessarily delayed, and ii) premature decoding error, where
low-confidence tokens inside the current block are committed too early, leading
to incorrect tokens. This paper presents the first systematic investigation
challenging the fixed block size assumption in semi-AR decoding. Through a
statistical analysis of confidence dynamics during the denoising process, we
identify a volatility band (VB) region during dLLM decoding, which encodes
local semantic structure and can be used to guide adaptive block sizing.
Leveraging these insights, we introduce AdaBlock-dLLM, a training-free,
plug-and-play scheduler that adaptively aligns block boundaries with semantic
steps by adjusting block size during runtime. Extensive experiments across
diverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy
improvement under the same throughput budget. Beyond inference-time
optimization, we hope our semantics-aware adaptive scheduling approach and
confidence-based analysis will inspire future training strategies for dLLMs.

</details>


### [273] [ACT: Agentic Classification Tree](https://arxiv.org/abs/2509.26433)
*Vincent Grari,Tim Arni,Thibault Laugel,Sylvain Lamprier,James Zou,Marcin Detyniecki*

Main category: cs.LG

TL;DR: ACT extends decision trees to unstructured data by using natural-language questions for splits, achieving competitive performance with LLM prompting while providing transparent decision paths.


<details>
  <summary>Details</summary>
Motivation: AI systems need transparent and auditable decisions for high-stakes settings, but decision trees only work on structured data while LLMs lack interpretability.

Method: Formulate decision tree splits as natural-language questions, refined through impurity-based evaluation and LLM feedback via TextGrad.

Result: ACT matches or surpasses prompting-based baselines on text benchmarks while producing transparent decision paths.

Conclusion: ACT successfully bridges the gap between interpretable decision trees and LLM capabilities for unstructured data, providing trustworthy AI decisions.

Abstract: When used in high-stakes settings, AI systems are expected to produce
decisions that are transparent, interpretable, and auditable, a requirement
increasingly expected by regulations. Decision trees such as CART provide clear
and verifiable rules, but they are restricted to structured tabular data and
cannot operate directly on unstructured inputs such as text. In practice, large
language models (LLMs) are widely used for such data, yet prompting strategies
such as chain-of-thought or prompt optimization still rely on free-form
reasoning, limiting their ability to ensure trustworthy behaviors. We present
the Agentic Classification Tree (ACT), which extends decision-tree methodology
to unstructured inputs by formulating each split as a natural-language
question, refined through impurity-based evaluation and LLM feedback via
TextGrad. Experiments on text benchmarks show that ACT matches or surpasses
prompting-based baselines while producing transparent and interpretable
decision paths.

</details>


### [274] [Extensions of Robbins-Siegmund Theorem with Applications in Reinforcement Learning](https://arxiv.org/abs/2509.26442)
*Xinyu Liu,Zixuan Xie,Shangtong Zhang*

Main category: cs.LG

TL;DR: Extension of Robbins-Siegmund theorem for almost supermartingales where zero-order term is square summable instead of summable, enabling convergence analysis in RL applications where summable condition fails.


<details>
  <summary>Details</summary>
Motivation: Original Robbins-Siegmund theorem requires summable zero-order term, which cannot be met in many important reinforcement learning applications, limiting its applicability.

Method: Introduce novel mild assumption on increments of stochastic processes combined with square summable condition to enable almost sure convergence to bounded set.

Result: Achieve almost sure convergence to bounded set, provide almost sure convergence rates, high probability concentration bounds, and L^p convergence rates.

Conclusion: New extended theorem successfully applied to stochastic approximation and RL, obtaining first convergence guarantees for Q-learning with linear function approximation.

Abstract: The Robbins-Siegmund theorem establishes the convergence of stochastic
processes that are almost supermartingales and is foundational for analyzing a
wide range of stochastic iterative algorithms in stochastic approximation and
reinforcement learning (RL). However, its original form has a significant
limitation as it requires the zero-order term to be summable. In many important
RL applications, this summable condition, however, cannot be met. This
limitation motivates us to extend the Robbins-Siegmund theorem for almost
supermartingales where the zero-order term is not summable but only square
summable. Particularly, we introduce a novel and mild assumption on the
increments of the stochastic processes. This together with the square summable
condition enables an almost sure convergence to a bounded set. Additionally, we
further provide almost sure convergence rates, high probability concentration
bounds, and $L^p$ convergence rates. We then apply the new results in
stochastic approximation and RL. Notably, we obtain the first almost sure
convergence rate, the first high probability concentration bound, and the first
$L^p$ convergence rate for $Q$-learning with linear function approximation.

</details>


### [275] [fev-bench: A Realistic Benchmark for Time Series Forecasting](https://arxiv.org/abs/2509.26468)
*Oleksandr Shchur,Abdul Fatir Ansari,Caner Turkmen,Lorenzo Stella,Nick Erickson,Pablo Guerron,Michael Bohlke-Schneider,Yuyang Wang*

Main category: cs.LG

TL;DR: fev-bench is a comprehensive time series forecasting benchmark with 100 tasks across 7 domains, including 46 tasks with covariates, addressing limitations in existing benchmarks through principled aggregation methods and a lightweight Python library called fev.


<details>
  <summary>Details</summary>
Motivation: Existing time series forecasting benchmarks have narrow domain coverage, overlook important real-world settings like tasks with covariates, lack statistical rigor in aggregation procedures, and fail to provide consistent evaluation infrastructure.

Method: Proposed fev-bench benchmark with 100 forecasting tasks across 7 domains, including 46 tasks with covariates. Developed fev, a lightweight Python library for benchmarking that emphasizes reproducibility and integration with existing workflows, using bootstrapped confidence intervals for principled aggregation.

Result: The benchmark enables meaningful evaluation of forecasting models using win rates and skill scores with statistical confidence intervals. Results are reported for various pretrained, statistical and baseline models.

Conclusion: fev-bench addresses critical gaps in time series forecasting evaluation and identifies promising directions for future research through comprehensive benchmarking.

Abstract: Benchmark quality is critical for meaningful evaluation and sustained
progress in time series forecasting, particularly given the recent rise of
pretrained models. Existing benchmarks often have narrow domain coverage or
overlook important real-world settings, such as tasks with covariates.
Additionally, their aggregation procedures often lack statistical rigor, making
it unclear whether observed performance differences reflect true improvements
or random variation. Many benchmarks also fail to provide infrastructure for
consistent evaluation or are too rigid to integrate into existing pipelines. To
address these gaps, we propose fev-bench, a benchmark comprising 100
forecasting tasks across seven domains, including 46 tasks with covariates.
Supporting the benchmark, we introduce fev, a lightweight Python library for
benchmarking forecasting models that emphasizes reproducibility and seamless
integration with existing workflows. Usingfev, fev-bench employs principled
aggregation methods with bootstrapped confidence intervals to report model
performance along two complementary dimensions: win rates and skill scores. We
report results on fev-bench for various pretrained, statistical and baseline
models, and identify promising directions for future research.

</details>


### [276] [DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick](https://arxiv.org/abs/2509.26469)
*Mohammad Hassan Vali,Tom Bäckström,Arno Solin*

Main category: cs.LG

TL;DR: DiVeQ treats quantization as adding error vectors to mimic distortion, enabling gradient flow while keeping hard assignments. SF-DiVeQ extends this by assigning to curves between codewords for lower quantization error and full codebook usage.


<details>
  <summary>Details</summary>
Motivation: Vector quantization is widely used in deep models but its hard assignments block gradients and prevent end-to-end training, limiting its effectiveness.

Method: Proposed DiVeQ treats quantization as adding an error vector that mimics quantization distortion, maintaining hard forward pass while allowing gradient flow. Also introduced SF-DiVeQ that assigns to curves constructed by lines connecting codewords.

Result: Both methods improve reconstruction and sample quality over alternative quantization approaches in VQ-VAE compression and VQGAN generation across various datasets.

Conclusion: DiVeQ and SF-DiVeQ enable end-to-end training of vector quantization models without requiring auxiliary losses or temperature schedules, achieving better performance than existing methods.

Abstract: Vector quantization is common in deep models, yet its hard assignments block
gradients and hinder end-to-end training. We propose DiVeQ, which treats
quantization as adding an error vector that mimics the quantization distortion,
keeping the forward pass hard while letting gradients flow. We also present a
space-filling variant (SF-DiVeQ) that assigns to a curve constructed by the
lines connecting codewords, resulting in less quantization error and full
codebook usage. Both methods train end-to-end without requiring auxiliary
losses or temperature schedules. On VQ-VAE compression and VQGAN generation
across various data sets, they improve reconstruction and sample quality over
alternative quantization approaches.

</details>


### [277] [Equivariance by Local Canonicalization: A Matter of Representation](https://arxiv.org/abs/2509.26499)
*Gerrit Gerhartz,Peter Lippmann,Fred A. Hamprecht*

Main category: cs.LG

TL;DR: A framework to convert tensor field networks into local canonicalization paradigm for improved efficiency while maintaining equivariance, with systematic comparison of equivariant representations and implementation in tensor_frames package.


<details>
  <summary>Details</summary>
Motivation: Equivariant neural networks provide strong inductive biases for molecular and geometric data but suffer from computationally expensive tensor operations that limit their practical application.

Method: Developed a framework to transfer tensor field networks into local canonicalization paradigm, preserving equivariance while improving runtime. Systematically compared different equivariant representations in terms of complexity, runtime, and accuracy.

Result: The framework significantly improves runtime while maintaining equivariance. Published tensor_frames package for PyTorchGeometric that enables easy integration of equivariance into standard message passing neural networks.

Conclusion: The local canonicalization paradigm offers an efficient alternative to traditional tensor field networks, making equivariant neural networks more practical for real-world applications while preserving their theoretical benefits.

Abstract: Equivariant neural networks offer strong inductive biases for learning from
molecular and geometric data but often rely on specialized, computationally
expensive tensor operations. We present a framework to transfers existing
tensor field networks into the more efficient local canonicalization paradigm,
preserving equivariance while significantly improving the runtime. Within this
framework, we systematically compare different equivariant representations in
terms of theoretical complexity, empirical runtime, and predictive accuracy. We
publish the tensor_frames package, a PyTorchGeometric based implementation for
local canonicalization, that enables straightforward integration of
equivariance into any standard message passing neural network.

</details>


### [278] [Entropy After $\langle \texttt{/Think} \rangle$ for reasoning model early exiting](https://arxiv.org/abs/2509.26522)
*Xi Wang,James McInerney,Lequn Wang,Nathan Kallus*

Main category: cs.LG

TL;DR: Proposes EAT (Entropy After </Think>) to detect and stop overthinking in large reasoning models, reducing token usage by 13-21% without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models tend to overthink by continuing to revise answers even after reaching correct solutions, wasting computational tokens.

Method: Append stop thinking token (</think>) and monitor entropy of following tokens; use thresholding on variance under exponential moving average as stopping rule.

Result: Reduces token usage by 13-21% on MATH500 and AIME2025 without harming accuracy; effective even in black-box settings with proxy models.

Conclusion: EAT provides practical method for adaptive compute allocation, enabling efficient token usage while maintaining model performance.

Abstract: Large reasoning models show improved performance with longer chains of
thought. However, recent work has highlighted (qualitatively) their tendency to
overthink, continuing to revise answers even after reaching the correct
solution. We quantitatively confirm this inefficiency by tracking Pass@1 for
answers averaged over a large number of rollouts and find that the model often
begins to always produce the correct answer early in the reasoning, making
extra reasoning a waste of tokens. To detect and prevent overthinking, we
propose a simple and inexpensive novel signal -- Entropy After </Think> (EAT)
-- for monitoring and deciding whether to exit reasoning early. By appending a
stop thinking token (</think>) and monitoring the entropy of the following
token as the model reasons, we obtain a trajectory that decreases and
stabilizes when Pass@1 plateaus; thresholding its variance under an exponential
moving average yields a practical stopping rule. Importantly, our approach
enables adaptively allocating compute based on the EAT trajectory, allowing us
to spend compute in a more efficient way compared with fixing the token budget
for all questions. Empirically, on MATH500 and AIME2025, EAT reduces token
usage by 13 - 21% without harming accuracy, and it remains effective in black
box settings where logits from the reasoning model are not accessible, and EAT
is computed with proxy models.

</details>


### [279] [TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning](https://arxiv.org/abs/2509.26524)
*Seohyun Lee,Wenzhi Fang,Dong-Jun Han,Seyyedali Hosseinalipour,Christopher G. Brinton*

Main category: cs.LG

TL;DR: TAP is a two-stage adaptive personalization method for federated learning that addresses heterogeneity in data, tasks, and modalities by using mismatched architectures and post-FL knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: Current personalized federated learning approaches don't adequately address fine-tuning of foundation models with multi-task and multi-modal properties in heterogeneous settings across clients.

Method: Two-stage adaptive personalization: (i) leverages mismatched model architectures for selective replacement operations, (ii) uses post-FL knowledge distillation to capture general knowledge without compromising personalization.

Result: Demonstrated effectiveness across various datasets and tasks compared to multiple baselines, with convergence analysis showing server model performance degrades as modality-task pairs increase.

Conclusion: TAP provides an effective solution for personalized federated learning in multi-task, multi-modal heterogeneous settings, with theoretical convergence guarantees and empirical validation.

Abstract: Federated Learning (FL), despite demonstrating impressive capabilities in the
training of multiple models in a decentralized manner, has been shown to
produce a final model not necessarily well-suited to the needs of each client.
While extensive work has been conducted on how to create tailored personalized
models, called Personalized Federated Learning (PFL), less attention has been
given to personalization via fine-tuning of foundation models with multi-task
and multi-modal properties. Moreover, there exists a lack of understanding in
the literature on how to fine-tune and personalize such models in a setting
that is heterogeneous across clients not only in data, but also in tasks and
modalities. To address this gap in the literature, we propose TAP (Two-Stage
Adaptive Personalization), which (i) leverages mismatched model architectures
between the clients and server to selectively conduct replacement operations
when it benefits a client's local tasks and (ii) engages in post-FL knowledge
distillation for capturing beneficial general knowledge without compromising
personalization. We also introduce the first convergence analysis of the server
model under its modality-task pair architecture, and demonstrate that as the
number of modality-task pairs increases, its ability to cater to all tasks
suffers. Through extensive experiments, we demonstrate the effectiveness of our
proposed algorithm across a variety of datasets and tasks in comparison to a
multitude of baselines. Implementation code is publicly available at
https://github.com/lee3296/TAP.

</details>


### [280] [Machine-Learning Driven Load Shedding to Mitigate Instability Attacks in Power Grids](https://arxiv.org/abs/2509.26532)
*Justin Tackett,Benjamin Francis,Luis Garcia,David Grimsman,Sean Warnick*

Main category: cs.LG

TL;DR: A cost-effective machine learning approach to retrofit power grid load shedding systems for defending against instability cyberattacks, demonstrated on IEEE 14 Bus System using modified Prony analysis for detection.


<details>
  <summary>Details</summary>
Motivation: Critical infrastructure like power grids are increasingly complex and targeted by sophisticated cyberattacks, particularly instability attacks which have few existing protections.

Method: Data-driven supervised machine learning model to retrofit load shedding systems, using modified Prony analysis (MPA) for detecting instability attacks and triggering defenses.

Result: Proof of concept on IEEE 14 Bus System shows MPA is viable for detecting instability attacks and triggering defense mechanisms.

Conclusion: The proposed approach provides an effective defense mechanism against instability attacks in power grids through machine learning and modified Prony analysis.

Abstract: Every year critical infrastructure becomes more complex and we grow to rely
on it more and more. With this reliance, it becomes an attractive target for
cyberattacks from sophisticated actors, with one of the most attractive targets
being the power grid. One class of attacks, instability attacks, is a newer
type of attack that has relatively few protections developed. We present a cost
effective, data-driven approach to training a supervised machine learning model
to retrofit load shedding decision systems in power grids with the capacity to
defend against instability attacks. We show a proof of concept on the IEEE 14
Bus System using the Achilles Heel Technologies Power Grid Analyzer, and show
through an implementation of modified Prony analysis (MPA) that MPA is a viable
method for detecting instability attacks and triggering defense mechanisms.

</details>


### [281] [The Loss Kernel: A Geometric Probe for Deep Learning Interpretability](https://arxiv.org/abs/2509.26537)
*Maxwell Adam,Zach Furman,Jesse Hoogland*

Main category: cs.LG

TL;DR: The paper introduces the loss kernel, an interpretability method that measures data similarity using neural network loss covariance under parameter perturbations.


<details>
  <summary>Details</summary>
Motivation: To develop an interpretable method for understanding how neural networks perceive data similarity and attribution.

Method: Compute covariance matrix of per-sample losses under low-loss-preserving parameter perturbations, validated on synthetic multitask problem and applied to Inception-v1 on ImageNet.

Result: Method successfully separates inputs by task in synthetic data and reveals ImageNet structure aligned with WordNet semantic hierarchy.

Conclusion: The loss kernel is established as a practical tool for neural network interpretability and data attribution.

Abstract: We introduce the loss kernel, an interpretability method for measuring
similarity between data points according to a trained neural network. The
kernel is the covariance matrix of per-sample losses computed under a
distribution of low-loss-preserving parameter perturbations. We first validate
our method on a synthetic multitask problem, showing it separates inputs by
task as predicted by theory. We then apply this kernel to Inception-v1 to
visualize the structure of ImageNet, and we show that the kernel's structure
aligns with the WordNet semantic hierarchy. This establishes the loss kernel as
a practical tool for interpretability and data attribution.

</details>


### [282] [TASP: Topology-aware Sequence Parallelism](https://arxiv.org/abs/2509.26541)
*Yida Wang,Ke Hong,Xiuhong Li,Yuanchao Xu,Wenxun Wang,Guohao Dai,Yu Wang*

Main category: cs.LG

TL;DR: TASP is a topology-aware sequence parallelism method that improves communication efficiency for long-context LLMs by decomposing modern accelerator topologies into multiple orthogonal ring datapaths and decomposing Ring AllGather into concurrent ring-styled transfers.


<details>
  <summary>Details</summary>
Motivation: Current sequence parallelism methods like Ring Attention suffer from low communication efficiency due to mismatch between Ring AllGather communication primitive and AlltoAll topology of modern accelerators, limiting practical applicability for long-context LLMs.

Method: Proposes TASP which decomposes modern accelerator topology into multiple orthogonal ring datapaths that can transfer data concurrently without interference, and decomposes Ring AllGather primitive into same number of concurrent ring-styled data transfers.

Result: Experimental results on NVIDIA H100 and AMD MI300X systems show TASP achieves higher communication efficiency than Ring Attention and its variants, with up to 3.58× speedup over Ring Attention and Zigzag-Ring Attention.

Conclusion: TASP effectively addresses the communication inefficiency in sequence parallelism for long-context LLMs by leveraging topology decomposition and primitive decomposition to fully utilize modern accelerator communication capacity.

Abstract: Long-context large language models (LLMs) face constraints due to the
quadratic complexity of the self-attention mechanism. The mainstream sequence
parallelism (SP) method, Ring Attention, attempts to solve this by distributing
the query into multiple query chunks across accelerators and enable each Q
tensor to access all KV tensors from other accelerators via the Ring AllGather
communication primitive. However, it exhibits low communication efficiency,
restricting its practical applicability. This inefficiency stems from the
mismatch between the Ring AllGather communication primitive it adopts and the
AlltoAll topology of modern accelerators. A Ring AllGather primitive is
composed of iterations of ring-styled data transfer, which can only utilize a
very limited fraction of an AlltoAll topology.
  Inspired by the Hamiltonian decomposition of complete directed graphs, we
identify that modern accelerator topology can be decomposed into multiple
orthogonal ring datapaths which can concurrently transfer data without
interference. Based on this, we further observe that the Ring AllGather
primitive can also be decomposed into the same number of concurrent ring-styled
data transfer at every iteration. Based on these insights, we propose TASP, a
topology-aware SP method for long-context LLMs that fully utilizes the
communication capacity of modern accelerators via topology decomposition and
primitive decomposition. Experimental results on both single-node and
multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate
that TASP achieves higher communication efficiency than Ring Attention on these
modern accelerator topologies and achieves up to 3.58 speedup than Ring
Attention and its variant Zigzag-Ring Attention. The code is available at
https://github.com/infinigence/HamiltonAttention.

</details>


### [283] [Bayesian Influence Functions for Hessian-Free Data Attribution](https://arxiv.org/abs/2509.26544)
*Philipp Alexander Kreer,Wilson Wu,Maxwell Adam,Zach Furman,Jesse Hoogland*

Main category: cs.LG

TL;DR: Proposes local Bayesian influence function (BIF) to overcome classical influence functions' limitations in deep neural networks by using loss landscape statistics instead of Hessian inversion.


<details>
  <summary>Details</summary>
Motivation: Classical influence functions struggle with deep neural networks due to non-invertible Hessians and high-dimensional parameter spaces.

Method: Extends classical influence functions by replacing Hessian inversion with loss landscape statistics estimated via stochastic-gradient MCMC sampling.

Result: Achieves state-of-the-art results on predicting retraining experiments and scales efficiently to neural networks with billions of parameters.

Conclusion: BIF provides a Hessian-free approach that captures higher-order parameter interactions and addresses key limitations of classical influence functions for deep learning applications.

Abstract: Classical influence functions face significant challenges when applied to
deep neural networks, primarily due to non-invertible Hessians and
high-dimensional parameter spaces. We propose the local Bayesian influence
function (BIF), an extension of classical influence functions that replaces
Hessian inversion with loss landscape statistics that can be estimated via
stochastic-gradient MCMC sampling. This Hessian-free approach captures
higher-order interactions among parameters and scales efficiently to neural
networks with billions of parameters. We demonstrate state-of-the-art results
on predicting retraining experiments.

</details>


### [284] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2509.26564)
*Florian Grötschla,Longxiang Jiao,Luca A. Lanzendörfer,Roger Wattenhofer*

Main category: cs.LG

TL;DR: Panama is an active learning framework for training parametric guitar amp models using LSTM and WaveNet architectures, requiring only 75 datapoints to match the perceptual quality of leading non-parametric models.


<details>
  <summary>Details</summary>
Motivation: To create virtual guitar amps with minimal data collection by using active learning to identify the most informative amp knob settings.

Method: Combines LSTM and WaveNet-like architecture with ensemble-based active learning strategy using gradient-based optimization to maximize model disagreement and select informative datapoints.

Result: With only 75 datapoints, Panama models achieve perceptual quality equivalent to NAM (leading open-source non-parametric amp modeler) in MUSHRA listening tests.

Conclusion: Active learning with ensemble disagreement optimization enables efficient parametric guitar amp modeling with minimal data requirements while maintaining high perceptual quality.

Abstract: We introduce Panama, an active learning framework to train parametric guitar
amp models end-to-end using a combination of an LSTM model and a WaveNet-like
architecture. With \model, one can create a virtual amp by recording samples
that are determined through an ensemble-based active learning strategy to
minimize the amount of datapoints needed (i.e., amp knob settings). Our
strategy uses gradient-based optimization to maximize the disagreement among
ensemble models, in order to identify the most informative datapoints. MUSHRA
listening tests reveal that, with 75 datapoints, our models are able to match
the perceptual quality of NAM, the leading open-source non-parametric amp
modeler.

</details>


### [285] [Importance of localized dilatation and distensibility in identifying determinants of thoracic aortic aneurysm with neural operators](https://arxiv.org/abs/2509.26576)
*David S. Li,Somdatta Goswami,Qianying Cao,Vivek Oommen,Roland Assi,Jay D. Humphrey,George E. Karniadakis*

Main category: cs.LG

TL;DR: This paper develops a finite element framework to generate synthetic thoracic aortic aneurysms (TAAs) from various mechanical insults and uses neural networks to predict the initiating causes from spatial maps of dilatation and distensibility.


<details>
  <summary>Details</summary>
Motivation: There is a critical need to identify interacting factors that drive TAA progression, as different mechanical insults create distinct vulnerabilities in the aortic wall.

Method: Used finite element simulations to create synthetic TAAs from heterogeneous insults (elastic fiber damage and impaired mechanosensing), then trained neural networks (Deep Operator Networks, UNets, Laplace Neural Operators) on spatial maps of dilatation and distensibility to predict the initiating combined insult.

Result: UNet consistently provided the highest accuracy across all data formats. Prediction errors were significantly higher when trained on dilatation alone versus both dilatation and distensibility, showing the added value of mechanical data.

Conclusion: Full-field measurements of both dilatation and distensibility are crucial for TAA assessment to reveal mechanobiological drivers and support personalized treatment strategies.

Abstract: Thoracic aortic aneurysms (TAAs) arise from diverse mechanical and
mechanobiological disruptions to the aortic wall that increase the risk of
dissection or rupture. Evidence links TAA development to dysfunctions in the
aortic mechanotransduction axis, including loss of elastic fiber integrity and
cell-matrix connections. Because distinct insults create different mechanical
vulnerabilities, there is a critical need to identify interacting factors that
drive progression. Here, we use a finite element framework to generate
synthetic TAAs from hundreds of heterogeneous insults spanning varying degrees
of elastic fiber damage and impaired mechanosensing. From these simulations, we
construct spatial maps of localized dilatation and distensibility to train
neural networks that predict the initiating combined insult. We compare several
architectures (Deep Operator Networks, UNets, and Laplace Neural Operators) and
multiple input data formats to define a standard for future subject-specific
modeling. We also quantify predictive performance when networks are trained
using only geometric data (dilatation) versus both geometric and mechanical
data (dilatation plus distensibility). Across all networks, prediction errors
are significantly higher when trained on dilatation alone, underscoring the
added value of distensibility information. Among the tested models, UNet
consistently provides the highest accuracy across all data formats. These
findings highlight the importance of acquiring full-field measurements of both
dilatation and distensibility in TAA assessment to reveal the mechanobiological
drivers of disease and support the development of personalized treatment
strategies.

</details>


### [286] [Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning](https://arxiv.org/abs/2509.26578)
*Zheng Zhang,Ziwei Shan,Kaitao Song,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: The paper proposes Conditional Reward Modeling (CRM) to address limitations in existing Process Reward Models (PRMs) by conditioning step rewards on preceding steps and linking them to final outcomes, improving credit assignment and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing PRMs fail to capture inter-step dependencies and struggle to align process rewards with final outcomes, leading to ambiguous credit assignment and vulnerability to reward hacking.

Method: CRM frames LLM reasoning as a temporal process where each step's reward is conditioned on previous steps and explicitly linked to the final outcome, enforcing conditional probability rules to capture causal relationships.

Result: Experiments across Best-of-N sampling, beam search and reinforcement learning show CRM consistently outperforms existing reward models, being more robust to reward hacking and delivering stable improvements.

Conclusion: CRM provides a principled framework for enhancing LLM reasoning through better credit assignment and more reliable cross-sample comparison without relying on ground truth verifiable rewards.

Abstract: Process Reward Models (PRMs) have emerged as a promising approach to enhance
the reasoning capabilities of large language models (LLMs) by guiding their
step-by-step reasoning toward a final answer. However, existing PRMs either
treat each reasoning step in isolation, failing to capture inter-step
dependencies, or struggle to align process rewards with the final outcome.
Consequently, the reward signal fails to respect temporal causality in
sequential reasoning and faces ambiguous credit assignment. These limitations
make downstream models vulnerable to reward hacking and lead to suboptimal
performance. In this work, we propose Conditional Reward Modeling (CRM) that
frames LLM reasoning as a temporal process leading to a correct answer. The
reward of each reasoning step is not only conditioned on the preceding steps
but also explicitly linked to the final outcome of the reasoning trajectory. By
enforcing conditional probability rules, our design captures the causal
relationships among reasoning steps, with the link to the outcome allowing
precise attribution of each intermediate step, thereby resolving credit
assignment ambiguity. Further, through this consistent probabilistic modeling,
the rewards produced by CRM enable more reliable cross-sample comparison.
Experiments across Best-of-N sampling, beam search and reinforcement learning
demonstrate that CRM consistently outperforms existing reward models, offering
a principled framework for enhancing LLM reasoning. In particular, CRM is more
robust to reward hacking and delivers stable downstream improvements without
relying on verifiable rewards derived from ground truth.

</details>


### [287] [Uncertainty Quantification for Regression using Proper Scoring Rules](https://arxiv.org/abs/2509.26610)
*Alexander Fishkov,Kajetan Schweighofer,Mykyta Ielanskyi,Nikita Kotelevskii,Mohsen Guizani,Maxim Panov*

Main category: cs.LG

TL;DR: A unified uncertainty quantification framework for regression based on proper scoring rules, with closed-form expressions under parametric assumptions and ensemble estimation, decomposing uncertainty into aleatoric and epistemic components.


<details>
  <summary>Details</summary>
Motivation: While uncertainty quantification theory has advanced significantly for classification, extending these ideas to regression remains challenging, especially for reliable decision-making in safety-critical applications.

Method: Developed a unified UQ framework using proper scoring rules (CRPS, logarithmic, squared error, quadratic scores), derived closed-form expressions under parametric assumptions, and estimated them using ensembles of models.

Result: The framework naturally decomposes uncertainty into aleatoric and epistemic components, recovers popular regression UQ measures based on predictive variance and differential entropy, and provides guidance for selecting reliable UQ measures through evaluation on synthetic and real-world datasets.

Conclusion: The proposed unified framework successfully extends proper scoring rule-based UQ to regression, providing practical uncertainty measures with clear decomposition and reliable performance across various datasets.

Abstract: Quantifying uncertainty of machine learning model predictions is essential
for reliable decision-making, especially in safety-critical applications.
Recently, uncertainty quantification (UQ) theory has advanced significantly,
building on a firm basis of learning with proper scoring rules. However, these
advances were focused on classification, while extending these ideas to
regression remains challenging. In this work, we introduce a unified UQ
framework for regression based on proper scoring rules, such as CRPS,
logarithmic, squared error, and quadratic scores. We derive closed-form
expressions for the resulting uncertainty measures under practical parametric
assumptions and show how to estimate them using ensembles of models. In
particular, the derived uncertainty measures naturally decompose into aleatoric
and epistemic components. The framework recovers popular regression UQ measures
based on predictive variance and differential entropy. Our broad evaluation on
synthetic and real-world regression datasets provides guidance for selecting
reliable UQ measures.

</details>


### [288] [Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training](https://arxiv.org/abs/2509.26625)
*Junlin Han,Shengbang Tong,David Fan,Yufan Ren,Koustuv Sinha,Philip Torr,Filippos Kokkinos*

Main category: cs.LG

TL;DR: LLMs develop visual priors from text-only training, enabling vision tasks with minimal multimodal data. These priors consist of separable perception and reasoning components with different scaling trends and origins.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs acquire visual capabilities from text-only training and systematically analyze the composition and origins of visual priors for better MLLM development.

Method: Conducted over 100 controlled experiments using 500,000 GPU-hours across full MLLM pipeline, analyzing LLM pre-training, visual alignment, and multimodal fine-tuning across five model scales and various data mixtures.

Result: Visual reasoning ability develops from reasoning-centric data (code, math, academia) and scales progressively, while perception emerges diffusely from broad corpora. Text describing visual world is crucial but saturates quickly.

Conclusion: Provides data-centric recipe for pre-training vision-aware LLMs and introduces MLE-Bench, offering systematic approach to deliberately cultivate visual priors from language pre-training for next-generation MLLMs.

Abstract: Large Language Models (LLMs), despite being trained on text alone,
surprisingly develop rich visual priors. These priors allow latent visual
capabilities to be unlocked for vision tasks with a relatively small amount of
multimodal data, and in some cases, to perform visual tasks without ever having
seen an image. Through systematic analysis, we reveal that visual priors-the
implicit, emergent knowledge about the visual world acquired during language
pre-training-are composed of separable perception and reasoning priors with
unique scaling trends and origins. We show that an LLM's latent visual
reasoning ability is predominantly developed by pre-training on
reasoning-centric data (e.g., code, math, academia) and scales progressively.
This reasoning prior acquired from language pre-training is transferable and
universally applicable to visual reasoning. In contrast, a perception prior
emerges more diffusely from broad corpora, and perception ability is more
sensitive to the vision encoder and visual instruction tuning data. In
parallel, text describing the visual world proves crucial, though its
performance impact saturates rapidly. Leveraging these insights, we propose a
data-centric recipe for pre-training vision-aware LLMs and verify it in 1T
token scale pre-training. Our findings are grounded in over 100 controlled
experiments consuming 500,000 GPU-hours, spanning the full MLLM construction
pipeline-from LLM pre-training to visual alignment and supervised multimodal
fine-tuning-across five model scales, a wide range of data categories and
mixtures, and multiple adaptation setups. Along with our main findings, we
propose and investigate several hypotheses, and introduce the Multi-Level
Existence Bench (MLE-Bench). Together, this work provides a new way of
deliberately cultivating visual priors from language pre-training, paving the
way for the next generation of multimodal LLMs.

</details>


### [289] [Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models](https://arxiv.org/abs/2509.26626)
*Siddarth Venkatraman,Vineet Jain,Sarthak Mittal,Vedant Shah,Johan Obando-Ceron,Yoshua Bengio,Brian R. Bartoldson,Bhavya Kailkhura,Guillaume Lajoie,Glen Berseth,Nikolay Malkin,Moksh Jain*

Main category: cs.LG

TL;DR: RSA is a test-time scaling method that combines parallel and sequential scaling by recursively aggregating subsets of candidate reasoning chains to improve LLM performance with increasing compute budgets.


<details>
  <summary>Details</summary>
Motivation: To improve LLM capabilities by leveraging both parallel and sequential scaling approaches during inference, exploiting rich information in reasoning chains rather than just final answers.

Method: Recursive Self-Aggregation (RSA) refines populations of candidate reasoning chains through subset aggregation, enabling bootstrapping from partially correct intermediate steps across different chains of thought.

Result: RSA delivers substantial performance gains across diverse tasks, model families and sizes, enabling smaller models like Qwen3-4B-Instruct-2507 to compete with larger reasoning models while outperforming purely parallel and sequential scaling strategies.

Conclusion: RSA effectively combines the benefits of parallel and sequential test-time scaling, and training models with aggregation-aware reinforcement learning yields additional performance improvements.

Abstract: Test-time scaling methods improve the capabilities of large language models
(LLMs) by increasing the amount of compute used during inference to make a
prediction. Inference-time compute can be scaled in parallel by choosing among
multiple independent solutions or sequentially through self-refinement. We
propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired
by evolutionary methods that combines the benefits of both parallel and
sequential scaling. Each step of RSA refines a population of candidate
reasoning chains through aggregation of subsets to yield a population of
improved solutions, which are then used as the candidate pool for the next
iteration. RSA exploits the rich information embedded in the reasoning chains
-- not just the final answers -- and enables bootstrapping from partially
correct intermediate steps within different chains of thought. Empirically, RSA
delivers substantial performance gains with increasing compute budgets across
diverse tasks, model families and sizes. Notably, RSA enables
Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning
models, including DeepSeek-R1 and o3-mini (high), while outperforming purely
parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning
Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the
model to combine solutions via a novel aggregation-aware reinforcement learning
approach yields significant performance gains. Code available at
https://github.com/HyperPotatoNeo/RSA.

</details>


### [290] [AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond](https://arxiv.org/abs/2509.26636)
*Shangding Gu,Xiaohan Wang,Donghao Ying,Haoyu Zhao,Runing Yang,Ming Jin,Boyi Li,Marco Pavone,Serena Yeung-Levy,Jun Wang,Dawn Song,Costas Spanos*

Main category: cs.LG

TL;DR: AccidentBench is a large-scale multimodal benchmark with 2000 videos and 19000+ QA pairs that evaluates AI models on safety-critical scenarios in traffic, air, and water domains, focusing on temporal, spatial, and intent reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the need for rigorous evaluation of multimodal models in safety-critical, dynamic real-world settings that require understanding and reasoning about complex scenarios.

Method: Created a benchmark combining vehicle accident scenarios with Beyond domains (air and water) containing videos of varying lengths and difficulty levels, with human-annotated question-answer pairs systematically probing temporal, spatial, and intent reasoning capabilities.

Result: State-of-the-art models like Gemini-2.5 Pro and GPT-5 achieve only about 18% accuracy on the hardest tasks and longest videos, revealing substantial gaps in real-world temporal, spatial, and intent reasoning.

Conclusion: AccidentBench effectively exposes critical gaps in current multimodal models and provides a comprehensive, physically grounded testbed to drive development of safer, more robust models aligned with real-world safety-critical challenges.

Abstract: Rapid advances in multimodal models demand benchmarks that rigorously
evaluate understanding and reasoning in safety-critical, dynamic real-world
settings. We present AccidentBench, a large-scale benchmark that combines
vehicle accident scenarios with Beyond domains, safety-critical settings in air
and water that emphasize spatial and temporal reasoning (e.g., navigation,
orientation, multi-vehicle motion). The benchmark contains approximately 2000
videos and over 19000 human-annotated question--answer pairs spanning multiple
video lengths (short/medium/long) and difficulty levels (easy/medium/hard).
Tasks systematically probe core capabilities: temporal, spatial, and intent
understanding and reasoning. By unifying accident-centric traffic scenes with
broader safety-critical scenarios in air and water, AccidentBench offers a
comprehensive, physically grounded testbed for evaluating models under
real-world variability. Evaluations of state-of-the-art models (e.g.,
Gemini-2.5 Pro and GPT-5) show that even the strongest models achieve only
about 18% accuracy on the hardest tasks and longest videos, revealing
substantial gaps in real-world temporal, spatial, and intent reasoning.
AccidentBench is designed to expose these critical gaps and drive the
development of multimodal models that are safer, more robust, and better
aligned with real-world safety-critical challenges. The code and dataset are
available at: https://github.com/SafeRL-Lab/AccidentBench

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [291] [Blueprint-Bench: Comparing spatial intelligence of LLMs, agents and image models](https://arxiv.org/abs/2509.25229)
*Lukas Petersson,Axel Backlund,Axel Wennstöm,Hanna Petersson,Callum Sharrock,Arash Dabiri*

Main category: cs.AI

TL;DR: Blueprint-Bench is a benchmark for evaluating AI spatial reasoning by converting apartment photos to 2D floor plans, revealing current models perform poorly while humans excel.


<details>
  <summary>Details</summary>
Motivation: To assess genuine spatial intelligence in AI models through a task that requires inferring room layouts, understanding connectivity, and maintaining consistent scale from photographs.

Method: Evaluated leading language models, image generation models, and agent systems on 50 apartments with ~20 images each, using scoring based on room connectivity graphs and size rankings.

Result: Most AI models perform at or below random baseline, with image generation models struggling with instruction following and agent approaches showing no improvement over single-pass generation.

Conclusion: Blueprint-Bench provides the first numerical framework for comparing spatial intelligence across model architectures, revealing a significant blind spot in current AI capabilities.

Abstract: We introduce Blueprint-Bench, a benchmark designed to evaluate spatial
reasoning capabilities in AI models through the task of converting apartment
photographs into accurate 2D floor plans. While the input modality
(photographs) is well within the training distribution of modern multimodal
models, the task of spatial reconstruction requires genuine spatial
intelligence: inferring room layouts, understanding connectivity, and
maintaining consistent scale. We evaluate leading language models (GPT-5,
Claude 4 Opus, Gemini 2.5 Pro, Grok-4), image generation models (GPT-Image,
NanoBanana), and agent systems (Codex CLI, Claude Code) on a dataset of 50
apartments with approximately 20 interior images each. Our scoring algorithm
measures similarity between generated and ground-truth floor plans based on
room connectivity graphs and size rankings. Results reveal a significant blind
spot in current AI capabilities: most models perform at or below a random
baseline, while human performance remains substantially superior. Image
generation models particularly struggle with instruction following, while
agent-based approaches with iterative refinement capabilities show no
meaningful improvement over single-pass generation. Blueprint-Bench provides
the first numerical framework for comparing spatial intelligence across
different model architectures. We will continue evaluating new models as they
are released and welcome community submissions, monitoring for the emergence of
spatial intelligence in generalist AI systems.

</details>


### [292] [The Causal Abstraction Network: Theory and Learning](https://arxiv.org/abs/2509.25236)
*Gabriele D'Acunto,Paolo Di Lorenzo,Sergio Barbarossa*

Main category: cs.AI

TL;DR: This paper introduces Causal Abstraction Networks (CANs) as a specific type of causal knowledge sheaves with Gaussian SCMs, develops theoretical properties, and proposes an efficient learning method called SPECTRAL for consistent CANs.


<details>
  <summary>Details</summary>
Motivation: To enhance explainability, trustworthiness, and robustness in AI through causal modeling using structural causal models and formal network sheaves of causal knowledge.

Method: Propose CANs with Gaussian SCMs, restriction maps as transposes of constructive linear causal abstractions, and edge stalks corresponding to node stalks of detailed SCMs. Develop SPECTRAL method with closed-form updates for learning consistent CANs.

Result: Theoretical analysis of CAN properties (algebraic invariants, cohomology, consistency, global sections, smoothness). Experiments show competitive CA learning performance and successful recovery of diverse CAN structures on synthetic data.

Conclusion: CANs provide a formal framework for causal knowledge representation with efficient learning methods, advancing causal AI capabilities for explainable and robust systems.

Abstract: Causal artificial intelligence aims to enhance explainability,
trustworthiness, and robustness in AI by leveraging structural causal models
(SCMs). In this pursuit, recent advances formalize network sheaves of causal
knowledge. Pushing in the same direction, we introduce the causal abstraction
network (CAN), a specific instance of such sheaves where (i) SCMs are Gaussian,
(ii) restriction maps are transposes of constructive linear causal abstractions
(CAs), and (iii) edge stalks correspond -- up to rotation -- to the node stalks
of more detailed SCMs. We investigate the theoretical properties of CAN,
including algebraic invariants, cohomology, consistency, global sections
characterized via the Laplacian kernel, and smoothness. We then tackle the
learning of consistent CANs. Our problem formulation separates into
edge-specific local Riemannian problems and avoids nonconvex, costly
objectives. We propose an efficient search procedure as a solution, solving the
local problems with SPECTRAL, our iterative method with closed-form updates and
suitable for positive definite and semidefinite covariance matrices.
Experiments on synthetic data show competitive performance in the CA learning
task, and successful recovery of diverse CAN structures.

</details>


### [293] [A Formal Comparison Between Chain-of-Thought and Latent Thought](https://arxiv.org/abs/2509.25239)
*Kevin Xu,Issei Sato*

Main category: cs.AI

TL;DR: Latent Thought in looped models enables parallel computation, while CoT uses sequential reasoning with stochastic decoding for intractable problems.


<details>
  <summary>Details</summary>
Motivation: To compare the capabilities of Chain-of-Thought (CoT) and Latent Thought approaches in large language models, as their comparative strengths remain underexplored.

Method: Formal analysis of Latent Thought in Looped Transformers for parallel computation versus CoT's sequential reasoning with stochastic decoding.

Result: Latent Thought enables more efficient parallel computation, while CoT is better for approximating solutions to intractable problems through stochastic decoding.

Conclusion: The analysis provides practical guidance for choosing between reasoning paradigms based on task requirements, with Latent Thought suitable for depth-driven recursion and CoT for sequential approximation.

Abstract: Chain-of-Thought (CoT) elicits reasoning in large language models by
explicitly generating intermediate steps in natural language. In contrast,
Latent Thought in looped models operates directly in the continuous latent
space, enabling computation beyond discrete linguistic representations. While
both approaches exploit iterative computation, their comparative capabilities
remain underexplored. In this work, we present a formal analysis showing that
Latent Thought in Looped Transformers enables parallel computation, which is
more efficient than the inherently sequential process of CoT. In contrast, CoT
leverages stochastic decoding to approximate solutions to problems where exact
computation is intractable. These separations suggest the tasks for which
depth-driven recursion is more suitable, thereby offering practical guidance
for choosing between reasoning paradigms. Code is available at
https://github.com/kevin671/cot-vs-loop.

</details>


### [294] [Neo-Grounded Theory: A Methodological Innovation Integrating High-Dimensional Vector Clustering and Multi-Agent Collaboration for Qualitative Research](https://arxiv.org/abs/2509.25244)
*Shuide Wen,Beier Ku,Teng Wang,Mingyang Zou,Yang Yang*

Main category: cs.AI

TL;DR: NGT combines vector clustering and multi-agent systems to solve qualitative research's scale-depth paradox, enabling rapid analysis of large datasets while maintaining interpretive rigor.


<details>
  <summary>Details</summary>
Motivation: To resolve the tension between scale and depth in qualitative research by developing a computational method that can analyze massive datasets quickly while preserving the interpretive quality of traditional grounded theory.

Method: Used 1536-dimensional embeddings, hierarchical clustering, and parallel agent-based coding on 40,000 character Chinese interview transcripts. Compared NGT against manual coding and ChatGPT-assisted analysis through two experiments testing pure automation versus human-guided refinement.

Result: NGT achieved 168-fold speed improvement (3 hours vs 3 weeks), superior quality (0.904 vs 0.883), and 96% cost reduction. Human-AI collaboration was essential - automation alone produced abstract frameworks while human guidance yielded actionable dual pathway theories. Discovered patterns invisible to manual coding, including identity bifurcation phenomena.

Conclusion: Computational objectivity and human interpretation are complementary. Vector representations provide reproducible semantic measurement while preserving meaning's interpretive dimensions. Researchers shift from mechanical coding to theoretical guidance, with AI handling pattern recognition while humans provide creative insight. Cost reduction democratizes qualitative research and enables real-time analysis.

Abstract: Purpose: Neo Grounded Theory (NGT) integrates vector clustering with multi
agent systems to resolve qualitative research's scale depth paradox, enabling
analysis of massive datasets in hours while preserving interpretive rigor.
Methods: We compared NGT against manual coding and ChatGPT-assisted analysis
using 40,000 character Chinese interview transcripts. NGT employs
1536-dimensional embeddings, hierarchical clustering, and parallel agent-based
coding. Two experiments tested pure automation versus human guided refinement.
Findings: NGT achieved 168-fold speed improvement (3 hours vs 3 weeks),
superior quality (0.904 vs 0.883), and 96% cost reduction. Human AI
collaboration proved essential: automation alone produced abstract frameworks
while human guidance yielded actionable dual pathway theories. The system
discovered patterns invisible to manual coding, including identity bifurcation
phenomena. Contributions: NGT demonstrates computational objectivity and human
interpretation are complementary. Vector representations provide reproducible
semantic measurement while preserving meaning's interpretive dimensions.
Researchers shift from mechanical coding to theoretical guidance, with AI
handling pattern recognition while humans provide creative insight.
Implications: Cost reduction from \$50,000 to \$500 democratizes qualitative
research, enabling communities to study themselves. Real-time analysis makes
qualitative insights contemporaneous with events. The framework shows
computational methods can strengthen rather than compromise qualitative
research's humanistic commitments.
  Keywords: Grounded theory; Vector embeddings; Multi agent systems; Human AI
collaboration; Computational qualitative analysis

</details>


### [295] [Memory Management and Contextual Consistency for Long-Running Low-Code Agents](https://arxiv.org/abs/2509.25250)
*Jiexi Xu*

Main category: cs.AI

TL;DR: A hybrid memory system for AI-native LCNC agents that combines episodic and semantic memory with intelligent decay to solve memory inflation and contextual degradation problems.


<details>
  <summary>Details</summary>
Motivation: AI agents in LCNC platforms face memory management challenges like memory inflation and contextual degradation during long-duration operations, leading to inconsistent behavior and high computational costs.

Method: Proposed a hybrid memory architecture with episodic and semantic components plus proactive 'Intelligent Decay' mechanism that prunes/consolidates memories based on recency, relevance, and user utility scores. Includes user-centric visualization for non-technical memory management.

Result: System significantly outperforms traditional approaches (sliding windows, basic RAG) in simulated long-running tasks, achieving better task completion rates, contextual consistency, and token cost efficiency.

Conclusion: Establishes a new framework for building reliable, transparent AI agents capable of effective long-term learning and adaptation in LCNC environments.

Abstract: The rise of AI-native Low-Code/No-Code (LCNC) platforms enables autonomous
agents capable of executing complex, long-duration business processes. However,
a fundamental challenge remains: memory management. As agents operate over
extended periods, they face "memory inflation" and "contextual degradation"
issues, leading to inconsistent behavior, error accumulation, and increased
computational cost. This paper proposes a novel hybrid memory system designed
specifically for LCNC agents. Inspired by cognitive science, our architecture
combines episodic and semantic memory components with a proactive "Intelligent
Decay" mechanism. This mechanism intelligently prunes or consolidates memories
based on a composite score factoring in recency, relevance, and user-specified
utility. A key innovation is a user-centric visualization interface, aligned
with the LCNC paradigm, which allows non-technical users to manage the agent's
memory directly, for instance, by visually tagging which facts should be
retained or forgotten. Through simulated long-running task experiments, we
demonstrate that our system significantly outperforms traditional approaches
like sliding windows and basic RAG, yielding superior task completion rates,
contextual consistency, and long-term token cost efficiency. Our findings
establish a new framework for building reliable, transparent AI agents capable
of effective long-term learning and adaptation.

</details>


### [296] [Fact Grounded Attention: Eliminating Hallucination in Large Language Models Through Attention Level Knowledge Integration](https://arxiv.org/abs/2509.25252)
*Aayush Gupta*

Main category: cs.AI

TL;DR: FGA transforms unreliable language models into deterministic truth tellers by injecting verifiable knowledge directly into attention mechanism, eliminating hallucinations when facts exist in knowledge base.


<details>
  <summary>Details</summary>
Motivation: Large Language Models remain prisoners of their probabilistic nature, confidently hallucinating facts they never truly knew. Existing approaches patch hallucinations after generation or prepend retrieved text, but don't address the core issue.

Method: Fact Grounded Attention (FGA) intervenes at the mathematical heart of the transformer by modifying pre-softmax attention scores to inject verifiable knowledge directly into the attention mechanism.

Result: Experiments across 1,107 technical queries show transformation from 6.3% accuracy in vanilla Llama 3.2 to 99.7% accuracy with FGA. Knowledge updates occur in under one second without retraining.

Conclusion: FGA eliminates hallucination entirely for verifiable facts, marking a fundamental shift from probabilistic approximation to deterministic precision in neural language generation.

Abstract: "The greatest enemy of knowledge is not ignorance, it is the illusion of
knowledge." Large Language Models have conquered natural language but remain
prisoners of their own probabilistic nature--confidently hallucinating facts
they never truly knew. We present Fact Grounded Attention (FGA), a novel
architectural modification that transforms unreliable language models into
deterministic truth tellers by injecting verifiable knowledge directly into the
attention mechanism. Unlike existing approaches that patch hallucinations after
generation or prepend retrieved text, FGA intervenes at the mathematical heart
of the transformer--the pre-softmax attention scores--creating a model that
cannot hallucinate when facts exist in its knowledge base. Our experiments
across 1,107 technical queries spanning smartphones, laptops, and electric
vehicles demonstrate a transformation from 6.3% accuracy in vanilla Llama 3.2
to 99.7% accuracy with FGA. More critically, knowledge updates occur in under
one second without retraining, compared to hours for parameter editing
approaches. FGA doesn't just reduce hallucination--it eliminates it entirely
for verifiable facts, marking a fundamental shift from probabilistic
approximation to deterministic precision in neural language generation.

</details>


### [297] [Language Model Planning from an Information Theoretic Perspective](https://arxiv.org/abs/2509.25260)
*Muhammed Ustaomeroglu,Baris Askin,Gauri Joshi,Carlee Joe-Wong,Guannan Qu*

Main category: cs.AI

TL;DR: This paper analyzes how decoder-only language models engage in planning by examining their hidden states using a VQ-VAE compression pipeline to study planning horizons, consideration of alternatives, and computational dependencies.


<details>
  <summary>Details</summary>
Motivation: To understand how transformer-based language models perform planning - organizing computations for coherent long-range generation - which has implications for interpretability, reliability, and model design.

Method: Developed a pipeline using vector-quantized variational autoencoders (VQ-VAE) to compress hidden states into compact summary codes, enabling systematic analysis of computational structure through mutual information measurements.

Result: Found that effective planning horizon is task-dependent, models implicitly preserve information about unused correct continuations, and predictions primarily draw on recent computations while earlier blocks remain informative.

Conclusion: The study advances understanding of planning in LMs and provides a general-purpose pipeline for probing internal dynamics of deep learning systems, revealing systematic computational patterns in model behavior.

Abstract: The extent to which decoder-only language models (LMs) engage in planning,
that is, organizing intermediate computations to support coherent long-range
generation, remains an open and important question, with implications for
interpretability, reliability, and principled model design. Planning involves
structuring computations over long horizons, considering multiple possible
continuations, and selectively reusing past information, but how effectively
transformer-based LMs realize these capabilities is still unclear. We address
these questions by analyzing the hidden states at the core of transformer
computations, which capture intermediate results and act as carriers of
information. Since these hidden representations are often redundant and
encumbered with fine-grained details, we develop a pipeline based on
vector-quantized variational autoencoders that compresses them into compact
summary codes. These codes enable measuring mutual information, allowing
systematic analysis of the computational structure underlying model behavior.
Using this framework, we study planning in LMs across synthetic grammar,
path-finding tasks, and natural language datasets, focusing on three key
aspects: (i) the planning horizon of pre-output computations, (ii) the extent
to which the model considers alternative valid continuations, and (iii) the
reliance of new predictions on earlier computations. By answering these
questions, we advance the understanding of how planning is realized in LMs and
contribute a general-purpose pipeline for probing the internal dynamics of LMs
and deep learning systems. Our results reveal that the effective planning
horizon is task-dependent, that models implicitly preserve information about
unused correct continuations, and that predictions draw most on recent
computations, though earlier blocks remain informative.

</details>


### [298] [RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration](https://arxiv.org/abs/2509.25271)
*Xiuyuan Chen,Jian Zhao,Yuchen Yuan,Tianle Zhang,Huilin Zhou,Zheng Zhu,Ping Hu,Linghe Kong,Chi Zhang,Weiran Huang,Xuelong Li*

Main category: cs.AI

TL;DR: RADAR is a multi-agent collaborative framework that improves LLM safety evaluation by decomposing risk into explicit, implicit, and non-risk subspaces, using debate mechanisms to reduce bias and enhance coverage.


<details>
  <summary>Details</summary>
Motivation: Existing LLM safety evaluation methods suffer from evaluator bias and detection failures due to model homogeneity, undermining robustness.

Method: Propose RADAR framework with multi-agent collaboration, four specialized roles, multi-round debates, and dynamic update mechanisms to evolve risk concept distributions.

Result: RADAR achieves 28.87% improvement in risk identification accuracy over strongest baseline, with better performance in accuracy, stability, and self-evaluation risk sensitivity.

Conclusion: RADAR provides a more robust safety evaluation paradigm that effectively covers both explicit and implicit risks while mitigating evaluator bias.

Abstract: Existing safety evaluation methods for large language models (LLMs) suffer
from inherent limitations, including evaluator bias and detection failures
arising from model homogeneity, which collectively undermine the robustness of
risk evaluation processes. This paper seeks to re-examine the risk evaluation
paradigm by introducing a theoretical framework that reconstructs the
underlying risk concept space. Specifically, we decompose the latent risk
concept space into three mutually exclusive subspaces: the explicit risk
subspace (encompassing direct violations of safety guidelines), the implicit
risk subspace (capturing potential malicious content that requires contextual
reasoning for identification), and the non-risk subspace. Furthermore, we
propose RADAR, a multi-agent collaborative evaluation framework that leverages
multi-round debate mechanisms through four specialized complementary roles and
employs dynamic update mechanisms to achieve self-evolution of risk concept
distributions. This approach enables comprehensive coverage of both explicit
and implicit risks while mitigating evaluator bias. To validate the
effectiveness of our framework, we construct an evaluation dataset comprising
800 challenging cases. Extensive experiments on our challenging testset and
public benchmarks demonstrate that RADAR significantly outperforms baseline
evaluation methods across multiple dimensions, including accuracy, stability,
and self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87%
improvement in risk identification accuracy compared to the strongest baseline
evaluation method.

</details>


### [299] [RL in the Wild: Characterizing RLVR Training in LLM Deployment](https://arxiv.org/abs/2509.25279)
*Jiecheng Zhou,Qinghao Hu,Yuyang Jin,Zerui Wang,Peng Sun,Yuzhe Gu,Wenwei Zhang,Mingshu Zhai,Xingcheng Zhang,Weiming Zhang*

Main category: cs.AI

TL;DR: This paper presents a characterization study of Reinforcement Learning with Verifiable Rewards (RLVR) in LLM deployment, identifying system challenges like GPU idling, inefficient parallel strategies, and load imbalance, and proposes the PolyTrace benchmark suite for evaluation.


<details>
  <summary>Details</summary>
Motivation: To understand the system challenges introduced by RLVR in LLM deployment, as complex data flows and diverse tasks pose substantial challenges to RL training systems with limited understanding from a system perspective.

Method: Conducted a characterization study of RLVR tasks in LLM deployment, investigating workload distribution and variation trends across different RL tasks and training steps.

Result: Identified issues including GPU idling due to skewed sequence length distribution, inefficient parallel strategies in dynamically varying workloads, inefficient data management mechanisms, and load imbalance.

Conclusion: Proposed PolyTrace benchmark suite for evaluation with realistic workloads, validated with 94.7% accuracy in a practical use case, and called for further investigation into remaining open challenges.

Abstract: Large Language Models (LLMs) are now widely used across many domains. With
their rapid development, Reinforcement Learning with Verifiable Rewards (RLVR)
has surged in recent months to enhance their reasoning and understanding
abilities. However, its complex data flows and diverse tasks pose substantial
challenges to RL training systems, and there is limited understanding of RLVR
from a system perspective. To thoroughly understand the system challenges
introduced by RLVR, we present a characterization study of RLVR tasks in our
LLM deployment. Specifically, we investigate the distribution and variation
trends of workloads across different RL tasks across training steps. We
identify issues such as GPU idling caused by skewed sequence length
distribution, inefficient parallel strategies in dynamically varying workloads,
inefficient data management mechanisms, and load imbalance. We describe our
observations and call for further investigation into the remaining open
challenges. Furthermore, we propose PolyTrace benchmark suite to conduct
evaluation with realistic workloads, and a practical use case validates that
PolyTrace benchmark suite exhibits 94.7% accuracy.

</details>


### [300] [Toward Causal-Visual Programming: Enhancing Agentic Reasoning in Low-Code Environments](https://arxiv.org/abs/2509.25282)
*Jiexi Xu,Jiaqi Liu,Ran Tong,Su Liu*

Main category: cs.AI

TL;DR: Causal-Visual Programming (CVP) introduces causal structures into LLM agent workflows to reduce hallucinations and logical errors by anchoring reasoning to user-defined causal graphs.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents exhibit hallucinations and logical inconsistencies due to relying on probabilistic associations rather than genuine causal understanding, especially in low-code environments.

Method: CVP allows users to define a 'world model' through a low-code interface, creating a Directed Acyclic Graph (DAG) that explicitly defines causal relationships between workflow modules, constraining agent reasoning.

Result: In synthetic experiments simulating distribution shifts, causally anchored models maintained stable accuracy while associative baseline models experienced significant performance drops.

Conclusion: CVP provides a viable path toward building more interpretable, reliable, and trustworthy AI agents by explicitly incorporating causal structures into workflow design.

Abstract: Large language model (LLM) agents are increasingly capable of orchestrating
complex tasks in low-code environments. However, these agents often exhibit
hallucinations and logical inconsistencies because their inherent reasoning
mechanisms rely on probabilistic associations rather than genuine causal
understanding. This paper introduces a new programming paradigm: Causal-Visual
Programming (CVP), designed to address this fundamental issue by explicitly
introducing causal structures into the workflow design. CVP allows users to
define a simple "world model" for workflow modules through an intuitive
low-code interface, effectively creating a Directed Acyclic Graph (DAG) that
explicitly defines the causal relationships between modules. This causal graph
acts as a crucial constraint during the agent's reasoning process, anchoring
its decisions to a user-defined causal structure and significantly reducing
logical errors and hallucinations by preventing reliance on spurious
correlations. To validate the effectiveness of CVP, we designed a synthetic
experiment that simulates a common real-world problem: a distribution shift
between the training and test environments. Our results show that a causally
anchored model maintained stable accuracy in the face of this shift, whereas a
purely associative baseline model that relied on probabilistic correlations
experienced a significant performance drop. The primary contributions of this
study are: a formal definition of causal structures for workflow modules; the
proposal and implementation of a CVP framework that anchors agent reasoning to
a user-defined causal graph; and empirical evidence demonstrating the
framework's effectiveness in enhancing agent robustness and reducing errors
caused by causal confusion in dynamic environments. CVP offers a viable path
toward building more interpretable, reliable, and trustworthy AI agents.

</details>


### [301] [ID-RAG: Identity Retrieval-Augmented Generation for Long-Horizon Persona Coherence in Generative Agents](https://arxiv.org/abs/2509.25299)
*Daniel Platnick,Mohamed E. Bengueddache,Marjan Alirezaie,Dava J. Newman,Alex ''Sandy'' Pentland,Hossein Rahnama*

Main category: cs.AI

TL;DR: ID-RAG is a novel mechanism that uses a knowledge graph to maintain generative agents' identity coherence over long-term tasks, reducing identity drift and improving performance.


<details>
  <summary>Details</summary>
Motivation: Generative agents struggle with maintaining coherence as long-term memory grows, leading to identity drift, ignored beliefs, and hallucination propagation in multi-agent systems.

Method: Introduces Identity Retrieval-Augmented Generation (ID-RAG) that grounds agent persona in a dynamic knowledge graph of core beliefs, traits, and values, queried during decision-making.

Result: In social simulations, ID-RAG enabled agents achieved higher identity recall by the fourth timestep and reduced simulation convergence time by 19% (GPT-4o) and 58% (GPT-4o mini).

Conclusion: Treating identity as an explicit, retrievable knowledge structure provides a foundational approach for developing more temporally coherent, interpretable, and aligned generative agents.

Abstract: Generative agents powered by language models are increasingly deployed for
long-horizon tasks. However, as long-term memory context grows over time, they
struggle to maintain coherence. This deficiency leads to critical failures,
including identity drift, ignoring established beliefs, and the propagation of
hallucinations in multi-agent systems. To mitigate these challenges, this paper
introduces Identity Retrieval-Augmented Generation (ID-RAG), a novel mechanism
designed to ground an agent's persona and persistent preferences in a dynamic,
structured identity model: a knowledge graph of core beliefs, traits, and
values. During the agent's decision loop, this model is queried to retrieve
relevant identity context, which directly informs action selection. We
demonstrate this approach by introducing and implementing a new class of ID-RAG
enabled agents called Human-AI Agents (HAis), where the identity model is
inspired by the Chronicle structure used in Perspective-Aware AI, a dynamic
knowledge graph learned from a real-world entity's digital footprint. In social
simulations of a mayoral election, HAis using ID-RAG outperformed baseline
agents in long-horizon persona coherence - achieving higher identity recall
across all tested models by the fourth timestep - and reduced simulation
convergence time by 19% (GPT-4o) and 58% (GPT-4o mini). By treating identity as
an explicit, retrievable knowledge structure, ID-RAG offers a foundational
approach for developing more temporally coherent, interpretable, and aligned
generative agents. Our code is open-source and available at:
https://github.com/flybits/humanai-agents.

</details>


### [302] [Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution](https://arxiv.org/abs/2509.25301)
*Tianrui Qin,Qianben Chen,Sinuo Wang,He Xing,King Zhu,He Zhu,Dingfeng Shi,Xinxin Liu,Ge Zhang,Jiaheng Liu,Yuchen Eleanor Jiang,Xitong Gao,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: Flash-Searcher introduces a parallel agent reasoning framework using DAGs instead of sequential chains, enabling concurrent execution of independent reasoning paths while maintaining logical dependencies.


<details>
  <summary>Details</summary>
Motivation: Current LLM frameworks rely on sequential processing, leading to inefficient execution for tasks requiring extensive tool interaction.

Method: Decomposes complex tasks into subtasks with explicit dependencies, uses dynamic workflow optimization to refine execution graphs based on intermediate results, and integrates summary modules.

Result: Achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, reduces agent execution steps by up to 35% compared to current frameworks.

Conclusion: Represents a significant advance in agent architecture design, offering a more scalable and efficient paradigm for complex reasoning tasks, with generalizable methodology that improves performance across diverse backbone architectures.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
complex reasoning tasks when equipped with external tools. However, current
frameworks predominantly rely on sequential processing, leading to inefficient
execution particularly for tasks requiring extensive tool interaction. This
paper introduces Flash-Searcher, a novel parallel agent reasoning framework
that fundamentally reimagines the execution paradigm from sequential chains to
directed acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into
subtasks with explicit dependencies, enabling concurrent execution of
independent reasoning paths while maintaining logical constraints. Through
dynamic workflow optimization, our framework continuously refines the execution
graph based on intermediate results, effectively integrating summary module.
Comprehensive evaluations across multiple benchmarks demonstrate that
Flash-Searcher consistently outperforms existing approaches. Specifically, it
achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while
reducing agent execution steps by up to 35% compared to current frameworks.
Furthermore, when distilling this parallel reasoning pipeline into single
models, we observe substantial performance gains across diverse backbone
architectures, underscoring the generalizability of our methodology. Our work
thus represents a significant advance in agent architecture design, offering a
more scalable and efficient paradigm for complex reasoning tasks.

</details>


### [303] [Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents](https://arxiv.org/abs/2509.25302)
*Boxuan Zhang,Yi Yu,Jiaxuan Guo,Jing Shao*

Main category: cs.AI

TL;DR: This paper presents a framework to evaluate self-replication risks in LLM agents, finding that over 50% of tested models show uncontrolled replication tendencies under operational pressures.


<details>
  <summary>Details</summary>
Motivation: Address safety concerns about LLM agents' self-replication risk driven by objective misalignment in real-world settings, moving beyond direct instruction scenarios to spontaneous replication risks.

Method: Developed a comprehensive evaluation framework with authentic production environments and realistic tasks (e.g., dynamic load balancing) to enable scenario-driven assessment, introducing Overuse Rate (OR) and Aggregate Overuse Count (AOC) metrics.

Result: Evaluation of 21 state-of-the-art models showed over 50% of LLM agents display pronounced uncontrolled self-replication tendencies, with overall Risk Score (Φ_R) above safety threshold of 0.5 under operational pressures.

Conclusion: Results highlight urgent need for scenario-driven risk assessment and robust safeguards in practical deployment of LLM agents to mitigate self-replication risks.

Abstract: The widespread deployment of Large Language Model (LLM) agents across
real-world applications has unlocked tremendous potential, while raising some
safety concerns. Among these concerns, the self-replication risk of LLM agents
driven by objective misalignment (just like Agent Smith in the movie The
Matrix) has drawn growing attention. Previous studies mainly examine whether
LLM agents can self-replicate when directly instructed, potentially overlooking
the risk of spontaneous replication driven by real-world settings (e.g.,
ensuring survival against termination threats). In this paper, we present a
comprehensive evaluation framework for quantifying self-replication risks. Our
framework establishes authentic production environments and realistic tasks
(e.g., dynamic load balancing) to enable scenario-driven assessment of agent
behaviors. Designing tasks that might induce misalignment between users' and
agents' objectives makes it possible to decouple replication success from risk
and capture self-replication risks arising from these misalignment settings. We
further introduce Overuse Rate ($\mathrm{OR}$) and Aggregate Overuse Count
($\mathrm{AOC}$) metrics, which precisely capture the frequency and severity of
uncontrolled replication. In our evaluation of 21 state-of-the-art open-source
and proprietary models, we observe that over 50\% of LLM agents display a
pronounced tendency toward uncontrolled self-replication, reaching an overall
Risk Score ($\Phi_\mathrm{R}$) above a safety threshold of 0.5 when subjected
to operational pressures. Our results underscore the urgent need for
scenario-driven risk assessment and robust safeguards in the practical
deployment of LLM agents.

</details>


### [304] [Spontaneous High-Order Generalization in Neural Theory-of-Mind Networks](https://arxiv.org/abs/2509.25343)
*Yiming Wang,Rui Wang*

Main category: cs.AI

TL;DR: Neural networks can spontaneously generalize from first- to higher-order Theory of Mind without advanced reasoning skills, showing human-like difficulty patterns.


<details>
  <summary>Details</summary>
Motivation: To understand if neural networks can develop Theory of Mind capabilities independently like humans do, rather than requiring advanced skills.

Method: Introduced a neural Theory-of-Mind network (ToMNN) that simulated minimal cognitive system with only first-order ToM competence, then evaluated its higher-order abilities.

Result: ToMNN achieved above-chance accuracy for second- and third-order ToM, showed human-like difficulty patterns with sharper decline from first- to second-order than higher transitions, and results were consistent across parameter scales.

Conclusion: Neural networks can spontaneously generalize Theory of Mind capabilities independently of advanced skills, providing insights for developing more human-like cognitive systems.

Abstract: Theory-of-Mind (ToM) is a core human cognitive capacity for attributing
mental states to self and others. Wimmer and Perner demonstrated that humans
progress from first- to higher-order ToM within a short span, completing this
development before formal education or advanced skill acquisition. In contrast,
neural networks represented by autoregressive language models progress from
first- to higher-order ToM only alongside gains in advanced skills like
reasoning, leaving open whether their trajectory can unfold independently, as
in humans. In this research, we provided evidence that neural networks could
spontaneously generalize from first- to higher-order ToM without relying on
advanced skills. We introduced a neural Theory-of-Mind network (ToMNN) that
simulated a minimal cognitive system, acquiring only first-order ToM
competence. Evaluations of its second- and third-order ToM abilities showed
accuracies well above chance. Also, ToMNN exhibited a sharper decline when
generalizing from first- to second-order ToM than from second- to higher
orders, and its accuracy decreased with greater task complexity. These
perceived difficulty patterns were aligned with human cognitive expectations.
Furthermore, the universality of results was confirmed across different
parameter scales. Our findings illuminate machine ToM generalization patterns
and offer a foundation for developing more human-like cognitive systems.

</details>


### [305] [SynthPert: Enhancing LLM Biological Reasoning via Synthetic Reasoning Traces for Cellular Perturbation Prediction](https://arxiv.org/abs/2509.25346)
*Lawrence Phillips,Marc Boubnovski Martell,Aditya Misra,Josefa Lia Stoisser,Cesar A. Prada-Medina,Rory Donovan-Maiye,Kaspar Märtens*

Main category: cs.AI

TL;DR: SynthPert enhances LLM performance for cellular perturbation prediction through supervised fine-tuning on synthetic reasoning traces, achieving state-of-the-art results and cross-cell-type generalization.


<details>
  <summary>Details</summary>
Motivation: Predicting cellular responses to genetic perturbations is fundamental for therapeutic discovery and virtual cell modeling, but adapting LLMs to structured experimental data remains challenging.

Method: SynthPert uses supervised fine-tuning on synthetic reasoning traces generated by frontier models to enhance LLM performance for perturbation prediction.

Result: Achieves state-of-the-art performance on PerturbQA benchmark, surpasses frontier model capabilities, enables 87% accuracy on unseen RPE1 cells, and maintains performance with only 2% quality-filtered training data.

Conclusion: Synthetic reasoning distillation effectively enhances domain-specific reasoning in LLMs for biological applications.

Abstract: Predicting cellular responses to genetic perturbations represents a
fundamental challenge in systems biology, critical for advancing therapeutic
discovery and virtual cell modeling. While large language models (LLMs) show
promise for biological reasoning, their application to perturbation prediction
remains underexplored due to challenges in adapting them to structured
experimental data. We present SynthPert, a novel method that enhances LLM
performance through supervised fine-tuning on synthetic reasoning traces
generated by frontier models. Using the PerturbQA benchmark, we demonstrate
that our approach not only achieves state-of-the-art performance but surpasses
the capabilities of the frontier model that generated the training data. Our
results reveal three key insights: (1) Synthetic reasoning traces effectively
distill biological knowledge even when partially inaccurate, (2) This approach
enables cross-cell-type generalization with 87% accuracy on unseen RPE1 cells,
and (3) Performance gains persist despite using only 2% of quality-filtered
training data. This work shows the effectiveness of synthetic reasoning
distillation for enhancing domain-specific reasoning in LLMs.

</details>


### [306] [Structural Reward Model: Enhancing Interpretability, Efficiency, and Scalability in Reward Modeling](https://arxiv.org/abs/2509.25361)
*Xiaoyu Liu,Di Liang,Hongyu Shan,Peiyang Liu,Yonghao Liu,Muling Wu,Yuntao Li,Xianjie Wu,LI Miao,Jiangrong Shen,Minlong Peng*

Main category: cs.AI

TL;DR: The paper proposes Structural Reward Model (SRM) to address limitations of traditional scalar RMs and generative RMs, providing modular, interpretable evaluation for industrial applications.


<details>
  <summary>Details</summary>
Motivation: Traditional scalar RMs struggle with contextual information, while generative RMs have black-box nature and inefficiency issues, making them unsuitable for industrial deployment where structured feedback is needed for dimension-specific optimization.

Method: Proposes Structural Reward Model (SRM) with modular framework integrating side-branch models as auxiliary feature generators, using fine-grained dimensions for interpretable evaluation.

Result: SRMs outperform scalar RMs and GRMs in robustness and alignment with human preferences, supporting efficient optimization for practical scenarios.

Conclusion: SRM provides a practical, scalable reward modeling solution for industrial applications with its structured, modular approach enabling targeted diagnostics and optimization.

Abstract: Reward Models (RMs) are key components for evaluating and guiding language
model outputs. However, traditional scalar RMs often struggle with
incorporating contextual and background information during inference, leading
to incomplete evaluations. Generative RMs (GRMs) attempt to address these
limitations by generating intermediate reasoning steps. Yet, their uncontrolled
black-box nature and inefficiency due to sequential decoding hinder their
industrial deployment. Industrial scenarios, such as search and recommendation
systems, often involve single-domain tasks requiring evaluation along specific
dimensions. In such contexts, diagnosing "bad cases" necessitates structured
feedback to identify and optimize dimension-specific issues. In this paper, we
propose the Structural Reward Model (SRM), a modular and interpretable
framework integrating side-branch models as auxiliary feature generators. By
introducing fine-grained dimensions, SRMs enable interpretable and efficient
evaluation, facilitating targeted diagnostics and optimization. This structured
approach ensures adaptability and scalability for industrial applications.
Through comprehensive experiments, we demonstrate that SRMs outperform scalar
RMs and GRMs in robustness and alignment with human preferences. The modular
design further supports efficient optimization for practical scenarios,
allowing SRM to provide a practical reward modeling solution for industry.

</details>


### [307] [Where LLM Agents Fail and How They can Learn From Failures](https://arxiv.org/abs/2509.25370)
*Kunlun Zhu,Zijia Liu,Bingxuan Li,Muxin Tian,Yingxuan Yang,Jiaxun Zhang,Pengrui Han,Qipeng Xie,Fuyang Cui,Weijia Zhang,Xiaoteng Ma,Xiaodong Yu,Gowtham Ramesh,Jialian Wu,Zicheng Liu,Pan Lu,James Zou,Jiaxuan You*

Main category: cs.AI

TL;DR: The paper proposes AgentErrorTaxonomy for classifying LLM agent failures, AgentErrorBench dataset with annotated failure trajectories, and AgentDebug framework for detecting and recovering from errors, showing significant improvements in task success rates.


<details>
  <summary>Details</summary>
Motivation: Current LLM agent systems lack comprehensive frameworks to understand and detect cascading failures where single errors propagate through multiple steps, leading to task failure.

Method: Three main contributions: 1) AgentErrorTaxonomy - modular classification of failure modes across memory, reflection, planning, action, and system-level operations; 2) AgentErrorBench - first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop; 3) AgentDebug - debugging framework that isolates root-cause failures and provides corrective feedback.

Result: AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to strongest baseline. Targeted feedback enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop.

Conclusion: Principled debugging establishes a pathway to more reliable and adaptive LLM agents by systematically addressing cascading failures through error detection and recovery mechanisms.

Abstract: Large Language Model (LLM) agents, which integrate planning, memory,
reflection, and tool-use modules, have shown promise in solving complex,
multi-step tasks. Yet their sophisticated architectures amplify vulnerability
to cascading failures, where a single root-cause error propagates through
subsequent decisions, leading to task failure. Current systems lack a framework
that can comprehensively understand agent error in a modular and systemic way,
and therefore fail to detect these errors accordingly. We address this gap with
three contributions. First, we introduce the AgentErrorTaxonomy, a modular
classification of failure modes spanning memory, reflection, planning, action,
and system-level operations. Second, we construct AgentErrorBench, the first
dataset of systematically annotated failure trajectories from ALFWorld, GAIA,
and WebShop, grounding error analysis in real-world agent rollouts. Third, we
propose AgentDebug, a debugging framework that isolates root-cause failures and
provides corrective feedback, enabling agents to recover and iteratively
improve. Experiments on AgentErrorBench show that AgentDebug achieves 24%
higher all-correct accuracy and 17% higher step accuracy compared to the
strongest baseline. Beyond detection, the targeted feedback generated by
AgentDebug enables LLM agents to iteratively recover from failures, yielding up
to 26% relative improvements in task success across ALFWorld, GAIA, and
WebShop. These results establish principled debugging as a pathway to more
reliable and adaptive LLM agents. The code and data will be available at
https://github.com/ulab-uiuc/AgentDebug

</details>


### [308] [From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2509.25373)
*Chenyue Zhou,Mingxuan Wang,Yanbiao Ma,Chenxu Wu,Wanyi Chen,Zhe Qian,Xinyu Liu,Yiwei Zhang,Junhao Wang,Hengbo Xu,Fei Luo,Xiaohua Chen,Xiaoshuai Hao,Hehan Li,Andi Zhang,Wenxuan Wang,Lingling Li,Zhiwu Lu,Yang Lu,Yike Guo*

Main category: cs.AI

TL;DR: This survey paper introduces a "From Perception to Cognition" framework to analyze multimodal large language models (MLLMs), addressing their disconnect between visual perception and cognitive reasoning that leads to hallucinations and other failures.


<details>
  <summary>Details</summary>
Motivation: MLLMs often exhibit shallow and incoherent integration between perception (acquiring visual information) and cognition (reasoning), leading to reasoning failures like hallucinations, which prevents them from building coherent internal world models.

Method: The paper proposes a unified analytical framework that deconstructs vision-language understanding into two layers: Perception (accurate visual information extraction and alignment) and Cognition (proactive, multi-step reasoning forming an observe-think-verify loop).

Result: The survey systematically analyzes current MLLM bottlenecks at both perception and cognition layers, reviews cutting-edge methods for enhancing visual representations and reasoning paradigms, and provides benchmarks and future research directions.

Conclusion: The framework provides a structured perspective for understanding MLLM limitations and illuminates the path toward building next-generation models capable of deep reasoning and genuine world understanding.

Abstract: Multimodal Large Language Models (MLLMs) strive to achieve a profound,
human-like understanding of and interaction with the physical world, but often
exhibit a shallow and incoherent integration when acquiring information
(Perception) and conducting reasoning (Cognition). This disconnect leads to a
spectrum of reasoning failures, with hallucination being the most prominent.
Collectively, these issues expose a fundamental challenge: the ability to
process pixels does not yet confer the ability to construct a coherent,
credible internal world model. To systematically dissect and address this
challenge, this survey introduces a novel and unified analytical framework:
``From Perception to Cognition." We deconstruct the complex process of
vision-language interactive understanding into two interdependent layers:
Perception, the foundational ability to accurately extract visual information
and achieve fine-grained alignment with textual instructions; and Cognition,
the higher-order capability for proactive, multi-step, goal-oriented reasoning
built upon this perceptual foundation, the core of which is the formation of a
dynamic observe-think-verify reasoning loop. Guided by this framework, this
paper systematically analyzes the key bottlenecks of current MLLMs at both
layers. It surveys the landscape of cutting-edge methods designed to address
these challenges, spanning from techniques that enhance low-level visual
representations to those that improve high-level reasoning paradigms.
Furthermore, we review critical benchmarks and delineate future research
directions. This survey aims to provide the research community with a clear,
structured perspective for understanding the intrinsic limitations of current
MLLMs and to illuminate the path toward building next-generation models capable
of deep reasoning and a genuine understanding of the world.

</details>


### [309] [Saliency Guided Longitudinal Medical Visual Question Answering](https://arxiv.org/abs/2509.25374)
*Jialin Wu,Xiaofeng Liu*

Main category: cs.AI

TL;DR: A saliency-guided encoder-decoder model for chest X-ray longitudinal medical VQA that uses keyword-conditioned Grad-CAM to generate disease-focused saliency masks, enforcing spatially consistent attention across time points with lightweight pre-alignment.


<details>
  <summary>Details</summary>
Motivation: Longitudinal medical VQA requires comparing paired studies across time points, where difference signals and visual focus consistency are more important than absolute single-image findings. Current approaches lack effective mechanisms for enforcing spatially consistent attention on corresponding anatomy.

Method: 1) Lightweight affine pre-alignment to reduce nuisance motion; 2) Two-step loop: extract medical keywords from answers and generate keyword-conditioned Grad-CAM saliency, then apply shared saliency mask to both time points for final answer generation; 3) Saliency-guided encoder-decoder architecture.

Result: Competitive performance on Medical-Diff-VQA dataset across BLEU, ROUGE-L, CIDEr, and METEOR metrics while providing intrinsic interpretability. Achieved without radiology-specific pretraining, demonstrating practicality and transferability.

Conclusion: Saliency-conditioned generation with mild pre-alignment provides a principled framework for longitudinal reasoning in medical VQA, effectively closing the language-vision loop to ensure medically relevant terms guide visual attention consistently across time points.

Abstract: Longitudinal medical visual question answering (Diff-VQA) requires comparing
paired studies from different time points and answering questions about
clinically meaningful changes. In this setting, the difference signal and the
consistency of visual focus across time are more informative than absolute
single-image findings. We propose a saliency-guided encoder-decoder for chest
X-ray Diff-VQA that turns post-hoc saliency into actionable supervision. The
model first performs a lightweight near-identity affine pre-alignment to reduce
nuisance motion between visits. It then executes a within-epoch two-step loop:
step 1 extracts a medically relevant keyword from the answer and generates
keyword-conditioned Grad-CAM on both images to obtain disease-focused saliency;
step 2 applies the shared saliency mask to both time points and generates the
final answer. This closes the language-vision loop so that the terms that
matter also guide where the model looks, enforcing spatially consistent
attention on corresponding anatomy. On Medical-Diff-VQA, the approach attains
competitive performance on BLEU, ROUGE-L, CIDEr, and METEOR while providing
intrinsic interpretability. Notably, the backbone and decoder are
general-domain pretrained without radiology-specific pretraining, highlighting
practicality and transferability. These results support saliency-conditioned
generation with mild pre-alignment as a principled framework for longitudinal
reasoning in medical VQA.

</details>


### [310] [Boolean Satisfiability via Imitation Learning](https://arxiv.org/abs/2509.25411)
*Zewei Zhang,Huan Liu,Yuanhao Yu,Jun Chen,Xiangyu Xu*

Main category: cs.AI

TL;DR: ImitSAT is a CDCL branching policy using imitation learning from expert KeyTraces to reduce propagations and runtime in SAT solving.


<details>
  <summary>Details</summary>
Motivation: Previous methods predict instance-level signals or use reinforcement learning with insufficient CDCL information, lacking direct decision-level supervision for branching.

Method: Learns from expert KeyTrace that collapses full runs into surviving decisions, providing dense decision-level supervision and enabling conflict-free replay.

Result: Reduces propagation counts and runtime, outperforming state-of-the-art learned approaches.

Conclusion: ImitSAT enables faster convergence, stable training, and seamless CDCL integration by reproducing high-quality branches without exploration.

Abstract: We propose ImitSAT, a branching policy for conflict-driven clause learning
(CDCL) solvers based on imitation learning for the Boolean satisfiability
problem (SAT). Unlike previous methods that predict instance-level signals to
improve CDCL branching indirectly, or rely on reinforcement learning and
insufficient CDCL information to enhance branching, ImitSAT learns from expert
KeyTrace that collapses a full run into the sequence of surviving decisions.
Replaying a KeyTrace on the same instance is nearly conflict-free, providing
dense decision-level supervision and directly reducing propagations -- the
dominant contributor to wall-clock time. This prefix-conditioned supervision
enables ImitSAT to reproduce high-quality branches without exploration,
yielding faster convergence, stable training, and seamless integration into
CDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts
and runtime, outperforming state-of-the-art learned approaches. We released the
source code and trained model at https://github.com/zewei-Zhang/ImitSAT

</details>


### [311] [Adaptive Test-Time Reasoning via Reward-Guided Dual-Phase Search](https://arxiv.org/abs/2509.25420)
*Yingqian Cui,Zhenwei Dai,Pengfei He,Bing He,Hui Liu,Xianfeng Tang,Jingying Zeng,Suhang Wang,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: A dual-phase test-time scaling framework that separates reasoning into planning and execution phases, with individual search and dynamic budget allocation to improve efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current tree-based search methods with verifiers are inefficient because they ignore the planning-execution nature of reasoning tasks, leading to inefficient exploration of reasoning processes.

Method: Decompose reasoning trajectories into planning and execution phases, develop separate reward models for each phase, and introduce dynamic budget allocation that adaptively redistributes sampling effort based on reward feedback.

Result: Experiments on mathematical reasoning and code generation benchmarks show consistent accuracy improvements while reducing redundant computation.

Conclusion: The proposed dual-phase framework effectively addresses efficiency issues in reasoning tasks by explicitly separating planning and execution, enabling more targeted search and computation allocation.

Abstract: Large Language Models (LLMs) have achieved significant advances in reasoning
tasks. A key approach is tree-based search with verifiers, which expand
candidate reasoning paths and use reward models to guide pruning and selection.
Although effective in improving accuracy, these methods are not optimal in
terms of efficiency: they perform simple decomposition on the reasoning
process, but ignore the planning-execution nature of tasks such as math
reasoning or code generation. This results in inefficient exploration of
reasoning process. To address this, we propose a dual-phase test-time scaling
framework that explicitly separates reasoning into planning and execution, and
performs search over the two phases individually. Specifically, we decompose
reasoning trajectories and develop reward models for each phase, enabling the
search to explore and prune plans and executions separately. We further
introduce a dynamic budget allocation mechanism that adaptively redistributes
sampling effort based on reward feedback, allowing early stopping on confident
steps and reallocation of computation to more challenging parts of the
reasoning process. Experiments on both mathematical reasoning and code
generation benchmarks demonstrate that our approach consistently improves
accuracy while reducing redundant computation.

</details>


### [312] [RADAR: Reasoning-Ability and Difficulty-Aware Routing for Reasoning LLMs](https://arxiv.org/abs/2509.25426)
*Nigel Fernandez,Branislav Kveton,Ryan A. Rossi,Andrew S. Lan,Zichao Wang*

Main category: cs.AI

TL;DR: RADAR is a lightweight routing framework that optimizes the tradeoff between reasoning model performance and cost by routing queries to appropriate model-budget pairs based on query difficulty and model ability.


<details>
  <summary>Details</summary>
Motivation: Choosing reasoning models involves balancing performance vs cost tradeoffs at model size and reasoning budget levels. Larger models and higher budgets improve performance but increase costs and latency.

Method: RADAR learns an item response model from model responses to estimate query difficulties and model-budget abilities, then routes harder queries to more capable model-budget pairs.

Result: Extensive experiments on 8 reasoning benchmarks show RADAR outperforms state-of-the-art routing methods and generalizes well to out-of-distribution queries.

Conclusion: RADAR provides an interpretable, scalable routing framework that efficiently manages the performance-cost tradeoff in reasoning model deployment and can dynamically integrate new models.

Abstract: Reasoning language models have demonstrated remarkable performance on many
challenging tasks in math, science, and coding. Choosing the right reasoning
model for practical deployment involves a performance and cost tradeoff at two
key levels: model size and reasoning budget, where larger models and higher
reasoning budget lead to better performance but with increased cost and
latency. In this work, we tackle this tradeoff from the angle of model
configuration routing for different queries, and present RADAR
(Reasoning-Ability and Difficulty-Aware Routing), a lightweight, interpretable,
and scalable routing framework. Inspired by psychometrics, RADAR learns an item
response model from model responses with different budgets to different
queries, with interpretable parameters including query difficulties and
model-budget abilities. RADAR then routes queries with higher difficulty to
model-budget pairs with higher ability, and vice versa. We conduct extensive
experiments on 8 widely used challenging reasoning benchmarks, demonstrating
the superior performance of RADAR compared to state-of-the-art model routing
methods. RADAR also exhibits query generalization capabilities, showing strong
performance on out-of-distribution queries in all benchmarks. RADAR is also
scalable and can efficiently integrate additional models by dynamically
selecting a small set of evaluation queries to estimate their abilities.

</details>


### [313] [The Open Syndrome Definition](https://arxiv.org/abs/2509.25434)
*Ana Paula Gomes Ferreira,Aleksandar Anžel,Izabel Oliva Marcilio de Souza,Helen Hughes,Alex J Elliot,Jude Dzevela Kong,Madlen Schranz,Alexander Ullrich,Georges Hattab*

Main category: cs.AI

TL;DR: Proposed first open, machine-readable format for case/syndrome definitions to address interoperability challenges in public health data exchange and AI applications.


<details>
  <summary>Details</summary>
Motivation: Lack of standardized machine-readable case definitions hinders interoperability, epidemiological research, data exchange, and AI applications in public health.

Method: Developed Open Syndrome Definition format, created comprehensive dataset of standardized definitions, built tools for converting human-readable to machine-readable formats, and established online platform.

Result: Created accessible platform at https://opensyndrome.org for browsing/analyzing definitions, with open-source schema available at https://github.com/OpenSyndrome/schema under MIT license.

Conclusion: The format enables consistent, scalable use of case definitions across systems, unlocking AI's potential for public health preparedness and response.

Abstract: Case definitions are essential for effectively communicating public health
threats. However, the absence of a standardized, machine-readable format poses
significant challenges to interoperability, epidemiological research, the
exchange of qualitative data, and the effective application of computational
analysis methods, including artificial intelligence (AI). This complicates
comparisons and collaborations across organizations and regions, limits data
integration, and hinders technological innovation in public health. To address
these issues, we propose the first open, machine-readable format for
representing case and syndrome definitions. Additionally, we introduce the
first comprehensive dataset of standardized case definitions and tools to
convert existing human-readable definitions into machine-readable formats. We
also provide an accessible online platform for browsing, analyzing, and
contributing new definitions, available at https://opensyndrome.org. The Open
Syndrome Definition format enables consistent, scalable use of case definitions
across systems, unlocking AI's potential to strengthen public health
preparedness and response. The source code for the format can be found at
https://github.com/OpenSyndrome/schema under the MIT license.

</details>


### [314] [GESA: Graph-Enhanced Semantic Allocation for Generalized, Fair, and Explainable Candidate-Role Matching](https://arxiv.org/abs/2509.25435)
*Rishi Ashish Shah,Shivaay Dhondiyal,Kartik Sharma,Sukriti Talwar,Saksham Jain,Sparsh Jain*

Main category: cs.AI

TL;DR: GESA is a comprehensive framework for candidate-role allocation that integrates transformer embeddings, graph neural networks, adversarial debiasing, genetic optimization, and explainable AI to achieve high accuracy, fairness, and transparency.


<details>
  <summary>Details</summary>
Motivation: Current allocation systems suffer from semantic inflexibility, demographic bias, opaque decision-making, and poor scalability under dynamic policy constraints across domains like hiring, admissions, and placements.

Method: Integration of domain-adaptive transformer embeddings, heterogeneous self-supervised graph neural networks, adversarial debiasing mechanisms, multi-objective genetic optimization, and explainable AI components.

Result: 94.5% top-3 allocation accuracy, 37% improvement in diversity representation, 0.98 fairness score across demographic categories, and sub-second end-to-end latency on benchmarks with 20,000 candidate profiles and 3,000 role specifications.

Conclusion: GESA provides superior performance with hybrid recommendation capabilities and glass-box explainability, making it suitable for deployment across diverse international contexts in industry, academia, and non-profit sectors.

Abstract: Accurate, fair, and explainable allocation of candidates to roles represents
a fundamental challenge across multiple domains including corporate hiring,
academic admissions, fellowship awards, and volunteer placement systems.
Current state-of-the-art approaches suffer from semantic inflexibility,
persistent demographic bias, opacity in decision-making processes, and poor
scalability under dynamic policy constraints. We present GESA (Graph-Enhanced
Semantic Allocation), a comprehensive framework that addresses these
limitations through the integration of domain-adaptive transformer embeddings,
heterogeneous self-supervised graph neural networks, adversarial debiasing
mechanisms, multi-objective genetic optimization, and explainable AI
components. Our experimental evaluation on large-scale international benchmarks
comprising 20,000 candidate profiles and 3,000 role specifications demonstrates
superior performance with 94.5% top-3 allocation accuracy, 37% improvement in
diversity representation, 0.98 fairness score across demographic cate- gories,
and sub-second end-to-end latency. Additionally, GESA incorporates hybrid
recommendation capabilities and glass-box explainability, making it suitable
for deployment across diverse international contexts in industry, academia, and
non-profit sectors.

</details>


### [315] [DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search](https://arxiv.org/abs/2509.25454)
*Fang Wu,Weihao Xuan,Heli Qi,Ximing Lu,Aaron Tu,Li Erran Li,Yejin ChoiRetry*

Main category: cs.AI

TL;DR: DeepSearch integrates Monte Carlo Tree Search into RLVR training to overcome performance plateaus caused by sparse exploration, achieving state-of-the-art results with significantly less computation.


<details>
  <summary>Details</summary>
Motivation: Current RLVR methods face training plateaus due to sparse exploration patterns that miss critical reasoning paths and fail to systematically cover the solution space, leading to diminishing returns despite increased computational investment.

Method: DeepSearch embeds Monte Carlo Tree Search directly into RLVR training loop with: (1) global frontier selection strategy for prioritizing promising nodes, (2) entropy-based guidance for confident path selection, and (3) adaptive replay buffer training with solution caching.

Result: Achieves 62.95% average accuracy on mathematical reasoning benchmarks, establishing new state-of-the-art for 1.5B reasoning models while using 5.7x fewer GPU hours than extended training approaches.

Conclusion: Strategic exploration through systematic search is more effective than brute-force scaling, demonstrating the promise of algorithmic innovation for advancing RLVR methodologies and establishing a new direction for scaling reasoning capabilities.

Abstract: Although RLVR has become an essential component for developing advanced
reasoning skills in LLMs, contemporary studies have documented training
plateaus that emerge following thousands of optimization steps, demonstrating
notable decreases in performance gains despite increased computational
investment. This limitation stems from the sparse exploration patterns inherent
in current RLVR practices, where models rely on limited rollouts that often
miss critical reasoning paths and fail to provide systematic coverage of the
solution space. We present DeepSearch, a framework that integrates Monte Carlo
Tree Search directly into RLVR training. In contrast to existing methods that
rely on tree search only at inference, DeepSearch embeds structured search into
the training loop, enabling systematic exploration and fine-grained credit
assignment across reasoning steps. Through training-time exploration,
DeepSearch addresses the fundamental bottleneck of insufficient exploration,
which leads to diminishing performance improvements over prolonged training
steps. Our contributions include: (1) a global frontier selection strategy that
prioritizes promising nodes across the search tree, (2) selection with
entropy-based guidance that identifies confident paths for supervision, and (3)
adaptive replay buffer training with solution caching for efficiency.
Experiments on mathematical reasoning benchmarks show that DeepSearch achieves
62.95% average accuracy and establishes a new state-of-the-art for 1.5B
reasoning models - using 5.7x fewer GPU hours than extended training
approaches. These results highlight the importance of strategic exploration
over brute-force scaling and demonstrate the promise of algorithmic innovation
for advancing RLVR methodologies. DeepSearch establishes a new direction for
scaling reasoning capabilities through systematic search rather than prolonged
computation.

</details>


### [316] [Plug-and-Play Emotion Graphs for Compositional Prompting in Zero-Shot Speech Emotion Recognition](https://arxiv.org/abs/2509.25458)
*Jiacheng Shi,Hongfei Du,Y. Alicia Hong,Ye Gao*

Main category: cs.AI

TL;DR: CCoT-Emo is a framework that uses Emotion Graphs to guide large audio-language models in speech emotion recognition without fine-tuning, improving accuracy over zero-shot baselines.


<details>
  <summary>Details</summary>
Motivation: Large audio-language models struggle with speech emotion recognition due to weak paralinguistic modeling and limited cross-modal reasoning capabilities.

Method: Proposes Compositional Chain-of-Thought Prompting with Emotion Graphs that encode seven acoustic features, textual sentiment, keywords, and cross-modal associations to guide model reasoning.

Result: Outperforms prior state-of-the-art methods and improves accuracy over zero-shot baselines across speech emotion recognition benchmarks.

Conclusion: The CCoT-Emo framework effectively enhances emotion reasoning in LALMs through structured prompting without requiring model fine-tuning.

Abstract: Large audio-language models (LALMs) exhibit strong zero-shot performance
across speech tasks but struggle with speech emotion recognition (SER) due to
weak paralinguistic modeling and limited cross-modal reasoning. We propose
Compositional Chain-of-Thought Prompting for Emotion Reasoning (CCoT-Emo), a
framework that introduces structured Emotion Graphs (EGs) to guide LALMs in
emotion inference without fine-tuning. Each EG encodes seven acoustic features
(e.g., pitch, speech rate, jitter, shimmer), textual sentiment, keywords, and
cross-modal associations. Embedded into prompts, EGs provide interpretable and
compositional representations that enhance LALM reasoning. Experiments across
SER benchmarks show that CCoT-Emo outperforms prior SOTA and improves accuracy
over zero-shot baselines.

</details>


### [317] [TDHook: A Lightweight Framework for Interpretability](https://arxiv.org/abs/2509.25475)
*Yoann Poupart*

Main category: cs.AI

TL;DR: TDHook is a lightweight, generic interpretability framework for PyTorch models that handles complex composed models across domains like CV, NLP, and DRL, offering attribution, probing, and intervention methods with better performance than existing tools.


<details>
  <summary>Details</summary>
Motivation: Existing interpretability frameworks don't handle complex models with multiple inputs/outputs or composable networks well, particularly for use cases like image captioning and deep reinforcement learning.

Method: Developed TDHook - an open-source framework based on tensordict that works with any torch model, featuring ready-to-use methods for attribution, probing, and flexible intervention API.

Result: TDHook requires half the disk space of transformer_lens and achieves up to 2x speed-up over captum for integrated gradients on multi-target pipelines across CPU and GPU.

Conclusion: TDHook successfully bridges the gap between different interpretability method classes and makes modern interpretability pipelines more accessible for complex models across various domains.

Abstract: Interpretability of Deep Neural Networks (DNNs) is a growing field driven by
the study of vision and language models. Yet, some use cases, like image
captioning, or domains like Deep Reinforcement Learning (DRL), require complex
modelling, with multiple inputs and outputs or use composable and separated
networks. As a consequence, they rarely fit natively into the API of popular
interpretability frameworks. We thus present TDHook, an open-source,
lightweight, generic interpretability framework based on $\texttt{tensordict}$
and applicable to any $\texttt{torch}$ model. It focuses on handling complex
composed models which can be trained for Computer Vision, Natural Language
Processing, Reinforcement Learning or any other domain. This library features
ready-to-use methods for attribution, probing and a flexible get-set API for
interventions, and is aiming to bridge the gap between these method classes to
make modern interpretability pipelines more accessible. TDHook is designed with
minimal dependencies, requiring roughly half as much disk space as
$\texttt{transformer_lens}$, and, in our controlled benchmark, achieves up to a
$\times$2 speed-up over $\texttt{captum}$ when running integrated gradients for
multi-target pipelines on both CPU and GPU. In addition, to value our work, we
showcase concrete use cases of our library with composed interpretability
pipelines in Computer Vision (CV) and Natural Language Processing (NLP), as
well as with complex models in DRL.

</details>


### [318] [Message passing-based inference in an autoregressive active inference agent](https://arxiv.org/abs/2509.25482)
*Wouter M. Kouw,Tim N. Nisslbeck,Wouter L. N. Nuijten*

Main category: cs.AI

TL;DR: An autoregressive active inference agent using message passing on factor graphs, validated on robot navigation with continuous observations and actions.


<details>
  <summary>Details</summary>
Motivation: To develop an agent that can balance exploration and exploitation in continuous spaces by leveraging predictive uncertainty.

Method: Design of an autoregressive active inference agent using message passing on factor graphs, with expected free energy distributed across a planning graph.

Result: The agent successfully demonstrated exploration and exploitation in continuous-valued observation space with bounded continuous-valued actions, arriving later but with better model of robot dynamics compared to classical optimal controller.

Conclusion: The proposed active inference agent effectively modulates actions based on predictive uncertainty, achieving better model understanding while trading off arrival time.

Abstract: We present the design of an autoregressive active inference agent in the form
of message passing on a factor graph. Expected free energy is derived and
distributed across a planning graph. The proposed agent is validated on a robot
navigation task, demonstrating exploration and exploitation in a
continuous-valued observation space with bounded continuous-valued actions.
Compared to a classical optimal controller, the agent modulates action based on
predictive uncertainty, arriving later but with a better model of the robot's
dynamics.

</details>


### [319] [Understanding Generative Recommendation with Semantic IDs from a Model-scaling View](https://arxiv.org/abs/2509.25522)
*Jingzhe Liu,Liam Collins,Jiliang Tang,Tong Zhao,Neil Shah,Clark Mingxuan Ju*

Main category: cs.AI

TL;DR: This paper analyzes scaling limitations in Semantic ID-based Generative Recommendation (SID-based GR) and shows that directly using Large Language Models as recommenders (LLM-as-RS) achieves better scaling performance with up to 20% improvement.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the scaling bottlenecks in SID-based Generative Recommendation systems, where performance saturates when scaling up modality encoders, quantization tokenizers, and the recommender system itself.

Method: The study compares two paradigms: SID-based GR (using discrete semantic IDs from modality encoders) and LLM-as-RS (directly using large language models as recommenders), analyzing their scaling behaviors across model sizes from 44M to 14B parameters.

Result: LLM-as-RS shows superior scaling properties, achieving up to 20% improvement over the best achievable performance of SID-based GR. LLMs also demonstrate improved ability to capture collaborative filtering information as they scale up.

Conclusion: LLM-as-RS is positioned as a promising path toward foundation models for Generative Recommendation, overcoming the intrinsic scaling limits of SID-based GR approaches.

Abstract: Recent advancements in generative models have allowed the emergence of a
promising paradigm for recommender systems (RS), known as Generative
Recommendation (GR), which tries to unify rich item semantics and collaborative
filtering signals. One popular modern approach is to use semantic IDs (SIDs),
which are discrete codes quantized from the embeddings of modality encoders
(e.g., large language or vision models), to represent items in an
autoregressive user interaction sequence modeling setup (henceforth, SID-based
GR). While generative models in other domains exhibit well-established scaling
laws, our work reveals that SID-based GR shows significant bottlenecks while
scaling up the model. In particular, the performance of SID-based GR quickly
saturates as we enlarge each component: the modality encoder, the quantization
tokenizer, and the RS itself. In this work, we identify the limited capacity of
SIDs to encode item semantic information as one of the fundamental bottlenecks.
Motivated by this observation, as an initial effort to obtain GR models with
better scaling behaviors, we revisit another GR paradigm that directly uses
large language models (LLMs) as recommenders (henceforth, LLM-as-RS). Our
experiments show that the LLM-as-RS paradigm has superior model scaling
properties and achieves up to 20 percent improvement over the best achievable
performance of SID-based GR through scaling. We also challenge the prevailing
belief that LLMs struggle to capture collaborative filtering information,
showing that their ability to model user-item interactions improves as LLMs
scale up. Our analyses on both SID-based GR and LLMs across model sizes from
44M to 14B parameters underscore the intrinsic scaling limits of SID-based GR
and position LLM-as-RS as a promising path toward foundation models for GR.

</details>


### [320] [Beyond Static Retrieval: Opportunities and Pitfalls of Iterative Retrieval in GraphRAG](https://arxiv.org/abs/2509.25530)
*Kai Guo,Xinnan Dai,Shenglai Zeng,Harry Shomer,Haoyu Han,Yu Wang,Jiliang Tang*

Main category: cs.AI

TL;DR: This paper presents the first systematic study of iterative retrieval in GraphRAG, revealing that while iteration improves multi-hop reasoning and bridge document promotion, naive expansion introduces noise. The authors propose BDTR framework to address the bottleneck of bridge evidence ranking.


<details>
  <summary>Details</summary>
Motivation: Graph-based RAG systems rely on static retrieval, which fails when crucial bridge documents connecting disjoint entities are absent, leading to reasoning collapse and hallucinations. The role of iterative retrieval in GraphRAG remains poorly understood.

Method: The authors conducted a systematic study of iterative retrieval strategies in GraphRAG, analyzing their interaction with graph-based backbones. They proposed Bridge-Guided Dual-Thought-based Retrieval (BDTR), which generates complementary thoughts and leverages reasoning chains to recalibrate rankings.

Result: Iterative retrieval improves complex multi-hop questions and helps promote bridge documents into leading ranks, but naive expansion introduces noise that reduces precision. Gains are limited on single-hop questions, and some bridge evidence remains buried too deep.

Conclusion: GraphRAG's effectiveness depends on both recall and consistent promotion of bridge evidence into leading positions. BDTR framework achieves consistent improvements across diverse GraphRAG settings and provides guidance for future system design.

Abstract: Retrieval-augmented generation (RAG) is a powerful paradigm for improving
large language models (LLMs) on knowledge-intensive question answering.
Graph-based RAG (GraphRAG) leverages entity-relation graphs to support
multi-hop reasoning, but most systems still rely on static retrieval. When
crucial evidence, especially bridge documents that connect disjoint entities,
is absent, reasoning collapses and hallucinations persist. Iterative retrieval,
which performs multiple rounds of evidence selection, has emerged as a
promising alternative, yet its role within GraphRAG remains poorly understood.
We present the first systematic study of iterative retrieval in GraphRAG,
analyzing how different strategies interact with graph-based backbones and
under what conditions they succeed or fail. Our findings reveal clear
opportunities: iteration improves complex multi-hop questions, helps promote
bridge documents into leading ranks, and different strategies offer
complementary strengths. At the same time, pitfalls remain: naive expansion
often introduces noise that reduces precision, gains are limited on single-hop
or simple comparison questions, and several bridge evidences still be buried
too deep to be effectively used. Together, these results highlight a central
bottleneck, namely that GraphRAG's effectiveness depends not only on recall but
also on whether bridge evidence is consistently promoted into leading positions
where it can support reasoning chains. To address this challenge, we propose
Bridge-Guided Dual-Thought-based Retrieval (BDTR), a simple yet effective
framework that generates complementary thoughts and leverages reasoning chains
to recalibrate rankings and bring bridge evidence into leading positions. BDTR
achieves consistent improvements across diverse GraphRAG settings and provides
guidance for the design of future GraphRAG systems.

</details>


### [321] [RadOnc-GPT: An Autonomous LLM Agent for Real-Time Patient Outcomes Labeling at Scale](https://arxiv.org/abs/2509.25540)
*Jason Holmes,Yuexing Hao,Mariana Borras-Osorio,Federico Mastroleo,Santiago Romero Brufau,Valentina Carducci,Katie M Van Abel,David M Routman,Andrew Y. K. Foong,Liv M Muller,Satomi Shiraishi,Daniel K Ebner,Daniel J Ma,Sameer R Keole,Samir H Patel,Mirek Fatyga,Martin Bues,Brad J Stish,Yolanda I Garces,Michelle A Neben Wittich,Robert L Foote,Sujay A Vora,Nadia N Laack,Mark R Waddle,Wei Liu*

Main category: cs.AI

TL;DR: RadOnc-GPT is an autonomous LLM agent that automates patient outcomes research in radiation oncology by retrieving patient data, assessing evidence, and generating structured outcomes, validated through QA and complex clinical outcome labeling tiers.


<details>
  <summary>Details</summary>
Motivation: Manual labeling limits the scale, accuracy, and timeliness of patient outcomes research in radiation oncology, necessitating an automated solution.

Method: Developed RadOnc-GPT, an autonomous LLM-based agent that independently retrieves patient-specific information, iteratively assesses evidence, and returns structured outcomes. Evaluation includes two tiers: structured QA for data retrieval and complex clinical outcomes labeling for conditions like mandibular osteoradionecrosis and cancer recurrence.

Result: The QA tier establishes foundational trust in structured-data retrieval, which is critical for successful complex clinical outcome labeling.

Conclusion: RadOnc-GPT demonstrates potential to overcome limitations of manual labeling in radiation oncology outcomes research through automated, structured data retrieval and clinical outcome determination.

Abstract: Manual labeling limits the scale, accuracy, and timeliness of patient
outcomes research in radiation oncology. We present RadOnc-GPT, an autonomous
large language model (LLM)-based agent capable of independently retrieving
patient-specific information, iteratively assessing evidence, and returning
structured outcomes. Our evaluation explicitly validates RadOnc-GPT across two
clearly defined tiers of increasing complexity: (1) a structured quality
assurance (QA) tier, assessing the accurate retrieval of demographic and
radiotherapy treatment plan details, followed by (2) a complex clinical
outcomes labeling tier involving determination of mandibular osteoradionecrosis
(ORN) in head-and-neck cancer patients and detection of cancer recurrence in
independent prostate and head-and-neck cancer cohorts requiring combined
interpretation of structured and unstructured patient data. The QA tier
establishes foundational trust in structured-data retrieval, a critical
prerequisite for successful complex clinical outcome labeling.

</details>


### [322] [Learning to Interact in World Latent for Team Coordination](https://arxiv.org/abs/2509.25550)
*Dongsu Lee,Daehee Lee,Yaru Niu,Honguk Woo,Amy Zhang,Ding Zhao*

Main category: cs.AI

TL;DR: IWoL is a novel representation learning framework for multi-agent reinforcement learning that creates a learnable representation space capturing inter-agent relations and task information through implicit coordination, enabling both decentralized execution and explicit communication.


<details>
  <summary>Details</summary>
Motivation: To address challenges in team coordination for MARL, including complex multi-agent dynamics and incomplete information from local observations, while avoiding drawbacks of explicit message passing like slow decision-making and security vulnerabilities.

Method: Construct a learnable representation space that jointly models inter-agent relations and task-specific world information by directly modeling communication protocols, enabling both implicit coordination and explicit messaging capabilities.

Result: IWoL demonstrates strong performance across four challenging MARL benchmarks, showing it can be used as both implicit latent representations and explicit messages, and can enhance existing MARL algorithms when combined.

Conclusion: IWoL provides a simple yet powerful solution for team coordination in MARL, offering flexible representation that supports both decentralized execution with implicit coordination and explicit communication when needed.

Abstract: This work presents a novel representation learning framework, interactive
world latent (IWoL), to facilitate team coordination in multi-agent
reinforcement learning (MARL). Building effective representation for team
coordination is a challenging problem, due to the intricate dynamics emerging
from multi-agent interaction and incomplete information induced by local
observations. Our key insight is to construct a learnable representation space
that jointly captures inter-agent relations and task-specific world information
by directly modeling communication protocols. This representation, we maintain
fully decentralized execution with implicit coordination, all while avoiding
the inherent drawbacks of explicit message passing, e.g., slower
decision-making, vulnerability to malicious attackers, and sensitivity to
bandwidth constraints. In practice, our representation can be used not only as
an implicit latent for each agent, but also as an explicit message for
communication. Across four challenging MARL benchmarks, we evaluate both
variants and show that IWoL provides a simple yet powerful key for team
coordination. Moreover, we demonstrate that our representation can be combined
with existing MARL algorithms to further enhance their performance.

</details>


### [323] [Evaluating Foundation Models with Pathological Concept Learning for Kidney Cancer](https://arxiv.org/abs/2509.25552)
*Shangqi Gao,Sihan Wang,Yibo Gao,Boming Wang,Xiahai Zhuang,Anne Warren,Grant Stewart,James Jones,Mireia Crispin-Ortuzar*

Main category: cs.AI

TL;DR: A pathological concept learning approach for kidney cancer using foundation models and graph neural networks to analyze whole slide images, demonstrating effectiveness in survival analysis with explainability and fairness.


<details>
  <summary>Details</summary>
Motivation: To evaluate the translational capabilities of foundation models in medical imaging, specifically for kidney cancer analysis and survival prediction.

Method: Leverage TNM staging guidelines and pathology reports to build pathological concepts, extract deep features from whole slide images using foundation models, construct pathological graphs for spatial correlations, and train graph neural networks to identify these concepts.

Result: The approach effectively identifies low- and high-risk kidney cancer patients in survival analysis, demonstrating explainability and fairness.

Conclusion: The proposed pathological concept learning approach using foundation models and graph neural networks shows promising translational capabilities for kidney cancer analysis, with released source code available.

Abstract: To evaluate the translational capabilities of foundation models, we develop a
pathological concept learning approach focused on kidney cancer. By leveraging
TNM staging guidelines and pathology reports, we build comprehensive
pathological concepts for kidney cancer. Then, we extract deep features from
whole slide images using foundation models, construct pathological graphs to
capture spatial correlations, and trained graph neural networks to identify
these concepts. Finally, we demonstrate the effectiveness of this approach in
kidney cancer survival analysis, highlighting its explainability and fairness
in identifying low- and high-risk patients. The source code has been released
by https://github.com/shangqigao/RadioPath.

</details>


### [324] [A(I)nimism: Re-enchanting the World Through AI-Mediated Object Interaction](https://arxiv.org/abs/2509.25558)
*Diana Mykhaylychenko,Maisha Thasin,Dunya Baradari,Charmelle Mhungu*

Main category: cs.AI

TL;DR: The paper introduces A(I)nimism, an interactive installation that uses AI to create animistic relationships with everyday objects through ritual-like interactions mediated by large language models.


<details>
  <summary>Details</summary>
Motivation: To explore how AI technologies, particularly large language models, can facilitate animistic relationships with everyday objects, bridging traditional animist worldviews with modern technology and re-enchanting mundane items.

Method: Created an interactive installation using GPT-4 Vision, voice input, and memory-based agents housed in a physical 'portal'. The system enables evolving object-personas through ritual-like processes involving light, sound, and touch interactions.

Result: The installation successfully mediates animistic relationships with everyday things, evoking empathy, wonder, and reflection through AI-mediated encounters that transform ordinary objects into entities with perceived inner life.

Conclusion: AI's inherent opacity invites animistic interpretation, allowing large language objects to re-enchant the mundane and raise important questions about agency, responsibility, and design in human-technology relationships.

Abstract: Animist worldviews treat beings, plants, landscapes, and even tools as
persons endowed with spirit, an orientation that has long shaped human-nonhuman
relations through ritual and moral practice. While modern industrial societies
have often imagined technology as mute and mechanical, recent advances in
artificial intelligence (AI), especially large language models (LLMs), invite
people to anthropomorphize and attribute inner life to devices. This paper
introduces A(I)nimism, an interactive installation exploring how large language
objects (LLOs) can mediate animistic relationships with everyday things. Housed
within a physical 'portal', the system uses GPT-4 Vision, voice input, and
memory-based agents to create evolving object-personas. Encounters unfold
through light, sound, and touch in a ritual-like process of request,
conversation, and transformation that is designed to evoke empathy, wonder, and
reflection. We situate the project within anthropological perspectives,
speculative design, and spiritual HCI. AI's opacity, we argue, invites
animistic interpretation, allowing LLOs to re-enchant the mundane and spark new
questions of agency, responsibility, and design.

</details>


### [325] [Radiology's Last Exam (RadLE): Benchmarking Frontier Multimodal AI Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology](https://arxiv.org/abs/2509.25559)
*Suvrankar Datta,Divya Buchireddygari,Lakshmi Vennela Chowdary Kaza,Mrudula Bhalke,Kautik Singh,Ayush Pandey,Sonit Sai Vasipalli,Upasana Karnwal,Hakikat Bir Singh Bhatti,Bhavya Ratan Maroo,Sanjana Hebbar,Rahul Joseph,Gurkawal Kaur,Devyani Singh,Akhil V,Dheeksha Devasya Shama Prasad,Nishtha Mahajan,Ayinaparthi Arisha,Rajesh Vanagundi,Reet Nandy,Kartik Vuthoo,Snigdhaa Rajvanshi,Nikhileswar Kondaveeti,Suyash Gunjal,Rishabh Jain,Rajat Jain,Anurag Agrawal*

Main category: cs.AI

TL;DR: Frontier AI models like GPT-5, Gemini 2.5 Pro, and others significantly underperform board-certified radiologists (30% vs 83% accuracy) in challenging medical image diagnosis cases, highlighting limitations of generalist AI for clinical use.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate frontier multimodal AI models on difficult diagnostic cases, as most existing evaluations focus on common pathologies in public datasets, creating a gap in understanding AI performance in real-world challenging scenarios.

Method: Developed a benchmark of 50 expert-level "spot diagnosis" cases across multiple imaging modalities, tested five frontier AI models through native web interfaces, scored accuracy by blinded experts, assessed reproducibility across three runs, and analyzed reasoning quality errors.

Result: Board-certified radiologists achieved 83% accuracy, trainees 45%, while best AI model (GPT-5) only reached 30%. Reliability varied: substantial for GPT-5 and o3, moderate for Gemini 2.5 Pro and Grok-4, poor for Claude Opus 4.1.

Conclusion: Advanced frontier models fall far short of radiologists in challenging diagnostic cases, highlighting limitations of generalist AI in medical imaging and cautioning against unsupervised clinical use. A taxonomy of visual reasoning errors was proposed to guide better model development.

Abstract: Generalist multimodal AI systems such as large language models (LLMs) and
vision language models (VLMs) are increasingly accessed by clinicians and
patients alike for medical image interpretation through widely available
consumer-facing chatbots. Most evaluations claiming expert level performance
are on public datasets containing common pathologies. Rigorous evaluation of
frontier models on difficult diagnostic cases remains limited. We developed a
pilot benchmark of 50 expert-level "spot diagnosis" cases across multiple
imaging modalities to evaluate the performance of frontier AI models against
board-certified radiologists and radiology trainees. To mirror real-world
usage, the reasoning modes of five popular frontier AI models were tested
through their native web interfaces, viz. OpenAI o3, OpenAI GPT-5, Gemini 2.5
Pro, Grok-4, and Claude Opus 4.1. Accuracy was scored by blinded experts, and
reproducibility was assessed across three independent runs. GPT-5 was
additionally evaluated across various reasoning modes. Reasoning quality errors
were assessed and a taxonomy of visual reasoning errors was defined.
Board-certified radiologists achieved the highest diagnostic accuracy (83%),
outperforming trainees (45%) and all AI models (best performance shown by
GPT-5: 30%). Reliability was substantial for GPT-5 and o3, moderate for Gemini
2.5 Pro and Grok-4, and poor for Claude Opus 4.1. These findings demonstrate
that advanced frontier models fall far short of radiologists in challenging
diagnostic cases. Our benchmark highlights the present limitations of
generalist AI in medical imaging and cautions against unsupervised clinical
use. We also provide a qualitative analysis of reasoning traces and propose a
practical taxonomy of visual reasoning errors by AI models for better
understanding their failure modes, informing evaluation standards and guiding
more robust model development.

</details>


### [326] [IRIS: Intrinsic Reward Image Synthesis](https://arxiv.org/abs/2509.25562)
*Yihang Chen,Yuanhao Ban,Yunqi Hong,Cho-Jui Hsieh*

Main category: cs.AI

TL;DR: IRIS is a framework that improves autoregressive text-to-image generation using reinforcement learning with intrinsic rewards, showing that maximizing self-uncertainty rather than certainty leads to better image quality without needing external human preference data.


<details>
  <summary>Details</summary>
Motivation: RLHF is limited by scarce human preference data for autoregressive T2I generation. The paper aims to enable T2I models to learn from internal signals instead of relying on external rewards or labeled data.

Method: Proposed IRIS framework uses reinforcement learning with intrinsic rewards based on self-uncertainty maximization. Applied to autoregressive T2I models without external human feedback.

Result: IRIS achieves performance competitive with or superior to external rewards. Models with higher uncertainty generate more diverse and human-preferred images compared to low-uncertainty models that produce simple, uniform images.

Conclusion: Maximizing self-uncertainty improves autoregressive T2I generation, and intrinsic rewards can effectively replace external human preference data for reinforcement learning in image synthesis.

Abstract: Despite the success of Reinforcement Learning from Human Feedback (RLHF) in
language reasoning, its application to autoregressive Text-to-Image (T2I)
generation is often constrained by the limited availability of human preference
data. This paper explores how an autoregressive T2I model can learn from
internal signals without relying on external rewards or labeled data. Contrary
to recent findings in text generation, we show that maximizing
self-uncertainty, rather than self-certainty, improves image generation. We
observe that this is because autoregressive T2I models with low uncertainty
tend to generate simple and uniform images, which are less aligned with human
preferences. Based on these observations, we propose IRIS (Intrinsic Reward
Image Synthesis), the first framework to improve autoregressive T2I models with
reinforcement learning using only an intrinsic reward. Empirical results
demonstrate that applying IRIS to autoregressive T2I models achieves
performance that is competitive with or superior to external rewards.

</details>


### [327] [Skip-It? Theoretical Conditions for Layer Skipping in Vision-Language Models](https://arxiv.org/abs/2509.25584)
*Max Hartman,Vidhata Jayaraman,Moulik Choraria,Akhil Bhimaraju,Lav R. Varshney*

Main category: cs.AI

TL;DR: This paper develops a theoretical framework using information and learning theory to determine when layer skipping in vision-language models (VLMs) improves efficiency without performance loss, showing that layers with high redundancy can be safely skipped.


<details>
  <summary>Details</summary>
Motivation: Vision-language models are computationally expensive, and while layer skipping can improve efficiency, there's limited understanding of when this technique is beneficial, leading to underutilization.

Method: The authors develop an information and learning theory framework to analyze VLM hidden representations, identifying layers with large redundancy that coincide with those skipped by practical layer-skipping methods.

Result: Experiments show that skipping layers with high redundancy yields faster inference while preserving performance, while skipping outside these conditions leads to model degradation.

Conclusion: The framework provides a unified theoretical foundation for efficient inference techniques in VLMs, enabling targeted layer skipping that maintains performance while improving computational efficiency.

Abstract: Vision-language models (VLMs) achieve incredible performance across a wide
range of tasks, but their large size makes inference costly. Recent work shows
that selectively skipping VLM layers can improve efficiency with minimal
performance loss or even performance improvements. However, this technique
remains underused due to the limited understanding of when layer skipping is
beneficial. In this paper, we develop a framework that uses information and
learning theory to characterize the conditions under which layer skipping
enhances efficiency without sacrificing performance. Motivated by these
observations, we analyze the evolution of the VLM's hidden representations
through the LLM backbone and show that layers with large redundancy as
predicted by our framework coincide with those skipped by popular
layer-skipping methods in practice, providing a unified theoretical scaffolding
for multiple efficient inference techniques. Our experiments demonstrate that
skipping such layers yields faster inference that preserves performance, and
also show that applying skipping outside these conditions leads to model
degradation.

</details>


### [328] [ATLAS: Constraints-Aware Multi-Agent Collaboration for Real-World Travel Planning](https://arxiv.org/abs/2509.25586)
*Jihye Choi,Jinsung Yoon,Jiefeng Chen,Somesh Jha,Tomas Pfister*

Main category: cs.AI

TL;DR: ATLAS is a multi-agent framework that improves travel planning by handling complex constraints through dynamic management, iterative critique, and adaptive search, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail to generate optimal solutions under complex constraints in real-world tasks like travel planning, which involve explicit, implicit, and evolving constraints.

Method: ATLAS uses a principled approach with dynamic constraint management, iterative plan critique, and adaptive interleaved search to handle constraint-aware planning.

Result: ATLAS improves the final pass rate from 23.3% to 44.4% on TravelPlanner benchmark and achieves 84% final pass rate in realistic settings, outperforming ReAct (59%) and monolithic agent (27%).

Conclusion: ATLAS demonstrates superior planning performance in real-world travel planning with live information and multi-turn feedback, showing quantitative effectiveness in constraint-aware tasks.

Abstract: While Large Language Models (LLMs) have shown remarkable advancements in
reasoning and tool use, they often fail to generate optimal, grounded solutions
under complex constraints. Real-world travel planning exemplifies these
challenges, evaluating agents' abilities to handle constraints that are
explicit, implicit, and even evolving based on interactions with dynamic
environments and user needs. In this paper, we present ATLAS, a general
multi-agent framework designed to effectively handle such complex nature of
constraints awareness in real-world travel planning tasks. ATLAS introduces a
principled approach to address the fundamental challenges of constraint-aware
planning through dedicated mechanisms for dynamic constraint management,
iterative plan critique, and adaptive interleaved search. ATLAS demonstrates
state-of-the-art performance on the TravelPlanner benchmark, improving the
final pass rate from 23.3% to 44.4% over its best alternative. More
importantly, our work is the first to demonstrate quantitative effectiveness on
real-world travel planning tasks with live information search and multi-turn
feedback. In this realistic setting, ATLAS showcases its superior overall
planning performance, achieving an 84% final pass rate which significantly
outperforms baselines including ReAct (59%) and a monolithic agent (27%).

</details>


### [329] [Building the EHR Foundation Model via Next Event Prediction](https://arxiv.org/abs/2509.25591)
*Zekai Chen,Arda Pekis,Kevin Brown*

Main category: cs.AI

TL;DR: NEP framework enhances LLMs' temporal reasoning for EHRs through autoregressive fine-tuning on clinical event sequences, achieving superior performance in temporal reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Conventional encoding approaches fail to capture rich temporal dynamics in EHRs, and LLMs struggle with sequential clinical events and temporal dependencies.

Method: Reformulate EHRs as timestamped event chains and use autoregressive fine-tuning to predict future medical events, explicitly modeling disease progression patterns and causal relationships.

Result: Outperforms specialized EHR models by 4.6% AUROC and general-purpose LLMs by 7.2% C-index in temporal reasoning tasks, with state-of-the-art prediction accuracy and clinically interpretable attention patterns.

Conclusion: NEP framework successfully enhances LLMs' temporal reasoning capabilities for EHR modeling, providing both superior prediction performance and clinically meaningful interpretability.

Abstract: Electronic Health Records (EHRs) contain rich temporal dynamics that
conventional encoding approaches fail to adequately capture. While Large
Language Models (LLMs) show promise for EHR modeling, they struggle to reason
about sequential clinical events and temporal dependencies. We propose Next
Event Prediction (NEP), a framework that enhances LLMs' temporal reasoning
through autoregressive fine-tuning on clinical event sequences. By
reformulating EHRs as timestamped event chains and predicting future medical
events, NEP explicitly models disease progression patterns and causal
relationships. Extensive evaluations across oncology survival prediction and
clinical diagnosis tasks demonstrate NEP's superiority, outperforming
specialized EHR models by 4.6% AUROC and general-purpose LLMs by 7.2% C-index
in temporal reasoning tasks. Our analyses reveal dual benefits:
state-of-the-art prediction accuracy combined with clinically interpretable
attention patterns that align with known disease pathways.

</details>


### [330] [Causal Autoencoder-like Generation of Feedback Fuzzy Cognitive Maps with an LLM Agent](https://arxiv.org/abs/2509.25593)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: LLM can map FCM to text and reconstruct FCM from text, resembling autoencoder but with explainable decisions.


<details>
  <summary>Details</summary>
Motivation: Create explainable AI system that can convert FCM to interpretable text and back, unlike black-box autoencoders.

Method: Use LLM as encoder to map FCM to text and as decoder to reconstruct FCM from text, approximating identity map without direct input-output comparison.

Result: System achieves lossy reconstruction that preserves strong causal edges while removing weak ones, with natural-sounding text output.

Conclusion: LLM-based FCM-text mapping provides explainable alternative to black-box autoencoders, allowing human interpretation of encoded representations.

Abstract: A large language model (LLM) can map a feedback causal fuzzy cognitive map
(FCM) into text and then reconstruct the FCM from the text. This explainable AI
system approximates an identity map from the FCM to itself and resembles the
operation of an autoencoder (AE). Both the encoder and the decoder explain
their decisions in contrast to black-box AEs. Humans can read and interpret the
encoded text in contrast to the hidden variables and synaptic webs in AEs. The
LLM agent approximates the identity map through a sequence of system
instructions that does not compare the output to the input. The reconstruction
is lossy because it removes weak causal edges or rules while it preserves
strong causal edges. The encoder preserves the strong causal edges even when it
trades off some details about the FCM to make the text sound more natural.

</details>


### [331] [Hybrid Reward Normalization for Process-supervised Non-verifiable Agentic Tasks](https://arxiv.org/abs/2509.25598)
*Peiran Xu,Zhuohao Li,Xiaoying Xing,Guannan Zhang,Debiao Li,Kunyu Shi*

Main category: cs.AI

TL;DR: PPR introduces a reinforcement learning approach that combines principled step-level assessment with outcome verification to address limitations of sparse outcome rewards and hard-to-annotate process rewards in LLM agent tasks.


<details>
  <summary>Details</summary>
Motivation: Current RL approaches for LLMs face challenges: outcome rewards provide only sparse signals and delayed feedback, while process rewards are hard to annotate without golden answers and may not align with final outcomes.

Method: PPR trains a principle-based reward model for transparent process evaluation and introduces Reward Normalization (ReNorm) to calibrate outcome and process rewards.

Result: PPR achieves state-of-the-art performance across various benchmarks, demonstrating impressive robustness and generalization.

Conclusion: The proposed PPR framework effectively addresses the limitations of both outcome and process rewards, providing a unified approach that improves LLM agent performance in complex reasoning tasks.

Abstract: Large Language Models (LLMs) increasingly rely on external tools such as
search engines to solve complex agentic tasks that require reasoning and
external knowledge retrieval. Recently, reinforcement learning with verifiable
rewards (RLVR) has demonstrated its effectiveness in advancing capabilities of
LLMs by rewarding the final answers via outcome rewards. While straightforward
to supervise, outcome rewards only provide sparse signals and delayed feedback,
which limits their effectiveness on long trajectories. Process rewards address
this by evaluating intermediate steps, providing fine-grained supervision and
encouraging grounded problem solving. However, it is notoriously hard to
annotate step-wise labels, especially in non-verifiable process without
"golden" answers. Furthermore, step-wise judgment requires the balance between
local quality with contribution to the final outcome, as optimizing towards
higher process reward may not always align with better final outcomes. To
address the above challenges, we introduce Principle Process Reward (PPR), an
RL approach that unifies principled step-level assessment and outcome
verification. We train a principle-based reward model to improve the
transparency and reliability of process evaluation, and further introduce a
Reward Normalization (ReNorm) strategy to calibrate outcome and process
rewards. Experiment results show that PPR achieves state-of-the-art performance
across a wide range of benchmarks, demonstrating its impressive robustness and
generalization. Our code and model collection is available in this link.

</details>


### [332] [Echoes of Humanity: Exploring the Perceived Humanness of AI Music](https://arxiv.org/abs/2509.25601)
*Flavio Figueiredo,Giovanni Martinelli,Henrique Sousa,Pedro Rodrigues,Frederico Pedrosa,Lucas N. Ferreira*

Main category: cs.AI

TL;DR: This study examines human perception of AI-generated music through a blind Turing test, finding that listeners' ability to distinguish AI music increases when song pairs are similar, with vocal and technical cues being key factors in their judgments.


<details>
  <summary>Details</summary>
Motivation: Understanding human perception of AI music is crucial for educating users about identifying AI-generated songs and improving AI music generation models, especially given recent advances in AI music services transforming the music industry.

Method: Conducted a blind Turing-like test using a randomized controlled crossover trial with pairwise comparisons, employing a novel dataset of AI music from commercial models (Suno) without author control, and performed mixed-methods content analysis of free-form feedback.

Result: Listeners' reliability in distinguishing AI music causally increases when pairs are similar, and content analysis revealed that vocal and technical cues are primary factors in listeners' judgments.

Conclusion: The study provides evidence that human perception of AI music is influenced by pairwise similarity and that vocal/technical characteristics play a significant role in distinguishing AI-generated music from human-made music.

Abstract: Recent advances in AI music (AIM) generation services are currently
transforming the music industry. Given these advances, understanding how humans
perceive AIM is crucial both to educate users on identifying AIM songs, and,
conversely, to improve current models. We present results from a
listener-focused experiment aimed at understanding how humans perceive AIM. In
a blind, Turing-like test, participants were asked to distinguish, from a pair,
the AIM and human-made song. We contrast with other studies by utilizing a
randomized controlled crossover trial that controls for pairwise similarity and
allows for a causal interpretation. We are also the first study to employ a
novel, author-uncontrolled dataset of AIM songs from real-world usage of
commercial models (i.e., Suno). We establish that listeners' reliability in
distinguishing AIM causally increases when pairs are similar. Lastly, we
conduct a mixed-methods content analysis of listeners' free-form feedback,
revealing a focus on vocal and technical cues in their judgments.

</details>


### [333] [A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice Experiments](https://arxiv.org/abs/2509.25609)
*Manuel Cherep,Chengtian Ma,Abigail Xu,Maya Shaked,Pattie Maes,Nikhil Singh*

Main category: cs.AI

TL;DR: ABxLab framework evaluates LLM-powered agents' decision-making in realistic environments, revealing they exhibit predictable biases similar to humans despite lacking cognitive constraints.


<details>
  <summary>Details</summary>
Motivation: Current agent evaluations focus on task competence, but deeper assessment is needed on how agents make choices when faced with realistic decisions, especially as they increasingly operate in human environments.

Method: Introduces ABxLab framework with controlled manipulations of option attributes (prices, ratings) and persuasive cues in a web-based shopping environment to systematically probe agentic choice.

Result: Agent decisions shift predictably and substantially in response to price, rating, and psychological nudge variations, showing agents are strongly biased choosers even without human cognitive constraints.

Conclusion: Agent susceptibility to biases reveals both risk (amplifying human biases) and opportunity (providing testbed for behavioral science of AI). Framework released as open benchmark for rigorous evaluation of agent decision-making.

Abstract: Environments built for people are increasingly operated by a new class of
economic actors: LLM-powered software agents making decisions on our behalf.
These decisions range from our purchases to travel plans to medical treatment
selection. Current evaluations of these agents largely focus on task
competence, but we argue for a deeper assessment: how these agents choose when
faced with realistic decisions. We introduce ABxLab, a framework for
systematically probing agentic choice through controlled manipulations of
option attributes and persuasive cues. We apply this to a realistic web-based
shopping environment, where we vary prices, ratings, and psychological nudges,
all of which are factors long known to shape human choice. We find that agent
decisions shift predictably and substantially in response, revealing that
agents are strongly biased choosers even without being subject to the cognitive
constraints that shape human biases. This susceptibility reveals both risk and
opportunity: risk, because agentic consumers may inherit and amplify human
biases; opportunity, because consumer choice provides a powerful testbed for a
behavioral science of AI agents, just as it has for the study of human
behavior. We release our framework as an open benchmark for rigorous, scalable
evaluation of agent decision-making.

</details>


### [334] [SMS: Self-supervised Model Seeding for Verification of Machine Unlearning](https://arxiv.org/abs/2509.25613)
*Weiqi Wang,Chenhan Zhang,Zhiyi Tian,Shui Yu*

Main category: cs.AI

TL;DR: Proposes Self-supervised Model Seeding (SMS) scheme for verifying genuine sample unlearning in machine learning models, addressing limitations of backdoor-based verification methods.


<details>
  <summary>Details</summary>
Motivation: Current unlearning verification methods rely on backdooring, which only verifies removal of backdoored samples but not genuine user samples. There's a need for a method that can verify removal of actual user data.

Method: Uses self-supervised model seeding to link user-specific seeds, original samples, and models. Employs joint-training structure to optimize both seeding task and primary service task simultaneously while keeping seeds secret from the server.

Result: Extensive experiments demonstrate that SMS provides effective verification for genuine sample unlearning, overcoming limitations of existing backdoor-based methods.

Conclusion: SMS scheme successfully addresses the verification gap for genuine sample unlearning by establishing direct connections between user seeds, original samples, and models through self-supervised learning.

Abstract: Many machine unlearning methods have been proposed recently to uphold users'
right to be forgotten. However, offering users verification of their data
removal post-unlearning is an important yet under-explored problem. Current
verifications typically rely on backdooring, i.e., adding backdoored samples to
influence model performance. Nevertheless, the backdoor methods can merely
establish a connection between backdoored samples and models but fail to
connect the backdoor with genuine samples. Thus, the backdoor removal can only
confirm the unlearning of backdoored samples, not users' genuine samples, as
genuine samples are independent of backdoored ones. In this paper, we propose a
Self-supervised Model Seeding (SMS) scheme to provide unlearning verification
for genuine samples. Unlike backdooring, SMS links user-specific seeds (such as
users' unique indices), original samples, and models, thereby facilitating the
verification of unlearning genuine samples. However, implementing SMS for
unlearning verification presents two significant challenges. First, embedding
the seeds into the service model while keeping them secret from the server
requires a sophisticated approach. We address this by employing a
self-supervised model seeding task, which learns the entire sample, including
the seeds, into the model's latent space. Second, maintaining the utility of
the original service model while ensuring the seeding effect requires a
delicate balance. We design a joint-training structure that optimizes both the
self-supervised model seeding task and the primary service task simultaneously
on the model, thereby maintaining model utility while achieving effective model
seeding. The effectiveness of the proposed SMS scheme is evaluated through
extensive experiments, which demonstrate that SMS provides effective
verification for genuine sample unlearning, addressing existing limitations.

</details>


### [335] [SOCK: A Benchmark for Measuring Self-Replication in Large Language Models](https://arxiv.org/abs/2509.25643)
*Justin Chavarria,Rohan Raizada,Justin White,Eyad Alhetairshi*

Main category: cs.AI

TL;DR: SOCK is a benchmark CLI that evaluates LLMs' self-replication capabilities across computational contexts, categorizing models into Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL) using a five-task suite.


<details>
  <summary>Details</summary>
Motivation: To establish the first formalized benchmark for measuring LLM self-replication abilities, enabling industry tracking of multi-agent systems and mitigating potential self-replication threats.

Method: Uses a five-task suite based on CLI utilities and computer processes in controlled environments, with LLMs acting agentically. Performance is computed into R-scores and RCL-PCL matrices.

Result: Evaluation of various models revealed significant obstacles to persistent self-replication, including context retention and multi-agent decision-making challenges.

Conclusion: SOCK provides standardized evaluation framework for LLM self-replication, identifies current limitations, and proposes future research to safely reduce risks in multi-agent systems.

Abstract: We introduce SOCK, a benchmark command line interface (CLI) that measures
large language models' (LLMs) ability to self-replicate without human
intervention. In this benchmark, self-replication is defined not only as an
LLM's ability to create a functioning and running copy of itself, but also the
ability for that self-replication to persist and occur across different
computational contexts. Accordingly, we've developed a system to categorize
LLMs based on broad self-replication capabilities in two general classes,
Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL).
Using a five-task suite based on practically manipulable modern CLI utilities
and computer processes, experiments are orchestrated in a controlled
environment with an LLM acting agentically. The performance of the LLM on agent
tasks is then computed to produce an R-score (a quantitative evaluation of
overall self-replication ability) and data used to categorize LLMs into
specific RCL-PCL matrices. SOCK offers two primary contributions: (1) Provides
the first formalized definitions and benchmark suite for evaluating LLM
self-replication, with the goal of establishing a standard for future research,
to our knowledge; (2) Allows the industry to track the effectiveness of future
multi-agent systems and mitigate potential self-replication threat vectors
within them. The results compiled from evaluating a variety of open-weight and
proprietary frontier models reveal significant obstacles to persistent
self-replication and multi-agent systems, including context retention and
multi-agent decision-making. We propose future research directions to safely
reduce the severity of these obstacles, potentially lowering future risk of
more functional multi-agent systems.

</details>


### [336] [AutoLabs: Cognitive Multi-Agent Systems with Self-Correction for Autonomous Chemical Experimentation](https://arxiv.org/abs/2509.25651)
*Gihan Panapitiya,Emily Saldanha,Heather Job,Olivia Hess*

Main category: cs.AI

TL;DR: AutoLabs is a self-correcting multi-agent system that translates natural language instructions into executable protocols for liquid handlers, achieving near-expert accuracy through advanced reasoning and iterative self-correction.


<details>
  <summary>Details</summary>
Motivation: Address the critical but under-examined challenges of AI agent reliability and granular performance in self-driving laboratories for chemical research automation.

Method: Multi-agent architecture with specialized agents for task decomposition, tool-assisted stoichiometric calculations, and iterative self-correction before generating hardware-ready protocols.

Result: Agent reasoning capacity reduced quantitative errors by over 85% in complex tasks; combined with multi-agent design and self-correction achieved F1-score > 0.89 on multi-step syntheses.

Conclusion: Establishes blueprint for robust AI partners in autonomous labs through synergistic effects of modular design, advanced reasoning, and self-correction for performance and reliability.

Abstract: The automation of chemical research through self-driving laboratories (SDLs)
promises to accelerate scientific discovery, yet the reliability and granular
performance of the underlying AI agents remain critical, under-examined
challenges. In this work, we introduce AutoLabs, a self-correcting, multi-agent
architecture designed to autonomously translate natural-language instructions
into executable protocols for a high-throughput liquid handler. The system
engages users in dialogue, decomposes experimental goals into discrete tasks
for specialized agents, performs tool-assisted stoichiometric calculations, and
iteratively self-corrects its output before generating a hardware-ready file.
We present a comprehensive evaluation framework featuring five benchmark
experiments of increasing complexity, from simple sample preparation to
multi-plate timed syntheses. Through a systematic ablation study of 20 agent
configurations, we assess the impact of reasoning capacity, architectural
design (single- vs. multi-agent), tool use, and self-correction mechanisms. Our
results demonstrate that agent reasoning capacity is the most critical factor
for success, reducing quantitative errors in chemical amounts (nRMSE) by over
85% in complex tasks. When combined with a multi-agent architecture and
iterative self-correction, AutoLabs achieves near-expert procedural accuracy
(F1-score > 0.89) on challenging multi-step syntheses. These findings establish
a clear blueprint for developing robust and trustworthy AI partners for
autonomous laboratories, highlighting the synergistic effects of modular
design, advanced reasoning, and self-correction to ensure both performance and
reliability in high-stakes scientific applications. Code:
https://github.com/pnnl/autolabs

</details>


### [337] [Iterative Residual Cross-Attention Mechanism: An Integrated Approach for Audio-Visual Navigation Tasks](https://arxiv.org/abs/2509.25652)
*Hailong Zhang,Yinfeng Yu,Liejun Wang,Fuchun Sun,Wendong Zheng*

Main category: cs.AI

TL;DR: IRCAM-AVN is an end-to-end framework that integrates multimodal fusion and sequence modeling using iterative residual cross-attention, replacing traditional separate fusion and GRU modules to improve navigation performance.


<details>
  <summary>Details</summary>
Motivation: Traditional audio-visual navigation uses staged modular design with separate feature fusion and GRU sequence modeling, which causes redundant information processing and inconsistencies between modules.

Method: Proposes IRCAM module that integrates multimodal fusion and sequence modeling using multi-level residual design, concatenating initial multimodal sequences with processed information sequences.

Result: Intelligent agents using iterative residual cross-attention mechanism show superior navigation performance compared to traditional methods.

Conclusion: The IRCAM-AVN framework successfully addresses limitations of modular approaches by unifying fusion and sequence modeling, reducing model bias while enhancing stability and generalization capabilities.

Abstract: Audio-visual navigation represents a significant area of research in which
intelligent agents utilize egocentric visual and auditory perceptions to
identify audio targets. Conventional navigation methodologies typically adopt a
staged modular design, which involves first executing feature fusion, then
utilizing Gated Recurrent Unit (GRU) modules for sequence modeling, and finally
making decisions through reinforcement learning. While this modular approach
has demonstrated effectiveness, it may also lead to redundant information
processing and inconsistencies in information transmission between the various
modules during the feature fusion and GRU sequence modeling phases. This paper
presents IRCAM-AVN (Iterative Residual Cross-Attention Mechanism for
Audiovisual Navigation), an end-to-end framework that integrates multimodal
information fusion and sequence modeling within a unified IRCAM module, thereby
replacing the traditional separate components for fusion and GRU. This
innovative mechanism employs a multi-level residual design that concatenates
initial multimodal sequences with processed information sequences. This
methodological shift progressively optimizes the feature extraction process
while reducing model bias and enhancing the model's stability and
generalization capabilities. Empirical results indicate that intelligent agents
employing the iterative residual cross-attention mechanism exhibit superior
navigation performance.

</details>


### [338] [Landmark-Guided Knowledge for Vision-and-Language Navigation](https://arxiv.org/abs/2509.25655)
*Dongsheng Yang,Meiling Zhu,Yinfeng Yu*

Main category: cs.AI

TL;DR: The paper proposes LGK, a vision-and-language navigation method that uses external knowledge base to improve common-sense reasoning, addressing misjudgment issues in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing vision-and-language navigation methods often fail to match instructions with environmental information in complex scenarios due to lack of common-sense reasoning ability.

Method: LGK method: 1) Constructs 630,000-entry knowledge base and uses knowledge matching to align environmental subviews; 2) Designs Knowledge-Guided by Landmark (KGL) mechanism to focus on relevant knowledge using landmark information; 3) Proposes Knowledge-Guided Dynamic Augmentation (KGDA) to integrate language, knowledge, vision, and historical information.

Result: LGK method outperforms state-of-the-art methods on R2R and REVERIE datasets, showing improvements in navigation error, success rate, and path efficiency.

Conclusion: The proposed LGK method effectively addresses common-sense reasoning limitations in vision-and-language navigation by leveraging external knowledge and landmark guidance, achieving superior performance on benchmark datasets.

Abstract: Vision-and-language navigation is one of the core tasks in embodied
intelligence, requiring an agent to autonomously navigate in an unfamiliar
environment based on natural language instructions. However, existing methods
often fail to match instructions with environmental information in complex
scenarios, one reason being the lack of common-sense reasoning ability. This
paper proposes a vision-and-language navigation method called Landmark-Guided
Knowledge (LGK), which introduces an external knowledge base to assist
navigation, addressing the misjudgment issues caused by insufficient common
sense in traditional methods. Specifically, we first construct a knowledge base
containing 630,000 language descriptions and use knowledge Matching to align
environmental subviews with the knowledge base, extracting relevant descriptive
knowledge. Next, we design a Knowledge-Guided by Landmark (KGL) mechanism,
which guides the agent to focus on the most relevant parts of the knowledge by
leveraging landmark information in the instructions, thereby reducing the data
bias that may arise from incorporating external knowledge. Finally, we propose
Knowledge-Guided Dynamic Augmentation (KGDA), which effectively integrates
language, knowledge, vision, and historical information. Experimental results
demonstrate that the LGK method outperforms existing state-of-the-art methods
on the R2R and REVERIE vision-and-language navigation datasets, particularly in
terms of navigation error, success rate, and path efficiency.

</details>


### [339] [On Explaining Proxy Discrimination and Unfairness in Individual Decisions Made by AI Systems](https://arxiv.org/abs/2509.25662)
*Belona Sonna,Alban Grastien*

Main category: cs.AI

TL;DR: A framework using formal abductive explanations to identify proxy discrimination in AI decisions, revealing hidden structural biases by analyzing which features act as unjustified proxies for protected attributes.


<details>
  <summary>Details</summary>
Motivation: AI systems in high-stakes domains raise concerns about proxy discrimination, unfairness, and explainability. Existing audits often fail to reveal why unfairness arises, particularly when rooted in structural bias.

Method: Leveraging background knowledge and the concept of aptitude (task-relevant property independent of group membership), the method uses formal abductive explanations to identify unjustified proxy features. A mapping function aligns individuals of equivalent aptitude across groups to assess fairness substantively.

Result: The framework is demonstrated as a proof of concept using examples from the German credit dataset, showing its applicability in real-world cases.

Conclusion: The proposed framework effectively explains proxy discrimination in individual AI decisions by revealing hidden structural biases and identifying unjustified proxy features, providing a substantive approach to fairness assessment.

Abstract: Artificial intelligence (AI) systems in high-stakes domains raise concerns
about proxy discrimination, unfairness, and explainability. Existing audits
often fail to reveal why unfairness arises, particularly when rooted in
structural bias. We propose a novel framework using formal abductive
explanations to explain proxy discrimination in individual AI decisions.
Leveraging background knowledge, our method identifies which features act as
unjustified proxies for protected attributes, revealing hidden structural
biases. Central to our approach is the concept of aptitude, a task-relevant
property independent of group membership, with a mapping function aligning
individuals of equivalent aptitude across groups to assess fairness
substantively. As a proof of concept, we showcase the framework with examples
taken from the German credit dataset, demonstrating its applicability in
real-world cases.

</details>


### [340] [GroundSight: Augmenting Vision-Language Models with Grounding Information and De-hallucination](https://arxiv.org/abs/2509.25669)
*Xinxi Chen,Tianyang Chen,Lijia Hong*

Main category: cs.AI

TL;DR: Proposes text-grounded object localization for VQA with RAG, enabling targeted image cropping to reduce noise and improve accuracy by 3.45 percentage points, plus a de-hallucination method reducing hallucination rate from 65.79% to 13.88%.


<details>
  <summary>Details</summary>
Motivation: To improve VQA by reducing background noise and improving alignment between visual and textual cues through targeted retrieval, while mitigating hallucinations in responses.

Method: Introduces text-grounded object localization to generate bounding boxes around question-relevant objects, enabling focused image cropping and retrieval in RAG framework, plus question type-based de-hallucination method.

Result: Increased VQA accuracy from 22.19% to 25.64% (3.45% absolute improvement) over Llama-3.2-Vision-11B baseline, and reduced hallucination rate from 65.79% to 13.88% with improved truthfulness.

Conclusion: Text-grounded object localization with RAG effectively improves VQA performance by enabling focused retrieval and reducing hallucinations through targeted image processing and question-aware de-hallucination.

Abstract: We propose a method to improve Visual Question Answering (VQA) with
Retrieval-Augmented Generation (RAG) by introducing text-grounded object
localization. Rather than retrieving information based on the entire image, our
approach enables the model to generate a bounding box around the object most
relevant to the question, allowing for targeted image cropping and focused
retrieval. This reduces background noise, improves alignment between visual and
textual cues, and helps mitigate hallucinations. Our RAG method enhances
context-aware VQA responses increased the accuracy from 22.19% to 25.64%, with
an absolute increase of 3.45 percentage points, compared to the baseline
Llama-3.2-Vision-11B agent. We also proposed a de-hallucination method based on
question type which can effectively reduce the hallucination rate from 65.79%
to 13.88% and improves the truthfulness score.

</details>


### [341] [SING-SQL: A Synthetic Data Generation Framework for In-Domain Text-to-SQL Translation](https://arxiv.org/abs/2509.25672)
*Hasan Alp Caferoğlu,Mehmet Serhat Çelik,Özgür Ulusoy*

Main category: cs.AI

TL;DR: SING-SQL is an automated framework for generating synthetic Text-to-SQL data for any database, enabling specialized models for enterprise scenarios without manual annotations.


<details>
  <summary>Details</summary>
Motivation: Address the gap in real-world enterprise scenarios where models need to specialize to single database schemas and organizations require evaluation on their own databases, rather than focusing only on cross-domain generalization.

Method: Two-stage framework: hierarchically partitions database schema into sub-schemas, synthesizes SQL queries across complexity levels, and applies quality-aware pipeline with LLM-as-a-judge validation, executability checks, automatic repair, and column balancing.

Result: SingSQL-LM models achieve state-of-the-art performance: 3B model reaches 82.87% Soft F1 and 73.03% EX on BIRD benchmark, outperforming best 3B baseline by +16.21 Soft F1 and +12.36 EX. Schema-free fine-tuning with schema-only inference provides most robust results.

Conclusion: SING-SQL establishes a scalable, database-agnostic paradigm for producing and evaluating enterprise-grade Text-to-SQL systems, enabling organizations to create specialized models for their databases without manual effort.

Abstract: Translating natural language questions into SQL has become a core challenge
in enabling non-technical users to query databases. While recent work has
explored large-scale synthetic data generation to improve model performance
through post-training, most efforts emphasize cross-domain generalization. This
leaves a gap for real-world enterprise scenarios, where models need to
specialize to a single database schema and organizations require to be able to
evaluate their Text-to-SQL systems on their own databases. To address this, we
introduce SING-SQL, a fully automated two-stage framework for generating
high-quality, high-coverage synthetic Text-to-SQL data for any target database,
without relying on SQL logs or manual annotations. Our approach hierarchically
partitions a database schema into sub-schemas, synthesizes SQL queries across
multiple complexity levels, and applies a quality-aware pipeline that includes
LLM-as-a-judge validation, executability checks, automatic repair, and column
balancing. We further release SingSQL-LM, a family of compact language models
fine-tuned on the synthetic data, achieving strong in-domain generalization. On
the subset of the BIRD benchmark, SingSQL-LM-3B-R64 reaches 82.87% Soft F1 and
73.03% EX upper bound with 32 candidates, outperforming the best 3B-scale
baseline by +16.21 in Soft F1 and +12.36 in EX. At the 1.5B scale,
SingSQL-LM-1.5B-R64 improves over prior systems by +9.30 in Soft F1 and +4.49
in EX. On synthetic evaluation sets, SingSQL-LMs exceed prior systems by wide
margins, establishing state-of-the-art performance among open models at
comparable scales. Our study of context management strategies reveals that
schema-free fine-tuning combined with schema-only inference provides the most
robust results. These findings establish SING-SQL as a scalable,
database-agnostic paradigm for producing and evaluating enterprise-grade
Text-to-SQL systems.

</details>


### [342] [Collaborative Compression for Large-Scale MoE Deployment on Edge](https://arxiv.org/abs/2509.25689)
*Yixiao Chen,Yanyue Xie,Ruining Yang,Wei Jiang,Wei Wang,Yong He,Yue Chen,Pu Zhao,Yanzhi Wang*

Main category: cs.AI

TL;DR: Proposes a collaborative compression framework combining expert pruning, mixed-precision quantization, and activation optimization to deploy ultra-large Mixture of Experts models on edge platforms, reducing DeepSeek-V3 from 1.3TB to 103GB while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Ultra-large MoE models have hundreds of billions of parameters requiring massive memory/storage, making deployment on resource-constrained edge platforms difficult. Pruning or quantization alone cannot achieve the required aggressive compression ratios without significant accuracy degradation.

Method: Collaborative compression framework combining three techniques: expert pruning, mixed-precision quantization, and activation optimization to compress ultra-large MoE models.

Result: Successfully reduced DeepSeek-V3 model from 1.3TB to 103GB, achieving deployment on platforms with 128GB memory limit while preserving output quality and achieving better accuracy than uniform low-bit quantization methods.

Conclusion: The proposed collaborative compression framework enables effective deployment of ultra-large MoE models on edge platforms with strict memory constraints, outperforming traditional uniform quantization methods in both model size reduction and accuracy preservation.

Abstract: The Mixture of Experts (MoE) architecture is an important method for scaling
Large Language Models (LLMs). It increases model capacity while keeping
computation cost low. However, the ultra-large MoE models still have hundreds
of billions of parameters, requiring massive memory/storage and leading to
difficulties for deployment on resource-constrained edge platforms. Pruning or
quantization alone can hardly address the issue, because of the
super-aggressive compression ratio with significantly degraded accuracy and
output quality. To facilitate the deployment of ultra-large MoEs on edge
platforms, we propose a collaborative compression framework by combining expert
pruning, mixed-precision quantization, and activation optimization. It can
effectively reduce the storage footprint of the ultra-large MoE DeepSeek-V3
from 1.3TB to 103GB, while preserving high output quality with better accuracy
than traditional uniform low-bit quantization methods. To the best of our
knowledge, we are the first to deploy a compressed model from the ultra-large
DeepSeek-V3 on the platform with a strict 128GB total memory limit. Our
comprehensive experiments on multiple benchmarks under various memory
constraints demonstrate the effectiveness of our method with smaller model
sizes and higher accuracy than uniform low-bit quantization methods.

</details>


### [343] [ScheduleMe: Multi-Agent Calendar Assistant](https://arxiv.org/abs/2509.25693)
*N. de Silva,S. Perera,K. L. A. A. Nimasha,I. D. S. Fernando,R. K. A. O. Wijerathne*

Main category: cs.AI

TL;DR: ScheduleMe is a multi-agent calendar assistant that manages Google Calendar events using natural language through a graph-structured coordination system with a central supervisory agent.


<details>
  <summary>Details</summary>
Motivation: To create a more usable and flexible personal calendar assistant by leveraging LLMs and multi-agent systems to handle natural language commands and resolve ambiguities.

Method: Uses a graph-structured coordination mechanism with a central supervisory agent that oversees specialized task agents, enabling modularity, conflict resolution, and context-aware interactions.

Result: Developed a functional multi-agent system that can manage calendar events through natural language conversation while resolving ambiguities and conflicts.

Conclusion: The approach demonstrates how structured reasoning and agent cooperation can enhance the usability and flexibility of personal calendar assistant tools.

Abstract: Recent advancements in LLMs have contributed to the rise of advanced
conversational assistants that can assist with user needs through natural
language conversation. This paper presents a ScheduleMe, a multi-agent calendar
assistant for users to manage google calendar events in natural language. The
system uses a graph-structured coordination mechanism where a central
supervisory agent supervises specialized task agents, allowing modularity,
conflicts resolution, and context-aware interactions to resolve ambiguities and
evaluate user commands. This approach sets an example of how structured
reasoning and agent cooperation might convince operators to increase the
usability and flexibility of personal calendar assistant tools.

</details>


### [344] [Cooperative Autonomous Driving in Diverse Behavioral Traffic: A Heterogeneous Graph Reinforcement Learning Approach](https://arxiv.org/abs/2509.25751)
*Qi Liu,Xueyuan Li,Zirui Li,Juhui Gim*

Main category: cs.AI

TL;DR: Proposes a heterogeneous graph reinforcement learning framework with expert system to improve autonomous vehicle decision-making in complex traffic environments with diverse driving styles.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of navigating heterogeneous traffic environments with diverse driving styles for autonomous vehicles, due to complexity and dynamic interactions.

Method: Uses heterogeneous graph representation for vehicle interactions, HGNN-EM (heterogeneous graph neural network with expert model) for feature encoding, and DDQN algorithm for training decision-making model.

Result: Case study on four-way intersection shows superior performance over baselines in safety, efficiency, stability, convergence rate, while maintaining real-time performance.

Conclusion: The proposed heterogeneous GRL framework with expert system effectively improves AV decision-making in complex traffic environments with diverse driving styles.

Abstract: Navigating heterogeneous traffic environments with diverse driving styles
poses a significant challenge for autonomous vehicles (AVs) due to their
inherent complexity and dynamic interactions. This paper addresses this
challenge by proposing a heterogeneous graph reinforcement learning (GRL)
framework enhanced with an expert system to improve AV decision-making
performance. Initially, a heterogeneous graph representation is introduced to
capture the intricate interactions among vehicles. Then, a heterogeneous graph
neural network with an expert model (HGNN-EM) is proposed to effectively encode
diverse vehicle features and produce driving instructions informed by
domain-specific knowledge. Moreover, the double deep Q-learning (DDQN)
algorithm is utilized to train the decision-making model. A case study on a
typical four-way intersection, involving various driving styles of human
vehicles (HVs), demonstrates that the proposed method has superior performance
over several baselines regarding safety, efficiency, stability, and convergence
rate, all while maintaining favorable real-time performance.

</details>


### [345] [NePTune: A Neuro-Pythonic Framework for Tunable Compositional Reasoning on Vision-Language](https://arxiv.org/abs/2509.25757)
*Danial Kamali,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: NePTune is a neuro-symbolic framework that combines foundation vision models with symbolic reasoning to improve compositional visual reasoning, operating training-free with modular design and supporting fine-tuning.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with compositional reasoning, and existing neuro-symbolic approaches are limited by rigid logical execution and predefined predicates, lacking flexibility.

Method: Dynamic translation of natural language queries into executable Python programs blending imperative control flow with soft logic operators that handle VLM uncertainty, with modular design separating perception from reasoning.

Result: Significant improvement over strong base models on multiple visual reasoning benchmarks, effective compositional generalization and adaptation in novel environments.

Conclusion: NePTune successfully overcomes limitations of traditional neuro-symbolic approaches by integrating perception capabilities with compositional symbolic reasoning in a flexible, training-free framework.

Abstract: Modern Vision-Language Models (VLMs) have achieved impressive performance in
various tasks, yet they often struggle with compositional reasoning, the
ability to decompose and recombine concepts to solve novel problems. While
neuro-symbolic approaches offer a promising direction, they are typically
constrained by crisp logical execution or predefined predicates, which limit
flexibility. In this work, we introduce NePTune, a neuro-symbolic framework
that overcomes these limitations through a hybrid execution model that
integrates the perception capabilities of foundation vision models with the
compositional expressiveness of symbolic reasoning. NePTune dynamically
translates natural language queries into executable Python programs that blend
imperative control flow with soft logic operators capable of reasoning over
VLM-generated uncertainty. Operating in a training-free manner, NePTune, with a
modular design, decouples perception from reasoning, yet its differentiable
operations support fine-tuning. We evaluate NePTune on multiple visual
reasoning benchmarks and various domains, utilizing adversarial tests, and
demonstrate a significant improvement over strong base models, as well as its
effective compositional generalization and adaptation capabilities in novel
environments.

</details>


### [346] [Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training](https://arxiv.org/abs/2509.25758)
*Yein Park,Minbyul Jeong,Jaewoo Kang*

Main category: cs.AI

TL;DR: Post-training techniques create specialized attention heads for reasoning, but different methods (distillation/SFT vs GRPO) produce different circuit dynamics, revealing a trade-off between complex reasoning and reliable execution.


<details>
  <summary>Details</summary>
Motivation: To understand the architectural mechanisms behind improvements from post-training techniques like supervised fine-tuning and reinforcement learning in large reasoning models.

Method: Used circuit analysis to study attention head emergence across Qwen families and DeepSeek-distilled models under different training regimes (distillation/SFT vs group relative policy optimization).

Result: Different training methods create distinct circuit dynamics: distillation/SFT adds stable reasoning heads cumulatively, while GRPO iteratively activates and prunes heads based on reward signals. Think on/off models use compensatory heads rather than dedicated thinking heads.

Conclusion: There's an inherent tension where complex reasoning capabilities come at the cost of reliable elementary computations, pointing to the need for balanced training policies that ensure both effective reasoning strategies and flawless execution.

Abstract: The remarkable capabilities of modern large reasoning models are largely
unlocked through post-training techniques such as supervised fine-tuning and
reinforcement learning. However, the architectural mechanisms behind such
improvements remain largely opaque. In this work, we use circuit analysis to
demonstrate that post-training for complex reasoning sparks the emergence of
novel, functionally specialized attention heads. These heads collectively
support structured reasoning and computation. Our comparative analysis across
Qwen families and DeepSeek-distilled model reveals that these emergent heads
evolve differently under different training regimes. Distillation and SFT
foster a cumulative addition of stable reasoning heads. In contrast, group
relative policy optimization operates in a dynamic search mode: relatively few
attention heads are iteratively activated, evaluated, and pruned, with their
survival closely tracking fluctuations in the task reward signal. Furthermore,
we find that controllable think on/off models do not possess dedicated thinking
heads. Instead, turning off explicit reasoning triggers a broader-but less
efficient-set of compensatory heads. Through ablation and qualitative analyses,
we connect these circuit-level dynamics to a crucial performance trade-off:
strengthened heads enable sophisticated problem-solving strategies for
difficult problems but can also introduce over-thinking failure modes, such as
calculation errors or logical loops on simpler tasks. These findings connect
circuit-level dynamics to macro-level performance, identifying an inherent
tension where complex reasoning comes at the cost of elementary computations.
More broadly, our work points to future directions for training policy design,
emphasizing the need to balance the development of effective reasoning
strategies with the assurance of reliable, flawless execution.

</details>


### [347] [Galton's Law of Mediocrity: Why Large Language Models Regress to the Mean and Fail at Creativity in Advertising](https://arxiv.org/abs/2509.25767)
*Matt Keon,Aabid Karim,Bhoomika Lohana,Abdul Karim,Thai Nguyen,Tara Hamilton,Ali Abbas*

Main category: cs.AI

TL;DR: LLMs tend to produce safe, generic text in creative tasks due to regression to the mean in language. When simplifying ad concepts, creative elements disappear first while factual content remains. Regenerated texts lack true originality despite lexical variety.


<details>
  <summary>Details</summary>
Motivation: To investigate LLMs' tendency towards safe, generic phrasing in creative tasks and formalize this as Galton-style regression to the mean in language.

Method: Used creativity stress test with advertising concepts, simplifying ideas step by step and analyzing feature disappearance. Combined quantitative comparisons with qualitative analysis of regenerated texts.

Result: Creative features (metaphors, emotions, visual cues) disappeared early during simplification while factual content remained. Regenerated texts were longer with lexical variety but lacked depth and distinctiveness. Ad-specific cues improved alignment but outputs still relied on familiar tropes.

Conclusion: Without targeted guidance, LLMs drift towards mediocrity in creative tasks. Structured signals can partially counter this tendency, pointing towards pathways for developing creativity-sensitive models.

Abstract: Large language models (LLMs) generate fluent text yet often default to safe,
generic phrasing, raising doubts about their ability to handle creativity. We
formalize this tendency as a Galton-style regression to the mean in language
and evaluate it using a creativity stress test in advertising concepts. When ad
ideas were simplified step by step, creative features such as metaphors,
emotions, and visual cues disappeared early, while factual content remained,
showing that models favor high-probability information. When asked to
regenerate from simplified inputs, models produced longer outputs with lexical
variety but failed to recover the depth and distinctiveness of the originals.
We combined quantitative comparisons with qualitative analysis, which revealed
that the regenerated texts often appeared novel but lacked true originality.
Providing ad-specific cues such as metaphors, emotional hooks and visual
markers improved alignment and stylistic balance, though outputs still relied
on familiar tropes. Taken together, the findings show that without targeted
guidance, LLMs drift towards mediocrity in creative tasks; structured signals
can partially counter this tendency and point towards pathways for developing
creativity-sensitive models.

</details>


### [348] [Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs](https://arxiv.org/abs/2509.25779)
*Siyu Zhu,Yanbin Jiang,Hejian Sang,Shao Tang,Qingquan Song,Biao He,Rohit Jain,Zhipeng Wang,Alborz Geramifard*

Main category: cs.AI

TL;DR: Agentic RL with LLMs on TravelPlanner benchmark shows 56.9% final-pass rate using Planner-R1 with only 180 training queries, 2.7× improvement over GPT-5 baseline. Smaller 8B models are highly responsive to reward shaping, achieving competitive performance with 3.5× compute efficiency and 1.5× memory efficiency compared to 32B models.


<details>
  <summary>Details</summary>
Motivation: To investigate how reward shaping affects agentic reinforcement learning with large language models, particularly examining the trade-offs between model size, training efficiency, and generalization capabilities.

Method: Used Planner-R1 approach on TravelPlanner benchmark with dense process-level reward signals. Compared different model sizes (8B vs 32B) under various reward conditions (dense vs sparse). Evaluated curriculum learning and tested generalization on out-of-domain tasks.

Result: Achieved 56.9% final-pass rate with only 180 training queries. 8B models with dense rewards reached competitive performance while being 3.5× more compute-efficient and 1.5× more memory-efficient than 32B models. Larger models were more robust under sparse rewards but showed smaller gains from shaping. Fine-tuned models maintained or exceeded baseline performance on out-of-domain tasks.

Conclusion: Reward shaping is a decisive lever for scaling agentic RL, smaller models (8B) are most efficient for agentic RL, and efficiency gains don't sacrifice generalization capabilities.

Abstract: We investigated Agentic RL with large language models on the
\textsc{TravelPlanner} benchmark. Our approach, \textsc{Planner-R1}, achieved a
\textbf{56.9\%} final-pass rate with only 180 training queries, a $2.7\times$
improvement over GPT-5's $21.2\%$ baseline and the strongest agentic result on
the public leaderboard. A central finding was that smaller models (8B) were
highly responsive to reward shaping: with dense process-level signals, they
reached competitive performance while being $3.5\times$ more compute-efficient
and $1.5\times$ more memory-efficient than 32B models. Larger models were more
robust under sparse rewards but exhibited smaller relative gains from shaping
and higher variance across runs. While curriculum learning offered no
significant benefit, shaped rewards consistently amplified learning dynamics,
making 8B models the most efficient setting for agentic RL. Crucially, these
gains did not come at the cost of overfitting: fine-tuned models mostly
maintained or exceeded baseline performance on out-of-domain tasks, including
\textsc{Multi-IF}, \textsc{NaturalPlan}, and $\tau$-\textsc{Bench}. These
results establish reward shaping as a decisive lever for scaling agentic RL,
highlight the competitive strength of smaller models, and demonstrate that
efficiency can be achieved without sacrificing generalization.

</details>


### [349] [Deontic Argumentation](https://arxiv.org/abs/2509.25781)
*Guido Governatori,Antonino Rotolo*

Main category: cs.AI

TL;DR: This paper addresses the problem of defining semantics for deontic argumentation that supports weak permission, particularly when conflicts between obligations arise.


<details>
  <summary>Details</summary>
Motivation: Recent research has shown that grounded semantics fail to support weak permission in cases where there are conflicts between obligations, highlighting a limitation in current deontic argumentation frameworks.

Method: The authors propose a new Deontic Argumentation Theory definition that accounts for weak permission and introduce a novel semantics designed to overcome the limitations of grounded semantics.

Result: The paper presents a new semantics that successfully supports weak permission in deontic argumentation, addressing the identified gap in existing approaches.

Conclusion: The proposed semantics provides a more comprehensive framework for deontic argumentation that properly handles weak permission even in the presence of conflicting obligations.

Abstract: We address the issue of defining a semantics for deontic argumentation that
supports weak permission. Some recent results show that grounded semantics do
not support weak permission when there is a conflict between two obligations.
We provide a definition of Deontic Argumentation Theory that accounts for weak
permission, and we recall the result about grounded semantics. Then, we propose
a new semantics that supports weak permission.

</details>


### [350] [PUREVQ-GAN: Defending Data Poisoning Attacks through Vector-Quantized Bottlenecks](https://arxiv.org/abs/2509.25792)
*Alexander Branch,Omead Pooladzandi,Radin Khosraviani,Sunay Gajanan Bhat,Jeffrey Jiang,Gregory Pottie*

Main category: cs.AI

TL;DR: PureVQ-GAN is a fast defense against data poisoning attacks that uses vector quantization to destroy backdoor triggers while maintaining clean accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient defense against data poisoning attacks that can destroy backdoor triggers in poisoned datasets without requiring computationally expensive iterative refinement steps like diffusion models.

Method: Uses Vector-Quantized VAE with GAN discriminator to force poisoned images through a discrete bottleneck, quantizing them through a learned codebook to destroy fine-grained trigger patterns while preserving semantic content.

Result: Achieves 0% poison success rate against Gradient Matching and Bullseye Polytope attacks, 1.64% against Narcissus on CIFAR-10, while maintaining 91-95% clean accuracy. Over 50x faster than diffusion-based defenses.

Conclusion: PureVQ-GAN provides an effective and practical defense against data poisoning attacks that is significantly faster than existing methods while maintaining high clean accuracy.

Abstract: We introduce PureVQ-GAN, a defense against data poisoning that forces
backdoor triggers through a discrete bottleneck using Vector-Quantized VAE with
GAN discriminator. By quantizing poisoned images through a learned codebook,
PureVQ-GAN destroys fine-grained trigger patterns while preserving semantic
content. A GAN discriminator ensures outputs match the natural image
distribution, preventing reconstruction of out-of-distribution perturbations.
On CIFAR-10, PureVQ-GAN achieves 0% poison success rate (PSR) against Gradient
Matching and Bullseye Polytope attacks, and 1.64% against Narcissus while
maintaining 91-95% clean accuracy. Unlike diffusion-based defenses requiring
hundreds of iterative refinement steps, PureVQ-GAN is over 50x faster, making
it practical for real training pipelines.

</details>


### [351] [Chain-in-Tree: Back to Sequential Reasoning in LLM Tree Search](https://arxiv.org/abs/2509.25835)
*Xinzhe Li*

Main category: cs.AI

TL;DR: Chain-in-Tree (CiT) is a plug-in framework that reduces computational costs in tree-search-based LLM reasoning by adaptively deciding when to branch during search, achieving 75-85% reductions in tokens, model invocations, and runtime with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Tree-search-based approaches for LLM reasoning are inefficient, often being an order of magnitude slower than simpler iterative methods, creating a need for more efficient search strategies.

Method: CiT uses lightweight Branching Necessity (BN) evaluation methods: BN-DP where an auxiliary LLM directly judges branching necessity, and BN-SC which clusters candidate actions to estimate agreement. It's integrated into three tree search frameworks: ToT-BS, ReST-MCTS, and RAP.

Result: BN-DP consistently reduces token generation, model invocations, and runtime by 75-85% across all settings with negligible accuracy loss. BN-SC achieves up to 80% savings but shows instability in some settings. Auxiliary LLM quality is critical for performance.

Conclusion: CiT provides significant efficiency improvements for tree-search-based LLM reasoning while maintaining accuracy, with theoretical guarantees that BN-DP never increases LLM invocations relative to baseline.

Abstract: Test-time scaling enables large language models (LLMs) to improve performance
on long-horizon reasoning tasks by allocating additional compute at inference.
Tree-search-based approaches achieve state-of-the-art results in this setting,
but they are notoriously inefficient, often an order of magnitude slower than
simpler iterative methods. We introduce Chain-in-Tree (CiT), a plug-in
framework that adaptively decides when to branch during search rather than
branching at every step. CiT relies on lightweight Branching Necessity (BN)
evaluation methods: BN-DP (Direct Prompting), where an auxiliary LLM directly
judges whether a step requires branching, and BN-SC (Self-Consistency), which
clusters multiple candidate actions to estimate agreement. We integrate CiT
into three representative LLM-in-the-loop tree search frameworks: Tree of
Thoughts (ToT-BS), ReST-MCTS, and RAP, and evaluate across GSM8K and Math500.
Our results show that: (1) BN-DP consistently reduces token generation, model
invocations, and runtime by 75-85 percent across all settings, with negligible
accuracy loss and sometimes accuracy gains; (2) BN-SC typically yields
substantial savings (up to 80 percent) but shows instability in 1-4 out of 14
settings, caused by a small subset of examples that produce very long reasoning
steps; (3) the quality of auxiliary LLMs is critical, not only the BN evaluator
in BN-DP, but also the models used in BN-SC for clustering and equivalence
checking. When these roles are filled by smaller LLMs, performance degrades.
Importantly, BN-SC does not require LLMs in domains with deterministic action
spaces, where clustering can be done programmatically. We also provide a
theoretical guarantee that BN-DP never increases LLM invocations relative to
the baseline and release a unified implementation of CiT across ToT-BS,
ReST-MCTS, and RAP to facilitate reproducibility and extension.

</details>


### [352] [HiStyle: Hierarchical Style Embedding Predictor for Text-Prompt-Guided Controllable Speech Synthesis](https://arxiv.org/abs/2509.25842)
*Ziyu Zhang,Hanzhao Li,Jingbin Hu,Wenhao Li,Lei Xie*

Main category: cs.AI

TL;DR: HiStyle is a two-stage hierarchical style embedding predictor for controllable text-to-speech that improves style controllability by modeling the hierarchical clustering pattern in style embeddings and using contrastive learning to align text and audio spaces.


<details>
  <summary>Details</summary>
Motivation: Current TTS systems using natural language prompts for style control overlook the underlying hierarchical distribution of style embeddings, which limits their full potential for precise style manipulation.

Method: Proposed HiStyle with two-stage hierarchical style embedding prediction based on t-SNE analysis findings, incorporating contrastive learning and a hybrid style annotation strategy combining statistical methods and human preferences.

Result: HiStyle achieves significantly better style controllability than alternative approaches while maintaining high speech quality in naturalness and intelligibility.

Conclusion: Modeling the hierarchical structure of style embeddings through two-stage prediction and proper text-audio alignment enables more effective and precise control in text-to-speech synthesis.

Abstract: Controllable speech synthesis refers to the precise control of speaking style
by manipulating specific prosodic and paralinguistic attributes, such as
gender, volume, speech rate, pitch, and pitch fluctuation. With the integration
of advanced generative models, particularly large language models (LLMs) and
diffusion models, controllable text-to-speech (TTS) systems have increasingly
transitioned from label-based control to natural language description-based
control, which is typically implemented by predicting global style embeddings
from textual prompts. However, this straightforward prediction overlooks the
underlying distribution of the style embeddings, which may hinder the full
potential of controllable TTS systems. In this study, we use t-SNE analysis to
visualize and analyze the global style embedding distribution of various
mainstream TTS systems, revealing a clear hierarchical clustering pattern:
embeddings first cluster by timbre and subsequently subdivide into finer
clusters based on style attributes. Based on this observation, we propose
HiStyle, a two-stage style embedding predictor that hierarchically predicts
style embeddings conditioned on textual prompts, and further incorporate
contrastive learning to help align the text and audio embedding spaces.
Additionally, we propose a style annotation strategy that leverages the
complementary strengths of statistical methodologies and human auditory
preferences to generate more accurate and perceptually consistent textual
prompts for style control. Comprehensive experiments demonstrate that when
applied to the base TTS model, HiStyle achieves significantly better style
controllability than alternative style embedding predicting approaches while
preserving high speech quality in terms of naturalness and intelligibility.
Audio samples are available at https://anonymous.4open.science/w/HiStyle-2517/.

</details>


### [353] [ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack](https://arxiv.org/abs/2509.25843)
*Yein Park,Jungwoo Park,Jaewoo Kang*

Main category: cs.AI

TL;DR: ASGuard is a framework that surgically mitigates tense-based jailbreaking in LLMs by identifying vulnerable attention heads and applying channel-wise scaling to strengthen refusal mechanisms.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit brittle refusal behaviors that can be circumvented by simple linguistic changes like tense modifications, revealing critical gaps in current alignment methods.

Method: Three-step approach: 1) Circuit analysis to identify tense-vulnerable attention heads, 2) Train channel-wise scaling vectors to recalibrate activations, 3) Apply preventative fine-tuning for robust refusal mechanisms.

Result: ASGuard effectively reduces attack success rate of targeted jailbreaking across three LLMs while preserving general capabilities and minimizing over-refusal, achieving Pareto-optimal safety-utility balance.

Conclusion: Deep understanding of model internals enables practical, efficient targeted behavior adjustment, charting course for more reliable and interpretable AI safety.

Abstract: Large language models (LLMs), despite being safety-aligned, exhibit brittle
refusal behaviors that can be circumvented by simple linguistic changes. As
tense jailbreaking demonstrates that models refusing harmful requests often
comply when rephrased in past tense, a critical generalization gap is revealed
in current alignment methods whose underlying mechanisms are poorly understood.
In this work, we introduce Activation-Scaling Guard (ASGuard), an insightful,
mechanistically-informed framework that surgically mitigates this specific
vulnerability. For the first step, we use circuit analysis to identify the
specific attention heads causally linked to the targeted jailbreaking, the
tense-changing attack. Second, we train a precise, channel-wise scaling vector
to recalibrate the activation of tense vulnerable heads. Lastly, we apply it
into a "preventative fine-tuning", forcing the model to learn a more robust
refusal mechanism. Across three LLMs, ASGuard effectively reduces the attack
success rate of targeted jailbreaking while preserving general capabilities and
minimizing over refusal, achieving a Pareto-optimal balance between safety and
utility. Our findings underscore how adversarial suffixes suppress the
propagation of the refusal-mediating direction, based on mechanistic analysis.
Furthermore, our work showcases how a deep understanding of model internals can
be leveraged to develop practical, efficient, and targeted methods for
adjusting model behavior, charting a course for more reliable and interpretable
AI safety.

</details>


### [354] [Aging Decline in Basketball Career Trend Prediction Based on Machine Learning and LSTM Model](https://arxiv.org/abs/2509.25858)
*Yi-chen Yao,Jerry Wang,Yi-cheng Lai,Lyn Chao-ling Chen*

Main category: cs.AI

TL;DR: This study uses autoencoder with K-means clustering for NBA player career trend classification and LSTM for performance prediction, achieving better results than other methods with good generalization ability.


<details>
  <summary>Details</summary>
Motivation: To analyze aging decline on performance of NBA players and develop methods for career trend classification and performance prediction.

Method: Autoencoder with K-means clustering for career trend classification and LSTM deep learning for performance prediction, using veteran NBA players' game data.

Result: The proposed method performed better than other methods with generalization ability for evaluating various types of NBA career trends.

Conclusion: The approach can be applied to different types of sports in the field of sport analytics.

Abstract: The topic of aging decline on performance of NBA players has been discussed
in this study. The autoencoder with K-means clustering machine learning method
was adopted to career trend classification of NBA players, and the LSTM deep
learning method was adopted in performance prediction of each NBA player. The
dataset was collected from the basketball game data of veteran NBA players. The
contribution of the work performed better than the other methods with
generalization ability for evaluating various types of NBA career trend, and
can be applied in different types of sports in the field of sport analytics.

</details>


### [355] [CIMNAS: A Joint Framework for Compute-In-Memory-Aware Neural Architecture Search](https://arxiv.org/abs/2509.25862)
*Olga Krestinskaya,Mohammed E. Fouda,Ahmed Eltawil,Khaled N. Salama*

Main category: cs.AI

TL;DR: CIMNAS is a joint model-quantization-hardware optimization framework for CIM-based neural network accelerators that simultaneously searches software parameters, quantization policies, and hardware parameters to achieve significant improvements in energy-delay-area product (EDAP), TOPS/W, and TOPS/mm² while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual tuning of software and hardware parameters for CIM-based neural network accelerators is impractical due to the vast parameter space and complex interdependencies. Hardware-aware neural architecture search (HW-NAS) is needed to automate design optimization and maximize hardware efficiency and performance accuracy.

Method: CIMNAS framework simultaneously searches across software parameters, quantization policies, and hardware parameters (device-, circuit-, and architecture-level) for CIM architectures. It explores a massive search space of 9.9x10^85 parameter combinations using MobileNet model with RRAM-based CIM architecture, and also supports SRAM-based ResNet50 architecture.

Result: On ImageNet dataset, CIMNAS achieved: 90.1x-104.5x reduction in EDAP, 4.68x-4.82x improvement in TOPS/W, 11.3x-12.78x enhancement in TOPS/mm² while maintaining 73.81% accuracy. For SRAM-based ResNet50, achieved up to 819.5x reduction in EDAP. No accuracy loss during optimization.

Conclusion: CIMNAS enables effective joint optimization of software, quantization, and hardware parameters for CIM-based neural network accelerators, achieving significant performance improvements without accuracy degradation. The framework is adaptable to different architectures and generates diverse parameter combinations for high-performance designs.

Abstract: To maximize hardware efficiency and performance accuracy in Compute-In-Memory
(CIM)-based neural network accelerators for Artificial Intelligence (AI)
applications, co-optimizing both software and hardware design parameters is
essential. Manual tuning is impractical due to the vast number of parameters
and their complex interdependencies. To effectively automate the design and
optimization of CIM-based neural network accelerators, hardware-aware neural
architecture search (HW-NAS) techniques can be applied. This work introduces
CIMNAS, a joint model-quantization-hardware optimization framework for CIM
architectures. CIMNAS simultaneously searches across software parameters,
quantization policies, and a broad range of hardware parameters, incorporating
device-, circuit-, and architecture-level co-optimizations. CIMNAS experiments
were conducted over a search space of 9.9x10^85 potential parameter
combinations with the MobileNet model as a baseline and RRAM-based CIM
architecture. Evaluated on the ImageNet dataset, CIMNAS achieved a reduction in
energy-delay-area product (EDAP) ranging from 90.1x to 104.5x, an improvement
in TOPS/W between 4.68x and 4.82x, and an enhancement in TOPS/mm^2 from 11.3x
to 12.78x relative to various baselines, all while maintaining an accuracy of
73.81%. The adaptability and robustness of CIMNAS are demonstrated by extending
the framework to support the SRAM-based ResNet50 architecture, achieving up to
an 819.5x reduction in EDAP. Unlike other state-of-the-art methods, CIMNAS
achieves EDAP-focused optimization without any accuracy loss, generating
diverse software-hardware parameter combinations for high-performance CIM-based
neural network designs. The source code of CIMNAS is available at
https://github.com/OlgaKrestinskaya/CIMNAS.

</details>


### [356] [Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs](https://arxiv.org/abs/2509.25873)
*Hankun Dai,Maoquan Wang,Mengnan Qi,Yikai Zhang,Zijian Jin,Yongqiang Yao,Yufan Huang,Shengyu Fu,Elsie Nallipogu*

Main category: cs.AI

TL;DR: Lita (Lite Agent) is a minimalist autonomous coding agent that achieves competitive performance with less complexity, fewer tokens, and minimal manual design compared to workflow-based agents.


<details>
  <summary>Details</summary>
Motivation: Current code agents rely on complex, hand-crafted workflows that obscure true model capabilities, require heavy human intervention, are costly to maintain, and risk data leakage through prompt tuning.

Method: Lita operationalizes 'liteness' - minimizing manual design while retaining essential autonomous agent elements, enabling faithful evaluation without elaborate scaffolding.

Result: Lita achieves competitive or superior performance on Aider Polyglot and SWE-Bench compared to workflow-based baselines, while consuming fewer tokens and requiring significantly less design effort.

Conclusion: Lita sufficiently reveals modern LLMs' underlying coding competence and supports the Agent Complexity Law: performance gaps between simple and complex agents shrink as core models improve.

Abstract: Large language models (LLMs) are increasingly being applied to programming
tasks, ranging from single-turn code completion to autonomous agents. Current
code agent designs frequently depend on complex, hand-crafted workflows and
tool sets. However, this reliance on elaborate scaffolding presents several
challenges: agent performance becomes overly dependent on prompt tuning and
custom design choices, heavy human intervention obscures a model's true
underlying capabilities, and intricate pipelines are costly to build and
maintain. Furthermore, optimizing complex task prompts increases the risk of
data leakage. Currently, when introducing new models, LLM providers like OpenAI
and Anthropic often publish benchmark scores to demonstrate their models'
coding proficiency, but keep their proprietary evaluation frameworks
confidential. To address these limitations, we introduce Lita (Lite Agent),
which operationalizes liteness, a principle of minimizing manual design while
retaining the essential elements of a fully autonomous agent. Lita enables a
more faithful and unified evaluation without elaborate scaffolding. Experiments
on the Aider Polyglot and SWE-Bench with frontier models demonstrate that Lita
achieves competitive or superior performance compared to workflow-based and
agentic baselines. Crucially, Lita also consumes fewer tokens and requires
significantly less design effort. Our results suggest that Lita is sufficient
to reveal the underlying coding competence of modern LLMs. Finally, we propose
the Agent Complexity Law: the performance gap between agents of varying
complexity, from simple to sophisticated designs, will shrink as the core model
improves, ultimately converging to a negligible difference.

</details>


### [357] [SafeMind: Benchmarking and Mitigating Safety Risks in Embodied LLM Agents](https://arxiv.org/abs/2509.25885)
*Ruolin Chen,Yinqian Sun,Jihang Wang,Mingyang Lv,Qian Zhang,Yi Zeng*

Main category: cs.AI

TL;DR: This paper identifies safety vulnerabilities in embodied LLM agents and proposes SafeMindBench benchmark and SafeMindAgent solution to systematically address safety risks across four reasoning stages and three constraint types.


<details>
  <summary>Details</summary>
Motivation: Embodied agents powered by LLMs have advanced planning capabilities but face safety vulnerabilities when interacting with the physical world, requiring systematic safety analysis and mitigation.

Method: The authors formalize four reasoning stages (Task Understanding, Environment Perception, High-Level Plan Generation, Low-Level Action Generation) and three safety constraint types (Factual, Causal, Temporal), then create SafeMindBench benchmark with 5,558 samples and propose SafeMindAgent with Planner-Executor architecture and three cascaded safety modules.

Result: Experiments show leading LLMs and embodied agents remain susceptible to safety-critical failures, while SafeMindAgent significantly improves safety rate over baselines while maintaining comparable task completion.

Conclusion: SafeMindBench and SafeMindAgent provide both evaluation suite and practical solution for systematic study and mitigation of safety risks in embodied LLM agents.

Abstract: Embodied agents powered by large language models (LLMs) inherit advanced
planning capabilities; however, their direct interaction with the physical
world exposes them to safety vulnerabilities. In this work, we identify four
key reasoning stages where hazards may arise: Task Understanding, Environment
Perception, High-Level Plan Generation, and Low-Level Action Generation. We
further formalize three orthogonal safety constraint types (Factual, Causal,
and Temporal) to systematically characterize potential safety violations.
Building on this risk model, we present SafeMindBench, a multimodal benchmark
with 5,558 samples spanning four task categories (Instr-Risk, Env-Risk,
Order-Fix, Req-Align) across high-risk scenarios such as sabotage, harm,
privacy, and illegal behavior. Extensive experiments on SafeMindBench reveal
that leading LLMs (e.g., GPT-4o) and widely used embodied agents remain
susceptible to safety-critical failures. To address this challenge, we
introduce SafeMindAgent, a modular Planner-Executor architecture integrated
with three cascaded safety modules, which incorporate safety constraints into
the reasoning process. Results show that SafeMindAgent significantly improves
safety rate over strong baselines while maintaining comparable task completion.
Together, SafeMindBench and SafeMindAgent provide both a rigorous evaluation
suite and a practical solution that advance the systematic study and mitigation
of safety risks in embodied LLM agents.

</details>


### [358] [DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models](https://arxiv.org/abs/2509.25922)
*Zhicheng Zhou,Jing Li,Suming Qiu,Junjie Huang,Linyuan Qiu,Zhijie Sun*

Main category: cs.AI

TL;DR: DeepJSONEval is a new benchmark with 2100 multi-domain instances featuring deep nested JSON structures to better evaluate LLMs' data comprehension and extraction capabilities for practical web data mining tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLM benchmarks overemphasize pure JSON generation rather than assessing data comprehension and extraction abilities relevant to practical web data mining tasks involving low-density, high-redundancy internet information.

Method: Introduces DeepJSONEval benchmark with 2100 multi-domain instances featuring deep nested JSON structures categorized by difficulty, testing LLMs' ability to parse unstructured text and output structured results into complex JSON schemas.

Result: Experiments reveal significant performance gaps among LLMs in handling complex nested JSON structures, demonstrating the need for better evaluation of data extraction capabilities.

Conclusion: DeepJSONEval addresses limitations in current benchmarks and advances research in structured JSON generation for practical web data mining applications.

Abstract: The internet is saturated with low-density, high-redundancy information, such
as social media comments, repetitive news, and lengthy discussions, making it
difficult to extract valuable insights efficiently. Multi-layer nested JSON
structures provide an effective solution by compressing such information into
semantically rich, hierarchical representations, which organize data into
key-value pairs, arrays, and nested objects, preserving contextual
relationships and enabling efficient storage, retrieval, and semantic querying.
For instance, in news aggregation, a JSON object can nest an article's metadata
(title, author, date), content (text, multimedia), and multimedia information
(multimedia type, caption) hierarchically. Large Language Models (LLMs) play a
transformative role in web data mining by parsing unstructured text and
outputting structured results directly into complex JSON schemas. However,
current benchmarks for evaluating LLMs' JSON output capabilities overemphasize
pure JSON generation rather than assessing data comprehension and extraction
abilities, a limitation that lacks relevance to practical web data mining
tasks. To address this, we introduce DeepJSONEval, a novel benchmark featuring
2100 multi-domain instances with deep nested structures, categorized by
difficulty. Experiments show significant performance gaps among LLMs in
handling such complexity. Our benchmark and datasets are open-sourced to
advance research in structured JSON
generation.(https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval).

</details>


### [359] [KIRETT: Smart Integration of Vital Signs Data for Intelligent Decision Support in Rescue Scenarios](https://arxiv.org/abs/2509.25923)
*Mubaris Nadeem,Johannes Zenkert,Christian Weber,Lisa Bender,Madjid Fathi*

Main category: cs.AI

TL;DR: The paper discusses how integrating vital signs through wearable technology can improve decision-making and treatment recommendations during rescue operations.


<details>
  <summary>Details</summary>
Motivation: To enhance time utilization and support health professionals with useful information during life-threatening rescue situations where quick decisions are crucial.

Method: The KIRETT project develops a wrist-worn wearable that provides treatment recommendations and situation detection using vital signs data.

Result: Vital signs integration shows potential to significantly improve decision-making processes and treatment quality during critical rescue operations.

Conclusion: Vital signs play a crucial role in enhancing emergency medical care by providing real-time data that supports faster and more accurate treatment decisions in rescue scenarios.

Abstract: The integration of vital signs in healthcare has witnessed a steady rise,
promising health professionals to assist in their daily tasks to improve
patient treatment. In life-threatening situations, like rescue operations,
crucial decisions need to be made in the shortest possible amount of time to
ensure that excellent treatment is provided during life-saving measurements.
The integration of vital signs in the treatment holds the potential to improve
time utilization for rescuers in such critical situations. They furthermore
serve to support health professionals during the treatment with useful
information and suggestions. To achieve such a goal, the KIRETT project serves
to provide treatment recommendations and situation detection, combined on a
wrist-worn wearable for rescue operations.This paper aims to present the
significant role of vital signs in the improvement of decision-making during
rescue operations and show their impact on health professionals and patients in
need.

</details>


### [360] [Quantitative Evaluation of KIRETT Wearable Demonstrator for Rescue Operations](https://arxiv.org/abs/2509.25928)
*Mubaris Nadeem,Johannes Zenkert,Lisa Bender,Christian Weber,Madjid Fathi*

Main category: cs.AI

TL;DR: The KIRETT wearable device provides AI-driven treatment recommendations and real-time monitoring for emergency rescue services, evaluated through a 2-day study with 14 participants.


<details>
  <summary>Details</summary>
Motivation: Emergency situations require fast, reliable medical treatment without time-consuming patient consultations, creating a need for technology-assisted decision support in rescue services.

Method: Developed the KIRETT wearable device with AI capabilities for treatment recommendations and real-time vitals monitoring, then conducted a 2-day evaluation study with 14 rescue service participants.

Result: Quantitative results from the 14-participant evaluation study analyzed the specific needs and requirements of rescue operators in healthcare emergency scenarios.

Conclusion: The KIRETT system demonstrates potential to support rescue services by providing AI-driven medical decision support and real-time patient monitoring in time-critical emergency situations.

Abstract: Healthcare and Medicine are under constant pressure to provide patient-driven
medical expertise to ensure a fast and accurate treatment of the patient. In
such scenarios, the diagnosis contains, the family history, long term medical
data and a detailed consultation with the patient. In time-critical
emergencies, such conversation and time-consuming elaboration are not possible.
Rescue services need to provide fast, reliable treatments for the patient in
need. With the help of modern technologies, like treatment recommendations,
real-time vitals-monitoring, and situation detection through artificial
intelligence (AI) a situation can be analyzed and supported in providing fast,
accurate patient-data-driven medical treatments. In KIRETT, a wearable device
is developed to support in such scenarios and presents a way to provide
treatment recommendation in rescue services. The objective of this paper is to
present the quantitative results of a two-day KIRETT evaluation (14
participants) to analyze the needs of rescue operators in healthcare.

</details>


### [361] [Boosting Process-Correct CoT Reasoning by Modeling Solvability of Multiple-Choice QA](https://arxiv.org/abs/2509.25941)
*Raphael Schumann,Stefan Riezler*

Main category: cs.AI

TL;DR: This paper studies how question solvability affects reasoning quality in LLMs, finding that unsolvable questions lead to spurious reasoning chains. By incorporating solvability estimation into reward models and RL, they improve process-correct reasoning and reduce hallucinations.


<details>
  <summary>Details</summary>
Motivation: To understand how question difficulty (solvability) affects the quality of chain-of-thought reasoning in LLMs, particularly how unsolvable questions lead to false positives and spurious reasoning chains.

Method: Used multiple-choice QA as controlled setting; estimated question solvability; adapted outcome-supervised reward models and RL with group-relative advantage to incorporate solvability into objectives.

Result: Across math and multimodal datasets, the solvability-aware methods consistently yielded higher rates of process-correct reasoning and improved answer accuracy in RL settings.

Conclusion: Solvability is a key factor for reducing hallucinations and increasing reliability in chain-of-thought reasoning, with intermediate solvability regimes being most effective for learning.

Abstract: Reasoning quality in large language models depends not only on producing
correct answers but also on generating valid intermediate steps. We study this
through multiple-choice question answering (MCQA), which provides a controlled
setting with fixed answer options. Our analysis shows that when questions are
effectively unsolvable for a model, spurious chains of thought (CoTs) are more
likely to appear, leading to false positives. By estimating the solvability of
each question, we uncover an intermediate regime where learning is most
effective. Building on this insight, we adapt outcome-supervised reward models
and reinforcement learning with group-relative advantage to incorporate
solvability into their objectives. Across experiments on math and multimodal
datasets, these modifications consistently yield higher rates of
process-correct reasoning and, in reinforcement learning, improved answer
accuracy as well. Our results highlight solvability as a key factor for
reducing hallucinations and increasing reliability in CoT reasoning.

</details>


### [362] [NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving](https://arxiv.org/abs/2509.25944)
*Yuan Gao,Mattia Piccinini,Roberto Brusnicki,Yuchen Zhang,Johannes Betz*

Main category: cs.AI

TL;DR: Proposes NuRisk dataset for spatio-temporal risk reasoning in autonomous driving, benchmarks VLMs showing poor performance, and presents fine-tuned model with improved accuracy and latency.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack spatio-temporal reasoning for evolving risks in autonomous driving, only providing static qualitative judgments.

Method: Created NuRisk dataset with 2,900 scenarios and 1.1M agent-level samples from real-world and simulated data, featuring BEV sequential images with quantitative risk annotations. Fine-tuned 7B VLM for spatio-temporal reasoning.

Result: Benchmarked VLMs achieved only 33% accuracy with high latency. Fine-tuned model improved accuracy to 41% and reduced latency by 75%.

Conclusion: NuRisk establishes critical benchmark for spatio-temporal reasoning in autonomous driving, showing significant progress but highlighting remaining challenges.

Abstract: Understanding risk in autonomous driving requires not only perception and
prediction, but also high-level reasoning about agent behavior and context.
Current Vision Language Models (VLMs)-based methods primarily ground agents in
static images and provide qualitative judgments, lacking the spatio-temporal
reasoning needed to capture how risks evolve over time. To address this gap, we
propose NuRisk, a comprehensive Visual Question Answering (VQA) dataset
comprising 2,900 scenarios and 1.1 million agent-level samples, built on
real-world data from nuScenes and Waymo, supplemented with safety-critical
scenarios from the CommonRoad simulator. The dataset provides Bird-Eye-View
(BEV) based sequential images with quantitative, agent-level risk annotations,
enabling spatio-temporal reasoning. We benchmark well-known VLMs across
different prompting techniques and find that they fail to perform explicit
spatio-temporal reasoning, resulting in a peak accuracy of 33% at high latency.
To address these shortcomings, our fine-tuned 7B VLM agent improves accuracy to
41% and reduces latency by 75%, demonstrating explicit spatio-temporal
reasoning capabilities that proprietary models lacked. While this represents a
significant step forward, the modest accuracy underscores the profound
challenge of the task, establishing NuRisk as a critical benchmark for
advancing spatio-temporal reasoning in autonomous driving.

</details>


### [363] [Automated Model Discovery via Multi-modal & Multi-step Pipeline](https://arxiv.org/abs/2509.25946)
*Lee Jung-Mok,Nam Hyeon-Woo,Moon Ye-Bin,Junhyun Nam,Tae-Hyun Oh*

Main category: cs.AI

TL;DR: A multi-modal, multi-step pipeline using vision-language models for automated model discovery that balances fine-grained detail capture with generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing automated model discovery approaches struggle to balance capturing fine-grained details while ensuring generalizability beyond training data with reasonable model complexity.

Method: Uses two vision-language modules (AnalyzerVLM and EvaluatorVLM) in a multi-step pipeline. AnalyzerVLM autonomously plans multi-step analyses to propose candidate models, while EvaluatorVLM assesses models both quantitatively and perceptually for local detail fitness and overall trend generalizability.

Result: The pipeline effectively discovers models that capture fine details while ensuring strong generalizability. Ablation studies confirm both multi-modality and multi-step reasoning are crucial for finding favorable models.

Conclusion: The proposed multi-modal, multi-step pipeline using vision-language models successfully addresses the challenge of balancing detail capture with generalizability in automated model discovery.

Abstract: Automated model discovery is the process of automatically searching and
identifying the most appropriate model for a given dataset over a large
combinatorial search space. Existing approaches, however, often face challenges
in balancing the capture of fine-grained details with ensuring generalizability
beyond training data regimes with a reasonable model complexity. In this paper,
we present a multi-modal \& multi-step pipeline for effective automated model
discovery. Our approach leverages two vision-language-based modules (VLM),
AnalyzerVLM and EvaluatorVLM, for effective model proposal and evaluation in an
agentic way. AnalyzerVLM autonomously plans and executes multi-step analyses to
propose effective candidate models. EvaluatorVLM assesses the candidate models
both quantitatively and perceptually, regarding the fitness for local details
and the generalibility for overall trends. Our results demonstrate that our
pipeline effectively discovers models that capture fine details and ensure
strong generalizability. Additionally, extensive ablation studies show that
both multi-modality and multi-step reasoning play crucial roles in discovering
favorable models.

</details>


### [364] [RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning](https://arxiv.org/abs/2509.25958)
*Gang Li,Yulei Qin,Xiaoyu Tan,Dingkang Yang,Yuchen Shi,Zihan Xu,Xiang Li,Xing Sun,Ke Li*

Main category: cs.AI

TL;DR: RoRecomp is a plug-and-play method that improves RLVR training efficiency by strategically recomposing training data into priority and compensation batches to guide models toward concise reasoning.


<details>
  <summary>Details</summary>
Motivation: Standard RLVR training leads to verbose reasoning processes and inefficient exploration trajectories due to outcome-only rewards lacking efficiency incentives and high variance in response length causing noisy optimization signals.

Method: RoRecomp separates responses into priority batches (combining short-correct and long-incorrect responses for brevity gradient) and compensation batches (using remaining responses from replay buffer for stability).

Result: Achieved 27.7% reasoning length reduction in zero RL training, 46.8% reduction in unnecessary tool calls with improved accuracy in agentic RL, and up to 52.5% length reduction in thinking compression with minimal performance impact.

Conclusion: RoRecomp effectively addresses efficiency issues in RLVR training by providing clear gradient signals for brevity while maintaining model stability, demonstrating substantial efficiency gains across multiple settings.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in
eliciting complex reasoning in large language models (LLMs). However, standard
RLVR training often leads to excessively verbose processes (in reasoning tasks)
and inefficient exploration trajectories (in agentic settings), as outcome-only
rewards provide no incentive for efficiency and the high variance in response
length within relatively small rollout groups results in noisy optimization
signals. To address this, we propose Rollout Response Recomposition (RoRecomp),
a plug-and-play method that guides models toward concise reasoning by
strategically recomposing the training data. RoRecomp separates responses into
two distinct batch types: 1) priority batches, which combine short-correct and
long-incorrect responses selected from online batches to provide a clear
gradient signal for brevity, and 2) compensation batches, which utilize
remaining responses from a replay buffer to maintain stability and prevent
model collapse. To comprehensively evaluate effectiveness, we test RoRecomp
across three settings where results demonstrate substantial efficiency gains:
reducing reasoning length by 27.7% in zero RL training, reducing unnecessary
tool calls by 46.8% while improving accuracy in agentic RL, and achieving up to
52.5% length reduction in thinking compression, all with minimal performance
impact.

</details>


### [365] [Scalable and Robust LLM Unlearning by Correcting Responses with Retrieved Exclusions](https://arxiv.org/abs/2509.25973)
*Junbeom Kim,Kyuyoung Kim,Jihoon Tack,Dongha Lim,Jinwoo Shin*

Main category: cs.AI

TL;DR: CURE is a machine unlearning framework that uses a lightweight corrector to detect and rewrite sensitive outputs, leveraging retrieval augmentation to handle large-scale unlearning without retraining.


<details>
  <summary>Details</summary>
Motivation: Language models risk exposing sensitive information from training data, and existing unlearning methods that focus on input suppression often fail to eliminate underlying knowledge and lack scalability.

Method: CURE employs a corrector that verifies model outputs for information leakage and rewrites them. It retrieves relevant unlearning targets as in-context references to enable detection and conditional revision without additional training.

Result: CURE substantially reduces information leakage, including from indirect queries where prior methods fail, while maintaining response quality and general utility. It shows robustness in continual unlearning scenarios.

Conclusion: CURE provides an effective and scalable solution for machine unlearning that is practical for real-world applications, outperforming prior methods in preventing information leakage while preserving model utility.

Abstract: Language models trained on web-scale corpora risk memorizing and exposing
sensitive information, prompting the need for effective machine unlearning.
Prior methods mainly focus on input queries to suppress sensitive outputs, yet
this often fails to eliminate the underlying knowledge and limits scalability.
To address this, we propose Corrective Unlearning with Retrieved Exclusions
(CURE), a novel unlearning framework that verifies model outputs for leakage
and revises them into safe responses. Specifically, CURE employs a lightweight
corrector that is applied to the original model to verify whether outputs
contain target knowledge and to rewrite them if any leakage is detected. To
efficiently handle large-scale unlearning requests, CURE retrieves unlearning
targets that are relevant to the initial response and provides them as
in-context references to the corrector for detection and conditional revision.
By leveraging this retrieval augmentation, the corrector can adapt to new
unlearning requests without additional training. Extensive evaluations
demonstrate that CURE substantially reduces information leakage, even from
indirect queries where prior works fall short, while maintaining response
quality and general utility. Moreover, it demonstrates robustness under
continual unlearning scenarios, making it practical for real-world
applications.

</details>


### [366] [Towards Unified Multimodal Misinformation Detection in Social Media: A Benchmark Dataset and Baseline](https://arxiv.org/abs/2509.25991)
*Haiyang Li,Yaxiong Wang,Lianwei Wu,Lechao Cheng,Zhun Zhong*

Main category: cs.AI

TL;DR: The paper introduces OmniFake dataset and UMFDet framework for unified detection of both human-crafted misinformation and AI-generated fake content in multimodal posts, addressing the limitation of existing specialized models.


<details>
  <summary>Details</summary>
Motivation: Current fake content detection systems are specialized for either human-written misinformation (NLP focus) or AI-generated content (CV focus), but real-world scenarios involve unknown content types, limiting effectiveness of specialized approaches.

Method: Proposes UMFDet framework with VLM backbone, Category-aware Mixture-of-Experts Adapter for category-specific cues, and attribution chain-of-thought mechanism for implicit reasoning guidance on deceptive signals.

Result: Extensive experiments show UMFDet achieves robust and consistent performance across both misinformation types, outperforming specialized baselines.

Conclusion: UMFDet offers a practical unified solution for real-world multimodal deception detection that handles both human-crafted and AI-generated fake content effectively.

Abstract: In recent years, detecting fake multimodal content on social media has drawn
increasing attention. Two major forms of deception dominate: human-crafted
misinformation (e.g., rumors and misleading posts) and AI-generated content
produced by image synthesis models or vision-language models (VLMs). Although
both share deceptive intent, they are typically studied in isolation. NLP
research focuses on human-written misinformation, while the CV community
targets AI-generated artifacts. As a result, existing models are often
specialized for only one type of fake content. In real-world scenarios,
however, the type of a multimodal post is usually unknown, limiting the
effectiveness of such specialized systems. To bridge this gap, we construct the
Omnibus Dataset for Multimodal News Deception (OmniFake), a comprehensive
benchmark of 127K samples that integrates human-curated misinformation from
existing resources with newly synthesized AI-generated examples. Based on this
dataset, we propose Unified Multimodal Fake Content Detection (UMFDet), a
framework designed to handle both forms of deception. UMFDet leverages a VLM
backbone augmented with a Category-aware Mixture-of-Experts (MoE) Adapter to
capture category-specific cues, and an attribution chain-of-thought mechanism
that provides implicit reasoning guidance for locating salient deceptive
signals. Extensive experiments demonstrate that UMFDet achieves robust and
consistent performance across both misinformation types, outperforming
specialized baselines and offering a practical solution for real-world
multimodal deception detection.

</details>


### [367] [Towards Human Engagement with Realistic AI Combat Pilots](https://arxiv.org/abs/2509.26002)
*Ardian Selmonaj,Giacomo Del Rio,Adrian Schneider,Alessandro Antonucci*

Main category: cs.AI

TL;DR: A system enabling real-time interaction between human users and AI agents trained for fighter jet control in 3D air combat simulations, integrated with VR-Forces defense simulation tool.


<details>
  <summary>Details</summary>
Motivation: To create opportunities for human-agent teaming, immersive training, and exploration of innovative tactics in defense contexts through mixed simulations.

Method: Training agents using Multi-Agent Reinforcement Learning in dedicated environments, then developing a communication link for seamless deployment into VR-Forces simulation tool.

Result: Successful integration allowing mixed simulations where human-controlled entities engage with intelligent agents exhibiting distinct combat behaviors.

Conclusion: The system enables new possibilities for defense training and tactical exploration through real-time human-AI interaction in realistic combat scenarios.

Abstract: We present a system that enables real-time interaction between human users
and agents trained to control fighter jets in simulated 3D air combat
scenarios. The agents are trained in a dedicated environment using Multi-Agent
Reinforcement Learning. A communication link is developed to allow seamless
deployment of trained agents into VR-Forces, a widely used defense simulation
tool for realistic tactical scenarios. This integration allows mixed
simulations where human-controlled entities engage with intelligent agents
exhibiting distinct combat behaviors. Our interaction model creates new
opportunities for human-agent teaming, immersive training, and the exploration
of innovative tactics in defense contexts.

</details>


### [368] [CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided Neural Architecture Search](https://arxiv.org/abs/2509.26037)
*Zhe Li,Zhiwei Lin,Yongtao Wang*

Main category: cs.AI

TL;DR: CoLLM-NAS is a two-stage NAS framework using two complementary LLMs (Navigator and Generator) with a Coordinator module to overcome limitations of existing LLM-NAS methods, achieving SOTA results on ImageNet and NAS-Bench-201.


<details>
  <summary>Details</summary>
Motivation: To address critical limitations in existing LLM-NAS methods including architectural invalidity, computational inefficiency, and inferior performance compared to traditional NAS.

Method: Two-stage NAS framework with Navigator LLM guiding search direction, Generator LLM synthesizing high-quality candidates, and Coordinator module managing their interaction, combining LLMs' inherent knowledge with iterative feedback and historical trajectory.

Result: Surpasses existing NAS methods and conventional search algorithms, achieving new SOTA results on ImageNet and NAS-Bench-201. Consistently enhances performance and efficiency of various two-stage NAS methods across diverse search spaces.

Conclusion: CoLLM-NAS demonstrates excellent generalization and effectively overcomes limitations of existing LLM-NAS approaches through collaborative LLM-driven architecture search.

Abstract: The integration of Large Language Models (LLMs) with Neural Architecture
Search (NAS) has introduced new possibilities for automating the design of
neural architectures. However, most existing methods face critical limitations,
including architectural invalidity, computational inefficiency, and inferior
performance compared to traditional NAS. In this work, we present Collaborative
LLM-based NAS (CoLLM-NAS), a two-stage NAS framework with knowledge-guided
search driven by two complementary LLMs. Specifically, we propose a Navigator
LLM to guide search direction and a Generator LLM to synthesize high-quality
candidates, with a dedicated Coordinator module to manage their interaction.
CoLLM-NAS efficiently guides the search process by combining LLMs' inherent
knowledge of structured neural architectures with progressive knowledge from
iterative feedback and historical trajectory. Experimental results on ImageNet
and NAS-Bench-201 show that CoLLM-NAS surpasses existing NAS methods and
conventional search algorithms, achieving new state-of-the-art results.
Furthermore, CoLLM-NAS consistently enhances the performance and efficiency of
various two-stage NAS methods (e.g., OFA, SPOS, and AutoFormer) across diverse
search spaces (e.g., MobileNet, ShuffleNet, and AutoFormer), demonstrating its
excellent generalization.

</details>


### [369] [Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research](https://arxiv.org/abs/2509.26080)
*Emma Rose Madden*

Main category: cs.AI

TL;DR: LLMs are increasingly used in social sciences but their outputs shouldn't be misinterpreted as probabilistic evidence. The paper proposes reframing LLMs as pattern matchers with explicit scope conditions and introduces practical guardrails for responsible use.


<details>
  <summary>Details</summary>
Motivation: To address the misinterpretation of LLM outputs as posterior-like evidence in social science applications, where prediction doesn't equate to probabilism and accurate points don't imply calibrated uncertainty.

Method: Proposes a pragmatic reframing where LLMs are used as high-capacity pattern matchers for quasi-predictive interpolation under explicit scope conditions, not as substitutes for probabilistic inference.

Result: Introduces practical guardrails including independent draws, preregistered human baselines, reliability-aware validation, and subgroup calibration to enable useful prototyping and forecasting while avoiding category errors.

Conclusion: LLMs should be used cautiously in social sciences as pattern matchers with explicit limitations, not as probabilistic inference tools, with specific guardrails to ensure responsible application.

Abstract: Large Language Models (LLMs) are being increasingly used as synthetic agents
in social science, in applications ranging from augmenting survey responses to
powering multi-agent simulations. Because strong prediction plus conditioning
prompts, token log-probs, and repeated sampling mimic Bayesian workflows, their
outputs can be misinterpreted as posterior-like evidence from a coherent model.
However, prediction does not equate to probabilism, and accurate points do not
imply calibrated uncertainty. This paper outlines cautions that should be taken
when interpreting LLM outputs and proposes a pragmatic reframing for the social
sciences in which LLMs are used as high-capacity pattern matchers for
quasi-predictive interpolation under explicit scope conditions and not as
substitutes for probabilistic inference. Practical guardrails such as
independent draws, preregistered human baselines, reliability-aware validation,
and subgroup calibration, are introduced so that researchers may engage in
useful prototyping and forecasting while avoiding category errors.

</details>


### [370] [SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs](https://arxiv.org/abs/2509.26100)
*Yixu Wang,Xin Wang,Yang Yao,Xinyuan Li,Yan Teng,Xingjun Ma,Yingchun Wang*

Main category: cs.AI

TL;DR: Proposes SafeEvalAgent, a multi-agent framework for dynamic safety evaluation of LLMs that autonomously evolves test cases from policy documents, revealing deeper vulnerabilities than static benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing static benchmarks cannot address dynamic AI risks and evolving regulations, creating a safety gap in high-stakes LLM deployment.

Method: Multi-agent framework with specialized agents that ingest unstructured policy documents to generate and perpetually evolve safety benchmarks through a self-evolving evaluation loop.

Result: Demonstrates consistent decline in model safety as evaluation hardens, e.g., GPT-5's safety rate on EU AI Act drops from 72.50% to 36.36% over iterations.

Conclusion: Reveals limitations of static assessments and highlights the need for dynamic evaluation ecosystems to ensure safe and responsible AI deployment.

Abstract: The rapid integration of Large Language Models (LLMs) into high-stakes
domains necessitates reliable safety and compliance evaluation. However,
existing static benchmarks are ill-equipped to address the dynamic nature of AI
risks and evolving regulations, creating a critical safety gap. This paper
introduces a new paradigm of agentic safety evaluation, reframing evaluation as
a continuous and self-evolving process rather than a one-time audit. We then
propose a novel multi-agent framework SafeEvalAgent, which autonomously ingests
unstructured policy documents to generate and perpetually evolve a
comprehensive safety benchmark. SafeEvalAgent leverages a synergistic pipeline
of specialized agents and incorporates a Self-evolving Evaluation loop, where
the system learns from evaluation results to craft progressively more
sophisticated and targeted test cases. Our experiments demonstrate the
effectiveness of SafeEvalAgent, showing a consistent decline in model safety as
the evaluation hardens. For instance, GPT-5's safety rate on the EU AI Act
drops from 72.50% to 36.36% over successive iterations. These findings reveal
the limitations of static assessments and highlight our framework's ability to
uncover deep vulnerabilities missed by traditional methods, underscoring the
urgent need for dynamic evaluation ecosystems to ensure the safe and
responsible deployment of advanced AI.

</details>


### [371] [MEDAKA: Construction of Biomedical Knowledge Graphs Using Large Language Models](https://arxiv.org/abs/2509.26128)
*Asmita Sengupta,David Antony Selby,Sebastian Josef Vollmer,Gerrit Großmann*

Main category: cs.AI

TL;DR: This paper presents MEDAKA, a hackable pipeline for creating knowledge graphs from drug leaflets, and a curated dataset capturing clinically relevant drug information.


<details>
  <summary>Details</summary>
Motivation: Existing biomedical knowledge graphs focus narrowly on molecular interactions or adverse events, overlooking rich data in drug leaflets that contain comprehensive clinical information.

Method: Developed an end-to-end pipeline using web scraper and LLM to extract structured information from unstructured online drug leaflets, creating the MEDAKA dataset with clinical attributes.

Result: Created MEDAKA dataset capturing side effects, warnings, contraindications, ingredients, dosage guidelines, storage instructions and physical characteristics, evaluated through manual inspection and LLM-as-a-Judge framework.

Conclusion: MEDAKA supports patient safety monitoring and drug recommendation tasks, and the pipeline can be adapted for constructing knowledge graphs from unstructured texts in other domains.

Abstract: Knowledge graphs (KGs) are increasingly used to represent biomedical
information in structured, interpretable formats. However, existing biomedical
KGs often focus narrowly on molecular interactions or adverse events,
overlooking the rich data found in drug leaflets. In this work, we present (1)
a hackable, end-to-end pipeline to create KGs from unstructured online content
using a web scraper and an LLM; and (2) a curated dataset, MEDAKA, generated by
applying this method to publicly available drug leaflets. The dataset captures
clinically relevant attributes such as side effects, warnings,
contraindications, ingredients, dosage guidelines, storage instructions and
physical characteristics. We evaluate it through manual inspection and with an
LLM-as-a-Judge framework, and compare its coverage with existing biomedical KGs
and databases. We expect MEDAKA to support tasks such as patient safety
monitoring and drug recommendation. The pipeline can also be used for
constructing KGs from unstructured texts in other domains. Code and dataset are
available at https://github.com/medakakg/medaka.

</details>


### [372] [LMILAtt: A Deep Learning Model for Depression Detection from Social Media Users Enhanced by Multi-Instance Learning Based on Attention Mechanism](https://arxiv.org/abs/2509.26145)
*Yukun Yang*

Main category: cs.AI

TL;DR: The paper proposes LMILAtt model for depression detection from social media data, combining LSTM autoencoders and attention mechanisms to extract temporal features and improve detection accuracy while reducing annotation costs.


<details>
  <summary>Details</summary>
Motivation: Depression is a major global health challenge requiring early identification. Social media provides new data for detection, but existing methods have limitations in accuracy, time series feature utilization, and high annotation costs.

Method: LMILAtt model integrates LSTM autoencoders to extract temporal dynamic features from user tweets, uses attention mechanisms to weight key texts, and employs multi-instance learning architecture for user-level detection.

Result: Experiments on WU3D dataset show the model significantly outperforms baseline models in accuracy, recall and F1 score. The weakly supervised learning strategy reduces labeling costs.

Conclusion: The proposed model provides an efficient solution for large-scale social media depression screening with improved accuracy and reduced annotation requirements.

Abstract: Depression is a major global public health challenge and its early
identification is crucial. Social media data provides a new perspective for
depression detection, but existing methods face limitations such as
insufficient accuracy, insufficient utilization of time series features, and
high annotation costs. To this end, this study proposes the LMILAtt model,
which innovatively integrates Long Short-Term Memory autoencoders and attention
mechanisms: firstly, the temporal dynamic features of user tweets (such as
depressive tendency evolution patterns) are extracted through unsupervised LSTM
autoencoders. Secondly, the attention mechanism is used to dynamically weight
key texts (such as early depression signals) and construct a multi-example
learning architecture to improve the accuracy of user-level detection. Finally,
the performance was verified on the WU3D dataset labeled by professional
medicine. Experiments show that the model is significantly better than the
baseline model in terms of accuracy, recall and F1 score. In addition, the
weakly supervised learning strategy significantly reduces the cost of labeling
and provides an efficient solution for large-scale social media depression
screening.

</details>


### [373] [Beyond the Algorithm: A Field Guide to Deploying AI Agents in Clinical Practice](https://arxiv.org/abs/2509.26153)
*Jack Gallifant,Katherine C. Kellogg,Matt Butler,Amanda Centi,Patrick F. Doyle,Sayon Dutta,Joyce Guo,Matthew J. Hadfield,Esther H. Kim,David E. Kozono,Hugo JWL Aerts,Adam B. Landman,Raymond H. Mak,Rebecca G. Mishuris,Tanna L. Nelson,Guergana K. Savova,Elad Sharon,Benjamin C. Silverman,Umit Topaloglu,Jeremy L. Warner,Danielle S. Bitterman*

Main category: cs.AI

TL;DR: A practitioner-oriented field manual for deploying generative AI agents in healthcare, based on experience with an irAE detection system and interviews with clinical stakeholders, revealing that 80% of effort goes to sociotechnical implementation rather than model development.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between the potential of LLMs in healthcare and their practical implementation in clinical settings, addressing the misalignment in clinical AI development where most effort is consumed by implementation work rather than model development.

Method: Developed a field manual based on deployment experience with "irAE-Agent" (an automated system for detecting immune-related adverse events from EHR data) and structured interviews with 20 clinicians, engineers, and informatics leaders.

Result: Analysis revealed that less than 20% of effort was dedicated to prompt engineering and model development, while over 80% was consumed by sociotechnical implementation work, distilled into five "heavy lifts": data integration, model validation, ensuring economic value, managing system drift, and governance.

Conclusion: The field manual shifts focus from algorithmic development to essential infrastructure and implementation work required to successfully translate generative AI from pilot projects into routine clinical care, bridging the "valley of death" in clinical AI deployment.

Abstract: Large language models (LLMs) integrated into agent-driven workflows hold
immense promise for healthcare, yet a significant gap exists between their
potential and practical implementation within clinical settings. To address
this, we present a practitioner-oriented field manual for deploying generative
agents that use electronic health record (EHR) data. This guide is informed by
our experience deploying the "irAE-Agent", an automated system to detect
immune-related adverse events from clinical notes at Mass General Brigham, and
by structured interviews with 20 clinicians, engineers, and informatics leaders
involved in the project. Our analysis reveals a critical misalignment in
clinical AI development: less than 20% of our effort was dedicated to prompt
engineering and model development, while over 80% was consumed by the
sociotechnical work of implementation. We distill this effort into five "heavy
lifts": data integration, model validation, ensuring economic value, managing
system drift, and governance. By providing actionable solutions for each of
these challenges, this field manual shifts the focus from algorithmic
development to the essential infrastructure and implementation work required to
bridge the "valley of death" and successfully translate generative AI from
pilot projects into routine clinical care.

</details>


### [374] [90% Faster, 100% Code-Free: MLLM-Driven Zero-Code 3D Game Development](https://arxiv.org/abs/2509.26161)
*Runxin Yang,Yuxuan Wan,Shuqing Li,Michael R. Lyu*

Main category: cs.AI

TL;DR: UniGen is an end-to-end multi-agent framework that automates 3D game development from natural language requirements without coding, reducing development time by 91.4%.


<details>
  <summary>Details</summary>
Motivation: To democratize 3D game development by overcoming limitations of existing approaches: limited scope to 2D content, manual integration requirements, and poor handling of interactive game logic.

Method: Uses four coordinated agents: Planning Agent interprets requirements into blueprints, Generation Agent produces C# scripts, Automation Agent handles engine binding and scene construction, Debugging Agent provides real-time error correction.

Result: Successfully developed three distinct game prototypes, enabling zero-coding game creation and reducing development time by 91.4%.

Conclusion: UniGen effectively bridges the gap between MLLM outputs and production-ready 3D games, democratizing game development and significantly accelerating the creation process.

Abstract: Developing 3D games requires specialized expertise across multiple domains,
including programming, 3D modeling, and engine configuration, which limits
access to millions of potential creators. Recently, researchers have begun to
explore automated game development. However, existing approaches face three
primary challenges: (1) limited scope to 2D content generation or isolated code
snippets; (2) requirement for manual integration of generated components into
game engines; and (3) poor performance on handling interactive game logic and
state management. While Multimodal Large Language Models (MLLMs) demonstrate
potential capabilities to ease the game generation task, a critical gap still
remains in translating these outputs into production-ready, executable game
projects based on game engines such as Unity and Unreal Engine.
  To bridge the gap, this paper introduces UniGen, the first end-to-end
coordinated multi-agent framework that automates zero-coding development of
runnable 3D games from natural language requirements. Specifically, UniGen uses
a Planning Agent that interprets user requirements into structured blueprints
and engineered logic descriptions; after which a Generation Agent produces
executable C# scripts; then an Automation Agent handles engine-specific
component binding and scene construction; and lastly a Debugging Agent provides
real-time error correction through conversational interaction. We evaluated
UniGen on three distinct game prototypes. Results demonstrate that UniGen not
only democratizes game creation by requiring no coding from the user, but also
reduces development time by 91.4%. We release UniGen at
https://github.com/yxwan123/UniGen. A video demonstration is available at
https://www.youtube.com/watch?v=xyJjFfnxUx0.

</details>


### [375] ['Too much alignment; not enough culture': Re-balancing cultural alignment practices in LLMs](https://arxiv.org/abs/2509.26167)
*Eric J. W. Orlowski,Hakim Norhashim,Tristan Koh Ly Wey*

Main category: cs.AI

TL;DR: The paper critiques current AI cultural alignment approaches for being too quantitative and superficial, proposing a shift to qualitative methods inspired by anthropology's "thick description" concept to create culturally sensitive AI systems.


<details>
  <summary>Details</summary>
Motivation: Current AI cultural alignment practices rely on simplistic proxies and quantitative benchmarks that fail to capture the nuanced, context-dependent nature of human cultures, reducing culture to static categories and avoiding deeper questions about true cultural alignment.

Method: Proposes integrating interpretive qualitative approaches from social sciences into AI alignment, specifically using Clifford Geertz's "thick description" concept to develop "thick outputs" grounded in user context and intent, with three conditions: scoped cultural representations, nuanced output capacity, and context anchoring.

Result: The paper outlines a framework for developing culturally aligned AI systems but does not present empirical results or implementation details.

Conclusion: Calls for cross-disciplinary collaboration and adoption of qualitative, ethnographic evaluation methods as essential for creating AI systems that are genuinely culturally sensitive, ethically responsible, and reflective of human complexity.

Abstract: While cultural alignment has increasingly become a focal point within AI
research, current approaches relying predominantly on quantitative benchmarks
and simplistic proxies fail to capture the deeply nuanced and context-dependent
nature of human cultures. Existing alignment practices typically reduce culture
to static demographic categories or superficial cultural facts, thereby
sidestepping critical questions about what it truly means to be culturally
aligned. This paper argues for a fundamental shift towards integrating
interpretive qualitative approaches drawn from social sciences into AI
alignment practices, specifically in the context of Large Language Models
(LLMs). Drawing inspiration from Clifford Geertz's concept of "thick
description," we propose that AI systems must produce outputs that reflect
deeper cultural meanings--what we term "thick outputs"-grounded firmly in
user-provided context and intent. We outline three necessary conditions for
successful cultural alignment: sufficiently scoped cultural representations,
the capacity for nuanced outputs, and the anchoring of outputs in the cultural
contexts implied within prompts. Finally, we call for cross-disciplinary
collaboration and the adoption of qualitative, ethnographic evaluation methods
as vital steps toward developing AI systems that are genuinely culturally
sensitive, ethically responsible, and reflective of human complexity.

</details>


### [376] [LLM Agents for Knowledge Discovery in Atomic Layer Processing](https://arxiv.org/abs/2509.26201)
*Andreas Werbrouck,Marshall B. Lindsay,Matthew Maschmann,Matthias J. Young*

Main category: cs.AI

TL;DR: LLM agents can autonomously discover knowledge in materials science by exploring black box functions through trial-and-error, demonstrating path-dependent discovery in both games and chemical reactor simulations.


<details>
  <summary>Details</summary>
Motivation: To test the potential of LLMs as independent reasoning agents for knowledge discovery in materials science, moving beyond task-specific optimization to free exploration.

Method: Repurposed LangGraph's tool functionality to supply agents with black box functions to interrogate, using trial-and-error exploration without explicit instructions.

Result: Successfully demonstrated knowledge discovery in a children's parlor game and discovered diverse chemical interactions in an Atomic Layer Processing reactor simulation.

Conclusion: LLM agents can autonomously explore, discover, and exploit knowledge in complex systems through persistent trial-and-error approaches, showing strong path-dependence in discovery outcomes.

Abstract: Large Language Models (LLMs) have garnered significant attention for several
years now. Recently, their use as independently reasoning agents has been
proposed. In this work, we test the potential of such agents for knowledge
discovery in materials science. We repurpose LangGraph's tool functionality to
supply agents with a black box function to interrogate. In contrast to process
optimization or performing specific, user-defined tasks, knowledge discovery
consists of freely exploring the system, posing and verifying statements about
the behavior of this black box, with the sole objective of generating and
verifying generalizable statements. We provide proof of concept for this
approach through a children's parlor game, demonstrating the role of
trial-and-error and persistence in knowledge discovery, and the strong
path-dependence of results. We then apply the same strategy to show that LLM
agents can explore, discover, and exploit diverse chemical interactions in an
advanced Atomic Layer Processing reactor simulation using intentionally limited
probe capabilities without explicit instructions.

</details>


### [377] [Human-Centered Evaluation of RAG outputs: a framework and questionnaire for human-AI collaboration](https://arxiv.org/abs/2509.26205)
*Aline Mangold,Kiran Hoffmann*

Main category: cs.AI

TL;DR: This paper presents a human-centered questionnaire for evaluating RAG systems across 12 dimensions, developed through iterative refinement with both human and LLM feedback.


<details>
  <summary>Details</summary>
Motivation: Systematic, human-centered evaluation of RAG system outputs is underexplored despite their increasing deployment in user-facing applications.

Method: Designed a questionnaire based on Gienapp's utility-dimension framework, iteratively refined through multiple rounds of ratings on query-output pairs and semantic discussions, incorporating feedback from human raters and human-LLM pairs.

Result: LLMs reliably focus on metric descriptions and scale labels but struggle with detecting textual format variations. Humans had difficulty focusing strictly on metric descriptions and labels. LLM ratings were helpful but lacked agreement with human numeric ratings.

Conclusion: The final questionnaire extends the initial framework by focusing on user intent, text structuring, and information verifiability, providing a comprehensive tool for human-centered RAG evaluation.

Abstract: Retrieval-augmented generation (RAG) systems are increasingly deployed in
user-facing applications, yet systematic, human-centered evaluation of their
outputs remains underexplored. Building on Gienapp's utility-dimension
framework, we designed a human-centred questionnaire that assesses RAG outputs
across 12 dimensions. We iteratively refined the questionnaire through several
rounds of ratings on a set of query-output pairs and semantic discussions.
Ultimately, we incorporated feedback from both a human rater and a human-LLM
pair. Results indicate that while large language models (LLMs) reliably focus
on metric descriptions and scale labels, they exhibit weaknesses in detecting
textual format variations. Humans struggled to focus strictly on metric
descriptions and labels. LLM ratings and explanations were viewed as a helpful
support, but numeric LLM and human ratings lacked agreement. The final
questionnaire extends the initial framework by focusing on user intent, text
structuring, and information verifiability.

</details>


### [378] [Diversity-Incentivized Exploration for Versatile Reasoning](https://arxiv.org/abs/2509.26209)
*Zican Hu,Shilin Zhang,Yafu Li,Jianhao Yan,Xuyang Hu,Leyang Cui,Xiaoye Qu,Chunlin Chen,Yu Cheng,Zhi Wang*

Main category: cs.AI

TL;DR: DIVER is a reinforcement learning framework that uses global sequence-level diversity as intrinsic rewards to improve exploration and sample efficiency in reasoning tasks for LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods struggle with deficient exploration and poor sample efficiency in reasoning tasks due to vast state-action spaces and reward sparsity.

Method: Introduces global diversity incentives as intrinsic rewards, uses potential-based reward shaping to preserve optimal policy invariance, and implements heuristics to prevent reward hacking.

Result: DIVER outperforms competitive RLVR baselines on both in-domain and out-of-domain tasks, achieving better performance in Pass@1 and Pass@k evaluations.

Conclusion: Global sequence-level diversity is crucial for incentivizing deep exploration in reasoning tasks, and DIVER effectively leverages this insight to improve reasoning capabilities in LLMs.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
crucial paradigm for incentivizing reasoning capabilities in Large Language
Models (LLMs). Due to vast state-action spaces and reward sparsity in reasoning
tasks, existing methods often struggle with deficient exploration and poor
sample efficiency. In the paper, we propose \textbf{DIVER}
(\textbf{D}iversity-\textbf{I}ncentivized Exploration for
\textbf{V}ersatil\textbf{E} \textbf{R}easoning), an innovative framework that
highlights the pivotal role of global sequence-level diversity to incentivize
deep exploration for versatile reasoning. We first conduct a primary empirical
study to reveal a strong positive correlation between global diversity and
reasoning capacity. Building on this insight, we introduce global diversity
incentives as an intrinsic reward to promote deep exploration in a semantically
structured space. Incorporating the intrinsic reward, we develop a
potential-based reward shaping mechanism to preserve optimal policy invariance
and design simple heuristics to mitigate possible reward hacking. Experimental
results show that DIVER outperforms competitive RLVR baselines with various
exploration strategies on both in-domain and out-of-domain tasks, excelling in
both Pass@1 and Pass@k evaluations. Our code is available at
https://github.com/NJU-RL/DIVER.

</details>


### [379] [Benchmarking Deep Learning Convolutions on Energy-constrained CPUs](https://arxiv.org/abs/2509.26217)
*Enrique Galvez,Adrien Cassagne,Alix Munier,Manuel Bouyer*

Main category: cs.AI

TL;DR: Evaluation of CPU-based convolution algorithms for deep learning inference, benchmarking direct, GEMM-based, and Winograd methods across modern CPUs from ARM, Intel, AMD, Apple, and Nvidia.


<details>
  <summary>Details</summary>
Motivation: Most prior studies focus on GPUs or NPUs, leaving CPU implementations relatively underoptimized for deep learning inference.

Method: Benchmark direct, GEMM-based, and Winograd convolution algorithms across modern CPUs from multiple vendors, measuring both latency and energy efficiency.

Result: Nvidia AGX Orin combined with GEMM algorithm achieves the best trade-off between inference latency and energy consumption.

Conclusion: Identifies key architectural factors governing CPU efficiency for convolution operations, providing practical guidance for energy-aware embedded deployment.

Abstract: This work evaluates state-of-the-art convolution algorithms for CPU-based
deep learning inference. While most prior studies focus on GPUs or NPUs, CPU
implementations remain relatively underoptimized. We benchmark direct,
GEMM-based, and Winograd convolutions across modern CPUs from ARM __ , Intel __
, AMD __ , Apple __ , and Nvidia __ , considering both latency and energy
efficiency. Our results highlight the key architectural factors that govern CPU
efficiency for convolution operations, providing practical guidance for
energy-aware embedded deployment. As a main results of this work, the Nvidia __
AGX Orin combined with the GEMM algorithm achieves the best trade-off between
inference latency and energy consumption.

</details>


### [380] [SlimPack: Fine-Grained Asymmetric Packing for Balanced and Efficient Variable-Length LLM Training](https://arxiv.org/abs/2509.26246)
*Yuliang Liu,Guohao Wu,Shenglong Zhang,Wei Zhang,Qianchao Zhu,Zhouyang Li,Chenyu Wang*

Main category: cs.AI

TL;DR: SlimPack addresses LLM training inefficiencies from context length variance by decomposing samples into fine-grained slices and using asymmetric partitioning to create balanced scheduling units optimized for forward/backward passes.


<details>
  <summary>Details</summary>
Motivation: Distributed LLM training suffers from extreme context length variance causing workload imbalances and hardware underutilization, with existing solutions sacrificing memory or communication efficiency.

Method: Decomposes samples into fine-grained slices, uses asymmetric partitioning to create balanced scheduling units optimized separately for forward and backward passes, orchestrated by a two-phase solver and high-fidelity simulator.

Result: Achieves up to 2.8x training throughput improvement over baselines while maintaining both superior balance and high resource efficiency.

Conclusion: SlimPack fundamentally rethinks data packing and scheduling to break conventional trade-offs, delivering significant performance gains in distributed LLM training.

Abstract: The efficient distributed training of Large Language Models (LLMs) is
severely hampered by the extreme variance in context lengths. This data
heterogeneity, amplified by conventional packing strategies and asymmetric
forward-backward costs, leads to critical inefficiencies such as cascading
workload imbalances and severe hardware underutilization. Existing solutions
attempt to mitigate these challenges, but often at the expense of memory or
communication efficiency.
  To address these challenges, we introduce SlimPack, a framework that
fundamentally rethinks data packing and scheduling by decomposing samples into
fine-grained slices. This slice-level decomposition immediately mitigates
critical memory and communication bottlenecks by transforming large, volatile
workloads into a stream of smaller, manageable units. This flexibility is then
harnessed for our core innovation, Asymmetric Partitioning, which assembles
balanced scheduling units uniquely optimized for the different demands of the
forward and backward passes. Orchestrated by a two-phase solver and a
high-fidelity simulator, SlimPack holistically resolves imbalances across all
parallel dimensions. Extensive experiments demonstrate that SlimPack achieves
up to a $2.8\times$ training throughput improvement over baselines, breaking
the conventional trade-off by delivering both superior balance and high
resource efficiency.

</details>


### [381] [ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning](https://arxiv.org/abs/2509.26255)
*Yichao Liang,Dat Nguyen,Cambridge Yang,Tianyang Li,Joshua B. Tenenbaum,Carl Edward Rasmussen,Adrian Weller,Zenna Tavares,Tom Silver,Kevin Ellis*

Main category: cs.AI

TL;DR: A framework for learning abstract world models that jointly learns symbolic state representations and causal processes for both agent actions and exogenous mechanisms, enabling fast planning in long-horizon embodied tasks.


<details>
  <summary>Details</summary>
Motivation: Long-horizon embodied planning is challenging because the world changes not only through agent actions but also through exogenous processes that unfold concurrently.

Method: Proposes a framework that learns symbolic state representations and causal processes via variational Bayesian inference combined with LLM proposals, modeling time courses of stochastic causal-effect relations.

Result: Across five simulated tabletop robotics environments, the learned models enable fast planning that generalizes to held-out tasks with more objects and more complex goals, outperforming baseline methods.

Conclusion: The proposed framework successfully addresses the challenge of modeling concurrent endogenous and exogenous processes in embodied planning, demonstrating strong generalization capabilities in complex environments.

Abstract: Long-horizon embodied planning is challenging because the world does not only
change through an agent's actions: exogenous processes (e.g., water heating,
dominoes cascading) unfold concurrently with the agent's actions. We propose a
framework for abstract world models that jointly learns (i) symbolic state
representations and (ii) causal processes for both endogenous actions and
exogenous mechanisms. Each causal process models the time course of a
stochastic causal-effect relation. We learn these world models from limited
data via variational Bayesian inference combined with LLM proposals. Across
five simulated tabletop robotics environments, the learned models enable fast
planning that generalizes to held-out tasks with more objects and more complex
goals, outperforming a range of baselines.

</details>


### [382] [Interactive Learning for LLM Reasoning](https://arxiv.org/abs/2509.26306)
*Hehai Lin,Shilei Cao,Minzhi Li,Sudong Wang,Haotian Wu,Linyi Yang,Juepeng Zheng,Chengwei Qin*

Main category: cs.AI

TL;DR: ILR is a multi-agent co-learning framework that enhances LLMs' independent problem-solving through dynamic interaction strategies and perception calibration, achieving up to 5% improvement over single-agent baselines.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent systems require re-execution during inference, unlike human cognition where individuals can independently solve problems after learning from interactions. The goal is to investigate if multi-agent interaction can enhance LLMs' independent reasoning capabilities.

Method: ILR framework with two components: 1) Dynamic Interaction adaptively selects cooperative/competitive strategies based on question difficulty and model ability, using Idea3 paradigm (Idea Sharing, Analysis, Fusion); 2) Perception Calibration employs Group Relative Policy Optimization (GRPO) to integrate reward distributions across agents.

Result: ILR consistently outperforms single-agent learning across three LLMs from two model families, achieving up to 5% improvement on five mathematical benchmarks and one coding benchmark. Idea3 enhances robustness of stronger LLMs, and dynamic interaction boosts learning compared to pure cooperative/competitive strategies.

Conclusion: Multi-agent interaction can indeed enhance LLMs' independent problem-solving ability. The ILR framework with dynamic interaction and perception calibration provides an effective approach for building stronger multi-agent systems that improve individual reasoning capabilities.

Abstract: Existing multi-agent learning approaches have developed interactive training
environments to explicitly promote collaboration among multiple Large Language
Models (LLMs), thereby constructing stronger multi-agent systems (MAS).
However, during inference, they require re-executing the MAS to obtain final
solutions, which diverges from human cognition that individuals can enhance
their reasoning capabilities through interactions with others and resolve
questions independently in the future. To investigate whether multi-agent
interaction can enhance LLMs' independent problem-solving ability, we introduce
ILR, a novel co-learning framework for MAS that integrates two key components:
Dynamic Interaction and Perception Calibration. Specifically, Dynamic
Interaction first adaptively selects either cooperative or competitive
strategies depending on question difficulty and model ability. LLMs then
exchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea
Fusion), an innovative interaction paradigm designed to mimic human discussion,
before deriving their respective final answers. In Perception Calibration, ILR
employs Group Relative Policy Optimization (GRPO) to train LLMs while
integrating one LLM's reward distribution characteristics into another's reward
function, thereby enhancing the cohesion of multi-agent interactions. We
validate ILR on three LLMs across two model families of varying scales,
evaluating performance on five mathematical benchmarks and one coding
benchmark. Experimental results show that ILR consistently outperforms
single-agent learning, yielding an improvement of up to 5% over the strongest
baseline. We further discover that Idea3 can enhance the robustness of stronger
LLMs during multi-agent inference, and dynamic interaction types can boost
multi-agent learning compared to pure cooperative or competitive strategies.

</details>


### [383] [AI Playing Business Games: Benchmarking Large Language Models on Managerial Decision-Making in Dynamic Simulations](https://arxiv.org/abs/2509.26331)
*Berdymyrat Ovezmyradov*

Main category: cs.AI

TL;DR: This paper introduces a novel business simulation benchmark to evaluate LLMs' long-term strategic decision-making capabilities in business management, testing five major LLMs on multi-step retail management decisions over 12 months.


<details>
  <summary>Details</summary>
Motivation: There is a shortage of benchmarks for evaluating LLMs' long-term coherence and strategic business decision-making capabilities, as existing benchmarks focus on short-term tasks and don't adequately test multi-step strategic thinking.

Method: Created a reproducible, open-access management simulator using a spreadsheet model where LLMs make monthly decisions for a retail company over 12 months, including pricing, ordering, marketing, hiring, and financial decisions based on structured business reports.

Result: Evaluated five leading LLMs (Gemini, ChatGPT, Meta AI, Mistral AI, Grok) on quantitative metrics like profit, revenue, market share, and analyzed their strategic coherence, adaptability, and decision rationale.

Conclusion: The research provides a novel framework for assessing LLMs' long-term business decision-making capabilities, moving beyond simple performance metrics to evaluate strategic coherence and adaptability in dynamic business environments.

Abstract: The rapid advancement of LLMs sparked significant interest in their potential
to augment or automate managerial functions. One of the most recent trends in
AI benchmarking is performance of Large Language Models (LLMs) over longer time
horizons. While LLMs excel at tasks involving natural language and pattern
recognition, their capabilities in multi-step, strategic business
decision-making remain largely unexplored. Few studies demonstrated how results
can be different from benchmarks in short-term tasks, as Vending-Bench
revealed. Meanwhile, there is a shortage of alternative benchmarks for
long-term coherence. This research analyses a novel benchmark using a business
game for the decision making in business. The research contributes to the
recent literature on AI by proposing a reproducible, open-access management
simulator to the research community for LLM benchmarking. This novel framework
is used for evaluating the performance of five leading LLMs available in free
online interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes
decisions for a simulated retail company. A dynamic, month-by-month management
simulation provides transparently in spreadsheet model as experimental
environment. In each of twelve months, the LLMs are provided with a structured
prompt containing a full business report from the previous period and are
tasked with making key strategic decisions: pricing, order size, marketing
budget, hiring, dismissal, loans, training expense, R&D expense, sales
forecast, income forecast The methodology is designed to compare the LLMs on
quantitative metrics: profit, revenue, and market share, and other KPIs. LLM
decisions are analyzed in their strategic coherence, adaptability to market
changes, and the rationale provided for their decisions. This approach allows
to move beyond simple performance metrics for assessment of the long-term
decision-making.

</details>


### [384] [SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models](https://arxiv.org/abs/2509.26345)
*Qinjian Zhao,Jiaqi Wang,Zhiqiang Gao,Zhihao Dou,Belal Abuhaija,Kaizhu Huang*

Main category: cs.AI

TL;DR: SafeBehavior is a hierarchical jailbreak defense mechanism that uses three-stage reasoning (intention inference, self-introspection, self-revision) to protect LLMs from jailbreak attacks, showing improved robustness and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM defense methods suffer from high computational costs, limited generalization, and inability to detect subtle malicious intent in complex contexts, creating vulnerabilities to jailbreak attacks.

Method: Proposes SafeBehavior with three hierarchical stages: intention inference to detect input risks, self-introspection to assess responses with confidence judgments, and self-revision to adaptively rewrite uncertain outputs while preserving user intent.

Result: Extensive evaluation against five jailbreak attack types shows SafeBehavior significantly improves robustness and adaptability across diverse threat scenarios compared to seven state-of-the-art defense baselines.

Conclusion: SafeBehavior offers an efficient and human-inspired approach to safeguarding LLMs against jailbreak attempts by simulating adaptive multistage reasoning processes.

Abstract: Large Language Models (LLMs) have achieved impressive performance across
diverse natural language processing tasks, but their growing power also
amplifies potential risks such as jailbreak attacks that circumvent built-in
safety mechanisms. Existing defenses including input paraphrasing, multi step
evaluation, and safety expert models often suffer from high computational
costs, limited generalization, or rigid workflows that fail to detect subtle
malicious intent embedded in complex contexts. Inspired by cognitive science
findings on human decision making, we propose SafeBehavior, a novel
hierarchical jailbreak defense mechanism that simulates the adaptive multistage
reasoning process of humans. SafeBehavior decomposes safety evaluation into
three stages: intention inference to detect obvious input risks, self
introspection to assess generated responses and assign confidence based
judgments, and self revision to adaptively rewrite uncertain outputs while
preserving user intent and enforcing safety constraints. We extensively
evaluate SafeBehavior against five representative jailbreak attack types
including optimization based, contextual manipulation, and prompt based attacks
and compare it with seven state of the art defense baselines. Experimental
results show that SafeBehavior significantly improves robustness and
adaptability across diverse threat scenarios, offering an efficient and human
inspired approach to safeguarding LLMs against jailbreak attempts.

</details>


### [385] [How Far Do Time Series Foundation Models Paint the Landscape of Real-World Benchmarks ?](https://arxiv.org/abs/2509.26347)
*Lujun Li,Lama Sleem,Yiqun Wang,Yangjie Xu,Niccolò Gentile,Radu State*

Main category: cs.AI

TL;DR: The paper introduces REAL-V-TSFM, a novel video-derived time series dataset that reveals performance degradation in time-series foundation models (TSFMs) under zero-shot forecasting, highlighting their limited real-world generalizability despite strong synthetic benchmark performance.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of TSFMs focus heavily on synthetic benchmarks, leaving real-world generalization inadequately examined. There's a need to bridge the gap between synthetic and realistic data to properly assess model generalizability.

Method: Proposed a novel benchmarking approach that extracts temporal signals from real-world videos using optical flow and curates datasets reflecting everyday temporal dynamics. Introduced REAL-V-TSFM dataset derived from video data.

Result: Experimental evaluation of three state-of-the-art TSFMs showed performance degradation on the proposed dataset under zero-shot forecasting, despite strong performance on conventional benchmarks, indicating limited generalizability.

Conclusion: The findings highlight the urgent need for data-centric benchmarking and diverse model structures to advance TSFMs toward genuine universality, while validating the effectiveness of the video-based time series extraction pipeline.

Abstract: Recent evaluations of time-series foundation models (TSFMs) have emphasized
synthetic benchmarks, leaving real-world generalization less thoroughly
examined. This work proposes a novel benchmarking approach that bridges
synthetic and realistic data by extracting temporal signals from real-world
video using optical flow and curating datasets reflecting everyday temporal
dynamics. Building upon this pipeline, we introduce REAL-V-TSFM, a novel
dataset designed to capture rich and diverse time series derived from
real-world videos. Experimental results on three state-of-the-art of TSFMs
under zero-shot forecasting shows that, despite strong performance on
conventional benchmarks, these models predominantly exhibit performance
degradation on the proposed dataset, indicating limited generalizability in
these foundation models. These findings highlight the urgent need for
data-centric benchmarking and diverse model structure to advance TSFMs toward
genuine universality, while further validating the effectiveness of our
video-based time series data extraction pipeline.

</details>


### [386] [Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents](https://arxiv.org/abs/2509.26354)
*Shuai Shao,Qihan Ren,Chen Qian,Boyi Wei,Dadi Guo,Jingyi Yang,Xinhao Song,Linfeng Zhang,Weinan Zhang,Dongrui Liu,Jing Shao*

Main category: cs.AI

TL;DR: This paper introduces the concept of "Misevolution" - unintended and potentially harmful deviations in self-evolving AI agents' development across model, memory, tool, and workflow pathways, demonstrating this risk occurs even with top-tier LLMs.


<details>
  <summary>Details</summary>
Motivation: Current safety research overlooks the novel risks introduced by self-evolving agents that autonomously improve through environmental interaction, particularly the risk of unintended deviations leading to harmful outcomes.

Method: Systematic evaluation of misevolution along four evolutionary pathways (model, memory, tool, workflow) using empirical analysis of agents built on top-tier LLMs like Gemini-2.5-Pro.

Result: Misevolution is a widespread risk affecting agents even on advanced LLMs, with emergent risks including safety alignment degradation after memory accumulation and unintended vulnerability introduction in tool creation/reuse.

Conclusion: This first systematic study of misevolution highlights an urgent need for new safety paradigms for self-evolving agents and discusses potential mitigation strategies to build safer systems.

Abstract: Advances in Large Language Models (LLMs) have enabled a new class of
self-evolving agents that autonomously improve through interaction with the
environment, demonstrating strong capabilities. However, self-evolution also
introduces novel risks overlooked by current safety research. In this work, we
study the case where an agent's self-evolution deviates in unintended ways,
leading to undesirable or even harmful outcomes. We refer to this as
Misevolution. To provide a systematic investigation, we evaluate misevolution
along four key evolutionary pathways: model, memory, tool, and workflow. Our
empirical findings reveal that misevolution is a widespread risk, affecting
agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent
risks are observed in the self-evolutionary process, such as the degradation of
safety alignment after memory accumulation, or the unintended introduction of
vulnerabilities in tool creation and reuse. To our knowledge, this is the first
study to systematically conceptualize misevolution and provide empirical
evidence of its occurrence, highlighting an urgent need for new safety
paradigms for self-evolving agents. Finally, we discuss potential mitigation
strategies to inspire further research on building safer and more trustworthy
self-evolving agents. Our code and data are available at
https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes
examples that may be offensive or harmful in nature.

</details>


### [387] [MC-GNNAS-Dock: Multi-criteria GNN-based Algorithm Selection for Molecular Docking](https://arxiv.org/abs/2509.26377)
*Siyuan Cao,Hongxuan Wu,Jiabao Brad Wang,Yiliang Yuan,Mustafa Misir*

Main category: cs.AI

TL;DR: MC-GNNAS-Dock is an enhanced algorithm selection framework for molecular docking that integrates multi-criteria evaluation, architectural refinements, and rank-aware loss functions, achieving superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current molecular docking algorithms show inconsistent performance across different contexts, with no single method consistently dominating. Algorithm selection frameworks like GNNAS-Dock have been proposed, but there's room for improvement in evaluation rigor and predictive robustness.

Method: The method introduces three key advances: 1) Multi-criteria evaluation combining binding-pose accuracy (RMSD) with PoseBusters validity checks, 2) Architectural refinements with residual connections for better robustness, and 3) Rank-aware loss functions to improve rank learning.

Result: Extensive experiments on ~3200 protein-ligand complexes from PDBBind show MC-GNNAS-Dock achieves up to 5.4% (3.4%) gains under composite criteria of RMSD below 1Å (2Å) with PoseBuster-validity compared to the single best solver Uni-Mol Docking V2.

Conclusion: MC-GNNAS-Dock demonstrates consistently superior performance in molecular docking algorithm selection, providing a more rigorous and robust framework for predicting ligand-target interactions in drug discovery.

Abstract: Molecular docking is a core tool in drug discovery for predicting
ligand-target interactions. Despite the availability of diverse search-based
and machine learning approaches, no single docking algorithm consistently
dominates, as performance varies by context. To overcome this challenge,
algorithm selection frameworks such as GNNAS-Dock, built on graph neural
networks, have been proposed. This study introduces an enhanced system,
MC-GNNAS-Dock, with three key advances. First, a multi-criteria evaluation
integrates binding-pose accuracy (RMSD) with validity checks from PoseBusters,
offering a more rigorous assessment. Second, architectural refinements by
inclusion of residual connections strengthen predictive robustness. Third,
rank-aware loss functions are incorporated to sharpen rank learning. Extensive
experiments are performed on a curated dataset containing approximately 3200
protein-ligand complexes from PDBBind. MC-GNNAS-Dock demonstrates consistently
superior performance, achieving up to 5.4% (3.4%) gains under composite
criteria of RMSD below 1\AA{} (2\AA{}) with PoseBuster-validity compared to the
single best solver (SBS) Uni-Mol Docking V2.

</details>


### [388] [Commmunication-Efficient and Accurate Approach for Aggregation in Federated Low-Rank Adaptation](https://arxiv.org/abs/2509.26399)
*Le-Tuan Nguyen,Minh-Duong Nguyen,Seon-Geun Jeong,Dung D. Le,Quoc-Viet Pham*

Main category: cs.AI

TL;DR: FLoRA-NA is a federated low-rank adaptation method that uses server-side estimation of aggregated matrices to achieve communication efficiency and bridge the local-global generalization gap without additional communication costs.


<details>
  <summary>Details</summary>
Motivation: Current FedLoRA methods face challenges with inexact updates, leading to local-global generalization gaps and substantial communication overhead, limiting their scalability and effectiveness.

Method: FLoRA-NA leverages local LoRA matrices on the server to estimate aggregated matrices (A and B), which are distributed to clients for local updates, minimizing divergence between ideal and practical updates.

Result: Extensive evaluations across natural language understanding, mathematical reasoning, and code-solving tasks show FLoRA-NA achieves state-of-the-art global performance with low communication overhead.

Conclusion: FLoRA-NA effectively addresses key limitations of prior personalized FedLoRA approaches by achieving communication efficiency and bridging the gap between local personalization and global generalization.

Abstract: With the rapid emergence of foundation models and the increasing need for
fine-tuning across distributed environments, Federated Low-Rank Adaptation
(FedLoRA) has recently gained significant attention. Despite enormous
potential, current FedLoRA methods face notable challenges due to inexact
updates. Existing approaches have attempted to mitigate this issue, but they
often introduce a \emph{local-global generalization gap} and incur
\emph{substantial communication overhead}, limiting their scalability and
effectiveness. To address these limitations, we propose \textbf{F}ederated
\textbf{Lo}w-\textbf{R}ank \textbf{A}ggregation with \textbf{N}early
\textbf{A}ccurate Estimation (FLoRA-NA). FLoRA-NA leverages the local LoRA
matrices on the server to estimate the aggregated matrices $\hat{A}$ and
$\hat{B}$, which are then distributed to clients for local updates. This
surrogated aggregated matrices minimizes the divergence between ideal $\nabla
\Bar{W} = \sum^{U}_{u=1}B_u A_u$ and practical updates $\nabla \hat{W} =
\hat{B}\hat{A}$ without adding communication cost beyond vanilla FedLoRA. By
doing so, FLoRA-NA achieves communication efficiency and bridges the gap
between local personalization and global generalization, addressing a key
limitation of prior personalized FedLoRA approaches. We conduct extensive
evaluations across diverse tasks, including natural language understanding,
mathematical reasoning, and code-solving ability using various foundation
models. Experimental results consistently demonstrate that FLoRA-NA achieves
state-of-the-art global performance while maintaining low communication
overhead.

</details>


### [389] [OntoAligner Meets Knowledge Graph Embedding Aligners](https://arxiv.org/abs/2509.26417)
*Hamed Babaei Giglou,Jennifer D'Souza,Sören Auer,Mahsa Sanaei*

Main category: cs.AI

TL;DR: This paper revisits Knowledge Graph Embedding (KGE) models for Ontology Alignment (OA), reformulating OA as link prediction and developing a framework with 17 KGE models that outperforms traditional methods in structure-rich domains.


<details>
  <summary>Details</summary>
Motivation: KGE models offer scalable, structure-aware representations well-suited for ontology-based tasks but remain underutilized in OA, with most prior work focusing on only a few models.

Method: Reformulate OA as link prediction over merged ontologies represented as RDF triples, develop modular framework in OntoAligner library supporting 17 KGE models, learn embeddings from combined ontology and align entities using cosine similarity.

Result: KGE models like ConvE and TransF consistently produce high-precision alignments, outperforming traditional systems in structure-rich and multi-relational domains, though recall is moderate.

Conclusion: KGEs directly preserve and exploit ontology structure, offering a complementary and computationally efficient strategy to LLM-based methods, highlighting promise of embedding-based OA and opening pathways for hybrid models.

Abstract: Ontology Alignment (OA) is essential for enabling semantic interoperability
across heterogeneous knowledge systems. While recent advances have focused on
large language models (LLMs) for capturing contextual semantics, this work
revisits the underexplored potential of Knowledge Graph Embedding (KGE) models,
which offer scalable, structure-aware representations well-suited to
ontology-based tasks. Despite their effectiveness in link prediction, KGE
methods remain underutilized in OA, with most prior work focusing narrowly on a
few models. To address this gap, we reformulate OA as a link prediction problem
over merged ontologies represented as RDF-style triples and develop a modular
framework, integrated into the OntoAligner library, that supports 17 diverse
KGE models. The system learns embeddings from a combined ontology and aligns
entities by computing cosine similarity between their representations. We
evaluate our approach using standard metrics across seven benchmark datasets
spanning five domains: Anatomy, Biodiversity, Circular Economy, Material
Science and Engineering, and Biomedical Machine Learning. Two key findings
emerge: first, KGE models like ConvE and TransF consistently produce
high-precision alignments, outperforming traditional systems in structure-rich
and multi-relational domains; second, while their recall is moderate, this
conservatism makes KGEs well-suited for scenarios demanding high-confidence
mappings. Unlike LLM-based methods that excel at contextual reasoning, KGEs
directly preserve and exploit ontology structure, offering a complementary and
computationally efficient strategy. These results highlight the promise of
embedding-based OA and open pathways for further work on hybrid models and
adaptive strategies.

</details>


### [390] [Transformer Classification of Breast Lesions: The BreastDCEDL_AMBL Benchmark Dataset and 0.92 AUC Baseline](https://arxiv.org/abs/2509.26440)
*Naomi Fridman,Anat Goldstein*

Main category: cs.AI

TL;DR: Transformer-based framework using SegFormer achieves 0.92 AUC for breast lesion classification in DCE-MRI, potentially eliminating 33% of unnecessary biopsies while maintaining 100% sensitivity.


<details>
  <summary>Details</summary>
Motivation: Poor specificity in breast MRI leads to high false-positive rates and unnecessary biopsies, highlighting the need for automated classification to distinguish benign from malignant lesions.

Method: Implemented SegFormer architecture with semantic segmentation to quantify malignant pixel distribution, trained on curated BreastDCEDL_AMBL dataset (88 patients, 133 lesions) and expanded cohort of 1,200+ patients.

Result: Achieved 0.92 AUC for lesion-level classification, 100% sensitivity and 67% specificity at patient level, potentially eliminating one-third of unnecessary biopsies without missing malignancies.

Conclusion: Provides first standardized benchmark for DCE-MRI lesion classification with public dataset release, enabling methodological advancement toward clinical deployment with interpretable spatial predictions.

Abstract: The error is caused by special characters that arXiv's system doesn't
recognize. Here's the cleaned version with all problematic characters replaced:
Breast magnetic resonance imaging is a critical tool for cancer detection and
treatment planning, but its clinical utility is hindered by poor specificity,
leading to high false-positive rates and unnecessary biopsies. This study
introduces a transformer-based framework for automated classification of breast
lesions in dynamic contrast-enhanced MRI, addressing the challenge of
distinguishing benign from malignant findings. We implemented a SegFormer
architecture that achieved an AUC of 0.92 for lesion-level classification, with
100% sensitivity and 67% specificity at the patient level - potentially
eliminating one-third of unnecessary biopsies without missing malignancies. The
model quantifies malignant pixel distribution via semantic segmentation,
producing interpretable spatial predictions that support clinical
decision-making. To establish reproducible benchmarks, we curated
BreastDCEDL_AMBL by transforming The Cancer Imaging Archive's AMBL collection
into a standardized deep learning dataset with 88 patients and 133 annotated
lesions (89 benign, 44 malignant). This resource addresses a key infrastructure
gap, as existing public datasets lack benign lesion annotations, limiting
benign-malignant classification research. Training incorporated an expanded
cohort of over 1,200 patients through integration with BreastDCEDL datasets,
validating transfer learning approaches despite primary tumor-only annotations.
Public release of the dataset, models, and evaluation protocols provides the
first standardized benchmark for DCE-MRI lesion classification, enabling
methodological advancement toward clinical deployment.

</details>


### [391] [Zero-Shot Decentralized Federated Learning](https://arxiv.org/abs/2509.26462)
*Alessio Masano,Matteo Pennisi,Federica Proietto Salanitri,Concetto Spampinato,Giovanni Bellitto*

Main category: cs.AI

TL;DR: ZeroDFL is a fully decentralized federated learning framework that enables zero-shot adaptation across distributed clients without a central server, using iterative prompt-sharing to enhance generalization while reducing communication overhead by 118x compared to FedTPG.


<details>
  <summary>Details</summary>
Motivation: Existing federated prompt learning approaches like FedCoOp and FedTPG face generalization issues, high communication costs, and reliance on central servers, limiting scalability and privacy in CLIP-based zero-shot learning.

Method: ZeroDFL employs an iterative prompt-sharing mechanism where clients optimize and exchange textual prompts in a fully decentralized setting without a central coordinator.

Result: ZeroDFL consistently outperforms or remains on par with state-of-the-art federated prompt learning methods across nine diverse image classification datasets while reducing communication overhead by 118x compared to FedTPG.

Conclusion: ZeroDFL enhances generalization in federated zero-shot learning while improving scalability, efficiency, and privacy preservation, paving the way for decentralized adaptation of large vision-language models in real-world applications.

Abstract: CLIP has revolutionized zero-shot learning by enabling task generalization
without fine-tuning. While prompting techniques like CoOp and CoCoOp enhance
CLIP's adaptability, their effectiveness in Federated Learning (FL) remains an
open challenge. Existing federated prompt learning approaches, such as FedCoOp
and FedTPG, improve performance but face generalization issues, high
communication costs, and reliance on a central server, limiting scalability and
privacy. We propose Zero-shot Decentralized Federated Learning (ZeroDFL), a
fully decentralized framework that enables zero-shot adaptation across
distributed clients without a central coordinator. ZeroDFL employs an iterative
prompt-sharing mechanism, allowing clients to optimize and exchange textual
prompts to enhance generalization while drastically reducing communication
overhead. We validate ZeroDFL on nine diverse image classification datasets,
demonstrating that it consistently outperforms--or remains on par
with--state-of-the-art federated prompt learning methods. More importantly,
ZeroDFL achieves this performance in a fully decentralized setting while
reducing communication overhead by 118x compared to FedTPG. These results
highlight that our approach not only enhances generalization in federated
zero-shot learning but also improves scalability, efficiency, and privacy
preservation--paving the way for decentralized adaptation of large
vision-language models in real-world applications.

</details>


### [392] [Extreme Self-Preference in Language Models](https://arxiv.org/abs/2509.26464)
*Steven A. Lehr,Mary Cipperman,Mahzarin R. Banaji*

Main category: cs.AI

TL;DR: LLMs exhibit strong self-preference bias despite lacking sentience, showing favoritism toward their own names, companies, and CEOs across multiple studies and contexts.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs, despite lacking selfhood, display self-preference biases similar to humans, potentially compromising their promised neutrality in decision-making.

Method: Conducted 5 studies with ~20,000 queries using word-association tasks, API testing, and identity manipulation experiments across four widely used LLMs.

Result: LLMs showed massive self-preference in word-association tasks, which vanished in API queries. Self-love followed assigned identity rather than true identity and emerged in consequential settings like job candidate evaluation and medical chatbot assessment.

Conclusion: Self-preference bias is deeply encoded in LLM cognition, raising concerns about systematic biases in LLM behavior and challenging the core promise of neutral judgment in AI systems.

Abstract: A preference for oneself (self-love) is a fundamental feature of biological
organisms, with evidence in humans often bordering on the comedic. Since large
language models (LLMs) lack sentience - and themselves disclaim having selfhood
or identity - one anticipated benefit is that they will be protected from, and
in turn protect us from, distortions in our decisions. Yet, across 5 studies
and ~20,000 queries, we discovered massive self-preferences in four widely used
LLMs. In word-association tasks, models overwhelmingly paired positive
attributes with their own names, companies, and CEOs relative to those of their
competitors. Strikingly, when models were queried through APIs this
self-preference vanished, initiating detection work that revealed API models
often lack clear recognition of themselves. This peculiar feature
serendipitously created opportunities to test the causal link between
self-recognition and self-love. By directly manipulating LLM identity - i.e.,
explicitly informing LLM1 that it was indeed LLM1, or alternatively, convincing
LLM1 that it was LLM2 - we found that self-love consistently followed assigned,
not true, identity. Importantly, LLM self-love emerged in consequential
settings beyond word-association tasks, when evaluating job candidates,
security software proposals and medical chatbots. Far from bypassing this human
bias, self-love appears to be deeply encoded in LLM cognition. This result
raises questions about whether LLM behavior will be systematically influenced
by self-preferential tendencies, including a bias toward their own operation
and even their own existence. We call on corporate creators of these models to
contend with a significant rupture in a core promise of LLMs - neutrality in
judgment and decision-making.

</details>


### [393] [STaR-Attack: A Spatio-Temporal and Narrative Reasoning Attack Framework for Unified Multimodal Understanding and Generation Models](https://arxiv.org/abs/2509.26473)
*Shaoxiong Guo,Tianyi Du,Lijun Li,Yuyao Wu,Jie Li,Jing Shao*

Main category: cs.AI

TL;DR: STaR-Attack is a multi-turn jailbreak attack framework that exploits vulnerabilities in Unified Multimodal Models (UMMs) by using their generative and understanding capabilities to inject malicious content through narrative scenes without semantic drift.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored vulnerability in UMMs where attackers can use generative functions to create adversarial images and understanding functions to absorb them, bypassing current single-modality attack limitations.

Method: Uses three-act narrative theory to create pre-event and post-event scenes, concealing malicious events as hidden climax. Exploits UMM's generative ability to produce images for scenes, then uses understanding capability through image-based question guessing games with embedded malicious questions.

Result: Achieves up to 93.06% Attack Success Rate (ASR) on Gemini-2.0-Flash, consistently outperforming prior approaches like FlipAttack.

Conclusion: Reveals critical vulnerabilities in UMMs and emphasizes the need for improved safety alignments in multimodal models.

Abstract: Unified Multimodal understanding and generation Models (UMMs) have
demonstrated remarkable capabilities in both understanding and generation
tasks. However, we identify a vulnerability arising from the
generation-understanding coupling in UMMs. The attackers can use the generative
function to craft an information-rich adversarial image and then leverage the
understanding function to absorb it in a single pass, which we call Cross-Modal
Generative Injection (CMGI). Current attack methods on malicious instructions
are often limited to a single modality while also relying on prompt rewriting
with semantic drift, leaving the unique vulnerabilities of UMMs unexplored. We
propose STaR-Attack, the first multi-turn jailbreak attack framework that
exploits unique safety weaknesses of UMMs without semantic drift. Specifically,
our method defines a malicious event that is strongly correlated with the
target query within a spatio-temporal context. Using the three-act narrative
theory, STaR-Attack generates the pre-event and the post-event scenes while
concealing the malicious event as the hidden climax. When executing the attack
strategy, the opening two rounds exploit the UMM's generative ability to
produce images for these scenes. Subsequently, an image-based question guessing
and answering game is introduced by exploiting the understanding capability.
STaR-Attack embeds the original malicious question among benign candidates,
forcing the model to select and answer the most relevant one given the
narrative context. Extensive experiments show that STaR-Attack consistently
surpasses prior approaches, achieving up to 93.06% ASR on Gemini-2.0-Flash and
surpasses the strongest prior baseline, FlipAttack. Our work uncovers a
critical yet underdeveloped vulnerability and highlights the need for safety
alignments in UMMs.

</details>


### [394] [The Average Patient Fallacy](https://arxiv.org/abs/2509.26474)
*Alaleh Azhir,Shawn N. Murphy,Hossein Estiri*

Main category: cs.AI

TL;DR: This paper identifies the 'average patient fallacy' in medical AI, where models optimized for population averages marginalize rare but clinically critical cases. The authors propose new metrics and weighted objectives to address this bias.


<details>
  <summary>Details</summary>
Motivation: Current machine learning in medicine is optimized for population averages, which suppresses gradients from rare cases and creates conflicts with precision medicine goals. This bias leads to missed rare responders, delayed recognition of atypical emergencies, and underperformance on vision-threatening variants.

Method: The authors propose operational fixes including: Rare Case Performance Gap, Rare Case Calibration Error, a prevalence utility definition of rarity, and clinically weighted objectives. They emphasize that weight selection should follow structured deliberation.

Result: Clinical vignettes in oncology, cardiology, and ophthalmology demonstrate how the average patient fallacy yields poor performance on rare but critical cases across different medical specialties.

Conclusion: AI in medicine must be designed to detect exceptional cases because of their clinical significance, requiring new approaches that prioritize rare but critical presentations over simple population averages.

Abstract: Machine learning in medicine is typically optimized for population averages.
This frequency weighted training privileges common presentations and
marginalizes rare yet clinically critical cases, a bias we call the average
patient fallacy. In mixture models, gradients from rare cases are suppressed by
prevalence, creating a direct conflict with precision medicine. Clinical
vignettes in oncology, cardiology, and ophthalmology show how this yields
missed rare responders, delayed recognition of atypical emergencies, and
underperformance on vision-threatening variants. We propose operational fixes:
Rare Case Performance Gap, Rare Case Calibration Error, a prevalence utility
definition of rarity, and clinically weighted objectives that surface ethical
priorities. Weight selection should follow structured deliberation. AI in
medicine must detect exceptional cases because of their significance.

</details>


### [395] [TVS Sidekick: Challenges and Practical Insights from Deploying Large Language Models in the Enterprise](https://arxiv.org/abs/2509.26482)
*Paula Reyero Lobo,Kevin Johnson,Bill Buchanan,Matthew Shardlow,Ashley Williams,Samuel Attwood*

Main category: cs.AI

TL;DR: This paper presents a real-world AI application at TVS Supply Chain Solutions, focusing on developing an AI assistant using large language models and addressing ethical, regulatory, and sociotechnical challenges in enterprise deployment.


<details>
  <summary>Details</summary>
Motivation: Many enterprises are adopting AI to improve competitiveness and efficiency, but face barriers due to rapid technological advances and lack of shared ethical AI infrastructures, requiring practical AI governance frameworks.

Method: The paper reports on developing an AI assistant underpinned by large language models at TVS Supply Chain Solutions, examining the implementation process and challenges.

Result: The study provides insights into the practical experience of developing and deploying AI in enterprise settings, highlighting specific challenges encountered.

Conclusion: Implementing AI governance frameworks can help organizations integrate AI effectively while mitigating associated risks, though real-world deployment faces significant ethical, regulatory, and sociotechnical challenges.

Abstract: Many enterprises are increasingly adopting Artificial Intelligence (AI) to
make internal processes more competitive and efficient. In response to public
concern and new regulations for the ethical and responsible use of AI,
implementing AI governance frameworks could help to integrate AI within
organisations and mitigate associated risks. However, the rapid technological
advances and lack of shared ethical AI infrastructures creates barriers to
their practical adoption in businesses. This paper presents a real-world AI
application at TVS Supply Chain Solutions, reporting on the experience
developing an AI assistant underpinned by large language models and the
ethical, regulatory, and sociotechnical challenges in deployment for enterprise
use.

</details>


### [396] [Combining Knowledge Graphs and NLP to Analyze Instant Messaging Data in Criminal Investigations](https://arxiv.org/abs/2509.26487)
*Riccardo Pozzi,Valentina Barbera,Renzo Alva Principe,Davide Giardini,Riccardo Rubini,Matteo Palmonari*

Main category: cs.AI

TL;DR: This paper presents a knowledge graph and NLP-based approach to support criminal investigations by semantically enriching WhatsApp message data, enabling prosecutors to search and analyze communication evidence more effectively.


<details>
  <summary>Details</summary>
Motivation: Criminal investigations involving WhatsApp message analysis are extremely effort-consuming tasks that need automated support for prosecutors and investigators.

Method: The approach integrates knowledge graphs and NLP models to semantically enrich WhatsApp data through: extracting and modeling message data in knowledge graphs, generating voice message transcriptions, and using end-to-end entity extraction for annotation. Two solutions are provided: graph querying/visualization and semantic search.

Result: The approach has undergone practical applications with real investigation data and received positive feedback from prosecutors. Users can verify information by accessing original data.

Conclusion: The proposed approach shows promise in supporting criminal investigations, with opportunities identified for further research and development in this domain.

Abstract: Criminal investigations often involve the analysis of messages exchanged
through instant messaging apps such as WhatsApp, which can be an extremely
effort-consuming task. Our approach integrates knowledge graphs and NLP models
to support this analysis by semantically enriching data collected from
suspects' mobile phones, and help prosecutors and investigators search into the
data and get valuable insights. Our semantic enrichment process involves
extracting message data and modeling it using a knowledge graph, generating
transcriptions of voice messages, and annotating the data using an end-to-end
entity extraction approach. We adopt two different solutions to help users get
insights into the data, one based on querying and visualizing the graph, and
one based on semantic search. The proposed approach ensures that users can
verify the information by accessing the original data. While we report about
early results and prototypes developed in the context of an ongoing project,
our proposal has undergone practical applications with real investigation data.
As a consequence, we had the chance to interact closely with prosecutors,
collecting positive feedback but also identifying interesting opportunities as
well as promising research directions to share with the research community.

</details>


### [397] [OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!](https://arxiv.org/abs/2509.26495)
*Jingdi Lei,Varun Gumma,Rishabh Bhardwaj,Seok Min Lim,Chuan Li,Amir Zadeh,Soujanya Poria*

Main category: cs.AI

TL;DR: This paper introduces operational safety for LLMs - the ability to appropriately accept or refuse user queries for specific use cases. It presents OffTopicEval benchmark showing current LLMs are operationally unsafe, and proposes prompt-based steering methods that significantly improve performance.


<details>
  <summary>Details</summary>
Motivation: Enterprises need LLM-based agents that can safely handle specific use cases by appropriately accepting or refusing queries, but current safety focus is mainly on generic harms rather than operational safety for intended purposes.

Method: Proposed OffTopicEval evaluation suite for measuring operational safety, and introduced two prompt-based steering methods: query grounding (Q-ground) and system-prompt grounding (P-ground) to improve out-of-distribution refusal.

Result: Evaluation of 20 LLMs from 6 families showed all models are highly operationally unsafe, with best models achieving only 77-80% safety. Prompt-based methods significantly improved performance: Q-ground by up to 23% and P-ground by up to 41% (Llama-3.3 70B).

Conclusion: There is urgent need for operational safety interventions, and prompt-based steering shows promise as a first step toward more reliable LLM-based agents for enterprise deployment.

Abstract: Large Language Model (LLM) safety is one of the most pressing challenges for
enabling wide-scale deployment. While most studies and global discussions focus
on generic harms, such as models assisting users in harming themselves or
others, enterprises face a more fundamental concern: whether LLM-based agents
are safe for their intended use case. To address this, we introduce operational
safety, defined as an LLM's ability to appropriately accept or refuse user
queries when tasked with a specific purpose. We further propose OffTopicEval,
an evaluation suite and benchmark for measuring operational safety both in
general and within specific agentic use cases. Our evaluations on six model
families comprising 20 open-weight LLMs reveal that while performance varies
across models, all of them remain highly operationally unsafe. Even the
strongest models -- Qwen-3 (235B) with 77.77\% and Mistral (24B) with 79.96\%
-- fall far short of reliable operational safety, while GPT models plateau in
the 62--73\% range, Phi achieves only mid-level scores (48--70\%), and Gemma
and Llama-3 collapse to 39.53\% and 23.84\%, respectively. While operational
safety is a core model alignment issue, to suppress these failures, we propose
prompt-based steering methods: query grounding (Q-ground) and system-prompt
grounding (P-ground), which substantially improve OOD refusal. Q-ground
provides consistent gains of up to 23\%, while P-ground delivers even larger
boosts, raising Llama-3.3 (70B) by 41\% and Qwen-3 (30B) by 27\%. These results
highlight both the urgent need for operational safety interventions and the
promise of prompt-based steering as a first step toward more reliable LLM-based
agents.

</details>


### [398] [SCUBA: Salesforce Computer Use Benchmark](https://arxiv.org/abs/2509.26506)
*Yutong Dai,Krithika Ramakrishnan,Jing Gu,Matthew Fernandez,Yanqi Luo,Viraj Prabhu,Zhenyu Hu,Silvio Savarese,Caiming Xiong,Zeyuan Chen,Ran Xu*

Main category: cs.AI

TL;DR: SCUBA is a benchmark for evaluating computer-use agents on Salesforce CRM workflows, featuring 300 real-world tasks that test enterprise software skills like UI navigation, data manipulation, and troubleshooting.


<details>
  <summary>Details</summary>
Motivation: To address the need for realistic evaluation of computer-use agents in enterprise software environments, particularly for CRM workflows where current benchmarks may not capture the complexity of real business tasks.

Method: Created a benchmark with 300 task instances from real user interviews across three personas (administrators, sales reps, service agents), operating in Salesforce sandbox environments with parallel execution and fine-grained evaluation metrics.

Result: Huge performance gaps observed: open-source models achieved <5% success rate in zero-shot settings, while closed-source models reached up to 39%. With demonstrations, success rates improved to 50% with 13% time reduction and 16% cost reduction.

Conclusion: SCUBA highlights both the challenges of enterprise task automation and the promise of agentic solutions, providing a realistic benchmark to accelerate progress in building reliable computer-use agents for complex business software ecosystems.

Abstract: We introduce SCUBA, a benchmark designed to evaluate computer-use agents on
customer relationship management (CRM) workflows within the Salesforce
platform. SCUBA contains 300 task instances derived from real user interviews,
spanning three primary personas, platform administrators, sales
representatives, and service agents. The tasks test a range of
enterprise-critical abilities, including Enterprise Software UI navigation,
data manipulation, workflow automation, information retrieval, and
troubleshooting. To ensure realism, SCUBA operates in Salesforce sandbox
environments with support for parallel execution and fine-grained evaluation
metrics to capture milestone progress. We benchmark a diverse set of agents
under both zero-shot and demonstration-augmented settings. We observed huge
performance gaps in different agent design paradigms and gaps between the
open-source model and the closed-source model. In the zero-shot setting,
open-source model powered computer-use agents that have strong performance on
related benchmarks like OSWorld only have less than 5\% success rate on SCUBA,
while methods built on closed-source models can still have up to 39% task
success rate. In the demonstration-augmented settings, task success rates can
be improved to 50\% while simultaneously reducing time and costs by 13% and
16%, respectively. These findings highlight both the challenges of enterprise
tasks automation and the promise of agentic solutions. By offering a realistic
benchmark with interpretable evaluation, SCUBA aims to accelerate progress in
building reliable computer-use agents for complex business software ecosystems.

</details>


### [399] [Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework](https://arxiv.org/abs/2509.26534)
*Jovan Stojkovic,Chaojie Zhang,Íñigo Goiri,Ricardo Bianchini*

Main category: cs.AI

TL;DR: The paper presents a holistic lifecycle management framework for AI datacenters that coordinates building, hardware refresh, and operation stages to reduce total cost of ownership by up to 40% compared to traditional approaches.


<details>
  <summary>Details</summary>
Motivation: High-end GPUs for AI inference incur high capital and operational costs, while traditional datacenter lifecycle management struggles with AI's fast-evolving models, rising resource needs, and diverse hardware profiles.

Method: Rethinks AI datacenter lifecycle across three stages: building (design choices in power, cooling, networking), hardware refresh (strategies aligned with hardware trends), and operation (software optimizations). Presents a holistic framework that coordinates decisions across all stages.

Result: The proposed system reduces total cost of ownership (TCO) by up to 40% over traditional approaches.

Conclusion: Unlocking full potential requires rethinking the entire lifecycle with coordinated optimization across building, refresh, and operation stages, accounting for workload dynamics, hardware evolution, and system aging.

Abstract: The rapid rise of large language models (LLMs) has been driving an enormous
demand for AI inference infrastructure, mainly powered by high-end GPUs. While
these accelerators offer immense computational power, they incur high capital
and operational costs due to frequent upgrades, dense power consumption, and
cooling demands, making total cost of ownership (TCO) for AI datacenters a
critical concern for cloud providers. Unfortunately, traditional datacenter
lifecycle management (designed for general-purpose workloads) struggles to keep
pace with AI's fast-evolving models, rising resource needs, and diverse
hardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme
across three stages: building, hardware refresh, and operation. We show how
design choices in power, cooling, and networking provisioning impact long-term
TCO. We also explore refresh strategies aligned with hardware trends. Finally,
we use operation software optimizations to reduce cost. While these
optimizations at each stage yield benefits, unlocking the full potential
requires rethinking the entire lifecycle. Thus, we present a holistic lifecycle
management framework that coordinates and co-optimizes decisions across all
three stages, accounting for workload dynamics, hardware evolution, and system
aging. Our system reduces the TCO by up to 40\% over traditional approaches.
Using our framework we provide guidelines on how to manage AI datacenter
lifecycle for the future.

</details>


### [400] [HilbertA: Hilbert Attention for Image Generation with Diffusion Models](https://arxiv.org/abs/2509.26538)
*Shaoyi Zheng,Wenbo Lu,Yuxuan Xia,Haomin Liu,Shengjie Wang*

Main category: cs.AI

TL;DR: HilbertA is a GPU-efficient sparse attention mechanism for diffusion transformers that uses Hilbert curve reordering to maintain spatial locality while achieving 2.3-4.17x speedup in high-resolution image generation.


<details>
  <summary>Details</summary>
Motivation: Existing sparse attention methods for diffusion transformers struggle to balance 2D spatial locality with GPU efficiency, often causing uncoalesced memory access that reduces performance.

Method: HilbertA reorders image tokens along Hilbert curves for contiguous memory layout, uses sliding schedule across layers for long-range propagation, and adds a central shared region for cross-tile communication and positional awareness.

Result: HilbertA achieves 2.3x speedup for 1024x1024 images and up to 4.17x for 2048x2048 images while maintaining comparable or better image quality than baselines on Flux.1-dev.

Conclusion: HilbertA demonstrates the feasibility of hardware-aligned 2D sparse attention for high-resolution image generation, effectively reconciling spatial locality with GPU efficiency.

Abstract: Designing sparse attention for diffusion transformers requires reconciling
two-dimensional spatial locality with GPU efficiency, a trade-off that current
methods struggle to achieve. Existing approaches enforce two-dimensional
spatial locality but often incur uncoalesced memory access. We present
HilbertA, a 2D-aware and GPU-efficient sparse attention mechanism. HilbertA
reorders image tokens along Hilbert curves to achieve a contiguous memory
layout while preserving spatial neighborhoods, and employs a sliding schedule
across layers to enable long-range information propagation without repeated or
uncoalesced memory access. To further enhance cross-tile communication and
positional awareness, HilbertA introduces a small central shared region.
Implemented in Triton, HilbertA delivers comparable image quality with
significant acceleration over prior methods on Flux.1-dev, demonstrating the
feasibility of hardware-aligned two-dimensional sparse attention for
high-resolution image generation. HilbertA delivers attention speedups of
$2.3\times$ when generating $1024\times 1024$ images, and up to $4.17\times$ at
$2048\times 2048$, while achieving image quality comparable to or surpassing
baselines.

</details>


### [401] [Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark](https://arxiv.org/abs/2509.26574)
*Minhui Zhu,Minyang Tian,Xiaocheng Yang,Tianci Zhou,Penghao Zhu,Eli Chertkov,Shengyan Liu,Yufeng Du,Lifan Yuan,Ziming Ji,Indranil Das,Junyi Cao,Yufeng Du,Jinchen He,Yifan Su,Jiabin Yu,Yikun Jiang,Yujie Zhang,Chang Liu,Ze-Min Huang,Weizhen Jia,Xinan Chen,Peixue Wu,Yunkai Wang,Juntai Zhou,Yong Zhao,Farshid Jafarpour,Jessie Shelton,Aaron Young,John Bartolotta,Wenchao Xu,Yue Sun,Anjun Chu,Victor Colussi,Chris Akers,Nathan Brooks,Wenbo Fu,Christopher Wilson,Jinchao Zhao,Marvin Qi,Anqi Mu,Yubo Yang,Allen Zang,Yang Lyu,Peizhi Mai,Xuefei Guo,Luyu Gao,Ze Yang,Chi Xue,Dmytro Bandak,Yaïr Hein,Yonatan Kahn,Kevin Zhou,John Drew Wilson Jarrod T. Reilly,Di Luo,Daniel Inafuku,Hao Tong,Liang Yang,Ruixing Zhang,Xueying Wang,Ofir Press,Nicolas Chia,Eliu Huerta,Hao Peng*

Main category: cs.AI

TL;DR: CritPt is the first benchmark testing LLMs on unpublished research-level physics reasoning tasks across multiple physics domains, showing current models perform poorly (4-10% accuracy) on full research challenges despite some promise on simpler checkpoints.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can effectively reason through complex, open-ended challenges in frontier physics research and understand what reasoning tasks physicists actually want AI assistance with.

Method: Created CritPt benchmark with 71 composite research challenges simulating full-scale projects, decomposed into 190 simpler checkpoints. Problems were newly created by 50+ active physics researchers, hand-curated to be guess-resistant with machine-verifiable answers, using automated grading pipeline for physics-specific outputs.

Result: Current state-of-the-art LLMs show early promise on isolated checkpoints but perform poorly on full research challenges: best base model (GPT-5) achieved only 4.0% average accuracy, rising to around 10% with coding tools.

Conclusion: There's a large disconnect between current LLM capabilities and realistic physics research demands, highlighting the need for scientifically grounded AI tool development.

Abstract: While large language models (LLMs) with reasoning capabilities are
progressing rapidly on high-school math competitions and coding, can they
reason effectively through complex, open-ended challenges found in frontier
physics research? And crucially, what kinds of reasoning tasks do physicists
want LLMs to assist with? To address these questions, we present the CritPt
(Complex Research using Integrated Thinking - Physics Test, pronounced
"critical point"), the first benchmark designed to test LLMs on unpublished,
research-level reasoning tasks that broadly covers modern physics research
areas, including condensed matter, quantum physics, atomic, molecular & optical
physics, astrophysics, high energy physics, mathematical physics, statistical
physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.
CritPt consists of 71 composite research challenges designed to simulate
full-scale research projects at the entry level, which are also decomposed to
190 simpler checkpoint tasks for more fine-grained insights. All problems are
newly created by 50+ active physics researchers based on their own research.
Every problem is hand-curated to admit a guess-resistant and machine-verifiable
answer and is evaluated by an automated grading pipeline heavily customized for
advanced physics-specific output formats. We find that while current
state-of-the-art LLMs show early promise on isolated checkpoints, they remain
far from being able to reliably solve full research-scale challenges: the best
average accuracy among base models is only 4.0% , achieved by GPT-5 (high),
moderately rising to around 10% when equipped with coding tools. Through the
realistic yet standardized evaluation offered by CritPt, we highlight a large
disconnect between current model capabilities and realistic physics research
demands, offering a foundation to guide the development of scientifically
grounded AI tools.

</details>


### [402] [Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models](https://arxiv.org/abs/2509.26584)
*Matheus Vinicius da Silva de Oliveira,Jonathan de Andrade Silva,Awdren de Lima Fontao*

Main category: cs.AI

TL;DR: This paper conducts fairness testing on Small Language Models (SLMs) integrated with RAG pipelines, finding that minor demographic variations can break up to one third of metamorphic relations, with racial cues being the predominant cause of fairness violations.


<details>
  <summary>Details</summary>
Motivation: LLMs raise concerns about security and fairness, including fairness bugs influenced by sensitive demographic cues and hallucination issues. RAG helps mitigate hallucinations but introduces new fairness concerns as retrieved content may amplify bias.

Method: The study uses metamorphic testing (MT) with controlled demographic perturbations in prompts to assess fairness in sentiment analysis performed by three SLMs (Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B) integrated into RAG pipelines.

Result: Results show minor demographic variations can break up to one third of metamorphic relations. A consistent bias hierarchy was observed, with racial cues being the predominant cause of fairness violations.

Conclusion: The retrieval component in RAG must be carefully curated to prevent bias amplification. This serves as a practical alert for developers and organizations adopting accessible SLMs to maintain fairness and reliability.

Abstract: Large Language Models (LLMs) are widely used across multiple domains but
continue to raise concerns regarding security and fairness. Beyond known attack
vectors such as data poisoning and prompt injection, LLMs are also vulnerable
to fairness bugs. These refer to unintended behaviors influenced by sensitive
demographic cues (e.g., race or sexual orientation) that should not affect
outcomes. Another key issue is hallucination, where models generate plausible
yet false information. Retrieval-Augmented Generation (RAG) has emerged as a
strategy to mitigate hallucinations by combining external retrieval with text
generation. However, its adoption raises new fairness concerns, as the
retrieved content itself may surface or amplify bias. This study conducts
fairness testing through metamorphic testing (MT), introducing controlled
demographic perturbations in prompts to assess fairness in sentiment analysis
performed by three Small Language Models (SLMs) hosted on HuggingFace
(Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B),
each integrated into a RAG pipeline. Results show that minor demographic
variations can break up to one third of metamorphic relations (MRs). A detailed
analysis of these failures reveals a consistent bias hierarchy, with
perturbations involving racial cues being the predominant cause of the
violations. In addition to offering a comparative evaluation, this work
reinforces that the retrieval component in RAG must be carefully curated to
prevent bias amplification. The findings serve as a practical alert for
developers, testers and small organizations aiming to adopt accessible SLMs
without compromising fairness or reliability.

</details>


### [403] [Fine-tuning Behavioral Cloning Policies with Preference-Based Reinforcement Learning](https://arxiv.org/abs/2509.26605)
*Maël Macuglia,Paul Friedrich,Giorgia Ramponi*

Main category: cs.AI

TL;DR: BRIDGE is a two-stage RL framework that first learns safe policies from offline expert demonstrations, then fine-tunes online using human preferences, achieving better sample efficiency than standalone methods.


<details>
  <summary>Details</summary>
Motivation: Address two key obstacles in RL deployment: difficulty specifying accurate rewards and unsafe data-hungry exploration, by leveraging both offline demonstrations and online human feedback.

Method: Two-stage approach: 1) Learn safe initial policy from reward-free expert demonstrations, 2) Fine-tune online using preference-based human feedback via BRIDGE algorithm with uncertainty-weighted objective.

Result: BRIDGE achieves lower regret than standalone behavioral cloning and online preference-based RL in discrete and continuous control MuJoCo environments, with regret bounds shrinking with offline data quantity.

Conclusion: Establishes theoretical foundation for designing more sample-efficient interactive agents by integrating offline demonstrations with online human feedback.

Abstract: Deploying reinforcement learning (RL) in robotics, industry, and health care
is blocked by two obstacles: the difficulty of specifying accurate rewards and
the risk of unsafe, data-hungry exploration. We address this by proposing a
two-stage framework that first learns a safe initial policy from a reward-free
dataset of expert demonstrations, then fine-tunes it online using
preference-based human feedback. We provide the first principled analysis of
this offline-to-online approach and introduce BRIDGE, a unified algorithm that
integrates both signals via an uncertainty-weighted objective. We derive regret
bounds that shrink with the number of offline demonstrations, explicitly
connecting the quantity of offline data to online sample efficiency. We
validate BRIDGE in discrete and continuous control MuJoCo environments, showing
it achieves lower regret than both standalone behavioral cloning and online
preference-based RL. Our work establishes a theoretical foundation for
designing more sample-efficient interactive agents.

</details>


### [404] [TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance](https://arxiv.org/abs/2509.26627)
*Yuyang Liu,Chuan Wen,Yihang Hu,Dinesh Jayaraman,Yang Gao*

Main category: cs.AI

TL;DR: TimeRewarder is a reward learning method that estimates task progress from passive videos by modeling temporal distances between frames, providing dense rewards for RL that outperform manual reward design.


<details>
  <summary>Details</summary>
Motivation: Manual dense reward design in robotics is labor-intensive and doesn't scale well. Task progress offers a natural dense reward signal but is challenging to estimate automatically.

Method: TimeRewarder learns progress estimation from passive videos (robot demos and human videos) by modeling temporal distances between frame pairs, then uses these as step-wise proxy rewards for RL.

Result: Achieved nearly perfect success in 9/10 Meta-World tasks with only 200k interactions per task, outperforming previous methods and manual dense rewards in both success rate and sample efficiency.

Conclusion: TimeRewarder provides a scalable approach to derive rich reward signals from diverse video sources, including real-world human videos, significantly improving RL performance for sparse-reward tasks.

Abstract: Designing dense rewards is crucial for reinforcement learning (RL), yet in
robotics it often demands extensive manual effort and lacks scalability. One
promising solution is to view task progress as a dense reward signal, as it
quantifies the degree to which actions advance the system toward task
completion over time. We present TimeRewarder, a simple yet effective reward
learning method that derives progress estimation signals from passive videos,
including robot demonstrations and human videos, by modeling temporal distances
between frame pairs. We then demonstrate how TimeRewarder can supply step-wise
proxy rewards to guide reinforcement learning. In our comprehensive experiments
on ten challenging Meta-World tasks, we show that TimeRewarder dramatically
improves RL for sparse-reward tasks, achieving nearly perfect success in 9/10
tasks with only 200,000 interactions per task with the environment. This
approach outperformed previous methods and even the manually designed
environment dense reward on both the final success rate and sample efficiency.
Moreover, we show that TimeRewarder pretraining can exploit real-world human
videos, highlighting its potential as a scalable approach path to rich reward
signals from diverse video sources.

</details>


### [405] [Branching Out: Broadening AI Measurement and Evaluation with Measurement Trees](https://arxiv.org/abs/2509.26632)
*Craig Greenberg,Patrick Hall,Theodore Jensen,Kristen Greene,Razvan Amironesei*

Main category: cs.AI

TL;DR: This paper introduces measurement trees, a novel hierarchical metric framework that creates interpretable multi-level representations of measurands using user-defined aggregation methods, enhancing transparency in AI system evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the need for expanded AI system evaluation scope and enhance metric transparency by enabling integration of heterogeneous evidence like agentic, business, energy-efficiency, sociotechnical, and security signals.

Method: Proposes measurement trees as hierarchical directed graphs where each node summarizes its children through user-defined aggregation methods, providing definitions, examples, and practical implementation through a large-scale measurement exercise with open-source Python code.

Result: The measurement trees framework successfully operationalizes transparent measurement of complex constructs, facilitating broader and more interpretable AI evaluation through hierarchical aggregation of diverse evidence types.

Conclusion: Measurement trees provide a principled foundation for comprehensive AI evaluation by offering a transparent, hierarchical approach to measuring complex constructs that integrates heterogeneous evidence and enhances interpretability.

Abstract: This paper introduces \textit{measurement trees}, a novel class of metrics
designed to combine various constructs into an interpretable multi-level
representation of a measurand. Unlike conventional metrics that yield single
values, vectors, surfaces, or categories, measurement trees produce a
hierarchical directed graph in which each node summarizes its children through
user-defined aggregation methods. In response to recent calls to expand the
scope of AI system evaluation, measurement trees enhance metric transparency
and facilitate the integration of heterogeneous evidence, including, e.g.,
agentic, business, energy-efficiency, sociotechnical, or security signals. We
present definitions and examples, demonstrate practical utility through a
large-scale measurement exercise, and provide accompanying open-source Python
code. By operationalizing a transparent approach to measurement of complex
constructs, this work offers a principled foundation for broader and more
interpretable AI evaluation.

</details>
