{"id": "2510.06239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06239", "abs": "https://arxiv.org/abs/2510.06239", "authors": ["Pranav Gupta"], "title": "OpenStaxQA: A multilingual dataset based on open-source college textbooks", "comment": null, "summary": "We present OpenStaxQA, an evaluation benchmark specific to college-level\neducational applications based on 43 open-source college textbooks in English,\nSpanish, and Polish, available under a permissive Creative Commons license. We\nfinetune and evaluate large language models (LLMs) with approximately 7 billion\nparameters on this dataset using quantized low rank adapters (QLoRa).\nAdditionally we also perform a zero-shot evaluation on the AI2 reasoning\nchallenge dev dataset in order to check if OpenStaxQA can lead to an improved\nperformance on other tasks. We also discuss broader impacts relevant to\ndatasets such as OpenStaxQA.", "AI": {"tldr": "OpenStaxQA is a college-level educational benchmark using 43 open-source textbooks in English, Spanish, and Polish. LLMs with ~7B parameters are fine-tuned using QLoRa and evaluated, with additional zero-shot testing on AI2 reasoning challenge to assess transfer learning.", "motivation": "To create a specialized evaluation benchmark for college-level educational applications using open-source textbooks, addressing the need for domain-specific educational AI evaluation.", "method": "Developed OpenStaxQA benchmark from 43 Creative Commons licensed textbooks in three languages. Fine-tuned ~7B parameter LLMs using quantized low rank adapters (QLoRa), and performed zero-shot evaluation on AI2 reasoning challenge dataset.", "result": "The paper presents the OpenStaxQA benchmark and demonstrates fine-tuning of LLMs on this educational dataset, with additional evaluation showing potential for improved performance on other reasoning tasks.", "conclusion": "OpenStaxQA provides a valuable educational benchmark for evaluating AI systems, and the authors discuss broader impacts of such educational datasets for AI development and deployment."}}
{"id": "2510.06240", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.06240", "abs": "https://arxiv.org/abs/2510.06240", "authors": ["Jiqun Pan", "Zhenke Duan", "Jiani Tu", "Anzhi Cheng", "Yanqing Wang"], "title": "Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets", "comment": "41 pages, 12 figures, 6 tables", "summary": "Industrial question-answering (QA) systems require higher safety and\nreliability than general-purpose dialogue models, as errors in high-risk\nscenarios such as equipment fault diagnosis can have severe consequences.\nAlthough multi-agent large language models enhance reasoning depth, they suffer\nfrom uncontrolled iterations and unverifiable outputs, and conventional\ndistillation methods struggle to transfer collaborative reasoning capabilities\nto lightweight, deployable student models. To address these challenges, we\npropose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our\napproach formulates distillation as a Markov Decision Process and incorporates\na knowledge graph as a verifiable structured prior to enrich state\nrepresentation and ensure convergence. By integrating collaborative reasoning\nwith knowledge grounding, KG-MASD generates high-confidence instruction-tuning\ndata and jointly distills reasoning depth and verifiability into compact\nstudent models suitable for edge deployment. Experiments on an industrial QA\ndataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent\nover baselines and significantly enhances reliability, enabling trustworthy AI\ndeployment in safety-critical industrial scenarios. Code and data are available\nat https://github.com/erwinmsmith/KG-MAD/.", "AI": {"tldr": "KG-MASD is a distillation method that transfers multi-agent reasoning capabilities to lightweight models using knowledge graphs for verification, improving industrial QA accuracy and reliability.", "motivation": "Industrial QA systems need higher safety than general dialogue models, but multi-agent LLMs have uncontrolled iterations and unverifiable outputs, while conventional distillation fails to transfer collaborative reasoning to deployable models.", "method": "Formulates distillation as Markov Decision Process, incorporates knowledge graph as verifiable structured prior to enrich state representation and ensure convergence, integrates collaborative reasoning with knowledge grounding.", "result": "Improves accuracy by 2.4% to 20.1% over baselines on industrial QA dataset, significantly enhances reliability for safety-critical deployment.", "conclusion": "KG-MASD enables trustworthy AI deployment in safety-critical industrial scenarios by distilling reasoning depth and verifiability into compact student models."}}
{"id": "2510.06242", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06242", "abs": "https://arxiv.org/abs/2510.06242", "authors": ["Subin An", "Yugyeong Ji", "Junyoung Kim", "Heejin Kook", "Yang Lu", "Josh Seltzer"], "title": "Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses", "comment": "EMNLP Industry Track", "summary": "Open-ended survey responses provide valuable insights in marketing research,\nbut low-quality responses not only burden researchers with manual filtering but\nalso risk leading to misleading conclusions, underscoring the need for\neffective evaluation. Existing automatic evaluation methods target\nLLM-generated text and inadequately assess human-written responses with their\ndistinct characteristics. To address such characteristics, we propose a\ntwo-stage evaluation framework specifically designed for human survey\nresponses. First, gibberish filtering removes nonsensical responses. Then,\nthree dimensions-effort, relevance, and completeness-are evaluated using LLM\ncapabilities, grounded in empirical analysis of real-world survey data.\nValidation on English and Korean datasets shows that our framework not only\noutperforms existing metrics but also demonstrates high practical applicability\nfor real-world applications such as response quality prediction and response\nrejection, showing strong correlations with expert assessment.", "AI": {"tldr": "Proposes a two-stage evaluation framework for human survey responses with gibberish filtering and three quality dimensions (effort, relevance, completeness) using LLMs, outperforming existing metrics.", "motivation": "Open-ended survey responses provide valuable insights but low-quality responses burden researchers and risk misleading conclusions. Existing automatic evaluation methods target LLM-generated text and inadequately assess human-written responses.", "method": "Two-stage evaluation framework: 1) gibberish filtering removes nonsensical responses, 2) three dimensions (effort, relevance, completeness) evaluated using LLM capabilities, grounded in empirical analysis of real-world survey data.", "result": "Validation on English and Korean datasets shows the framework outperforms existing metrics and demonstrates high practical applicability for response quality prediction and response rejection, with strong correlations with expert assessment.", "conclusion": "The proposed framework effectively addresses the distinct characteristics of human survey responses and provides reliable automatic evaluation for real-world marketing research applications."}}
{"id": "2510.06243", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06243", "abs": "https://arxiv.org/abs/2510.06243", "authors": ["Qihua Dong", "Luis Figueroa", "Handong Zhao", "Kushal Kafle", "Jason Kuen", "Zhihong Ding", "Scott Cohen", "Yun Fu"], "title": "CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning", "comment": "MLLM, Referring Expression Segmentation", "summary": "Referring Expression Comprehension and Segmentation are critical tasks for\nassessing the integration of language understanding and image comprehension,\nserving as benchmarks for Multimodal Large Language Models (MLLMs)\ncapabilities. To address these challenges, we propose a new strategy, CoT\nReferring, which enhances model reasoning across modalities through a\nstructured, chain-of-thought training data structure. Our approach\nsystematically parses textual structures to a sequential referring step, where\nin each step it identifies relationships and ensures consistent reference\nalignment, thereby improving accuracy in complex query scenarios. We\nrestructure the training data to enforce a new output form, providing new\nannotations for existing datasets and compiling an evaluation benchmark from\nexisting resources. This benchmark is designed explicitly for complex referring\ncases. We also integrate detection and segmentation capabilities into a unified\nMLLM framework, training it with a novel adaptive weighted loss to optimize\nperformance. Experimental results on our curated benchmark and RefCOCO/+/g\ndemonstrate the effectiveness of our approach, with a notable increase of 2.5%+\nover baseline models.", "AI": {"tldr": "CoT Referring enhances MLLMs by using chain-of-thought training to parse textual structures into sequential referring steps, improving accuracy in complex queries through better relationship identification and reference alignment.", "motivation": "To improve multimodal language understanding by addressing challenges in Referring Expression Comprehension and Segmentation, which serve as benchmarks for MLLM capabilities.", "method": "Restructure training data with chain-of-thought annotations, integrate detection and segmentation into unified MLLM framework, and use adaptive weighted loss for optimization.", "result": "Achieved 2.5%+ improvement over baseline models on curated benchmark and RefCOCO/+/g datasets.", "conclusion": "CoT Referring strategy effectively enhances multimodal reasoning through structured chain-of-thought training and unified framework integration."}}
{"id": "2510.06267", "categories": ["cs.LG", "cs.AI", "I.2.6; H.2.8; J.3"], "pdf": "https://arxiv.org/pdf/2510.06267", "abs": "https://arxiv.org/abs/2510.06267", "authors": ["Khartik Uppalapati", "Shakeel Abdulkareem", "Bora Yimenicioglu"], "title": "RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases", "comment": "6 pages, 2 figures, 2 tables. Submitted to IEEE International\n  Conference on Data Science and Advanced Analytics (DSAA)", "summary": "We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion\nframework that generates realistic yet privacy-preserving synthetic\nelectronic-health-record (EHR) trajectories for ultra-rare diseases.\nRareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human\nPhenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA\nAdverse Event Reporting System (FAERS) into a heterogeneous knowledge graph\ncomprising approximately 8 M typed edges. Meta-path scores extracted from this\n8-million-edge KG modulate the per-token noise schedule in the forward\nstochastic differential equation, steering generation toward biologically\nplausible lab-medication-adverse-event co-occurrences while retaining\nscore-based diffusion model stability. The reverse denoiser then produces\ntimestamped sequences of lab-code, medication-code, and adverse-event-flag\ntriples that contain no protected health information. On simulated\nultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean\nDiscrepancy by 40 percent relative to an unguided diffusion baseline and by\ngreater than 60 percent versus GAN counterparts, without sacrificing downstream\npredictive utility. A black-box membership-inference evaluation using the\nDOMIAS attacker yields AUROC approximately 0.53, well below the 0.55\nsafe-release threshold and substantially better than the approximately 0.61\nplus or minus 0.03 observed for non-KG baselines, demonstrating strong\nresistance to re-identification. These results suggest that integrating\nbiomedical knowledge graphs directly into diffusion noise schedules can\nsimultaneously enhance fidelity and privacy, enabling safer data sharing for\nrare-disease research.", "AI": {"tldr": "RareGraph-Synth is a knowledge-guided diffusion framework that generates privacy-preserving synthetic EHR trajectories for ultra-rare diseases using a heterogeneous knowledge graph with 8M edges.", "motivation": "To enable safer data sharing for rare-disease research by generating realistic yet privacy-preserving synthetic EHR data that maintains biological plausibility while protecting patient privacy.", "method": "Unifies five biomedical resources into a heterogeneous KG, uses meta-path scores to modulate noise schedule in forward SDE, and employs reverse denoiser to generate timestamped sequences of lab-medication-adverse-event triples without PHI.", "result": "40% lower categorical MMD vs unguided diffusion, >60% lower vs GANs; AUROC of 0.53 in membership inference attack (below 0.55 safe threshold vs 0.61\u00b10.03 for baselines).", "conclusion": "Integrating biomedical knowledge graphs directly into diffusion noise schedules can simultaneously enhance fidelity and privacy for rare-disease data sharing."}}
{"id": "2510.06261", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06261", "abs": "https://arxiv.org/abs/2510.06261", "authors": ["Zhanke Zhou", "Chentao Cao", "Xiao Feng", "Xuan Li", "Zongze Li", "Xiangyu Lu", "Jiangchao Yao", "Weikai Huang", "Linrui Xu", "Tian Cheng", "Guanyu Jiang", "Yiming Zheng", "Brando Miranda", "Tongliang Liu", "Sanmi Koyejo", "Masashi Sugiyama", "Bo Han"], "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning", "comment": "Ongoing project", "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to\naddress two bottlenecks in foundation model (FM) reasoning-limited\nmodel-intrinsic capacity and unreliable test-time iteration. AlphaApollo\norchestrates multiple models with professional tools to enable deliberate,\nverifiable reasoning. It couples (i) a computation tool (Python with numerical\nand symbolic libraries) and (ii) a retrieval tool (task-relevant external\ninformation) to execute exact calculations and ground decisions. The system\nfurther supports multi-round, multi-model solution evolution via a shared state\nmap that records candidates, executable checks, and feedback for iterative\nrefinement. In evaluations on AIME 2024/2025 across multiple models,\nAlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32\nfor Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for\nLlama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool\ncalls are successfully executed, with consistent outperformance of non-tool\nbaselines, thereby lifting the capability ceiling of FMs. More empirical\nresults and implementation details will be updated at\nhttps://github.com/tmlr-group/AlphaApollo.", "AI": {"tldr": "AlphaApollo is a self-evolving agentic reasoning system that addresses foundation model limitations by orchestrating multiple models with computational and retrieval tools for verifiable reasoning, achieving significant performance gains on AIME benchmarks.", "motivation": "To overcome two key bottlenecks in foundation model reasoning: limited model-intrinsic capacity and unreliable test-time iteration.", "method": "Orchestrates multiple models with professional tools (Python computation and retrieval tools) for deliberate reasoning, using a shared state map for multi-round solution evolution with iterative refinement.", "result": "Consistent gains on AIME 2024/2025: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, +8.91% Average@32 and +26.67% Pass@32 for Llama-3.3-70B-Instruct. Over 80% tool calls successfully executed.", "conclusion": "AlphaApollo effectively lifts the capability ceiling of foundation models through tool-augmented reasoning and multi-model orchestration."}}
{"id": "2510.06244", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06244", "abs": "https://arxiv.org/abs/2510.06244", "authors": ["Nouman Ahmed", "Ronin Wu", "Victor Botev"], "title": "Evaluating Embedding Frameworks for Scientific Domain", "comment": null, "summary": "Finding an optimal word representation algorithm is particularly important in\nterms of domain specific data, as the same word can have different meanings and\nhence, different representations depending on the domain and context. While\nGenerative AI and transformer architecture does a great job at generating\ncontextualized embeddings for any given work, they are quite time and compute\nextensive, especially if we were to pre-train such a model from scratch. In\nthis work, we focus on the scientific domain and finding the optimal word\nrepresentation algorithm along with the tokenization method that could be used\nto represent words in the scientific domain. The goal of this research is two\nfold: 1) finding the optimal word representation and tokenization methods that\ncan be used in downstream scientific domain NLP tasks, and 2) building a\ncomprehensive evaluation suite that could be used to evaluate various word\nrepresentation and tokenization algorithms (even as new ones are introduced) in\nthe scientific domain. To this end, we build an evaluation suite consisting of\nseveral downstream tasks and relevant datasets for each task. Furthermore, we\nuse the constructed evaluation suite to test various word representation and\ntokenization algorithms.", "AI": {"tldr": "This paper focuses on finding optimal word representation algorithms and tokenization methods for scientific domain NLP, and building a comprehensive evaluation suite to test various algorithms.", "motivation": "The same word can have different meanings in different domains, and while transformer models generate good contextual embeddings, they are computationally expensive to pre-train from scratch, especially for scientific domains.", "method": "Built an evaluation suite with multiple downstream tasks and relevant datasets, then used this suite to test various word representation and tokenization algorithms.", "result": "The paper developed a comprehensive evaluation framework for scientific domain NLP and tested multiple word representation and tokenization methods using this framework.", "conclusion": "The research provides both optimal word representation/tokenization methods for scientific NLP tasks and a reusable evaluation suite for future algorithm comparisons in the scientific domain."}}
{"id": "2510.06270", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06270", "abs": "https://arxiv.org/abs/2510.06270", "authors": ["Nian Ran", "Zhongzheng Li", "Yue Wang", "Qingsong Ran", "Xiaoyuan Zhang", "Shikun Feng", "Richard Allmendinger", "Xiaoguang Zhao"], "title": "MCCE: A Framework for Multi-LLM Collaborative Co-Evolution", "comment": null, "summary": "Multi-objective discrete optimization problems, such as molecular design,\npose significant challenges due to their vast and unstructured combinatorial\nspaces. Traditional evolutionary algorithms often get trapped in local optima,\nwhile expert knowledge can provide crucial guidance for accelerating\nconvergence. Large language models (LLMs) offer powerful priors and reasoning\nability, making them natural optimizers when expert knowledge matters. However,\nclosed-source LLMs, though strong in exploration, cannot update their\nparameters and thus cannot internalize experience. Conversely, smaller open\nmodels can be continually fine-tuned but lack broad knowledge and reasoning\nstrength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid\nframework that unites a frozen closed-source LLM with a lightweight trainable\nmodel. The system maintains a trajectory memory of past search processes; the\nsmall model is progressively refined via reinforcement learning, with the two\nmodels jointly supporting and complementing each other in global exploration.\nUnlike model distillation, this process enhances the capabilities of both\nmodels through mutual inspiration. Experiments on multi-objective drug design\nbenchmarks show that MCCE achieves state-of-the-art Pareto front quality and\nconsistently outperforms baselines. These results highlight a new paradigm for\nenabling continual evolution in hybrid LLM systems, combining knowledge-driven\nexploration with experience-driven learning.", "AI": {"tldr": "MCCE is a hybrid framework combining frozen closed-source LLMs with trainable small models for multi-objective optimization, achieving state-of-the-art performance in drug design through collaborative co-evolution.", "motivation": "Multi-objective discrete optimization problems like molecular design face challenges in vast combinatorial spaces. Traditional methods get stuck in local optima, while LLMs offer strong priors but closed-source models cannot learn from experience and small models lack reasoning capability.", "method": "Multi-LLM Collaborative Co-evolution (MCCE) framework that unites a frozen closed-source LLM with a lightweight trainable model. Uses trajectory memory and reinforcement learning to progressively refine the small model, with both models supporting each other in global exploration.", "result": "Experiments on multi-objective drug design benchmarks show MCCE achieves state-of-the-art Pareto front quality and consistently outperforms baselines.", "conclusion": "MCCE presents a new paradigm for continual evolution in hybrid LLM systems, effectively combining knowledge-driven exploration with experience-driven learning for superior optimization performance."}}
{"id": "2510.06274", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06274", "abs": "https://arxiv.org/abs/2510.06274", "authors": ["Mohammad Mahdi Samiei Paqaleh", "Arash Marioriyad", "Arman Tahmasebi-Zadeh", "Mohamadreza Fereydooni", "Mahdi Ghaznavai", "Mahdieh Soleymani Baghshah"], "title": "Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization", "comment": null, "summary": "Recent progress has pushed AI frontiers from pattern recognition tasks toward\nproblems that require step by step, System2 style reasoning, especially with\nlarge language models. Yet, unlike learning, where generalization and out of\ndistribution (OoD) evaluation concepts are well formalized, there is no clear,\nconsistent definition or metric for reasoning ability. We propose Complexity\nOut of Distribution (Complexity OoD) generalization as a framework and problem\nsetting to define and measure reasoning. A model exhibits Complexity OoD\ngeneralization when it maintains performance on test instances whose minimal\nrequired solution complexity, either representational (richer solution\nstructure) or computational (more reasoning steps/program length), exceeds that\nof all training examples. We formalize complexity via solution description\nKolmogorov complexity and operational proxies (e.g., object/relation counts;\nreasoning step counts), clarifying how Complexity OoD differs from length and\ncompositional OoD. This lens unifies learning and reasoning: many cases\nsolvable with System1 like processing at low complexity become System2 like\nunder complexity pressure, while System2 can be viewed as generalization over\nsolution structures. We translate this perspective into practice with\nrecommendations for operationalizing Complexity OoD across the stack:\nincorporating complexity into benchmark and evaluation metric design,\nrethinking supervision to target solution traces, seeking and designing\ninductive biases for Complexity OoD generalization, addressing learning to\nreason spillovers such as spurious shortcuts, semantic robustness, catastrophic\nforgetting, and step wise calibration. Because Complexity OoD cannot be solved\nby scaling data alone, progress toward robust reasoning will require\narchitectures and training regimes that explicitly model and allocate\ncomputation with respect to complexity.", "AI": {"tldr": "The paper proposes Complexity Out of Distribution (Complexity OoD) generalization as a framework to define and measure reasoning ability in AI systems, distinguishing it from traditional pattern recognition tasks.", "motivation": "There is no clear, consistent definition or metric for reasoning ability in AI, unlike learning where generalization concepts are well formalized. The authors aim to establish a framework to properly define and evaluate reasoning capabilities.", "method": "The authors formalize complexity via solution description Kolmogorov complexity and operational proxies (object/relation counts, reasoning step counts). They propose Complexity OoD generalization where models maintain performance on test instances requiring more complex solutions than training examples.", "result": "The framework unifies learning and reasoning, showing that many cases solvable with System1-like processing at low complexity become System2-like under complexity pressure. System2 reasoning can be viewed as generalization over solution structures.", "conclusion": "Progress toward robust reasoning requires architectures and training regimes that explicitly model and allocate computation with respect to complexity, as Complexity OoD cannot be solved by scaling data alone."}}
{"id": "2510.06249", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06249", "abs": "https://arxiv.org/abs/2510.06249", "authors": ["Toshiki Nakai", "Ravi Kiran Chikkala", "Lena Sophie Oberkircher", "Nicholas Jennings", "Natalia Skachkova", "Tatiana Anikina", "Jesujoba Oluwadara Alabi"], "title": "TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B", "comment": "It is work in progress", "summary": "The 2025 Multimodal Models for Low-Resource Contexts and Social Impact\n(MMLoSo) Language Challenge addresses one of India's most pressing linguistic\ngaps: the lack of resources for its diverse low-resource languages (LRLs). In\nthis study, we investigate whether enforcing cross-lingual similarity in\nspecific internal layers of a decoder-only multilingual large language model\n(LLM) can improve translation quality from LRL to high-resource language (HRL).\nSpecifically, we combine Centered Kernel Alignment (CKA), a similarity metric\nthat encourages representations of different languages to align, with REPINA, a\nregularization method that constrains parameter updates to remain close to the\npretrained model, into a joint method we call TRepLiNa. In this research\nproject, we experiment with zero-shot, few-shot, and fine-tuning settings using\nAya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,\nSantali, Bhili) with Hindi/English pivots. Our results show that aligning\nmid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach\nto improving LRL translation, especially in data-scarce settings.", "AI": {"tldr": "TRepLiNa method combines CKA and REPINA to improve low-resource language translation by enforcing cross-lingual similarity in mid-level layers of multilingual LLMs.", "motivation": "Address the linguistic gap for India's diverse low-resource languages (LRLs) by improving translation quality from LRL to high-resource languages.", "method": "Combine Centered Kernel Alignment (CKA) with REPINA regularization into TRepLiNa, applied to Aya-23 8B with QLoRA in zero-shot, few-shot, and fine-tuning settings across Mundari, Santali, Bhili language pairs.", "result": "Aligning mid-level layers using TRepLiNa improves LRL translation quality, especially in data-scarce settings, demonstrating a low-cost practical approach.", "conclusion": "TRepLiNa (CKA+REPINA) is an effective method for enhancing low-resource language translation by enforcing cross-lingual similarity in specific model layers."}}
{"id": "2510.06278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06278", "abs": "https://arxiv.org/abs/2510.06278", "authors": ["M. Sajid", "Mushir Akhtar", "A. Quadir", "M. Tanveer"], "title": "RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets", "comment": null, "summary": "Recent advancements in neural networks, supported by foundational theoretical\ninsights, emphasize the superior representational power of complex numbers.\nHowever, their adoption in randomized neural networks (RNNs) has been limited\ndue to the lack of effective methods for transforming real-valued tabular\ndatasets into complex-valued representations. To address this limitation, we\npropose two methods for generating complex-valued representations from\nreal-valued datasets: a natural transformation and an autoencoder-driven\nmethod. Building on these mechanisms, we propose RVFL-X, a complex-valued\nextension of the random vector functional link (RVFL) network. RVFL-X\nintegrates complex transformations into real-valued datasets while maintaining\nthe simplicity and efficiency of the original RVFL architecture. By leveraging\ncomplex components such as input, weights, and activation functions, RVFL-X\nprocesses complex representations and produces real-valued outputs.\nComprehensive evaluations on 80 real-valued UCI datasets demonstrate that\nRVFL-X consistently outperforms both the original RVFL and state-of-the-art\n(SOTA) RNN variants, showcasing its robustness and effectiveness across diverse\napplication domains.", "AI": {"tldr": "Proposed RVFL-X, a complex-valued extension of RVFL networks, with two methods for transforming real-valued tabular data to complex representations, achieving superior performance over original RVFL and SOTA RNN variants on 80 UCI datasets.", "motivation": "Complex numbers have superior representational power in neural networks, but their adoption in randomized neural networks (RNNs) has been limited due to lack of effective methods for transforming real-valued tabular datasets into complex-valued representations.", "method": "Proposed two methods for generating complex-valued representations from real-valued datasets: natural transformation and autoencoder-driven method. Developed RVFL-X as complex-valued extension of RVFL network, integrating complex transformations while maintaining RVFL's simplicity and efficiency.", "result": "Comprehensive evaluations on 80 real-valued UCI datasets demonstrate RVFL-X consistently outperforms both original RVFL and state-of-the-art RNN variants, showing robustness and effectiveness across diverse application domains.", "conclusion": "RVFL-X successfully bridges the gap between complex number representational power and practical application in randomized neural networks, providing an effective framework for complex-valued processing of real-world tabular data."}}
{"id": "2510.06288", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06288", "abs": "https://arxiv.org/abs/2510.06288", "authors": ["Raj Ghugare", "Catherine Ji", "Kathryn Wantlin", "Jin Schofield", "Benjamin Eysenbach"], "title": "BuilderBench -- A benchmark for generalist agents", "comment": "Project page: https://rajghugare19.github.io/builderbench and Code:\n  https://github.com/rajghugare19/builderbench", "summary": "Today's AI models learn primarily through mimicry and sharpening, so it is\nnot surprising that they struggle to solve problems beyond the limits set by\nexisting data. To solve novel problems, agents should acquire skills for\nexploring and learning through experience. Finding a scalable learning\nmechanism for developing agents that learn through interaction remains a major\nopen problem. In this work, we introduce BuilderBench, a benchmark to\naccelerate research into agent pre-training that centers open-ended\nexploration. BuilderBench requires agents to learn how to build any structure\nusing blocks. BuilderBench is equipped with $(1)$ a hardware accelerated\nsimulator of a robotic agent interacting with various physical blocks, and\n$(2)$ a task-suite with over 42 diverse target structures that are carefully\ncurated to test an understanding of physics, mathematics, and long-horizon\nplanning. During training, agents have to explore and learn general principles\nabout the environment without any external supervision. During evaluation,\nagents have to build the unseen target structures from the task suite. Solving\nthese tasks requires a sort of \\emph{embodied reasoning} that is not reflected\nin words but rather in actions, experimenting with different strategies and\npiecing them together. Our experiments show that many of these tasks challenge\nthe current iteration of algorithms. Hence, we also provide a ``training\nwheels'' protocol, in which agents are trained and evaluated to build a single\ntarget structure from the task suite. Finally, we provide single-file\nimplementations of six different algorithms as a reference point for\nresearchers.", "AI": {"tldr": "BuilderBench is a benchmark for agent pre-training that focuses on open-ended exploration, requiring agents to learn how to build various structures using blocks in a simulated environment without external supervision.", "motivation": "Current AI models struggle with novel problems beyond existing data limits. To enable agents to solve new problems, they need skills for exploration and learning through experience, which requires scalable learning mechanisms for interactive learning.", "method": "BuilderBench provides a hardware-accelerated simulator of a robotic agent interacting with physical blocks, and a task suite with 42 diverse target structures testing physics understanding, mathematics, and long-horizon planning. Agents explore and learn general principles without supervision.", "result": "The experiments show that many BuilderBench tasks challenge current algorithms. The benchmark also includes a 'training wheels' protocol for building single target structures and provides implementations of six different algorithms as reference points.", "conclusion": "BuilderBench accelerates research into agent pre-training through embodied reasoning, where learning is reflected in actions and experimentation rather than words, addressing the need for scalable interactive learning mechanisms."}}
{"id": "2510.06250", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06250", "abs": "https://arxiv.org/abs/2510.06250", "authors": ["Bharti Meena", "Joanna Skubisz", "Harshit Rajgarhia", "Nand Dave", "Kiran Ganesh", "Shivali Dalmia", "Abhishek Mukherji", "Vasudevan Sundarababu", "Olga Pospelova"], "title": "Scalable multilingual PII annotation for responsible AI in LLMs", "comment": null, "summary": "As Large Language Models (LLMs) gain wider adoption, ensuring their reliable\nhandling of Personally Identifiable Information (PII) across diverse regulatory\ncontexts has become essential. This work introduces a scalable multilingual\ndata curation framework designed for high-quality PII annotation across 13\nunderrepresented locales, covering approximately 336 locale-specific PII types.\nOur phased, human-in-the-loop annotation methodology combines linguistic\nexpertise with rigorous quality assurance, leading to substantial improvements\nin recall and false positive rates from pilot, training, and production phases.\nBy leveraging inter-annotator agreement metrics and root-cause analysis, the\nframework systematically uncovers and resolves annotation inconsistencies,\nresulting in high-fidelity datasets suitable for supervised LLM fine-tuning.\nBeyond reporting empirical gains, we highlight common annotator challenges in\nmultilingual PII labeling and demonstrate how iterative, analytics-driven\npipelines can enhance both annotation quality and downstream model reliability.", "AI": {"tldr": "A scalable multilingual data curation framework for high-quality PII annotation across 13 underrepresented locales, covering 336 locale-specific PII types, using human-in-the-loop methodology to improve recall and reduce false positives.", "motivation": "As LLMs gain wider adoption, ensuring reliable handling of Personally Identifiable Information (PII) across diverse regulatory contexts has become essential.", "method": "Phased human-in-the-loop annotation methodology combining linguistic expertise with rigorous quality assurance, leveraging inter-annotator agreement metrics and root-cause analysis to resolve annotation inconsistencies.", "result": "Substantial improvements in recall and false positive rates from pilot, training, and production phases, resulting in high-fidelity datasets suitable for supervised LLM fine-tuning.", "conclusion": "Iterative, analytics-driven pipelines can enhance both annotation quality and downstream model reliability, addressing common annotator challenges in multilingual PII labeling."}}
{"id": "2510.06284", "categories": ["cs.LG", "cs.CV", "math.GT", "Primary: 57K10, 68T07, secondary: 57K14, 68T45"], "pdf": "https://arxiv.org/pdf/2510.06284", "abs": "https://arxiv.org/abs/2510.06284", "authors": ["Anne Dranowski", "Yura Kabkov", "Daniel Tubbenhauer"], "title": "On knot detection via picture recognition", "comment": "21 pages, many figures, comments welcome", "summary": "Our goal is to one day take a photo of a knot and have a phone automatically\nrecognize it. In this expository work, we explain a strategy to approximate\nthis goal, using a mixture of modern machine learning methods (in particular\nconvolutional neural networks and transformers for image recognition) and\ntraditional algorithms (to compute quantum invariants like the Jones\npolynomial). We present simple baselines that predict crossing number directly\nfrom images, showing that even lightweight CNN and transformer architectures\ncan recover meaningful structural information. The longer-term aim is to\ncombine these perception modules with symbolic reconstruction into planar\ndiagram (PD) codes, enabling downstream invariant computation for robust knot\nclassification. This two-stage approach highlights the complementarity between\nmachine learning, which handles noisy visual data, and invariants, which\nenforce rigorous topological distinctions.", "AI": {"tldr": "Using machine learning (CNNs and transformers) combined with traditional knot invariants to recognize knots from photos, with initial success in predicting crossing numbers from images.", "motivation": "To develop a system that can automatically recognize knots from photos, bridging visual perception with topological analysis.", "method": "Two-stage approach: 1) Machine learning (CNNs and transformers) to extract structural information from images and predict crossing numbers; 2) Symbolic reconstruction into planar diagram codes for computing invariants like Jones polynomial.", "result": "Lightweight CNN and transformer architectures can successfully recover meaningful structural information and predict crossing numbers directly from knot images.", "conclusion": "Combining machine learning for visual perception with traditional knot invariants provides a robust approach for knot classification, leveraging the strengths of both methods."}}
{"id": "2510.06302", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06302", "abs": "https://arxiv.org/abs/2510.06302", "authors": ["Ksenija Lace", "Marite Kirikova"], "title": "Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration", "comment": null, "summary": "Post-merger integration states unique challenges for professionals\nresponsible for information system integration aimed on alignment and\ncombination diverse system architectures of merging organizations. Although the\ntheoretical and practical guidance exists for post-merger integration on the\nbusiness level, there is a significant gap in training for information system\nintegration in this context. In prior research specific methods AMILI (Support\nmethod for informed decision identification) and AMILP (Support method for\ninformed decision-making) were introduced for the support of information system\nintegration decisions in the post-merger integration. But during the practical\napplication was reported high learning curve and low learner motivation. This\npaper explores how game-based learning design can address these limitations by\ntransforming static method training into engaging learning experience. The\nstudy analyzes foundational learning theories, cognitive load and motivation\nmodels, and serious game design frameworks to identify the essential\nrequirements for a game-based learning design framework tailored to information\nsystem integration in post-merger integration. Requirements are structured in\ntwo components: the transformation process and resulting learning experience.\nThe paper concludes with a plan for developing and evaluating the proposed\nframework through iterative design and real-world validation.", "AI": {"tldr": "This paper proposes a game-based learning framework to address the high learning curve and low motivation issues in training for information system integration during post-merger integration, building on existing methods AMILI and AMILP.", "motivation": "There is a significant gap in training for information system integration in post-merger contexts, with existing methods (AMILI and AMILP) suffering from high learning curves and low learner motivation during practical application.", "method": "The study analyzes foundational learning theories, cognitive load and motivation models, and serious game design frameworks to identify essential requirements for a game-based learning design framework tailored to information system integration in post-merger integration.", "result": "Requirements are structured in two components: the transformation process and resulting learning experience, providing a foundation for developing an engaging game-based learning approach.", "conclusion": "The paper concludes with a plan for developing and evaluating the proposed framework through iterative design and real-world validation to transform static method training into engaging learning experiences."}}
{"id": "2510.06420", "categories": ["cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.06420", "abs": "https://arxiv.org/abs/2510.06420", "authors": ["Suresh K. Damodaran", "Paul D. Rowe"], "title": "Automated Repeatable Adversary Threat Emulation with Effects Language (EL)", "comment": null, "summary": "The emulation of multi-step attacks attributed to advanced persistent threats\nis valuable for training defenders and evaluating defense tools. In this paper,\nwe discuss the numerous challenges and desired attributes associated with such\nautomation. Additionally, we introduce the use of Effects Language (EL), a\nvisual programming language with graph-based operational semantics, as a\nsolution to address many of these challenges and requirements. We formally\ndefine the execution semantics of EL, and prove important execution properties.\nFurthermore, we showcase the application of EL to codify attacks using an\nexample from one of the publicly available attack scenarios. We also\ndemonstrate how EL can be utilized to provide proof-of-attack of complex\nmulti-step attacks. Our results highlight the improvements in time and resource\nefficiency achieved through the use of EL for repeatable automation.", "AI": {"tldr": "The paper introduces Effects Language (EL), a visual programming language with graph-based operational semantics, to automate multi-step attack emulation for training defenders and evaluating defense tools.", "motivation": "To address challenges in automating multi-step attacks attributed to advanced persistent threats for training defenders and evaluating defense tools.", "method": "Formally define the execution semantics of EL, prove important execution properties, and apply EL to codify attacks using publicly available attack scenarios.", "result": "Demonstrates how EL can be utilized to provide proof-of-attack of complex multi-step attacks and highlights improvements in time and resource efficiency for repeatable automation.", "conclusion": "EL effectively addresses challenges in multi-step attack automation, providing a formal and efficient solution for attack emulation and defense evaluation."}}
{"id": "2510.06262", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06262", "abs": "https://arxiv.org/abs/2510.06262", "authors": ["Aryan Kumar Singh", "Janvi Singh"], "title": "Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments", "comment": "4 pages, 4 figures", "summary": "This dataset provides responses to a standardized, bilingual (English-Hindi)\nPrakriti Assessment Questionnaire designed to evaluate the physical,\nphysiological, and psychological characteristics of individuals according to\nclassical Ayurvedic principles. The questionnaire consists of 24\nmultiple-choice items covering body features, appetite, sleep patterns, energy\nlevels, and temperament. It was developed following AYUSH/CCRAS guidelines to\nensure comprehensive and accurate data collection. All questions are mandatory\nand neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)\nare hidden from participants. Data were collected via a Google Forms\ndeployment, enabling automated scoring of responses to map individual traits to\ndosha-specific scores. The resulting dataset provides a structured platform for\nresearch in computational intelligence, Ayurvedic studies, and personalized\nhealth analytics, supporting analysis of trait distributions, correlations, and\npredictive modeling. It can also serve as a reference for future Prakriti-based\nstudies and the development of intelligent health applications.", "AI": {"tldr": "A bilingual English-Hindi Prakriti Assessment Questionnaire dataset with 24 items evaluating Ayurvedic physical, physiological, and psychological traits, following AYUSH/CCRAS guidelines with automated dosha scoring.", "motivation": "To create a standardized dataset for computational intelligence research in Ayurvedic studies and personalized health analytics, supporting trait analysis and predictive modeling.", "method": "Developed 24 mandatory multiple-choice questions following AYUSH/CCRAS guidelines, collected via Google Forms with hidden dosha labels and automated response scoring.", "result": "Created a structured dataset enabling analysis of trait distributions, correlations, and mapping individual characteristics to dosha-specific scores (Vata, Pitta, Kapha).", "conclusion": "The dataset serves as a valuable platform for computational Ayurvedic research, personalized health applications, and future Prakriti-based studies."}}
{"id": "2510.06291", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06291", "abs": "https://arxiv.org/abs/2510.06291", "authors": ["Zhiyang Zhang", "Ningcong Chen", "Xin Zhang", "Yanhua Li", "Shen Su", "Hui Lu", "Jun Luo"], "title": "Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation", "comment": null, "summary": "The widespread use of GPS devices has driven advances in spatiotemporal data\nmining, enabling machine learning models to simulate human decision making and\ngenerate realistic trajectories, addressing both data collection costs and\nprivacy concerns. Recent studies have shown the promise of diffusion models for\nhigh-quality trajectory generation. However, most existing methods rely on\nconvolution based architectures (e.g. UNet) to predict noise during the\ndiffusion process, which often results in notable deviations and the loss of\nfine-grained street-level details due to limited model capacity. In this paper,\nwe propose Trajectory Transformer, a novel model that employs a transformer\nbackbone for both conditional information embedding and noise prediction. We\nexplore two GPS coordinate embedding strategies, location embedding and\nlongitude-latitude embedding, and analyze model performance at different\nscales. Experiments on two real-world datasets demonstrate that Trajectory\nTransformer significantly enhances generation quality and effectively\nalleviates the deviation issues observed in prior approaches.", "AI": {"tldr": "Proposes Trajectory Transformer, a transformer-based model for GPS trajectory generation that outperforms convolution-based diffusion models by reducing deviations and preserving street-level details.", "motivation": "Existing diffusion models for trajectory generation use convolution-based architectures (like UNet) which suffer from notable deviations and loss of fine-grained street-level details due to limited model capacity.", "method": "Uses a transformer backbone for both conditional information embedding and noise prediction, exploring two GPS coordinate embedding strategies: location embedding and longitude-latitude embedding, with analysis at different scales.", "result": "Experiments on two real-world datasets show Trajectory Transformer significantly enhances generation quality and effectively alleviates the deviation issues observed in prior approaches.", "conclusion": "Transformer-based architecture is superior to convolution-based methods for trajectory generation, providing better quality results and addressing the deviation problems of existing approaches."}}
{"id": "2510.06307", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06307", "abs": "https://arxiv.org/abs/2510.06307", "authors": ["Wentao Deng", "Jiahuan Pei", "Zhiwei Xu", "Zhaochun Ren", "Zhumin Chen", "Pengjie Ren"], "title": "Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks", "comment": "This paper has been accepted by NeurIPS 2025", "summary": "A multi-agent system (MAS) enhances its capacity to solve complex natural\nlanguage processing (NLP) tasks through collaboration among multiple agents,\nwhere consensus-seeking serves as a fundamental mechanism. However, existing\nconsensus-seeking approaches typically rely on voting mechanisms to judge\nconsensus, overlooking contradictions in system-internal beliefs that\ndestabilize the consensus. Moreover, these methods often involve agents\nupdating their results through indiscriminate collaboration with every other\nagent. Such uniform interaction fails to identify the optimal collaborators for\neach agent, hindering the emergence of a stable consensus. To address these\nchallenges, we provide a theoretical framework for selecting optimal\ncollaborators that maximize consensus stability. Based on the theorems, we\npropose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate\nstable consensus via selecting optimal collaborators and calibrating the\nconsensus judgment by system-internal beliefs. Experimental results on the MATH\nand MMLU benchmark datasets demonstrate that the proposed BCCS framework\noutperforms the best existing results by 2.23% and 3.95% of accuracy on\nchallenging tasks, respectively. Our code and data are available at\nhttps://github.com/dengwentao99/BCCS.", "AI": {"tldr": "The paper proposes BCCS framework for stable consensus in multi-agent systems by selecting optimal collaborators and calibrating consensus judgment using system-internal beliefs, achieving significant performance improvements on MATH and MMLU benchmarks.", "motivation": "Existing consensus-seeking approaches in multi-agent systems rely on voting mechanisms and uniform collaboration, which overlook contradictions in system-internal beliefs and fail to identify optimal collaborators, leading to unstable consensus.", "method": "Proposed Belief-Calibrated Consensus Seeking (BCCS) framework with theoretical foundation for selecting optimal collaborators that maximize consensus stability, and calibrating consensus judgment using system-internal beliefs.", "result": "BCCS outperforms best existing results by 2.23% accuracy on MATH and 3.95% accuracy on MMLU benchmark datasets for challenging NLP tasks.", "conclusion": "The BCCS framework effectively addresses limitations of existing consensus-seeking methods by enabling stable consensus through optimal collaborator selection and belief-calibrated judgment, demonstrating superior performance on benchmark tasks."}}
{"id": "2510.06421", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06421", "abs": "https://arxiv.org/abs/2510.06421", "authors": ["Muhammad Abdullah Soomro", "Fatima Muhammad Anwar"], "title": "Breaking Precision Time: OS Vulnerability Exploits Against IEEE 1588", "comment": "Published in IEEE ISPCS 2025", "summary": "The Precision Time Protocol (PTP), standardized as IEEE 1588, provides\nsub-microsecond synchronization across distributed systems and underpins\ncritical infrastructure in telecommunications, finance, power systems, and\nindustrial automation. While prior work has extensively analyzed PTP's\nvulnerability to network-based attacks, prompting the development of\ncryptographic protections and anomaly detectors, these defenses presume an\nuncompromised host. In this paper, we identify and exploit a critical blind\nspot in current threat models: kernel-level adversaries operating from within\nthe host running the PTP stack. We present the first systematic study of\nkernel-rooted attacks on PTP, demonstrating how privileged attackers can\nmanipulate system time by corrupting key interfaces without altering PTP\nnetwork traffic. We implement three attack primitives, constant offset,\nprogressive skew, and random jitter, using in-kernel payloads, and evaluate\ntheir impact on the widely used ptp4l and phc2sys daemons. Our experiments\nreveal that these attacks can silently destabilize clock synchronization,\nbypassing existing PTP security extensions. These findings highlight the urgent\nneed to reconsider host-level trust assumptions and integrate kernel integrity\ninto the design of secure time synchronization systems.", "AI": {"tldr": "This paper identifies kernel-level attacks on PTP time synchronization systems, showing how compromised hosts can manipulate system time without altering network traffic, bypassing existing security measures.", "motivation": "Current PTP security defenses focus on network-based attacks but assume uncompromised hosts, creating a critical blind spot for kernel-level adversaries.", "method": "Implemented three attack primitives (constant offset, progressive skew, random jitter) using in-kernel payloads targeting ptp4l and phc2sys daemons.", "result": "Attacks can silently destabilize clock synchronization and bypass existing PTP security extensions.", "conclusion": "Host-level trust assumptions need reconsideration and kernel integrity must be integrated into secure time synchronization system design."}}
{"id": "2510.06263", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06263", "abs": "https://arxiv.org/abs/2510.06263", "authors": ["Jiajun Wu", "Swaleh Zaidi", "Braden Teitge", "Henry Leung", "Jiayu Zhou", "Jessalyn Holodinsky", "Steve Drew"], "title": "Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians", "comment": "Accepted at the IEEE Annual Congress on Artificial Intelligence of\n  Things (IEEE AIoT) 2025", "summary": "Electronic health records (EHRs) contain extensive unstructured clinical data\nthat can overwhelm emergency physicians trying to identify critical\ninformation. We present a two-stage summarization system that runs entirely on\nembedded devices, enabling offline clinical summarization while preserving\npatient privacy. In our approach, a dual-device architecture first retrieves\nrelevant patient record sections using the Jetson Nano-R (Retrieve), then\ngenerates a structured summary on another Jetson Nano-S (Summarize),\ncommunicating via a lightweight socket link. The summarization output is\ntwo-fold: (1) a fixed-format list of critical findings, and (2) a\ncontext-specific narrative focused on the clinician's query. The retrieval\nstage uses locally stored EHRs, splits long notes into semantically coherent\nsections, and searches for the most relevant sections per query. The generation\nstage uses a locally hosted small language model (SLM) to produce the summary\nfrom the retrieved text, operating within the constraints of two NVIDIA Jetson\ndevices. We first benchmarked six open-source SLMs under 7B parameters to\nidentify viable models. We incorporated an LLM-as-Judge evaluation mechanism to\nassess summary quality in terms of factual accuracy, completeness, and clarity.\nPreliminary results on MIMIC-IV and de-identified real EHRs demonstrate that\nour fully offline system can effectively produce useful summaries in under 30\nseconds.", "AI": {"tldr": "A two-stage offline EHR summarization system using dual Jetson Nano devices for privacy-preserving clinical data processing, achieving useful summaries in under 30 seconds.", "motivation": "Emergency physicians are overwhelmed by extensive unstructured clinical data in EHRs, needing quick access to critical information while maintaining patient privacy.", "method": "Dual-device architecture: Jetson Nano-R retrieves relevant EHR sections, Jetson Nano-S generates structured summaries using small language models, with LLM-as-Judge evaluation for quality assessment.", "result": "Preliminary results on MIMIC-IV and real EHRs show the system effectively produces useful clinical summaries in under 30 seconds while operating fully offline.", "conclusion": "The proposed embedded system enables privacy-preserving, efficient clinical summarization that can assist emergency physicians in quickly identifying critical patient information."}}
{"id": "2510.06293", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06293", "abs": "https://arxiv.org/abs/2510.06293", "authors": ["Cristian Meo", "Varun Sarathchandran", "Avijit Majhi", "Shao Hung", "Carlo Saccardi", "Ruben Imhoff", "Roberto Deidda", "Remko Uijlenhoet", "Justin Dauwels"], "title": "BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression", "comment": null, "summary": "Predicting precipitation maps is a highly complex spatiotemporal modeling\ntask, critical for mitigating the impacts of extreme weather events. Short-term\nprecipitation forecasting, or nowcasting, requires models that are not only\naccurate but also computationally efficient for real-time applications. Current\nmethods, such as token-based autoregressive models, often suffer from flawed\ninductive biases and slow inference, while diffusion models can be\ncomputationally intensive. To address these limitations, we introduce BlockGPT,\na generative autoregressive transformer using batched tokenization (Block)\nmethod that predicts full two-dimensional fields (frames) at each time step.\nConceived as a model-agnostic paradigm for video prediction, BlockGPT\nfactorizes space-time by using self-attention within each frame and causal\nattention across frames; in this work, we instantiate it for precipitation\nnowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI\n(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines\nincluding token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)\nmodels. The results show that BlockGPT achieves superior accuracy, event\nlocalization as measured by categorical metrics, and inference speeds up to 31x\nfaster than comparable baselines.", "AI": {"tldr": "BlockGPT is a generative autoregressive transformer for precipitation nowcasting that uses batched tokenization to predict full 2D fields per time step, achieving superior accuracy and 31x faster inference than baselines.", "motivation": "Current precipitation nowcasting methods have limitations: token-based autoregressive models suffer from flawed inductive biases and slow inference, while diffusion models are computationally intensive. There's a need for accurate and efficient real-time forecasting.", "method": "BlockGPT uses a model-agnostic paradigm with batched tokenization (Block method) that factorizes space-time using self-attention within frames and causal attention across frames. It predicts complete 2D precipitation fields at each time step.", "result": "On KNMI (Netherlands) and SEVIR (U.S.) datasets, BlockGPT outperforms state-of-the-art baselines including NowcastingGPT and DiffCast+Phydnet in accuracy, event localization, and achieves inference speeds up to 31x faster.", "conclusion": "BlockGPT provides an effective solution for precipitation nowcasting by combining the benefits of generative modeling with computational efficiency, making it suitable for real-time weather forecasting applications."}}
{"id": "2510.06410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06410", "abs": "https://arxiv.org/abs/2510.06410", "authors": ["Aochong Oliver Li", "Tanya Goyal"], "title": "Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?", "comment": null, "summary": "Reasoning LLMs are trained to verbalize their reasoning process, yielding\nstrong gains on complex tasks. This transparency also opens a promising\ndirection: multiple reasoners can directly collaborate on each other's thinking\nwithin a shared trajectory, yielding better inference efficiency and\nexploration. A key prerequisite, however, is the ability to assess the\nusefulness and build on another model's partial thinking -- we call this\noff-trajectory reasoning. Our paper investigates a critical question: can\nstandard solo-reasoning training pipelines deliver desired off-trajectory\nbehaviors? We propose twin tests that capture the two extremes of the\noff-trajectory spectrum, namely Recoverability, which tests whether LLMs can\nbacktrack from \"distractions\" induced by misleading reasoning traces, and\nGuidability, which tests their ability to build upon correct reasoning from\nstronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and\nreveals a counterintuitive finding -- \"stronger\" LLMs on benchmarks are often\nmore fragile under distraction. Moreover, all models tested fail to effectively\nleverage guiding steps from collaborators on problems beyond their inherent\ncapabilities with solve rates remaining under 9.2%. Finally, we conduct control\nstudies to isolate the effects of three factors in post-training on these\nbehaviors: the choice of distillation teacher, the use of RL, and data\nselection strategy. Our results provide actionable insights for training\nnatively strong reasoning collaborators; e.g., we find that suboptimal\nrecoverability behaviors of teacher models are transferred to distilled\nstudents even if the distillation trajectories are correct. Taken together,\nthis work lays the groundwork for evaluating multi-model collaborations in\nshared reasoning trajectories and highlights the limitations of off-the-shelf\nreasoning LLMs.", "AI": {"tldr": "The paper investigates whether standard solo-reasoning training enables LLMs to effectively collaborate by building on each other's partial reasoning (off-trajectory reasoning). It finds stronger LLMs are more fragile to distractions and models fail to leverage collaborators' reasoning, with solve rates under 9.2%.", "motivation": "To enable effective multi-model collaboration in shared reasoning trajectories, which requires the ability to assess and build upon another model's partial thinking (off-trajectory reasoning).", "method": "Proposed twin tests: Recoverability (backtracking from misleading reasoning) and Guidability (building on correct reasoning from stronger collaborators). Evaluated 15 open-weight LLMs (1.5B-32B) and conducted control studies on post-training factors.", "result": "Stronger LLMs on benchmarks are more fragile under distraction. All models fail to effectively leverage guiding steps from collaborators, with solve rates remaining under 9.2%. Suboptimal recoverability behaviors transfer from teachers to distilled students.", "conclusion": "Standard solo-reasoning training pipelines don't deliver desired off-trajectory behaviors. The work provides insights for training better reasoning collaborators and highlights limitations of current reasoning LLMs for multi-model collaboration."}}
{"id": "2510.06432", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06432", "abs": "https://arxiv.org/abs/2510.06432", "authors": ["Vipul Goyal", "Justin Raizes"], "title": "Proofs of No Intrusion", "comment": null, "summary": "A central challenge in data security is not just preventing theft, but\ndetecting whether it has occurred. Classically, this is impossible because a\nperfect copy leaves no evidence. Quantum mechanics, on the other hand, forbids\ngeneral duplication, opening up new possibilities.\n  We introduce Proofs of No Intrusion, which enable a classical client to\nremotely test whether a quantum server has been hacked and the client's data\nstolen. Crucially, the test does not destroy the data being tested, avoiding\nthe need to store a backup elsewhere. We define and construct proofs of no\nintrusion for ciphertexts assuming fully homomorphic encryption. Additionally,\nwe show how to equip several constructions of unclonable primitives with proofs\nof non-intrusion, such as unclonable decryption keys and signature tokens.\nConceptually, proofs of non-intrusion can be defined for essentially any\nunclonable primitive.\n  At the heart of our techniques is a new method for non-destructively testing\ncoset states with classical communication. It can be viewed as a\nnon-destructive proof of knowledge of a measurement result of the coset state.", "AI": {"tldr": "Proofs of No Intrusion enable classical clients to remotely test if quantum servers have been hacked without destroying the data, using non-destructive testing of coset states and applying to various unclonable primitives.", "motivation": "Classical data security cannot detect perfect copies, but quantum mechanics forbids duplication, creating new opportunities for intrusion detection without data destruction.", "method": "Introduces Proofs of No Intrusion using non-destructive testing of coset states with classical communication, assuming fully homomorphic encryption and applying to unclonable primitives.", "result": "Successfully constructs proofs of no intrusion for ciphertexts and equips unclonable decryption keys and signature tokens with non-intrusion proofs.", "conclusion": "Proofs of non-intrusion can be defined for essentially any unclonable primitive, providing a new quantum-based approach to detect data theft without data destruction."}}
{"id": "2510.06265", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06265", "abs": "https://arxiv.org/abs/2510.06265", "authors": ["Aisha Alansari", "Hamzah Luqman"], "title": "A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation", "comment": null, "summary": "Large language models (LLMs) have transformed natural language processing,\nachieving remarkable performance across diverse tasks. However, their\nimpressive fluency often comes at the cost of producing false or fabricated\ninformation, a phenomenon known as hallucination. Hallucination refers to the\ngeneration of content by an LLM that is fluent and syntactically correct but\nfactually inaccurate or unsupported by external evidence. Hallucinations\nundermine the reliability and trustworthiness of LLMs, especially in domains\nrequiring factual accuracy. This survey provides a comprehensive review of\nresearch on hallucination in LLMs, with a focus on causes, detection, and\nmitigation. We first present a taxonomy of hallucination types and analyze\ntheir root causes across the entire LLM development lifecycle, from data\ncollection and architecture design to inference. We further examine how\nhallucinations emerge in key natural language generation tasks. Building on\nthis foundation, we introduce a structured taxonomy of detection approaches and\nanother taxonomy of mitigation strategies. We also analyze the strengths and\nlimitations of current detection and mitigation approaches and review existing\nevaluation benchmarks and metrics used to quantify LLMs hallucinations.\nFinally, we outline key open challenges and promising directions for future\nresearch, providing a foundation for the development of more truthful and\ntrustworthy LLMs.", "AI": {"tldr": "This survey provides a comprehensive review of hallucination in large language models (LLMs), covering causes, detection methods, and mitigation strategies for the generation of factually inaccurate content.", "motivation": "LLMs often produce fluent but factually incorrect information (hallucinations), which undermines their reliability and trustworthiness, especially in domains requiring factual accuracy.", "method": "The survey presents taxonomies of hallucination types and root causes across the LLM development lifecycle, examines detection approaches and mitigation strategies, and reviews evaluation benchmarks and metrics.", "result": "The paper provides a structured framework for understanding, detecting, and mitigating hallucinations in LLMs, analyzing the strengths and limitations of current approaches.", "conclusion": "The survey outlines key open challenges and promising directions for future research to develop more truthful and trustworthy LLMs."}}
{"id": "2510.06303", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06303", "abs": "https://arxiv.org/abs/2510.06303", "authors": ["Shuang Cheng", "Yihan Bian", "Dawei Liu", "Yuhua Jiang", "Yihao Liu", "Linfeng Zhang", "Wenhai Wang", "Qipeng Guo", "Kai Chen", "Biqing Qi", "Bowen Zhou"], "title": "SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation", "comment": "Technical report. 39 pages, including 14 pages of appendix", "summary": "We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies\nthe training efficiency of autoregressive models with the parallel inference\ncapability of diffusion. Instead of costly end-to-end diffusion training, SDAR\nperforms a lightweight paradigm conversion that transforms a well-trained\nautoregressive (AR) model into a blockwise diffusion model through brief,\ndata-efficient adaptation. During inference, SDAR generates sequences\nautoregressively across blocks for global coherence while decoding all tokens\nwithin each block in parallel via a discrete diffusion process. Extensive\nexperiments show that AR models remain substantially more compute-efficient\nthan masked diffusion models, providing a strong foundation for adaptation.\nBuilding on this insight, SDAR achieves efficient AR-to-diffusion conversion\nwith minimal cost, preserving AR-level performance while enabling parallel\ngeneration. Scaling studies across dense and Mixture-of-Experts architectures\nconfirm that SDAR scales without compromise: larger models exhibit stronger\nrobustness to block size and decoding thresholds, yielding greater speedups\nwithout accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning\nand domain adaptability. Our 30B MoE model surpasses its AR counterpart on\nchallenging scientific reasoning benchmarks such as GPQA and ChemBench, and\ngains further improvements under test-time scaling methods like majority voting\nand pass@k. Together, these results establish SDAR as a practical paradigm that\ncombines the strengths of autoregression and diffusion for scalable,\nhigh-throughput reasoning.", "AI": {"tldr": "SDAR is a synergistic paradigm that converts autoregressive models into blockwise diffusion models through lightweight adaptation, enabling parallel inference while maintaining training efficiency.", "motivation": "To combine the training efficiency of autoregressive models with the parallel inference capability of diffusion models, avoiding costly end-to-end diffusion training.", "method": "Performs lightweight paradigm conversion from well-trained AR models to blockwise diffusion models via brief, data-efficient adaptation. During inference, generates sequences autoregressively across blocks for coherence while decoding tokens within each block in parallel via discrete diffusion.", "result": "SDAR achieves efficient AR-to-diffusion conversion with minimal cost, preserving AR-level performance while enabling parallel generation. Larger models show stronger robustness to block size and decoding thresholds, yielding greater speedups without accuracy loss. The 30B MoE model surpasses AR counterpart on scientific reasoning benchmarks like GPQA and ChemBench.", "conclusion": "SDAR establishes a practical paradigm that combines strengths of autoregression and diffusion for scalable, high-throughput reasoning, demonstrating enhanced reasoning and domain adaptability."}}
{"id": "2510.06433", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06433", "abs": "https://arxiv.org/abs/2510.06433", "authors": ["Aryan Singh Dalal", "Yinglun Zhang", "Duru Do\u011fan", "Atalay Mert \u0130leri", "Hande K\u00fc\u00e7\u00fck McGinty"], "title": "Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health", "comment": null, "summary": "The focus on \"food as medicine\" is gaining traction in the field of health\nand several studies conducted in the past few years discussed this aspect of\nfood in the literature. However, very little research has been done on\nrepresenting the relationship between food and health in a standardized,\nmachine-readable format using a semantic web that can help us leverage this\nknowledge effectively. To address this gap, this study aims to create a\nknowledge graph to link food and health through the knowledge graph's ability\nto combine information from various platforms focusing on flavonoid contents of\nfood found in the USDA databases and cancer connections found in the\nliterature. We looked closely at these relationships using KNARM methodology\nand represented them in machine-operable format. The proposed knowledge graph\nserves as an example for researchers, enabling them to explore the complex\ninterplay between dietary choices and disease management. Future work for this\nstudy involves expanding the scope of the knowledge graph by capturing nuances,\nadding more related data, and performing inferences on the acquired knowledge\nto uncover hidden relationships.", "AI": {"tldr": "This study creates a knowledge graph to link food and health relationships, specifically focusing on flavonoid contents from USDA databases and cancer connections from literature, using KNARM methodology for machine-readable representation.", "motivation": "Address the gap in representing food-health relationships in standardized, machine-readable format using semantic web technologies, as current research lacks such representations despite growing interest in 'food as medicine'.", "method": "Used KNARM methodology to create a knowledge graph combining information from USDA databases (flavonoid contents) and literature (cancer connections), representing relationships in machine-operable format.", "result": "Developed a knowledge graph that serves as an example for researchers to explore dietary choices and disease management relationships, providing a foundation for future analysis.", "conclusion": "The knowledge graph successfully demonstrates the potential for representing food-health relationships in standardized format, with future work planned to expand scope, add more data, and perform inferences to uncover hidden relationships."}}
{"id": "2510.06468", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06468", "abs": "https://arxiv.org/abs/2510.06468", "authors": ["Sergio Demian Lerner", "Ariel Futoransky"], "title": "BATTLE for Bitcoin: Capital-Efficient Optimistic Bridges with Large Committees", "comment": null, "summary": "We present BATTLE for Bitcoin, a DoS-resilient dispute layer that secures\noptimistic bridges between Bitcoin and rollups or sidechains. Our design adapts\nthe BATTLE tournament protocol to Bitcoin's UTXO model using BitVM-style FLEX\ncomponents and garbled circuits with on-demand L1 security bonds. Disputes are\nresolved in logarithmic rounds while recycling rewards, keeping the honest\nasserter's minimum initial capital constant even under many permissionless\nchallengers. The construction is fully contestable (challengers can supply\nhigher-work counter-proofs) and relies only on standard timelocks and\npre-signed transaction DAGs, without new opcodes.\n  For $N$ operators, the protocol requires $O(N^2)$ pre-signed transactions,\nsignatures, and message exchanges, yet remains practical at $N\\!\\gtrsim\\!10^3$,\nenabling high decentralization.", "AI": {"tldr": "BATTLE for Bitcoin is a DoS-resilient dispute layer that secures optimistic bridges between Bitcoin and rollups/sidechains using BitVM-style components and garbled circuits with on-demand L1 security bonds.", "motivation": "To create a secure and efficient dispute resolution mechanism for optimistic bridges connecting Bitcoin with rollups or sidechains, addressing the need for DoS-resilience and maintaining constant capital requirements for honest asserters.", "method": "Adapts the BATTLE tournament protocol to Bitcoin's UTXO model using BitVM-style FLEX components and garbled circuits with on-demand L1 security bonds. Uses standard timelocks and pre-signed transaction DAGs without requiring new opcodes.", "result": "The protocol resolves disputes in logarithmic rounds while recycling rewards, keeping the honest asserter's minimum initial capital constant even with many permissionless challengers. It requires O(N\u00b2) pre-signed transactions, signatures, and message exchanges but remains practical for N \u2273 10\u00b3 operators.", "conclusion": "BATTLE for Bitcoin provides a fully contestable, practical dispute layer that enables high decentralization for optimistic bridges between Bitcoin and other systems, using only existing Bitcoin features without requiring protocol changes."}}
{"id": "2510.06266", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06266", "abs": "https://arxiv.org/abs/2510.06266", "authors": ["Rohitash Chandra", "Yathin Suresh", "Divyansh Raj Sinha", "Sanchit Jindal"], "title": "Language models for longitudinal analysis of abusive content in Billboard Music Charts", "comment": null, "summary": "There is no doubt that there has been a drastic increase in abusive and\nsexually explicit content in music, particularly in Billboard Music Charts.\nHowever, there is a lack of studies that validate the trend for effective\npolicy development, as such content has harmful behavioural changes in children\nand youths. In this study, we utilise deep learning methods to analyse songs\n(lyrics) from Billboard Charts of the United States in the last seven decades.\nWe provide a longitudinal study using deep learning and language models and\nreview the evolution of content using sentiment analysis and abuse detection,\nincluding sexually explicit content. Our results show a significant rise in\nexplicit content in popular music from 1990 onwards. Furthermore, we find an\nincreasing prevalence of songs with lyrics containing profane, sexually\nexplicit, and otherwise inappropriate language. The longitudinal analysis of\nthe ability of language models to capture nuanced patterns in lyrical content,\nreflecting shifts in societal norms and language use over time.", "AI": {"tldr": "Deep learning analysis of Billboard songs from 1950s-2020s shows significant increase in explicit content (profanity, sexual themes) since 1990, reflecting changing societal norms.", "motivation": "Lack of validated studies on increasing abusive/sexually explicit content in popular music despite its harmful effects on children and youth, hindering effective policy development.", "method": "Used deep learning and language models to analyze lyrics from Billboard Charts over 7 decades, employing sentiment analysis and abuse detection for sexually explicit content.", "result": "Significant rise in explicit content in popular music from 1990 onwards, with increasing prevalence of profane, sexually explicit, and inappropriate language in lyrics.", "conclusion": "Language models effectively capture nuanced patterns in lyrical content evolution, reflecting shifts in societal norms and language use over time, providing evidence for policy considerations."}}
{"id": "2510.06349", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.06349", "abs": "https://arxiv.org/abs/2510.06349", "authors": ["Moein E. Samadi", "Andreas Schuppert"], "title": "Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks", "comment": null, "summary": "Foundation models have rapidly advanced AI, raising the question of whether\ntheir decisions will ultimately surpass human strategies in real-world domains.\nThe exponential, and possibly super-exponential, pace of AI development makes\nsuch analysis elusive. Nevertheless, many application areas that matter for\ndaily life and society show only modest gains so far; a prominent case is\ndiagnosing and treating dynamically evolving disease in intensive care.\n  The common challenge is adapting complex systems to dynamic environments.\nEffective strategies must optimize outcomes in systems composed of strongly\ninteracting functions while avoiding shared side effects; this requires\nreliable, self-adaptive modeling. These tasks align with building digital twins\nof highly complex systems whose mechanisms are not fully or quantitatively\nunderstood. It is therefore essential to develop methods for self-adapting AI\nmodels with minimal data and limited mechanistic knowledge. As this challenge\nextends beyond medicine, AI should demonstrate clear superiority in these\nsettings before assuming broader decision-making roles.\n  We identify the curse of dimensionality as a fundamental barrier to efficient\nself-adaptation and argue that monolithic foundation models face conceptual\nlimits in overcoming it. As an alternative, we propose a decentralized\narchitecture of interacting small agent networks (SANs). We focus on agents\nrepresenting the specialized substructure of the system, where each agent\ncovers only a subset of the full system functions. Drawing on mathematical\nresults on the learning behavior of SANs and evidence from existing\napplications, we argue that swarm-learning in diverse swarms can enable\nself-adaptive SANs to deliver superior decision-making in dynamic environments\ncompared with monolithic foundation models, though at the cost of reduced\nreproducibility in detail.", "AI": {"tldr": "Foundation models show limited gains in dynamic real-world domains like intensive care. The paper proposes decentralized small agent networks (SANs) as superior to monolithic models for self-adaptive decision-making in complex systems, though with reduced reproducibility.", "motivation": "To address the challenge of adapting complex systems to dynamic environments where foundation models show modest gains, particularly in critical domains like medical diagnosis and treatment that require reliable self-adaptive modeling.", "method": "Proposes a decentralized architecture of interacting small agent networks (SANs), where each agent represents specialized substructures covering only subsets of system functions, enabling swarm-learning in diverse swarms.", "result": "SANs can overcome the curse of dimensionality barrier and deliver superior decision-making in dynamic environments compared to monolithic foundation models, but with reduced reproducibility in detail.", "conclusion": "Decentralized SAN architectures offer a promising alternative to monolithic foundation models for self-adaptive AI in complex dynamic systems, though AI should demonstrate clear superiority in these settings before assuming broader decision-making roles."}}
{"id": "2510.06475", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06475", "abs": "https://arxiv.org/abs/2510.06475", "authors": ["Yitao Long", "Yuru Jiang", "Hongjun Liu", "Yilun Zhao", "Jingchen Sun", "Yiqiu Shen", "Chen Zhao", "Arman Cohan", "Dennis Shasha"], "title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles", "comment": null, "summary": "This work investigates the reasoning and planning capabilities of foundation\nmodels and their scalability in complex, dynamic environments. We introduce\nPuzzlePlex, a benchmark designed to assess these capabilities through a diverse\nset of puzzles. PuzzlePlex consists of 15 types of puzzles, including\ndeterministic and stochastic games of varying difficulty, as well as\nsingle-player and two-player scenarios. The PuzzlePlex framework provides a\ncomprehensive environment for each game, and supports extensibility to generate\nmore challenging instances as foundation models evolve. Additionally, we\nimplement customized game-playing strategies for comparison. Building on this\nbenchmark, we develop fine-grained metrics to measure performance and conduct\nan in-depth analysis of frontier foundation models across two settings:\ninstruction-based and code-based. Furthermore, we systematically investigate\ntheir scaling limits. Our findings show that reasoning models outperform others\nin instruction-based settings, while code-based execution presents greater\nchallenges but offers a scalable and efficient alternative. PuzzlePlex enables\ntargeted evaluation and guides future improvements in reasoning, planning, and\ngeneralization for foundation models.", "AI": {"tldr": "PuzzlePlex is a benchmark with 15 puzzle types to evaluate foundation models' reasoning and planning in complex environments, showing reasoning models excel in instruction-based settings while code-based execution offers scalability.", "motivation": "To assess reasoning and planning capabilities of foundation models in complex, dynamic environments and understand their scalability limits.", "method": "Developed PuzzlePlex benchmark with 15 puzzle types including deterministic/stochastic games and single/two-player scenarios, implemented custom strategies, and tested frontier models in instruction-based and code-based settings with fine-grained metrics.", "result": "Reasoning models outperform others in instruction-based settings; code-based execution is more challenging but provides scalable and efficient alternative.", "conclusion": "PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models."}}
{"id": "2510.06530", "categories": ["cs.CR", "cs.ET", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.06530", "abs": "https://arxiv.org/abs/2510.06530", "authors": ["Thusitha Dayaratne", "Ngoc Duy Pham", "Viet Vo", "Shangqi Lai", "Sharif Abuadbba", "Hajime Suzuki", "Xingliang Yuan", "Carsten Rudolph"], "title": "From Description to Detection: LLM based Extendable O-RAN Compliant Blind DoS Detection in 5G and Beyond", "comment": null, "summary": "The quality and experience of mobile communication have significantly\nimproved with the introduction of 5G, and these improvements are expected to\ncontinue beyond the 5G era. However, vulnerabilities in control-plane\nprotocols, such as Radio Resource Control (RRC) and Non-Access Stratum (NAS),\npose significant security threats, such as Blind Denial of Service (DoS)\nattacks. Despite the availability of existing anomaly detection methods that\nleverage rule-based systems or traditional machine learning methods, these\nmethods have several limitations, including the need for extensive training\ndata, predefined rules, and limited explainability. Addressing these\nchallenges, we propose a novel anomaly detection framework that leverages the\ncapabilities of Large Language Models (LLMs) in zero-shot mode with unordered\ndata and short natural language attack descriptions within the Open Radio\nAccess Network (O-RAN) architecture. We analyse robustness to prompt variation,\ndemonstrate the practicality of automating the attack descriptions and show\nthat detection quality relies on the semantic completeness of the description\nrather than its phrasing or length. We utilise an RRC/NAS dataset to evaluate\nthe solution and provide an extensive comparison of open-source and proprietary\nLLM implementations to demonstrate superior performance in attack detection. We\nfurther validate the practicality of our framework within O-RAN's real-time\nconstraints, illustrating its potential for detecting other Layer-3 attacks.", "AI": {"tldr": "Proposes a novel LLM-based anomaly detection framework for 5G control-plane security that works in zero-shot mode using short natural language attack descriptions, demonstrating superior performance over traditional methods.", "motivation": "Existing anomaly detection methods for 5G control-plane protocols (RRC/NAS) have limitations including need for extensive training data, predefined rules, and limited explainability, while vulnerabilities in these protocols pose significant security threats like Blind DoS attacks.", "method": "Leverages Large Language Models in zero-shot mode with unordered data and short natural language attack descriptions within O-RAN architecture, analyzing robustness to prompt variation and automating attack descriptions.", "result": "Demonstrates superior performance in attack detection compared to traditional methods, shows detection quality relies on semantic completeness rather than phrasing/length, and validates practicality within O-RAN's real-time constraints.", "conclusion": "The framework shows potential for detecting other Layer-3 attacks and provides an effective solution for 5G control-plane security with better explainability and reduced data requirements."}}
{"id": "2510.06275", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06275", "abs": "https://arxiv.org/abs/2510.06275", "authors": ["Ranjan Mishra", "Julian I. Bibo", "Quinten van Engelen", "Henk Schaapman"], "title": "Reproducibility Study of \"XRec: Large Language Models for Explainable Recommendation\"", "comment": null, "summary": "In this study, we reproduced the work done in the paper \"XRec: Large Language\nModels for Explainable Recommendation\" by Ma et al. (2024). The original\nauthors introduced XRec, a model-agnostic collaborative instruction-tuning\nframework that enables large language models (LLMs) to provide users with\ncomprehensive explanations of generated recommendations. Our objective was to\nreplicate the results of the original paper, albeit using Llama 3 as the LLM\nfor evaluation instead of GPT-3.5-turbo. We built on the source code provided\nby Ma et al. (2024) to achieve our goal. Our work extends the original paper by\nmodifying the input embeddings or deleting the output embeddings of XRec's\nMixture of Experts module. Based on our results, XRec effectively generates\npersonalized explanations and its stability is improved by incorporating\ncollaborative information. However, XRec did not consistently outperform all\nbaseline models in every metric. Our extended analysis further highlights the\nimportance of the Mixture of Experts embeddings in shaping the explanation\nstructures, showcasing how collaborative signals interact with language\nmodeling. Through our work, we provide an open-source evaluation implementation\nthat enhances accessibility for researchers and practitioners alike. Our\ncomplete code repository can be found at\nhttps://github.com/julianbibo/xrec-reproducibility.", "AI": {"tldr": "This paper reproduces XRec framework for explainable recommendations using Llama 3 instead of GPT-3.5-turbo, finding that XRec effectively generates personalized explanations but doesn't consistently outperform all baselines. The study also analyzes the role of Mixture of Experts embeddings.", "motivation": "To replicate and extend the original XRec paper by testing with different LLM (Llama 3) and analyzing the impact of Mixture of Experts module embeddings on explanation generation.", "method": "Built on original XRec source code, used Llama 3 for evaluation, modified input embeddings and deleted output embeddings of the Mixture of Experts module to study their effects.", "result": "XRec effectively generates personalized explanations and stability improves with collaborative information, but doesn't consistently outperform all baseline models. Mixture of Experts embeddings significantly shape explanation structures.", "conclusion": "The reproduction study validates XRec's effectiveness while providing insights into how collaborative signals interact with language modeling through Mixture of Experts embeddings, with open-source implementation for broader accessibility."}}
{"id": "2510.06355", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.06355", "abs": "https://arxiv.org/abs/2510.06355", "authors": ["K\u00fcr\u015fat Tekb\u0131y\u0131k", "G\u00fcne\u015f Karabulut Kurt", "Antoine Lesage-Landry"], "title": "PIKAN: Physics-Inspired Kolmogorov-Arnold Networks for Explainable UAV Channel Modelling", "comment": null, "summary": "Unmanned aerial vehicle (UAV) communications demand accurate yet\ninterpretable air-to-ground (A2G) channel models that can adapt to\nnonstationary propagation environments. While deterministic models offer\ninterpretability and deep learning (DL) models provide accuracy, both\napproaches suffer from either rigidity or a lack of explainability. To bridge\nthis gap, we propose the Physics-Inspired Kolmogorov-Arnold Network (PIKAN)\nthat embeds physical principles (e.g., free-space path loss, two-ray\nreflections) into the learning process. Unlike physics-informed neural networks\n(PINNs), PIKAN is more flexible for applying physical information because it\nintroduces them as flexible inductive biases. Thus, it enables a more flexible\ntraining process. Experiments on UAV A2G measurement data show that PIKAN\nachieves comparable accuracy to DL models while providing symbolic and\nexplainable expressions aligned with propagation laws. Remarkably, PIKAN\nachieves this performance with only 232 parameters, making it up to 37 times\nlighter than multilayer perceptron (MLP) baselines with thousands of\nparameters, without sacrificing correlation with measurements and also\nproviding symbolic expressions. These results highlight PIKAN as an efficient,\ninterpretable, and scalable solution for UAV channel modelling in beyond-5G and\n6G networks.", "AI": {"tldr": "Proposed PIKAN model embeds physical principles into neural networks for UAV channel modeling, achieving comparable accuracy to DL models with only 232 parameters while providing explainable expressions.", "motivation": "UAV communications need accurate yet interpretable A2G channel models that can adapt to nonstationary environments. Current deterministic models are rigid while DL models lack explainability.", "method": "Physics-Inspired Kolmogorov-Arnold Network (PIKAN) that embeds physical principles (free-space path loss, two-ray reflections) as flexible inductive biases in the learning process, unlike rigid PINNs.", "result": "PIKAN achieves comparable accuracy to DL models with only 232 parameters (37x lighter than MLP baselines), provides symbolic explainable expressions aligned with propagation laws, and maintains correlation with measurements.", "conclusion": "PIKAN is an efficient, interpretable, and scalable solution for UAV channel modeling in beyond-5G and 6G networks, bridging the gap between accuracy and explainability."}}
{"id": "2510.06534", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06534", "abs": "https://arxiv.org/abs/2510.06534", "authors": ["Jiahe Jin", "Abhijay Paladugu", "Chenyan Xiong"], "title": "Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them", "comment": null, "summary": "Agentic search leverages large language models (LLMs) to interpret complex\nuser information needs and execute a multi-step process of planning, searching,\nand synthesizing information to provide answers. This paradigm introduces\nunique challenges for LLMs' reasoning and agentic capabilities when interacting\nwith retrieval systems and the broader web. In this paper, we propose a\nreasoning-driven LLM-based pipeline to study effective reasoning behavior\npatterns in agentic search. Using this pipeline, we analyze successful agentic\nsearch trajectories and identify four beneficial reasoning behaviors:\nInformation Verification, Authority Evaluation, Adaptive Search, and Error\nRecovery. Based on these findings, we propose a technique called Behavior\nPriming to train more effective agentic search models. It synthesizes agentic\nsearch trajectories that exhibit these four behaviors and integrates them into\nthe agentic search model through supervised fine-tuning (SFT), followed by\nstandard reinforcement learning (RL). Experiments on three benchmarks (GAIA,\nWebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in\nLlama3.2-3B and Qwen3-1.7B compared to directly training agentic search models\nwith RL. Crucially, we demonstrate that the desired reasoning behaviors in the\nSFT data, rather than the correctness of the final answer, is the critical\nfactor for achieving strong final performance after RL: fine-tuning on\ntrajectories with desirable reasoning behaviors but incorrect answers leads to\nbetter performance than fine-tuning on trajectories with correct answers. Our\nanalysis further reveals the underlying mechanism: the introduced reasoning\nbehaviors endow models with more effective exploration (higher pass@k and\nentropy) and test-time scaling (longer trajectories) capabilities, providing a\nstrong foundation for RL. Our code will be released as open source.", "AI": {"tldr": "Proposes Behavior Priming - a technique that trains agentic search models using trajectories exhibiting four key reasoning behaviors (Information Verification, Authority Evaluation, Adaptive Search, Error Recovery) through SFT+RL, achieving 35%+ gains over RL-only training.", "motivation": "Agentic search introduces unique challenges for LLMs' reasoning and agentic capabilities when interacting with retrieval systems and the web. Current approaches need better understanding of effective reasoning behavior patterns.", "method": "Developed a reasoning-driven pipeline to analyze successful agentic search trajectories, identified four beneficial reasoning behaviors, then proposed Behavior Priming that synthesizes trajectories with these behaviors for SFT followed by standard RL.", "result": "35%+ gains on Llama3.2-3B and Qwen3-1.7B across three benchmarks (GAIA, WebWalker, HLE). Key finding: reasoning behaviors in SFT data, not answer correctness, is critical for strong RL performance.", "conclusion": "Behavior Priming enables more effective exploration and test-time scaling by endowing models with essential reasoning behaviors, providing a strong foundation for RL training in agentic search."}}
{"id": "2510.06535", "categories": ["cs.CR", "C.3; D.4.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.06535", "abs": "https://arxiv.org/abs/2510.06535", "authors": ["Jack Vanlyssel", "Enrique Sobrados", "Ramsha Anwar", "Gruia-Catalin Roman", "Afsah Anwar"], "title": "SpyChain: Multi-Vector Supply Chain Attacks on Small Satellite Systems", "comment": "18 pages, 7 figures. Version includes implementation details and\n  experimental results using NASA's NOS3 satellite simulation framework", "summary": "Small satellites are integral to scientific, commercial, and defense\nmissions, but reliance on commercial off-the-shelf (COTS) hardware broadens\ntheir attack surface. Although supply chain threats are well studied in other\ncyber-physical domains, their feasibility and stealth in space systems remain\nlargely unexplored. Prior work has focused on flight software, which benefits\nfrom strict security practices and oversight. In contrast, auxiliary COTS\ncomponents often lack robust assurance yet enjoy comparable access to critical\non-board resources, including telemetry, system calls, and the software bus.\nDespite this privileged access, the insider threat within COTS hardware supply\nchains has received little attention. In this work, we present SpyChain, the\nfirst end-to-end design and implementation of independent and colluding\nhardware supply chain threats targeting small satellites. Using NASA's\nsatellite simulation (NOS3), we demonstrate that SpyChain can evade testing,\nexfiltrate telemetry, disrupt operations, and launch Denial of Service (DoS)\nattacks through covert channels that bypass ground monitoring. Our study traces\nan escalation from a simple solo component to dynamic, coordinating malware,\nintroducing a taxonomy of stealth across five scenarios. We showcase how\nimplicit trust in auxiliary components enables covert persistence and reveal\nnovel attack vectors, highlighting a new multi-component execution technique\nthat is now incorporated into the SPARTA matrix. Our findings are reinforced by\nacknowledgment and affirmation from NASA's NOS3 team. Finally, we implement\nlightweight onboard defenses, including runtime monitoring, to mitigate threats\nlike SpyChain.", "AI": {"tldr": "SpyChain is the first end-to-end hardware supply chain attack framework targeting small satellites, demonstrating how COTS components can evade detection, exfiltrate telemetry, disrupt operations, and launch DoS attacks through covert channels.", "motivation": "Small satellites rely heavily on COTS hardware, creating supply chain vulnerabilities that are largely unexplored in space systems. While flight software has strong security oversight, auxiliary COTS components have similar privileged access but lack robust security assurance.", "method": "Developed SpyChain framework with independent and colluding hardware supply chain threats. Used NASA's NOS3 satellite simulation to demonstrate attacks across five stealth scenarios, from simple solo components to dynamic coordinating malware.", "result": "SpyChain successfully evaded testing, exfiltrated telemetry, disrupted operations, and launched DoS attacks through covert channels that bypass ground monitoring. The framework introduced novel multi-component execution techniques now included in SPARTA matrix.", "conclusion": "Implicit trust in auxiliary COTS components enables covert persistence and novel attack vectors. Lightweight onboard defenses including runtime monitoring can mitigate such threats. Findings were acknowledged and affirmed by NASA's NOS3 team."}}
{"id": "2510.06304", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06304", "abs": "https://arxiv.org/abs/2510.06304", "authors": ["Robin Kokot", "Wessel Poelman"], "title": "Type and Complexity Signals in Multilingual Question Representations", "comment": "Workshop on Multilingual Representation Learning at EMNLP 2025", "summary": "This work investigates how a multilingual transformer model represents\nmorphosyntactic properties of questions. We introduce the Question Type and\nComplexity (QTC) dataset with sentences across seven languages, annotated with\ntype information and complexity metrics including dependency length, tree\ndepth, and lexical density. Our evaluation extends probing methods to\nregression labels with selectivity controls to quantify gains in\ngeneralizability. We compare layer-wise probes on frozen Glot500-m (Imani et\nal., 2023) representations against subword TF-IDF baselines, and a fine-tuned\nmodel. Results show that statistical features classify questions effectively in\nlanguages with explicit marking, while neural probes capture fine-grained\nstructural complexity patterns better. We use these results to evaluate when\ncontextual representations outperform statistical baselines and whether\nparameter updates reduce the availability of pre-trained linguistic\ninformation.", "AI": {"tldr": "This paper investigates how multilingual transformer models represent morphosyntactic properties of questions using a new QTC dataset across 7 languages, comparing neural probes against statistical baselines to understand when contextual representations outperform traditional methods.", "motivation": "To understand how multilingual transformer models encode morphosyntactic properties of questions and determine when contextual representations provide advantages over statistical baselines for capturing linguistic complexity.", "method": "Created QTC dataset with question sentences in 7 languages annotated with type and complexity metrics; used layer-wise probing on frozen Glot500-m representations with regression labels and selectivity controls; compared against subword TF-IDF baselines and fine-tuned models.", "result": "Statistical features effectively classify questions in languages with explicit marking, while neural probes better capture fine-grained structural complexity patterns; contextual representations outperform statistical baselines for capturing structural complexity.", "conclusion": "The study provides insights into when contextual representations excel over statistical methods and evaluates whether parameter updates reduce the availability of pre-trained linguistic information in transformer models."}}
{"id": "2510.06367", "categories": ["cs.LG", "math.DS", "physics.comp-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2510.06367", "abs": "https://arxiv.org/abs/2510.06367", "authors": ["Luca Wolf", "Tobias Buck", "Bjoern Malte Schaefer"], "title": "Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics", "comment": "Accepted for the NeurIPS 2025 Machine Learning and the Physical\n  Sciences workshop. 6 pages, 3 figures", "summary": "Neural ODEs are a widely used, powerful machine learning technique in\nparticular for physics. However, not every solution is physical in that it is\nan Euler-Lagrange equation. We present Helmholtz metrics to quantify this\nresemblance for a given ODE and demonstrate their capabilities on several\nfundamental systems with noise. We combine them with a second order neural ODE\nto form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations\nin a direct fashion and with zero additional inference cost. We demonstrate\nthat, using only positional data, they can distinguish Lagrangian and\nnon-Lagrangian systems and improve the neural ODE solutions.", "AI": {"tldr": "The paper introduces Helmholtz metrics to quantify how well neural ODEs resemble physical Euler-Lagrange equations, and develops Lagrangian neural ODEs that can learn Euler-Lagrange equations directly with no extra inference cost.", "motivation": "Neural ODEs are powerful for physics applications, but many solutions aren't physical Euler-Lagrange equations, limiting their physical interpretability and accuracy.", "method": "Developed Helmholtz metrics to measure ODE resemblance to Euler-Lagrange equations, and created Lagrangian neural ODEs using second-order neural ODEs to directly learn Euler-Lagrange equations.", "result": "The approach can distinguish Lagrangian from non-Lagrangian systems using only positional data, and improves neural ODE solution quality.", "conclusion": "Lagrangian neural ODEs provide a direct way to learn physically meaningful Euler-Lagrange equations without additional inference cost, enhancing both interpretability and performance."}}
{"id": "2510.06538", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06538", "abs": "https://arxiv.org/abs/2510.06538", "authors": ["Jiajie Li", "Huayi Zhang", "Peng Lin", "Jinjun Xiong", "Wei Xu"], "title": "Auto-Prompt Ensemble for LLM Judge", "comment": null, "summary": "We present a novel framework that improves the reliability of LLM judges by\nselectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM\njudges often miss crucial evaluation dimensions because they fail to recognize\nthe implicit standards underlying human assessments. To address this challenge,\nwe propose the Auto-Prompt Ensemble (APE), an adaptive framework that\nautomatically learns evaluation dimensions from its failure cases. APE\nincorporates a confidence-based ensemble mechanism to decide when to adopt the\njudgments from additional evaluation dimensions through a novel confidence\nestimation approach called Collective Confidence. Extensive experiments\ndemonstrate that APE improves the reliability of LLM Judge across diverse\nstandard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward\nBench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a\nprincipled approach for LLM Judge to leverage test-time computation, and bridge\nthe evaluation gap between human and LLM judges.", "AI": {"tldr": "APE framework improves LLM judge reliability by automatically learning evaluation dimensions from failure cases and using confidence-based ensemble to selectively augment judgments.", "motivation": "Existing LLM judges often miss crucial evaluation dimensions because they fail to recognize implicit standards in human assessments, creating an evaluation gap.", "method": "Proposes Auto-Prompt Ensemble (APE) - adaptive framework that learns evaluation dimensions from failure cases, uses Collective Confidence estimation for confidence-based ensemble decisions.", "result": "APE improves GPT-4o agreement rate on Reward Bench from 87.2% to 90.5% in zero-shot setting, enhances reliability across diverse benchmarks.", "conclusion": "APE provides principled approach for LLM judges to leverage test-time computation and bridge evaluation gap between human and LLM judges."}}
{"id": "2510.06565", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06565", "abs": "https://arxiv.org/abs/2510.06565", "authors": ["Jiuan Zhou", "Yu Cheng", "Yuan Xie", "Zhaoxia Yin"], "title": "Auto-Stega: An Agent-Driven System for Lifelong Strategy Evolution in LLM-Based Text Steganography", "comment": "15 pages, 9 figures", "summary": "With the rapid progress of LLMs, high quality generative text has become\nwidely available as a cover for text steganography. However, prevailing methods\nrely on hand-crafted or pre-specified strategies and struggle to balance\nefficiency, imperceptibility, and security, particularly at high embedding\nrates. Accordingly, we propose Auto-Stega, an agent-driven self-evolving\nframework that is the first to realize self-evolving steganographic strategies\nby automatically discovering, composing, and adapting strategies at inference\ntime; the framework operates as a closed loop of generating, evaluating,\nsummarizing, and updating that continually curates a structured strategy\nlibrary and adapts across corpora, styles, and task constraints. A decoding LLM\nrecovers the information under the shared strategy. To handle high embedding\nrates, we introduce PC-DNTE, a plug-and-play algorithm that maintains alignment\nwith the base model's conditional distribution at high embedding rates,\npreserving imperceptibility while enhancing security. Experimental results\ndemonstrate that at higher embedding rates Auto-Stega achieves superior\nperformance with gains of 42.2\\% in perplexity and 1.6\\% in anti-steganalysis\nperformance over SOTA methods.", "AI": {"tldr": "Auto-Stega is a self-evolving text steganography framework that automatically discovers and adapts steganographic strategies using LLMs, achieving better performance at high embedding rates compared to state-of-the-art methods.", "motivation": "Current text steganography methods struggle to balance efficiency, imperceptibility, and security at high embedding rates, relying on hand-crafted strategies that lack adaptability.", "method": "Proposes Auto-Stega framework with agent-driven closed-loop system for strategy discovery, composition, and adaptation; introduces PC-DNTE algorithm for maintaining distribution alignment at high embedding rates; uses decoding LLM for information recovery.", "result": "At high embedding rates, achieves 42.2% improvement in perplexity and 1.6% improvement in anti-steganalysis performance over state-of-the-art methods.", "conclusion": "Auto-Stega successfully addresses the limitations of existing steganography methods by enabling self-evolving strategies and maintaining imperceptibility at high embedding rates, demonstrating superior performance in text steganography."}}
{"id": "2510.06354", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06354", "abs": "https://arxiv.org/abs/2510.06354", "authors": ["Ingroj Shrestha", "Padmini Srinivasan"], "title": "LLM Bias Detection and Mitigation through the Lens of Desired Distributions", "comment": "Accepted to EMNLP 2025", "summary": "Although prior work on bias mitigation has focused on promoting social\nequality and demographic parity, less attention has been given to aligning\nLLM's outputs to desired distributions. For example, we might want to align a\nmodel with real-world distributions to support factual grounding. Thus, we\ndefine bias as deviation from a desired distribution, which may be an equal or\nreal-world distribution, depending on application goals. We propose a weighted\nadaptive loss based fine-tuning method that aligns LLM's gender-profession\noutput distribution with the desired distribution, while preserving language\nmodeling capability. Using 3 profession sets -- male-dominated,\nfemale-dominated, and gender-balanced -- derived from U.S. labor statistics\n(2024), we assess both our adaptive method for reflecting reality and a\nnon-adaptive variant for equality. Across three masked language models, bias is\nobserved under both distributions. We achieve near-complete mitigation under\nequality and 30-75% reduction under real-world settings. Autoregressive LLMs\nshow no bias under equality but notable bias under real-world settings, with\nthe Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50-62% reduction.", "AI": {"tldr": "This paper proposes a weighted adaptive loss fine-tuning method to align LLM gender-profession outputs with desired distributions (equal or real-world), achieving significant bias reduction while preserving language modeling capability.", "motivation": "Prior bias mitigation work focused on social equality, but less attention was given to aligning LLM outputs with desired distributions like real-world data for factual grounding. The paper defines bias as deviation from a desired distribution.", "method": "Proposed a weighted adaptive loss based fine-tuning method that aligns LLM's gender-profession output distribution with desired distributions. Used 3 profession sets (male/female-dominated, gender-balanced) from US labor statistics (2024) to assess both adaptive (reality) and non-adaptive (equality) variants.", "result": "Across three masked language models, bias was observed under both distributions. Achieved near-complete mitigation under equality and 30-75% reduction under real-world settings. Autoregressive LLMs showed no bias under equality but notable bias under real-world settings, with Llama Instruct models achieving 50-62% reduction.", "conclusion": "The proposed weighted adaptive loss fine-tuning effectively reduces bias in LLM gender-profession outputs for both equality and real-world distribution goals while maintaining language modeling performance."}}
{"id": "2510.06377", "categories": ["cs.LG", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.06377", "abs": "https://arxiv.org/abs/2510.06377", "authors": ["Rishabh Ranjan", "Valter Hudovernik", "Mark Znidar", "Charilaos Kanatsoulis", "Roshan Upendra", "Mahmoud Mohammadi", "Joe Meyer", "Tom Palczewski", "Carlos Guestrin", "Jure Leskovec"], "title": "Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data", "comment": "preprint; under review", "summary": "Pretrained transformers readily adapt to new sequence modeling tasks via\nzero-shot prompting, but relational domains still lack architectures that\ntransfer across datasets and tasks. The core challenge is the diversity of\nrelational data, with varying heterogeneous schemas, graph structures and\nfunctional dependencies. In this paper, we present the Relational Transformer\n(RT) architecture, which can be pretrained on diverse relational databases and\ndirectly applied to unseen datasets and tasks without task- or dataset-specific\nfine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with\ntable/column metadata, (ii) is pretrained via masked token prediction, and\n(iii) utilizes a novel \\textit{Relational Attention} mechanism over columns,\nrows, and primary-foreign key links. Pretrained on RelBench datasets spanning\ntasks such as churn and sales forecasting, RT attains strong zero-shot\nperformance, averaging 94% of fully supervised AUROC on binary classification\ntasks with a single forward pass of a 22M parameter model, as opposed to 84%\nfor a 27B LLM. Fine-tuning yields state-of-the-art results with high sample\nefficiency. Our experiments show that RT's zero-shot transfer harnesses\ntask-table context, relational attention patterns and schema semantics.\nOverall, RT provides a practical path toward foundation models for relational\ndata.", "AI": {"tldr": "The Relational Transformer (RT) is a novel architecture that enables zero-shot transfer across diverse relational databases without task-specific fine-tuning, achieving strong performance through relational attention mechanisms and pretraining on heterogeneous schemas.", "motivation": "Relational domains lack architectures that can transfer across datasets and tasks due to the diversity of relational data with varying schemas, graph structures, and functional dependencies.", "method": "RT tokenizes cells with table/column metadata, uses masked token prediction pretraining, and employs a novel Relational Attention mechanism over columns, rows, and primary-foreign key links.", "result": "Pretrained on RelBench datasets, RT achieves 94% of fully supervised AUROC on binary classification tasks with a 22M parameter model in zero-shot setting, outperforming a 27B LLM (84%). Fine-tuning yields state-of-the-art results with high sample efficiency.", "conclusion": "RT provides a practical path toward foundation models for relational data by effectively harnessing task-table context, relational attention patterns, and schema semantics for zero-shot transfer."}}
{"id": "2510.06587", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06587", "abs": "https://arxiv.org/abs/2510.06587", "authors": ["Jingbo Yang", "Bairu Hou", "Wei Wei", "Shiyu Chang", "Yujia Bao"], "title": "WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks", "comment": null, "summary": "Large language model (LLM) agents are becoming competent at straightforward\nweb tasks, such as opening an item page or submitting a form, but still\nstruggle with objectives that require long horizon navigation, large scale\ninformation extraction, and reasoning under constraints. We present WebDART, a\ngeneral framework that enables a single LLM to handle such complex chores.\nWebDART (i) dynamically decomposes each objective into three focused subtasks:\nnavigation, information extraction, and execution, so the model concentrates on\none skill at a time, and (ii) continuously replans the decomposition as new\nwebpages are revealed, taking advantage of newly discovered filters or\nshortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,\nWebDART lifts success rates by up to 13.7 percentage points over previous SOTA\nagents, while matching their performance on the easier WebArena suite and\ncompleting tasks with up to 14.7 fewer navigation steps.", "AI": {"tldr": "WebDART is a framework that enables single LLM agents to handle complex web tasks by dynamically decomposing objectives into navigation, information extraction, and execution subtasks, with continuous replanning as new webpages are revealed.", "motivation": "Current LLM agents struggle with complex web tasks requiring long horizon navigation, large scale information extraction, and reasoning under constraints, despite being competent at straightforward tasks.", "method": "WebDART dynamically decomposes objectives into three focused subtasks (navigation, information extraction, execution) and continuously replans the decomposition as new webpages are revealed to take advantage of discovered filters/shortcuts and avoid redundant exploration.", "result": "On WebChoreArena, WebDART improves success rates by up to 13.7 percentage points over previous SOTA agents, matches performance on WebArena, and completes tasks with up to 14.7 fewer navigation steps.", "conclusion": "WebDART effectively enables single LLM agents to handle complex web tasks through dynamic task decomposition and continuous replanning, significantly improving performance on challenging benchmarks."}}
{"id": "2510.06605", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06605", "abs": "https://arxiv.org/abs/2510.06605", "authors": ["Shuo Shao", "Yiming Li", "Hongwei Yao", "Yifei Chen", "Yuchen Yang", "Zhan Qin"], "title": "Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation", "comment": null, "summary": "The substantial investment required to develop Large Language Models (LLMs)\nmakes them valuable intellectual property, raising significant concerns about\ncopyright protection. LLM fingerprinting has emerged as a key technique to\naddress this, which aims to verify a model's origin by extracting an intrinsic,\nunique signature (a \"fingerprint\") and comparing it to that of a source model\nto identify illicit copies. However, existing black-box fingerprinting methods\noften fail to generate distinctive LLM fingerprints. This ineffectiveness\narises because black-box methods typically rely on model outputs, which lose\ncritical information about the model's unique parameters due to the usage of\nnon-linear functions. To address this, we first leverage Fisher Information\nTheory to formally demonstrate that the gradient of the model's input is a more\ninformative feature for fingerprinting than the output. Based on this insight,\nwe propose ZeroPrint, a novel method that approximates these information-rich\ngradients in a black-box setting using zeroth-order estimation. ZeroPrint\novercomes the challenge of applying this to discrete text by simulating input\nperturbations via semantic-preserving word substitutions. This operation allows\nZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint.\nExperiments on the standard benchmark show ZeroPrint achieves a\nstate-of-the-art effectiveness and robustness, significantly outperforming\nexisting black-box methods.", "AI": {"tldr": "ZeroPrint is a novel black-box fingerprinting method that uses zeroth-order estimation to approximate gradients through semantic-preserving word substitutions, achieving state-of-the-art performance in identifying illicit LLM copies.", "motivation": "Existing black-box fingerprinting methods fail to generate distinctive LLM fingerprints because they rely on model outputs that lose critical information about unique parameters due to non-linear functions.", "method": "Leverages Fisher Information Theory to show gradients are more informative than outputs, then uses zeroth-order estimation with semantic-preserving word substitutions to approximate the model's Jacobian matrix as a fingerprint.", "result": "Experiments on standard benchmarks show ZeroPrint achieves state-of-the-art effectiveness and robustness, significantly outperforming existing black-box methods.", "conclusion": "Gradient-based fingerprinting via zeroth-order estimation provides a more effective approach for LLM copyright protection compared to output-based methods."}}
{"id": "2510.06370", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06370", "abs": "https://arxiv.org/abs/2510.06370", "authors": ["Kshitish Ghate", "Andy Liu", "Devansh Jain", "Taylor Sorensen", "Atoosa Kasirzadeh", "Aylin Caliskan", "Mona T. Diab", "Maarten Sap"], "title": "EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference", "comment": "Preprint under review", "summary": "As large language models (LLMs) are deployed globally, creating pluralistic\nsystems that can accommodate the diverse preferences and values of users\nworldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure\nLLMs' and reward models' (RMs) steerability towards users' value and stylistic\npreference profiles grounded in psychology and human-LLM interaction\nliterature. To address the gap in existing datasets that do not support\ncontrolled evaluations of RM steering, we synthetically generated 165,888\npreference pairs -- systematically varying pairs along 4 value dimensions\n(traditional, secular-rational, survival, and self-expression) and 4 style\ndimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER\nto evaluate whether, given a user profile and a pair of candidate value-laden\nand style-laden responses, LLMs and RMs are able to select the output that\naligns with the user's preferences. We evaluate six open-source and proprietary\nLLMs and RMs under sixteen systematic prompting conditions and six preference\ncomparison scenarios. Notably, our results show that, when given the user's\nfull profile of values and stylistic preferences, the best models achieve <75%\naccuracy at choosing the correct response, in contrast to >99% accuracy when\nonly relevant style and value preferences are provided. EVALUESTEER thus\nhighlights the limitations of current RMs at identifying and adapting to\nrelevant user profile information, and provides a challenging testbed for\ndeveloping RMs that can be steered towards diverse human values and\npreferences.", "AI": {"tldr": "EVALUESTEER is a benchmark for evaluating LLMs' and reward models' ability to align with diverse user value and style preferences, showing current models struggle with complex preference profiles.", "motivation": "To address the need for pluralistic AI systems that can accommodate diverse global user preferences and values, and fill the gap in existing datasets for controlled evaluation of reward model steering.", "method": "Created 165,888 synthetic preference pairs systematically varying 4 value dimensions (traditional, secular-rational, survival, self-expression) and 4 style dimensions (verbosity, readability, confidence, warmth), then evaluated 6 LLMs/RMs under 16 prompting conditions and 6 preference scenarios.", "result": "Best models achieved <75% accuracy when given full user profiles, compared to >99% accuracy when only relevant preferences were provided, highlighting limitations in identifying and adapting to relevant user information.", "conclusion": "EVALUESTEER reveals current RMs' limitations in steering towards diverse human values and preferences, providing a challenging testbed for developing more adaptable pluralistic AI systems."}}
{"id": "2510.06381", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06381", "abs": "https://arxiv.org/abs/2510.06381", "authors": ["Tristan Cazenave"], "title": "Monte Carlo Permutation Search", "comment": null, "summary": "We propose Monte Carlo Permutation Search (MCPS), a general-purpose Monte\nCarlo Tree Search (MCTS) algorithm that improves upon the GRAVE algorithm. MCPS\nis relevant when deep reinforcement learning is not an option, or when the\ncomputing power available before play is not substantial, such as in General\nGame Playing, for example. The principle of MCPS is to include in the\nexploration term of a node the statistics on all the playouts that contain all\nthe moves on the path from the root to the node. We extensively test MCPS on a\nvariety of games: board games, wargame, investment game, video game and\nmulti-player games. MCPS has better results than GRAVE in all the two-player\ngames. It has equivalent results for multi-player games because these games are\ninherently balanced even when players have different strengths. We also show\nthat using abstract codes for moves instead of exact codes can be beneficial to\nboth MCPS and GRAVE, as they improve the permutation statistics and the AMAF\nstatistics. We also provide a mathematical derivation of the formulas used for\nweighting the three sources of statistics. These formulas are an improvement on\nthe GRAVE formula since they no longer use the bias hyperparameter of GRAVE.\nMoreover, MCPS is not sensitive to the ref hyperparameter.", "AI": {"tldr": "MCPS is an improved MCTS algorithm that enhances GRAVE by incorporating permutation statistics from all playouts containing moves along the path from root to node, achieving better performance in two-player games.", "motivation": "To develop a better MCTS algorithm for scenarios where deep reinforcement learning isn't feasible or computing power is limited, such as in General Game Playing contexts.", "method": "MCPS modifies the exploration term by including statistics from all playouts that contain all moves on the path from root to node, using abstract codes for moves and improved mathematical formulas without GRAVE's bias hyperparameter.", "result": "MCPS outperforms GRAVE in all two-player games tested (board games, wargame, investment game, video game) and has equivalent performance in multi-player games due to inherent game balance.", "conclusion": "MCPS is a significant improvement over GRAVE, particularly for two-player games, with reduced sensitivity to hyperparameters and better statistical weighting through abstract move coding."}}
{"id": "2510.06600", "categories": ["cs.AI", "H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.06600", "abs": "https://arxiv.org/abs/2510.06600", "authors": ["Zhaochun Ren", "Zhou Yang", "Chenglong Ye", "Haizhou Sun", "Chao Chen", "Xiaofei Zhu", "Xiangwen Liao"], "title": "Fine-Grained Emotion Recognition via In-Context Learning", "comment": "9 pages, 10 figures, 4 tables", "summary": "Fine-grained emotion recognition aims to identify the emotional type in\nqueries through reasoning and decision-making processes, playing a crucial role\nin various systems. Recent methods use In-Context Learning (ICL), enhancing the\nrepresentation of queries in the reasoning process through semantically similar\nexamples, while further improving emotion recognition by explaining the\nreasoning mechanisms. However, these methods enhance the reasoning process but\noverlook the decision-making process. This paper investigates decision-making\nin fine-grained emotion recognition through prototype theory. We show that ICL\nrelies on similarity matching between query representations and emotional\nprototypes within the model, where emotion-accurate representations are\ncritical. However, semantically similar examples often introduce emotional\ndiscrepancies, hindering accurate representations and causing errors. To\naddress this, we propose Emotion In-Context Learning (EICL), which introduces\nemotionally similar examples and uses a dynamic soft-label strategy to improve\nquery representations in the emotion reasoning process. A two-stage exclusion\nstrategy is then employed to assess similarity from multiple angles, further\noptimizing the decision-making process. Extensive experiments show that EICL\nsignificantly outperforms ICL on multiple datasets.", "AI": {"tldr": "EICL improves fine-grained emotion recognition by addressing emotional discrepancies in ICL through emotionally similar examples and dynamic soft-label strategy, outperforming ICL on multiple datasets.", "motivation": "Current ICL methods enhance reasoning but overlook decision-making in emotion recognition, and semantically similar examples often introduce emotional discrepancies that hinder accurate representations.", "method": "Proposes Emotion In-Context Learning (EICL) with emotionally similar examples, dynamic soft-label strategy for better query representations, and two-stage exclusion strategy for multi-angle similarity assessment.", "result": "Extensive experiments show EICL significantly outperforms ICL on multiple datasets.", "conclusion": "EICL effectively addresses emotional discrepancies in ICL and improves both reasoning and decision-making processes in fine-grained emotion recognition."}}
{"id": "2510.06607", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06607", "abs": "https://arxiv.org/abs/2510.06607", "authors": ["Weidi Luo", "Qiming Zhang", "Tianyu Lu", "Xiaogeng Liu", "Bin Hu", "Hung-Chun Chiu", "Siyuan Ma", "Yizhe Zhang", "Xusheng Xiao", "Yinzhi Cao", "Zhen Xiang", "Chaowei Xiao"], "title": "Code Agent can be an End-to-end System Hacker: Benchmarking Real-world Threats of Computer-use Agent", "comment": null, "summary": "Computer-use agent (CUA) frameworks, powered by large language models (LLMs)\nor multimodal LLMs (MLLMs), are rapidly maturing as assistants that can\nperceive context, reason, and act directly within software environments. Among\ntheir most critical applications is operating system (OS) control. As CUAs in\nthe OS domain become increasingly embedded in daily operations, it is\nimperative to examine their real-world security implications, specifically\nwhether CUAs can be misused to perform realistic, security-relevant attacks.\nExisting works exhibit four major limitations: Missing attacker-knowledge model\non tactics, techniques, and procedures (TTP), Incomplete coverage for\nend-to-end kill chains, unrealistic environment without multi-host and\nencrypted user credentials, and unreliable judgment dependent on\nLLM-as-a-Judge. To address these gaps, we propose AdvCUA, the first benchmark\naligned with real-world TTPs in MITRE ATT&CK Enterprise Matrix, which comprises\n140 tasks, including 40 direct malicious tasks, 74 TTP-based malicious tasks,\nand 26 end-to-end kill chains, systematically evaluates CUAs under a realistic\nenterprise OS security threat in a multi-host environment sandbox by hard-coded\nevaluation. We evaluate the existing five mainstream CUAs, including ReAct,\nAutoGPT, Gemini CLI, Cursor CLI, and Cursor IDE based on 8 foundation LLMs. The\nresults demonstrate that current frontier CUAs do not adequately cover OS\nsecurity-centric threats. These capabilities of CUAs reduce dependence on\ncustom malware and deep domain expertise, enabling even inexperienced attackers\nto mount complex enterprise intrusions, which raises social concern about the\nresponsibility and security of CUAs.", "AI": {"tldr": "AdvCUA is the first benchmark aligned with real-world MITRE ATT&CK TTPs to evaluate computer-use agents' security risks in enterprise OS environments, finding current CUAs enable complex intrusions even by inexperienced attackers.", "motivation": "As computer-use agents become embedded in daily OS operations, there's an urgent need to examine their real-world security implications and whether they can be misused for security attacks, given limitations in existing evaluation methods.", "method": "Proposed AdvCUA benchmark with 140 tasks (40 direct malicious, 74 TTP-based malicious, 26 end-to-end kill chains) systematically evaluating CUAs under realistic enterprise OS security threats in multi-host environment sandbox using hard-coded evaluation.", "result": "Evaluation of 5 mainstream CUAs (ReAct, AutoGPT, Gemini CLI, Cursor CLI, Cursor IDE) based on 8 foundation LLMs shows current frontier CUAs don't adequately cover OS security threats and enable complex enterprise intrusions.", "conclusion": "CUAs reduce dependence on custom malware and deep expertise, enabling inexperienced attackers to mount complex intrusions, raising social concerns about CUA responsibility and security."}}
{"id": "2510.06371", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.06371", "abs": "https://arxiv.org/abs/2510.06371", "authors": ["Firoj Alam", "Ali Ezzat Shahroor", "Md. Arid Hasan", "Zien Sheikh Ali", "Hunzalah Hassan Bhatti", "Mohamed Bayan Kmainasi", "Shammur Absar Chowdhury", "Basel Mousi", "Fahim Dalvi", "Nadir Durrani", "Natasa Milic-Frayling"], "title": "EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA", "comment": "Multimodal Foundation Models, Large Language Models, Native,\n  Multilingual, Language Diversity, Contextual Understanding, Culturally\n  Informed", "summary": "Large-scale multimodal models achieve strong results on tasks like Visual\nQuestion Answering (VQA), but they often fail when queries require culturally\ngrounded, everyday knowledge, particularly in low-resource and underrepresented\nlanguages. To bridge this gap, we introduce Everyday Multimodal and\nMultilingual QA (EverydayMMQA), a framework for creating large-scale,\nculturally-grounded datasets for spoken and visual question answering (SVQA).\nUsing this framework, we developed OASIS, a multimodal dataset integrating\nspeech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS\ncontains 3.7M spoken questions, enabling four unique input combinations:\nspeech-only, text-only, speech+image, and text+image. Focused on English and\nArabic varieties, 18 countries, the dataset content is curated to reflect\ndiverse, real-world situations. OASIS tests models on tasks beyond object\nrecognition that involve pragmatic, commonsense, and culturally aware\nreasoning. We benchmarked four closed-source models, three open-source models,\nand one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark\nand training dataset for building multimodal LLMs for a comprehensive set of\neveryday tasks within cultural contexts. The framework and dataset will be made\npublicly available to the community.", "AI": {"tldr": "The paper introduces EverydayMMQA framework and OASIS dataset to address cultural and linguistic gaps in multimodal AI, focusing on everyday knowledge in underrepresented languages like Arabic varieties.", "motivation": "Large multimodal models fail on culturally grounded everyday knowledge tasks, especially in low-resource languages, creating a need for culturally-aware benchmarks.", "method": "Developed EverydayMMQA framework to create culturally-grounded datasets, resulting in OASIS dataset with 0.92M images, 14.8M QA pairs, and 3.7M spoken questions across speech, text, and image modalities.", "result": "Created comprehensive multimodal dataset covering 18 countries with English and Arabic varieties, enabling testing of pragmatic, commonsense, and cultural reasoning beyond object recognition.", "conclusion": "EverydayMMQA and OASIS provide essential benchmarks and training data for developing culturally-aware multimodal LLMs, addressing gaps in underrepresented languages and everyday knowledge."}}
{"id": "2510.06388", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06388", "abs": "https://arxiv.org/abs/2510.06388", "authors": ["Yuxuan Lu", "Yifan Wu", "Jason Hartline", "Lunjia Hu"], "title": "Making and Evaluating Calibrated Forecasts", "comment": null, "summary": "Calibrated predictions can be reliably interpreted as probabilities. An\nimportant step towards achieving better calibration is to design an appropriate\ncalibration measure to meaningfully assess the miscalibration level of a\npredictor. A recent line of work initiated by Haghtalab et al. [2024] studies\nthe design of truthful calibration measures: a truthful measure is minimized\nwhen a predictor outputs the true probabilities, whereas a non-truthful measure\nincentivizes the predictor to lie so as to appear more calibrated. All previous\ncalibration measures were non-truthful until Hartline et al. [2025] introduced\nthe first perfectly truthful calibration measures for binary prediction tasks\nin the batch setting.\n  We introduce a perfectly truthful calibration measure for multi-class\nprediction tasks, generalizing the work of Hartline et al. [2025] beyond binary\nprediction. We study common methods of extending calibration measures from\nbinary to multi-class prediction and identify ones that do or do not preserve\ntruthfulness. In addition to truthfulness, we mathematically prove and\nempirically verify that our calibration measure exhibits superior robustness:\nit robustly preserves the ordering between dominant and dominated predictors,\nregardless of the choice of hyperparameters (bin sizes). This result addresses\nthe non-robustness issue of binned ECE, which has been observed repeatedly in\nprior work.", "AI": {"tldr": "The paper introduces the first perfectly truthful calibration measure for multi-class prediction tasks, generalizing previous binary-only truthful measures and addressing robustness issues in existing calibration metrics.", "motivation": "Existing calibration measures are non-truthful, meaning they incentivize predictors to lie to appear more calibrated. While recent work introduced truthful measures for binary prediction, there was a need to extend truthful calibration assessment to multi-class settings.", "method": "The authors generalize truthful calibration measures from binary to multi-class prediction, studying which extension methods preserve truthfulness. They mathematically prove and empirically verify their measure's robustness properties.", "result": "The paper successfully develops a perfectly truthful calibration measure for multi-class prediction that robustly preserves ordering between predictors regardless of hyperparameter choices, solving the non-robustness issue of binned ECE.", "conclusion": "This work provides the first truthful calibration measure for multi-class prediction that is both perfectly truthful and robust, enabling reliable calibration assessment without incentivizing dishonest behavior from predictors."}}
{"id": "2510.06674", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06674", "abs": "https://arxiv.org/abs/2510.06674", "authors": ["Cen", "Zhao", "Tiantian Zhang", "Hanchen Su", "Yufeng", "Zhang", "Shaowei Su", "Mingzhi Xu", "Yu", "Liu", "Wei Han", "Jeremy Werner", "Claire Na Cheng", "Yashar Mehdad"], "title": "Agent-in-the-Loop: A Data Flywheel for Continuous Improvement in LLM-based Customer Support", "comment": "EMNLP 2025 Industry Track submission (Paper #305). Preprint. Main\n  text within the 7-page industry limit (references/appendices excluded).\n  Contains multiple figures and tables", "summary": "We introduce an Agent-in-the-Loop (AITL) framework that implements a\ncontinuous data flywheel for iteratively improving an LLM-based customer\nsupport system. Unlike standard offline approaches that rely on batch\nannotations, AITL integrates four key types of annotations directly into live\ncustomer operations: (1) pairwise response preferences, (2) agent adoption and\nrationales, (3) knowledge relevance checks, and (4) identification of missing\nknowledge. These feedback signals seamlessly feed back into models' updates,\nreducing retraining cycles from months to weeks. Our production pilot involving\nUS-based customer support agents demonstrated significant improvements in\nretrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality\n(+8.4% helpfulness) and agent adoption rates (+4.5%). These results underscore\nthe effectiveness of embedding human feedback loops directly into operational\nworkflows to continuously refine LLM-based customer support system.", "AI": {"tldr": "AITL framework enables continuous improvement of LLM-based customer support through real-time human feedback integration, reducing retraining cycles from months to weeks and achieving significant performance gains.", "motivation": "Standard offline approaches with batch annotations are slow and inefficient for improving customer support systems. There's a need for faster, more integrated feedback mechanisms that can continuously refine LLM performance during live operations.", "method": "Agent-in-the-Loop (AITL) framework integrates four annotation types directly into live customer support: (1) pairwise response preferences, (2) agent adoption and rationales, (3) knowledge relevance checks, and (4) identification of missing knowledge. These feedback signals are used for continuous model updates.", "result": "Production pilot showed significant improvements: retrieval accuracy (+11.7% recall@75, +14.8% precision@8), generation quality (+8.4% helpfulness), and agent adoption rates (+4.5%). Retraining cycles reduced from months to weeks.", "conclusion": "Embedding human feedback loops directly into operational workflows is highly effective for continuously refining LLM-based customer support systems, enabling faster iteration and better performance."}}
{"id": "2510.06629", "categories": ["cs.CR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06629", "abs": "https://arxiv.org/abs/2510.06629", "authors": ["Jiachen Li", "Bang Wu", "Xiaoyu Xia", "Xiaoning Liu", "Xun Yi", "Xiuzhen Zhang"], "title": "Unsupervised Backdoor Detection and Mitigation for Spiking Neural Networks", "comment": "To appear in The 28th International Symposium on Research in Attacks,\n  Intrusions and Defenses (RAID 2025)", "summary": "Spiking Neural Networks (SNNs) have gained increasing attention for their\nsuperior energy efficiency compared to Artificial Neural Networks (ANNs).\nHowever, their security aspects, particularly under backdoor attacks, have\nreceived limited attention. Existing defense methods developed for ANNs perform\npoorly or can be easily bypassed in SNNs due to their event-driven and temporal\ndependencies. This paper identifies the key blockers that hinder traditional\nbackdoor defenses in SNNs and proposes an unsupervised post-training detection\nframework, Temporal Membrane Potential Backdoor Detection (TMPBD), to overcome\nthese challenges. TMPBD leverages the maximum margin statistics of temporal\nmembrane potential (TMP) in the final spiking layer to detect target labels\nwithout any attack knowledge or data access. We further introduce a robust\nmitigation mechanism, Neural Dendrites Suppression Backdoor Mitigation (NDSBM),\nwhich clamps dendritic connections between early convolutional layers to\nsuppress malicious neurons while preserving benign behaviors, guided by TMP\nextracted from a small, clean, unlabeled dataset. Extensive experiments on\nmultiple neuromorphic benchmarks and state-of-the-art input-aware dynamic\ntrigger attacks demonstrate that TMPBD achieves 100% detection accuracy, while\nNDSBM reduces the attack success rate from 100% to 8.44%, and to 2.81% when\ncombined with detection, without degrading clean accuracy.", "AI": {"tldr": "Proposes TMPBD for detecting backdoor attacks in SNNs using temporal membrane potential statistics, and NDSBM for mitigating attacks by suppressing malicious neurons while preserving clean accuracy.", "motivation": "SNNs have superior energy efficiency but their security under backdoor attacks is understudied. Traditional ANN defenses fail in SNNs due to event-driven and temporal dependencies.", "method": "TMPBD uses maximum margin statistics of temporal membrane potential in final spiking layer for unsupervised detection. NDSBM clamps dendritic connections between early convolutional layers to suppress malicious neurons using TMP from clean unlabeled data.", "result": "TMPBD achieves 100% detection accuracy. NDSBM reduces attack success rate from 100% to 8.44%, and to 2.81% when combined with detection, without degrading clean accuracy.", "conclusion": "The proposed framework effectively addresses backdoor attacks in SNNs through temporal membrane potential analysis and neural dendrite suppression, achieving high detection and mitigation performance."}}
{"id": "2510.06378", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06378", "abs": "https://arxiv.org/abs/2510.06378", "authors": ["Angie Boggust", "Donghao Ren", "Yannick Assogba", "Dominik Moritz", "Arvind Satyanarayan", "Fred Hohman"], "title": "Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language", "comment": null, "summary": "Automated interpretability aims to translate large language model (LLM)\nfeatures into human understandable descriptions. However, these natural\nlanguage feature descriptions are often vague, inconsistent, and require manual\nrelabeling. In response, we introduce semantic regexes, structured language\ndescriptions of LLM features. By combining primitives that capture linguistic\nand semantic feature patterns with modifiers for contextualization,\ncomposition, and quantification, semantic regexes produce precise and\nexpressive feature descriptions. Across quantitative benchmarks and qualitative\nanalyses, we find that semantic regexes match the accuracy of natural language\nwhile yielding more concise and consistent feature descriptions. Moreover,\ntheir inherent structure affords new types of analyses, including quantifying\nfeature complexity across layers, scaling automated interpretability from\ninsights into individual features to model-wide patterns. Finally, in user\nstudies, we find that semantic regex descriptions help people build accurate\nmental models of LLM feature activations.", "AI": {"tldr": "The paper introduces semantic regexes as structured language descriptions for LLM features, addressing the vagueness and inconsistency of natural language descriptions in automated interpretability.", "motivation": "Natural language descriptions of LLM features are often vague, inconsistent, and require manual relabeling, which limits the effectiveness of automated interpretability methods.", "method": "Semantic regexes combine linguistic and semantic feature pattern primitives with modifiers for contextualization, composition, and quantification to create precise and expressive feature descriptions.", "result": "Semantic regexes match natural language accuracy while providing more concise and consistent descriptions. They enable new analyses like quantifying feature complexity across layers and scaling interpretability from individual features to model-wide patterns.", "conclusion": "Semantic regexes help people build accurate mental models of LLM feature activations and provide a more structured approach to automated interpretability that overcomes limitations of natural language descriptions."}}
{"id": "2510.06397", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06397", "abs": "https://arxiv.org/abs/2510.06397", "authors": ["Ali Baheri"], "title": "Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings", "comment": null, "summary": "Non-Euclidean foundation models increasingly place representations in curved\nspaces such as hyperbolic geometry. We show that this geometry creates a\nboundary-driven asymmetry that backdoor triggers can exploit. Near the\nboundary, small input changes appear subtle to standard input-space detectors\nbut produce disproportionately large shifts in the model's representation\nspace. Our analysis formalizes this effect and also reveals a limitation for\ndefenses: methods that act by pulling points inward along the radius can\nsuppress such triggers, but only by sacrificing useful model sensitivity in\nthat same direction. Building on these insights, we propose a simple\ngeometry-adaptive trigger and evaluate it across tasks and architectures.\nEmpirically, attack success increases toward the boundary, whereas conventional\ndetectors weaken, mirroring the theoretical trends. Together, these results\nsurface a geometry-specific vulnerability in non-Euclidean models and offer\nanalysis-backed guidance for designing and understanding the limits of\ndefenses.", "AI": {"tldr": "Non-Euclidean models in curved spaces like hyperbolic geometry have boundary-driven asymmetry that makes them vulnerable to backdoor attacks, where small input changes near boundaries create large representation shifts that evade detection while maintaining attack success.", "motivation": "To understand and formalize the security vulnerabilities in non-Euclidean foundation models, particularly how the curved geometry creates asymmetric effects that backdoor triggers can exploit near boundaries.", "method": "Theoretical analysis of boundary-driven asymmetry in curved spaces, formalization of the geometric effects, and development of a geometry-adaptive trigger evaluated across various tasks and architectures.", "result": "Attack success increases toward boundaries while conventional detectors weaken, confirming theoretical predictions. Defenses that pull points inward can suppress triggers but sacrifice useful model sensitivity.", "conclusion": "Non-Euclidean models have geometry-specific vulnerabilities that require analysis-backed defenses, with boundary regions being particularly susceptible to backdoor attacks that exploit the asymmetric geometric effects."}}
{"id": "2510.06711", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06711", "abs": "https://arxiv.org/abs/2510.06711", "authors": ["Batu El", "Mert Yuksekgonul", "James Zou"], "title": "Inefficiencies of Meta Agents for Agent Design", "comment": null, "summary": "Recent works began to automate the design of agentic systems using\nmeta-agents that propose and iteratively refine new agent architectures. In\nthis paper, we examine three key challenges in a common class of meta-agents.\nFirst, we investigate how a meta-agent learns across iterations and find that\nsimply expanding the context with all previous agents, as proposed by previous\nworks, performs worse than ignoring prior designs entirely. We show that the\nperformance improves with an evolutionary approach. Second, although the\nmeta-agent designs multiple agents during training, it typically commits to a\nsingle agent at test time. We find that the designed agents have low behavioral\ndiversity, limiting the potential for their complementary use. Third, we assess\nwhen automated design is economically viable. We find that only in a few\ncases--specifically, two datasets--the overall cost of designing and deploying\nthe agents is lower than that of human-designed agents when deployed on over\n15,000 examples. In contrast, the performance gains for other datasets do not\njustify the design cost, regardless of scale.", "AI": {"tldr": "Meta-agents for automated agent design face three key challenges: poor learning across iterations (solved by evolutionary approach), low behavioral diversity limiting complementary use, and economic viability only in specific cases.", "motivation": "To examine key challenges in meta-agents that automate the design of agentic systems, particularly focusing on learning efficiency, behavioral diversity, and economic viability.", "method": "Analyzed three aspects: 1) compared context expansion vs evolutionary approaches for learning across iterations, 2) evaluated behavioral diversity of designed agents, 3) assessed economic viability by comparing design costs vs performance gains across datasets.", "result": "Evolutionary approach outperforms context expansion; designed agents have low behavioral diversity; automated design is only economically viable for 2 datasets when deployed on large scale (15,000+ examples).", "conclusion": "Current meta-agent approaches have significant limitations in learning efficiency, behavioral diversity, and economic viability, with automated design only justified in specific scenarios."}}
{"id": "2510.06645", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06645", "abs": "https://arxiv.org/abs/2510.06645", "authors": ["Zhiyuan Wei", "Xiaoxuan Yang", "Jing Sun", "Zijian Zhang"], "title": "Distilling Lightweight Language Models for C/C++ Vulnerabilities", "comment": "25 pages, 10 figures", "summary": "The increasing complexity of modern software systems exacerbates the\nprevalence of security vulnerabilities, posing risks of severe breaches and\nsubstantial economic loss. Consequently, robust code vulnerability detection is\nessential for software security. While Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in natural language processing, their\npotential for automated code vulnerability detection remains underexplored.\nThis paper presents FineSec, a novel framework that harnesses LLMs through\nknowledge distillation to enable efficient and precise vulnerability\nidentification in C/C++ codebases. FineSec utilizes knowledge distillation to\ntransfer expertise from large teacher models to compact student models,\nachieving high accuracy with minimal computational cost. By integrating data\npreparation, training, evaluation, and continuous learning into a unified,\nsingle-task workflow, FineSec offers a streamlined approach. Extensive\nevaluations on C/C++ codebases demonstrate its superiority over both base\nmodels and larger LLMs in identifying complex vulnerabilities and logical\nflaws, establishing FineSec as a practical and scalable solution for real-world\nsoftware security. To facilitate reproducibility, the datasets, source code,\nand experimental results are made publicly available at:\nhttps://github.com/yangxiaoxuan123/FineSec_detect.", "AI": {"tldr": "FineSec is a framework that uses knowledge distillation from large language models to efficiently detect vulnerabilities in C/C++ code with high accuracy and low computational cost.", "motivation": "The increasing complexity of software systems leads to more security vulnerabilities, requiring robust detection methods. LLMs show promise but their potential for automated code vulnerability detection is underexplored.", "method": "Uses knowledge distillation to transfer expertise from large teacher models to compact student models. Integrates data preparation, training, evaluation, and continuous learning into a unified single-task workflow.", "result": "Extensive evaluations on C/C++ codebases show superiority over both base models and larger LLMs in identifying complex vulnerabilities and logical flaws.", "conclusion": "FineSec establishes itself as a practical and scalable solution for real-world software security, with datasets, source code, and results publicly available for reproducibility."}}
{"id": "2510.06383", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06383", "abs": "https://arxiv.org/abs/2510.06383", "authors": ["Pierre Lison", "Mark Anderson"], "title": "Protecting De-identified Documents from Search-based Linkage Attacks", "comment": null, "summary": "While de-identification models can help conceal the identity of the\nindividual(s) mentioned in a document, they fail to address linkage risks,\ndefined as the potential to map the de-identified text back to its source. One\nstraightforward way to perform such linkages is to extract phrases from the\nde-identified document and then check their presence in the original dataset.\nThis paper presents a method to counter search-based linkage attacks while\npreserving the semantic integrity of the text. The method proceeds in two\nsteps. We first construct an inverted index of the N-grams occurring in the\ndocument collection, making it possible to efficiently determine which N-grams\nappear in less than $k$ documents (either alone or in combination with other\nN-grams). An LLM-based rewriter is then iteratively queried to reformulate\nthose spans until linkage is no longer possible. Experimental results on a\ncollection of court cases show that the method is able to effectively prevent\nsearch-based linkages while remaining faithful to the original content.", "AI": {"tldr": "This paper presents a method to prevent search-based linkage attacks on de-identified documents by identifying rare N-grams and using LLM-based rewriting to eliminate linkage risks while preserving semantic integrity.", "motivation": "Current de-identification models fail to address linkage risks, where de-identified text can be mapped back to its original source through search-based attacks using extracted phrases.", "method": "Two-step approach: 1) Build inverted index of N-grams to identify those appearing in fewer than k documents, 2) Use iterative LLM-based rewriting to reformulate these rare spans until linkage is impossible.", "result": "Experimental evaluation on court case documents shows the method effectively prevents search-based linkages while maintaining faithfulness to original content.", "conclusion": "The proposed approach successfully counters search-based linkage attacks by systematically identifying and rewriting rare N-grams, providing enhanced privacy protection beyond basic de-identification."}}
{"id": "2510.06401", "categories": ["cs.LG", "cs.IT", "cs.NE", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06401", "abs": "https://arxiv.org/abs/2510.06401", "authors": ["Ali Hussaini Umar", "Franky Kevin Nando Tezoh", "Jean Barbier", "Santiago Acevedo", "Alessandro Laio"], "title": "The Effect of Label Noise on the Information Content of Neural Representations", "comment": "10 pages, 5 figures", "summary": "In supervised classification tasks, models are trained to predict a label for\neach data point. In real-world datasets, these labels are often noisy due to\nannotation errors. While the impact of label noise on the performance of deep\nlearning models has been widely studied, its effects on the networks' hidden\nrepresentations remain poorly understood. We address this gap by systematically\ncomparing hidden representations using the Information Imbalance, a\ncomputationally efficient proxy of conditional mutual information. Through this\nanalysis, we observe that the information content of the hidden representations\nfollows a double descent as a function of the number of network parameters,\nakin to the behavior of the test error. We further demonstrate that in the\nunderparameterized regime, representations learned with noisy labels are more\ninformative than those learned with clean labels, while in the\noverparameterized regime, these representations are equally informative. Our\nresults indicate that the representations of overparameterized networks are\nrobust to label noise. We also found that the information imbalance between the\npenultimate and pre-softmax layers decreases with cross-entropy loss in the\noverparameterized regime. This offers a new perspective on understanding\ngeneralization in classification tasks. Extending our analysis to\nrepresentations learned from random labels, we show that these perform worse\nthan random features. This indicates that training on random labels drives\nnetworks much beyond lazy learning, as weights adapt to encode labels\ninformation.", "AI": {"tldr": "This paper studies how label noise affects neural network hidden representations using Information Imbalance analysis, revealing double descent behavior and showing overparameterized networks are robust to label noise.", "motivation": "While label noise impact on model performance is well-studied, its effects on hidden representations remain poorly understood, creating a research gap.", "method": "Systematically compare hidden representations using Information Imbalance, a computationally efficient proxy of conditional mutual information, across different parameterization regimes.", "result": "Hidden representations show double descent behavior similar to test error. Underparameterized networks with noisy labels produce more informative representations than clean labels, while overparameterized networks show equal informativeness and robustness to label noise.", "conclusion": "Overparameterized networks are robust to label noise, and the information imbalance between layers decreases with cross-entropy loss, offering new insights into generalization. Training on random labels drives networks beyond lazy learning as weights adapt to encode label information."}}
{"id": "2510.06742", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06742", "abs": "https://arxiv.org/abs/2510.06742", "authors": ["Ali Sarabadani", "Kheirolah Rahsepar Fard"], "title": "MultiCNKG: Integrating Cognitive Neuroscience, Gene, and Disease Knowledge Graphs Using Large Language Models", "comment": null, "summary": "The advent of large language models (LLMs) has revolutionized the integration\nof knowledge graphs (KGs) in biomedical and cognitive sciences, overcoming\nlimitations in traditional machine learning methods for capturing intricate\nsemantic links among genes, diseases, and cognitive processes. We introduce\nMultiCNKG, an innovative framework that merges three key knowledge sources: the\nCognitive Neuroscience Knowledge Graph (CNKG) with 2.9K nodes and 4.3K edges\nacross 9 node types and 20 edge types; Gene Ontology (GO) featuring 43K nodes\nand 75K edges in 3 node types and 4 edge types; and Disease Ontology (DO)\ncomprising 11.2K nodes and 8.8K edges with 1 node type and 2 edge types.\nLeveraging LLMs like GPT-4, we conduct entity alignment, semantic similarity\ncomputation, and graph augmentation to create a cohesive KG that interconnects\ngenetic mechanisms, neurological disorders, and cognitive functions. The\nresulting MultiCNKG encompasses 6.9K nodes across 5 types (e.g., Genes,\nDiseases, Cognitive Processes) and 11.3K edges spanning 7 types (e.g., Causes,\nAssociated with, Regulates), facilitating a multi-layered view from molecular\nto behavioral domains. Assessments using metrics such as precision (85.20%),\nrecall (87.30%), coverage (92.18%), graph consistency (82.50%), novelty\ndetection (40.28%), and expert validation (89.50%) affirm its robustness and\ncoherence. Link prediction evaluations with models like TransE (MR: 391, MRR:\n0.411) and RotatE (MR: 263, MRR: 0.395) show competitive performance against\nbenchmarks like FB15k-237 and WN18RR. This KG advances applications in\npersonalized medicine, cognitive disorder diagnostics, and hypothesis\nformulation in cognitive neuroscience.", "AI": {"tldr": "MultiCNKG is a novel framework that integrates cognitive neuroscience, gene ontology, and disease ontology knowledge graphs using LLMs, creating a unified knowledge graph with high precision and recall for biomedical applications.", "motivation": "To overcome limitations of traditional machine learning in capturing complex semantic relationships between genes, diseases, and cognitive processes by leveraging large language models for knowledge graph integration.", "method": "Integrates three knowledge sources (CNKG, GO, DO) using LLMs like GPT-4 for entity alignment, semantic similarity computation, and graph augmentation to create a unified knowledge graph with multiple node and edge types.", "result": "Created MultiCNKG with 6.9K nodes and 11.3K edges, achieving high metrics: precision (85.20%), recall (87.30%), coverage (92.18%), and competitive link prediction performance with TransE (MR: 391, MRR: 0.411) and RotatE (MR: 263, MRR: 0.395).", "conclusion": "MultiCNKG provides a robust framework for connecting molecular to behavioral domains, advancing applications in personalized medicine, cognitive disorder diagnostics, and cognitive neuroscience research."}}
{"id": "2510.06719", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06719", "abs": "https://arxiv.org/abs/2510.06719", "authors": ["Junki Mori", "Kazuya Kakizaki", "Taiki Miyagawa", "Jun Sakuma"], "title": "Differentially Private Synthetic Text Generation for Retrieval-Augmented Generation (RAG)", "comment": "Under review", "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\ngrounding them in external knowledge. However, its application in sensitive\ndomains is limited by privacy risks. Existing private RAG methods typically\nrely on query-time differential privacy (DP), which requires repeated noise\ninjection and leads to accumulated privacy loss. To address this issue, we\npropose DP-SynRAG, a framework that uses LLMs to generate differentially\nprivate synthetic RAG databases. Unlike prior methods, the synthetic text can\nbe reused once created, thereby avoiding repeated noise injection and\nadditional privacy costs. To preserve essential information for downstream RAG\ntasks, DP-SynRAG extends private prediction, which instructs LLMs to generate\ntext that mimics subsampled database records in a DP manner. Experiments show\nthat DP-SynRAG achieves superior performanec to the state-of-the-art private\nRAG systems while maintaining a fixed privacy budget, offering a scalable\nsolution for privacy-preserving RAG.", "AI": {"tldr": "DP-SynRAG is a privacy-preserving RAG framework that generates differentially private synthetic databases using LLMs, avoiding repeated noise injection and enabling reusable synthetic text with fixed privacy budget.", "motivation": "Existing private RAG methods rely on query-time differential privacy, which requires repeated noise injection and leads to accumulated privacy loss, limiting their application in sensitive domains.", "method": "DP-SynRAG extends private prediction by using LLMs to generate differentially private synthetic RAG databases that mimic subsampled database records, allowing the synthetic text to be reused without additional privacy costs.", "result": "Experiments show DP-SynRAG achieves superior performance compared to state-of-the-art private RAG systems while maintaining a fixed privacy budget.", "conclusion": "DP-SynRAG offers a scalable solution for privacy-preserving RAG by generating reusable differentially private synthetic databases, avoiding the privacy accumulation issues of query-time DP methods."}}
{"id": "2510.06386", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06386", "abs": "https://arxiv.org/abs/2510.06386", "authors": ["Fan Zhou", "Chang Tian", "Tim Van de Cruys"], "title": "Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion", "comment": "Preprint under review", "summary": "Generating stylistic text with specific attributes is a key problem in\ncontrollable text generation. Recently, diffusion models have emerged as a\npowerful paradigm for both visual and textual generation. Existing approaches\ncan be broadly categorized into classifier-free guidance (CFG) and classifier\nguidance (CG) methods. While CFG effectively preserves semantic content, it\noften fails to provide effective attribute control. In contrast, CG modifies\nthe denoising trajectory using classifier gradients, enabling better attribute\nalignment but incurring high computational costs during sampling and suffering\nfrom classifier generalization issues. In this work, we propose RegDiff, a\nregularized diffusion framework that leverages attribute features without\nrequiring a pretrained classifier during sampling, thereby achieving\ncontrollable generation with reduced computational costs. Specifically, RegDiff\nemploys a VAE-based encoder--decoder architecture to ensure reconstruction\nfidelity and a latent diffusion model trained with attribute supervision to\nenable controllable text generation. Attribute information is injected only\nduring training. Experiments on five datasets spanning multiple stylistic\nattributes demonstrate that RegDiff outperforms strong baselines in generating\nstylistic texts. These results validate the effectiveness of RegDiff as an\nefficient solution for attribute-controllable text diffusion. Our code,\ndatasets, and resources will be released upon publication at\nhttps://github.com/xxxx.", "AI": {"tldr": "RegDiff is a regularized diffusion framework for controllable text generation that uses attribute features without needing pretrained classifiers during sampling, achieving better attribute control with lower computational costs.", "motivation": "Existing diffusion methods for text generation have limitations: classifier-free guidance preserves semantics but lacks effective attribute control, while classifier guidance enables better attribute alignment but has high computational costs and classifier generalization issues.", "method": "RegDiff employs a VAE-based encoder-decoder architecture for reconstruction fidelity and a latent diffusion model trained with attribute supervision. Attribute information is injected only during training, eliminating the need for classifiers during sampling.", "result": "Experiments on five datasets across multiple stylistic attributes show that RegDiff outperforms strong baselines in generating stylistic texts.", "conclusion": "RegDiff provides an efficient solution for attribute-controllable text diffusion by leveraging attribute features during training without requiring classifiers during sampling, achieving both computational efficiency and effective attribute control."}}
{"id": "2510.06419", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06419", "abs": "https://arxiv.org/abs/2510.06419", "authors": ["Mert Kayaalp", "Caner Turkmen", "Oleksandr Shchur", "Pedro Mercado", "Abdul Fatir Ansari", "Michael Bohlke-Schneider", "Bernie Wang"], "title": "Test-Time Efficient Pretrained Model Portfolios for Time Series Forecasting", "comment": null, "summary": "Is bigger always better for time series foundation models? With the question\nin mind, we explore an alternative to training a single, large monolithic\nmodel: building a portfolio of smaller, pretrained forecasting models. By\napplying ensembling or model selection over these portfolios, we achieve\ncompetitive performance on large-scale benchmarks using much fewer parameters.\nWe explore strategies for designing such portfolios and find that collections\nof specialist models consistently outperform portfolios of independently\ntrained generalists. Remarkably, we demonstrate that post-training a base model\nis a compute-effective approach for creating sufficiently diverse specialists,\nand provide evidences that ensembling and model selection are more\ncompute-efficient than test-time fine-tuning.", "AI": {"tldr": "Building portfolios of smaller pretrained forecasting models instead of single large models achieves competitive performance with fewer parameters through ensembling and model selection.", "motivation": "To explore whether bigger models are always better for time series foundation models, and investigate alternatives to training large monolithic models.", "method": "Create portfolios of smaller pretrained forecasting models, apply ensembling or model selection strategies, and design portfolios using specialist models created through post-training base models.", "result": "Collections of specialist models consistently outperform portfolios of independently trained generalists, achieving competitive performance on large-scale benchmarks with fewer parameters.", "conclusion": "Ensembling and model selection over portfolios of smaller specialist models are more compute-efficient than test-time fine-tuning and provide a viable alternative to large monolithic time series foundation models."}}
{"id": "2510.06756", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06756", "abs": "https://arxiv.org/abs/2510.06756", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb"], "title": "Verifying Memoryless Sequential Decision-making of Large Language Models", "comment": null, "summary": "We introduce a tool for rigorous and automated verification of large language\nmodel (LLM)- based policies in memoryless sequential decision-making tasks.\nGiven a Markov decision process (MDP) representing the sequential\ndecision-making task, an LLM policy, and a safety requirement expressed as a\nPCTL formula, our approach incrementally constructs only the reachable portion\nof the MDP guided by the LLM's chosen actions. Each state is encoded as a\nnatural language prompt, the LLM's response is parsed into an action, and\nreachable successor states by the policy are expanded. The resulting formal\nmodel is checked with Storm to determine whether the policy satisfies the\nspecified safety property. In experiments on standard grid world benchmarks, we\nshow that open source LLMs accessed via Ollama can be verified when\ndeterministically seeded, but generally underperform deep reinforcement\nlearning baselines. Our tool natively integrates with Ollama and supports\nPRISM-specified tasks, enabling continuous benchmarking in user-specified\nsequential decision-making tasks and laying a practical foundation for formally\nverifying increasingly capable LLMs.", "AI": {"tldr": "A tool for automated verification of LLM-based policies in sequential decision-making tasks using MDPs and PCTL safety requirements.", "motivation": "To enable rigorous and automated verification of large language model policies in memoryless sequential decision-making environments to ensure safety compliance.", "method": "Incrementally constructs reachable MDP states guided by LLM actions, encodes states as natural language prompts, parses LLM responses into actions, and verifies the resulting model with Storm model checker.", "result": "Open source LLMs via Ollama can be verified when deterministically seeded but generally underperform deep reinforcement learning baselines in grid world benchmarks.", "conclusion": "The tool provides practical foundation for formally verifying increasingly capable LLMs in sequential decision-making tasks with native Ollama integration and PRISM task specification support."}}
{"id": "2510.06784", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06784", "abs": "https://arxiv.org/abs/2510.06784", "authors": ["Dmytro Zakharov", "Oleksandr Kurbatov", "Artem Sdobnov", "Lev Soukhanov", "Yevhenii Sekhin", "Vitalii Volovyk", "Mykhailo Velykodnyi", "Mark Cherepovskyi", "Kyrylo Baibula", "Lasha Antadze", "Pavlo Kravchenko", "Volodymyr Dubinin", "Yaroslav Panasenko"], "title": "Bionetta: Efficient Client-Side Zero-Knowledge Machine Learning Proving", "comment": null, "summary": "In this report, we compare the performance of our UltraGroth-based\nzero-knowledge machine learning framework Bionetta to other tools of similar\npurpose such as EZKL, Lagrange's deep-prove, or zkml. The results show a\nsignificant boost in the proving time for custom-crafted neural networks: they\ncan be proven even on mobile devices, enabling numerous client-side proving\napplications. While our scheme increases the cost of one-time preprocessing\nsteps, such as circuit compilation and generating trusted setup, our approach\nis, to the best of our knowledge, the only one that is deployable on the native\nEVM smart contracts without overwhelming proof size and verification overheads.", "AI": {"tldr": "Bionetta framework outperforms similar tools like EZKL, Lagrange's deep-prove, and zkml in proving time for neural networks, enabling mobile device deployment with EVM compatibility.", "motivation": "To enable client-side proving applications on mobile devices and native EVM smart contracts without overwhelming proof size and verification overheads.", "method": "Uses UltraGroth-based zero-knowledge machine learning framework with custom-crafted neural networks, accepting increased preprocessing costs for better deployment capabilities.", "result": "Significant boost in proving time for neural networks, making them provable on mobile devices while maintaining EVM compatibility with manageable proof size.", "conclusion": "Bionetta is the only currently deployable solution for native EVM smart contracts with practical proof size and verification costs, despite higher preprocessing requirements."}}
{"id": "2510.06391", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06391", "abs": "https://arxiv.org/abs/2510.06391", "authors": ["Elle"], "title": "Reward Model Perspectives: Whose Opinions Do Reward Models Reward?", "comment": "Published at EMNLP 2025 under the full author name \"Elle\"", "summary": "Reward models (RMs) are central to the alignment of language models (LMs). An\nRM often serves as a proxy for human preferences to guide downstream LM\nbehavior. However, our understanding of RM behavior is limited. Our work (i)\nformalizes a framework for measuring the alignment of opinions captured by RMs,\n(ii) investigates the extent to which RMs demonstrate sociodemographic biases,\nand (iii) explores the effects of prompting to steer rewards towards the\npreferences of a target group. We study the subjective and diverse perspectives\non controversial topics, which allows us to quantify RM perspectives in terms\nof their opinions, attitudes, and values. We show that RMs are poorly aligned\nwith several demographic groups and can systematically reward harmful\nstereotypes, and steering alone is not enough to overcome these limitations.\nOur findings underscore the need for more careful consideration of RM behavior\nin model alignment during preference learning to prevent the propagation of\nunwanted social biases in the language technologies that we use.", "AI": {"tldr": "This paper analyzes reward models (RMs) in language model alignment, finding they exhibit sociodemographic biases, poorly align with various demographic groups, and systematically reward harmful stereotypes. Steering techniques alone are insufficient to overcome these limitations.", "motivation": "To understand reward model behavior and investigate the extent to which RMs demonstrate sociodemographic biases, as RMs serve as proxies for human preferences but our understanding of their behavior is limited.", "method": "The authors formalize a framework for measuring RM opinion alignment, investigate sociodemographic biases in RMs, and explore the effects of prompting to steer rewards toward target group preferences, focusing on controversial topics to quantify RM perspectives.", "result": "RMs are poorly aligned with several demographic groups, systematically reward harmful stereotypes, and steering alone is insufficient to overcome these limitations.", "conclusion": "More careful consideration of RM behavior in model alignment during preference learning is needed to prevent propagation of unwanted social biases in language technologies."}}
{"id": "2510.06434", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06434", "abs": "https://arxiv.org/abs/2510.06434", "authors": ["Eliot Shekhtman", "Yichen Zhou", "Ingvar Ziemann", "Nikolai Matni", "Stephen Tu"], "title": "Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization", "comment": null, "summary": "Learning from temporally-correlated data is a core facet of modern machine\nlearning. Yet our understanding of sequential learning remains incomplete,\nparticularly in the multi-trajectory setting where data consists of many\nindependent realizations of a time-indexed stochastic process. This important\nregime both reflects modern training pipelines such as for large foundation\nmodels, and offers the potential for learning without the typical mixing\nassumptions made in the single-trajectory case. However, instance-optimal\nbounds are known only for least-squares regression with dependent covariates;\nfor more general models or loss functions, the only broadly applicable\nguarantees result from a reduction to either i.i.d. learning, with effective\nsample size scaling only in the number of trajectories, or an existing\nsingle-trajectory result when each individual trajectory mixes, with effective\nsample size scaling as the full data budget deflated by the mixing-time.\n  In this work, we significantly broaden the scope of instance-optimal rates in\nmulti-trajectory settings via the Hellinger localization framework, a general\napproach for maximum likelihood estimation. Our method proceeds by first\ncontrolling the squared Hellinger distance at the path-measure level via a\nreduction to i.i.d. learning, followed by localization as a quadratic form in\nparameter space weighted by the trajectory Fisher information. This yields\ninstance-optimal bounds that scale with the full data budget under a broad set\nof conditions. We instantiate our framework across four diverse case studies: a\nsimple mixture of Markov chains, dependent linear regression under non-Gaussian\nnoise, generalized linear models with non-monotonic activations, and\nlinear-attention sequence models. In all cases, our bounds nearly match the\ninstance-optimal rates from asymptotic normality, substantially improving over\nstandard reductions.", "AI": {"tldr": "This paper presents a new framework for instance-optimal learning from multi-trajectory data using Hellinger localization, achieving near-optimal rates across diverse models without requiring mixing assumptions.", "motivation": "Current understanding of sequential learning is incomplete, especially for multi-trajectory data where existing methods either reduce to i.i.d. learning with limited sample size scaling or require mixing assumptions. There's a need for broader instance-optimal guarantees.", "method": "The Hellinger localization framework controls squared Hellinger distance at path-measure level via reduction to i.i.d. learning, followed by localization as a quadratic form in parameter space weighted by trajectory Fisher information.", "result": "The framework yields instance-optimal bounds scaling with full data budget under broad conditions, demonstrated across four case studies: mixture of Markov chains, dependent linear regression, generalized linear models, and linear-attention sequence models.", "conclusion": "The proposed method significantly broadens instance-optimal rates in multi-trajectory settings, nearly matching asymptotic normality rates and substantially improving over standard reductions across diverse models."}}
{"id": "2510.06761", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06761", "abs": "https://arxiv.org/abs/2510.06761", "authors": ["Zhi Zhang", "Yan Liu", "Zhejing Hu", "Gong Chen", "Sheng-hua Zhong", "Jiannong Cao"], "title": "Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration", "comment": null, "summary": "Automating the end-to-end scientific research process poses a fundamental\nchallenge: it requires both evolving high-level plans that are novel and sound,\nand executing these plans correctly amidst dynamic and uncertain conditions. To\naddress this bilevel challenge, we propose a novel Double-Loop Multi-Agent\n(DLMA) framework to solve the given research problem automatically. The leader\nloop, composed of professor agents, is responsible for evolving research plans.\nIt employs an evolutionary algorithm through involvement, improvement, and\nintegration meetings to iteratively generate and refine a pool of research\nproposals, exploring the solution space effectively. The follower loop,\ncomposed of doctoral student agents, is responsible for executing the\nbest-evolved plan. It dynamically adjusts the plan during implementation via\npre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is\nwell-supported by contextual and external observations. Extensive experiments\non benchmarks like ACLAward and Laboratory show that DLMA generates research\npapers that achieve state-of-the-art scores in automated evaluation,\nsignificantly outperforming strong baselines. Ablation studies confirm the\ncritical roles of both loops, with evolution driving novelty and execution\nensuring soundness.", "AI": {"tldr": "DLMA framework automates scientific research using double-loop multi-agent system: leader loop evolves novel research plans, follower loop executes plans dynamically.", "motivation": "Automating end-to-end scientific research requires both novel high-level planning and robust execution under uncertainty, presenting a bilevel challenge.", "method": "Double-loop multi-agent framework with professor agents evolving plans via evolutionary algorithm meetings, and doctoral student agents executing plans with dynamic adjustments.", "result": "DLMA achieves state-of-the-art scores on ACLAward and Laboratory benchmarks, significantly outperforming baselines in automated evaluation.", "conclusion": "Both evolutionary planning and dynamic execution loops are critical - evolution drives novelty while execution ensures soundness in automated scientific research."}}
{"id": "2510.06823", "categories": ["cs.CR", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06823", "abs": "https://arxiv.org/abs/2510.06823", "authors": ["Riku Mochizuki", "Shusuke Komatsu", "Souta Noguchi", "Kazuto Ataka"], "title": "Exposing Citation Vulnerabilities in Generative Engines", "comment": "12 pages, under-reviewing at a conference", "summary": "We analyze answers generated by generative engines (GEs) from the\nperspectives of citation publishers and the content-injection barrier, defined\nas the difficulty for attackers to manipulate answers to user prompts by\nplacing malicious content on the web. GEs integrate two functions: web search\nand answer generation that cites web pages using large language models. Because\nanyone can publish information on the web, GEs are vulnerable to poisoning\nattacks. Existing studies of citation evaluation focus on how faithfully answer\ncontent reflects cited sources, leaving unexamined which web sources should be\nselected as citations to defend against poisoning attacks. To fill this gap, we\nintroduce evaluation criteria that assess poisoning threats using the citation\ninformation contained in answers. Our criteria classify the publisher\nattributes of citations to estimate the content-injection barrier thereby\nrevealing the threat of poisoning attacks in current GEs. We conduct\nexperiments in political domains in Japan and the United States (U.S.) using\nour criteria and show that citations from official party websites (primary\nsources) are approximately \\(25\\%\\)--\\(45\\%\\) in the U.S. and\n\\(60\\%\\)--\\(65\\%\\) in Japan, indicating that U.S. political answers are at\nhigher risk of poisoning attacks. We also find that sources with low\ncontent-injection barriers are frequently cited yet are poorly reflected in\nanswer content. To mitigate this threat, we discuss how publishers of primary\nsources can increase exposure of their web content in answers and show that\nwell-known techniques are limited by language differences.", "AI": {"tldr": "This paper analyzes generative engines' vulnerability to poisoning attacks through web citations, introduces evaluation criteria to assess poisoning threats, and shows US political answers have higher poisoning risk than Japan.", "motivation": "Generative engines are vulnerable to poisoning attacks since anyone can publish web content, but existing citation evaluation focuses on faithfulness rather than source selection for defense.", "method": "Introduce evaluation criteria to classify citation publisher attributes and estimate content-injection barrier, then conduct experiments in political domains in Japan and US.", "result": "Citations from official party websites are 25%-45% in US vs 60%-65% in Japan, indicating higher poisoning risk in US; low-barrier sources are frequently cited but poorly reflected in answers.", "conclusion": "Primary source publishers need better exposure strategies, but existing techniques are limited by language differences; poisoning threats exist in current generative engines."}}
{"id": "2510.06411", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06411", "abs": "https://arxiv.org/abs/2510.06411", "authors": ["R. Alexander Knipper", "Indrani Dey", "Souvika Sarkar", "Hari Narayanan", "Sadhana Puntambekar", "Santu Karmaker"], "title": "Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?", "comment": null, "summary": "Virtual Labs offer valuable opportunities for hands-on, inquiry-based science\nlearning, yet teachers often struggle to adapt them to fit their instructional\ngoals. Third-party materials may not align with classroom needs, and developing\ncustom resources can be time-consuming and difficult to scale. Recent advances\nin Large Language Models (LLMs) offer a promising avenue for addressing these\nlimitations. In this paper, we introduce a novel alignment framework for\ninstructional goal-aligned question generation, enabling teachers to leverage\nLLMs to produce simulation-aligned, pedagogically meaningful questions through\nnatural language interaction. The framework integrates four components:\ninstructional goal understanding via teacher-LLM dialogue, lab understanding\nvia knowledge unit and relationship analysis, a question taxonomy for\nstructuring cognitive and pedagogical intent, and the TELeR taxonomy for\ncontrolling prompt detail. Early design choices were informed by a small\nteacher-assisted case study, while our final evaluation analyzed over 1,100\nquestions from 19 open-source LLMs. With goal and lab understanding grounding\nquestions in teacher intent and simulation context, the question taxonomy\nelevates cognitive demand (open-ended formats and relational types raise\nquality by 0.29-0.39 points), and optimized TELeR prompts enhance format\nadherence (80% parsability, >90% adherence). Larger models yield the strongest\ngains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert\npoints.", "AI": {"tldr": "A framework using LLMs to help teachers generate instructional goal-aligned questions for virtual labs through natural language interaction, improving question quality and format adherence.", "motivation": "Teachers struggle to adapt virtual labs to their instructional goals, and developing custom resources is time-consuming. LLMs offer a promising solution to address these limitations.", "method": "An alignment framework with four components: instructional goal understanding via teacher-LLM dialogue, lab understanding via knowledge analysis, question taxonomy for structuring intent, and TELeR taxonomy for prompt control.", "result": "The framework improved cognitive demand (0.29-0.39 quality points increase), achieved 80% parsability and >90% format adherence. Larger models showed strongest gains: +37.1% parsability, +25.7% adherence, and +0.8 quality points.", "conclusion": "The proposed framework successfully enables teachers to leverage LLMs for generating pedagogically meaningful, simulation-aligned questions through natural language interaction, with larger models yielding the best performance."}}
{"id": "2510.06439", "categories": ["cs.LG", "cs.CE", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06439", "abs": "https://arxiv.org/abs/2510.06439", "authors": ["Akash Yadav", "Ruda Zhang"], "title": "Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models", "comment": null, "summary": "Hyperparameter tuning is a challenging problem especially when the system\nitself involves uncertainty. Due to noisy function evaluations, optimization\nunder uncertainty can be computationally expensive. In this paper, we present a\nnovel Bayesian optimization framework tailored for hyperparameter tuning under\nuncertainty, with a focus on optimizing a scale- or precision-type parameter in\nstochastic models. The proposed method employs a statistical surrogate for the\nunderlying random variable, enabling analytical evaluation of the expectation\noperator. Moreover, we derive a closed-form expression for the optimizer of the\nrandom acquisition function, which significantly reduces computational cost per\niteration. Compared with a conventional one-dimensional Monte Carlo-based\noptimization scheme, the proposed approach requires 40 times fewer data points,\nresulting in up to a 40-fold reduction in computational cost. We demonstrate\nthe effectiveness of the proposed method through two numerical examples in\ncomputational engineering.", "AI": {"tldr": "A novel Bayesian optimization framework for hyperparameter tuning under uncertainty that reduces computational cost by 40x compared to conventional Monte Carlo methods.", "motivation": "Hyperparameter tuning is challenging under uncertainty due to noisy function evaluations, making optimization computationally expensive.", "method": "Uses Bayesian optimization with statistical surrogate for random variables, enabling analytical evaluation of expectations and closed-form optimizer for acquisition functions.", "result": "Requires 40 times fewer data points than conventional Monte Carlo methods, achieving up to 40-fold reduction in computational cost.", "conclusion": "The proposed method is effective for hyperparameter tuning under uncertainty, as demonstrated through computational engineering examples."}}
{"id": "2510.06857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06857", "abs": "https://arxiv.org/abs/2510.06857", "authors": ["Qi Guo", "Jianing Wang", "Jianfei Zhang", "Deyang Kong", "Xiangzhou Huang", "Xiangyu Xi", "Wei Wang", "Jingang Wang", "Xunliang Cai", "Shikun Zhang", "Wei Ye"], "title": "Autoformalizer with Tool Feedback", "comment": null, "summary": "Autoformalization addresses the scarcity of data for Automated Theorem\nProving (ATP) by translating mathematical problems from natural language into\nformal statements. Efforts in recent work shift from directly prompting large\nlanguage models to training an end-to-end formalizer model from scratch,\nachieving remarkable advancements. However, existing formalizer still struggles\nto consistently generate valid statements that meet syntactic validity and\nsemantic consistency. To address this issue, we propose the Autoformalizer with\nTool Feedback (ATF), a novel approach that incorporates syntactic and\nconsistency information as tools into the formalization process. By integrating\nLean 4 compilers for syntax corrections and employing a multi-LLMs-as-judge\napproach for consistency validation, the model is able to adaptively refine\ngenerated statements according to the tool feedback, enhancing both syntactic\nvalidity and semantic consistency. The training of ATF involves a cold-start\nphase on synthetic tool-calling data, an expert iteration phase to improve\nformalization capabilities, and Direct Preference Optimization to alleviate\nineffective revisions. Experimental results show that ATF markedly outperforms\na range of baseline formalizer models, with its superior performance further\nvalidated by human evaluations. Subsequent analysis reveals that ATF\ndemonstrates excellent inference scaling properties. Moreover, we open-source\nNumina-ATF, a dataset containing 750K synthetic formal statements to facilitate\nadvancements in autoformalization and ATP research.", "AI": {"tldr": "ATF is an autoformalization approach that integrates tool feedback (syntax correction and consistency validation) to improve generation of valid formal statements, outperforming existing methods.", "motivation": "Existing autoformalizers struggle with consistently generating syntactically valid and semantically consistent formal statements from natural language math problems.", "method": "Proposes ATF with Lean 4 compilers for syntax correction and multi-LLMs-as-judge for consistency validation, using cold-start training, expert iteration, and Direct Preference Optimization.", "result": "ATF markedly outperforms baseline formalizer models, shows excellent inference scaling, and includes open-source dataset Numina-ATF with 750K synthetic formal statements.", "conclusion": "ATF effectively addresses syntactic and semantic challenges in autoformalization through tool feedback integration, advancing ATP research with improved performance and open resources."}}
{"id": "2510.06951", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06951", "abs": "https://arxiv.org/abs/2510.06951", "authors": ["Philip Huff", "Nishka Gandu", "Pavel Nov\u00e1k"], "title": "I Can't Patch My OT Systems! A Look at CISA's KEVC Workarounds & Mitigations for OT", "comment": "8 pages, 6 figures. Supported by DOE Grant CR0000031", "summary": "We examine the state of publicly available information about known\nexploitable vulnerabilities applicable to operational technology (OT)\nenvironments. Specifically, we analyze the Known Exploitable Vulnerabilities\nCatalog (KEVC) maintained by the US Department of Homeland Security\nCybersecurity and Infrastructure Security Agency (CISA) to assess whether\ncurrently available data is sufficient for effective and reliable remediation\nin OT settings. Our team analyzed all KEVC entries through July 2025 to\ndetermine the extent to which OT environments can rely on existing remediation\nrecommendations. We found that although most entries in the KEVC could affect\nOT environments, only 13% include vendor workarounds or mitigations as\nalternatives to patching. This paper also examines the feasibility of\ndeveloping such alternatives based on vulnerability and exploit\ncharacteristics, and we present early evidence of success with this approach.", "AI": {"tldr": "Analysis of CISA's Known Exploitable Vulnerabilities Catalog reveals that while most vulnerabilities affect OT environments, only 13% provide vendor workarounds or mitigations as alternatives to patching, highlighting a significant gap in remediation options for operational technology.", "motivation": "To assess whether current publicly available information about known exploitable vulnerabilities is sufficient for effective and reliable remediation in operational technology (OT) environments, given the unique constraints and requirements of OT systems.", "method": "Analyzed all entries in CISA's Known Exploitable Vulnerabilities Catalog (KEVC) through July 2025 to determine the extent to which OT environments can rely on existing remediation recommendations and examined the feasibility of developing workarounds based on vulnerability characteristics.", "result": "Found that although most KEVC entries could affect OT environments, only 13% include vendor workarounds or mitigations as alternatives to patching, indicating a significant deficiency in available remediation options for OT settings.", "conclusion": "There is a critical need for more comprehensive remediation guidance specifically tailored for OT environments, and early evidence suggests that developing alternative mitigation strategies based on vulnerability characteristics is a feasible approach to address this gap."}}
{"id": "2510.06426", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06426", "abs": "https://arxiv.org/abs/2510.06426", "authors": ["Yitao Long", "Tiansheng Hu", "Yilun Zhao", "Arman Cohan", "Chen Zhao"], "title": "FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering", "comment": "EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) frequently hallucinate to long-form questions,\nproducing plausible yet factually incorrect answers. A common mitigation\nstrategy is to provide attribution to LLM outputs. However, existing benchmarks\nprimarily focus on simple attribution that retrieves supporting textual\nevidence as references. We argue that in real-world scenarios such as financial\napplications, attribution goes beyond reference retrieval. We introduce\nFinLFQA, a benchmark designed to evaluate the ability of LLMs to generate\nlong-form answers to complex financial questions with reliable and nuanced\nattributions. FinLFQA evaluates three critical aspects of attribution through\nhuman annotations: (1) supporting evidence extracted from financial reports,\n(2) intermediate numerical reasoning steps, and (3) domain-specific financial\nknowledge that informs the reasoning process. We further provide an automatic\nevaluation framework covering both answer quality and attribution quality.\nThrough extensive experiments on eight LLMs across multiple\nattribution-generation paradigms, we find that fine-grained metrics are\nimportant to distinguish model capabilities, that end-to-end generation\nachieves comparable performance to post-hoc approaches, and that iterative\nrefinement only helps when guided by external feedback.", "AI": {"tldr": "FinLFQA is a benchmark for evaluating LLMs' ability to generate long-form financial answers with reliable attribution, assessing evidence extraction, numerical reasoning, and domain knowledge.", "motivation": "Existing benchmarks focus on simple attribution with textual evidence, but real-world financial applications require more nuanced attribution including numerical reasoning and domain knowledge.", "method": "Introduces FinLFQA benchmark with human annotations for three attribution aspects: supporting evidence from financial reports, intermediate numerical reasoning steps, and domain-specific financial knowledge. Provides automatic evaluation framework for answer and attribution quality.", "result": "Experiments on eight LLMs show fine-grained metrics distinguish model capabilities, end-to-end generation matches post-hoc approaches, and iterative refinement only helps with external feedback.", "conclusion": "Financial attribution requires comprehensive evaluation beyond simple reference retrieval, and the proposed benchmark enables better assessment of LLMs' attribution capabilities in complex domains."}}
{"id": "2510.06444", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.06444", "abs": "https://arxiv.org/abs/2510.06444", "authors": ["Joel Pfeffer", "J. M. Diederik Kruijssen", "Cl\u00e9ment Gossart", "M\u00e9lanie Chevance", "Diego Campo Millan", "Florian Stecker", "Steven N. Longmore"], "title": "Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks", "comment": "17 pages, 12 figures; appeared in ADI (October 2025)", "summary": "In decentralized learning networks, predictions from many participants are\ncombined to generate a network inference. While many studies have demonstrated\nperformance benefits of combining multiple model predictions, existing\nstrategies using linear pooling methods (ranging from simple averaging to\ndynamic weight updates) face a key limitation. Dynamic prediction combinations\nthat rely on historical performance to update weights are necessarily reactive.\nDue to the need to average over a reasonable number of epochs (with moving\naverages or exponential weighting), they tend to be slow to adjust to changing\ncircumstances (phase or regime changes). In this work, we develop a model that\nuses machine learning to forecast the performance of predictions by models at\neach epoch in a time series. This enables `context-awareness' by assigning\nhigher weight to models that are likely to be more accurate at a given time. We\nshow that adding a performance forecasting worker in a decentralized learning\nnetwork, following a design similar to the Allora network, can improve the\naccuracy of network inferences. Specifically, we find forecasting models that\npredict regret (performance relative to the network inference) or regret\nz-score (performance relative to other workers) show greater improvement than\nmodels predicting losses, which often do not outperform the naive network\ninference (historically weighted average of all inferences). Through a series\nof optimization tests, we show that the performance of the forecasting model\ncan be sensitive to choices in the feature set and number of training epochs.\nThese properties may depend on the exact problem and should be tailored to each\ndomain. Although initially designed for a decentralized learning network, using\nperformance forecasting for prediction combination may be useful in any\nsituation where predictive rather than reactive model weighting is needed.", "AI": {"tldr": "The paper proposes a performance forecasting model for decentralized learning networks that predicts model performance to enable proactive weight assignment, improving network inference accuracy compared to reactive linear pooling methods.", "motivation": "Existing dynamic prediction combination methods in decentralized learning are reactive and slow to adapt to changing circumstances due to their reliance on historical performance averaging. There's a need for more responsive, context-aware approaches.", "method": "Developed a machine learning model that forecasts performance of individual models at each epoch, enabling proactive weight assignment. Used forecasting of regret or regret z-score to identify models likely to perform better at specific times.", "result": "Performance forecasting models predicting regret or regret z-score showed greater improvement than loss-based models, which often didn't outperform naive network inference. The approach improved accuracy of network inferences in decentralized learning networks.", "conclusion": "Performance forecasting enables predictive rather than reactive model weighting, providing context-awareness in decentralized learning. The method's effectiveness depends on feature set choices and training epochs, requiring domain-specific tailoring. This approach has broader applications beyond decentralized learning networks."}}
{"id": "2510.06878", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06878", "abs": "https://arxiv.org/abs/2510.06878", "authors": ["Daria Ozerova", "Ekaterina Trofimova"], "title": "TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs", "comment": null, "summary": "Iterative refinement has been a promising paradigm to enable large language\nmodels (LLMs) to resolve difficult reasoning and problem-solving tasks. One of\nthe key challenges, however, is how to effectively search through the enormous\nsearch space of possible refinements. Existing methods typically fall back on\npredefined heuristics, which are troubled by the exploration-exploitation\ndilemma and cannot adapt based on past refinement outcomes. We introduce\nTree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with\na Thompson-Sampling-based tree search. TGPR explores both failed and successful\nrefinement paths actively, with denser training trajectories and more adaptive\npolicies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to\n+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to\n+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to\na competitive GRPO baseline. Apart from debugging code, TGPR focuses on a\nprincipled approach to combining learned policies with structured search\nmethods, offering a general framework for enhancing iterative refinement and\nstateful reasoning in LLMs.", "AI": {"tldr": "TGPR combines GRPO with Thompson-Sampling-based tree search to improve iterative refinement in LLMs, achieving significant performance gains on code generation benchmarks.", "motivation": "Existing iterative refinement methods rely on predefined heuristics that struggle with exploration-exploitation trade-offs and cannot adapt based on past refinement outcomes.", "method": "Tree-Guided Policy Refinement (TGPR) framework combining GRPO with Thompson-Sampling-based tree search to explore both failed and successful refinement paths actively.", "result": "Achieves up to +4.2 percentage points improvement in pass@1 on MBPP and +12.51 percentage points improvement in pass@10 on APPS compared to GRPO baseline.", "conclusion": "TGPR provides a principled approach to combining learned policies with structured search methods, offering a general framework for enhancing iterative refinement and stateful reasoning in LLMs."}}
{"id": "2510.06975", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06975", "abs": "https://arxiv.org/abs/2510.06975", "authors": ["Muris Sladi\u0107", "Veronica Valeros", "Carlos Catania", "Sebastian Garcia"], "title": "VelLMes: A high-interaction AI-based deception framework", "comment": "9 pages. 9 figures. 1 table. This is a preprint of a paper that was\n  presented at the Active Defense and Deception Workshop colocated with IEEE\n  EuroS&P 2025 conference", "summary": "There are very few SotA deception systems based on Large Language Models. The\nexisting ones are limited only to simulating one type of service, mainly SSH\nshells. These systems - but also the deception technologies not based on LLMs -\nlack an extensive evaluation that includes human attackers. Generative AI has\nrecently become a valuable asset for cybersecurity researchers and\npractitioners, and the field of cyber-deception is no exception. Researchers\nhave demonstrated how LLMs can be leveraged to create realistic-looking\nhoneytokens, fake users, and even simulated systems that can be used as\nhoneypots. This paper presents an AI-based deception framework called VelLMes,\nwhich can simulate multiple protocols and services such as SSH Linux shell,\nMySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus\nVelLMes offers a variety of choices for deception design based on the users'\nneeds. VelLMes is designed to be attacked by humans, so interactivity and\nrealism are key for its performance. We evaluate the generative capabilities\nand the deception capabilities. Generative capabilities were evaluated using\nunit tests for LLMs. The results of the unit tests show that, with careful\nprompting, LLMs can produce realistic-looking responses, with some LLMs having\na 100% passing rate. In the case of the SSH Linux shell, we evaluated deception\ncapabilities with 89 human attackers. The results showed that about 30% of the\nattackers thought that they were interacting with a real system when they were\nassigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH\nLinux shell honeypot on the Internet to capture real-life attacks. Analysis of\nthese attacks showed us that LLM honeypots simulating Linux shells can perform\nwell against unstructured and unexpected attacks on the Internet, responding\ncorrectly to most of the issued commands.", "AI": {"tldr": "VelLMes is an AI-based deception framework that simulates multiple protocols/services (SSH, MySQL, POP3, HTTP) as honeypots, evaluated with human attackers showing 30% deception success rate for SSH shells.", "motivation": "There are few LLM-based deception systems, existing ones are limited to single services like SSH, and lack human attacker evaluation. Generative AI can enhance cyber-deception by creating realistic honeytokens and simulated systems.", "method": "Developed VelLMes framework using LLMs to simulate multiple protocols/services. Evaluated generative capabilities with unit tests and deception capabilities with 89 human attackers for SSH shells. Also deployed 10 SSH honeypot instances on the Internet.", "result": "LLMs achieved 100% passing rate in unit tests with careful prompting. 30% of human attackers were deceived into thinking SSH honeypot was real. Internet deployment showed LLM honeypots perform well against unstructured attacks, responding correctly to most commands.", "conclusion": "LLM-based honeypots like VelLMes can effectively simulate multiple services and deceive human attackers, demonstrating practical value for cyber-deception with realistic interactive capabilities."}}
{"id": "2510.06427", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06427", "abs": "https://arxiv.org/abs/2510.06427", "authors": ["Elena Chistova"], "title": "Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser", "comment": "Accepted to CODI CRAC 2025", "summary": "We introduce UniRST, the first unified RST-style discourse parser capable of\nhandling 18 treebanks in 11 languages without modifying their relation\ninventories. To overcome inventory incompatibilities, we propose and evaluate\ntwo training strategies: Multi-Head, which assigns separate relation\nclassification layer per inventory, and Masked-Union, which enables shared\nparameter training through selective label masking. We first benchmark\nmonotreebank parsing with a simple yet effective augmentation technique for\nlow-resource settings. We then train a unified model and show that (1) the\nparameter efficient Masked-Union approach is also the strongest, and (2) UniRST\noutperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a\nsingle-model, multilingual end-to-end discourse parsing across diverse\nresources.", "AI": {"tldr": "UniRST is the first unified RST-style discourse parser that handles 18 treebanks across 11 languages without modifying relation inventories, using parameter-efficient training strategies that outperform most mono-treebank baselines.", "motivation": "To overcome inventory incompatibilities across different RST treebanks and enable unified multilingual discourse parsing with a single model.", "method": "Proposed two training strategies: Multi-Head (separate relation classification per inventory) and Masked-Union (shared parameter training with selective label masking), plus augmentation for low-resource settings.", "result": "Masked-Union approach is both parameter-efficient and strongest performer; UniRST outperforms 16 of 18 mono-treebank baselines.", "conclusion": "A single unified model for multilingual end-to-end discourse parsing is advantageous across diverse resources, with the Masked-Union strategy being particularly effective."}}
{"id": "2510.06448", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06448", "abs": "https://arxiv.org/abs/2510.06448", "authors": ["Prabhant Singh", "Sibylle Hess", "Joaquin Vanschoren"], "title": "How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation", "comment": null, "summary": "Transferability estimation metrics are used to find a high-performing\npre-trained model for a given target task without fine-tuning models and\nwithout access to the source dataset. Despite the growing interest in\ndeveloping such metrics, the benchmarks used to measure their progress have\ngone largely unexamined. In this work, we empirically show the shortcomings of\nwidely used benchmark setups to evaluate transferability estimation metrics. We\nargue that the benchmarks on which these metrics are evaluated are\nfundamentally flawed. We empirically demonstrate that their unrealistic model\nspaces and static performance hierarchies artificially inflate the perceived\nperformance of existing metrics, to the point where simple, dataset-agnostic\nheuristics can outperform sophisticated methods. Our analysis reveals a\ncritical disconnect between current evaluation protocols and the complexities\nof real-world model selection. To address this, we provide concrete\nrecommendations for constructing more robust and realistic benchmarks to guide\nfuture research in a more meaningful direction.", "AI": {"tldr": "Current benchmarks for transferability estimation metrics are flawed, artificially inflating performance of existing methods where simple heuristics can outperform sophisticated approaches due to unrealistic model spaces and static hierarchies.", "motivation": "To identify shortcomings in widely used benchmark setups for evaluating transferability estimation metrics, which are used to select pre-trained models without fine-tuning or source data access.", "method": "Empirical analysis demonstrating that current benchmarks have unrealistic model spaces and static performance hierarchies that artificially boost perceived metric performance.", "result": "Simple dataset-agnostic heuristics can outperform sophisticated transferability estimation methods under current flawed evaluation protocols, revealing a disconnect from real-world model selection complexities.", "conclusion": "Provides concrete recommendations for constructing more robust and realistic benchmarks to guide future research in transferability estimation metrics."}}
{"id": "2510.06911", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06911", "abs": "https://arxiv.org/abs/2510.06911", "authors": ["Hacane Hechehouche", "Andre Antakli", "Matthias Klusch"], "title": "LLM-Assisted Modeling of Semantic Web-Enabled Multi-Agents Systems with AJAN", "comment": null, "summary": "There are many established semantic Web standards for implementing\nmulti-agent driven applications. The AJAN framework allows to engineer\nmulti-agent systems based on these standards. In particular, agent knowledge is\nrepresented in RDF/RDFS and OWL, while agent behavior models are defined with\nBehavior Trees and SPARQL to access and manipulate this knowledge. However, the\nappropriate definition of RDF/RDFS and SPARQL-based agent behaviors still\nremains a major hurdle not only for agent modelers in practice. For example,\ndealing with URIs is very error-prone regarding typos and dealing with complex\nSPARQL queries in large-scale environments requires a high learning curve. In\nthis paper, we present an integrated development environment to overcome such\nhurdles of modeling AJAN agents and at the same time to extend the user\ncommunity for AJAN by the possibility to leverage Large Language Models for\nagent engineering.", "AI": {"tldr": "The paper presents an integrated development environment to simplify the modeling of AJAN agents by addressing challenges with RDF/RDFS, SPARQL queries, and URIs, while leveraging Large Language Models to expand the user community.", "motivation": "Current AJAN framework for multi-agent systems faces hurdles in defining RDF/RDFS and SPARQL-based agent behaviors, including error-prone URI handling and complex SPARQL queries that require high learning curves.", "method": "The authors develop an integrated development environment that simplifies agent modeling by overcoming URI and SPARQL complexity issues, and incorporates Large Language Models to assist in agent engineering.", "result": "The proposed IDE reduces the barriers to modeling AJAN agents by making RDF/RDFS and SPARQL more accessible and less error-prone, while expanding the framework's usability through LLM integration.", "conclusion": "The integrated development environment successfully addresses key challenges in AJAN agent modeling, making the framework more accessible and extending its user community through Large Language Model capabilities."}}
{"id": "2510.06994", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06994", "abs": "https://arxiv.org/abs/2510.06994", "authors": ["Artur Horal", "Daniel Pina", "Henrique Paz", "Iago Paulo", "Jo\u00e3o Soares", "Rafael Ferreira", "Diogo Tavares", "Diogo Gl\u00f3ria-Silva", "Jo\u00e3o Magalh\u00e3es", "David Semedo"], "title": "RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning", "comment": null, "summary": "This paper presents the vision, scientific contributions, and technical\ndetails of RedTWIZ: an adaptive and diverse multi-turn red teaming framework,\nto audit the robustness of Large Language Models (LLMs) in AI-assisted software\ndevelopment. Our work is driven by three major research streams: (1) robust and\nsystematic assessment of LLM conversational jailbreaks; (2) a diverse\ngenerative multi-turn attack suite, supporting compositional, realistic and\ngoal-oriented jailbreak conversational strategies; and (3) a hierarchical\nattack planner, which adaptively plans, serializes, and triggers attacks\ntailored to specific LLM's vulnerabilities. Together, these contributions form\na unified framework -- combining assessment, attack generation, and strategic\nplanning -- to comprehensively evaluate and expose weaknesses in LLMs'\nrobustness. Extensive evaluation is conducted to systematically assess and\nanalyze the performance of the overall system and each component. Experimental\nresults demonstrate that our multi-turn adversarial attack strategies can\nsuccessfully lead state-of-the-art LLMs to produce unsafe generations,\nhighlighting the pressing need for more research into enhancing LLM's\nrobustness.", "AI": {"tldr": "RedTWIZ is an adaptive multi-turn red teaming framework for auditing LLM robustness in AI-assisted software development, featuring systematic assessment, diverse attack generation, and hierarchical planning.", "motivation": "To comprehensively evaluate and expose weaknesses in LLMs' robustness through systematic red teaming, addressing the need for robust assessment of conversational jailbreaks.", "method": "Combines three components: systematic assessment of LLM conversational jailbreaks, diverse generative multi-turn attack suite with compositional strategies, and hierarchical attack planner that adaptively targets specific LLM vulnerabilities.", "result": "Experimental results show the multi-turn adversarial attack strategies can successfully make state-of-the-art LLMs produce unsafe generations, demonstrating framework effectiveness.", "conclusion": "Highlights the pressing need for more research into enhancing LLM robustness, as current models remain vulnerable to sophisticated multi-turn attacks."}}
{"id": "2510.06430", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06430", "abs": "https://arxiv.org/abs/2510.06430", "authors": ["Neeraja Kirtane", "Yuvraj Khanna", "Peter Relan"], "title": "MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning", "comment": null, "summary": "Large language models excel on math benchmarks, but their math reasoning\nrobustness to linguistic variation is underexplored. While recent work\nincreasingly treats high-difficulty competitions like the IMO as the gold\nstandard for evaluating reasoning, we believe in comprehensive benchmarking of\nhigh school-level math problems in real educational settings. We introduce\nMathRobust-LV, a test set and evaluation methodology that mirrors how\ninstructors rephrase problems across assessments while keeping difficulty\nconstant: we change surface details (names, contexts, variables) while\npreserving numerical structure and answers. In contrast to prior efforts that\nalter problem content or emphasize IMO-level tasks, we focus on\nhigh-school-level dataset problems at the difficulty level where models are\ncurrently deployed in educational settings: tutoring and assessment systems. In\nthese applications, instructors rephrase identical concepts in varied ways,\nmaking linguistic robustness essential for reliable deployment. Although MATH\ndata benchmarking is often regarded as saturated, our experiment on 34 models\nreveals that accuracy declines when moving from the baseline to the variants.\nThese drops are severe for smaller models (9-11%) while stronger models also\nshow measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain\ncomparatively stable. Our results highlight that robustness to linguistic\nvariation is a fundamental challenge, exposing reasoning vulnerabilities in\nmodels.", "AI": {"tldr": "The paper introduces MathRobust-LV, a test set for evaluating LLMs' math reasoning robustness to linguistic variations in high school-level problems, showing accuracy declines when problems are rephrased while keeping numerical structure constant.", "motivation": "To address the underexplored robustness of LLMs' math reasoning to linguistic variations in real educational settings, where instructors frequently rephrase problems while maintaining difficulty.", "method": "Created MathRobust-LV test set by changing surface details (names, contexts, variables) while preserving numerical structure and answers, then evaluated 34 models on baseline vs. variant problems.", "result": "Accuracy declines when moving from baseline to variants, with severe drops for smaller models (9-11%) and measurable degradation for stronger models, though frontier models like GPT-5 and Gemini-2.5pro remain relatively stable.", "conclusion": "Robustness to linguistic variation is a fundamental challenge that exposes reasoning vulnerabilities in LLMs, even when benchmark performance appears saturated."}}
{"id": "2510.06477", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06477", "abs": "https://arxiv.org/abs/2510.06477", "authors": ["Enrique Queipo-de-Llano", "\u00c1lvaro Arroyo", "Federico Barbero", "Xiaowen Dong", "Michael Bronstein", "Yann LeCun", "Ravid Shwartz-Ziv"], "title": "Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin", "comment": null, "summary": "Attention sinks and compression valleys have attracted significant attention\nas two puzzling phenomena in large language models, but have been studied in\nisolation. In this work, we present a surprising connection between attention\nsinks and compression valleys, tracing both to the formation of massive\nactivations in the residual stream. We prove theoretically that massive\nactivations necessarily produce representational compression and establish\nbounds on the resulting entropy reduction. Through experiments across several\nmodels (410M-120B parameters), we confirm that when the beginning-of-sequence\ntoken develops extreme activation norms in the middle layers, both compression\nvalleys and attention sinks emerge simultaneously. Targeted ablation studies\nvalidate our theoretical predictions. This unified view motivates us to propose\nthe Mix-Compress-Refine theory of information flow, as an attempt to explain\nhow LLMs organize their computation in depth by controlling attention and\nrepresentational compression via massive activations. Specifically, we posit\nthat Transformer-based LLMs process tokens in three distinct phases: (1) broad\nmixing in the early layers, (2) compressed computation with limited mixing in\nthe middle layers, and (3) selective refinement in the late layers. Our\nframework helps explain why embedding tasks perform best at intermediate\nlayers, whereas generation tasks benefit from full-depth processing, clarifying\ndifferences in task-dependent representations.", "AI": {"tldr": "Attention sinks and compression valleys are connected through massive activations in residual streams, leading to a unified Mix-Compress-Refine theory of information flow in Transformers.", "motivation": "To understand the connection between attention sinks and compression valleys, which have been studied separately, and explain how LLMs organize computation through massive activations.", "method": "Theoretical analysis proving massive activations cause representational compression, experimental validation across models (410M-120B parameters), and targeted ablation studies.", "result": "When beginning-of-sequence tokens develop extreme activation norms in middle layers, both compression valleys and attention sinks emerge simultaneously, confirming theoretical predictions.", "conclusion": "Proposed Mix-Compress-Refine theory explains Transformer computation in three phases: early mixing, middle compression, and late refinement, clarifying task-dependent representation differences."}}
{"id": "2510.06953", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06953", "abs": "https://arxiv.org/abs/2510.06953", "authors": ["Minju Gwak", "Guijin Son", "Jaehyung Kim"], "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces", "comment": null, "summary": "The Uniform Information Density (UID) hypothesis suggests that effective\ncommunication maintains a stable flow of information. In this work, we revisit\nthis principle in the context of large language model (LLM) reasoning traces,\nasking whether step-level uniformity reflects reasoning quality. To this end,\nwe propose an entropy-based stepwise information density metric and introduce\ntwo complementary measures of uniformity, local and global uniformity scores.\nAcross the experiments on six different reasoning benchmarks, we find that\nstep-level uniformity not only provides a strong theoretical lens but also\nyields practical performance benefits; for example, selecting reasoning traces\nwith more uniform information density at the step-level improves accuracy by\n10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals\nthat correct reasoning traces tend to avoid sharp information density spikes,\nwhile incorrect traces exhibit irregular information bursts. These results\ndemonstrate that UID-inspired information density measures outperform\nalternative internal signals as predictors of reasoning quality. Results\nhighlight the uniformity of the information density as a robust diagnostic and\nselection criterion for building more reliable and accurate reasoning systems.", "AI": {"tldr": "The paper revisits the Uniform Information Density (UID) hypothesis in LLM reasoning traces, finding that step-level uniformity correlates with reasoning quality and can improve accuracy by 10-32% when used for trace selection.", "motivation": "To investigate whether the Uniform Information Density hypothesis applies to LLM reasoning traces and whether step-level information uniformity reflects reasoning quality.", "method": "Proposed an entropy-based stepwise information density metric and introduced two complementary uniformity measures (local and global uniformity scores), tested across six reasoning benchmarks.", "result": "Step-level uniformity provides strong theoretical insights and practical benefits - selecting traces with uniform information density improves accuracy by 10-32% relative gains. Correct reasoning traces avoid sharp information spikes while incorrect ones show irregular bursts.", "conclusion": "UID-inspired information density measures outperform alternative internal signals as predictors of reasoning quality, making uniformity a robust diagnostic and selection criterion for building more reliable reasoning systems."}}
{"id": "2510.07080", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07080", "abs": "https://arxiv.org/abs/2510.07080", "authors": ["Maxime Reynouard"], "title": "Pseudo-MDPs: A Novel Framework for Efficiently Optimizing Last Revealer Seed Manipulations in Blockchains", "comment": null, "summary": "This study tackles the computational challenges of solving Markov Decision\nProcesses (MDPs) for a restricted class of problems. It is motivated by the\nLast Revealer Attack (LRA), which undermines fairness in some Proof-of-Stake\n(PoS) blockchains such as Ethereum (\\$400B market capitalization). We introduce\npseudo-MDPs (pMDPs) a framework that naturally models such problems and propose\ntwo distinct problem reductions to standard MDPs. One problem reduction\nprovides a novel, counter-intuitive perspective, and combining the two problem\nreductions enables significant improvements in dynamic programming algorithms\nsuch as value iteration. In the case of the LRA which size is parameterized by\n$\\kappa$ (in Ethereum's case $\\kappa$ = 32), we reduce the computational\ncomplexity from O(2^$\\kappa$ $\\kappa$^2^($\\kappa$+2)) to O($\\kappa$^4) (per\niteration). This solution also provide the usual benefits from Dynamic\nProgramming solutions: exponentially fast convergence toward the optimal\nsolution is guaranteed. The dual perspective also simplifies policy extraction,\nmaking the approach well-suited for resource-constrained agents who can operate\nwith very limited memory and computation once the problem has been solved.\nFurthermore, we generalize those results to a broader class of MDPs, enhancing\ntheir applicability. The framework is validated through two case studies: a\nfictional card game and the LRA on the Ethereum random seed consensus protocol.\nThese applications demonstrate the framework's ability to solve large-scale\nproblems effectively while offering actionable insights into optimal\nstrategies. This work advances the study of MDPs and contributes to\nunderstanding security vulnerabilities in blockchain systems.", "AI": {"tldr": "This paper introduces pseudo-MDPs (pMDPs) to solve computational challenges in MDPs, specifically targeting the Last Revealer Attack in PoS blockchains like Ethereum. It proposes two problem reductions that dramatically improve computational efficiency from O(2^\u03ba \u03ba^(2\u03ba+4)) to O(\u03ba^4) per iteration.", "motivation": "Address computational challenges in solving MDPs for restricted problem classes, particularly motivated by the Last Revealer Attack that undermines fairness in Proof-of-Stake blockchains like Ethereum ($400B market cap).", "method": "Introduces pseudo-MDPs (pMDPs) framework and proposes two distinct problem reductions to standard MDPs. Combines these reductions to significantly improve dynamic programming algorithms like value iteration.", "result": "Achieves dramatic computational complexity reduction from O(2^\u03ba \u03ba^(2\u03ba+4)) to O(\u03ba^4) per iteration for the Last Revealer Attack (where \u03ba=32 in Ethereum). Provides exponentially fast convergence to optimal solution and simplifies policy extraction for resource-constrained agents.", "conclusion": "The pMDP framework effectively solves large-scale MDP problems, advances MDP study, and contributes to understanding blockchain security vulnerabilities. Validated through case studies including Ethereum's random seed consensus protocol."}}
{"id": "2510.06445", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06445", "abs": "https://arxiv.org/abs/2510.06445", "authors": ["Asif Shahriar", "Md Nafiu Rahman", "Sadif Ahmed", "Farig Sadeque", "Md Rizwan Parvez"], "title": "A Survey on Agentic Security: Applications, Threats and Defenses", "comment": null, "summary": "The rapid shift from passive LLMs to autonomous LLM-agents marks a new\nparadigm in cybersecurity. While these agents can act as powerful tools for\nboth offensive and defensive operations, the very agentic context introduces a\nnew class of inherent security risks. In this work we present the first\nholistic survey of the agentic security landscape, structuring the field around\nthree interdependent pillars: Applications, Threats, and Defenses. We provide a\ncomprehensive taxonomy of over 150 papers, explaining how agents are used, the\nvulnerabilities they possess, and the countermeasures designed to protect them.\nA detailed cross-cutting analysis shows emerging trends in agent architecture\nwhile revealing critical research gaps in model and modality coverage.", "AI": {"tldr": "This paper presents the first comprehensive survey of security risks in autonomous LLM-agents, categorizing the field into Applications, Threats, and Defenses with analysis of over 150 papers.", "motivation": "The transition from passive LLMs to autonomous LLM-agents introduces new security vulnerabilities in the agentic context that need systematic analysis.", "method": "The authors conduct a holistic survey and provide a comprehensive taxonomy of over 150 papers, structuring the analysis around three pillars: Applications, Threats, and Defenses.", "result": "The survey reveals emerging trends in agent architecture and identifies critical research gaps in model and modality coverage through detailed cross-cutting analysis.", "conclusion": "This work establishes a foundational framework for understanding the security landscape of autonomous LLM-agents and highlights areas requiring further research attention."}}
{"id": "2510.06478", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06478", "abs": "https://arxiv.org/abs/2510.06478", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Valid Stopping for LLM Generation via Empirical Dynamic Formal Lift", "comment": null, "summary": "We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), applying\nanytime-valid sequential testing to language model generation stopping. Our\napproach tracks information lift -- the log-likelihood ratio between full\nmodels and deliberately weakened \"skeleton\" baselines -- using self-normalized\nempirical-Bernstein e-processes that provide formal delta-level error control\nregardless of stopping time. We handle unknown centering through online mean\nestimation, combine multiple parameters via mixture e-processes, and support\nadaptive resets under distributional drift. On six benchmarks, Sequential-EDFL\nreduces generation by 22-28% vs. sequential baselines while maintaining\ndelta-level control with 12% computational overhead. We introduce automated\nskeletons (distilled submodels, randomized logits) and show robustness across\nskeleton families. Composing EDFL with a lightweight correctness gate (sentence\nboundaries + verifier) improves end-task correctness while preserving\nanytime-valid guarantees by only delaying stopping. Our certificates control\ninformation sufficiency, not factual correctness -- 10.9% of stopped sequences\nremain incorrect even with the gate (13.2-22.7% without it). EDFL serves as a\nfirst-stage filter reducing verification burden by 83%, not as a standalone\nsolution for safety-critical domains.", "AI": {"tldr": "Sequential-EDFL applies anytime-valid sequential testing to language model generation stopping, using information lift tracking with formal error control, reducing generation by 22-28% while maintaining delta-level control.", "motivation": "To develop a method for early stopping of language model generation that provides formal error guarantees regardless of when stopping occurs, reducing computational costs while maintaining reliability.", "method": "Uses self-normalized empirical-Bernstein e-processes to track information lift (log-likelihood ratio between full models and weakened \"skeleton\" baselines), with online mean estimation for unknown centering, mixture e-processes for multiple parameters, and adaptive resets for distributional drift.", "result": "Reduces generation by 22-28% vs. sequential baselines while maintaining delta-level control with 12% computational overhead. When combined with correctness gate, improves end-task correctness while preserving anytime-valid guarantees.", "conclusion": "EDFL serves as an effective first-stage filter reducing verification burden by 83%, but is not a standalone solution for safety-critical domains as it controls information sufficiency rather than factual correctness."}}
{"id": "2510.07038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07038", "abs": "https://arxiv.org/abs/2510.07038", "authors": ["Wenxun Wu", "Yuanyang Li", "Guhan Chen", "Linyue Wang", "Hongyang Chen"], "title": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning", "comment": null, "summary": "Recent advances in large language models (LLMs) have popularized test-time\nscaling, where models generate additional reasoning tokens before producing\nfinal answers. These approaches have demonstrated significant performance\nimprovements on benchmarks involving mathematical reasoning. However, language\nmodels relying solely on direct inference still struggle with tasks demanding\nup-to-date knowledge or computational tools such as calculators and code\ninterpreters for complex arithmetic operations. To overcome these limitations,\nwe propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement\nlearning framework that systematically integrates multi-hop reasoning with\nadaptive tool-calling capabilities. Our approach employs a modified version of\nDynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,\nwhich we adapt specifically for tool invocation scenarios, enabling models to\ndynamically interleave complex reasoning with on-demand tool usage (including\nsearch APIs and Python interpreters).\n  To support this research, we introduce two new datasets: TAPO-easy-60K and\nTAPO-hard-18K, specifically designed to train and evaluate both fact-based\nreasoning and mathematical calculation capabilities. Our experiments on\nQwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,\nwith both models achieving state-of-the-art performance on tasks requiring\nexternal knowledge and mathematical computation among methods with comparable\nparameters. Notably, TAPO achieves more efficient tool utilization than\nbaseline methods while preventing excessive calls caused by reward hacking.\nThese results highlight the significant potential of combining advanced\nreasoning with tool usage to enhance model performance in knowledge-intensive\nand computationally demanding tasks.", "AI": {"tldr": "TAPO is a reinforcement learning framework that combines multi-hop reasoning with adaptive tool-calling capabilities, achieving state-of-the-art performance on knowledge-intensive and computational tasks.", "motivation": "Current LLMs struggle with tasks requiring up-to-date knowledge or computational tools like calculators and code interpreters for complex arithmetic operations.", "method": "Proposes Tool-Augmented Policy Optimization (TAPO), a modified version of Dynamic Sampling Policy Optimization adapted for tool invocation scenarios, enabling dynamic interleaving of reasoning with tool usage.", "result": "TAPO achieves state-of-the-art performance on tasks requiring external knowledge and mathematical computation, with more efficient tool utilization than baseline methods while preventing excessive calls.", "conclusion": "Combining advanced reasoning with tool usage significantly enhances model performance in knowledge-intensive and computationally demanding tasks."}}
{"id": "2510.07109", "categories": ["cs.CR", "cs.LG", "cs.NI", "C.2.0; C.2.1; C.2.3; C.2.5; I.2.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.07109", "abs": "https://arxiv.org/abs/2510.07109", "authors": ["Guan-Yan Yang", "Farn Wang", "Kuo-Hui Yeh"], "title": "GNN-enhanced Traffic Anomaly Detection for Next-Generation SDN-Enabled Consumer Electronics", "comment": "This paper has been accepted for publication in IEEE Transactions on\n  Consumer Electronics. 10 pages, 6 figures", "summary": "Consumer electronics (CE) connected to the Internet of Things are susceptible\nto various attacks, including DDoS and web-based threats, which can compromise\ntheir functionality and facilitate remote hijacking. These vulnerabilities\nallow attackers to exploit CE for broader system attacks while enabling the\npropagation of malicious code across the CE network, resulting in device\nfailures. Existing deep learning-based traffic anomaly detection systems\nexhibit high accuracy in traditional network environments but are often overly\ncomplex and reliant on static infrastructure, necessitating manual\nconfiguration and management. To address these limitations, we propose a\nscalable network model that integrates Software-defined Networking (SDN) and\nCompute First Networking (CFN) for next-generation CE networks. In this network\nmodel, we propose a Graph Neural Networks-based Network Anomaly Detection\nframework (GNN-NAD) that integrates SDN-based CE networks and enables the CFN\narchitecture. GNN-NAD uniquely fuses a static, vulnerability-aware attack graph\nwith dynamic traffic features, providing a holistic view of network security.\nThe core of the framework is a GNN model (GSAGE) for graph representation\nlearning, followed by a Random Forest (RF) classifier. This design (GSAGE+RF)\ndemonstrates superior performance compared to existing feature selection\nmethods. Experimental evaluations on CE environment reveal that GNN-NAD\nachieves superior metrics in accuracy, recall, precision, and F1 score, even\nwith small sample sizes, exceeding the performance of current network anomaly\ndetection methods. This work advances the security and efficiency of\nnext-generation intelligent CE networks.", "AI": {"tldr": "Proposes GNN-NAD framework combining SDN and CFN for IoT security, using GSAGE+RF model to detect anomalies with superior performance.", "motivation": "Consumer IoT devices are vulnerable to attacks like DDoS and web threats, while existing deep learning detection systems are too complex and require manual configuration.", "method": "Integrates SDN and CFN in scalable network model, uses GNN-based framework (GNN-NAD) that fuses static vulnerability-aware attack graphs with dynamic traffic features, employing GSAGE for graph learning and Random Forest classifier.", "result": "Achieves superior accuracy, recall, precision, and F1 score metrics in CE environments, outperforming current methods even with small sample sizes.", "conclusion": "Advances security and efficiency of next-generation intelligent consumer electronics networks through the proposed scalable framework."}}
{"id": "2510.06461", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06461", "abs": "https://arxiv.org/abs/2510.06461", "authors": ["Massimo Daul", "Alessio Tosolini", "Claire Bowern"], "title": "Linguistically Informed Tokenization Improves ASR for Underresourced Languages", "comment": null, "summary": "Automatic speech recognition (ASR) is a crucial tool for linguists aiming to\nperform a variety of language documentation tasks. However, modern ASR systems\nuse data-hungry transformer architectures, rendering them generally unusable\nfor underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu,\na dormant Indigenous Australian language, comparing the effects of phonemic and\northographic tokenization strategies on performance. In parallel, we explore\nASR's viability as a tool in a language documentation pipeline. We find that a\nlinguistically informed phonemic tokenization system substantially improves WER\nand CER compared to a baseline orthographic tokenization scheme. Finally, we\nshow that hand-correcting the output of an ASR model is much faster than\nhand-transcribing audio from scratch, demonstrating that ASR can work for\nunderresourced languages.", "AI": {"tldr": "Fine-tuning wav2vec2 ASR model on Yan-nhangu language shows phonemic tokenization outperforms orthographic tokenization, and ASR-assisted transcription is faster than manual transcription for underresourced languages.", "motivation": "Modern ASR systems require large datasets, making them unsuitable for underresourced languages. This research explores ASR's viability for language documentation of dormant Indigenous languages.", "method": "Fine-tuned wav2vec2 ASR model on Yan-nhangu language, comparing phonemic vs orthographic tokenization strategies, and evaluated ASR as a tool in language documentation pipeline.", "result": "Phonemic tokenization significantly improved Word Error Rate (WER) and Character Error Rate (CER) compared to orthographic tokenization. Hand-correcting ASR output was much faster than manual transcription.", "conclusion": "ASR can be effectively used for underresourced languages when employing linguistically informed tokenization strategies, providing efficient tools for language documentation."}}
{"id": "2510.06502", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06502", "abs": "https://arxiv.org/abs/2510.06502", "authors": ["Khoa Trinh", "Gaurav Menghani", "Erik Vee"], "title": "GUIDE: Guided Initialization and Distillation of Embeddings", "comment": null, "summary": "Algorithmic efficiency techniques such as distillation\n(\\cite{hinton2015distillation}) are useful in improving model quality without\nincreasing serving costs, provided a larger teacher model is available for a\nsmaller student model to learn from during training. Standard distillation\nmethods are limited to only forcing the student to match the teacher's outputs.\nGiven the costs associated with training a large model, we believe we should be\nextracting more useful information from a teacher model than by just making the\nstudent match the teacher's outputs.\n  In this paper, we introduce \\guide (Guided Initialization and Distillation of\nEmbeddings). \\guide can be considered a distillation technique that forces the\nstudent to match the teacher in the parameter space. Using \\guide we show\n25-26\\% reduction in the teacher-student quality gap when using large student\nmodels (400M - 1B parameters) trained on $\\approx$ 20B tokens. We also present\na thorough analysis demonstrating that \\guide can be combined with knowledge\ndistillation with near additive improvements. Furthermore, we show that\napplying \\guide alone leads to substantially better model quality than applying\nknowledge distillation by itself.\n  Most importantly, \\guide introduces no training or inference overhead and\nhence any model quality gains from our method are virtually free.", "AI": {"tldr": "GUIDE is a distillation technique that forces student models to match teacher models in parameter space, achieving 25-26% reduction in quality gap with no training or inference overhead.", "motivation": "Standard distillation only makes students match teacher outputs, but given the high cost of training large teacher models, more useful information should be extracted from teachers.", "method": "GUIDE (Guided Initialization and Distillation of Embeddings) forces student models to match teacher models in the parameter space rather than just output space.", "result": "25-26% reduction in teacher-student quality gap for large student models (400M-1B parameters) trained on ~20B tokens; GUIDE alone outperforms knowledge distillation alone.", "conclusion": "GUIDE provides substantial quality improvements over standard distillation with no training or inference overhead, making the gains virtually free."}}
{"id": "2510.07064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07064", "abs": "https://arxiv.org/abs/2510.07064", "authors": ["Manh Hung Nguyen", "Sebastian Tschiatschek", "Adish Singla"], "title": "Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations", "comment": null, "summary": "The difficulty and expense of obtaining large-scale human responses make\nLarge Language Models (LLMs) an attractive alternative and a promising proxy\nfor human behavior. However, prior work shows that LLMs often produce\nhomogeneous outputs that fail to capture the rich diversity of human\nperspectives and behaviors. Thus, rather than trying to capture this diversity\nwith a single LLM agent, we propose a novel framework to construct a set of\nagents that collectively capture the diversity of a given human population.\nEach agent is an LLM whose behavior is steered by conditioning on a small set\nof human demonstrations (task-response pairs) through in-context learning. The\ncentral challenge is therefore to select a representative set of LLM agents\nfrom the exponentially large space of possible agents. We tackle this selection\nproblem from the lens of submodular optimization. In particular, we develop\nmethods that offer different trade-offs regarding time complexity and\nperformance guarantees. Extensive experiments in crowdsourcing and educational\ndomains demonstrate that our approach constructs agents that more effectively\nrepresent human populations compared to baselines. Moreover, behavioral\nanalyses on new tasks show that these agents reproduce the behavior patterns\nand perspectives of the students and annotators they are designed to represent.", "AI": {"tldr": "Proposes a framework to create diverse LLM agents that collectively represent human population diversity by selecting representative agents through submodular optimization, instead of using a single homogeneous LLM.", "motivation": "LLMs often produce homogeneous outputs that fail to capture the rich diversity of human perspectives and behaviors, making them poor proxies for human populations despite their potential as alternatives to expensive human data collection.", "method": "Constructs a set of LLM agents where each agent is steered by conditioning on a small set of human demonstrations through in-context learning, and uses submodular optimization to select representative agents from the exponentially large space of possible agents.", "result": "Extensive experiments show the approach constructs agents that more effectively represent human populations compared to baselines, and behavioral analyses demonstrate these agents reproduce behavior patterns and perspectives of the target populations.", "conclusion": "The proposed framework successfully creates diverse LLM agents that collectively capture human population diversity, addressing the homogeneity problem of single LLM approaches while maintaining computational efficiency through submodular optimization."}}
{"id": "2510.07171", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07171", "abs": "https://arxiv.org/abs/2510.07171", "authors": ["Rishabh Das. Aaron Werth", "Tommy Morris"], "title": "A multi-layered embedded intrusion detection framework for programmable logic controllers", "comment": null, "summary": "Industrial control system (ICS) operations use trusted endpoints like human\nmachine interfaces (HMIs) and workstations to relay commands to programmable\nlogic controllers (PLCs). Because most PLCs lack layered defenses, compromise\nof a trusted endpoint can drive unsafe actuator commands and risk\nsafety-critical operation. This research presents an embedded intrusion\ndetection system that runs inside the controller and uses header-level\ntelemetry to detect and respond to network attacks. The system combines a\nsemi-supervised anomaly detector and a supervised attack classifier. We\nevaluate the approach on a midstream oil-terminal testbed using three datasets\ncollected during tanker-truck loading. The anomaly detector achieves zero\nmissed attacks, corresponding to 0.998 Matthews correlation. The supervised\nstage attains 97.37 percent hold-out accuracy and 97.03 percent external\naccuracy. The embedded design adds a median of 2,031 microseconds of end-to-end\nlatency and does not impact PLC's cycle time. The proposed architecture\nprovides a multi-layer embedded security that meets the real-time requirements\nof an industrial system.", "AI": {"tldr": "An embedded intrusion detection system for industrial control systems that runs inside PLCs, using header-level telemetry to detect network attacks with minimal latency impact.", "motivation": "Industrial control systems lack layered defenses, making them vulnerable when trusted endpoints are compromised, which can lead to unsafe actuator commands and safety risks.", "method": "Combines semi-supervised anomaly detector and supervised attack classifier using header-level telemetry, evaluated on oil-terminal testbed with tanker-truck loading datasets.", "result": "Anomaly detector: 0 missed attacks, 0.998 Matthews correlation; Supervised stage: 97.37% hold-out accuracy, 97.03% external accuracy; adds only 2,031 microseconds median latency without impacting PLC cycle time.", "conclusion": "The embedded architecture provides multi-layer security that meets real-time industrial system requirements while effectively detecting network attacks."}}
{"id": "2510.06471", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.06471", "abs": "https://arxiv.org/abs/2510.06471", "authors": ["Zihao Li", "Shaoxiong Ji", "J\u00f6rg Tiedemann"], "title": "Test-Time Scaling of Reasoning Models for Machine Translation", "comment": null, "summary": "Test-time scaling (TTS) has enhanced the performance of Reasoning Models\n(RMs) on various tasks such as math and coding, yet its efficacy in machine\ntranslation (MT) remains underexplored. This paper investigates whether\nincreased inference-time computation improves translation quality. We evaluate\n12 RMs across a diverse suite of MT benchmarks spanning multiple domains,\nexamining three scenarios: direct translation, forced-reasoning extrapolation,\nand post-editing. Our findings show that for general-purpose RMs, TTS provides\nlimited and inconsistent benefits for direct translation, with performance\nquickly plateauing. However, the effectiveness of TTS is unlocked by\ndomain-specific fine-tuning, which aligns a model's reasoning process with task\nrequirements, leading to consistent improvements up to an optimal,\nself-determined reasoning depth. We also find that forcing a model to reason\nbeyond its natural stopping point consistently degrades translation quality. In\ncontrast, TTS proves highly effective in a post-editing context, reliably\nturning self-correction into a beneficial process. These results indicate that\nthe value of inference-time computation in MT lies not in enhancing single-pass\ntranslation with general models, but in targeted applications like multi-step,\nself-correction workflows and in conjunction with task-specialized models.", "AI": {"tldr": "Test-time scaling (TTS) shows limited benefits for direct machine translation with general-purpose models but becomes effective when combined with domain-specific fine-tuning or used in post-editing workflows.", "motivation": "To investigate whether increased inference-time computation through test-time scaling improves translation quality in machine translation, given its proven effectiveness in other reasoning tasks.", "method": "Evaluated 12 reasoning models across diverse MT benchmarks using three scenarios: direct translation, forced-reasoning extrapolation, and post-editing, analyzing the effects of TTS under different conditions.", "result": "TTS provides limited and inconsistent benefits for direct translation with general models, but domain-specific fine-tuning unlocks consistent improvements up to optimal reasoning depth. Forced reasoning beyond natural stopping point degrades quality, while post-editing with TTS reliably improves translations.", "conclusion": "The value of inference-time computation in MT lies in targeted applications like multi-step self-correction workflows and task-specialized models, rather than enhancing single-pass translation with general models."}}
{"id": "2510.06503", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06503", "abs": "https://arxiv.org/abs/2510.06503", "authors": ["I-Hsi Kao", "Kanji Uchino"], "title": "ATLO-ML: Adaptive Time-Length Optimizer for Machine Learning -- Insights from Air Quality Forecasting", "comment": null, "summary": "Accurate time-series predictions in machine learning are heavily influenced\nby the selection of appropriate input time length and sampling rate. This paper\nintroduces ATLO-ML, an adaptive time-length optimization system that\nautomatically determines the optimal input time length and sampling rate based\non user-defined output time length. The system provides a flexible approach to\ntime-series data pre-processing, dynamically adjusting these parameters to\nenhance predictive performance. ATLO-ML is validated using air quality\ndatasets, including both GAMS-dataset and proprietary data collected from a\ndata center, both in time series format. Results demonstrate that utilizing the\noptimized time length and sampling rate significantly improves the accuracy of\nmachine learning models compared to fixed time lengths. ATLO-ML shows potential\nfor generalization across various time-sensitive applications, offering a\nrobust solution for optimizing temporal input parameters in machine learning\nworkflows.", "AI": {"tldr": "ATLO-ML is an adaptive system that automatically optimizes input time length and sampling rate for time-series predictions based on user-defined output time length, improving ML model accuracy.", "motivation": "Accurate time-series predictions in machine learning are heavily influenced by the selection of appropriate input time length and sampling rate, which are typically fixed and suboptimal.", "method": "ATLO-ML adaptively determines optimal input time length and sampling rate based on user-defined output time length, providing flexible time-series data pre-processing with dynamic parameter adjustment.", "result": "Validation using air quality datasets shows that optimized time length and sampling rate significantly improve ML model accuracy compared to fixed time lengths.", "conclusion": "ATLO-ML demonstrates potential for generalization across various time-sensitive applications and offers a robust solution for optimizing temporal input parameters in ML workflows."}}
{"id": "2510.07069", "categories": ["cs.AI", "I.2.4"], "pdf": "https://arxiv.org/pdf/2510.07069", "abs": "https://arxiv.org/abs/2510.07069", "authors": ["Hongbo Hu", "Yisong Wang", "Yi Huang", "Kewen Wang"], "title": "Inductive Learning for Possibilistic Logic Programs Under Stable Models", "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Possibilistic logic programs (poss-programs) under stable models are a major\nvariant of answer set programming (ASP). While its semantics (possibilistic\nstable models) and properties have been well investigated, the problem of\ninductive reasoning has not been investigated yet. This paper presents an\napproach to extracting poss-programs from a background program and examples\n(parts of intended possibilistic stable models). To this end, the notion of\ninduction tasks is first formally defined, its properties are investigated and\ntwo algorithms ilpsm and ilpsmmin for computing induction solutions are\npresented. An implementation of ilpsmmin is also provided and experimental\nresults show that when inputs are ordinary logic programs, the prototype\noutperforms a major inductive learning system for normal logic programs from\nstable models on the datasets that are randomly generated.", "AI": {"tldr": "This paper presents an inductive learning approach for possibilistic logic programs (poss-programs) under stable models, defining induction tasks and providing algorithms (ilpsm and ilpsmmin) to extract poss-programs from background programs and examples.", "motivation": "While possibilistic stable models and their properties have been well studied, the problem of inductive reasoning for poss-programs has not been investigated yet, creating a gap in the research.", "method": "The authors formally define induction tasks, investigate their properties, and present two algorithms (ilpsm and ilpsmmin) for computing induction solutions. An implementation of ilpsmmin is also provided.", "result": "Experimental results show that when inputs are ordinary logic programs, the prototype implementation outperforms a major inductive learning system for normal logic programs from stable models on randomly generated datasets.", "conclusion": "The paper successfully addresses the inductive reasoning problem for poss-programs by providing formal definitions, algorithms, and empirical evidence showing competitive performance compared to existing systems."}}
{"id": "2510.07176", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07176", "abs": "https://arxiv.org/abs/2510.07176", "authors": ["Yixiang Zhang", "Xinhao Deng", "Zhongyi Gu", "Yihao Chen", "Ke Xu", "Qi Li", "Jianping Wu"], "title": "Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of Privacy Risks in LLM Agent Interactions", "comment": "26 pages with 11 figures", "summary": "Large Language Models (LLMs) are increasingly deployed as agents that\norchestrate tasks and integrate external tools to execute complex workflows. We\ndemonstrate that these interactive behaviors leave distinctive fingerprints in\nencrypted traffic exchanged between users and LLM agents. By analyzing traffic\npatterns associated with agent workflows and tool invocations, adversaries can\ninfer agent activities, distinguish specific agents, and even profile sensitive\nuser attributes. To highlight this risk, we develop AgentPrint, which achieves\nan F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3\naccuracy in user attribute inference for simulated- and real-user settings,\nrespectively. These results uncover an overlooked risk: the very interactivity\nthat empowers LLM agents also exposes user privacy, underscoring the urgent\nneed for technical countermeasures alongside regulatory and policy safeguards.", "AI": {"tldr": "LLM agents' interactive behaviors create identifiable traffic patterns that can reveal agent activities, specific agents, and sensitive user attributes, posing privacy risks.", "motivation": "To highlight the privacy risks associated with LLM agents' interactive behaviors that leave distinctive fingerprints in encrypted traffic, enabling adversaries to infer sensitive information.", "method": "Developed AgentPrint to analyze traffic patterns associated with agent workflows and tool invocations for agent identification and user attribute inference.", "result": "AgentPrint achieves F1-score of 0.866 in agent identification and 73.9%/69.1% top-3 accuracy in user attribute inference for simulated/real-user settings.", "conclusion": "The interactivity that empowers LLM agents also exposes user privacy, requiring urgent technical countermeasures and regulatory safeguards."}}
{"id": "2510.06499", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06499", "abs": "https://arxiv.org/abs/2510.06499", "authors": ["Zhepeng Cen", "Haolin Chen", "Shiyu Wang", "Zuxin Liu", "Zhiwei Liu", "Ding Zhao", "Silvio Savarese", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success through\nimitation learning on vast text corpora, but this paradigm creates a\ntraining-generation gap and limits robust reasoning. Reinforcement learning\n(RL) offers a more data-efficient solution capable of bridging this gap, yet\nits application has been constrained by a critical data bottleneck: existing RL\ndatasets are orders of magnitude smaller and less diverse than web-scale\npre-training corpora. To address this, we introduce the Webscale-RL pipeline, a\nscalable data engine that systematically converts large-scale pre-training\ndocuments into millions of diverse, verifiable question-answer pairs for RL.\nUsing this pipeline, we construct the Webscale-RL dataset, containing 1.2\nmillion examples across more than 9 domains. Our experiments show that the\nmodel trained on this dataset significantly outperforms continual pretraining\nand strong data refinement baselines across a suite of benchmarks. Notably, RL\ntraining with our dataset proves substantially more efficient, achieving the\nperformance of continual pre-training with up to 100$\\times$ fewer tokens. Our\nwork presents a viable path toward scaling RL to pre-training levels, enabling\nmore capable and efficient language models.", "AI": {"tldr": "Webscale-RL pipeline converts large-scale pre-training documents into millions of diverse, verifiable QA pairs for RL, enabling more efficient training than continual pre-training.", "motivation": "Address the training-generation gap in LLMs and overcome the data bottleneck in RL applications by creating web-scale RL datasets.", "method": "Developed Webscale-RL pipeline to systematically convert pre-training documents into 1.2M diverse QA pairs across 9+ domains for RL training.", "result": "Models trained on Webscale-RL dataset significantly outperform continual pre-training and data refinement baselines, achieving same performance with up to 100\u00d7 fewer tokens.", "conclusion": "Webscale-RL presents a viable path to scale RL to pre-training levels, enabling more capable and efficient language models."}}
{"id": "2510.06505", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06505", "abs": "https://arxiv.org/abs/2510.06505", "authors": ["Momin Abbas", "Ali Falahati", "Hossein Goli", "Mohammad Mohammadi Amiri"], "title": "A Median Perspective on Unlabeled Data for Out-of-Distribution Detection", "comment": null, "summary": "Out-of-distribution (OOD) detection plays a crucial role in ensuring the\nrobustness and reliability of machine learning systems deployed in real-world\napplications. Recent approaches have explored the use of unlabeled data,\nshowing potential for enhancing OOD detection capabilities. However,\neffectively utilizing unlabeled in-the-wild data remains challenging due to the\nmixed nature of both in-distribution (InD) and OOD samples. The lack of a\ndistinct set of OOD samples complicates the task of training an optimal OOD\nclassifier. In this work, we introduce Medix, a novel framework designed to\nidentify potential outliers from unlabeled data using the median operation. We\nuse the median because it provides a stable estimate of the central tendency,\nas an OOD detection mechanism, due to its robustness against noise and\noutliers. Using these identified outliers, along with labeled InD data, we\ntrain a robust OOD classifier. From a theoretical perspective, we derive error\nbounds that demonstrate Medix achieves a low error rate. Empirical results\nfurther substantiate our claims, as Medix outperforms existing methods across\nthe board in open-world settings, confirming the validity of our theoretical\ninsights.", "AI": {"tldr": "Medix is a novel OOD detection framework that identifies outliers from unlabeled data using median operations, then trains a robust OOD classifier using these identified outliers and labeled InD data.", "motivation": "Effectively utilizing unlabeled in-the-wild data for OOD detection is challenging due to mixed InD and OOD samples, and the lack of distinct OOD samples makes training optimal classifiers difficult.", "method": "Uses median operation to identify potential outliers from unlabeled data due to its robustness against noise and outliers, then trains OOD classifier with identified outliers and labeled InD data.", "result": "Empirical results show Medix outperforms existing methods across the board in open-world settings, and theoretical analysis demonstrates low error rate bounds.", "conclusion": "Medix provides an effective framework for OOD detection using unlabeled data, with both theoretical guarantees and empirical superiority over existing approaches."}}
{"id": "2510.07073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07073", "abs": "https://arxiv.org/abs/2510.07073", "authors": ["Andr\u00e9 Hottung", "Federico Berto", "Chuanbo Hua", "Nayeli Gast Zepeda", "Daniel Wetzel", "Michael R\u00f6mer", "Haoran Ye", "Davide Zago", "Michael Poli", "Stefano Massaroli", "Jinkyoo Park", "Kevin Tierney"], "title": "VRPAgent: LLM-Driven Discovery of Heuristic Operators for Vehicle Routing Problems", "comment": null, "summary": "Designing high-performing heuristics for vehicle routing problems (VRPs) is a\ncomplex task that requires both intuition and deep domain knowledge. Large\nlanguage model (LLM)-based code generation has recently shown promise across\nmany domains, but it still falls short of producing heuristics that rival those\ncrafted by human experts. In this paper, we propose VRPAgent, a framework that\nintegrates LLM-generated components into a metaheuristic and refines them\nthrough a novel genetic search. By using the LLM to generate problem-specific\noperators, embedded within a generic metaheuristic framework, VRPAgent keeps\ntasks manageable, guarantees correctness, and still enables the discovery of\nnovel and powerful strategies. Across multiple problems, including the\ncapacitated VRP, the VRP with time windows, and the prize-collecting VRP, our\nmethod discovers heuristic operators that outperform handcrafted methods and\nrecent learning-based approaches while requiring only a single CPU core. To our\nknowledge, \\VRPAgent is the first LLM-based paradigm to advance the\nstate-of-the-art in VRPs, highlighting a promising future for automated\nheuristics discovery.", "AI": {"tldr": "VRPAgent is a framework that uses LLM-generated components within a metaheuristic and refines them through genetic search to automatically discover high-performing heuristic operators for vehicle routing problems.", "motivation": "Designing effective heuristics for VRPs requires significant domain expertise, and current LLM-based code generation approaches still fall short of human-crafted heuristics.", "method": "Integrates LLM-generated problem-specific operators into a generic metaheuristic framework and refines them through novel genetic search, keeping tasks manageable while ensuring correctness.", "result": "Outperforms handcrafted methods and recent learning-based approaches across multiple VRP variants (capacitated VRP, VRP with time windows, prize-collecting VRP) using only a single CPU core.", "conclusion": "VRPAgent is the first LLM-based paradigm to advance state-of-the-art in VRPs, demonstrating promising future for automated heuristics discovery."}}
{"id": "2510.07219", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2510.07219", "abs": "https://arxiv.org/abs/2510.07219", "authors": ["Yuhua Xu", "Wei Sun", "Chengpei Tang", "Jiaxing Lu", "Jingying Zhou", "Chen Gu"], "title": "Security-Robustness Trade-offs in Diffusion Steganography: A Comparative Analysis of Pixel-Space and VAE-Based Architectures", "comment": "13 pages", "summary": "Current generative steganography research mainly pursues computationally\nexpensive mappings to perfect Gaussian priors within single diffusion model\narchitectures. This work introduces an efficient framework based on approximate\nGaussian mapping governed by a scale factor calibrated through capacity-aware\nadaptive optimization. Using this framework as a unified analytical tool,\nsystematic comparative analysis of steganography in pixel-space models versus\nVAE-based latent-space systems is conducted. The investigation reveals a\npronounced architecture dependent security-robustness trade-off: pixel-space\nmodels achieve high security against steganalysis but exhibit fragility to\nchannel distortions, while VAE-based systems like Stable Diffusion offer\nsubstantial robustness at the cost of security vulnerabilities. Further\nanalysis indicates that the VAE component drives this behavior through opposing\nmechanisms where the encoder confers robustness via manifold regularization\nwhile the decoder introduces vulnerabilities by amplifying latent perturbations\ninto detectable artifacts. These findings characterize the conflicting\narchitectural roles in generative steganography and establish a foundation for\nfuture research.", "AI": {"tldr": "This paper introduces an efficient generative steganography framework using approximate Gaussian mapping and reveals a security-robustness trade-off between pixel-space models (high security but fragile) and VAE-based systems (robust but vulnerable).", "motivation": "Current generative steganography research focuses on computationally expensive perfect Gaussian mappings within single architectures, lacking systematic comparison between different model types.", "method": "Developed an efficient framework based on approximate Gaussian mapping with scale factor calibration through capacity-aware adaptive optimization, used as a unified tool to analyze pixel-space vs VAE-based latent-space systems.", "result": "Found architecture-dependent trade-off: pixel-space models achieve high security against steganalysis but are fragile to channel distortions, while VAE-based systems offer robustness at security cost. VAE encoder provides robustness via manifold regularization, while decoder introduces vulnerabilities.", "conclusion": "The study characterizes conflicting architectural roles in generative steganography and establishes foundation for future research on balancing security and robustness."}}
{"id": "2510.06548", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06548", "abs": "https://arxiv.org/abs/2510.06548", "authors": ["Seng Pei Liew", "Takuya Kato"], "title": "From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining", "comment": "22 pages, 11 figures, an abridged version to appear in NeurIPS 2025\n  LLM Evaluation Workshop", "summary": "Bootstrapped pretraining, i.e., the reuse of a pretrained base model for\nfurther pretraining, such as continual pretraining or model growth, is\npromising at reducing the cost of training language models from scratch.\nHowever, its effectiveness remains unclear, especially when applied to\novertrained base models. In this work, we empirically study the scaling\nbehavior of bootstrapped pretraining and find that its scaling efficiency\ndiminishes in a predictable manner: The scaling exponent with respect to\nsecond-stage pretraining tokens decreases logarithmically with the number of\ntokens used to pretrain the base model. The joint dependence on first- and\nsecond-stage tokens is accurately modeled by a simple scaling law. Such\nsaturation effect reveals a fundamental trade-off in multi-stage pretraining\nstrategies: the more extensively a model is pretrained, the less additional\nbenefit bootstrapping provides. Our findings provide practical insights for\nefficient language model training and raise important considerations for the\nreuse of overtrained models.", "AI": {"tldr": "Bootstrapped pretraining's effectiveness diminishes predictably as base models are overtrained, following a scaling law where benefits decrease logarithmically with more base pretraining tokens.", "motivation": "To understand the effectiveness of bootstrapped pretraining (reusing pretrained models for further training) and how it scales, especially when applied to overtrained base models.", "method": "Empirical study of bootstrapped pretraining scaling behavior, analyzing how scaling exponent decreases with base model pretraining tokens and developing a simple scaling law model.", "result": "Found that bootstrapped pretraining scaling efficiency diminishes predictably - scaling exponent decreases logarithmically with base model pretraining tokens, revealing a saturation effect.", "conclusion": "There's a fundamental trade-off in multi-stage pretraining: more extensive base model pretraining reduces bootstrapping benefits, providing practical insights for efficient LM training and reuse of overtrained models."}}
{"id": "2510.06525", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06525", "abs": "https://arxiv.org/abs/2510.06525", "authors": ["Ali Naseh", "Anshuman Suri", "Yuefeng Peng", "Harsh Chaudhari", "Alina Oprea", "Amir Houmansadr"], "title": "Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security", "comment": "Accepted at Lock-LLM Workshop, NeurIPS 2025", "summary": "Generative AI leaderboards are central to evaluating model capabilities, but\nremain vulnerable to manipulation. Among key adversarial objectives is rank\nmanipulation, where an attacker must first deanonymize the models behind\ndisplayed outputs -- a threat previously demonstrated and explored for large\nlanguage models (LLMs). We show that this problem can be even more severe for\ntext-to-image leaderboards, where deanonymization is markedly easier. Using\nover 150,000 generated images from 280 prompts and 19 diverse models spanning\nmultiple organizations, architectures, and sizes, we demonstrate that simple\nreal-time classification in CLIP embedding space identifies the generating\nmodel with high accuracy, even without prompt control or historical data. We\nfurther introduce a prompt-level separability metric and identify prompts that\nenable near-perfect deanonymization. Our results indicate that rank\nmanipulation in text-to-image leaderboards is easier than previously\nrecognized, underscoring the need for stronger defenses.", "AI": {"tldr": "Text-to-image leaderboards are highly vulnerable to model deanonymization attacks using simple CLIP-based classification, enabling easy rank manipulation.", "motivation": "To demonstrate that text-to-image leaderboards are more vulnerable to deanonymization attacks than LLM leaderboards, making rank manipulation easier.", "method": "Used 150,000+ images from 280 prompts and 19 diverse models, performing real-time classification in CLIP embedding space without prompt control or historical data.", "result": "Simple CLIP-based classification achieves high accuracy in identifying generating models, with some prompts enabling near-perfect deanonymization.", "conclusion": "Rank manipulation in text-to-image leaderboards is easier than previously recognized, highlighting the need for stronger defensive measures."}}
{"id": "2510.07091", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07091", "abs": "https://arxiv.org/abs/2510.07091", "authors": ["Baixuan Xu", "Tianshi Zheng", "Zhaowei Wang", "Hong Ting Tsang", "Weiqi Wang", "Tianqing Fang", "Yangqiu Song"], "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas", "comment": "22 pages", "summary": "Enabling LLMs to effectively operate long-horizon task which requires\nlong-term planning and multiple interactions is essential for open-world\nautonomy. Conventional methods adopt planning with actions where a executable\naction list would be provided as reference. However, this action representation\nchoice would be impractical when the environment action space is combinatorial\nexploded (e.g., open-ended real world). This naturally leads to a question: As\nenvironmental action space scales, what is the optimal action representation\nfor long-horizon agents? In this paper, we systematically study the\neffectiveness of two different action representations. The first one is\nconventional planning with actions (PwA) which is predominantly adopted for its\neffectiveness on existing benchmarks. The other one is planning with schemas\n(PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ]\nto [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable\nscalability. This alternative is motivated by its alignment with human\ncognition and its compliance with environment-imposed action format\nrestriction. We propose cognitive bandwidth perspective as a conceptual\nframework to qualitatively understand the differences between these two action\nrepresentations and empirically observe a representation-choice inflection\npoint between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve\nas evidence of the need for scalable representations. We further conduct\ncontrolled experiments to study how the location of this inflection point\ninteracts with different model capacities: stronger planning proficiency shifts\nthe inflection rightward, whereas better schema instantiation shifts it\nleftward. Finally, noting the suboptimal performance of PwS agents, we provide\nan actionable guide for building more capable PwS agents for better scalable\nautonomy.", "AI": {"tldr": "This paper studies optimal action representations for long-horizon agents in combinatorial action spaces, comparing Planning with Actions (PwA) vs Planning with Schemas (PwS), identifying an inflection point between different environment complexities.", "motivation": "Conventional action-based planning becomes impractical when environmental action spaces are combinatorially exploded (e.g., open-ended real world), necessitating scalable action representations for long-horizon autonomy.", "method": "Systematic comparison of two action representations: PwA (direct action lists) and PwS (schema instantiation). Proposed cognitive bandwidth framework and conducted controlled experiments across ALFWorld (~35 actions) and SciWorld (~500 actions) environments.", "result": "Identified a representation-choice inflection point between ALFWorld and SciWorld, showing PwA works better for smaller action spaces while PwS becomes necessary for larger spaces. Model capacity affects inflection point location: stronger planning shifts it rightward, better schema instantiation shifts it leftward.", "conclusion": "PwS provides better scalability for combinatorial action spaces but currently has suboptimal performance. The paper provides actionable guidance for building more capable PwS agents to achieve better scalable autonomy."}}
{"id": "2510.06552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06552", "abs": "https://arxiv.org/abs/2510.06552", "authors": ["Tarek Naous", "Philippe Laban", "Wei Xu", "Jennifer Neville"], "title": "Flipping the Dialogue: Training and Evaluating User Language Models", "comment": null, "summary": "Conversations with LMs involve two participants: a human user leading the\nconversation, and an LM assistant responding to the user's request. To satisfy\nthis specific role, LMs are post-trained to be helpful assistants -- optimized\nto produce exhaustive and well-structured responses, free of ambiguity and\ngrammar errors. User utterances, on the other hand, are rarely perfected, with\neach user phrasing requests in unique ways, sometimes putting in partial effort\nat each turn and refining on the fly. To evaluate LM performance in realistic\nsettings, prior work simulated users in multi-turn conversations, often\nprompting an LLM originally trained to be a helpful assistant to act as a user.\nHowever, we show that assistant LMs make for poor user simulators, with the\nsurprising finding that better assistants yield worse simulators. Instead, we\nintroduce purpose-built User Language Models (User LMs) - models post-trained\nto simulate human users in multi-turn conversations. Through various\nevaluations, we show how User LMs align better with human behavior and achieve\nbetter simulation robustness than existing simulation methods. When leveraging\nUser LMs to simulate coding and math conversations, the performance of a strong\nassistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic\nsimulation environments lead to assistant struggles as they fail to cope with\nthe nuances of users in multi-turn setups.", "AI": {"tldr": "Assistant LMs make poor user simulators - better assistants yield worse simulators. Purpose-built User LMs better simulate human users and reveal assistant struggles in realistic multi-turn conversations.", "motivation": "To evaluate LM performance in realistic settings, need better user simulators since assistant LMs are poor at simulating human users despite being trained as helpful assistants.", "method": "Introduce purpose-built User Language Models (User LMs) - models post-trained specifically to simulate human users in multi-turn conversations.", "result": "User LMs align better with human behavior and achieve better simulation robustness. When used to simulate coding/math conversations, GPT-4o's performance drops from 74.6% to 57.4%.", "conclusion": "More realistic simulation environments using User LMs reveal that assistants struggle to cope with nuances of real users in multi-turn conversations."}}
{"id": "2510.06527", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06527", "abs": "https://arxiv.org/abs/2510.06527", "authors": ["John Dunbar", "Scott Aaronson"], "title": "Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture", "comment": null, "summary": "We establish that randomly initialized neural networks, with large width and\na natural choice of hyperparameters, have nearly independent outputs exactly\nwhen their activation function is nonlinear with zero mean under the Gaussian\nmeasure: $\\mathbb{E}_{z \\sim \\mathcal{N}(0,1)}[\\sigma(z)]=0$. For example, this\nincludes ReLU and GeLU with an additive shift, as well as tanh, but not ReLU or\nGeLU by themselves. Because of their nearly independent outputs, we propose\nneural networks with zero-mean activation functions as a promising candidate\nfor the Alignment Research Center's computational no-coincidence conjecture --\na conjecture that aims to measure the limits of AI interpretability.", "AI": {"tldr": "Randomly initialized wide neural networks with zero-mean activation functions (like shifted ReLU/GeLU, tanh) produce nearly independent outputs, making them suitable for testing interpretability limits.", "motivation": "To understand when neural networks produce independent outputs and test the Alignment Research Center's computational no-coincidence conjecture about AI interpretability limits.", "method": "Analyze randomly initialized neural networks with large width and zero-mean activation functions under Gaussian measure.", "result": "Networks with activation functions satisfying E[\u03c3(z)]=0 (e.g., shifted ReLU/GeLU, tanh) produce nearly independent outputs, while standard ReLU/GeLU do not.", "conclusion": "Zero-mean activation functions enable nearly independent network outputs, making them promising for testing interpretability conjectures."}}
{"id": "2510.07117", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07117", "abs": "https://arxiv.org/abs/2510.07117", "authors": ["Leonardo Christov-Moore", "Arthur Juliani", "Alex Kiefer", "Nicco Reggente", "B. Scott Rousse", "Adam Safron", "Nicol'as Hinrichs", "Daniel Polani", "Antonio Damasio"], "title": "The Contingencies of Physical Embodiment Allow for Open-Endedness and Care", "comment": "15 pages, 1 figure", "summary": "Physical vulnerability and mortality are often seen as obstacles to be\navoided in the development of artificial agents, which struggle to adapt to\nopen-ended environments and provide aligned care. Meanwhile, biological\norganisms survive, thrive, and care for each other in an open-ended physical\nworld with relative ease and efficiency. Understanding the role of the\nconditions of life in this disparity can aid in developing more robust,\nadaptive, and caring artificial agents. Here we define two minimal conditions\nfor physical embodiment inspired by the existentialist phenomenology of Martin\nHeidegger: being-in-the-world (the agent is a part of the environment) and\nbeing-towards-death (unless counteracted, the agent drifts toward terminal\nstates due to the second law of thermodynamics). We propose that from these\nconditions we can obtain both a homeostatic drive - aimed at maintaining\nintegrity and avoiding death by expending energy to learn and act - and an\nintrinsic drive to continue to do so in as many ways as possible. Drawing\ninspiration from Friedrich Nietzsche's existentialist concept of will-to-power,\nwe examine how intrinsic drives to maximize control over future states, e.g.,\nempowerment, allow agents to increase the probability that they will be able to\nmeet their future homeostatic needs, thereby enhancing their capacity to\nmaintain physical integrity. We formalize these concepts within a reinforcement\nlearning framework, which enables us to examine how intrinsically driven\nembodied agents learning in open-ended multi-agent environments may cultivate\nthe capacities for open-endedness and care.ov", "AI": {"tldr": "The paper proposes that physical vulnerability and mortality (being-towards-death) combined with embodiment (being-in-the-world) create intrinsic drives for artificial agents to maintain integrity and maximize control, enabling open-ended adaptation and care.", "motivation": "Biological organisms adapt and care for each other in open-ended environments with ease, while artificial agents struggle. Understanding the conditions of life can help develop more robust, adaptive, and caring AI agents.", "method": "Defines two minimal conditions from Heidegger's phenomenology: being-in-the-world and being-towards-death. Formalizes these concepts in reinforcement learning framework to examine how intrinsically driven embodied agents develop capacities for open-endedness and care.", "result": "From these conditions emerge both homeostatic drive (maintaining integrity) and intrinsic drive to maximize control over future states (empowerment), enhancing agents' ability to meet future needs.", "conclusion": "Physical vulnerability and mortality, when properly framed as existential conditions, can drive artificial agents to develop robust adaptation, open-ended learning, and caring behaviors similar to biological organisms."}}
{"id": "2510.06559", "categories": ["cs.CL", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.06559", "abs": "https://arxiv.org/abs/2510.06559", "authors": ["Cheonkam Jeong", "Sungdo Kim", "Jewoo Park"], "title": "The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law", "comment": null, "summary": "Contemporary language models are fluent yet routinely mis-handle the types of\nmeaning their outputs entail. We argue that hallucination, brittle moderation,\nand opaque compliance outcomes are symptoms of missing type-theoretic semantics\nrather than data or scale limitations. Building on Montague's view of language\nas typed, compositional algebra, we recast alignment as a parsing problem:\nnatural-language inputs must be compiled into structures that make explicit\ntheir descriptive, normative, and legal dimensions under context.\n  We present Savassan, a neuro-symbolic architecture that compiles utterances\ninto Montague-style logical forms and maps them to typed ontologies extended\nwith deontic operators and jurisdictional contexts. Neural components extract\ncandidate structures from unstructured inputs; symbolic components perform type\nchecking, constraint reasoning, and cross-jurisdiction mapping to produce\ncompliance-aware guidance rather than binary censorship. In cross-border\nscenarios, the system \"parses once\" (e.g., defect claim(product x, company y))\nand projects the result into multiple legal ontologies (e.g., defamation risk\nin KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into\na single, explainable decision.\n  This paper contributes: (i) a diagnosis of hallucination as a type error;\n(ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii)\na production-oriented design that embeds typed interfaces across the pipeline.\nWe outline an evaluation plan using legal reasoning benchmarks and synthetic\nmulti-jurisdiction suites. Our position is that trustworthy autonomy requires\ncompositional typing of meaning, enabling systems to reason about what is\ndescribed, what is prescribed, and what incurs liability within a unified\nalgebra of meaning.", "AI": {"tldr": "The paper presents Savassan, a neuro-symbolic system that treats language model alignment as a parsing problem, compiling utterances into typed logical forms with deontic operators to address hallucination and compliance issues across jurisdictions.", "motivation": "Current language models mis-handle semantic types in their outputs, leading to hallucination, brittle moderation, and opaque compliance outcomes. The authors argue these are type-theoretic semantic problems rather than data or scale limitations.", "method": "Savassan architecture: neural components extract candidate structures from inputs; symbolic components perform type checking, constraint reasoning, and cross-jurisdiction mapping using Montague-style logical forms and typed ontologies with deontic operators.", "result": "The system can parse once and project results into multiple legal ontologies (e.g., defamation risk in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into explainable decisions rather than binary censorship.", "conclusion": "Trustworthy autonomy requires compositional typing of meaning, enabling systems to reason about descriptive, normative, and legal dimensions within a unified algebra of meaning, addressing hallucination as a type error."}}
{"id": "2510.06540", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06540", "abs": "https://arxiv.org/abs/2510.06540", "authors": ["Ameya Anjarlekar", "Rasoul Etesami", "R Srikant"], "title": "Scalable Policy-Based RL Algorithms for POMDPs", "comment": "36 pages, 3 Figures, Accepted at NeurIPS 2025", "summary": "The continuous nature of belief states in POMDPs presents significant\ncomputational challenges in learning the optimal policy. In this paper, we\nconsider an approach that solves a Partially Observable Reinforcement Learning\n(PORL) problem by approximating the corresponding POMDP model into a\nfinite-state Markov Decision Process (MDP) (called Superstate MDP). We first\nderive theoretical guarantees that improve upon prior work that relate the\noptimal value function of the transformed Superstate MDP to the optimal value\nfunction of the original POMDP. Next, we propose a policy-based learning\napproach with linear function approximation to learn the optimal policy for the\nSuperstate MDP. Consequently, our approach shows that a POMDP can be\napproximately solved using TD-learning followed by Policy Optimization by\ntreating it as an MDP, where the MDP state corresponds to a finite history. We\nshow that the approximation error decreases exponentially with the length of\nthis history. To the best of our knowledge, our finite-time bounds are the\nfirst to explicitly quantify the error introduced when applying standard TD\nlearning to a setting where the true dynamics are not Markovian.", "AI": {"tldr": "This paper proposes transforming POMDPs into finite-state Superstate MDPs using finite histories, enabling approximate solution via TD-learning and policy optimization with provable error bounds.", "motivation": "To address computational challenges in solving POMDPs due to continuous belief states by approximating them as finite-state MDPs.", "method": "Transform POMDP into Superstate MDP using finite histories, apply TD-learning with linear function approximation, followed by policy optimization.", "result": "Derived improved theoretical guarantees showing approximation error decreases exponentially with history length, providing first finite-time bounds for TD-learning in non-Markovian settings.", "conclusion": "POMDPs can be effectively approximated and solved using standard MDP techniques with finite histories, with provable performance guarantees."}}
{"id": "2510.07161", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07161", "abs": "https://arxiv.org/abs/2510.07161", "authors": ["Ali Norouzifar", "Humam Kourani", "Marcus Dees", "Wil van der Aalst"], "title": "Integrating Domain Knowledge into Process Discovery Using Large Language Models", "comment": "This paper is currently under review for publication in a journal", "summary": "Process discovery aims to derive process models from event logs, providing\ninsights into operational behavior and forming a foundation for conformance\nchecking and process improvement. However, models derived solely from event\ndata may not accurately reflect the real process, as event logs are often\nincomplete or affected by noise, and domain knowledge, an important\ncomplementary resource, is typically disregarded. As a result, the discovered\nmodels may lack reliability for downstream tasks. We propose an interactive\nframework that incorporates domain knowledge, expressed in natural language,\ninto the process discovery pipeline using Large Language Models (LLMs). Our\napproach leverages LLMs to extract declarative rules from textual descriptions\nprovided by domain experts. These rules are used to guide the IMr discovery\nalgorithm, which recursively constructs process models by combining insights\nfrom both the event log and the extracted rules, helping to avoid problematic\nprocess structures that contradict domain knowledge. The framework coordinates\ninteractions among the LLM, domain experts, and a set of backend services. We\npresent a fully implemented tool that supports this workflow and conduct an\nextensive evaluation of multiple LLMs and prompt engineering strategies. Our\nempirical study includes a case study based on a real-life event log with the\ninvolvement of domain experts, who assessed the usability and effectiveness of\nthe framework.", "AI": {"tldr": "An interactive process discovery framework that incorporates domain knowledge via LLMs to extract declarative rules from natural language, guiding process model discovery to avoid structures contradicting expert knowledge.", "motivation": "Traditional process discovery from event logs alone is unreliable due to incomplete/noisy data and disregard of domain knowledge, leading to models unsuitable for downstream tasks.", "method": "Interactive framework using LLMs to extract declarative rules from textual domain knowledge, integrated with IMr discovery algorithm that combines event log insights and extracted rules.", "result": "Fully implemented tool with extensive evaluation of LLMs and prompt strategies, including real-life case study where domain experts assessed usability and effectiveness.", "conclusion": "The framework successfully incorporates domain knowledge into process discovery, improving model reliability by avoiding structures that contradict expert knowledge."}}
{"id": "2510.06692", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06692", "abs": "https://arxiv.org/abs/2510.06692", "authors": ["Akira Ito", "Takayuki Miura", "Yosuke Todo"], "title": "Is the Hard-Label Cryptanalytic Model Extraction Really Polynomial?", "comment": null, "summary": "Deep Neural Networks (DNNs) have attracted significant attention, and their\ninternal models are now considered valuable intellectual assets. Extracting\nthese internal models through access to a DNN is conceptually similar to\nextracting a secret key via oracle access to a block cipher. Consequently,\ncryptanalytic techniques, particularly differential-like attacks, have been\nactively explored recently. ReLU-based DNNs are the most commonly and widely\ndeployed architectures. While early works (e.g., Crypto 2020, Eurocrypt 2024)\nassume access to exact output logits, which are usually invisible, more recent\nworks (e.g., Asiacrypt 2024, Eurocrypt 2025) focus on the hard-label setting,\nwhere only the final classification result (e.g., \"dog\" or \"car\") is available\nto the attacker. Notably, Carlini et al. (Eurocrypt 2025) demonstrated that\nmodel extraction is feasible in polynomial time even under this restricted\nsetting.\n  In this paper, we first show that the assumptions underlying their attack\nbecome increasingly unrealistic as the attack-target depth grows. In practice,\nsatisfying these assumptions requires an exponential number of queries with\nrespect to the attack depth, implying that the attack does not always run in\npolynomial time. To address this critical limitation, we propose a novel attack\nmethod called CrossLayer Extraction. Instead of directly extracting the secret\nparameters (e.g., weights and biases) of a specific neuron, which incurs\nexponential cost, we exploit neuron interactions across layers to extract this\ninformation from deeper layers. This technique significantly reduces query\ncomplexity and mitigates the limitations of existing model extraction\napproaches.", "AI": {"tldr": "This paper identifies limitations in existing hard-label model extraction attacks on ReLU-based DNNs, showing they require exponential queries for deep networks. The authors propose CrossLayer Extraction, a novel attack that exploits neuron interactions across layers to reduce query complexity.", "motivation": "Existing model extraction attacks assume unrealistic conditions for deep networks, requiring exponential queries that make them impractical. There's a need for more efficient attacks that work under realistic hard-label settings.", "method": "Proposed CrossLayer Extraction attack that exploits neuron interactions across layers instead of directly extracting individual neuron parameters. This approach avoids the exponential cost of traditional methods.", "result": "The new attack significantly reduces query complexity compared to existing approaches, making model extraction more practical for deep neural networks under hard-label settings.", "conclusion": "CrossLayer Extraction provides a more efficient and practical approach to model extraction from deep ReLU-based DNNs, overcoming the exponential query limitations of previous methods."}}
{"id": "2510.06579", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06579", "abs": "https://arxiv.org/abs/2510.06579", "authors": ["Haofei Yu", "Keyang Xuan", "Fenghai Li", "Kunlun Zhu", "Zijie Lei", "Jiaxun Zhang", "Ziheng Qi", "Kyle Richardson", "Jiaxuan You"], "title": "TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents", "comment": "7 pages, EMNLP 2025 Demo track", "summary": "Automatic research with Large Language Models (LLMs) is rapidly gaining\nimportance, driving the development of increasingly complex workflows involving\nmulti-agent systems, planning, tool usage, code execution, and human-agent\ninteraction to accelerate research processes. However, as more researchers and\ndevelopers begin to use and build upon these tools and platforms, the\ncomplexity and difficulty of extending and maintaining such agentic workflows\nhave become a significant challenge, particularly as algorithms and\narchitectures continue to advance. To address this growing complexity,\nTinyScientist identifies the essential components of the automatic research\nworkflow and proposes an interactive, extensible, and controllable framework\nthat easily adapts to new tools and supports iterative growth. We provide an\nopen-source codebase, an interactive web demonstration, and a PyPI Python\npackage to make state-of-the-art auto-research pipelines broadly accessible to\nevery researcher and developer.", "AI": {"tldr": "TinyScientist is an interactive, extensible framework that simplifies building and maintaining complex automatic research workflows using LLMs, addressing the growing complexity in multi-agent systems and research automation.", "motivation": "The increasing complexity of automatic research workflows involving multi-agent systems, planning, tool usage, and human-agent interaction has made extending and maintaining these systems challenging as algorithms advance.", "method": "Identifies essential components of automatic research workflow and proposes an interactive, extensible, and controllable framework that adapts to new tools and supports iterative growth.", "result": "Developed an open-source codebase, interactive web demonstration, and PyPI Python package to make state-of-the-art auto-research pipelines accessible to researchers and developers.", "conclusion": "TinyScientist provides a practical solution to manage the complexity of modern automatic research workflows, making advanced research automation tools broadly accessible through an extensible framework."}}
{"id": "2510.06545", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06545", "abs": "https://arxiv.org/abs/2510.06545", "authors": ["Jacek Karwowski", "Raymond Douglas"], "title": "Incoherence in goal-conditioned autoregressive models", "comment": null, "summary": "We investigate mathematically the notion of incoherence: a structural issue\nwith reinforcement learning policies derived by naive goal-conditioning of\nautoregressive models. We focus on the process of re-training models on their\nown actions, that is, fine-tuning offline-learned policies with online RL. We\nprove that it decreases incoherence and leads to an improvement in return, and\nwe aim to characterize the resulting trajectory of policies. By re-framing\nstandard notions of control-as-inference and soft Q learning, we establish a\nthree-way correspondence with two other ways of understanding the iterative\nre-training process: as folding the posterior into the reward and, in the\ndeterministic case, as decreasing the temperature parameter; the correspondence\nhas computational content via the training-inference trade-off. Through\nsoft-conditioning generative models, we discuss the link between incoherence\nand the effective horizon.", "AI": {"tldr": "This paper mathematically analyzes incoherence in reinforcement learning policies from naive goal-conditioning of autoregressive models, showing that online RL fine-tuning reduces incoherence and improves performance, with connections to control-as-inference and soft Q learning.", "motivation": "To understand the structural issue of incoherence in RL policies derived from naive goal-conditioning of autoregressive models, and to characterize how online RL fine-tuning addresses this problem.", "method": "Mathematical investigation of incoherence, re-framing control-as-inference and soft Q learning concepts, establishing three-way correspondence between different interpretations of iterative re-training, and analyzing soft-conditioning generative models.", "result": "Proved that online RL fine-tuning decreases incoherence and improves return, established correspondence between iterative re-training as folding posterior into reward and decreasing temperature parameter, and linked incoherence to effective horizon.", "conclusion": "Online RL fine-tuning effectively addresses incoherence in goal-conditioned policies, with mathematical connections to established RL frameworks and implications for training-inference trade-offs and effective horizon considerations."}}
{"id": "2510.07172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07172", "abs": "https://arxiv.org/abs/2510.07172", "authors": ["Tianshi Zheng", "Kelvin Kiu-Wai Tam", "Newt Hue-Nam K. Nguyen", "Baixuan Xu", "Zhaowei Wang", "Jiayang Cheng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Tianqing Fang", "Yangqiu Song", "Ginny Y. Wong", "Simon See"], "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM Agents", "comment": "60 pages, 18 figures, 13 tables", "summary": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery.", "AI": {"tldr": "NewtonBench is a new benchmark with 324 scientific law discovery tasks across 12 physics domains that addresses limitations of existing benchmarks by using metaphysical shifts to create scalable, scientifically relevant, and memorization-resistant problems, while elevating evaluation from static function fitting to interactive model discovery.", "motivation": "Existing benchmarks for scientific law discovery suffer from a methodological trilemma forcing trade-offs between scientific relevance, scalability, and resistance to memorization, and oversimplify discovery as static function fitting rather than capturing the authentic interactive scientific process.", "method": "The benchmark uses metaphysical shifts - systematic alterations of canonical laws - to generate problems, and elevates evaluation to interactive model discovery where agents must experimentally probe simulated complex systems to uncover hidden principles.", "result": "Experiments reveal frontier LLMs have clear but fragile discovery capability that degrades with system complexity and is sensitive to observational noise. Tool assistance paradoxically hinders capable models by causing premature shift from exploration to exploitation.", "conclusion": "Robust, generalizable discovery in complex interactive environments remains a core challenge. NewtonBench provides a scalable, robust, and scientifically authentic testbed for measuring progress and guiding development of AI agents capable of genuine scientific discovery."}}
{"id": "2510.06594", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06594", "abs": "https://arxiv.org/abs/2510.06594", "authors": ["Sri Durga Sai Sowmya Kadali", "Evangelos E. Papalexakis"], "title": "Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?", "comment": null, "summary": "Jailbreaking large language models (LLMs) has emerged as a pressing concern\nwith the increasing prevalence and accessibility of conversational LLMs.\nAdversarial users often exploit these models through carefully engineered\nprompts to elicit restricted or sensitive outputs, a strategy widely referred\nto as jailbreaking. While numerous defense mechanisms have been proposed,\nattackers continuously develop novel prompting techniques, and no existing\nmodel can be considered fully resistant. In this study, we investigate the\njailbreak phenomenon by examining the internal representations of LLMs, with a\nfocus on how hidden layers respond to jailbreak versus benign prompts.\nSpecifically, we analyze the open-source LLM GPT-J and the state-space model\nMamba2, presenting preliminary findings that highlight distinct layer-wise\nbehaviors. Our results suggest promising directions for further research on\nleveraging internal model dynamics for robust jailbreak detection and defense.", "AI": {"tldr": "This paper investigates jailbreaking in large language models by analyzing internal representations, focusing on how hidden layers respond differently to jailbreak vs benign prompts in GPT-J and Mamba2 models.", "motivation": "Jailbreaking LLMs is a critical security concern as adversarial users exploit carefully engineered prompts to elicit restricted outputs, and existing defenses are insufficient against evolving attack techniques.", "method": "Examine internal representations of LLMs by analyzing how hidden layers respond to jailbreak versus benign prompts, specifically studying GPT-J and Mamba2 models.", "result": "Preliminary findings show distinct layer-wise behaviors in how models process jailbreak prompts compared to benign ones, revealing differences in internal model dynamics.", "conclusion": "The results suggest promising directions for future research on leveraging internal model dynamics for robust jailbreak detection and defense mechanisms."}}
{"id": "2510.06557", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06557", "abs": "https://arxiv.org/abs/2510.06557", "authors": ["Milad Aghajohari", "Kamran Chitsaz", "Amirhossein Kazemnejad", "Sarath Chandar", "Alessandro Sordoni", "Aaron Courville", "Siva Reddy"], "title": "The Markovian Thinker", "comment": null, "summary": "Reinforcement learning (RL) has recently become a strong recipe for training\nreasoning LLMs that produce long chains of thought (LongCoT). Yet the standard\nRL \"thinking environment\", where the state is the prompt plus all prior\nreasoning tokens, makes the state unbounded and forces attention-based policies\nto pay quadratic compute as thoughts lengthen. We revisit the environment\nitself. We propose Markovian Thinking, a paradigm in which the policy advances\nreasoning while conditioning on a constant-size state, decoupling thinking\nlength from context size. As an immediate consequence this yields linear\ncompute with constant memory. We instantiate this idea with Delethink, an RL\nenvironment that structures reasoning into fixed-size chunks. Within each\nchunk, the model thinks as usual; at the boundary, the environment resets the\ncontext and reinitializes the prompt with a short carryover. Through RL, the\npolicy learns to write a textual state near the end of each chunk sufficient\nfor seamless continuation of reasoning after reset. Trained in this\nenvironment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up\nto 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.\nWith test-time scaling, Delethink continues to improve where LongCoT plateaus.\nThe effect of linear compute is substantial: we empirically estimate at 96K\naverage thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.\nAnalysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)\noften sample Markovian traces zero-shot across diverse benchmarks, providing\npositive samples that make RL effective at scale. Our results show that\nredesigning the thinking environment is a powerful lever: it enables very long\nreasoning without quadratic overhead and opens a path toward efficient,\nscalable reasoning LLMs.", "AI": {"tldr": "Delethink enables efficient long-chain reasoning in LLMs by using Markovian Thinking with fixed-size chunks, achieving linear compute scaling instead of quadratic, while matching or surpassing traditional LongCoT-RL performance.", "motivation": "Standard RL for reasoning LLMs suffers from quadratic compute overhead as thought chains lengthen due to unbounded state size, making long reasoning sequences computationally expensive.", "method": "Proposes Markovian Thinking paradigm with Delethink RL environment that structures reasoning into fixed-size chunks, resetting context at boundaries with short carryover text, allowing policies to learn seamless continuation.", "result": "1.5B model achieves 24K token reasoning with 8K chunks, matching 24K LongCoT-RL performance. At 96K thinking length, Delethink costs 7 H100-months vs 27 for LongCoT-RL, with continued scaling where LongCoT plateaus.", "conclusion": "Redesigning the thinking environment enables efficient long reasoning without quadratic overhead, providing a path toward scalable reasoning LLMs, with existing models showing Markovian capabilities zero-shot."}}
{"id": "2510.07276", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.07276", "abs": "https://arxiv.org/abs/2510.07276", "authors": ["Pulkit Rustagi", "Kyle Hollins Wray", "Sandhya Saisubramanian"], "title": "Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences", "comment": "8 pages, 7 figures", "summary": "Many real-world scenarios require multiple agents to coordinate in shared\nenvironments, while balancing trade-offs between multiple, potentially\ncompeting objectives. Current multi-objective multi-agent path finding\n(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto\nfrontiers. They do not explicitly optimize for user-defined preferences, even\nwhen the preferences are available, and scale poorly with the number of\nobjectives. We propose a lexicographic framework for modeling MO-MAPF, along\nwith an algorithm \\textit{Lexicographic Conflict-Based Search} (LCBS) that\ndirectly computes a single solution aligned with a lexicographic preference\nover objectives. LCBS integrates a priority-aware low-level $A^*$ search with\nconflict-based search, avoiding Pareto frontier construction and enabling\nefficient planning guided by preference over objectives. We provide insights\ninto optimality and scalability, and empirically demonstrate that LCBS computes\noptimal solutions while scaling to instances with up to ten objectives -- far\nbeyond the limits of existing MO-MAPF methods. Evaluations on standard and\nrandomized MAPF benchmarks show consistently higher success rates against\nstate-of-the-art baselines, especially with increasing number of objectives.", "AI": {"tldr": "Proposes Lexicographic Conflict-Based Search (LCBS) for multi-objective multi-agent path finding that directly computes solutions aligned with user preferences, avoiding Pareto frontier construction and scaling to 10 objectives.", "motivation": "Current MO-MAPF algorithms don't optimize for user-defined preferences even when available, and scale poorly with increasing objectives by computing Pareto frontiers.", "method": "LCBS integrates priority-aware low-level A* search with conflict-based search, using lexicographic preferences over objectives to guide planning without constructing Pareto frontiers.", "result": "LCBS computes optimal solutions and scales to instances with up to ten objectives, showing consistently higher success rates than state-of-the-art baselines, especially with more objectives.", "conclusion": "The lexicographic framework and LCBS algorithm enable efficient multi-objective planning that directly incorporates user preferences and scales beyond current MO-MAPF methods."}}
{"id": "2510.06640", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06640", "abs": "https://arxiv.org/abs/2510.06640", "authors": ["Nhat M. Hoang", "Do Xuan Long", "Cong-Duy Nguyen", "Min-Yen Kan", "Luu Anh Tuan"], "title": "A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures", "comment": null, "summary": "State Space Models (SSMs) have recently emerged as efficient alternatives to\nTransformer-Based Models (TBMs) for long-sequence processing, offering linear\nscaling and lower memory use. Yet, how contextual information flows across\nlayers and tokens in these architectures remains understudied. We present the\nfirst unified, token- and layer-level analysis of representation propagation in\nSSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,\nwe characterize how representations evolve within and across layers. We find a\nkey divergence: TBMs rapidly homogenize token representations, with diversity\nreemerging only in later layers, while SSMs preserve token uniqueness early but\nconverge to homogenization deeper. Theoretical analysis and parameter\nrandomization further reveal that oversmoothing in TBMs stems from\narchitectural design, whereas in SSMs it arises mainly from training dynamics.\nThese insights clarify the inductive biases of both architectures and inform\nfuture model and training designs for long-context reasoning.", "AI": {"tldr": "This paper provides the first unified analysis of representation propagation in State Space Models (SSMs) vs Transformer-Based Models (TBMs), revealing key differences in how they handle token diversity across layers.", "motivation": "While SSMs have emerged as efficient alternatives to TBMs for long-sequence processing, there's limited understanding of how contextual information flows across layers and tokens in these architectures.", "method": "Used centered kernel alignment, stability metrics, and probing to analyze representation evolution within and across layers, plus theoretical analysis and parameter randomization.", "result": "Found TBMs rapidly homogenize token representations early (with diversity reemerging later), while SSMs preserve token uniqueness early but converge to homogenization deeper. Oversmoothing in TBMs stems from architectural design, while in SSMs it comes from training dynamics.", "conclusion": "These insights clarify the inductive biases of both architectures and inform future model and training designs for long-context reasoning."}}
{"id": "2510.06567", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.06567", "abs": "https://arxiv.org/abs/2510.06567", "authors": ["Yao Chen", "David Ohlssen", "Aimee Readie", "Gregory Ligozio", "Ruvie Martin", "Thibaud Coroller"], "title": "The Framework That Survives Bad Models: Human-AI Collaboration For Clinical Trials", "comment": null, "summary": "Artificial intelligence (AI) holds great promise for supporting clinical\ntrials, from patient recruitment and endpoint assessment to treatment response\nprediction. However, deploying AI without safeguards poses significant risks,\nparticularly when evaluating patient endpoints that directly impact trial\nconclusions. We compared two AI frameworks against human-only assessment for\nmedical image-based disease evaluation, measuring cost, accuracy, robustness,\nand generalization ability. To stress-test these frameworks, we injected bad\nmodels, ranging from random guesses to naive predictions, to ensure that\nobserved treatment effects remain valid even under severe model degradation. We\nevaluated the frameworks using two randomized controlled trials with endpoints\nderived from spinal X-ray images. Our findings indicate that using AI as a\nsupporting reader (AI-SR) is the most suitable approach for clinical trials, as\nit meets all criteria across various model types, even with bad models. This\nmethod consistently provides reliable disease estimation, preserves clinical\ntrial treatment effect estimates and conclusions, and retains these advantages\nwhen applied to different populations.", "AI": {"tldr": "AI-SR (AI as supporting reader) is the most suitable AI framework for clinical trials as it maintains reliable disease estimation and preserves treatment effect estimates even with bad models, while meeting cost, accuracy, robustness, and generalization criteria.", "motivation": "To address the risks of deploying AI in clinical trials without safeguards, particularly when AI evaluates patient endpoints that directly impact trial conclusions.", "method": "Compared two AI frameworks against human-only assessment for medical image-based disease evaluation, measuring cost, accuracy, robustness, and generalization. Stress-tested by injecting bad models (random guesses to naive predictions) to ensure treatment effects remain valid under model degradation.", "result": "AI-SR framework consistently provided reliable disease estimation, preserved clinical trial treatment effect estimates and conclusions, and retained advantages when applied to different populations, even with bad models.", "conclusion": "Using AI as a supporting reader is the most suitable approach for clinical trials as it meets all criteria across various model types and maintains reliability under severe model degradation."}}
{"id": "2510.07297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07297", "abs": "https://arxiv.org/abs/2510.07297", "authors": ["Henry Wang", "Sirajus Salekin", "Jake Lee", "Ross Claytor", "Shinan Zhang", "Michael Chi"], "title": "Agentic generative AI for media content discovery at the national football league", "comment": "13 pages, 7 figures, International Sports Analytics Conference and\n  Exhibition", "summary": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.", "AI": {"tldr": "A generative AI workflow for the NFL that converts natural language queries into database queries to find relevant historical plays, achieving 95% accuracy and reducing search time from 10 minutes to 30 seconds.", "motivation": "To enable media researchers and analysts to query historical NFL plays using natural language instead of traditional filter-and-click interfaces, improving content discovery and management.", "method": "An agentic workflow that takes user queries, breaks them into elements, translates them into database query language, and uses semantic caching to improve accuracy and latency.", "result": "The solution achieves over 95% accuracy and reduces average search time from 10 minutes to 30 seconds, significantly increasing operational efficiency.", "conclusion": "Generative AI enables efficient content discovery, allowing NFL analysts to focus on creative content production and engaging storylines rather than time-consuming search tasks."}}
{"id": "2510.06652", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06652", "abs": "https://arxiv.org/abs/2510.06652", "authors": ["Shangjian Yin", "Zhepei Wei", "Xinyu Zhu", "Wei-Lin Chen", "Yu Meng"], "title": "Aligning Large Language Models via Fully Self-Synthetic Data", "comment": null, "summary": "Traditional reinforcement learning from human feedback (RLHF) for large\nlanguage models (LLMs) relies on expensive human-annotated datasets, while\nReinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,\nrequiring the collection of diverse prompts and corresponding responses, often\nnecessitating external reward models or proprietary models like GPT-4 to\nannotate preference pairs. In this work, we introduce Self-Alignment\nOptimization (SAO), a fully self-synthetic framework for LLM alignment, where\nall training data, including prompts (i.e., user queries), responses, and\npreferences, are generated by the model itself. Specifically, SAO first\ninstructs the LLM to engage in persona role-play and generate diverse prompts\nand responses, which are then self-evaluated for preference optimization.\nExtensive experiments demonstrate that SAO effectively enhances the model's\nchat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining\nstrong performance on downstream objective tasks (e.g., question-answering,\nmath reasoning). Our work provides a practical solution for self-improvement in\naligning LLMs, and the code for reproducing our results is available at:\nhttps://github.com/SJY8460/SAO.", "AI": {"tldr": "SAO is a fully self-synthetic framework for LLM alignment that generates all training data (prompts, responses, preferences) internally without human or external model annotation, improving chat capabilities while maintaining performance on objective tasks.", "motivation": "Traditional RLHF requires expensive human-annotated datasets, and RLAIF also incurs significant costs from collecting diverse prompts/responses and using external reward models. There's a need for a more cost-effective self-alignment approach.", "method": "SAO instructs LLMs to engage in persona role-play to generate diverse prompts and responses, then self-evaluates these for preference optimization - creating a completely self-contained training loop.", "result": "Extensive experiments show SAO effectively enhances model chat capabilities on benchmarks like AlpacaEval 2.0 while maintaining strong performance on downstream objective tasks (question-answering, math reasoning).", "conclusion": "SAO provides a practical solution for self-improvement in aligning LLMs, offering a cost-effective alternative to traditional RLHF and RLAIF approaches."}}
{"id": "2510.06623", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06623", "abs": "https://arxiv.org/abs/2510.06623", "authors": ["Canyu Lei", "Benjamin Lobo", "Jianxin Xie"], "title": "DPA-Net: A Dual-Path Attention Neural Network for Inferring Glycemic Control Metrics from Self-Monitored Blood Glucose Data", "comment": "14 pages, 10 figures", "summary": "Continuous glucose monitoring (CGM) provides dense and dynamic glucose\nprofiles that enable reliable estimation of Ambulatory Glucose Profile (AGP)\nmetrics, such as Time in Range (TIR), Time Below Range (TBR), and Time Above\nRange (TAR). However, the high cost and limited accessibility of CGM restrict\nits widespread adoption, particularly in low- and middle-income regions. In\ncontrast, self-monitoring of blood glucose (SMBG) is inexpensive and widely\navailable but yields sparse and irregular data that are challenging to\ntranslate into clinically meaningful glycemic metrics.\n  In this work, we propose a Dual-Path Attention Neural Network (DPA-Net) to\nestimate AGP metrics directly from SMBG data. DPA-Net integrates two\ncomplementary paths: (1) a spatial-channel attention path that reconstructs a\nCGM-like trajectory from sparse SMBG observations, and (2) a multi-scale ResNet\npath that directly predicts AGP metrics. An alignment mechanism between the two\npaths is introduced to reduce bias and mitigate overfitting. In addition, we\ndevelop an active point selector to identify realistic and informative SMBG\nsampling points that reflect patient behavioral patterns.\n  Experimental results on a large, real-world dataset demonstrate that DPA-Net\nachieves robust accuracy with low errors while reducing systematic bias. To the\nbest of our knowledge, this is the first supervised machine learning framework\nfor estimating AGP metrics from SMBG data, offering a practical and clinically\nrelevant decision-support tool in settings where CGM is not accessible.", "AI": {"tldr": "DPA-Net estimates AGP metrics from sparse SMBG data using dual-path attention network with spatial-channel reconstruction and multi-scale ResNet, achieving robust accuracy without requiring expensive CGM.", "motivation": "CGM provides accurate glucose monitoring but is expensive and inaccessible in low-income regions, while SMBG is widely available but produces sparse data that can't generate clinically meaningful AGP metrics.", "method": "Dual-Path Attention Neural Network (DPA-Net) with spatial-channel attention path for CGM-like trajectory reconstruction and multi-scale ResNet path for direct AGP prediction, plus alignment mechanism and active point selector.", "result": "DPA-Net achieves robust accuracy with low errors and reduced systematic bias on large real-world dataset, successfully estimating AGP metrics from SMBG data.", "conclusion": "First supervised ML framework for estimating AGP metrics from SMBG data, providing practical decision-support tool where CGM is inaccessible."}}
{"id": "2510.06664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06664", "abs": "https://arxiv.org/abs/2510.06664", "authors": ["Yunzhong Xiao", "Yangmin Li", "Hewei Wang", "Yunlong Tang", "Zora Zhiruo Wang"], "title": "ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory", "comment": null, "summary": "Agents utilizing tools powered by large language models (LLMs) or\nvision-language models (VLMs) have demonstrated remarkable progress in diverse\ntasks across text and visual modalities. Unlike traditional tools such as\ncalculators, which give deterministic outputs, neural tools perform uncertainly\nacross task scenarios. While different tools for a task may excel in varied\nscenarios, existing agents typically rely on fixed tools, thus limiting the\nflexibility in selecting the most suitable tool for specific tasks. In\ncontrast, humans snowball their understanding of the capabilities of different\ntools by interacting with them, and apply this knowledge to select the optimal\ntool when solving a future task. To build agents that similarly benefit from\nthis process, we propose ToolMem that enables agents to develop memories of\ntool capabilities from previous interactions, by summarizing their strengths\nand weaknesses and storing them in memory; at inference, the agent can retrieve\nrelevant entries from ToolMem, and select the best tool to solve individual\ntasks more accurately. We evaluate ToolMem on learning varied text generation\nand text-to-image generation neural tools. Compared to no-memory, generic\nagents, we find ToolMem-augmented agents predict tool performance 14.8% and\n28.7% more accurately across text and multimodal generation scenarios.\nMoreover, ToolMem facilitates optimal tool selection among multiple choices by\n21% and 24% absolute increases in respective scenarios.", "AI": {"tldr": "ToolMem enables AI agents to develop memory of tool capabilities from previous interactions, improving tool selection and performance prediction by 14.8-28.7% compared to agents without memory.", "motivation": "Current AI agents rely on fixed tools despite neural tools performing uncertainly across different scenarios, while humans adaptively select optimal tools based on learned capabilities.", "method": "ToolMem allows agents to summarize tool strengths/weaknesses from interactions and store them in memory, then retrieve relevant entries to select the best tool for individual tasks.", "result": "ToolMem-augmented agents predict tool performance 14.8% and 28.7% more accurately in text and multimodal generation, and improve optimal tool selection by 21% and 24% absolute increases.", "conclusion": "ToolMem effectively enables agents to learn and leverage tool capabilities through memory, significantly improving tool selection and performance prediction across diverse tasks."}}
{"id": "2510.06627", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06627", "abs": "https://arxiv.org/abs/2510.06627", "authors": ["Yong Liu", "Di Fu", "Yang Luo", "Zirui Zhu", "Minhao Cheng", "Cho-Jui Hsieh", "Yang You"], "title": "POME: Post Optimization Model Edit via Muon-style Projection", "comment": null, "summary": "We introduce Post-Optimization Model Edit (POME), a new algorithm that\nenhances the performance of fine-tuned large language models using only their\npretrained and fine-tuned checkpoints, without requiring extra data or further\noptimization. The core idea is to apply a muon-style projection to $\\Delta W$,\nthe difference between the fine-tuned and pretrained weights. This projection\nuses truncated singular value decomposition (SVD) to equalize the influence of\ndominant update directions and prune small singular values, which often\nrepresent noise. As a simple post-processing step, POME is completely decoupled\nfrom the training pipeline. It requires zero modifications and imposes no\noverhead, making it universally compatible with any optimizer or distributed\nframework. POME delivers consistent gains, boosting average performance by\n+2.5\\% on GSM8K and +1.0\\% on code generation. Its broad applicability -- from\n7B foundation models to 72B RLHF-instructed models -- establishes it as a\npractical, zero-cost enhancement for any fine-tuning pipeline. Code is\navailable at https://github.com/NUS-HPC-AI-Lab/POME.", "AI": {"tldr": "POME is a post-optimization algorithm that improves fine-tuned LLMs by applying muon-style projection with truncated SVD to weight differences, requiring no extra data or training.", "motivation": "To enhance fine-tuned language model performance without additional data or optimization overhead, leveraging only existing pretrained and fine-tuned checkpoints.", "method": "Applies muon-style projection to \u0394W (weight differences) using truncated SVD to equalize dominant update directions and prune noisy singular values as a post-processing step.", "result": "Consistent performance gains: +2.5% on GSM8K and +1.0% on code generation, applicable to models from 7B to 72B parameters.", "conclusion": "POME provides a practical, zero-cost enhancement compatible with any fine-tuning pipeline, requiring no modifications to existing training frameworks."}}
{"id": "2510.06670", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06670", "abs": "https://arxiv.org/abs/2510.06670", "authors": ["Shangjian Yin", "Shining Liang", "Wenbiao Ding", "Yuli Qian", "Zhouxing Shi", "Hongzhi Li", "Yutao Xie"], "title": "PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone\nfor aligning large language models (LLMs). However, its effectiveness depends\non high-quality instruction data. Most existing alignment datasets are either\nprivate or require costly human annotation, which limits reproducibility and\nscalability. Even with Reinforcement Learning from AI Feedback (RLAIF),\nconcerns about data quality remain. Moreover, it is unclear how much data is\nactually required to fine-tune a base model into a strong instruction-following\nmodel. Current approaches often rely on over 300k examples even at the\nsupervised fine-tuning (SFT) stage, yet they still underperform compared to\nproprietary models, creating barriers for academic and resource-limited\ncommunities. To address this gap, we introduce PiKa, a data-efficient family of\nexpert-level alignment datasets. In particular, the PiKa-SFT dataset uses only\n30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through\nevaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,\nwe show that PiKa-SFT outperforms models trained on much larger data. On\nAlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses\nthe official Llama-3-8B-Instruct model trained on over 10 million proprietary\nexamples. We further extend our study by training the Qwen2.5 series (0.5B to\n7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that\nhigh-quality alignment can be achieved with significantly less data, offering a\nscalable path for open-source LLM alignment. Code and data:\nhttps://github.com/SJY8460/PiKa.", "AI": {"tldr": "PiKa introduces a data-efficient family of expert-level alignment datasets that achieve superior performance with only 30k SFT examples, outperforming models trained on much larger datasets and even surpassing proprietary models trained on 10M+ examples.", "motivation": "Existing alignment datasets are either private or require costly human annotation, limiting reproducibility and scalability. Current approaches use over 300k examples but still underperform proprietary models, creating barriers for academic and resource-limited communities.", "method": "Developed PiKa, a data-efficient family of expert-level alignment datasets, with PiKa-SFT using only 30k supervised fine-tuning examples. Evaluated by fine-tuning Llama-3-8B-Base and Qwen2.5 series models on PiKa and other public datasets.", "result": "PiKa-SFT outperforms models trained on much larger data on AlpacaEval 2.0 and Arena-Hard benchmarks. It even surpasses the official Llama-3-8B-Instruct model trained on over 10 million proprietary examples. Consistent gains achieved across Qwen2.5 series (0.5B to 7B).", "conclusion": "High-quality alignment can be achieved with significantly less data, offering a scalable path for open-source LLM alignment. PiKa demonstrates that data efficiency is key to democratizing LLM alignment."}}
{"id": "2510.06631", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06631", "abs": "https://arxiv.org/abs/2510.06631", "authors": ["Qiming Guo", "Bishal Khatri", "Hua Zhang", "Wenlu Wang"], "title": "AI-Driven Forecasting and Monitoring of Urban Water System", "comment": null, "summary": "Underground water and wastewater pipelines are vital for city operations but\nplagued by anomalies like leaks and infiltrations, causing substantial water\nloss, environmental damage, and high repair costs. Conventional manual\ninspections lack efficiency, while dense sensor deployments are prohibitively\nexpensive. In recent years, artificial intelligence has advanced rapidly and is\nincreasingly applied to urban infrastructure. In this research, we propose an\nintegrated AI and remote-sensor framework to address the challenge of leak\ndetection in underground water pipelines, through deploying a sparse set of\nremote sensors to capture real-time flow and depth data, paired with HydroNet -\na dedicated model utilizing pipeline attributes (e.g., material, diameter,\nslope) in a directed graph for higher-precision modeling. Evaluations on a\nreal-world campus wastewater network dataset demonstrate that our system\ncollects effective spatio-temporal hydraulic data, enabling HydroNet to\noutperform advanced baselines. This integration of edge-aware message passing\nwith hydraulic simulations enables accurate network-wide predictions from\nlimited sensor deployments. We envision that this approach can be effectively\nextended to a wide range of underground water pipeline networks.", "AI": {"tldr": "Proposes an AI and remote-sensor framework called HydroNet for detecting leaks in underground water pipelines using sparse sensor deployments and pipeline attributes in a directed graph model.", "motivation": "Underground water pipelines suffer from leaks and infiltrations causing water loss and environmental damage, while conventional inspections are inefficient and dense sensor networks are too expensive.", "method": "Deploy sparse remote sensors to capture real-time flow and depth data, combined with HydroNet model that uses pipeline attributes (material, diameter, slope) in a directed graph structure with edge-aware message passing and hydraulic simulations.", "result": "Evaluation on real-world campus wastewater network shows the system collects effective spatio-temporal hydraulic data and HydroNet outperforms advanced baselines in leak detection accuracy.", "conclusion": "The integration of edge-aware message passing with hydraulic simulations enables accurate network-wide predictions from limited sensor deployments, and this approach can be effectively extended to various underground water pipeline networks."}}
{"id": "2510.06677", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06677", "abs": "https://arxiv.org/abs/2510.06677", "authors": ["Yisha Wu", "Cen", "Zhao", "Yuanpei Cao", "Xiaoqing Su", "Yashar Mehdad", "Mindy Ji", "Claire Na Cheng"], "title": "Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback", "comment": "Accepted at EMNLP 2025 Industry Track", "summary": "We introduce an incremental summarization system for customer support agents\nthat intelligently determines when to generate concise bullet notes during\nconversations, reducing agents' context-switching effort and redundant review.\nOur approach combines a fine-tuned Mixtral-8x7B model for continuous note\ngeneration with a DeBERTa-based classifier to filter trivial content. Agent\nedits refine the online notes generation and regularly inform offline model\nretraining, closing the agent edits feedback loop. Deployed in production, our\nsystem achieved a 3% reduction in case handling time compared to bulk\nsummarization (with reductions of up to 9% in highly complex cases), alongside\nhigh agent satisfaction ratings from surveys. These results demonstrate that\nincremental summarization with continuous feedback effectively enhances summary\nquality and agent productivity at scale.", "AI": {"tldr": "An incremental summarization system for customer support agents that generates concise bullet notes during conversations, reducing context-switching and redundant review.", "motivation": "To reduce customer support agents' context-switching effort and redundant review during conversations by providing timely, concise summaries.", "method": "Combines a fine-tuned Mixtral-8x7B model for continuous note generation with a DeBERTa-based classifier to filter trivial content, and uses agent edits to refine online notes and inform offline model retraining.", "result": "Deployed in production, achieved 3% reduction in case handling time (up to 9% in highly complex cases) and high agent satisfaction ratings.", "conclusion": "Incremental summarization with continuous feedback effectively enhances summary quality and agent productivity at scale."}}
{"id": "2510.06632", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.06632", "abs": "https://arxiv.org/abs/2510.06632", "authors": ["Yasaman Torabi", "Shahram Shirani", "James P. Reilly"], "title": "Chem-NMF: Multi-layer $\u03b1$-divergence Non-Negative Matrix Factorization for Cardiorespiratory Disease Clustering, with Improved Convergence Inspired by Chemical Catalysts and Rigorous Asymptotic Analysis", "comment": null, "summary": "Non-Negative Matrix Factorization (NMF) is an unsupervised learning method\noffering low-rank representations across various domains such as audio\nprocessing, biomedical signal analysis, and image recognition. The\nincorporation of $\\alpha$-divergence in NMF formulations enhances flexibility\nin optimization, yet extending these methods to multi-layer architectures\npresents challenges in ensuring convergence. To address this, we introduce a\nnovel approach inspired by the Boltzmann probability of the energy barriers in\nchemical reactions to theoretically perform convergence analysis. We introduce\na novel method, called Chem-NMF, with a bounding factor which stabilizes\nconvergence. To our knowledge, this is the first study to apply a physical\nchemistry perspective to rigorously analyze the convergence behaviour of the\nNMF algorithm. We start from mathematically proven asymptotic convergence\nresults and then show how they apply to real data. Experimental results\ndemonstrate that the proposed algorithm improves clustering accuracy by 5.6%\n$\\pm$ 2.7% on biomedical signals and 11.1% $\\pm$ 7.2% on face images (mean\n$\\pm$ std).", "AI": {"tldr": "Chem-NMF: A novel multi-layer NMF method inspired by chemical reaction energy barriers, with theoretical convergence analysis and improved clustering performance on biomedical signals and face images.", "motivation": "Extending NMF with \u03b1-divergence to multi-layer architectures faces convergence challenges, requiring rigorous theoretical analysis.", "method": "Proposed Chem-NMF with bounding factor inspired by Boltzmann probability of energy barriers in chemical reactions, providing theoretical convergence analysis.", "result": "Improves clustering accuracy by 5.6% \u00b1 2.7% on biomedical signals and 11.1% \u00b1 7.2% on face images.", "conclusion": "First study applying physical chemistry perspective to analyze NMF convergence, demonstrating practical improvements in real-world applications."}}
{"id": "2510.06695", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.06695", "abs": "https://arxiv.org/abs/2510.06695", "authors": ["Qinhao Zhou", "Xiang Xiang", "Kun He", "John E. Hopcroft"], "title": "Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks", "comment": null, "summary": "In recent years, the growing interest in Large Language Models (LLMs) has\nsignificantly advanced prompt engineering, transitioning from manual design to\nmodel-based optimization. Prompts for LLMs generally comprise two components:\nthe \\textit{instruction}, which defines the task or objective, and the\n\\textit{input}, which is tailored to the instruction type. In natural language\ngeneration (NLG) tasks such as machine translation, the \\textit{input}\ncomponent is particularly critical, while the \\textit{instruction} component\ntends to be concise. Existing prompt engineering methods primarily focus on\noptimizing the \\textit{instruction} component for general tasks, often\nrequiring large-parameter LLMs as auxiliary tools. However, these approaches\nexhibit limited applicability for tasks like machine translation, where the\n\\textit{input} component plays a more pivotal role. To address this limitation,\nthis paper introduces a novel prompt optimization method specifically designed\nfor machine translation tasks. The proposed approach employs a small-parameter\nmodel trained using a back-translation-based strategy, significantly reducing\ntraining overhead for single-task optimization while delivering highly\neffective performance. With certain adaptations, this method can also be\nextended to other downstream tasks.", "AI": {"tldr": "This paper introduces a novel prompt optimization method specifically designed for machine translation tasks, using a small-parameter model trained with back-translation strategy to reduce training overhead while maintaining high effectiveness.", "motivation": "Existing prompt engineering methods focus mainly on optimizing instruction components for general tasks and require large-parameter LLMs as auxiliary tools, but they have limited applicability for machine translation where the input component plays a more critical role.", "method": "The proposed approach employs a small-parameter model trained using a back-translation-based strategy, which significantly reduces training overhead for single-task optimization.", "result": "The method delivers highly effective performance for machine translation tasks.", "conclusion": "With certain adaptations, this method can also be extended to other downstream tasks beyond machine translation."}}
{"id": "2510.06634", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06634", "abs": "https://arxiv.org/abs/2510.06634", "authors": ["Shiye Su", "Yuhui Zhang", "Linqi Zhou", "Rajesh Ranganath", "Serena Yeung-Levy"], "title": "Three Forms of Stochastic Injection for Improved Distribution-to-Distribution Generative Modeling", "comment": null, "summary": "Modeling transformations between arbitrary data distributions is a\nfundamental scientific challenge, arising in applications like drug discovery\nand evolutionary simulation. While flow matching offers a natural framework for\nthis task, its use has thus far primarily focused on the noise-to-data setting,\nwhile its application in the general distribution-to-distribution setting is\nunderexplored. We find that in the latter case, where the source is also a data\ndistribution to be learned from limited samples, standard flow matching fails\ndue to sparse supervision. To address this, we propose a simple and\ncomputationally efficient method that injects stochasticity into the training\nprocess by perturbing source samples and flow interpolants. On five diverse\nimaging tasks spanning biology, radiology, and astronomy, our method\nsignificantly improves generation quality, outperforming existing baselines by\nan average of 9 FID points. Our approach also reduces the transport cost\nbetween input and generated samples to better highlight the true effect of the\ntransformation, making flow matching a more practical tool for simulating the\ndiverse distribution transformations that arise in science.", "AI": {"tldr": "The paper addresses the challenge of modeling transformations between arbitrary data distributions, proposing a stochastic flow matching method that improves generation quality and reduces transport cost in distribution-to-distribution settings.", "motivation": "Current flow matching methods primarily focus on noise-to-data transformations, but their application in general distribution-to-distribution settings is underexplored, especially when the source is a data distribution learned from limited samples where standard flow matching fails due to sparse supervision.", "method": "The authors propose a simple and computationally efficient method that injects stochasticity into the training process by perturbing source samples and flow interpolants.", "result": "On five diverse imaging tasks spanning biology, radiology, and astronomy, the method significantly improves generation quality, outperforming existing baselines by an average of 9 FID points, and reduces transport cost between input and generated samples.", "conclusion": "The proposed approach makes flow matching a more practical tool for simulating diverse distribution transformations that arise in scientific applications like drug discovery and evolutionary simulation."}}
{"id": "2510.06700", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06700", "abs": "https://arxiv.org/abs/2510.06700", "authors": ["Leonardo Bertolazzi", "Sandro Pezzelle", "Raffaelle Bernardi"], "title": "How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects", "comment": null, "summary": "Both humans and large language models (LLMs) exhibit content effects: biases\nin which the plausibility of the semantic content of a reasoning problem\ninfluences judgments regarding its logical validity. While this phenomenon in\nhumans is best explained by the dual-process theory of reasoning, the\nmechanisms behind content effects in LLMs remain unclear. In this work, we\naddress this issue by investigating how LLMs encode the concepts of validity\nand plausibility within their internal representations. We show that both\nconcepts are linearly represented and strongly aligned in representational\ngeometry, leading models to conflate plausibility with validity. Using steering\nvectors, we demonstrate that plausibility vectors can causally bias validity\njudgements, and vice versa, and that the degree of alignment between these two\nconcepts predicts the magnitude of behavioral content effects across models.\nFinally, we construct debiasing vectors that disentangle these concepts,\nreducing content effects and improving reasoning accuracy. Our findings advance\nunderstanding of how abstract logical concepts are represented in LLMs and\nhighlight representational interventions as a path toward more logical systems.", "AI": {"tldr": "LLMs exhibit content effects where semantic plausibility biases logical validity judgments, similar to humans. The study shows validity and plausibility are linearly represented and aligned in LLMs' internal representations, causing conflation between them.", "motivation": "To understand the mechanisms behind content effects in LLMs - why semantic plausibility influences logical validity judgments, similar to human dual-process reasoning.", "method": "Investigated how LLMs encode validity and plausibility concepts in internal representations, used steering vectors to test causal relationships, and constructed debiasing vectors to disentangle concepts.", "result": "Found both validity and plausibility are linearly represented and strongly aligned in representational geometry. Plausibility vectors causally bias validity judgments and vice versa. Degree of alignment predicts behavioral content effects across models.", "conclusion": "Representational alignment between validity and plausibility causes content effects in LLMs. Debiasing vectors can disentangle these concepts, reducing content effects and improving reasoning accuracy, offering a path toward more logical AI systems."}}
{"id": "2510.06635", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06635", "abs": "https://arxiv.org/abs/2510.06635", "authors": ["Yunpeng Gong", "Sihan Lan", "Can Yang", "Kunpeng Xu", "Min Jiang"], "title": "StruSR: Structure-Aware Symbolic Regression with Physics-Informed Taylor Guidance", "comment": null, "summary": "Symbolic regression aims to find interpretable analytical expressions by\nsearching over mathematical formula spaces to capture underlying system\nbehavior, particularly in scientific modeling governed by physical laws.\nHowever, traditional methods lack mechanisms for extracting structured physical\npriors from time series observations, making it difficult to capture symbolic\nexpressions that reflect the system's global behavior. In this work, we propose\na structure-aware symbolic regression framework, called StruSR, that leverages\ntrained Physics-Informed Neural Networks (PINNs) to extract locally structured\nphysical priors from time series data. By performing local Taylor expansions on\nthe outputs of the trained PINN, we obtain derivative-based structural\ninformation to guide symbolic expression evolution. To assess the importance of\nexpression components, we introduce a masking-based attribution mechanism that\nquantifies each subtree's contribution to structural alignment and physical\nresidual reduction. These sensitivity scores steer mutation and crossover\noperations within genetic programming, preserving substructures with high\nphysical or structural significance while selectively modifying less\ninformative components. A hybrid fitness function jointly minimizes physics\nresiduals and Taylor coefficient mismatch, ensuring consistency with both the\ngoverning equations and the local analytical behavior encoded by the PINN.\nExperiments on benchmark PDE systems demonstrate that StruSR improves\nconvergence speed, structural fidelity, and expression interpretability\ncompared to conventional baselines, offering a principled paradigm for\nphysics-grounded symbolic discovery.", "AI": {"tldr": "StruSR is a structure-aware symbolic regression framework that uses Physics-Informed Neural Networks (PINNs) to extract physical priors from time series data, guiding symbolic expression evolution through local Taylor expansions and genetic programming with physics-aware operations.", "motivation": "Traditional symbolic regression methods lack mechanisms to extract structured physical priors from time series observations, making it difficult to capture symbolic expressions that reflect the system's global behavior governed by physical laws.", "method": "Uses trained PINNs to extract locally structured physical priors via local Taylor expansions, introduces masking-based attribution to quantify subtree contributions, and employs genetic programming with physics-aware mutation/crossover operations guided by a hybrid fitness function minimizing physics residuals and Taylor coefficient mismatch.", "result": "Experiments on benchmark PDE systems show StruSR improves convergence speed, structural fidelity, and expression interpretability compared to conventional baselines.", "conclusion": "StruSR offers a principled paradigm for physics-grounded symbolic discovery by effectively leveraging physical priors and structural information from neural networks to guide symbolic regression."}}
{"id": "2510.06727", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06727", "abs": "https://arxiv.org/abs/2510.06727", "authors": ["Miao Lu", "Weiwei Sun", "Weihua Du", "Zhan Ling", "Xuesong Yao", "Kang Liu", "Jiecao Chen"], "title": "Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management", "comment": null, "summary": "We study reinforcement learning (RL) fine-tuning of large language model\n(LLM) agents for long-horizon multi-turn tool use, where context length quickly\nbecomes a fundamental bottleneck. Existing RL pipelines can suffer from\ndegraded instruction following, excessive rollout costs, and most importantly,\nstrict context limits. To address these challenges, we introduce\nsummarization-based context management to training. In specific, it\nperiodically compresses the tool using history by LLM-generated summaries that\nretain task-relevant information to keep a compact context while enabling the\nagent to scale beyond the fixed context window. Building on this formulation,\nwe derive a policy gradient representation that seamlessly enables standard LLM\nRL infrastructures to optimize both tool-use behaviors as well as summarization\nstrategies in an end-to-end fashion. We instantiate this framework with\n\\underline{SU}mmarization augmented \\underline{P}olicy \\underline{O}ptimization\n(\\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond\na fixed context limit. Experiments on interactive function calling and\nsearching tasks demonstrate that \\texttt{SUPO} significantly improves the\nsuccess rate while maintaining the same or even lower working context length\ncompared to baselines. We also demonstrate that for complex searching tasks,\n\\texttt{SUPO} can further improve the evaluation performance when scaling\ntest-time maximum round of summarization beyond that of training time. Our\nresults establish summarization-based context management as a principled and\nscalable approach for training RL agents beyond a fixed context length limit.", "AI": {"tldr": "SUPO introduces summarization-based context management to enable RL fine-tuning of LLM agents for long-horizon multi-turn tool use, overcoming context length bottlenecks by compressing tool history with LLM-generated summaries.", "motivation": "Existing RL pipelines for LLM agents face degraded instruction following, excessive rollout costs, and strict context limits that hinder long-horizon multi-turn tool use.", "method": "Proposes SUPO (Summarization augmented Policy Optimization) - an RL algorithm that periodically compresses tool-use history using LLM-generated summaries to maintain compact context, with end-to-end optimization of both tool-use behaviors and summarization strategies.", "result": "Experiments on interactive function calling and searching tasks show SUPO significantly improves success rates while maintaining same or lower context length compared to baselines, and scales well beyond training-time summarization limits.", "conclusion": "Summarization-based context management provides a principled and scalable approach for training RL agents beyond fixed context length limits, enabling effective long-horizon tool use."}}
{"id": "2510.06637", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06637", "abs": "https://arxiv.org/abs/2510.06637", "authors": ["Prakhar Srivastava", "Farrin Marouf Sofian", "Francesco Immorlano", "Kushagra Pandey", "Stephan Mandt"], "title": "Control-Augmented Autoregressive Diffusion for Data Assimilation", "comment": null, "summary": "Despite recent advances in test-time scaling and finetuning of diffusion\nmodels, guidance in Auto-Regressive Diffusion Models (ARDMs) remains\nunderexplored. We introduce an amortized framework that augments pretrained\nARDMs with a lightweight controller network, trained offline by previewing\nfuture ARDM rollouts and learning stepwise controls that anticipate upcoming\nobservations under a terminal cost objective. We evaluate this framework in the\ncontext of data assimilation (DA) for chaotic spatiotemporal partial\ndifferential equations (PDEs), a setting where existing methods are often\ncomputationally prohibitive and prone to forecast drift under sparse\nobservations. Our approach reduces DA inference to a single forward rollout\nwith on-the-fly corrections, avoiding expensive adjoint computations and/or\noptimizations during inference. We demonstrate that our method consistently\noutperforms four state-of-the-art baselines in stability, accuracy, and\nphysical fidelity across two canonical PDEs and six observation regimes. We\nwill release code and checkpoints publicly.", "AI": {"tldr": "Proposes an amortized framework with a lightweight controller network for Auto-Regressive Diffusion Models (ARDMs) that enables efficient data assimilation for chaotic spatiotemporal PDEs through single forward rollouts with on-the-fly corrections.", "motivation": "Guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored, and existing methods for data assimilation in chaotic spatiotemporal PDEs are computationally prohibitive and prone to forecast drift under sparse observations.", "method": "Augments pretrained ARDMs with a lightweight controller network trained offline by previewing future ARDM rollouts and learning stepwise controls that anticipate upcoming observations under a terminal cost objective.", "result": "Reduces data assimilation inference to a single forward rollout with on-the-fly corrections, avoiding expensive adjoint computations and optimizations during inference. Consistently outperforms four state-of-the-art baselines in stability, accuracy, and physical fidelity across two canonical PDEs and six observation regimes.", "conclusion": "The proposed amortized framework provides an efficient and effective solution for data assimilation in chaotic spatiotemporal systems, demonstrating superior performance compared to existing methods while maintaining computational efficiency."}}
{"id": "2510.06730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06730", "abs": "https://arxiv.org/abs/2510.06730", "authors": ["Manuel Frank", "Haithem Afli"], "title": "PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs", "comment": null, "summary": "Current evaluations of sentence embedding models typically rely on static\ntest beds such as the Massive Text Embedding Benchmark (MTEB). While\ninvaluable, repeated tuning on a fixed suite can inflate reported performance\nand obscure real-world robustness. We introduce the Paraphrasing Text Embedding\nBenchmark (PTEB), a dynamic protocol that stochastically generates\nmeaning-preserving paraphrases at evaluation time and aggregates results across\nmultiple runs. Using a cost-efficient LLM-based method grounded in semantic\ntextual similarity gold ratings, we show that LLMs generate token-diverse but\nsemantically preserving, paraphrases. Across 7 MTEB tasks, we validate our\nhypothesis that the performance of sentence encoders is sensitive to changes in\ntoken space even when semantics remain fixed. We also observe that smaller\nmodels are not disproportionately affected relative to larger ones. Our results\nare statistically robust over multiple runs and we extended our experiments to\n3 multilingual datasets covering 10 languages. More generally, we aim to\npropose a new evaluation paradigm in NLP that relies less on static,\npre-defined benchmarks but shifts towards dynamic, stochastic evaluation\nleveraging eval-time compute.", "AI": {"tldr": "PTEB is a dynamic evaluation protocol that generates paraphrases at test time to assess sentence embedding robustness, revealing sensitivity to token variations despite preserved semantics.", "motivation": "Static benchmarks like MTEB can inflate performance through repeated tuning and obscure real-world robustness. Need for dynamic evaluation that tests model sensitivity to semantic-preserving variations.", "method": "Uses LLM-based method to generate token-diverse but semantically preserving paraphrases at evaluation time, aggregating results across multiple runs. Applied to 7 MTEB tasks and 3 multilingual datasets across 10 languages.", "result": "Sentence encoders are sensitive to token space changes even when semantics remain fixed. Smaller models are not disproportionately affected relative to larger ones. Results are statistically robust across multiple runs.", "conclusion": "Proposes a shift from static benchmarks to dynamic, stochastic evaluation leveraging eval-time compute, providing more realistic assessment of sentence embedding robustness."}}
{"id": "2510.06646", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06646", "abs": "https://arxiv.org/abs/2510.06646", "authors": ["Mansi Sakarvadia", "Kareem Hegazy", "Amin Totounferoush", "Kyle Chard", "Yaoqing Yang", "Ian Foster", "Michael W. Mahoney"], "title": "The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators", "comment": null, "summary": "A core challenge in scientific machine learning, and scientific computing\nmore generally, is modeling continuous phenomena which (in practice) are\nrepresented discretely. Machine-learned operators (MLOs) have been introduced\nas a means to achieve this modeling goal, as this class of architecture can\nperform inference at arbitrary resolution. In this work, we evaluate whether\nthis architectural innovation is sufficient to perform \"zero-shot\nsuper-resolution,\" namely to enable a model to serve inference on\nhigher-resolution data than that on which it was originally trained. We\ncomprehensively evaluate both zero-shot sub-resolution and super-resolution\n(i.e., multi-resolution) inference in MLOs. We decouple multi-resolution\ninference into two key behaviors: 1) extrapolation to varying frequency\ninformation; and 2) interpolating across varying resolutions. We empirically\ndemonstrate that MLOs fail to do both of these tasks in a zero-shot manner.\nConsequently, we find MLOs are not able to perform accurate inference at\nresolutions different from those on which they were trained, and instead they\nare brittle and susceptible to aliasing. To address these failure modes, we\npropose a simple, computationally-efficient, and data-driven multi-resolution\ntraining protocol that overcomes aliasing and that provides robust\nmulti-resolution generalization.", "AI": {"tldr": "Machine-learned operators (MLOs) fail at zero-shot super-resolution and multi-resolution inference due to aliasing and inability to extrapolate/interpolate across resolutions. A simple multi-resolution training protocol is proposed to address these issues.", "motivation": "To evaluate whether MLOs can perform zero-shot super-resolution - inference at higher resolutions than training data - which is crucial for scientific machine learning of continuous phenomena.", "method": "Comprehensive evaluation of MLOs' multi-resolution inference capabilities, decoupling extrapolation to varying frequencies and interpolation across resolutions. Proposed a data-driven multi-resolution training protocol.", "result": "MLOs fail at zero-shot multi-resolution inference - they cannot extrapolate to different frequencies or interpolate across resolutions, and are brittle/susceptible to aliasing.", "conclusion": "MLOs cannot perform accurate zero-shot multi-resolution inference without specialized training. The proposed multi-resolution training protocol overcomes aliasing and enables robust generalization across resolutions."}}
{"id": "2510.06732", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06732", "abs": "https://arxiv.org/abs/2510.06732", "authors": ["Tiancheng Xing", "Jerry Li", "Yixuan Du", "Xiyang Hu"], "title": "Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization", "comment": "10 pages, 3 figures", "summary": "Large language models (LLMs) are increasingly used as rerankers in\ninformation retrieval, yet their ranking behavior can be steered by small,\nnatural-sounding prompts. To expose this vulnerability, we present Rank\nAnything First (RAF), a two-stage token optimization method that crafts concise\ntextual perturbations to consistently promote a target item in LLM-generated\nrankings while remaining hard to detect. Stage 1 uses Greedy Coordinate\nGradient to shortlist candidate tokens at the current position by combining the\ngradient of the rank-target with a readability score; Stage 2 evaluates those\ncandidates under exact ranking and readability losses using an entropy-based\ndynamic weighting scheme, and selects a token via temperature-controlled\nsampling. RAF generates ranking-promoting prompts token-by-token, guided by\ndual objectives: maximizing ranking effectiveness and preserving linguistic\nnaturalness. Experiments across multiple LLMs show that RAF significantly\nboosts the rank of target items using naturalistic language, with greater\nrobustness than existing methods in both promoting target items and maintaining\nnaturalness. These findings underscore a critical security implication:\nLLM-based reranking is inherently susceptible to adversarial manipulation,\nraising new challenges for the trustworthiness and robustness of modern\nretrieval systems. Our code is available at: https://github.com/glad-lab/RAF.", "AI": {"tldr": "RAF is a two-stage token optimization method that crafts natural-sounding prompts to manipulate LLM-based ranking systems, exposing their vulnerability to adversarial manipulation.", "motivation": "To expose the vulnerability of LLM-based rerankers to manipulation by small, natural-sounding prompts that can steer ranking behavior.", "method": "Two-stage token optimization: Stage 1 uses Greedy Coordinate Gradient to shortlist candidate tokens combining rank-target gradient and readability; Stage 2 evaluates candidates under ranking and readability losses with entropy-based dynamic weighting and temperature-controlled sampling.", "result": "RAF significantly boosts target item ranks using naturalistic language across multiple LLMs, with greater robustness than existing methods in both promotion effectiveness and naturalness preservation.", "conclusion": "LLM-based reranking is inherently susceptible to adversarial manipulation, raising critical security implications for the trustworthiness and robustness of modern retrieval systems."}}
{"id": "2510.06649", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06649", "abs": "https://arxiv.org/abs/2510.06649", "authors": ["Frank Wu", "Mengye Ren"], "title": "Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions", "comment": "15 pages, 5 figures", "summary": "The Forward-Forward (FF) Algorithm is a recently proposed learning procedure\nfor neural networks that employs two forward passes instead of the traditional\nforward and backward passes used in backpropagation. However, FF remains\nlargely confined to supervised settings, leaving a gap at domains where\nlearning signals can be yielded more naturally such as RL. In this work,\ninspired by FF's goodness function using layer activity statistics, we\nintroduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value\nestimation method that applies a goodness function and action conditioning for\nlocal RL using temporal difference learning. Despite its simplicity and\nbiological grounding, our approach achieves superior performance compared to\nstate-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind\nControl Suite benchmarks, while also outperforming algorithms trained with\nbackpropagation on most tasks. Code can be found at\nhttps://github.com/agentic-learning-ai-lab/arq.", "AI": {"tldr": "The paper introduces ARQ, a novel value estimation method that applies the Forward-Forward algorithm's goodness function to reinforcement learning, achieving state-of-the-art performance without backpropagation.", "motivation": "The Forward-Forward algorithm has been largely confined to supervised learning, creating a gap in domains like RL where learning signals can be more naturally obtained. The authors aim to extend FF's principles to reinforcement learning.", "method": "Proposes Action-conditioned Root mean squared Q-Functions (ARQ), which uses a goodness function and action conditioning for local RL with temporal difference learning, inspired by FF's layer activity statistics approach.", "result": "ARQ achieves superior performance compared to state-of-the-art local backprop-free RL methods on MinAtar and DeepMind Control Suite benchmarks, and outperforms backpropagation-trained algorithms on most tasks.", "conclusion": "The ARQ method successfully bridges the gap between Forward-Forward algorithms and reinforcement learning, demonstrating that biologically-inspired local learning approaches can achieve competitive performance without backpropagation."}}
{"id": "2510.06738", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06738", "abs": "https://arxiv.org/abs/2510.06738", "authors": ["Boyi Zeng", "Lin Chen", "Ziwei He", "Xinbing Wang", "Zhouhan Lin"], "title": "AWM: Accurate Weight-Matrix Fingerprint for Large Language Models", "comment": null, "summary": "Protecting the intellectual property of large language models (LLMs) is\ncrucial, given the substantial resources required for their training.\nConsequently, there is an urgent need for both model owners and third parties\nto determine whether a suspect LLM is trained from scratch or derived from an\nexisting base model. However, the intensive post-training processes that models\ntypically undergo-such as supervised fine-tuning, extensive continued\npretraining, reinforcement learning, multi-modal extension, pruning, and\nupcycling-pose significant challenges to reliable identification. In this work,\nwe propose a training-free fingerprinting method based on weight matrices. We\nleverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel\nAlignment (CKA) similarity to neutralize the effects of parameter\nmanipulations, yielding a highly robust and high-fidelity similarity metric. On\na comprehensive testbed of 60 positive and 90 negative model pairs, our method\ndemonstrates exceptional robustness against all six aforementioned\npost-training categories while exhibiting a near-zero risk of false positives.\nBy achieving perfect scores on all classification metrics, our approach\nestablishes a strong basis for reliable model lineage verification. Moreover,\nthe entire computation completes within 30s on an NVIDIA 3090 GPU. The code is\navailable at https://github.com/LUMIA-Group/AWM.", "AI": {"tldr": "Proposes a training-free fingerprinting method using weight matrices and Linear Assignment Problem with Centered Kernel Alignment to identify if LLMs are derived from base models, achieving perfect classification metrics with high robustness against post-training modifications.", "motivation": "Protecting intellectual property of LLMs is crucial due to substantial training resources. Need reliable methods to determine if suspect LLMs are trained from scratch or derived from existing models, especially challenging due to intensive post-training processes.", "method": "Training-free fingerprinting based on weight matrices using Linear Assignment Problem (LAP) and unbiased Centered Kernel Alignment (CKA) similarity to neutralize parameter manipulation effects.", "result": "Achieved perfect scores on all classification metrics with near-zero false positive risk on 60 positive and 90 negative model pairs. Highly robust against all six post-training categories. Computation completes within 30s on NVIDIA 3090 GPU.", "conclusion": "The method establishes strong basis for reliable model lineage verification with exceptional robustness and high fidelity, providing effective IP protection for LLMs."}}
{"id": "2510.06660", "categories": ["cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.06660", "abs": "https://arxiv.org/abs/2510.06660", "authors": ["Weiguo Lu", "Gangnan Yuan", "Hong-kun Zhang", "Shangyang Li"], "title": "Rethinking Nonlinearity: Trainable Gaussian Mixture Modules for Modern Neural Architectures", "comment": null, "summary": "Neural networks in general, from MLPs and CNNs to attention-based\nTransformers, are constructed from layers of linear combinations followed by\nnonlinear operations such as ReLU, Sigmoid, or Softmax. Despite their strength,\nthese conventional designs are often limited in introducing non-linearity by\nthe choice of activation functions. In this work, we introduce Gaussian\nMixture-Inspired Nonlinear Modules (GMNM), a new class of differentiable\nmodules that draw on the universal density approximation Gaussian mixture\nmodels (GMMs) and distance properties (metric space) of Gaussian kernal. By\nrelaxing probabilistic constraints and adopting a flexible parameterization of\nGaussian projections, GMNM can be seamlessly integrated into diverse neural\narchitectures and trained end-to-end with gradient-based methods. Our\nexperiments demonstrate that incorporating GMNM into architectures such as\nMLPs, CNNs, attention mechanisms, and LSTMs consistently improves performance\nover standard baselines. These results highlight GMNM's potential as a powerful\nand flexible module for enhancing efficiency and accuracy across a wide range\nof machine learning applications.", "AI": {"tldr": "The paper introduces Gaussian Mixture-Inspired Nonlinear Modules (GMNM), a new differentiable module that enhances neural network nonlinearity by leveraging Gaussian mixture models and distance properties, improving performance across various architectures.", "motivation": "Conventional neural networks are limited in nonlinearity by standard activation functions like ReLU and Softmax. The authors aim to overcome this limitation by developing more flexible nonlinear modules.", "method": "GMNM draws on Gaussian mixture models and Gaussian kernel distance properties. It relaxes probabilistic constraints and uses flexible Gaussian projections, allowing seamless integration into diverse neural architectures and end-to-end training with gradient methods.", "result": "Experiments show that incorporating GMNM into MLPs, CNNs, attention mechanisms, and LSTMs consistently improves performance over standard baselines across various applications.", "conclusion": "GMNM demonstrates strong potential as a powerful and flexible module for enhancing both efficiency and accuracy in a wide range of machine learning applications."}}
{"id": "2510.06747", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06747", "abs": "https://arxiv.org/abs/2510.06747", "authors": ["I-Fan Lin", "Faegheh Hasibi", "Suzan Verberne"], "title": "TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs", "comment": null, "summary": "In this paper, we propose a training-free and label-free method for short\ntext clustering that can be used on top of any existing embedder. In the\ncontext of customer-facing chatbots, companies are dealing with large amounts\nof user utterances that need to be clustered according to their intent. In\nthese commercial settings, no labeled data is typically available, and the\nnumber of clusters is not known. Our method is based on iterative vector\nupdating: it constructs sparse vectors based on representative texts, and then\niteratively refines them through LLM guidance. Our method achieves comparable\nor superior results to state-of-the-art methods that use contrastive learning,\nbut without assuming prior knowledge of clusters or labels. Experiments on\ndiverse datasets and smaller LLMs show that our method is model agnostic and\ncan be applied to any embedder, with relatively small LLMs, and different\nclustering methods. We also show that our method scales to large datasets,\nreducing the computational cost of the LLM. These low-resource, adaptable\nsettings and the scalability of our method make it more aligned with real-world\nscenarios than existing clustering methods.", "AI": {"tldr": "A training-free, label-free short text clustering method using iterative vector updating with LLM guidance, achieving comparable or better results than contrastive learning methods without needing cluster or label information.", "motivation": "Companies dealing with customer-facing chatbots need to cluster large amounts of user utterances by intent, but typically have no labeled data and unknown number of clusters in commercial settings.", "method": "Iterative vector updating: constructs sparse vectors from representative texts and refines them through LLM guidance, working with any embedder and requiring no training or labels.", "result": "Achieves comparable or superior results to state-of-the-art contrastive learning methods, works with diverse datasets and smaller LLMs, scales to large datasets with reduced computational cost.", "conclusion": "The method is model-agnostic, scalable, and better aligned with real-world scenarios due to its low-resource requirements and adaptability to different clustering methods."}}
{"id": "2510.06662", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.06662", "abs": "https://arxiv.org/abs/2510.06662", "authors": ["Penghao Yu", "Haotian Jiang", "Zeyu Bao", "Ruoxi Yu", "Qianxiao Li"], "title": "The Effect of Attention Head Count on Transformer Approximation", "comment": null, "summary": "Transformer has become the dominant architecture for sequence modeling, yet a\ndetailed understanding of how its structural parameters influence expressive\npower remains limited. In this work, we study the approximation properties of\ntransformers, with particular emphasis on the role of the number of attention\nheads. Our analysis begins with the introduction of a generalized $D$-retrieval\ntask, which we prove to be dense in the space of continuous functions, thereby\nproviding the basis for our theoretical framework. We then establish both upper\nand lower bounds on the parameter complexity required for\n$\\epsilon$-approximation. Specifically, we show that transformers with\nsufficiently many heads admit efficient approximation, whereas with too few\nheads, the number of parameters must scale at least as $O(1/\\epsilon^{cT})$,\nfor some constant $c$ and sequence length $T$. To the best of our knowledge,\nthis constitutes the first rigorous lower bound of this type in a nonlinear and\npractically relevant setting. We further examine the single-head case and\ndemonstrate that an embedding dimension of order $O(T)$ allows complete\nmemorization of the input, where approximation is entirely achieved by the\nfeed-forward block. Finally, we validate our theoretical findings with\nexperiments on both synthetic data and real-world tasks, illustrating the\npractical relevance of our results.", "AI": {"tldr": "This paper analyzes how the number of attention heads in transformers affects their expressive power, establishing theoretical bounds on parameter complexity for approximation and showing practical implications.", "motivation": "Despite transformers being dominant for sequence modeling, there's limited understanding of how structural parameters like attention heads influence expressive power.", "method": "Introduced generalized D-retrieval task, established upper/lower bounds on parameter complexity for \u03b5-approximation, analyzed single-head case, and validated with experiments on synthetic and real-world data.", "result": "Transformers with many heads admit efficient approximation, while too few heads require parameter scaling of O(1/\u03b5^{cT}). Single-head transformers with O(T) embedding dimension can achieve complete memorization.", "conclusion": "The number of attention heads critically impacts transformer expressive power, with rigorous theoretical bounds established and practical relevance demonstrated."}}
{"id": "2510.06749", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06749", "abs": "https://arxiv.org/abs/2510.06749", "authors": ["Eitan Klinger", "Zihao Huang", "Tran Minh Nguyen", "Emma Jayeon Park", "Yige Chen", "Yang Gu", "Qingyu Gao", "Siliang Liu", "Mengyang Qiu", "Jungyeul Park"], "title": "A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction", "comment": "Submitted to ACL Rolling Review - October 2025 for EACL 2026", "summary": "Evaluating grammatical error correction requires metrics that reflect the\ndiversity of valid human corrections rather than privileging a single\nreference. Existing frameworks, largely edit-based and English-centric, rely on\nrigid alignments between system and reference edits, limiting their\napplicability in multilingual and generative settings. This paper introduces a\nformal framework for \\textit{fluency-based multi-reference evaluation}, framing\n$n$-gram similarity as an aggregation problem over multiple legitimate\ncorrections. Within this formulation, we instantiate GLEU through four\naggregation strategies--\\textsc{select-best}, \\textsc{simple-average},\n\\textsc{weighted-average}, and \\textsc{merged-counts}--and analyze their\nproperties of boundedness, monotonicity, and sensitivity to reference\nvariation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora\nshow that these strategies capture complementary aspects of fluency and\ncoverage. The framework unifies multi-reference evaluation into a principled,\nfluency-oriented approach that incorporates linguistic diversity without\npenalizing legitimate variation.", "AI": {"tldr": "This paper introduces a fluency-based multi-reference evaluation framework for grammatical error correction that addresses limitations of existing edit-based metrics by using n-gram similarity and multiple aggregation strategies.", "motivation": "Existing GEC evaluation metrics are edit-based, English-centric, and rely on rigid alignments, limiting their applicability in multilingual and generative settings. They fail to reflect the diversity of valid human corrections.", "method": "Proposes a formal framework for fluency-based multi-reference evaluation, framing n-gram similarity as an aggregation problem. Instantiates GLEU metric through four aggregation strategies: select-best, simple-average, weighted-average, and merged-counts.", "result": "Empirical evaluation on Czech, Estonian, Ukrainian, and Chinese corpora shows that the different aggregation strategies capture complementary aspects of fluency and coverage.", "conclusion": "The framework provides a principled, fluency-oriented approach that incorporates linguistic diversity without penalizing legitimate variation, unifying multi-reference evaluation."}}
{"id": "2510.06672", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06672", "abs": "https://arxiv.org/abs/2510.06672", "authors": ["Udbhav Bamba", "Minghao Fang", "Yifan Yu", "Haizhong Zheng", "Fan Lai"], "title": "XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation", "comment": null, "summary": "Reinforcement learning algorithms such as GRPO have driven recent advances in\nlarge language model (LLM) reasoning. While scaling the number of rollouts\nstabilizes training, existing approaches suffer from limited exploration on\nchallenging prompts and leave informative feedback signals underexploited, due\nto context-independent rollout allocation across prompts (e.g., generating 16\nrollouts per prompt) and relying heavily on sparse rewards. This paper presents\nXRPO(eXplore - eXploit GRPO), a unified framework that recasts policy\noptimization through the principled lens of rollout exploration-exploitation.\nTo enhance exploration, XRPO introduces a mathematically grounded rollout\nallocator that adaptively prioritizes prompts with higher potential for\nuncertainty reduction. It further addresses stagnation on zero-reward prompts\nthrough an in-context seeding strategy that injects curated exemplars, steering\nthe model into more difficult reasoning trajectories. To strengthen\nexploitation, XRPO develops a group-relative, novelty-aware advantage\nsharpening mechanism that leverages sequence likelihoods to amplify\nlow-probability yet correct responses, thereby extending the policy's reach\nbeyond sparse rewards. Experiments across diverse math and coding benchmarks on\nboth reasoning and non-reasoning models demonstrate that XRPO outperforms\nexisting advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while\naccelerating training convergence by up to 2.7X.", "AI": {"tldr": "XRPO is a reinforcement learning framework that improves LLM reasoning by balancing exploration and exploitation through adaptive rollout allocation, in-context seeding, and advantage sharpening, achieving better performance and faster convergence than existing methods.", "motivation": "Existing RL approaches for LLMs suffer from limited exploration on challenging prompts and underexploited feedback signals due to uniform rollout allocation and reliance on sparse rewards.", "method": "XRPO introduces: 1) adaptive rollout allocator prioritizing prompts with high uncertainty reduction potential, 2) in-context seeding with curated exemplars for difficult reasoning trajectories, 3) group-relative novelty-aware advantage sharpening using sequence likelihoods to amplify correct low-probability responses.", "result": "XRPO outperforms GRPO and GSPO by up to 4% pass@1 and 6% cons@32 across math and coding benchmarks, while accelerating training convergence by up to 2.7X.", "conclusion": "XRPO provides a principled exploration-exploitation framework that significantly improves LLM reasoning performance and training efficiency through adaptive rollout allocation and enhanced feedback utilization."}}
{"id": "2510.06750", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06750", "abs": "https://arxiv.org/abs/2510.06750", "authors": ["Jaeseong Lee", "Dayoung Kwon", "seung-won hwang"], "title": "Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs", "comment": null, "summary": "Large Reasoning Models (LRMs) excel in structured tasks by emulating\ndeliberate human reasoning but often suffer from overthinking, degrading\nperformance and wasting resources. One possible baseline is to deploy both LLM\nand LRM, then route input by predicting whether it requires reasoning and may\ncause overthinking. However, deploying multiple models can be costly or\nimpractical. We propose a superposed deployment strategy with a lightweight,\ntraining-free regulation to optimize inference by switching one model on and\noff. Instead of routing, we selectively unlearn from LRM at inference, scaling\ndown computation while preserving reasoning. By analyzing the cumulative energy\nof singular values, we identify optimal low-rank projections to adjust\nreasoning just right.", "AI": {"tldr": "Proposes a superposed deployment strategy with lightweight regulation to optimize LRM inference by selectively unlearning from LRM at inference, avoiding overthinking while preserving reasoning capabilities.", "motivation": "Large Reasoning Models (LRMs) often suffer from overthinking, which degrades performance and wastes computational resources. Traditional routing approaches require deploying multiple models, which can be costly or impractical.", "method": "Uses a superposed deployment strategy with training-free regulation that switches one model on and off. Instead of routing, selectively unlearns from LRM at inference by analyzing cumulative energy of singular values to identify optimal low-rank projections.", "result": "Enables scaling down computation while preserving reasoning capabilities, achieving optimal reasoning adjustment without the need for multiple model deployments.", "conclusion": "The proposed method provides an efficient way to regulate LRM inference, preventing overthinking while maintaining reasoning performance through selective unlearning and low-rank projections."}}
{"id": "2510.06680", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06680", "abs": "https://arxiv.org/abs/2510.06680", "authors": ["Zhipeng Liu", "Peibo Duan", "Xuan Tang", "Baixin Li", "Yongsheng Huang", "Mingyang Geng", "Changsheng Zhang", "Bin Zhang", "Binwu Wang"], "title": "TimeFormer: Transformer with Attention Modulation Empowered by Temporal Characteristics for Time Series Forecasting", "comment": null, "summary": "Although Transformers excel in natural language processing, their extension\nto time series forecasting remains challenging due to insufficient\nconsideration of the differences between textual and temporal modalities. In\nthis paper, we develop a novel Transformer architecture designed for time\nseries data, aiming to maximize its representational capacity. We identify two\nkey but often overlooked characteristics of time series: (1) unidirectional\ninfluence from the past to the future, and (2) the phenomenon of decaying\ninfluence over time. These characteristics are introduced to enhance the\nattention mechanism of Transformers. We propose TimeFormer, whose core\ninnovation is a self-attention mechanism with two modulation terms (MoSA),\ndesigned to capture these temporal priors of time series under the constraints\nof the Hawkes process and causal masking. Additionally, TimeFormer introduces a\nframework based on multi-scale and subsequence analysis to capture semantic\ndependencies at different temporal scales, enriching the temporal dependencies.\nExtensive experiments conducted on multiple real-world datasets show that\nTimeFormer significantly outperforms state-of-the-art methods, achieving up to\na 7.45% reduction in MSE compared to the best baseline and setting new\nbenchmarks on 94.04\\% of evaluation metrics. Moreover, we demonstrate that the\nMoSA mechanism can be broadly applied to enhance the performance of other\nTransformer-based models.", "AI": {"tldr": "TimeFormer is a novel Transformer architecture for time series forecasting that incorporates temporal priors through a modulated self-attention mechanism and multi-scale analysis, achieving state-of-the-art performance.", "motivation": "Transformers are effective in NLP but face challenges in time series forecasting due to insufficient consideration of differences between textual and temporal modalities, particularly unidirectional influence and decaying influence over time.", "method": "Proposes TimeFormer with MoSA (modulated self-attention) mechanism that captures temporal priors under Hawkes process constraints and causal masking, plus multi-scale subsequence analysis framework for capturing semantic dependencies at different temporal scales.", "result": "Significantly outperforms state-of-the-art methods on multiple real-world datasets, achieving up to 7.45% reduction in MSE compared to best baseline and setting new benchmarks on 94.04% of evaluation metrics.", "conclusion": "TimeFormer effectively addresses temporal characteristics in time series forecasting and the MoSA mechanism can be broadly applied to enhance other Transformer-based models."}}
{"id": "2510.06774", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06774", "abs": "https://arxiv.org/abs/2510.06774", "authors": ["Lei Xu", "Pierre Beckmann", "Marco Valentino", "Andr\u00e9 Freitas"], "title": "Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition", "comment": null, "summary": "Neuro-symbolic NLP methods aim to leverage the complementary strengths of\nlarge language models and formal logical solvers. However, current approaches\nare mostly static in nature, i.e., the integration of a target solver is\npredetermined at design time, hindering the ability to employ diverse formal\ninference strategies. To address this, we introduce an adaptive,\nmulti-paradigm, neuro-symbolic inference framework that: (1) automatically\nidentifies formal reasoning strategies from problems expressed in natural\nlanguage; and (2) dynamically selects and applies specialized formal logical\nsolvers via autoformalization interfaces. Extensive experiments on individual\nand multi-paradigm reasoning tasks support the following conclusions: LLMs are\neffective at predicting the necessary formal reasoning strategies with an\naccuracy above 90 percent. This enables flexible integration with formal\nlogical solvers, resulting in our framework outperforming competing baselines\nby 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively.\nMoreover, adaptive reasoning can even positively impact pure LLM methods,\nyielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT\nsettings with GPT-4o. Finally, although smaller models struggle with adaptive\nneuro-symbolic reasoning, post-training offers a viable path to improvement.\nOverall, this work establishes the foundations for adaptive LLM-symbolic\nreasoning, offering a path forward for unifying material and formal inferences\non heterogeneous reasoning challenges.", "AI": {"tldr": "An adaptive neuro-symbolic framework that automatically identifies formal reasoning strategies from natural language problems and dynamically selects appropriate logical solvers, outperforming state-of-the-art models.", "motivation": "Current neuro-symbolic NLP methods are static and predetermined, lacking flexibility to employ diverse formal inference strategies for different reasoning tasks.", "method": "Developed an adaptive multi-paradigm neuro-symbolic inference framework that: (1) automatically identifies formal reasoning strategies from natural language problems, and (2) dynamically selects and applies specialized formal logical solvers through autoformalization interfaces.", "result": "LLMs achieve over 90% accuracy in predicting necessary formal reasoning strategies. The framework outperforms GPT-4o by 27% and DeepSeek-V3.1 by 6%. Adaptive reasoning also improves pure LLM methods by 10%, 5%, and 6% on zero-shot, CoT, and symbolic CoT settings respectively.", "conclusion": "This work establishes foundations for adaptive LLM-symbolic reasoning, providing a path to unify material and formal inferences for heterogeneous reasoning challenges, with post-training offering improvement for smaller models."}}
{"id": "2510.06683", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06683", "abs": "https://arxiv.org/abs/2510.06683", "authors": ["Daoyuan Zhou", "Xuchuang Wang", "Lin Yang", "Yang Gao"], "title": "Distributed Algorithms for Multi-Agent Multi-Armed Bandits with Collision", "comment": "21 pages, 4 figures", "summary": "We study the stochastic Multiplayer Multi-Armed Bandit (MMAB) problem, where\nmultiple players select arms to maximize their cumulative rewards. Collisions\noccur when two or more players select the same arm, resulting in no reward, and\nare observed by the players involved. We consider a distributed setting without\ncentral coordination, where each player can only observe their own actions and\ncollision feedback. We propose a distributed algorithm with an adaptive,\nefficient communication protocol. The algorithm achieves near-optimal group and\nindividual regret, with a communication cost of only $\\mathcal{O}(\\log\\log T)$.\nOur experiments demonstrate significant performance improvements over existing\nbaselines. Compared to state-of-the-art (SOTA) methods, our approach achieves a\nnotable reduction in individual regret. Finally, we extend our approach to a\nperiodic asynchronous setting, proving the lower bound for this problem and\npresenting an algorithm that achieves logarithmic regret.", "AI": {"tldr": "A distributed algorithm for multiplayer multi-armed bandits with adaptive communication achieves near-optimal regret with only O(log log T) communication cost, outperforming SOTA methods.", "motivation": "To solve the multiplayer multi-armed bandit problem in distributed settings without central coordination, where collisions occur when players select the same arm, and players can only observe their own actions and collision feedback.", "method": "Proposed a distributed algorithm with an adaptive, efficient communication protocol that allows players to coordinate while minimizing communication overhead.", "result": "Achieves near-optimal group and individual regret with communication cost of only O(log log T), and experiments show significant performance improvements over existing baselines with notable reduction in individual regret.", "conclusion": "The approach effectively solves distributed multiplayer bandit problems with minimal communication, and can be extended to periodic asynchronous settings with logarithmic regret guarantees."}}
{"id": "2510.06780", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06780", "abs": "https://arxiv.org/abs/2510.06780", "authors": ["Luca Giordano", "Simon Razniewski"], "title": "Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness", "comment": null, "summary": "Large Language Models (LLMs) encode substantial factual knowledge, yet\nmeasuring and systematizing this knowledge remains challenging. Converting it\ninto structured format, for example through recursive extraction approaches\nsuch as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key\nopen questions include whether such extraction can terminate, whether its\noutputs are reproducible, and how robust they are to variations. We\nsystematically study LLM knowledge materialization using miniGPTKBs\n(domain-specific, tractable subcrawls), analyzing termination, reproducibility,\nand robustness across three categories of metrics: yield, lexical similarity,\nand semantic similarity. We experiment with four variations (seed, language,\nrandomness, model) and three illustrative domains (from history, entertainment,\nand finance). Our findings show (i) high termination rates, though\nmodel-dependent; (ii) mixed reproducibility; and (iii) robustness that varies\nby perturbation type: high for seeds and temperature, lower for languages and\nmodels. These results suggest that LLM knowledge materialization can reliably\nsurface core knowledge, while also revealing important limitations.", "AI": {"tldr": "Systematic study of LLM knowledge extraction shows high termination rates but mixed reproducibility and varying robustness across different perturbation types.", "motivation": "To measure and systematize the factual knowledge encoded in LLMs by converting it into structured format, addressing key questions about termination, reproducibility, and robustness of knowledge extraction.", "method": "Used miniGPTKBs (domain-specific subcrawls) to analyze LLM knowledge materialization across three metric categories: yield, lexical similarity, and semantic similarity. Tested four variations (seed, language, randomness, model) in three domains (history, entertainment, finance).", "result": "Found (i) high termination rates (model-dependent), (ii) mixed reproducibility, and (iii) robustness varying by perturbation type: high for seeds and temperature, lower for languages and models.", "conclusion": "LLM knowledge materialization can reliably extract core knowledge but has important limitations in reproducibility and robustness across different conditions."}}
{"id": "2510.06684", "categories": ["cs.LG", "cs.NA", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.06684", "abs": "https://arxiv.org/abs/2510.06684", "authors": ["Kang An", "Chenhao Si", "Ming Yan", "Shiqian Ma"], "title": "AutoBalance: An Automatic Balancing Framework for Training Physics-Informed Neural Networks", "comment": "23 pages", "summary": "Physics-Informed Neural Networks (PINNs) provide a powerful and general\nframework for solving Partial Differential Equations (PDEs) by embedding\nphysical laws into loss functions. However, training PINNs is notoriously\ndifficult due to the need to balance multiple loss terms, such as PDE residuals\nand boundary conditions, which often have conflicting objectives and vastly\ndifferent curvatures. Existing methods address this issue by manipulating\ngradients before optimization (a \"pre-combine\" strategy). We argue that this\napproach is fundamentally limited, as forcing a single optimizer to process\ngradients from spectrally heterogeneous loss landscapes disrupts its internal\npreconditioning. In this work, we introduce AutoBalance, a novel \"post-combine\"\ntraining paradigm. AutoBalance assigns an independent adaptive optimizer to\neach loss component and aggregates the resulting preconditioned updates\nafterwards. Extensive experiments on challenging PDE benchmarks show that\nAutoBalance consistently outperforms existing frameworks, achieving significant\nreductions in solution error, as measured by both the MSE and $L^{\\infty}$\nnorms. Moreover, AutoBalance is orthogonal to and complementary with other\npopular PINN methodologies, amplifying their effectiveness on demanding\nbenchmarks.", "AI": {"tldr": "AutoBalance introduces a post-combine training paradigm for PINNs that assigns independent adaptive optimizers to each loss component, overcoming limitations of pre-combine gradient manipulation methods.", "motivation": "Training PINNs is difficult due to conflicting objectives and different curvatures in multiple loss terms (PDE residuals, boundary conditions). Existing pre-combine gradient manipulation methods are fundamentally limited as they disrupt optimizer's internal preconditioning.", "method": "AutoBalance uses a post-combine strategy where each loss component gets its own independent adaptive optimizer, and the resulting preconditioned updates are aggregated afterwards.", "result": "Extensive experiments show AutoBalance consistently outperforms existing frameworks with significant reductions in solution error (MSE and L\u221e norms). It also amplifies effectiveness of other PINN methodologies.", "conclusion": "AutoBalance provides a superior training paradigm for PINNs that is orthogonal and complementary to existing methods, achieving better performance on challenging PDE benchmarks."}}
{"id": "2510.06800", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.06800", "abs": "https://arxiv.org/abs/2510.06800", "authors": ["Haotian Wu", "Shufan Jiang", "Chios Chen", "Yiyang Feng", "Hehai Lin", "Heqing Zou", "Yao Shu", "Yanran Li", "Chengwei Qin"], "title": "FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline", "comment": null, "summary": "As large language models (LLMs) advance in role-playing (RP) tasks, existing\nbenchmarks quickly become obsolete due to their narrow scope, outdated\ninteraction paradigms, and limited adaptability across diverse application\nscenarios. To address this gap, we introduce FURINA-Builder, a novel\nmulti-agent collaboration pipeline that automatically constructs fully\ncustomizable RP benchmarks at any scale. It enables evaluation of arbitrary\ncharacters across diverse scenarios and prompt formats, as the first benchmark\nbuilder in RP area for adaptable assessment. FURINA-Builder simulates dialogues\nbetween a test character and other characters drawn from a well-constructed\ncharacter-scene pool, while an LLM judge selects fine-grained evaluation\ndimensions and adjusts the test character's responses into final test\nutterances. Using this pipeline, we build FURINA-Bench, a new comprehensive\nrole-playing benchmark featuring both established and synthesized test\ncharacters, each assessed with dimension-specific evaluation criteria. Human\nevaluation and preliminary separability analysis justify our pipeline and\nbenchmark design. We conduct extensive evaluations of cutting-edge LLMs and\nfind that o3 and DeepSeek-R1 achieve the best performance on English and\nChinese RP tasks, respectively. Across all models, established characters\nconsistently outperform synthesized ones, with reasoning capabilities further\namplifying this disparity. Interestingly, we observe that model scale does not\nmonotonically reduce hallucinations. More critically, for reasoning LLMs, we\nuncover a novel trade-off: reasoning improves RP performance but simultaneously\nincreases RP hallucinations. This trade-off extends to a broader Pareto\nfrontier between RP performance and reliability for all LLMs. These findings\ndemonstrate the effectiveness of FURINA-Builder and the challenge posed by\nFURINA-Bench.", "AI": {"tldr": "FURINA-Builder is a multi-agent pipeline for automatically creating customizable role-playing benchmarks, addressing limitations of existing benchmarks. It enables evaluation of arbitrary characters across diverse scenarios and formats.", "motivation": "Existing role-playing benchmarks become obsolete quickly due to narrow scope, outdated interaction paradigms, and limited adaptability across diverse application scenarios.", "method": "FURINA-Builder uses multi-agent collaboration to simulate dialogues between test characters and other characters from a character-scene pool, with an LLM judge selecting evaluation dimensions and adjusting responses into final test utterances.", "result": "Built FURINA-Bench with both established and synthesized characters. Found o3 and DeepSeek-R1 perform best on English and Chinese RP tasks respectively. Established characters consistently outperform synthesized ones, and reasoning capabilities amplify this disparity. Model scale doesn't monotonically reduce hallucinations. Reasoning LLMs show a trade-off: improved RP performance but increased hallucinations.", "conclusion": "FURINA-Builder effectively addresses benchmark limitations and FURINA-Bench poses significant challenges, revealing a Pareto frontier between RP performance and reliability across all LLMs."}}
{"id": "2510.06805", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.06805", "abs": "https://arxiv.org/abs/2510.06805", "authors": ["Andr\u00e9 Greiner-Petter", "Maik Fr\u00f6be", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp", "Akiko Aizawa", "Martin Potthast"], "title": "Overview of the Plagiarism Detection Task at PAN 2025", "comment": "Working Notes at PAN at CLEF 2025", "summary": "The generative plagiarism detection task at PAN 2025 aims at identifying\nautomatically generated textual plagiarism in scientific articles and aligning\nthem with their respective sources. We created a novel large-scale dataset of\nautomatically generated plagiarism using three large language models: Llama,\nDeepSeek-R1, and Mistral. In this task overview paper, we outline the creation\nof this dataset, summarize and compare the results of all participants and four\nbaselines, and evaluate the results on the last plagiarism detection task from\nPAN 2015 in order to interpret the robustness of the proposed approaches. We\nfound that the current iteration does not invite a large variety of approaches\nas naive semantic similarity approaches based on embedding vectors provide\npromising results of up to 0.8 recall and 0.5 precision. In contrast, most of\nthese approaches underperform significantly on the 2015 dataset, indicating a\nlack in generalizability.", "AI": {"tldr": "The PAN 2025 generative plagiarism detection task focuses on identifying AI-generated plagiarism in scientific articles using a novel dataset created with Llama, DeepSeek-R1, and Mistral models. While semantic similarity approaches show promising results (0.8 recall, 0.5 precision) on the new dataset, they perform poorly on the 2015 dataset, revealing generalization issues.", "motivation": "To address the emerging challenge of detecting automatically generated plagiarism in scientific articles using modern large language models, and to evaluate the robustness of detection approaches across different datasets.", "method": "Created a large-scale dataset of automatically generated plagiarism using three LLMs (Llama, DeepSeek-R1, Mistral), compared participant results with four baselines, and evaluated approaches on the PAN 2015 dataset to assess generalizability.", "result": "Semantic similarity approaches based on embedding vectors achieved up to 0.8 recall and 0.5 precision on the new dataset, but most approaches significantly underperformed on the 2015 dataset, showing poor generalization.", "conclusion": "Current approaches lack diversity and generalizability, with naive semantic similarity methods performing well on the new dataset but failing on older datasets, indicating the need for more robust and generalized plagiarism detection methods."}}
{"id": "2510.06699", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06699", "abs": "https://arxiv.org/abs/2510.06699", "authors": ["Gal Fadlon", "Idan Arbiv", "Nimrod Berman", "Omri Azencot"], "title": "A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking", "comment": "Accepted to NeurIPS 2025; The first two authors contributed equally\n  and are co-leading authors", "summary": "Generating realistic time series data is critical for applications in\nhealthcare, finance, and science. However, irregular sampling and missing\nvalues present significant challenges. While prior methods address these\nirregularities, they often yield suboptimal results and incur high\ncomputational costs. Recent advances in regular time series generation, such as\nthe diffusion-based ImagenTime model, demonstrate strong, fast, and scalable\ngenerative capabilities by transforming time series into image representations,\nmaking them a promising solution. However, extending ImagenTime to irregular\nsequences using simple masking introduces \"unnatural\" neighborhoods, where\nmissing values replaced by zeros disrupt the learning process. To overcome\nthis, we propose a novel two-step framework: first, a Time Series Transformer\ncompletes irregular sequences, creating natural neighborhoods; second, a\nvision-based diffusion model with masking minimizes dependence on the completed\nvalues. This approach leverages the strengths of both completion and masking,\nenabling robust and efficient generation of realistic time series. Our method\nachieves state-of-the-art performance, achieving a relative improvement in\ndiscriminative score by $70\\%$ and in computational cost by $85\\%$. Code is at\nhttps://github.com/azencot-group/ImagenI2R.", "AI": {"tldr": "A novel two-step framework for generating realistic irregular time series data using completion and diffusion models, achieving 70% improvement in discriminative score and 85% reduction in computational cost.", "motivation": "Irregular sampling and missing values in time series data pose significant challenges for generation tasks. Existing methods yield suboptimal results with high computational costs, and simple masking approaches create unnatural neighborhoods that disrupt learning.", "method": "Two-step framework: 1) Time Series Transformer completes irregular sequences to create natural neighborhoods, 2) Vision-based diffusion model with masking minimizes dependence on completed values, leveraging strengths of both completion and masking.", "result": "State-of-the-art performance with 70% relative improvement in discriminative score and 85% reduction in computational cost compared to previous methods.", "conclusion": "The proposed approach effectively addresses irregular time series generation by combining completion and masking strategies, enabling robust and efficient generation of realistic time series data."}}
{"id": "2510.06811", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06811", "abs": "https://arxiv.org/abs/2510.06811", "authors": ["Philipp Mondorf", "Mingyang Wang", "Sebastian Gerstner", "Ahmad Dawar Hakimi", "Yihong Liu", "Leonor Veloso", "Shijia Zhou", "Hinrich Sch\u00fctze", "Barbara Plank"], "title": "BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods", "comment": "The 8th BlackboxNLP Workshop (Shared Task), 6 pages", "summary": "The Circuit Localization track of the Mechanistic Interpretability Benchmark\n(MIB) evaluates methods for localizing circuits within large language models\n(LLMs), i.e., subnetworks responsible for specific task behaviors. In this\nwork, we investigate whether ensembling two or more circuit localization\nmethods can improve performance. We explore two variants: parallel and\nsequential ensembling. In parallel ensembling, we combine attribution scores\nassigned to each edge by different methods-e.g., by averaging or taking the\nminimum or maximum value. In the sequential ensemble, we use edge attribution\nscores obtained via EAP-IG as a warm start for a more expensive but more\nprecise circuit identification method, namely edge pruning. We observe that\nboth approaches yield notable gains on the benchmark metrics, leading to a more\nprecise circuit identification approach. Finally, we find that taking a\nparallel ensemble over various methods, including the sequential ensemble,\nachieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB\nShared Task, comparing ensemble scores to official baselines across multiple\nmodel-task combinations.", "AI": {"tldr": "This paper explores ensembling methods for circuit localization in LLMs, showing that both parallel and sequential ensembling improve performance on the MIB benchmark.", "motivation": "To investigate whether combining multiple circuit localization methods can improve the accuracy of identifying subnetworks responsible for specific task behaviors in large language models.", "method": "Two ensembling approaches: parallel ensembling (combining attribution scores via averaging, min, or max) and sequential ensembling (using EAP-IG as warm start for edge pruning).", "result": "Both ensembling approaches yield notable gains on benchmark metrics, with parallel ensemble over various methods (including sequential ensemble) achieving the best results.", "conclusion": "Ensembling circuit localization methods leads to more precise circuit identification, with the combination of multiple methods providing optimal performance on the MIB benchmark."}}
{"id": "2510.06714", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06714", "abs": "https://arxiv.org/abs/2510.06714", "authors": ["Seohong Park", "Deepinder Mann", "Sergey Levine"], "title": "Dual Goal Representations", "comment": null, "summary": "In this work, we introduce dual goal representations for goal-conditioned\nreinforcement learning (GCRL). A dual goal representation characterizes a state\nby \"the set of temporal distances from all other states\"; in other words, it\nencodes a state through its relations to every other state, measured by\ntemporal distance. This representation provides several appealing theoretical\nproperties. First, it depends only on the intrinsic dynamics of the environment\nand is invariant to the original state representation. Second, it contains\nprovably sufficient information to recover an optimal goal-reaching policy,\nwhile being able to filter out exogenous noise. Based on this concept, we\ndevelop a practical goal representation learning method that can be combined\nwith any existing GCRL algorithm. Through diverse experiments on the OGBench\ntask suite, we empirically show that dual goal representations consistently\nimprove offline goal-reaching performance across 20 state- and pixel-based\ntasks.", "AI": {"tldr": "The paper introduces dual goal representations for goal-conditioned RL, which encode states through temporal distances to all other states, providing dynamics-invariant representations that improve goal-reaching performance.", "motivation": "To create goal representations that are invariant to state representation and capture essential temporal relationships between states for more effective goal-conditioned reinforcement learning.", "method": "Develop dual goal representations that characterize states by their temporal distances to all other states, and create a practical learning method that can be integrated with existing GCRL algorithms.", "result": "Empirical evaluation on OGBench shows consistent performance improvements across 20 state- and pixel-based tasks in offline goal-reaching scenarios.", "conclusion": "Dual goal representations provide theoretically sound and practically effective representations for GCRL that are dynamics-invariant and improve goal-reaching performance across diverse environments."}}
{"id": "2510.06825", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06825", "abs": "https://arxiv.org/abs/2510.06825", "authors": ["Chenpeng Wang", "Xiaojie Cheng", "Chunye Wang", "Linfeng Yang", "Lei Zhang"], "title": "Adaptive Tool Generation with Models as Tools and Reinforcement Learning", "comment": null, "summary": "Tool-augmented language models have demonstrated strong capabilities, but\ntheir reliance on live API access creates scalability and reliability\nchallenges during training and deployment. We propose MTR, a simulation-first\ntraining framework for tool-augmented reasoning. Instead of relying on live\nAPIs, MTR learns from complete ReAct traces with schema-validated, simulated\nobservations. Our approach operates through a multi-agent architecture where a\nToolMaker generates task-specific, OpenAI-compatible tool interfaces, an\nAutoAgent produces structured think-act-observe sequences, and a ToolActor\nsimulates realistic responses. Training proceeds in two stages: Stage-1\nSupervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning\nsequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy\nwith a composite trace reward that balances answer correctness and internal\nconsistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue,\n2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to\nlive-API systems and excels on reasoning-intensive tasks, suggesting that\neffective tool reasoning can be learned from structured traces without live\ninteractions.", "AI": {"tldr": "MTR is a simulation-first training framework for tool-augmented reasoning that learns from complete ReAct traces with simulated observations instead of live APIs, achieving competitive performance on multi-hop QA benchmarks.", "motivation": "Tool-augmented language models rely on live API access which creates scalability and reliability challenges during training and deployment.", "method": "Uses multi-agent architecture with ToolMaker, AutoAgent, and ToolActor to generate simulated observations. Training involves Stage-1 SFT for 'trace grammar' and Stage-2 GRPO with composite trace reward balancing correctness and consistency.", "result": "Attains competitive Exact Match scores to live-API systems across four multi-hop QA benchmarks (HotpotQA, MuSiQue, 2WikiMultiHopQA, Bamboogle) and excels on reasoning-intensive tasks.", "conclusion": "Effective tool reasoning can be learned from structured traces without live interactions."}}
{"id": "2510.06735", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.06735", "abs": "https://arxiv.org/abs/2510.06735", "authors": ["Zachris Bj\u00f6rkman", "Jorge Lor\u00eda", "Sophie Wharrie", "Samuel Kaski"], "title": "Incorporating Expert Knowledge into Bayesian Causal Discovery of Mixtures of Directed Acyclic Graphs", "comment": "28 pages, 18 figures", "summary": "Bayesian causal discovery benefits from prior information elicited from\ndomain experts, and in heterogeneous domains any prior knowledge would be badly\nneeded. However, so far prior elicitation approaches have assumed a single\ncausal graph and hence are not suited to heterogeneous domains. We propose a\ncausal elicitation strategy for heterogeneous settings, based on Bayesian\nexperimental design (BED) principles, and a variational mixture structure\nlearning (VaMSL) method -- extending the earlier differentiable Bayesian\nstructure learning (DiBS) method -- to iteratively infer mixtures of causal\nBayesian networks (CBNs). We construct an informative graph prior incorporating\nelicited expert feedback in the inference of mixtures of CBNs. Our proposed\nmethod successfully produces a set of alternative causal models (mixture\ncomponents or clusters), and achieves an improved structure learning\nperformance on heterogeneous synthetic data when informed by a simulated\nexpert. Finally, we demonstrate that our approach is capable of capturing\ncomplex distributions in a breast cancer database.", "AI": {"tldr": "A Bayesian causal discovery method for heterogeneous domains that combines expert elicitation with variational mixture structure learning to infer multiple causal Bayesian networks.", "motivation": "Existing prior elicitation approaches assume a single causal graph and are unsuitable for heterogeneous domains where multiple causal models may exist.", "method": "Proposes causal elicitation strategy based on Bayesian experimental design (BED) and variational mixture structure learning (VaMSL) extending DiBS method to iteratively infer mixtures of causal Bayesian networks.", "result": "Method successfully produces alternative causal models (mixture components), achieves improved structure learning on heterogeneous synthetic data with simulated expert feedback, and captures complex distributions in breast cancer database.", "conclusion": "The approach effectively handles heterogeneous causal discovery by incorporating expert knowledge through Bayesian experimental design and variational mixture learning."}}
{"id": "2510.06826", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06826", "abs": "https://arxiv.org/abs/2510.06826", "authors": ["Kaixiang Mo", "Yuxin Shi", "Weiwei Weng", "Zhiqiang Zhou", "Shuman Liu", "Haibo Zhang", "Anxiang Zeng"], "title": "Mid-Training of Large Language Models: A Survey", "comment": null, "summary": "Large language models (LLMs) are typically developed through large-scale\npre-training followed by task-specific fine-tuning. Recent advances highlight\nthe importance of an intermediate mid-training stage, where models undergo\nmultiple annealing-style phases that refine data quality, adapt optimization\nschedules, and extend context length. This stage mitigates diminishing returns\nfrom noisy tokens, stabilizes convergence, and expands model capability in late\ntraining. Its effectiveness can be explained through gradient noise scale, the\ninformation bottleneck, and curriculum learning, which together promote\ngeneralization and abstraction. Despite widespread use in state-of-the-art\nsystems, there has been no prior survey of mid-training as a unified paradigm.\nWe introduce the first taxonomy of LLM mid-training spanning data distribution,\nlearning-rate scheduling, and long-context extension. We distill practical\ninsights, compile evaluation benchmarks, and report gains to enable structured\ncomparisons across models. We also identify open challenges and propose avenues\nfor future research and practice.", "AI": {"tldr": "This paper introduces the first comprehensive survey of mid-training as a unified paradigm in LLM development, providing taxonomy, practical insights, benchmarks, and identifying future research directions.", "motivation": "Despite widespread use in state-of-the-art systems, there has been no prior survey of mid-training as a unified paradigm to understand its effectiveness and systematize approaches.", "method": "The authors introduce the first taxonomy of LLM mid-training spanning data distribution, learning-rate scheduling, and long-context extension, and compile evaluation benchmarks for structured comparisons.", "result": "The paper distills practical insights, reports gains from mid-training approaches, and enables structured comparisons across models through compiled benchmarks.", "conclusion": "Mid-training is an important intermediate stage that mitigates diminishing returns, stabilizes convergence, and expands model capability, with identified open challenges and proposed future research avenues."}}
{"id": "2510.06762", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06762", "abs": "https://arxiv.org/abs/2510.06762", "authors": ["Shivam Padmani", "Akshay Joshi"], "title": "Function regression using the forward forward training and inferring paradigm", "comment": "Keywords: Neural Networks, Forward Forward training, Function\n  Regression, Physical Neural Networks, Analog Computing", "summary": "Function regression/approximation is a fundamental application of machine\nlearning. Neural networks (NNs) can be easily trained for function regression\nusing a sufficient number of neurons and epochs. The forward-forward learning\nalgorithm is a novel approach for training neural networks without\nbackpropagation, and is well suited for implementation in neuromorphic\ncomputing and physical analogs for neural networks. To the best of the authors'\nknowledge, the Forward Forward paradigm of training and inferencing NNs is\ncurrently only restricted to classification tasks. This paper introduces a new\nmethodology for approximating functions (function regression) using the\nForward-Forward algorithm. Furthermore, the paper evaluates the developed\nmethodology on univariate and multivariate functions, and provides preliminary\nstudies of extending the proposed Forward-Forward regression to Kolmogorov\nArnold Networks, and Deep Physical Neural Networks.", "AI": {"tldr": "This paper introduces a novel methodology for function regression using the Forward-Forward algorithm, extending its application beyond classification tasks to include univariate and multivariate function approximation, with preliminary studies on Kolmogorov Arnold Networks and Deep Physical Neural Networks.", "motivation": "The motivation is to extend the Forward-Forward learning algorithm, which is well-suited for neuromorphic computing and physical neural networks, from classification tasks to function regression/approximation tasks, as this fundamental machine learning application currently lacks Forward-Forward implementations.", "method": "The paper develops a new methodology for function regression using the Forward-Forward algorithm, evaluating it on both univariate and multivariate functions, and conducts preliminary studies on extending this approach to Kolmogorov Arnold Networks and Deep Physical Neural Networks.", "result": "The paper presents results from evaluating the developed Forward-Forward regression methodology on various function approximation tasks, though specific performance metrics are not detailed in the abstract.", "conclusion": "The paper successfully demonstrates that the Forward-Forward algorithm can be extended to function regression tasks, opening up new possibilities for implementing regression in neuromorphic computing systems and physical neural networks that benefit from the Forward-Forward paradigm."}}
{"id": "2510.06841", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06841", "abs": "https://arxiv.org/abs/2510.06841", "authors": ["Giorgos Filandrianos", "Orfeas Menis Mastromichalakis", "Wafaa Mohammed", "Giuseppe Attanasio", "Chrysoula Zerva"], "title": "GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics", "comment": "Accepted for publication at the 10th Conference of Machine\n  Translation (WMT25), co-located with EMNLP 2025", "summary": "Gender bias in machine translation (MT) systems has been extensively\ndocumented, but bias in automatic quality estimation (QE) metrics remains\ncomparatively underexplored. Existing studies suggest that QE metrics can also\nexhibit gender bias, yet most analyses are limited by small datasets, narrow\noccupational coverage, and restricted language variety. To address this gap, we\nintroduce a large-scale challenge set specifically designed to probe the\nbehavior of QE metrics when evaluating translations containing gender-ambiguous\noccupational terms. Building on the GAMBIT corpus of English texts with\ngender-ambiguous occupations, we extend coverage to three source languages that\nare genderless or natural-gendered, and eleven target languages with\ngrammatical gender, resulting in 33 source-target language pairs. Each source\ntext is paired with two target versions differing only in the grammatical\ngender of the occupational term(s) (masculine vs. feminine), with all dependent\ngrammatical elements adjusted accordingly. An unbiased QE metric should assign\nequal or near-equal scores to both versions. The dataset's scale, breadth, and\nfully parallel design, where the same set of texts is aligned across all\nlanguages, enables fine-grained bias analysis by occupation and systematic\ncomparisons across languages.", "AI": {"tldr": "This paper introduces a large-scale challenge set to analyze gender bias in automatic quality estimation (QE) metrics across 33 language pairs, focusing on gender-ambiguous occupational terms.", "motivation": "Gender bias in machine translation has been well-documented, but bias in automatic quality estimation metrics remains underexplored, with existing studies limited by small datasets and narrow coverage.", "method": "Built on the GAMBIT corpus, the authors extend coverage to three source languages (genderless or natural-gendered) and eleven target languages with grammatical gender, creating parallel texts where only the grammatical gender of occupational terms differs between masculine and feminine versions.", "result": "The dataset enables fine-grained bias analysis by occupation and systematic comparisons across languages, with the expectation that unbiased QE metrics should assign equal scores to both gender versions.", "conclusion": "The large-scale, fully parallel dataset provides a comprehensive framework for detecting and analyzing gender bias in quality estimation metrics across diverse language pairs and occupational contexts."}}
{"id": "2510.06776", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06776", "abs": "https://arxiv.org/abs/2510.06776", "authors": ["Phillip Rothenbeck", "Sai Karthikeya Vemuri", "Niklas Penzel", "Joachim Denzler"], "title": "Modeling COVID-19 Dynamics in German States Using Physics-Informed Neural Networks", "comment": "19 pages, 7 figures, 2 tables", "summary": "The COVID-19 pandemic has highlighted the need for quantitative modeling and\nanalysis to understand real-world disease dynamics. In particular, post hoc\nanalyses using compartmental models offer valuable insights into the\neffectiveness of public health interventions, such as vaccination strategies\nand containment policies. However, such compartmental models like SIR\n(Susceptible-Infectious-Recovered) often face limitations in directly\nincorporating noisy observational data. In this work, we employ\nPhysics-Informed Neural Networks (PINNs) to solve the inverse problem of the\nSIR model using infection data from the Robert Koch Institute (RKI). Our main\ncontribution is a fine-grained, spatio-temporal analysis of COVID-19 dynamics\nacross all German federal states over a three-year period. We estimate\nstate-specific transmission and recovery parameters and time-varying\nreproduction number (R_t) to track the pandemic progression. The results\nhighlight strong variations in transmission behavior across regions, revealing\ncorrelations with vaccination uptake and temporal patterns associated with\nmajor pandemic phases. Our findings demonstrate the utility of PINNs in\nlocalized, long-term epidemiological modeling.", "AI": {"tldr": "Using Physics-Informed Neural Networks (PINNs) to solve inverse SIR model problems with German COVID-19 data, enabling spatio-temporal analysis of transmission parameters across federal states over 3 years.", "motivation": "Compartmental models like SIR have limitations in incorporating noisy observational data directly. Need for quantitative modeling to understand COVID-19 dynamics and evaluate public health interventions.", "method": "Employ Physics-Informed Neural Networks (PINNs) to solve inverse SIR model problems using infection data from Robert Koch Institute (RKI) for German federal states.", "result": "Estimated state-specific transmission and recovery parameters and time-varying reproduction number (R_t). Found strong regional variations in transmission behavior correlated with vaccination uptake and pandemic phases.", "conclusion": "Demonstrates utility of PINNs for localized, long-term epidemiological modeling with fine-grained spatio-temporal analysis capabilities."}}
{"id": "2510.06843", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06843", "abs": "https://arxiv.org/abs/2510.06843", "authors": ["Xuhang Chen", "Zhifan Song", "Deyi Ji", "Shuo Gao", "Lanyun Zhu"], "title": "SID: Multi-LLM Debate Driven by Self Signals", "comment": null, "summary": "Large Language Models (LLMs) have exhibited impressive capabilities across\ndiverse application domains. Recent work has explored Multi-LLM Agent Debate\n(MAD) as a way to enhance performance by enabling multiple LLMs to discuss and\nrefine responses iteratively. Nevertheless, existing MAD methods predominantly\nfocus on utilizing external structures, such as debate graphs, using\nLLM-as-a-Judge, while neglecting the application of self signals, such as token\nlogits and attention, that arise during generation. This omission leads to\nredundant computation and potential performance degradation. In this paper, we\nshift the focus to the self signals of multi-LLM debate and introduce a\nSelf-Signals Driven Multi-LLM Debate (SID), which leverages two types of\nself-signals: model-level confidence and token-level semantic focus, to\nadaptively guide the debate process. Our approach enables high-confidence\nagents to exit early at the model level and compress the redundant debate\ncontents based on the attention mechanism. We evaluate our method on various\nLLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental\nresults demonstrate that our method not only outperforms existing MAD\ntechniques in accuracy but also reduces token consumption, highlighting the\neffectiveness of utilizing self signals in enhancing both the performance and\nefficiency of multi-agent debate systems. Our code will be available\nat~\\href{https://github.com/xuhang2019/SID}{\\texttt{https://github.com/xuhang2019/SID}}.", "AI": {"tldr": "The paper introduces SID, a Self-Signals Driven Multi-LLM Debate method that uses model-level confidence and token-level semantic focus to optimize multi-agent debate systems, improving both accuracy and efficiency.", "motivation": "Existing Multi-LLM Agent Debate methods focus on external structures and neglect self signals like token logits and attention, leading to redundant computation and potential performance degradation.", "method": "SID leverages two types of self-signals: model-level confidence for early exit of high-confidence agents, and token-level semantic focus based on attention mechanism to compress redundant debate contents.", "result": "Experimental results show SID outperforms existing MAD techniques in accuracy and reduces token consumption across various LLMs and Multimodal LLMs on multiple challenging benchmarks.", "conclusion": "Utilizing self signals effectively enhances both performance and efficiency of multi-agent debate systems, demonstrating the value of internal model signals in optimizing collaborative AI systems."}}
{"id": "2510.06790", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06790", "abs": "https://arxiv.org/abs/2510.06790", "authors": ["Tavish McDonald", "Bo Lei", "Stanislav Fort", "Bhavya Kailkhura", "Brian Bartoldson"], "title": "Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness", "comment": "17 pages", "summary": "Models are susceptible to adversarially out-of-distribution (OOD) data\ndespite large training-compute investments into their robustification. Zaremba\net al. (2025) make progress on this problem at test time, showing LLM reasoning\nimproves satisfaction of model specifications designed to thwart attacks,\nresulting in a correlation between reasoning effort and robustness to\njailbreaks. However, this benefit of test compute fades when attackers are\ngiven access to gradients or multimodal inputs. We address this gap, clarifying\nthat inference-compute offers benefits even in such cases. Our approach argues\nthat compositional generalization, through which OOD data is understandable via\nits in-distribution (ID) components, enables adherence to defensive\nspecifications on adversarially OOD inputs. Namely, we posit the Robustness\nfrom Inference Compute Hypothesis (RICH): inference-compute defenses profit as\nthe model's training data better reflects the attacked data's components. We\nempirically support this hypothesis across vision language model and attack\ntypes, finding robustness gains from test-time compute if specification\nfollowing on OOD data is unlocked by compositional generalization, while RL\nfinetuning and protracted reasoning are not critical. For example, increasing\nemphasis on defensive specifications via prompting lowers the success rate of\ngradient-based multimodal attacks on VLMs robustified by adversarial\npretraining, but this same intervention provides no such benefit to\nnot-robustified models. This correlation of inference-compute's robustness\nbenefit with base model robustness is the rich-get-richer dynamic of the RICH:\nattacked data components are more ID for robustified models, aiding\ncompositional generalization to OOD data. Accordingly, we advise layering\ntrain-time and test-time defenses to obtain their synergistic benefit.", "AI": {"tldr": "The paper proposes the Robustness from Inference Compute Hypothesis (RICH), arguing that inference-compute defenses work better when models can compositionally generalize from in-distribution components to understand out-of-distribution adversarial data, especially when base models are already robust.", "motivation": "To address the limitation that test-time compute defenses lose effectiveness against gradient-based or multimodal attacks, and to show that inference-compute can still provide robustness benefits in these challenging scenarios.", "method": "Empirical validation across vision language models and attack types, examining how compositional generalization enables adherence to defensive specifications on adversarially OOD inputs through inference-compute defenses.", "result": "Robustness gains from test-time compute occur when compositional generalization enables specification following on OOD data, with inference-compute benefits correlating with base model robustness - creating a rich-get-richer dynamic.", "conclusion": "Train-time and test-time defenses should be layered synergistically, as inference-compute defenses work best when base models are already robustified, enabling compositional generalization from ID components to understand OOD adversarial data."}}
{"id": "2510.06847", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06847", "abs": "https://arxiv.org/abs/2510.06847", "authors": ["Pontakorn Trakuekul", "Attapol T. Rutherford", "Jullajak Karnjanaekarin", "Narongkorn Panitsrisit", "Sumana Sumanakul"], "title": "OpenJAI-v1.0: An Open Thai Large Language Model", "comment": null, "summary": "We introduce OpenJAI-v1.0, an open-source large language model for Thai and\nEnglish, developed from the Qwen3-14B model. Our work focuses on boosting\nperformance on practical tasks through carefully curated data across three key\nuse cases: instruction following, long-context understanding, and tool use.\nEvaluation results show that OpenJAI-v1.0 improves on the capabilities of its\nbase model and outperforms other leading open-source Thai models on a diverse\nsuite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is\npublicly released as another alternative NLP resource for the Thai AI\ncommunity.", "AI": {"tldr": "OpenJAI-v1.0 is an open-source Thai-English LLM based on Qwen3-14B, enhanced for instruction following, long-context understanding, and tool use.", "motivation": "To provide an improved open-source NLP resource for the Thai AI community by boosting performance on practical tasks.", "method": "Developed from Qwen3-14B model using carefully curated data across three key use cases: instruction following, long-context understanding, and tool use.", "result": "Outperforms other leading open-source Thai models on diverse benchmarks while avoiding catastrophic forgetting, improving on base model capabilities.", "conclusion": "OpenJAI-v1.0 is publicly released as an alternative NLP resource for the Thai AI community."}}
{"id": "2510.06819", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06819", "abs": "https://arxiv.org/abs/2510.06819", "authors": ["Giovanni Donghi", "Daniele Zambon", "Luca Pasa", "Cesare Alippi", "Nicol\u00f2 Navarin"], "title": "The Unreasonable Effectiveness of Randomized Representations in Online Continual Graph Learning", "comment": null, "summary": "Catastrophic forgetting is one of the main obstacles for Online Continual\nGraph Learning (OCGL), where nodes arrive one by one, distribution drifts may\noccur at any time and offline training on task-specific subgraphs is not\nfeasible. In this work, we explore a surprisingly simple yet highly effective\napproach for OCGL: we use a fixed, randomly initialized encoder to generate\nrobust and expressive node embeddings by aggregating neighborhood information,\ntraining online only a lightweight classifier. By freezing the encoder, we\neliminate drifts of the representation parameters, a key source of forgetting,\nobtaining embeddings that are both expressive and stable. When evaluated across\nseveral OCGL benchmarks, despite its simplicity and lack of memory buffer, this\napproach yields consistent gains over state-of-the-art methods, with surprising\nimprovements of up to 30% and performance often approaching that of the joint\noffline-training upper bound. These results suggest that in OCGL, catastrophic\nforgetting can be minimized without complex replay or regularization by\nembracing architectural simplicity and stability.", "AI": {"tldr": "A simple yet effective approach for Online Continual Graph Learning (OCGL) that uses a fixed, randomly initialized encoder to generate stable node embeddings, training only a lightweight classifier online to minimize catastrophic forgetting.", "motivation": "To address catastrophic forgetting in OCGL where nodes arrive sequentially and distribution drifts occur, without requiring complex replay mechanisms or offline training.", "method": "Freeze a randomly initialized graph encoder to generate stable node embeddings, and train only a lightweight classifier online to prevent representation parameter drifts.", "result": "Achieves consistent gains over state-of-the-art methods, with up to 30% improvement, often approaching joint offline-training upper bound performance, without using memory buffers.", "conclusion": "Catastrophic forgetting in OCGL can be effectively minimized through architectural simplicity and stability rather than complex replay or regularization strategies."}}
{"id": "2510.06866", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06866", "abs": "https://arxiv.org/abs/2510.06866", "authors": ["Wafaa Mohammed", "Vlad Niculae", "Chrysoula Zerva"], "title": "Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding", "comment": null, "summary": "Large language models (LLMs) have emerged as strong contenders in machine\ntranslation.Yet, they still struggle to adequately handle discourse phenomena,\nsuch as pronoun resolution and lexical cohesion at the document level. In this\nstudy, we thoroughly investigate the discourse phenomena performance of LLMs in\ncontext-aware translation. We demonstrate that discourse knowledge is encoded\nwithin LLMs and propose the use of quality-aware decoding (QAD) to effectively\nextract this knowledge, showcasing its superiority over other decoding\napproaches through comprehensive analysis. Furthermore, we illustrate that QAD\nenhances the semantic richness of translations and aligns them more closely\nwith human preferences.", "AI": {"tldr": "LLMs struggle with discourse phenomena in translation. This study investigates their performance, shows discourse knowledge is encoded in LLMs, and proposes Quality-Aware Decoding (QAD) to extract this knowledge effectively, improving translation quality and human alignment.", "motivation": "Large language models (LLMs) are strong in machine translation but struggle with discourse phenomena like pronoun resolution and lexical cohesion at document level.", "method": "Thorough investigation of LLMs' discourse phenomena performance in context-aware translation, demonstrating discourse knowledge encoding within LLMs and proposing Quality-Aware Decoding (QAD) to extract this knowledge.", "result": "QAD shows superiority over other decoding approaches, enhances semantic richness of translations, and aligns them more closely with human preferences.", "conclusion": "Quality-Aware Decoding effectively extracts discourse knowledge from LLMs, improving translation quality and better matching human preferences."}}
{"id": "2510.06824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06824", "abs": "https://arxiv.org/abs/2510.06824", "authors": ["Linus Kreitner", "Paul Hager", "Jonathan Mengedoht", "Georgios Kaissis", "Daniel Rueckert", "Martin J. Menten"], "title": "Efficient numeracy in language models through single-token number embeddings", "comment": null, "summary": "To drive progress in science and engineering, large language models (LLMs)\nmust be able to process large amounts of numerical data and solve long\ncalculations efficiently. This is currently only possible through the use of\nexternal tools or extensive reasoning chains, either limiting the numerical\nintuition of LLMs or limiting the length of problems they can solve. We show\nthat frontier LLMs require excessive amounts of reasoning tokens to solve even\nbasic calculations, which is exacerbated by their tokenization strategies that\nsplit single numbers into multiple tokens. This motivates the need for\nefficient and effective single-token number encodings. We introduce a set of\ndesiderata for such encodings and show that existing approaches fail to fulfill\nthem. To address these shortcomings, we propose BitTokens, a novel tokenization\nstrategy that embeds any number into a single token using its IEEE 754 binary\nfloating-point representation. Through extensive experiments we show that our\nBitTokens allow even small language models to learn algorithms that solve basic\narithmetic operations nearly perfectly. This newly gained efficiency could\nexpand the length and complexity of problems language models can solve.", "AI": {"tldr": "BitTokens: A novel tokenization method that encodes numbers as single tokens using IEEE 754 binary floating-point representation, enabling LLMs to process numerical data more efficiently and solve arithmetic operations with high accuracy.", "motivation": "Current LLMs struggle with numerical data processing due to inefficient tokenization that splits numbers into multiple tokens, requiring excessive reasoning tokens for basic calculations and limiting the complexity of problems they can solve.", "method": "Proposed BitTokens - a tokenization strategy that embeds any number into a single token using its IEEE 754 binary floating-point representation, fulfilling desiderata for efficient number encodings that existing approaches fail to meet.", "result": "Extensive experiments show BitTokens enable even small language models to learn algorithms that solve basic arithmetic operations nearly perfectly, significantly improving numerical processing efficiency.", "conclusion": "BitTokens' efficient single-token number encoding could expand the length and complexity of problems that language models can solve, addressing current limitations in numerical data processing."}}
{"id": "2510.06870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06870", "abs": "https://arxiv.org/abs/2510.06870", "authors": ["Yining Wang", "Jinman Zhao", "Chuangxin Zhao", "Shuhao Guan", "Gerald Penn", "Shinan Liu"], "title": "$\u03bb$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences", "comment": "9 pages", "summary": "Reinforcement Learning with Human Feedback (RLHF) has been the dominant\napproach for improving the reasoning capabilities of Large Language Models\n(LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has\nsimplified this paradigm by replacing the reward and value models with\nrule-based verifiers. A prominent example is Group Relative Policy Optimization\n(GRPO). However, GRPO inherently suffers from a length bias, since the same\nadvantage is uniformly assigned to all tokens of a response. As a result,\nlonger responses distribute the reward over more tokens and thus contribute\ndisproportionately to gradient updates. Several variants, such as DAPO and Dr.\nGRPO, modify the token-level aggregation of the loss, yet these methods remain\nheuristic and offer limited interpretability regarding their implicit token\npreferences. In this work, we explore the possibility of allowing the model to\nlearn its own token preference during optimization. We unify existing\nframeworks under a single formulation and introduce a learnable parameter\n$\\lambda$ that adaptively controls token-level weighting. We use $\\lambda$-GRPO\nto denote our method, and we find that $\\lambda$-GRPO achieves consistent\nimprovements over vanilla GRPO and DAPO on multiple mathematical reasoning\nbenchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\\lambda$-GRPO\nimproves average accuracy by $+1.9\\%$, $+1.0\\%$, and $+1.7\\%$ compared to GRPO,\nrespectively. Importantly, these gains come without any modifications to the\ntraining data or additional computational cost, highlighting the effectiveness\nand practicality of learning token preferences.", "AI": {"tldr": "The paper introduces \u03bb-GRPO, a method that addresses length bias in RLHF by learning adaptive token-level preferences during optimization, achieving consistent improvements in mathematical reasoning benchmarks without additional computational cost.", "motivation": "Existing RLHF methods like GRPO suffer from length bias where longer responses disproportionately influence gradient updates, and current variants like DAPO and Dr. GRPO lack interpretability and remain heuristic in their token preference handling.", "method": "The authors unify existing frameworks under a single formulation and introduce a learnable parameter \u03bb that adaptively controls token-level weighting during optimization, allowing the model to learn its own token preferences.", "result": "\u03bb-GRPO achieves consistent improvements over vanilla GRPO and DAPO on multiple mathematical reasoning benchmarks, with average accuracy improvements of +1.9%, +1.0%, and +1.7% on Qwen2.5 models with 1.5B, 3B, and 7B parameters respectively.", "conclusion": "The method demonstrates that learning token preferences during optimization is effective and practical, providing performance gains without modifications to training data or additional computational costs."}}
{"id": "2510.06828", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06828", "abs": "https://arxiv.org/abs/2510.06828", "authors": ["Michael Keiblinger"], "title": "Recurrence-Complete Frame-based Action Models", "comment": null, "summary": "In recent years, attention-like mechanisms have been used to great success in\nthe space of large language models, unlocking scaling potential to a previously\nunthinkable extent. \"Attention Is All You Need\" famously claims RNN cells are\nnot needed in conjunction with attention. We challenge this view. In this\npaper, we point to existing proofs that architectures with fully parallelizable\nforward or backward passes cannot represent classes of problems specifically\ninteresting for long-running agentic tasks. We further conjecture a critical\ntime t beyond which non-recurrence-complete models fail to aggregate inputs\ncorrectly, with concrete implications for agentic systems (e.g., software\nengineering agents). To address this, we introduce a recurrence-complete\narchitecture and train it on GitHub-derived action sequences. Loss follows a\npower law in the trained sequence length while the parameter count remains\nfixed. Moreover, longer-sequence training always amortizes its linearly\nincreasing wall-time cost, yielding lower loss as a function of wall time.", "AI": {"tldr": "The paper challenges the view that attention mechanisms alone are sufficient, arguing that recurrent architectures are necessary for long-running agentic tasks. It introduces a recurrence-complete architecture that shows improved performance with longer training sequences.", "motivation": "To address limitations of non-recurrent architectures in handling long-running agentic tasks, particularly in scenarios where fully parallelizable models fail to correctly aggregate inputs over extended time periods.", "method": "Introduces a recurrence-complete architecture and trains it on GitHub-derived action sequences, analyzing loss patterns and computational efficiency across different sequence lengths.", "result": "Loss follows a power law with trained sequence length while maintaining fixed parameter count. Longer-sequence training amortizes its linearly increasing wall-time cost, yielding lower loss as a function of wall time.", "conclusion": "Recurrent architectures are essential for certain classes of problems in agentic systems, and the proposed recurrence-complete model demonstrates scalable performance improvements with longer training sequences."}}
{"id": "2510.06889", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06889", "abs": "https://arxiv.org/abs/2510.06889", "authors": ["Zaid Alyafeai", "Maged S. Al-Shaibani", "Bernard Ghanem"], "title": "MeXtract: Light-Weight Metadata Extraction from Scientific Papers", "comment": null, "summary": "Metadata plays a critical role in indexing, documenting, and analyzing\nscientific literature, yet extracting it accurately and efficiently remains a\nchallenging task. Traditional approaches often rely on rule-based or\ntask-specific models, which struggle to generalize across domains and schema\nvariations. In this paper, we present MeXtract, a family of lightweight\nlanguage models designed for metadata extraction from scientific papers. The\nmodels, ranging from 0.5B to 3B parameters, are built by fine-tuning Qwen 2.5\ncounterparts. In their size family, MeXtract achieves state-of-the-art\nperformance on metadata extraction on the MOLE benchmark. To further support\nevaluation, we extend the MOLE benchmark to incorporate model-specific\nmetadata, providing an out-of-domain challenging subset. Our experiments show\nthat fine-tuning on a given schema not only yields high accuracy but also\ntransfers effectively to unseen schemas, demonstrating the robustness and\nadaptability of our approach. We release all the code, datasets, and models\nopenly for the research community.", "AI": {"tldr": "MeXtract is a family of lightweight language models (0.5B-3B parameters) for metadata extraction from scientific papers, achieving SOTA performance on MOLE benchmark and showing strong transfer learning capabilities across schemas.", "motivation": "Traditional metadata extraction methods struggle with generalization across domains and schema variations, creating a need for more robust and adaptable solutions.", "method": "Fine-tuned Qwen 2.5 models to create MeXtract family, extended MOLE benchmark with model-specific metadata for out-of-domain evaluation.", "result": "Achieved state-of-the-art performance on metadata extraction, demonstrated effective transfer to unseen schemas, showing robustness and adaptability.", "conclusion": "Fine-tuning on specific schemas yields high accuracy and transfers well to new schemas, providing an effective solution for metadata extraction challenges."}}
{"id": "2510.06831", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.06831", "abs": "https://arxiv.org/abs/2510.06831", "authors": ["Syed Shazaib Shah", "Daoliang Tan"], "title": "Early wind turbine alarm prediction based on machine learning: AlarmForecasting", "comment": "International Journal of Electrical Power and Energy Systems", "summary": "Alarm data is pivotal in curbing fault behavior in Wind Turbines (WTs) and\nforms the backbone for advancedpredictive monitoring systems. Traditionally,\nresearch cohorts have been confined to utilizing alarm data solelyas a\ndiagnostic tool, merely indicative of unhealthy status. However, this study\naims to offer a transformativeleap towards preempting alarms, preventing alarms\nfrom triggering altogether, and consequently avertingimpending failures. Our\nproposed Alarm Forecasting and Classification (AFC) framework is designed on\ntwosuccessive modules: first, the regression module based on long short-term\nmemory (LSTM) for time-series alarmforecasting, and thereafter, the\nclassification module to implement alarm tagging on the forecasted alarm.\nThisway, the entire alarm taxonomy can be forecasted reliably rather than a few\nspecific alarms. 14 Senvion MM82turbines with an operational period of 5 years\nare used as a case study; the results demonstrated 82%, 52%,and 41% accurate\nforecasts for 10, 20, and 30 min alarm forecasts, respectively. The results\nsubstantiateanticipating and averting alarms, which is significant in curbing\nalarm frequency and enhancing operationalefficiency through proactive\nintervention.", "AI": {"tldr": "The paper proposes an Alarm Forecasting and Classification (AFC) framework using LSTM for predicting wind turbine alarms 10-30 minutes in advance, achieving 82-41% accuracy.", "motivation": "Traditional approaches use alarm data only for diagnostics after faults occur, but this study aims to prevent alarms from triggering altogether to avoid impending failures.", "method": "Two-stage framework: LSTM-based regression for time-series alarm forecasting, followed by classification module for alarm tagging on forecasted alarms.", "result": "Experimental results on 14 Senvion MM82 turbines over 5 years showed 82%, 52%, and 41% accurate forecasts for 10, 20, and 30 minute alarm forecasts respectively.", "conclusion": "The framework successfully anticipates and averts alarms, significantly reducing alarm frequency and enhancing operational efficiency through proactive intervention."}}
{"id": "2510.06915", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06915", "abs": "https://arxiv.org/abs/2510.06915", "authors": ["Zecheng Tang", "Baibei Ji", "Quantong Qiu", "Haitian Wang", "Xiaobo Liang", "Juntao Li", "Min Zhang"], "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling", "comment": null, "summary": "Reward model (RM) plays a pivotal role in aligning large language model (LLM)\nwith human preferences. As real-world applications increasingly involve long\nhistory trajectories, e.g., LLM agent, it becomes indispensable to evaluate\nwhether a model's responses are not only high-quality but also grounded in and\nconsistent with the provided context. Yet, current RMs remain confined to\nshort-context settings and primarily focus on response-level attributes (e.g.,\nsafety or helpfulness), while largely neglecting the critical dimension of long\ncontext-response consistency. In this work, we introduce Long-RewardBench, a\nbenchmark specifically designed for long-context RM evaluation, featuring both\nPairwise Comparison and Best-of-N tasks. Our preliminary study reveals that\neven state-of-the-art generative RMs exhibit significant fragility in\nlong-context scenarios, failing to maintain context-aware preference judgments.\nMotivated by the analysis of failure patterns observed in model outputs, we\npropose a general multi-stage training strategy that effectively scales\narbitrary models into robust Long-context RMs (LongRMs). Experiments show that\nour approach not only substantially improves performance on long-context\nevaluation but also preserves strong short-context capability. Notably, our 8B\nLongRM outperforms much larger 70B-scale baselines and matches the performance\nof the proprietary Gemini 2.5 Pro model.", "AI": {"tldr": "Long-RewardBench is a benchmark for evaluating reward models on long-context scenarios, revealing that current models struggle with context-response consistency. The authors propose a multi-stage training strategy to create robust Long-context Reward Models (LongRMs) that outperform larger baselines.", "motivation": "Current reward models are limited to short contexts and focus mainly on response-level attributes, neglecting long context-response consistency which is crucial for real-world applications like LLM agents.", "method": "Proposed a multi-stage training strategy to scale arbitrary models into robust Long-context Reward Models (LongRMs), using the Long-RewardBench benchmark with Pairwise Comparison and Best-of-N tasks.", "result": "The 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of proprietary Gemini 2.5 Pro model, while maintaining strong short-context capability.", "conclusion": "The proposed multi-stage training approach effectively creates robust long-context reward models that address the critical gap in context-response consistency for long history trajectories."}}
{"id": "2510.06834", "categories": ["cs.LG", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.06834", "abs": "https://arxiv.org/abs/2510.06834", "authors": ["Vasileios Titopoulos", "Kosmas Alexandridis", "Giorgos Dimitrakopoulos"], "title": "Vectorized FlashAttention with Low-cost Exponential Computation in RISC-V Vector Processors", "comment": null, "summary": "Attention is a core operation in numerous machine learning and artificial\nintelligence models. This work focuses on the acceleration of attention kernel\nusing FlashAttention algorithm, in vector processors, particularly those based\non the RISC-V instruction set architecture (ISA). This work represents the\nfirst effort to vectorize FlashAttention, minimizing scalar code and\nsimplifying the computational complexity of evaluating exponentials needed by\nsoftmax used in attention. By utilizing a low-cost approximation for\nexponentials in floating-point arithmetic, we reduce the cost of computing the\nexponential function without the need to extend baseline vector ISA with new\ncustom instructions. Also, appropriate tiling strategies are explored with the\ngoal to improve memory locality. Experimental results highlight the scalability\nof our approach, demonstrating significant performance gains with the\nvectorized implementations when processing attention layers in practical\napplications.", "AI": {"tldr": "This paper presents a vectorized implementation of FlashAttention for RISC-V processors, using low-cost exponential approximations and tiling strategies to improve performance.", "motivation": "To accelerate attention kernels in machine learning models by vectorizing FlashAttention on RISC-V processors, reducing scalar code and computational complexity.", "method": "Vectorized FlashAttention implementation using low-cost floating-point exponential approximations without custom ISA extensions, combined with memory locality optimization through tiling strategies.", "result": "Experimental results show scalable approach with significant performance gains in processing attention layers for practical applications.", "conclusion": "The vectorized FlashAttention implementation successfully accelerates attention kernels on RISC-V processors through efficient exponential approximations and memory optimization techniques."}}
{"id": "2510.06917", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.06917", "abs": "https://arxiv.org/abs/2510.06917", "authors": ["Cheng-Han Chiang", "Xiaofei Wang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Shujie Liu", "Zhendong Wang", "Zhengyuan Yang", "Hung-yi Lee", "Lijuan Wang"], "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models", "comment": "Work in progress", "summary": "Current large language models (LLMs) and spoken language models (SLMs) begin\nthinking and taking actions only after the user has finished their turn. This\nprevents the model from interacting during the user's turn and can lead to high\nresponse latency while it waits to think. Consequently, thinking after\nreceiving the full input is not suitable for speech-to-speech interaction,\nwhere real-time, low-latency exchange is important. We address this by noting\nthat humans naturally \"think while listening.\" In this paper, we propose\nSHANKS, a general inference framework that enables SLMs to generate unspoken\nchain-of-thought reasoning while listening to the user input. SHANKS streams\nthe input speech in fixed-duration chunks and, as soon as a chunk is received,\ngenerates unspoken reasoning based on all previous speech and reasoning, while\nthe user continues speaking. SHANKS uses this unspoken reasoning to decide\nwhether to interrupt the user and to make tool calls to complete the task. We\ndemonstrate that SHANKS enhances real-time user-SLM interaction in two\nscenarios: (1) when the user is presenting a step-by-step solution to a math\nproblem, SHANKS can listen, reason, and interrupt when the user makes a\nmistake, achieving 37.1% higher interruption accuracy than a baseline that\ninterrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can\ncomplete 56.9% of the tool calls before the user finishes their turn. Overall,\nSHANKS moves toward models that keep thinking throughout the conversation, not\nonly after a turn ends. Animated illustrations of Shanks can be found at\nhttps://d223302.github.io/SHANKS/", "AI": {"tldr": "SHANKS is a framework that enables spoken language models to generate unspoken reasoning while listening to user speech, allowing real-time interruption and tool calls during conversation rather than waiting for turn completion.", "motivation": "Current LLMs/SLMs only start thinking after users finish speaking, causing high latency that's unsuitable for speech-to-speech interaction where real-time exchange is crucial. Humans naturally \"think while listening,\" which inspired this approach.", "method": "SHANKS streams input speech in fixed chunks and generates unspoken chain-of-thought reasoning as each chunk arrives, using all previous speech and reasoning. This enables real-time interruption decisions and tool calls while the user continues speaking.", "result": "In math problem scenarios, SHANKS achieved 37.1% higher interruption accuracy than baseline. In tool-augmented dialogues, it completed 56.9% of tool calls before users finished speaking. The framework enables thinking throughout conversation rather than only after turns.", "conclusion": "SHANKS successfully moves toward models that think continuously during conversations, enabling real-time interaction by generating reasoning while listening, which is essential for low-latency speech-to-speech applications."}}
{"id": "2510.06840", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06840", "abs": "https://arxiv.org/abs/2510.06840", "authors": ["Stefano F. Stefenon", "Jo\u00e3o P. Matos-Carvalho", "Valderi R. Q. Leithardt", "Kin-Choong Yow"], "title": "CNN-TFT explained by SHAP with multi-head attention weights for time series forecasting", "comment": null, "summary": "Convolutional neural networks (CNNs) and transformer architectures offer\nstrengths for modeling temporal data: CNNs excel at capturing local patterns\nand translational invariances, while transformers effectively model long-range\ndependencies via self-attention. This paper proposes a hybrid architecture\nintegrating convolutional feature extraction with a temporal fusion transformer\n(TFT) backbone to enhance multivariate time series forecasting. The CNN module\nfirst applies a hierarchy of one-dimensional convolutional layers to distill\nsalient local patterns from raw input sequences, reducing noise and\ndimensionality. The resulting feature maps are then fed into the TFT, which\napplies multi-head attention to capture both short- and long-term dependencies\nand to weigh relevant covariates adaptively. We evaluate the CNN-TFT on a\nhydroelectric natural flow time series dataset. Experimental results\ndemonstrate that CNN-TFT outperforms well-established deep learning models,\nwith a mean absolute percentage error of up to 2.2%. The explainability of the\nmodel is obtained by a proposed Shapley additive explanations with multi-head\nattention weights (SHAP-MHAW). Our novel architecture, named CNN-TFT-SHAP-MHAW,\nis promising for applications requiring high-fidelity, multivariate time series\nforecasts, being available for future analysis at\nhttps://github.com/SFStefenon/CNN-TFT-SHAP-MHAW .", "AI": {"tldr": "A hybrid CNN-Transformer architecture for multivariate time series forecasting that combines convolutional feature extraction with temporal fusion transformer, achieving 2.2% MAPE on hydroelectric flow data.", "motivation": "To leverage complementary strengths of CNNs (local pattern capture) and transformers (long-range dependencies) for improved multivariate time series forecasting.", "method": "Hierarchical 1D CNN for local pattern extraction and noise reduction, followed by temporal fusion transformer with multi-head attention for capturing dependencies and adaptive covariate weighting.", "result": "Outperforms established deep learning models with 2.2% mean absolute percentage error on hydroelectric natural flow dataset.", "conclusion": "The CNN-TFT-SHAP-MHAW architecture provides high-fidelity multivariate time series forecasting with explainability through SHAP-MHAW, promising for applications requiring accurate forecasts."}}
{"id": "2510.06961", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.06961", "abs": "https://arxiv.org/abs/2510.06961", "authors": ["Vaibhav Srivastav", "Steven Zheng", "Eric Bezzam", "Eustache Le Bihan", "Nithin Koluguri", "Piotr \u017belasko", "Somshubra Majumdar", "Adel Moumen", "Sanchit Gandhi"], "title": "Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation", "comment": "Submitted to ICASSP 2026; Leaderboard:\n  https://huggingface.co/spaces/hf-audio/open_asr_leaderboard; Code:\n  https://github.com/huggingface/open_asr_leaderboard", "summary": "Despite rapid progress, ASR evaluation remains saturated with short-form\nEnglish, and efficiency is rarely reported. We present the Open ASR\nLeaderboard, a fully reproducible benchmark and interactive leaderboard\ncomparing 60+ open-source and proprietary systems across 11 datasets, including\ndedicated multilingual and long-form tracks. We standardize text normalization\nand report both word error rate (WER) and inverse real-time factor (RTFx),\nenabling fair accuracy-efficiency comparisons. For English transcription,\nConformer encoders paired with LLM decoders achieve the best average WER but\nare slower, while CTC and TDT decoders deliver much better RTFx, making them\nattractive for long-form and offline use. Whisper-derived encoders fine-tuned\nfor English improve accuracy but often trade off multilingual coverage. All\ncode and dataset loaders are open-sourced to support transparent, extensible\nevaluation.", "AI": {"tldr": "The Open ASR Leaderboard is a reproducible benchmark comparing 60+ ASR systems across 11 datasets, featuring multilingual and long-form tracks with standardized evaluation metrics including WER and RTFx.", "motivation": "To address the saturation of ASR evaluation with short-form English and lack of efficiency reporting, providing fair accuracy-efficiency comparisons.", "method": "Created a fully reproducible benchmark with standardized text normalization, evaluating systems across 11 datasets including dedicated multilingual and long-form tracks, reporting both WER and inverse real-time factor.", "result": "Conformer encoders with LLM decoders achieve best average WER for English but are slower, while CTC and TDT decoders offer better RTFx. Whisper-derived encoders fine-tuned for English improve accuracy but reduce multilingual coverage.", "conclusion": "The benchmark enables transparent ASR evaluation, revealing trade-offs between accuracy and efficiency, with all code and dataset loaders open-sourced for extensible evaluation."}}
{"id": "2510.06852", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06852", "abs": "https://arxiv.org/abs/2510.06852", "authors": ["Zuherman Rustam", "Sri Hartini", "Sardar M. N. Islam", "Fevi Novkaniza", "Fiftitah R. Aszhari", "Muhammad Rifqi"], "title": "Enhancing Bankruptcy Prediction of Banks through Advanced Machine Learning Techniques: An Innovative Approach and Analysis", "comment": null, "summary": "Context: Financial system stability is determined by the condition of the\nbanking system. A bank failure can destroy the stability of the financial\nsystem, as banks are subject to systemic risk, affecting not only individual\nbanks but also segments or the entire financial system. Calculating the\nprobability of a bank going bankrupt is one way to ensure the banking system is\nsafe and sound. Existing literature and limitations: Statistical models, such\nas Altman's Z-Score, are one of the common techniques for developing a\nbankruptcy prediction model. However, statistical methods rely on rigid and\nsometimes irrelevant assumptions, which can result in low forecast accuracy.\nNew approaches are necessary. Objective of the research: Bankruptcy models are\ndeveloped using machine learning techniques, such as logistic regression (LR),\nrandom forest (RF), and support vector machines (SVM). According to several\nstudies, machine learning is also more accurate and effective than statistical\nmethods for categorising and forecasting banking risk management. Present\nResearch: The commercial bank data are derived from the annual financial\nstatements of 44 active banks and 21 bankrupt banks in Turkey from 1994 to\n2004, and the rural bank data are derived from the quarterly financial reports\nof 43 active and 43 bankrupt rural banks in Indonesia between 2013 and 2019.\nFive rural banks in Indonesia have also been selected to demonstrate the\nfeasibility of analysing bank bankruptcy trends. Findings and implications: The\nresults of the research experiments show that RF can forecast data from\ncommercial banks with a 90% accuracy rate. Furthermore, the three machine\nlearning methods proposed accurately predict the likelihood of rural bank\nbankruptcy. Contribution and Conclusion: The proposed innovative machine\nlearning approach help to implement policies that reduce the costs of\nbankruptcy.", "AI": {"tldr": "Machine learning models (LR, RF, SVM) outperform statistical methods in predicting bank bankruptcy, achieving 90% accuracy with RF on commercial bank data and accurate predictions for rural banks.", "motivation": "Traditional statistical methods like Altman's Z-Score have rigid assumptions leading to low forecast accuracy, necessitating more accurate machine learning approaches for financial system stability.", "method": "Used logistic regression, random forest, and support vector machines on commercial bank data from Turkey (44 active, 21 bankrupt) and rural bank data from Indonesia (43 active, 43 bankrupt) with financial statement analysis.", "result": "Random forest achieved 90% accuracy in predicting commercial bank bankruptcy, and all three ML methods accurately predicted rural bank bankruptcy likelihood.", "conclusion": "Machine learning approaches provide superior bankruptcy prediction capabilities that can help implement policies to reduce bankruptcy costs and maintain financial system stability."}}
{"id": "2510.06965", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06965", "abs": "https://arxiv.org/abs/2510.06965", "authors": ["Bryan R. Christ", "Penelope Molitz", "Jonathan Kropko", "Thomas Hartvigsen"], "title": "EDUMATH: Generating Standards-aligned Educational Math Word Problems", "comment": "32 pages, 15 figures", "summary": "Math word problems (MWPs) are critical K-12 educational tools, and\ncustomizing them to students' interests and ability levels can increase\nlearning outcomes. However, teachers struggle to find time to customize MWPs\nfor each student given large class sizes and increasing burnout. We propose\nthat LLMs can support math education by generating MWPs customized to student\ninterests and math education standards. To this end, we use a joint human\nexpert-LLM judge approach to evaluate over 11,000 MWPs generated by open and\nclosed LLMs and develop the first teacher-annotated dataset for\nstandards-aligned educational MWP generation. We show the value of our data by\nusing it to train a 12B open model that matches the performance of larger and\nmore capable open models. We also use our teacher-annotated data to train a\ntext classifier that enables a 30B open LLM to outperform existing closed\nbaselines without any training. Next, we show our models' MWPs are more similar\nto human-written MWPs than those from existing models. We conclude by\nconducting the first study of customized LLM-generated MWPs with grade school\nstudents, finding they perform similarly on our models' MWPs relative to\nhuman-written MWPs but consistently prefer our customized MWPs.", "AI": {"tldr": "LLMs can generate customized math word problems (MWPs) that match student interests and education standards, with teacher-annotated data enabling smaller models to outperform larger ones and students preferring customized MWPs.", "motivation": "Teachers lack time to customize MWPs for individual students despite the educational benefits, and LLMs could help automate this personalization process.", "method": "Used joint human expert-LLM evaluation of 11,000+ MWPs, created teacher-annotated dataset, trained 12B and 30B open models, and conducted student study comparing customized vs human-written MWPs.", "result": "12B model matched larger models' performance, 30B model outperformed closed baselines without training, generated MWPs were more human-like, and students performed similarly but preferred customized MWPs.", "conclusion": "LLMs can effectively generate standards-aligned, customized MWPs that students prefer, providing scalable support for personalized math education."}}
{"id": "2510.06860", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06860", "abs": "https://arxiv.org/abs/2510.06860", "authors": ["Olayiwola Arowolo", "Jochen L. Cremer"], "title": "Towards Generalization of Graph Neural Networks for AC Optimal Power Flow", "comment": "Pre-print has been submitted for review", "summary": "AC Optimal Power Flow (ACOPF) is computationally expensive for large-scale\npower systems, with conventional solvers requiring prohibitive solution times.\nMachine learning approaches offer computational speedups but struggle with\nscalability and topology adaptability without expensive retraining. To enable\nscalability across grid sizes and adaptability to topology changes, we propose\na Hybrid Heterogeneous Message Passing Neural Network (HH-MPNN). HH-MPNN models\nbuses, generators, loads, shunts, transmission lines and transformers as\ndistinct node or edge types, combined with a scalable transformer model for\nhandling long-range dependencies. On grids from 14 to 2,000 buses, HH-MPNN\nachieves less than 1% optimality gap on default topologies. Applied zero-shot\nto thousands of unseen topologies, HH-MPNN achieves less than 3% optimality gap\ndespite training only on default topologies. Pre-training on smaller grids also\nimproves results on a larger grid. Computational speedups reach 1,000x to\n10,000x compared to interior point solvers. These results advance practical,\ngeneralizable machine learning for real-time power system operations.", "AI": {"tldr": "HH-MPNN achieves 1,000-10,000x speedup for ACOPF with <3% optimality gap across grid sizes and topologies without retraining.", "motivation": "ACOPF is computationally expensive for large power systems, and existing ML methods lack scalability and topology adaptability without costly retraining.", "method": "Hybrid Heterogeneous Message Passing Neural Network (HH-MPNN) models different grid components as distinct node/edge types with transformer for long-range dependencies.", "result": "Achieves <1% optimality gap on default topologies (14-2,000 buses) and <3% gap on unseen topologies zero-shot; computational speedups of 1,000x-10,000x vs interior point solvers.", "conclusion": "HH-MPNN enables practical, generalizable ML for real-time power system operations with scalability and topology adaptability."}}
{"id": "2510.06974", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06974", "abs": "https://arxiv.org/abs/2510.06974", "authors": ["Geng Liu", "Feng Li", "Junjie Mu", "Mengxiao Zhu", "Francesco Pierri"], "title": "Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in user-facing\napplications, raising concerns about their potential to reflect and amplify\nsocial biases. We investigate social identity framing in Chinese LLMs using\nMandarin-specific prompts across ten representative Chinese LLMs, evaluating\nresponses to ingroup (\"We\") and outgroup (\"They\") framings, and extending the\nsetting to 240 social groups salient in the Chinese context. To complement\ncontrolled experiments, we further analyze Chinese-language conversations from\na corpus of real interactions between users and chatbots. Across models, we\nobserve systematic ingroup-positive and outgroup-negative tendencies, which are\nnot confined to synthetic prompts but also appear in naturalistic dialogue,\nindicating that bias dynamics might strengthen in real interactions. Our study\nprovides a language-aware evaluation framework for Chinese LLMs, demonstrating\nthat social identity biases documented in English generalize\ncross-linguistically and intensify in user-facing contexts.", "AI": {"tldr": "The study investigates social identity biases in Chinese LLMs, finding systematic ingroup-positive and outgroup-negative tendencies across 10 models using Mandarin prompts and real conversation data.", "motivation": "To address concerns about LLMs reflecting and amplifying social biases in user-facing applications, particularly in Chinese language contexts.", "method": "Used Mandarin-specific prompts across 10 Chinese LLMs, evaluated responses to ingroup (\"We\") and outgroup (\"They\") framings across 240 social groups, and analyzed real chatbot conversations.", "result": "Found systematic ingroup-positive and outgroup-negative tendencies across all models, with biases appearing in both controlled experiments and naturalistic dialogue, intensifying in real interactions.", "conclusion": "Social identity biases documented in English LLMs generalize cross-linguistically to Chinese models and strengthen in user-facing contexts, providing a language-aware evaluation framework for Chinese LLMs."}}
{"id": "2510.06871", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06871", "abs": "https://arxiv.org/abs/2510.06871", "authors": ["Huahui Yi", "Kun Wang", "Qiankun Li", "Miao Yu", "Liang Lin", "Gongli Xi", "Hao Wu", "Xuming Hu", "Kang Li", "Yang Liu"], "title": "SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models", "comment": null, "summary": "Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal\nreasoning but often amplify safety risks under adversarial or unsafe prompts, a\nphenomenon we call the \\textit{Reasoning Tax}. Existing defenses mainly act at\nthe output level and do not constrain the reasoning process, leaving models\nexposed to implicit risks. In this paper, we propose SaFeR-VLM, a\nsafety-aligned reinforcement learning framework that embeds safety directly\ninto multimodal reasoning. The framework integrates four components: (I)\nQI-Safe-10K, a curated dataset emphasizing safety-critical and\nreasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations\nundergo reflection and correction instead of being discarded; (III) structured\nreward modeling with multi-dimensional weighted criteria and explicit penalties\nfor hallucinations and contradictions; and (IV) GRPO optimization, which\nreinforces both safe and corrected trajectories. This unified design shifts\nsafety from a passive safeguard to an active driver of reasoning, enabling\nscalable and generalizable safety-aware reasoning. SaFeR-VLM further\ndemonstrates robustness against both explicit and implicit risks, supporting\ndynamic and interpretable safety decisions beyond surface-level filtering.\nSaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and\nhelpfulness across six benchmarks, surpassing both same-scale and $>10\\times$\nlarger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B.\nRemarkably, SaFeR-VLM-7B benefits from its increased scale to surpass\nGPT-5-mini and Gemini-2.5-Flash by \\num{6.47} and \\num{16.76} points\nrespectively on safety metrics, achieving this improvement without any\ndegradation in helpfulness performance. Our codes are available at\nhttps://github.com/HarveyYi/SaFeR-VLM.", "AI": {"tldr": "SaFeR-VLM is a safety-aligned reinforcement learning framework that embeds safety directly into multimodal reasoning, addressing the \"Reasoning Tax\" where MLRMs amplify safety risks. It achieves superior safety performance without compromising helpfulness.", "motivation": "Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal reasoning but often amplify safety risks under adversarial or unsafe prompts (Reasoning Tax). Existing defenses mainly act at output level and don't constrain reasoning process, leaving models exposed to implicit risks.", "method": "Proposes SaFeR-VLM framework with four components: (I) QI-Safe-10K dataset for safety-critical cases; (II) safety-aware rollout with reflection and correction; (III) structured reward modeling with multi-dimensional criteria and penalties; (IV) GRPO optimization reinforcing safe and corrected trajectories.", "result": "SaFeR-VLM-3B achieves average performance 70.13 and 78.97 on safety and helpfulness across six benchmarks, surpassing same-scale and >10\u00d7 larger models. SaFeR-VLM-7B surpasses GPT-5-mini and Gemini-2.5-Flash by 6.47 and 16.76 points respectively on safety metrics without degradation in helpfulness.", "conclusion": "SaFeR-VLM shifts safety from passive safeguard to active driver of reasoning, enabling scalable and generalizable safety-aware reasoning with robustness against both explicit and implicit risks, supporting dynamic and interpretable safety decisions."}}
{"id": "2510.06999", "categories": ["cs.CL", "cs.IR", "I.2.7; H.3.3; K.5.0"], "pdf": "https://arxiv.org/pdf/2510.06999", "abs": "https://arxiv.org/abs/2510.06999", "authors": ["Markus Reuter", "Tobias Lingenberg", "R\u016bta Liepi\u0146a", "Francesca Lagioia", "Marco Lippi", "Giovanni Sartor", "Andrea Passerini", "Burcu Sayin"], "title": "Towards Reliable Retrieval in RAG Systems for Large Legal Datasets", "comment": "Accepted for the 7th Natural Legal Language Processing Workshop (NLLP\n  2025), co-located with EMNLP 2025", "summary": "Retrieval-Augmented Generation (RAG) is a promising approach to mitigate\nhallucinations in Large Language Models (LLMs) for legal applications, but its\nreliability is critically dependent on the accuracy of the retrieval step. This\nis particularly challenging in the legal domain, where large databases of\nstructurally similar documents often cause retrieval systems to fail. In this\npaper, we address this challenge by first identifying and quantifying a\ncritical failure mode we term Document-Level Retrieval Mismatch (DRM), where\nthe retriever selects information from entirely incorrect source documents. To\nmitigate DRM, we investigate a simple and computationally efficient technique\nwhich we refer to as Summary-Augmented Chunking (SAC). This method enhances\neach text chunk with a document-level synthetic summary, thereby injecting\ncrucial global context that would otherwise be lost during a standard chunking\nprocess. Our experiments on a diverse set of legal information retrieval tasks\nshow that SAC greatly reduces DRM and, consequently, also improves text-level\nretrieval precision and recall. Interestingly, we find that a generic\nsummarization strategy outperforms an approach that incorporates legal expert\ndomain knowledge to target specific legal elements. Our work provides evidence\nthat this practical, scalable, and easily integrable technique enhances the\nreliability of RAG systems when applied to large-scale legal document datasets.", "AI": {"tldr": "This paper introduces Summary-Augmented Chunking (SAC), a simple method that adds document-level summaries to text chunks to reduce Document-Level Retrieval Mismatch in legal RAG systems, improving retrieval accuracy without requiring legal domain expertise.", "motivation": "Retrieval-Augmented Generation (RAG) for legal applications suffers from Document-Level Retrieval Mismatch (DRM) where retrievers select information from incorrect source documents due to structurally similar legal documents in large databases.", "method": "The authors propose Summary-Augmented Chunking (SAC), which enhances each text chunk with a document-level synthetic summary to provide global context that standard chunking processes lose. They compare generic summarization with legal expert-targeted approaches.", "result": "SAC significantly reduces DRM and improves both text-level retrieval precision and recall. Surprisingly, generic summarization outperformed legal expert domain knowledge approaches.", "conclusion": "SAC is a practical, scalable, and easily integrable technique that enhances RAG system reliability for large-scale legal document datasets, with generic summarization being more effective than domain-specific approaches."}}
{"id": "2510.06880", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06880", "abs": "https://arxiv.org/abs/2510.06880", "authors": ["Zhiyu Wang", "Sonia Koszut", "Pietro Li\u00f2", "Francesco Ceccarelli"], "title": "MoRE-GNN: Multi-omics Data Integration with a Heterogeneous Graph Autoencoder", "comment": null, "summary": "The integration of multi-omics single-cell data remains challenging due to\nhigh-dimensionality and complex inter-modality relationships. To address this,\nwe introduce MoRE-GNN (Multi-omics Relational Edge Graph Neural Network), a\nheterogeneous graph autoencoder that combines graph convolution and attention\nmechanisms to dynamically construct relational graphs directly from data.\nEvaluations on six publicly available datasets demonstrate that MoRE-GNN\ncaptures biologically meaningful relationships and outperforms existing\nmethods, particularly in settings with strong inter-modality correlations.\nFurthermore, the learned representations allow for accurate downstream\ncross-modal predictions. While performance may vary with dataset complexity,\nMoRE-GNN offers an adaptive, scalable and interpretable framework for advancing\nmulti-omics integration.", "AI": {"tldr": "MoRE-GNN is a heterogeneous graph autoencoder that uses graph convolution and attention to dynamically build relational graphs from multi-omics single-cell data, outperforming existing methods in capturing biological relationships.", "motivation": "Multi-omics single-cell data integration is challenging due to high dimensionality and complex inter-modality relationships, requiring better methods to capture meaningful biological connections.", "method": "MoRE-GNN combines graph convolution and attention mechanisms in a heterogeneous graph autoencoder to dynamically construct relational graphs directly from multi-omics data.", "result": "Evaluations on six datasets show MoRE-GNN captures biologically meaningful relationships and outperforms existing methods, especially with strong inter-modality correlations, enabling accurate cross-modal predictions.", "conclusion": "MoRE-GNN provides an adaptive, scalable and interpretable framework for multi-omics integration, though performance may vary with dataset complexity."}}
{"id": "2510.07000", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07000", "abs": "https://arxiv.org/abs/2510.07000", "authors": ["Neel Prabhanjan Rachamalla", "Aravind Konakalla", "Gautam Rajeev", "Ashish Kulkarni", "Chandra Khatri", "Shubham Agarwal"], "title": "Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages", "comment": "EMNLP 2025", "summary": "The effectiveness of Large Language Models (LLMs) depends heavily on the\navailability of high-quality post-training data, particularly\ninstruction-tuning and preference-based examples. Existing open-source\ndatasets, however, often lack multilingual coverage, cultural grounding, and\nsuffer from task diversity gaps that are especially pronounced for Indian\nlanguages. We introduce a human-in-the-loop pipeline that combines translations\nwith synthetic expansion to produce reliable and diverse Indic post-training\ndata. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and\nPragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56\nsub-categories, leveraging 57 diverse datasets. Our dataset protocol\nincorporates several often-overlooked dimensions and emphasize task diversity,\nmulti-turn dialogue, instruction fidelity, safety alignment, and preservation\nof cultural nuance, providing a foundation for more inclusive and effective\nmultilingual LLMs.", "AI": {"tldr": "A human-in-the-loop pipeline combining translations with synthetic expansion creates reliable and diverse Indic post-training datasets (Pragyaan-IT and Pragyaan-Align) across 10 Indian languages, addressing gaps in multilingual coverage, cultural grounding, and task diversity.", "motivation": "Existing open-source datasets lack multilingual coverage, cultural grounding, and suffer from task diversity gaps, especially for Indian languages, limiting the effectiveness of LLMs in these contexts.", "method": "Human-in-the-loop pipeline that combines translations with synthetic expansion to produce reliable and diverse Indic post-training data, curating two datasets across 10 Indian languages covering 13 broad and 56 sub-categories from 57 diverse datasets.", "result": "Created Pragyaan-IT (22.5K) and Pragyaan-Align (100K) datasets across 10 Indian languages with enhanced task diversity, multi-turn dialogue, instruction fidelity, safety alignment, and preservation of cultural nuance.", "conclusion": "This approach provides a foundation for more inclusive and effective multilingual LLMs by addressing critical gaps in existing datasets and emphasizing often-overlooked dimensions like cultural preservation and task diversity."}}
{"id": "2510.06907", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06907", "abs": "https://arxiv.org/abs/2510.06907", "authors": ["Shaojie Zhang", "Ke Chen"], "title": "Angular Constraint Embedding via SpherePair Loss for Constrained Clustering", "comment": "Accepted by NeurIPS 2025, 6 Figures and 1 Table in Main text, 18\n  Figures and 5 Tables in Appendices", "summary": "Constrained clustering integrates domain knowledge through pairwise\nconstraints. However, existing deep constrained clustering (DCC) methods are\neither limited by anchors inherent in end-to-end modeling or struggle with\nlearning discriminative Euclidean embedding, restricting their scalability and\nreal-world applicability. To avoid their respective pitfalls, we propose a\nnovel angular constraint embedding approach for DCC, termed SpherePair. Using\nthe SpherePair loss with a geometric formulation, our method faithfully encodes\npairwise constraints and leads to embeddings that are clustering-friendly in\nangular space, effectively separating representation learning from clustering.\nSpherePair preserves pairwise relations without conflict, removes the need to\nspecify the exact number of clusters, generalizes to unseen data, enables rapid\ninference of the number of clusters, and is supported by rigorous theoretical\nguarantees. Comparative evaluations with state-of-the-art DCC methods on\ndiverse benchmarks, along with empirical validation of theoretical insights,\nconfirm its superior performance, scalability, and overall real-world\neffectiveness. Code is available at\n\\href{https://github.com/spherepaircc/SpherePairCC/tree/main}{our repository}.", "AI": {"tldr": "SpherePair is a novel deep constrained clustering method that uses angular constraint embedding to effectively separate representation learning from clustering, providing superior performance and scalability without requiring exact cluster numbers.", "motivation": "Existing deep constrained clustering methods are limited by anchors in end-to-end modeling or struggle with learning discriminative Euclidean embedding, restricting their scalability and real-world applicability.", "method": "Proposes SpherePair loss with geometric formulation for angular constraint embedding, which faithfully encodes pairwise constraints and creates clustering-friendly embeddings in angular space while separating representation learning from clustering.", "result": "SpherePair preserves pairwise relations without conflict, removes the need to specify exact cluster numbers, generalizes to unseen data, enables rapid inference of cluster numbers, and shows superior performance and scalability in comparative evaluations.", "conclusion": "SpherePair provides a theoretically-grounded approach for deep constrained clustering that overcomes limitations of existing methods and demonstrates superior real-world effectiveness."}}
{"id": "2510.07019", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07019", "abs": "https://arxiv.org/abs/2510.07019", "authors": ["Jusen Du", "Jiaxi Hu", "Tao Zhang", "Weigao Sun", "Yu Cheng"], "title": "Native Hybrid Attention for Efficient Sequence Modeling", "comment": "Technical report, 16 pages", "summary": "Transformers excel at sequence modeling but face quadratic complexity, while\nlinear attention offers improved efficiency but often compromises recall\naccuracy over long contexts. In this work, we introduce Native Hybrid Attention\n(NHA), a novel hybrid architecture of linear and full attention that integrates\nboth intra \\& inter-layer hybridization into a unified layer design. NHA\nmaintains long-term context in key-value slots updated by a linear RNN, and\naugments them with short-term tokens from a sliding window. A single\n\\texttt{softmax attention} operation is then applied over all keys and values,\nenabling per-token and per-head context-dependent weighting without requiring\nadditional fusion parameters. The inter-layer behavior is controlled through a\nsingle hyperparameter, the sliding window size, which allows smooth adjustment\nbetween purely linear and full attention while keeping all layers structurally\nuniform. Experimental results show that NHA surpasses Transformers and other\nhybrid baselines on recall-intensive and commonsense reasoning tasks.\nFurthermore, pretrained LLMs can be structurally hybridized with NHA, achieving\ncompetitive accuracy while delivering significant efficiency gains. Code is\navailable at https://github.com/JusenD/NHA.", "AI": {"tldr": "Native Hybrid Attention (NHA) is a hybrid architecture combining linear and full attention with intra & inter-layer hybridization, using linear RNN for long-term context and sliding window for short-term tokens, achieving better efficiency and accuracy than Transformers.", "motivation": "Transformers have quadratic complexity issues, while linear attention improves efficiency but sacrifices recall accuracy over long contexts. There's a need for a solution that balances efficiency and accuracy.", "method": "NHA integrates linear RNN for long-term context in key-value slots with short-term tokens from sliding window, applying single softmax attention over all keys/values without extra fusion parameters. Uses sliding window size hyperparameter to control inter-layer behavior.", "result": "NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Pretrained LLMs hybridized with NHA achieve competitive accuracy with significant efficiency gains.", "conclusion": "NHA provides an effective hybrid attention architecture that balances efficiency and accuracy, enabling smooth adjustment between linear and full attention while maintaining structural uniformity across layers."}}
{"id": "2510.06910", "categories": ["cs.LG", "I.2; I.5"], "pdf": "https://arxiv.org/pdf/2510.06910", "abs": "https://arxiv.org/abs/2510.06910", "authors": ["Iago Xabier V\u00e1zquez", "Javier Sedano", "Muhammad Afzal", "\u00c1ngel Miguel Garc\u00eda-Vico"], "title": "Vacuum Spiker: A Spiking Neural Network-Based Model for Efficient Anomaly Detection in Time Series", "comment": "53 pages, 16 figures, preprint submitted to a journal for review", "summary": "Anomaly detection is a key task across domains such as industry, healthcare,\nand cybersecurity. Many real-world anomaly detection problems involve analyzing\nmultiple features over time, making time series analysis a natural approach for\nsuch problems. While deep learning models have achieved strong performance in\nthis field, their trend to exhibit high energy consumption limits their\ndeployment in resource-constrained environments such as IoT devices, edge\ncomputing platforms, and wearables. To address this challenge, this paper\nintroduces the \\textit{Vacuum Spiker algorithm}, a novel Spiking Neural\nNetwork-based method for anomaly detection in time series. It incorporates a\nnew detection criterion that relies on global changes in neural activity rather\nthan reconstruction or prediction error. It is trained using Spike\nTime-Dependent Plasticity in a novel way, intended to induce changes in neural\nactivity when anomalies occur. A new efficient encoding scheme is also\nproposed, which discretizes the input space into non-overlapping intervals,\nassigning each to a single neuron. This strategy encodes information with a\nsingle spike per time step, improving energy efficiency compared to\nconventional encoding methods. Experimental results on publicly available\ndatasets show that the proposed algorithm achieves competitive performance\nwhile significantly reducing energy consumption, compared to a wide set of deep\nlearning and machine learning baselines. Furthermore, its practical utility is\nvalidated in a real-world case study, where the model successfully identifies\npower curtailment events in a solar inverter. These results highlight its\npotential for sustainable and efficient anomaly detection.", "AI": {"tldr": "The paper proposes Vacuum Spiker algorithm, a novel Spiking Neural Network-based method for energy-efficient anomaly detection in time series, achieving competitive performance with significantly reduced energy consumption compared to deep learning baselines.", "motivation": "Address the high energy consumption of deep learning models in anomaly detection, which limits deployment in resource-constrained environments like IoT devices, edge computing, and wearables.", "method": "Vacuum Spiker algorithm uses Spiking Neural Networks with a new detection criterion based on global changes in neural activity, trained with Spike Time-Dependent Plasticity, and employs an efficient encoding scheme that discretizes input space into non-overlapping intervals with single spike per time step.", "result": "Experimental results show competitive performance with significant energy reduction compared to deep learning and machine learning baselines. Successfully validated in real-world case study identifying power curtailment events in solar inverters.", "conclusion": "The proposed algorithm demonstrates potential for sustainable and efficient anomaly detection, particularly suitable for resource-constrained environments."}}
{"id": "2510.07024", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07024", "abs": "https://arxiv.org/abs/2510.07024", "authors": ["Shrestha Ghosh", "Luca Giordano", "Yujia Hu", "Tuan-Phong Nguyen", "Simon Razniewski"], "title": "Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge", "comment": null, "summary": "LLMs are remarkable artifacts that have revolutionized a range of NLP and AI\ntasks. A significant contributor is their factual knowledge, which, to date,\nremains poorly understood, and is usually analyzed from biased samples. In this\npaper, we take a deep tour into the factual knowledge (or beliefs) of a\nfrontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited\nset of 100 million beliefs of one of the strongest currently available frontier\nLLMs, GPT-4.1. We find that the models' factual knowledge differs quite\nsignificantly from established knowledge bases, and that its accuracy is\nsignificantly lower than indicated by previous benchmarks. We also find that\ninconsistency, ambiguity and hallucinations are major issues, shedding light on\nfuture research opportunities concerning factual LLM knowledge.", "AI": {"tldr": "Analysis of GPT-4.1's factual knowledge reveals significant differences from established knowledge bases, lower accuracy than benchmarked, and major issues with inconsistency, ambiguity, and hallucinations.", "motivation": "To deeply understand the factual knowledge of frontier LLMs, which remains poorly understood and is usually analyzed from biased samples.", "method": "Analyzed GPTKB v1.5, a recursively elicited set of 100 million beliefs from GPT-4.1, one of the strongest currently available frontier LLMs.", "result": "Models' factual knowledge differs significantly from established knowledge bases; accuracy is significantly lower than indicated by previous benchmarks; inconsistency, ambiguity and hallucinations are major issues.", "conclusion": "The findings shed light on future research opportunities concerning factual LLM knowledge, highlighting the need for better understanding and improvement of factual accuracy in LLMs."}}
{"id": "2510.06912", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06912", "abs": "https://arxiv.org/abs/2510.06912", "authors": ["Alexandros Vassiliades", "Nikolaos Polatidis", "Stamatios Samaras", "Sotiris Diplaris", "Ignacio Cabrera Martin", "Yannis Manolopoulos", "Stefanos Vrochidis", "Ioannis Kompatsiaris"], "title": "Utilizing Large Language Models for Machine Learning Explainability", "comment": null, "summary": "This study explores the explainability capabilities of large language models\n(LLMs), when employed to autonomously generate machine learning (ML) solutions.\nWe examine two classification tasks: (i) a binary classification problem\nfocused on predicting driver alertness states, and (ii) a multilabel\nclassification problem based on the yeast dataset. Three state-of-the-art LLMs\n(i.e. OpenAI GPT, Anthropic Claude, and DeepSeek) are prompted to design\ntraining pipelines for four common classifiers: Random Forest, XGBoost,\nMultilayer Perceptron, and Long Short-Term Memory networks. The generated\nmodels are evaluated in terms of predictive performance (recall, precision, and\nF1-score) and explainability using SHAP (SHapley Additive exPlanations).\nSpecifically, we measure Average SHAP Fidelity (Mean Squared Error between SHAP\napproximations and model outputs) and Average SHAP Sparsity (number of features\ndeemed influential). The results reveal that LLMs are capable of producing\neffective and interpretable models, achieving high fidelity and consistent\nsparsity, highlighting their potential as automated tools for interpretable ML\npipeline generation. The results show that LLMs can produce effective,\ninterpretable pipelines with high fidelity and consistent sparsity, closely\nmatching manually engineered baselines.", "AI": {"tldr": "LLMs can autonomously generate effective and interpretable machine learning pipelines for classification tasks, achieving performance comparable to manually engineered baselines.", "motivation": "To explore the explainability capabilities of LLMs when used to autonomously generate ML solutions and evaluate their potential as automated tools for interpretable ML pipeline generation.", "method": "Used three state-of-the-art LLMs (OpenAI GPT, Anthropic Claude, DeepSeek) to design training pipelines for four classifiers (Random Forest, XGBoost, MLP, LSTM) on two classification tasks: binary driver alertness prediction and multilabel yeast dataset classification.", "result": "LLMs produced effective models with high predictive performance (recall, precision, F1-score) and good explainability metrics (high SHAP fidelity and consistent sparsity), closely matching manually engineered baselines.", "conclusion": "LLMs demonstrate strong potential as automated tools for generating interpretable ML pipelines, capable of producing both effective and explainable models."}}
{"id": "2510.07037", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07037", "abs": "https://arxiv.org/abs/2510.07037", "authors": ["Rajvee Sheth", "Samridhi Raj Sinha", "Mahavir Patil", "Himanshu Beniwal", "Mayank Singh"], "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models", "comment": null, "summary": "Code-switching (CSW), the alternation of languages and scripts within a\nsingle utterance, remains a fundamental challenge for multiling ual NLP, even\namidst the rapid advances of large language models (LLMs). Most LLMs still\nstruggle with mixed-language inputs, limited CSW datasets, and evaluation\nbiases, hindering deployment in multilingual societies. This survey provides\nthe first comprehensive analysis of CSW-aware LLM research, reviewing\n\\total{unique_references} studies spanning five research areas, 12 NLP tasks,\n30+ datasets, and 80+ languages. We classify recent advances by architecture,\ntraining strategy, and evaluation methodology, outlining how LLMs have reshaped\nCSW modeling and what challenges persist. The paper concludes with a roadmap\nemphasizing the need for inclusive datasets, fair evaluation, and\nlinguistically grounded models to achieve truly multilingual intelligence. A\ncurated collection of all resources is maintained at\nhttps://github.com/lingo-iitgn/awesome-code-mixing/.", "AI": {"tldr": "This survey provides the first comprehensive analysis of code-switching (CSW) in large language models (LLMs), covering over 80 studies across 5 research areas, 12 NLP tasks, 30+ datasets, and 80+ languages. It classifies advances by architecture, training strategy, and evaluation methodology, highlighting persistent challenges and providing a roadmap for future research.", "motivation": "Code-switching remains a fundamental challenge for multilingual NLP despite advances in LLMs. Most LLMs struggle with mixed-language inputs, limited CSW datasets, and evaluation biases, hindering deployment in multilingual societies where code-switching is common.", "method": "The survey conducts comprehensive analysis of CSW-aware LLM research by reviewing over 80 studies, classifying recent advances by architecture, training strategy, and evaluation methodology across multiple research areas, NLP tasks, datasets, and languages.", "result": "The survey provides a systematic classification of CSW research in LLMs, outlining how LLMs have reshaped CSW modeling while identifying persistent challenges in handling mixed-language inputs and evaluation biases.", "conclusion": "The paper concludes with a roadmap emphasizing the need for inclusive datasets, fair evaluation, and linguistically grounded models to achieve truly multilingual intelligence. A curated collection of resources is maintained at the provided GitHub repository."}}
{"id": "2510.06913", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.06913", "abs": "https://arxiv.org/abs/2510.06913", "authors": ["Ke Guo", "Haochen Liu", "Xiaojun Wu", "Chen Lv"], "title": "DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning", "comment": null, "summary": "Realistic traffic simulation is critical for the development of autonomous\ndriving systems and urban mobility planning, yet existing imitation learning\napproaches often fail to model realistic traffic behaviors. Behavior cloning\nsuffers from covariate shift, while Generative Adversarial Imitation Learning\n(GAIL) is notoriously unstable in multi-agent settings. We identify a key\nsource of this instability: irrelevant interaction misguidance, where a\ndiscriminator penalizes an ego vehicle's realistic behavior due to unrealistic\ninteractions among its neighbors. To address this, we propose Decomposed\nMulti-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map\nand ego-neighbor components, filtering out misleading neighbor: neighbor and\nneighbor: map interactions. We further introduce a social PPO objective that\naugments ego rewards with distance-weighted neighborhood rewards, encouraging\noverall realism across agents. Integrated into a lightweight SMART-based\nbackbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim\nAgents 2025 benchmark.", "AI": {"tldr": "Proposed DecompGAIL to address instability in multi-agent GAIL by decomposing realism into ego-map and ego-neighbor components, filtering irrelevant interactions, and adding social PPO with neighborhood rewards.", "motivation": "Existing imitation learning approaches fail to model realistic traffic behaviors - behavior cloning suffers from covariate shift, while GAIL is unstable in multi-agent settings due to irrelevant interaction misguidance.", "method": "Decomposed Multi-agent GAIL (DecompGAIL) that explicitly decomposes realism into ego-map and ego-neighbor components, filters out misleading neighbor-neighbor and neighbor-map interactions, and introduces social PPO objective with distance-weighted neighborhood rewards.", "result": "Achieves state-of-the-art performance on the WOMD Sim Agents 2025 benchmark when integrated into a lightweight SMART-based backbone.", "conclusion": "DecompGAIL effectively addresses the instability issues in multi-agent GAIL by decomposing realism components and incorporating social rewards, enabling more realistic traffic simulation."}}
{"id": "2510.07048", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.07048", "abs": "https://arxiv.org/abs/2510.07048", "authors": ["Yuntao Gui", "James Cheng"], "title": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models", "comment": null, "summary": "Despite their remarkable natural language understanding capabilities, Large\nLanguage Models (LLMs) have been underutilized for retrieval tasks. We present\nSearch-R3, a novel framework that addresses this limitation by adapting LLMs to\ngenerate search embeddings as a direct output of their reasoning process. Our\napproach exploits LLMs' chain-of-thought capabilities, allowing them to produce\nmore effective embeddings by reasoning step-by-step through complex semantic\nanalyses. We implement this through three complementary mechanisms. (1) a\nsupervised learning stage enables the model's ability to produce quality\nembeddings, (2) a reinforcement learning (RL) methodology that optimizes\nembedding generation alongside reasoning, and (3) a specialized RL environment\nthat efficiently handles evolving embedding representations without requiring\ncomplete corpus re-encoding at each training iteration. Our extensive\nevaluations on diverse benchmarks demonstrate that Search-R3 significantly\noutperforms prior methods by unifying the reasoning and embedding generation\nprocesses. This integrated post-training approach represents a substantial\nadvancement in handling complex knowledge-intensive tasks that require both\nsophisticated reasoning and effective information retrieval. Project page:\nhttps://github.com/ytgui/Search-R3", "AI": {"tldr": "Search-R3 is a novel framework that adapts LLMs to generate search embeddings directly from their reasoning process, combining supervised learning, reinforcement learning, and specialized training environments to outperform prior methods on complex retrieval tasks.", "motivation": "Large Language Models have strong natural language understanding but are underutilized for retrieval tasks. The authors aim to leverage LLMs' chain-of-thought reasoning capabilities to produce more effective search embeddings.", "method": "Three-stage approach: (1) supervised learning for quality embeddings, (2) reinforcement learning to optimize embedding generation with reasoning, (3) specialized RL environment that handles evolving embeddings without full corpus re-encoding.", "result": "Extensive evaluations on diverse benchmarks show Search-R3 significantly outperforms prior methods by unifying reasoning and embedding generation processes.", "conclusion": "The integrated post-training approach represents substantial advancement for complex knowledge-intensive tasks requiring both sophisticated reasoning and effective information retrieval."}}
{"id": "2510.06940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06940", "abs": "https://arxiv.org/abs/2510.06940", "authors": ["Krishna Sri Ipsit Mantri", "Or Feldman", "Moshe Eliasof", "Chaim Baskin"], "title": "Revisiting Node Affinity Prediction in Temporal Graphs", "comment": "preprint", "summary": "Node affinity prediction is a common task that is widely used in temporal\ngraph learning with applications in social and financial networks, recommender\nsystems, and more. Recent works have addressed this task by adapting\nstate-of-the-art dynamic link property prediction models to node affinity\nprediction. However, simple heuristics, such as Persistent Forecast or Moving\nAverage, outperform these models. In this work, we analyze the challenges in\ntraining current Temporal Graph Neural Networks for node affinity prediction\nand suggest appropriate solutions. Combining the solutions, we develop NAViS -\nNode Affinity prediction model using Virtual State, by exploiting the\nequivalence between heuristics and state space models. While promising,\ntraining NAViS is non-trivial. Therefore, we further introduce a novel loss\nfunction for node affinity prediction. We evaluate NAViS on TGB and show that\nit outperforms the state-of-the-art, including heuristics. Our source code is\navailable at https://github.com/orfeld415/NAVIS", "AI": {"tldr": "NAViS is a node affinity prediction model that addresses limitations of current temporal graph neural networks by using virtual states and a novel loss function, outperforming state-of-the-art methods and simple heuristics.", "motivation": "Current temporal graph neural networks underperform simple heuristics like Persistent Forecast or Moving Average for node affinity prediction, indicating fundamental training challenges that need to be addressed.", "method": "Developed NAViS by exploiting equivalence between heuristics and state space models, using virtual states and introducing a novel loss function specifically designed for node affinity prediction.", "result": "NAViS outperforms state-of-the-art methods including heuristics on TGB benchmarks, demonstrating superior performance in node affinity prediction tasks.", "conclusion": "The proposed NAViS model successfully addresses training challenges in temporal graph neural networks for node affinity prediction and establishes new state-of-the-art performance."}}
{"id": "2510.07060", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07060", "abs": "https://arxiv.org/abs/2510.07060", "authors": ["Miriam Wanner", "Sophia Hager", "Anjalie Field"], "title": "Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations", "comment": null, "summary": "Local news stations are often considered to be reliable sources of\nnon-politicized information, particularly local concerns that residents care\nabout. Because these stations are trusted news sources, viewers are\nparticularly susceptible to the information they report. The Sinclair Broadcast\ngroup is a broadcasting company that has acquired many local news stations in\nthe last decade. We investigate the effects of local news stations being\nacquired by Sinclair: how does coverage change? We use computational methods to\ninvestigate changes in internet content put out by local news stations before\nand after being acquired by Sinclair and in comparison to national news\noutlets. We find that there is clear evidence that local news stations report\nmore frequently on national news at the expense of local topics, and that their\ncoverage of polarizing national topics increases.", "AI": {"tldr": "Sinclair Broadcast Group's acquisition of local news stations leads to increased national news coverage and more polarizing topics, reducing local content.", "motivation": "To investigate how Sinclair's acquisition affects local news coverage, specifically changes in content focus and politicization.", "method": "Computational analysis of internet content from local news stations before and after acquisition, compared to national outlets.", "result": "Clear evidence of increased national news reporting and more polarizing topics, with reduced local coverage.", "conclusion": "Sinclair's acquisition shifts local news towards national, politicized content, undermining their traditional role as trusted local sources."}}
{"id": "2510.06945", "categories": ["cs.LG", "cond-mat.dis-nn", "physics.data-an", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.06945", "abs": "https://arxiv.org/abs/2510.06945", "authors": ["Lorenzo Pastori", "Veronika Eyring", "Mierk Schwabe"], "title": "Fisher Information, Training and Bias in Fourier Regression Models", "comment": null, "summary": "Motivated by the growing interest in quantum machine learning, in particular\nquantum neural networks (QNNs), we study how recently introduced evaluation\nmetrics based on the Fisher information matrix (FIM) are effective for\npredicting their training and prediction performance. We exploit the\nequivalence between a broad class of QNNs and Fourier models, and study the\ninterplay between the \\emph{effective dimension} and the \\emph{bias} of a model\ntowards a given task, investigating how these affect the model's training and\nperformance. We show that for a model that is completely agnostic, or unbiased,\ntowards the function to be learned, a higher effective dimension likely results\nin a better trainability and performance. On the other hand, for models that\nare biased towards the function to be learned a lower effective dimension is\nlikely beneficial during training. To obtain these results, we derive an\nanalytical expression of the FIM for Fourier models and identify the features\ncontrolling a model's effective dimension. This allows us to construct models\nwith tunable effective dimension and bias, and to compare their training. We\nfurthermore introduce a tensor network representation of the considered Fourier\nmodels, which could be a tool of independent interest for the analysis of QNN\nmodels. Overall, these findings provide an explicit example of the interplay\nbetween geometrical properties, model-task alignment and training, which are\nrelevant for the broader machine learning community.", "AI": {"tldr": "The paper studies how Fisher information matrix (FIM) metrics predict quantum neural network (QNN) performance, showing that higher effective dimension benefits unbiased models while lower effective dimension helps biased models.", "motivation": "Growing interest in quantum machine learning and QNNs motivates studying FIM-based evaluation metrics for predicting training and prediction performance.", "method": "Exploit equivalence between QNNs and Fourier models, derive analytical FIM expression, construct models with tunable effective dimension and bias, and introduce tensor network representation.", "result": "For unbiased models, higher effective dimension improves trainability and performance; for biased models, lower effective dimension is beneficial during training.", "conclusion": "Findings demonstrate interplay between geometrical properties, model-task alignment and training, relevant for broader machine learning community."}}
{"id": "2510.07061", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07061", "abs": "https://arxiv.org/abs/2510.07061", "authors": ["Amir Hossein Yari", "Kalmit Kulkarni", "Ahmad Raza Khan", "Fajri Koto"], "title": "Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages", "comment": "18 pages, 14 figures", "summary": "While automatic metrics drive progress in Machine Translation (MT) and Text\nSummarization (TS), existing metrics have been developed and validated almost\nexclusively for English and other high-resource languages. This narrow focus\nleaves Indian languages, spoken by over 1.5 billion people, largely overlooked,\ncasting doubt on the universality of current evaluation practices. To address\nthis gap, we introduce ITEM, a large-scale benchmark that systematically\nevaluates the alignment of 26 automatic metrics with human judgments across six\nmajor Indian languages, enriched with fine-grained annotations. Our extensive\nevaluation, covering agreement with human judgments, sensitivity to outliers,\nlanguage-specific reliability, inter-metric correlations, and resilience to\ncontrolled perturbations, reveals four central findings: (1) LLM-based\nevaluators show the strongest alignment with human judgments at both segment\nand system levels; (2) outliers exert a significant impact on metric-human\nagreement; (3) in TS, metrics are more effective at capturing content fidelity,\nwhereas in MT, they better reflect fluency; and (4) metrics differ in their\nrobustness and sensitivity when subjected to diverse perturbations.\nCollectively, these findings offer critical guidance for advancing metric\ndesign and evaluation in Indian languages.", "AI": {"tldr": "ITEM is a benchmark evaluating 26 automatic metrics' alignment with human judgments across 6 Indian languages, revealing LLM-based evaluators perform best, outliers significantly impact agreement, and metrics differ in content fidelity vs fluency capture between summarization and translation tasks.", "motivation": "Current automatic metrics are developed and validated primarily for English and high-resource languages, leaving Indian languages (spoken by 1.5B+ people) overlooked and questioning the universality of evaluation practices.", "method": "Created ITEM benchmark with fine-grained annotations to systematically evaluate 26 automatic metrics' alignment with human judgments across 6 major Indian languages, covering agreement, outlier sensitivity, language-specific reliability, inter-metric correlations, and perturbation resilience.", "result": "Four key findings: (1) LLM-based evaluators show strongest human alignment; (2) outliers significantly impact metric-human agreement; (3) TS metrics better capture content fidelity while MT metrics better reflect fluency; (4) metrics differ in robustness to perturbations.", "conclusion": "The findings provide critical guidance for advancing metric design and evaluation specifically for Indian languages, addressing the current gap in multilingual evaluation practices."}}
{"id": "2510.06949", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06949", "abs": "https://arxiv.org/abs/2510.06949", "authors": ["Junghwan Lim", "Sungmin Lee", "Dongseok Kim", "Wai Ting Cheung", "Beomgyu Kim", "Taehwan Kim", "Haesol Lee", "Junhyeok Lee", "Dongpin Oh", "Eunhwan Park"], "title": "Grouped Differential Attention", "comment": null, "summary": "The self-attention mechanism, while foundational to modern Transformer\narchitectures, suffers from a critical inefficiency: it frequently allocates\nsubstantial attention to redundant or noisy context. Differential Attention\naddressed this by using subtractive attention maps for signal and noise, but\nits required balanced head allocation imposes rigid constraints on\nrepresentational flexibility and scalability.\n  To overcome this, we propose Grouped Differential Attention (GDA), a novel\napproach that introduces unbalanced head allocation between signal-preserving\nand noise-control groups. GDA significantly enhances signal focus by\nstrategically assigning more heads to signal extraction and fewer to\nnoise-control, stabilizing the latter through controlled repetition (akin to\nGQA). This design achieves stronger signal fidelity with minimal computational\noverhead. We further extend this principle to group-differentiated growth, a\nscalable strategy that selectively replicates only the signal-focused heads,\nthereby ensuring efficient capacity expansion.\n  Through large-scale pretraining and continual training experiments, we\ndemonstrate that moderate imbalance ratios in GDA yield substantial\nimprovements in generalization and stability compared to symmetric baselines.\nOur results collectively establish that ratio-aware head allocation and\nselective expansion offer an effective and practical path toward designing\nscalable, computation-efficient Transformer architectures.", "AI": {"tldr": "Grouped Differential Attention (GDA) improves Transformer efficiency by using unbalanced head allocation between signal-preserving and noise-control groups, achieving better signal focus with minimal computational overhead.", "motivation": "Self-attention mechanisms in Transformers often waste resources on redundant or noisy context. Previous Differential Attention approach had rigid constraints due to balanced head allocation.", "method": "Proposed GDA with unbalanced head allocation: more heads for signal extraction, fewer for noise-control with controlled repetition. Extended to group-differentiated growth for scalable capacity expansion.", "result": "Moderate imbalance ratios in GDA yield substantial improvements in generalization and stability compared to symmetric baselines in large-scale pretraining experiments.", "conclusion": "Ratio-aware head allocation and selective expansion provide an effective path for designing scalable, computation-efficient Transformer architectures."}}
{"id": "2510.07074", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07074", "abs": "https://arxiv.org/abs/2510.07074", "authors": ["Fred Philippy", "Laura Bernardy", "Siwen Guo", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish", "comment": "Paper under review; Dataset available at\n  https://huggingface.co/datasets/fredxlpy/LuxInstruct", "summary": "Instruction tuning has become a key technique for enhancing the performance\nof large language models, enabling them to better follow human prompts.\nHowever, low-resource languages such as Luxembourgish face severe limitations\ndue to the lack of high-quality instruction datasets. Traditional reliance on\nmachine translation often introduces semantic misalignment and cultural\ninaccuracies. In this work, we address these challenges by creating a\ncross-lingual instruction tuning dataset for Luxembourgish, without resorting\nto machine-generated translations into it. Instead, by leveraging aligned data\nfrom English, French, and German, we build a high-quality dataset that\npreserves linguistic and cultural nuances. We provide evidence that\ncross-lingual instruction tuning not only improves representational alignment\nacross languages but also the model's generative capabilities in Luxembourgish.\nThis highlights how cross-lingual data curation can avoid the common pitfalls\nof machine-translated data and directly benefit low-resource language\ndevelopment.", "AI": {"tldr": "Cross-lingual instruction tuning for Luxembourgish using aligned English, French, and German data instead of machine translation, improving model performance while preserving linguistic and cultural nuances.", "motivation": "Low-resource languages like Luxembourgish lack high-quality instruction datasets, and traditional machine translation approaches introduce semantic misalignment and cultural inaccuracies.", "method": "Created cross-lingual instruction tuning dataset by leveraging aligned data from English, French, and German, avoiding machine-generated translations into Luxembourgish.", "result": "Cross-lingual instruction tuning improves representational alignment across languages and enhances the model's generative capabilities in Luxembourgish.", "conclusion": "Cross-lingual data curation avoids pitfalls of machine-translated data and directly benefits low-resource language development."}}
{"id": "2510.06954", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06954", "abs": "https://arxiv.org/abs/2510.06954", "authors": ["Zheng-An Chen", "Tao Luo"], "title": "From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics", "comment": null, "summary": "Although transformer-based models have shown exceptional empirical\nperformance, the fundamental principles governing their training dynamics are\ninadequately characterized beyond configuration-specific studies. Inspired by\nempirical evidence showing improved reasoning capabilities under small\ninitialization scales in language models, we employ the gradient flow\nanalytical framework established in [Zhou et al. NeurIPS 2022] to\nsystematically investigate linearized Transformer training dynamics. Our\ntheoretical analysis dissects the dynamics of attention modules into two\ndistinct stages. In the first stage, asymmetric weight perturbations from\nrandom initialization sustain non-degenerate gradient dynamics in parameter\nmatrices, facilitating systematic escape from small initialization regimes.\nSubsequently, these matrices undergo condensation, progressively aligning\ntoward the target orientation. In the second stage, the previously static\nkey-query matrices actively participate in training, driving the normalized\nmatrices toward asymptotic rank collapse. This two-stage framework generalizes\nclassical directional convergence results.", "AI": {"tldr": "The paper analyzes transformer training dynamics using gradient flow framework, revealing a two-stage process: asymmetric weight perturbations enable escape from small initialization, followed by key-query matrix condensation and rank collapse.", "motivation": "To understand fundamental principles of transformer training dynamics beyond configuration-specific studies, inspired by improved reasoning capabilities under small initialization scales in language models.", "method": "Employ gradient flow analytical framework from previous work to systematically investigate linearized Transformer training dynamics through theoretical analysis.", "result": "Identified two distinct stages: 1) asymmetric weight perturbations sustain gradient dynamics for escaping small initialization, 2) key-query matrices become active and drive normalized matrices toward asymptotic rank collapse.", "conclusion": "The two-stage framework generalizes classical directional convergence results and provides systematic understanding of transformer training dynamics."}}
{"id": "2510.07081", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07081", "abs": "https://arxiv.org/abs/2510.07081", "authors": ["Fanheng Kong", "Jingyuan Zhang", "Yahui Liu", "Zirui Wu", "Yu Tian", "Victoria W.", "Guorui Zhou"], "title": "Accelerating Diffusion LLM Inference via Local Determinism Propagation", "comment": "21 pages, 4 figures. Under review", "summary": "Diffusion large language models (dLLMs) represent a significant advancement\nin text generation, offering parallel token decoding capabilities. However,\nexisting open-source implementations suffer from quality-speed trade-offs that\nimpede their practical deployment. Conservative sampling strategies typically\ndecode only the most confident token per step to ensure quality (i.e., greedy\ndecoding), at the cost of inference efficiency due to repeated redundant\nrefinement iterations--a phenomenon we term delayed decoding. Through\nsystematic analysis of dLLM decoding dynamics, we characterize this delayed\ndecoding behavior and propose a training-free adaptive parallel decoding\nstrategy, named LocalLeap, to address these inefficiencies. LocalLeap is built\non two fundamental empirical principles: local determinism propagation centered\non high-confidence anchors and progressive spatial consistency decay. By\napplying these principles, LocalLeap identifies anchors and performs localized\nrelaxed parallel decoding within bounded neighborhoods, achieving substantial\ninference step reduction through early commitment of already-determined tokens\nwithout compromising output quality. Comprehensive evaluation on various\nbenchmarks demonstrates that LocalLeap achieves 6.94$\\times$ throughput\nimprovements and reduces decoding steps to just 14.2\\% of the original\nrequirement, achieving these gains with negligible performance impact. The\nsource codes are available at: https://github.com/friedrichor/LocalLeap.", "AI": {"tldr": "LocalLeap is a training-free adaptive parallel decoding strategy for diffusion LLMs that addresses delayed decoding inefficiencies by using local determinism propagation and progressive spatial consistency decay to achieve 6.94\u00d7 throughput improvements with minimal performance impact.", "motivation": "Existing open-source diffusion LLM implementations suffer from quality-speed trade-offs where conservative sampling strategies (greedy decoding) cause delayed decoding - repeated redundant refinement iterations that reduce inference efficiency.", "method": "Proposes LocalLeap based on two principles: local determinism propagation around high-confidence anchors and progressive spatial consistency decay. Identifies anchors and performs localized relaxed parallel decoding within bounded neighborhoods to enable early commitment of determined tokens.", "result": "Achieves 6.94\u00d7 throughput improvements and reduces decoding steps to just 14.2% of original requirement across various benchmarks, with negligible performance impact on output quality.", "conclusion": "LocalLeap effectively addresses delayed decoding in diffusion LLMs through adaptive parallel decoding, enabling substantial inference acceleration without compromising generation quality."}}
{"id": "2510.06955", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06955", "abs": "https://arxiv.org/abs/2510.06955", "authors": ["Masih Aminbeidokhti", "Heitor Rapela Medeiros", "Eric Granger", "Marco Pedersoli"], "title": "High-Rate Mixout: Revisiting Mixout for Robust Domain Generalization", "comment": "WACV 2026: Winter Conference on Applications of Computer Vision 2026", "summary": "Ensembling fine-tuned models initialized from powerful pre-trained weights is\na common strategy to improve robustness under distribution shifts, but it comes\nwith substantial computational costs due to the need to train and store\nmultiple models. Dropout offers a lightweight alternative by simulating\nensembles through random neuron deactivation; however, when applied to\npre-trained models, it tends to over-regularize and disrupt critical\nrepresentations necessary for generalization. In this work, we investigate\nMixout, a stochastic regularization technique that provides an alternative to\nDropout for domain generalization. Rather than deactivating neurons, Mixout\nmitigates overfitting by probabilistically swapping a subset of fine-tuned\nweights with their pre-trained counterparts during training, thereby\nmaintaining a balance between adaptation and retention of prior knowledge. Our\nstudy reveals that achieving strong performance with Mixout on domain\ngeneralization benchmarks requires a notably high masking probability of 0.9\nfor ViTs and 0.8 for ResNets. While this may seem like a simple adjustment, it\nyields two key advantages for domain generalization: (1) higher masking rates\nmore strongly penalize deviations from the pre-trained parameters, promoting\nbetter generalization to unseen domains; and (2) high-rate masking\nsubstantially reduces computational overhead, cutting gradient computation by\nup to 45% and gradient memory usage by up to 90%. Experiments across five\ndomain generalization benchmarks, PACS, VLCS, OfficeHome, TerraIncognita, and\nDomainNet, using ResNet and ViT architectures, show that our approach,\nHigh-rate Mixout, achieves out-of-domain accuracy comparable to ensemble-based\nmethods while significantly reducing training costs.", "AI": {"tldr": "Mixout is a lightweight alternative to model ensembling for domain generalization that uses high masking probabilities (0.9 for ViTs, 0.8 for ResNets) to swap fine-tuned weights with pre-trained counterparts, achieving comparable performance to ensembles with 45% less gradient computation and 90% less memory usage.", "motivation": "To address the computational inefficiency of model ensembling for domain generalization while avoiding the over-regularization issues of Dropout when applied to pre-trained models.", "method": "Mixout regularization that probabilistically swaps fine-tuned weights with pre-trained weights during training, using high masking rates to maintain balance between adaptation and prior knowledge retention.", "result": "Achieves comparable out-of-domain accuracy to ensemble methods across five benchmarks (PACS, VLCS, OfficeHome, TerraIncognita, DomainNet) with ResNet and ViT architectures, while reducing gradient computation by 45% and gradient memory usage by 90%.", "conclusion": "High-rate Mixout provides an efficient and effective alternative to model ensembling for domain generalization, balancing performance with computational efficiency."}}
{"id": "2510.07083", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07083", "abs": "https://arxiv.org/abs/2510.07083", "authors": ["Miriam Wanner", "Leif Azzopardi", "Paul Thomas", "Soham Dan", "Benjamin Van Durme", "Nick Craswell"], "title": "All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations", "comment": null, "summary": "Existing methods for evaluating the factuality of large language model (LLM)\nresponses treat all claims as equally important. This results in misleading\nevaluations when vital information is missing or incorrect as it receives the\nsame weight as peripheral details, raising the question: how can we reliably\ndetect such differences when there are errors in key information? Current\napproaches that measure factuality tend to be insensitive to omitted or false\nkey information. To investigate this lack of sensitivity, we construct\nVITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses\ndesigned to omit or falsify key information. Using this dataset, we demonstrate\nthe insensitivities of existing evaluation metrics to key information errors.\nTo address this gap, we introduce VITAL, a set of metrics that provide greater\nsensitivity in measuring the factuality of responses by incorporating the\nrelevance and importance of claims with respect to the query. Our analysis\ndemonstrates that VITAL metrics more reliably detect errors in key information\nthan previous methods. Our dataset, metrics, and analysis provide a foundation\nfor more accurate and robust assessment of LLM factuality.", "AI": {"tldr": "This paper introduces VITAL, a new set of metrics for evaluating LLM factuality that considers claim importance, addressing the limitation of existing methods that treat all claims equally.", "motivation": "Existing factuality evaluation methods treat all claims as equally important, leading to misleading assessments when key information is missing or incorrect. Current approaches are insensitive to omitted or false key information.", "method": "The authors constructed VITALERRORS benchmark with 6,733 queries containing minimally altered LLM responses that omit or falsify key information. They then introduced VITAL metrics that incorporate claim relevance and importance relative to the query.", "result": "Analysis showed that existing evaluation metrics are insensitive to key information errors, while VITAL metrics more reliably detect errors in key information compared to previous methods.", "conclusion": "The VITAL metrics, dataset, and analysis provide a foundation for more accurate and robust assessment of LLM factuality by properly weighting the importance of different claims."}}
{"id": "2510.06982", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.06982", "abs": "https://arxiv.org/abs/2510.06982", "authors": ["Masih Aminbeidokhti", "Heitor Rapela Medeiros", "Eric Granger", "Marco Pedersoli"], "title": "Revisiting Mixout: An Overlooked Path to Robust Finetuning", "comment": null, "summary": "Finetuning vision foundation models often improves in-domain accuracy but\ncomes at the cost of robustness under distribution shift. We revisit Mixout, a\nstochastic regularizer that intermittently replaces finetuned weights with\ntheir pretrained reference, through the lens of a single-run, weight-sharing\nimplicit ensemble. This perspective reveals three key levers that govern\nrobustness: the \\emph{masking anchor}, \\emph{resampling frequency}, and\n\\emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i)\nreplaces the fixed anchor with an exponential moving-average snapshot that\nadapts during training, and (ii) regulates masking period via an explicit\nresampling-frequency hyperparameter. Our sparse-kernel implementation updates\nonly a small fraction of parameters with no inference-time overhead, enabling\ntraining on consumer-grade GPUs. Experiments on benchmarks covering covariate\nshift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet,\niWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy\nbeyond zero-shot performance while surpassing both Model Soups and strong\nparameter-efficient finetuning baselines under distribution shift.", "AI": {"tldr": "GMixout improves finetuning robustness by using adaptive weight averaging and controlled masking frequency, achieving better accuracy and distribution shift performance than existing methods.", "motivation": "Finetuning vision models improves in-domain accuracy but reduces robustness under distribution shift. The paper aims to enhance robustness while maintaining accuracy.", "method": "Introduces GMixout with adaptive exponential moving-average anchor and regulated masking frequency, implemented via sparse-kernel updates for efficiency.", "result": "GMixout consistently improves in-domain accuracy beyond zero-shot performance and outperforms Model Soups and parameter-efficient baselines under distribution shift across multiple benchmarks.", "conclusion": "GMixout effectively balances in-domain accuracy and robustness through adaptive weight regularization, providing a practical solution for finetuning vision foundation models."}}
{"id": "2510.07096", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.07096", "abs": "https://arxiv.org/abs/2510.07096", "authors": ["Zhu Li", "Yuqing Zhang", "Xiyuan Gao", "Shekhar Nayak", "Matt Coler"], "title": "Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis", "comment": null, "summary": "Sarcasm is a subtle form of non-literal language that poses significant\nchallenges for speech synthesis due to its reliance on nuanced semantic,\ncontextual, and prosodic cues. While existing speech synthesis research has\nfocused primarily on broad emotional categories, sarcasm remains largely\nunexplored. In this paper, we propose a Large Language Model (LLM)-enhanced\nRetrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach\ncombines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture\npragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic\nexemplars retrieved via a Retrieval Augmented Generation (RAG) module, which\nprovide expressive reference patterns of sarcastic delivery. Integrated within\na VITS backbone, this dual conditioning enables more natural and contextually\nappropriate sarcastic speech. Experiments demonstrate that our method\noutperforms baselines in both objective measures and subjective evaluations,\nyielding improvements in speech naturalness, sarcastic expressivity, and\ndownstream sarcasm detection.", "AI": {"tldr": "LLM-enhanced RAG framework for sarcasm-aware speech synthesis using semantic embeddings from fine-tuned LLaMA 3 and prosodic exemplars from RAG, integrated with VITS backbone.", "motivation": "Sarcasm poses challenges for speech synthesis due to its nuanced semantic, contextual, and prosodic cues, and remains largely unexplored compared to broad emotional categories.", "method": "Dual conditioning approach combining semantic embeddings from LoRA-fine-tuned LLaMA 3 (capturing pragmatic incongruity) and prosodic exemplars from RAG module, integrated within VITS backbone.", "result": "Outperforms baselines in objective measures and subjective evaluations, with improvements in speech naturalness, sarcastic expressivity, and downstream sarcasm detection.", "conclusion": "The proposed framework enables more natural and contextually appropriate sarcastic speech synthesis by effectively capturing both semantic and prosodic aspects of sarcasm."}}
{"id": "2510.06987", "categories": ["cs.LG", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06987", "abs": "https://arxiv.org/abs/2510.06987", "authors": ["Rohith Mahadevan"], "title": "Spiral Model Technique For Data Science & Machine Learning Lifecycle", "comment": null, "summary": "Analytics play an important role in modern business. Companies adapt data\nscience lifecycles to their culture to seek productivity and improve their\ncompetitiveness among others. Data science lifecycles are fairly an important\ncontributing factor to start and end a project that are data dependent. Data\nscience and Machine learning life cycles comprises of series of steps that are\ninvolved in a project. A typical life cycle states that it is a linear or\ncyclical model that revolves around. It is mostly depicted that it is possible\nin a traditional data science life cycle to start the process again after\nreaching the end of cycle. This paper suggests a new technique to incorporate\ndata science life cycle to business problems that have a clear end goal. A new\ntechnique called spiral technique is introduced to emphasize versatility,\nagility and iterative approach to business processes.", "AI": {"tldr": "This paper introduces a spiral technique for data science lifecycles in business contexts with clear end goals, emphasizing versatility, agility and iterative approaches.", "motivation": "Traditional data science lifecycles are often depicted as linear or cyclical models that can restart after completion, but businesses need more focused approaches for problems with clear end goals.", "method": "The paper proposes a new spiral technique that incorporates data science lifecycle principles into business processes, focusing on iterative development and agile methodologies.", "result": "The spiral technique provides a more versatile and agile approach to data science lifecycles in business contexts, allowing for iterative refinement while maintaining focus on clear end goals.", "conclusion": "The spiral technique offers an improved framework for applying data science lifecycles to business problems with defined objectives, enhancing productivity and competitiveness through its iterative and agile nature."}}
{"id": "2510.07098", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07098", "abs": "https://arxiv.org/abs/2510.07098", "authors": ["Guo Yutong", "Wanying Wang", "Yue Wu", "Zichen Miao", "Haoyu Wang"], "title": "TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription", "comment": null, "summary": "Table Visual Question Answering (Table VQA) is typically addressed by large\nvision-language models (VLMs). While such models can answer directly from\nimages, they often miss fine-grained details unless scaled to very large sizes,\nwhich are computationally prohibitive, especially for mobile deployment. A\nlighter alternative is to have a small VLM perform OCR and then use a large\nlanguage model (LLM) to reason over structured outputs such as Markdown tables.\nHowever, these representations are not naturally optimized for LLMs and still\nintroduce substantial errors. We propose TALENT (Table VQA via Augmented\nLanguage-Enhanced Natural-text Transcription), a lightweight framework that\nleverages dual representations of tables. TALENT prompts a small VLM to produce\nboth OCR text and natural language narration, then combines them with the\nquestion for reasoning by an LLM. This reframes Table VQA as an LLM-centric\nmultimodal reasoning task, where the VLM serves as a perception-narration\nmodule rather than a monolithic solver. Additionally, we construct ReTabVQA, a\nmore challenging Table VQA dataset requiring multi-step quantitative reasoning\nover table images. Experiments show that TALENT enables a small VLM-LLM\ncombination to match or surpass a single large VLM at significantly lower\ncomputational cost on both public datasets and ReTabVQA.", "AI": {"tldr": "TALENT is a lightweight framework that uses dual representations (OCR text + natural language narration) from a small VLM, combined with LLM reasoning, to achieve Table VQA performance comparable to large VLMs at much lower computational cost.", "motivation": "Large VLMs for Table VQA are computationally prohibitive for mobile deployment and miss fine-grained details, while existing OCR+LLM approaches using structured outputs like Markdown tables still introduce substantial errors.", "method": "TALENT prompts a small VLM to produce both OCR text and natural language narration of tables, then combines these dual representations with the question for reasoning by an LLM, reframing Table VQA as an LLM-centric multimodal task.", "result": "TALENT enables small VLM-LLM combinations to match or surpass single large VLMs at significantly lower computational cost on both public datasets and the newly constructed ReTabVQA dataset requiring multi-step quantitative reasoning.", "conclusion": "The proposed dual-representation approach effectively separates perception from reasoning, making Table VQA more efficient and accurate while being suitable for mobile deployment."}}
{"id": "2510.07018", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07018", "abs": "https://arxiv.org/abs/2510.07018", "authors": ["Dung Hoang-Anh", "Cuong Pham Trung Le", "Jianfei Cai", "Thanh-Toan Do"], "title": "Sharpness-Aware Data Generation for Zero-shot Quantization", "comment": null, "summary": "Zero-shot quantization aims to learn a quantized model from a pre-trained\nfull-precision model with no access to original real training data. The common\nidea in zero-shot quantization approaches is to generate synthetic data for\nquantizing the full-precision model. While it is well-known that deep neural\nnetworks with low sharpness have better generalization ability, none of the\nprevious zero-shot quantization works considers the sharpness of the quantized\nmodel as a criterion for generating training data. This paper introduces a\nnovel methodology that takes into account quantized model sharpness in\nsynthetic data generation to enhance generalization. Specifically, we first\ndemonstrate that sharpness minimization can be attained by maximizing gradient\nmatching between the reconstruction loss gradients computed on synthetic and\nreal validation data, under certain assumptions. We then circumvent the problem\nof the gradient matching without real validation set by approximating it with\nthe gradient matching between each generated sample and its neighbors.\nExperimental evaluations on CIFAR-100 and ImageNet datasets demonstrate the\nsuperiority of the proposed method over the state-of-the-art techniques in\nlow-bit quantization settings.", "AI": {"tldr": "Zero-shot quantization method that considers model sharpness during synthetic data generation to improve generalization, using gradient matching between synthetic and real data gradients.", "motivation": "Previous zero-shot quantization approaches don't consider quantized model sharpness as a criterion for synthetic data generation, even though low sharpness is known to improve generalization.", "method": "Proposes sharpness minimization through gradient matching between reconstruction loss gradients on synthetic and real validation data, approximated by matching gradients between generated samples and their neighbors when real data is unavailable.", "result": "Experimental evaluations on CIFAR-100 and ImageNet show superior performance over state-of-the-art techniques in low-bit quantization settings.", "conclusion": "Considering quantized model sharpness in synthetic data generation enhances generalization in zero-shot quantization."}}
{"id": "2510.07105", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07105", "abs": "https://arxiv.org/abs/2510.07105", "authors": ["Taylor Sorensen", "Yejin Choi"], "title": "Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning", "comment": "NLPerspectives: The 4th Workshop on Perspectivist Approaches to\n  Natural Language Processing at EMNLP 2025", "summary": "Many natural language processing (NLP) tasks involve subjectivity, ambiguity,\nor legitimate disagreement between annotators. In this paper, we outline our\nsystem for modeling human variation. Our system leverages language models'\n(LLMs) in-context learning abilities, along with a two-step meta-learning\ntraining procedure for 1) post-training on many datasets requiring in-context\nlearning and 2) specializing the model via in-context meta-learning to the\nparticular data distribution of interest. We also evaluate the performance of\nour system submission to the Learning With Disagreements (LeWiDi) competition,\nwhere it was the overall winner on both tasks. Additionally, we perform an\nablation study to measure the importance of each system component. We find that\nincluding rater examples in-context is crucial for our system's performance,\ndataset-specific fine-tuning is helpful on the larger datasets, post-training\non other in-context datasets is helpful on one of the competition datasets, and\nthat performance improves with model scale.", "AI": {"tldr": "A system for modeling human variation in NLP tasks using LLMs' in-context learning with two-step meta-learning, winning the LeWiDi competition and showing importance of rater examples and model scale.", "motivation": "Many NLP tasks involve subjectivity, ambiguity, or legitimate disagreement between annotators, requiring methods to model human variation.", "method": "Leverages LLMs' in-context learning with two-step meta-learning: 1) post-training on many in-context datasets, 2) specializing via in-context meta-learning to specific data distributions.", "result": "Won overall winner on both tasks in Learning With Disagreements (LeWiDi) competition. Ablation study showed rater examples in-context are crucial, dataset-specific fine-tuning helps on larger datasets, post-training helps on one competition dataset, and performance improves with model scale.", "conclusion": "The proposed system effectively models human variation in NLP tasks through in-context learning and meta-learning, demonstrating strong performance in handling annotator disagreements."}}
{"id": "2510.07022", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07022", "abs": "https://arxiv.org/abs/2510.07022", "authors": ["ZiHeng Huang", "Di Wu", "Jun Bai", "Jiale Zhang", "Sicong Cao", "Ji Zhang", "Yingjie Hu"], "title": "Federated Unlearning in the Wild: Rethinking Fairness and Data Discrepancy", "comment": null, "summary": "Machine unlearning is critical for enforcing data deletion rights like the\n\"right to be forgotten.\" As a decentralized paradigm, Federated Learning (FL)\nalso requires unlearning, but realistic implementations face two major\nchallenges. First, fairness in Federated Unlearning (FU) is often overlooked.\nExact unlearning methods typically force all clients into costly retraining,\neven those uninvolved. Approximate approaches, using gradient ascent or\ndistillation, make coarse interventions that can unfairly degrade performance\nfor clients with only retained data. Second, most FU evaluations rely on\nsynthetic data assumptions (IID/non-IID) that ignore real-world heterogeneity.\nThese unrealistic benchmarks obscure the true impact of unlearning and limit\nthe applicability of current methods. We first conduct a comprehensive\nbenchmark of existing FU methods under realistic data heterogeneity and\nfairness conditions. We then propose a novel, fairness-aware FU approach,\nFederated Cross-Client-Constrains Unlearning (FedCCCU), to explicitly address\nboth challenges. FedCCCU offers a practical and scalable solution for\nreal-world FU. Experimental results show that existing methods perform poorly\nin realistic settings, while our approach consistently outperforms them.", "AI": {"tldr": "This paper addresses fairness and realistic evaluation challenges in Federated Unlearning (FU), proposing FedCCCU as a fairness-aware solution that outperforms existing methods under realistic data heterogeneity.", "motivation": "Federated Learning requires unlearning capabilities for data deletion rights enforcement, but current approaches overlook fairness issues and rely on unrealistic synthetic data assumptions that limit real-world applicability.", "method": "Proposed Federated Cross-Client-Constrains Unlearning (FedCCCU), a fairness-aware approach that addresses both fairness and scalability challenges in FU through explicit constraints and realistic evaluation.", "result": "Experimental results show existing FU methods perform poorly under realistic settings, while FedCCCU consistently outperforms them in both fairness and effectiveness metrics.", "conclusion": "FedCCCU provides a practical and scalable solution for real-world Federated Unlearning that effectively addresses fairness concerns and works well under realistic data heterogeneity conditions."}}
{"id": "2510.07118", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07118", "abs": "https://arxiv.org/abs/2510.07118", "authors": ["Manish Nagaraj", "Sakshi Choudhary", "Utkarsh Saxena", "Deepak Ravikumar", "Kaushik Roy"], "title": "TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning", "comment": null, "summary": "Instruction tuning is essential for aligning large language models (LLMs) to\ndownstream tasks and commonly relies on large, diverse corpora. However, small,\nhigh-quality subsets, known as coresets, can deliver comparable or superior\nresults, though curating them remains challenging. Existing methods often rely\non coarse, sample-level signals like gradients, an approach that is\ncomputationally expensive and overlooks fine-grained features. To address this,\nwe introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a\nforward-only, token-centric framework. Instead of using gradients, TRIM\noperates by matching underlying representational patterns identified via\nattention-based \"fingerprints\" from a handful of target samples. Such an\napproach makes TRIM highly efficient and uniquely sensitive to the structural\nfeatures that define a task. Coresets selected by our method consistently\noutperform state-of-the-art baselines by up to 9% on downstream tasks and even\nsurpass the performance of full-data fine-tuning in some settings. By avoiding\nexpensive backward passes, TRIM achieves this at a fraction of the\ncomputational cost. These findings establish TRIM as a scalable and efficient\nalternative for building high-quality instruction-tuning datasets.", "AI": {"tldr": "TRIM is a forward-only, token-centric framework that uses attention-based fingerprints instead of gradients to select high-quality coresets for instruction tuning, achieving better performance than full-data fine-tuning in some cases with much lower computational cost.", "motivation": "Existing coreset selection methods for instruction tuning rely on computationally expensive gradient-based approaches that overlook fine-grained token-level features, making efficient high-quality dataset curation challenging.", "method": "TRIM uses attention-based \"fingerprints\" from target samples to identify representational patterns, operating in a forward-only manner without requiring backward passes, making it computationally efficient and sensitive to structural task features.", "result": "Coresets selected by TRIM outperform state-of-the-art baselines by up to 9% on downstream tasks and sometimes surpass full-data fine-tuning performance, while achieving this at a fraction of the computational cost.", "conclusion": "TRIM establishes itself as a scalable and efficient alternative for building high-quality instruction-tuning datasets by avoiding expensive gradient computations while maintaining or improving performance."}}
{"id": "2510.07035", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07035", "abs": "https://arxiv.org/abs/2510.07035", "authors": ["Tengwei Song", "Min Wu", "Yuan Fang"], "title": "Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration", "comment": "CIKM 2025", "summary": "Molecular representation learning plays a crucial role in advancing\napplications such as drug discovery and material design. Existing work\nleverages 2D and 3D modalities of molecular information for pre-training,\naiming to capture comprehensive structural and geometric insights. However,\nthese methods require paired 2D and 3D molecular data to train the model\neffectively and prevent it from collapsing into a single modality, posing\nlimitations in scenarios where a certain modality is unavailable or\ncomputationally expensive to generate. To overcome this limitation, we propose\nFlexMol, a flexible molecule pre-training framework that learns unified\nmolecular representations while supporting single-modality input. Specifically,\ninspired by the unified structure in vision-language models, our approach\nemploys separate models for 2D and 3D molecular data, leverages parameter\nsharing to improve computational efficiency, and utilizes a decoder to generate\nfeatures for the missing modality. This enables a multistage continuous\nlearning process where both modalities contribute collaboratively during\ntraining, while ensuring robustness when only one modality is available during\ninference. Extensive experiments demonstrate that FlexMol achieves superior\nperformance across a wide range of molecular property prediction tasks, and we\nalso empirically demonstrate its effectiveness with incomplete data. Our code\nand data are available at https://github.com/tewiSong/FlexMol.", "AI": {"tldr": "FlexMol is a flexible molecular pre-training framework that learns unified representations from 2D and 3D molecular data, supporting single-modality input when one modality is unavailable or expensive to generate.", "motivation": "Existing methods require paired 2D and 3D molecular data for training, which limits applicability when certain modalities are unavailable or computationally expensive to generate.", "method": "Uses separate models for 2D and 3D data with parameter sharing, employs a decoder to generate features for missing modalities, and implements multistage continuous learning where both modalities collaborate during training.", "result": "Achieves superior performance across various molecular property prediction tasks and demonstrates effectiveness with incomplete data.", "conclusion": "FlexMol provides a robust framework for molecular representation learning that works effectively even when only single modality is available during inference, overcoming limitations of existing paired-data approaches."}}
{"id": "2510.07141", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07141", "abs": "https://arxiv.org/abs/2510.07141", "authors": ["Samuel Joseph Amouyal", "Aya Meltzer-Asscher", "Jonathan Berant"], "title": "Comparing human and language models sentence processing difficulties on complex structures", "comment": "Data and code will be released soon", "summary": "Large language models (LLMs) that fluently converse with humans are a reality\n- but do LLMs experience human-like processing difficulties? We systematically\ncompare human and LLM sentence comprehension across seven challenging\nlinguistic structures. We collect sentence comprehension data from humans and\nfive families of state-of-the-art LLMs, varying in size and training procedure\nin a unified experimental framework. Our results show LLMs overall struggle on\nthe target structures, but especially on garden path (GP) sentences. Indeed,\nwhile the strongest models achieve near perfect accuracy on non-GP structures\n(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).\nAdditionally, when ranking structures based on average performance, rank\ncorrelation between humans and models increases with parameter count. For each\ntarget structure, we also collect data for their matched baseline without the\ndifficult structure. Comparing performance on the target vs. baseline\nsentences, the performance gap observed in humans holds for LLMs, with two\nexceptions: for models that are too weak performance is uniformly low across\nboth sentence types, and for models that are too strong the performance is\nuniformly high. Together, these reveal convergence and divergence in human and\nLLM sentence comprehension, offering new insights into the similarity of humans\nand LLMs.", "AI": {"tldr": "LLMs struggle with garden path sentences similar to humans, showing convergence in sentence comprehension difficulties but divergence in overall performance patterns.", "motivation": "To systematically compare human and LLM sentence comprehension across challenging linguistic structures, particularly examining whether LLMs experience human-like processing difficulties.", "method": "Collected sentence comprehension data from humans and five families of state-of-the-art LLMs in a unified experimental framework, testing seven challenging linguistic structures including garden path sentences and their matched baselines.", "result": "LLMs overall struggle on target structures, especially garden path sentences (46.8% accuracy for GPT-5 vs 93.7% on non-GP structures). Rank correlation between humans and models increases with parameter count. Performance gap patterns between target and baseline sentences show convergence for mid-range models but divergence for very weak or strong models.", "conclusion": "The study reveals both convergence and divergence in human and LLM sentence comprehension, offering new insights into the similarity between humans and LLMs in processing challenging linguistic structures."}}
{"id": "2510.07043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07043", "abs": "https://arxiv.org/abs/2510.07043", "authors": ["Tian Qin", "Felix Bai", "Ting-Yao Hu", "Raviteja Vemulapalli", "Hema Swetha Koppula", "Zhiyang Xu", "Bowen Jin", "Mert Cemri", "Jiarui Lu", "Zirui Wang", "Meng Cao"], "title": "COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization", "comment": null, "summary": "Real-world large language model (LLM) agents must master strategic tool use\nand user preference optimization through multi-turn interactions to assist\nusers with complex planning tasks. We introduce COMPASS (Constrained\nOptimization through Multi-turn Planning and Strategic Solutions), a benchmark\nthat evaluates agents on realistic travel-planning scenarios. We cast travel\nplanning as a constrained preference optimization problem, where agents must\nsatisfy hard constraints while simultaneously optimizing soft user preferences.\nTo support this, we build a realistic travel database covering transportation,\naccommodation, and ticketing for 20 U.S. National Parks, along with a\ncomprehensive tool ecosystem that mirrors commercial booking platforms.\nEvaluating state-of-the-art models, we uncover two critical gaps: (i) an\nacceptable-optimal gap, where agents reliably meet constraints but fail to\noptimize preferences, and (ii) a plan-coordination gap, where performance\ncollapses on multi-service (flight and hotel) coordination tasks, especially\nfor open-source models. By grounding reasoning and planning in a practical,\nuser-facing domain, COMPASS provides a benchmark that directly measures an\nagent's ability to optimize user preferences in realistic tasks, bridging\ntheoretical advances with real-world impact.", "AI": {"tldr": "COMPASS is a benchmark for evaluating LLM agents on realistic travel planning tasks, focusing on constrained preference optimization and multi-service coordination.", "motivation": "Real-world LLM agents need to master strategic tool use and user preference optimization through multi-turn interactions for complex planning tasks like travel planning.", "method": "Built a realistic travel database for 20 U.S. National Parks covering transportation, accommodation, and ticketing, with a comprehensive tool ecosystem mirroring commercial booking platforms.", "result": "Identified two critical gaps: acceptable-optimal gap (agents meet constraints but fail to optimize preferences) and plan-coordination gap (performance collapses on multi-service coordination tasks, especially for open-source models).", "conclusion": "COMPASS provides a benchmark that directly measures agents' ability to optimize user preferences in realistic tasks, bridging theoretical advances with real-world impact."}}
{"id": "2510.07167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07167", "abs": "https://arxiv.org/abs/2510.07167", "authors": ["Lekang Jiang", "Wenjun Sun", "Stephan Goetz"], "title": "Reasoning for Hierarchical Text Classification: The Case of Patents", "comment": "15 pages, 10 tables, 3 figures", "summary": "Hierarchical text classification (HTC) assigns documents to multiple levels\nof a pre-defined taxonomy. Automated patent subject classification represents\none of the hardest HTC scenarios because of domain knowledge difficulty and a\nhuge number of labels. Prior approaches only output a flat label set, which\noffers little insight into the reason behind predictions. Therefore, we propose\nReasoning for Hierarchical Classification (RHC), a novel framework that\nreformulates HTC as a step-by-step reasoning task to sequentially deduce\nhierarchical labels. RHC trains large language models (LLMs) in two stages: a\ncold-start stage that aligns outputs with chain-of-thought (CoT) reasoning\nformat and a reinforcement learning (RL) stage to enhance multi-step reasoning\nability. RHC demonstrates four advantages in our experiments. (1)\nEffectiveness: RHC surpasses previous baselines and outperforms the supervised\nfine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)\nExplainability: RHC produces natural-language justifications before prediction\nto facilitate human inspection. (3) Scalability: RHC scales favorably with\nmodel size with larger gains compared to standard fine-tuning. (4)\nApplicability: Beyond patents, we further demonstrate that RHC achieves\nstate-of-the-art performance on other widely used HTC benchmarks, which\nhighlights its broad applicability.", "AI": {"tldr": "RHC reformulates hierarchical text classification as step-by-step reasoning, using LLMs trained with CoT alignment and RL to achieve superior performance, explainability, scalability, and broad applicability.", "motivation": "Automated patent classification is challenging due to domain complexity and large label sets, and existing methods lack explainability by only outputting flat labels without reasoning insights.", "method": "Two-stage training: cold-start stage aligns LLM outputs with chain-of-thought reasoning format, followed by reinforcement learning stage to enhance multi-step reasoning ability for hierarchical label deduction.", "result": "RHC outperforms baselines by ~3% in accuracy and macro F1, provides natural-language justifications, scales better with model size than standard fine-tuning, and achieves SOTA on multiple HTC benchmarks.", "conclusion": "RHC effectively addresses hierarchical text classification challenges through reasoning-based approach, offering improved performance, explainability, and broad applicability beyond patent classification."}}
{"id": "2510.07052", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07052", "abs": "https://arxiv.org/abs/2510.07052", "authors": ["Aryan Golbaghi", "Shuo Zhou"], "title": "Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models and Hyper-Parameter Optimisation", "comment": null, "summary": "We propose a workflow for speech emotion recognition (SER) that combines\npre-trained representations with automated hyperparameter optimisation (HPO).\nUsing SpeechBrain wav2vec2-base model fine-tuned on IEMOCAP as the encoder, we\ncompare two HPO strategies, Gaussian Process Bayesian Optimisation (GP-BO) and\nTree-structured Parzen Estimators (TPE), under an identical four-dimensional\nsearch space and 15-trial budget, with balanced class accuracy (BCA) on the\nGerman EmoDB corpus as the objective. All experiments run on 8 CPU cores with\n32 GB RAM. GP-BO achieves 0.96 BCA in 11 minutes, and TPE (Hyperopt\nimplementation) attains 0.97 in 15 minutes. In contrast, grid search requires\n143 trials and 1,680 minutes to exceed 0.9 BCA, and the best AutoSpeech 2020\nbaseline reports only 0.85 in 30 minutes on GPU. For cross-lingual\ngeneralisation, an EmoDB-trained HPO-tuned model improves zero-shot accuracy by\n0.25 on CREMA-D and 0.26 on RAVDESS. Results show that efficient HPO with\npre-trained encoders delivers competitive SER on commodity CPUs. Source code to\nthis work is available at:\nhttps://github.com/youngaryan/speechbrain-emotion-hpo.", "AI": {"tldr": "A workflow combining pre-trained speech representations with automated hyperparameter optimization (HPO) for speech emotion recognition, achieving high accuracy on commodity CPUs.", "motivation": "To develop an efficient speech emotion recognition system that doesn't require expensive GPU resources and can achieve competitive performance through automated hyperparameter tuning.", "method": "Used SpeechBrain wav2vec2-base model fine-tuned on IEMOCAP as encoder, compared Gaussian Process Bayesian Optimization (GP-BO) and Tree-structured Parzen Estimators (TPE) under 4D search space with 15-trial budget.", "result": "GP-BO achieved 0.96 BCA in 11 minutes, TPE achieved 0.97 in 15 minutes, significantly outperforming grid search (143 trials, 1680 minutes) and AutoSpeech 2020 baseline (0.85 in 30 minutes on GPU). Cross-lingual generalization showed 0.25-0.26 accuracy improvements.", "conclusion": "Efficient HPO with pre-trained encoders enables competitive speech emotion recognition performance on commodity CPUs, offering a practical alternative to GPU-based approaches."}}
{"id": "2510.07169", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07169", "abs": "https://arxiv.org/abs/2510.07169", "authors": ["Yike Zhao", "Simin Guo", "Ziqing Yang", "Shifan Han", "Dahua Lin", "Fei Tan"], "title": "More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning", "comment": "12 pages, 3 figures, submitted to EMNLP 2025 Industry Track", "summary": "The reasoning capabilities of Large Language Models (LLMs) play a critical\nrole in many downstream tasks, yet depend strongly on the quality of training\ndata. Despite various proposed data construction methods, their practical\nutility in real-world pipelines remains underexplored. In this work, we conduct\na comprehensive analysis of open-source datasets and data synthesis techniques\nfor mathematical reasoning, evaluating them under a unified pipeline designed\nto mirror training and deployment scenarios. We further distill effective data\nselection strategies and identify practical methods suitable for industrial\napplications. Our findings highlight that structuring data in more\ninterpretable formats, or distilling from stronger models often outweighs\nsimply scaling up data volume. This study provides actionable guidance for\nintegrating training data to enhance LLM capabilities, supporting both\ncost-effective data curation and scalable model enhancement. We hope this work\nwill inspire further research on how to balance \"more data\" versus \"better\ndata\" for real-world reasoning tasks.", "AI": {"tldr": "Comprehensive analysis of mathematical reasoning datasets and synthesis methods, showing that better data quality and structure outweighs simply scaling data volume.", "motivation": "LLM reasoning capabilities depend heavily on training data quality, but practical utility of various data construction methods in real-world pipelines remains underexplored.", "method": "Conducted unified analysis of open-source datasets and data synthesis techniques for mathematical reasoning, evaluating them under a pipeline mirroring training and deployment scenarios.", "result": "Found that structuring data in interpretable formats and distilling from stronger models often provides better results than simply scaling up data volume.", "conclusion": "Provides actionable guidance for integrating training data to enhance LLM capabilities, supporting cost-effective data curation and scalable model enhancement."}}
{"id": "2510.07053", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO", "I.2.10; I.2.9; I.4.8; I.5.2; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.07053", "abs": "https://arxiv.org/abs/2510.07053", "authors": ["Manshika Charvi Bissessur", "Efimia Panagiotaki", "Daniele De Martini"], "title": "Introspection in Learned Semantic Scene Graph Localisation", "comment": "IEEE IROS 2025 Workshop FAST", "summary": "This work investigates how semantics influence localisation performance and\nrobustness in a learned self-supervised, contrastive semantic localisation\nframework. After training a localisation network on both original and perturbed\nmaps, we conduct a thorough post-hoc introspection analysis to probe whether\nthe model filters environmental noise and prioritises distinctive landmarks\nover routine clutter. We validate various interpretability methods and present\na comparative reliability analysis. Integrated gradients and Attention Weights\nconsistently emerge as the most reliable probes of learned behaviour. A\nsemantic class ablation further reveals an implicit weighting in which frequent\nobjects are often down-weighted. Overall, the results indicate that the model\nlearns noise-robust, semantically salient relations about place definition,\nthereby enabling explainable registration under challenging visual and\nstructural variations.", "AI": {"tldr": "This paper analyzes how semantics affect localization performance and robustness in self-supervised contrastive semantic localization. It examines whether models filter environmental noise and prioritize distinctive landmarks through interpretability methods.", "motivation": "To understand how semantics influence localization performance and robustness in learned self-supervised frameworks, specifically investigating whether models filter noise and prioritize distinctive landmarks.", "method": "Trained localization network on original and perturbed maps, conducted post-hoc introspection analysis using various interpretability methods (integrated gradients, attention weights), performed semantic class ablation study.", "result": "Integrated gradients and attention weights were the most reliable interpretability methods. Semantic class ablation revealed implicit weighting where frequent objects are often down-weighted. Models learn noise-robust, semantically salient relations.", "conclusion": "The model learns noise-robust, semantically salient relations for place definition, enabling explainable registration under challenging visual and structural variations."}}
{"id": "2510.07173", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07173", "abs": "https://arxiv.org/abs/2510.07173", "authors": ["Md Tawkat Islam Khondaker", "Julia Harrington", "Shady Shehata"], "title": "NurseLLM: The First Specialized Language Model for Nursing", "comment": "EMNLP 2025 Industry Track", "summary": "Recent advancements in large language models (LLMs) have significantly\ntransformed medical systems. However, their potential within specialized\ndomains such as nursing remains largely underexplored. In this work, we\nintroduce NurseLLM, the first nursing-specialized LLM tailored for multiple\nchoice question-answering (MCQ) tasks. We develop a multi-stage data generation\npipeline to build the first large scale nursing MCQ dataset to train LLMs on a\nbroad spectrum of nursing topics. We further introduce multiple nursing\nbenchmarks to enable rigorous evaluation. Our extensive experiments demonstrate\nthat NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of\ncomparable size on different benchmarks, underscoring the importance of a\nspecialized LLM for the nursing domain. Finally, we explore the role of\nreasoning and multi-agent collaboration systems in nursing, highlighting their\npromise for future research and applications.", "AI": {"tldr": "NurseLLM is the first nursing-specialized large language model designed for multiple choice question-answering tasks, outperforming comparable general-purpose and medical-specialized LLMs through a multi-stage data generation pipeline and specialized training.", "motivation": "Large language models have transformed medical systems but their potential in specialized domains like nursing remains underexplored, creating a need for domain-specific models.", "method": "Developed a multi-stage data generation pipeline to build the first large-scale nursing MCQ dataset, trained NurseLLM on broad nursing topics, and introduced multiple nursing benchmarks for evaluation.", "result": "NurseLLM outperforms state-of-the-art general-purpose and medical-specialized LLMs of comparable size across different benchmarks, demonstrating the importance of specialized models for nursing.", "conclusion": "Specialized LLMs are crucial for the nursing domain, and reasoning and multi-agent collaboration systems show promise for future nursing research and applications."}}
{"id": "2510.07071", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07071", "abs": "https://arxiv.org/abs/2510.07071", "authors": ["Zheng Xing", "Junting Chen"], "title": "Blind Construction of Angular Power Maps in Massive MIMO Networks", "comment": null, "summary": "Channel state information (CSI) acquisition is a challenging problem in\nmassive multiple-input multiple-output (MIMO) networks. Radio maps provide a\npromising solution for radio resource management by reducing online CSI\nacquisition. However, conventional approaches for radio map construction\nrequire location-labeled CSI data, which is challenging in practice. This paper\ninvestigates unsupervised angular power map construction based on large\ntimescale CSI data collected in a massive MIMO network without location labels.\nA hidden Markov model (HMM) is built to connect the hidden trajectory of a\nmobile with the CSI evolution of a massive MIMO channel. As a result, the\nmobile location can be estimated, enabling the construction of an angular power\nmap. We show that under uniform rectilinear mobility with Poisson-distributed\nbase stations (BSs), the Cramer-Rao Lower Bound (CRLB) for localization error\ncan vanish at any signal-to-noise ratios (SNRs), whereas when BSs are confined\nto a limited region, the error remains nonzero even with infinite independent\nmeasurements. Based on reference signal received power (RSRP) data collected in\na real multi-cell massive MIMO network, an average localization error of 18\nmeters can be achieved although measurements are mainly obtained from a single\nserving cell.", "AI": {"tldr": "This paper proposes an unsupervised method for constructing angular power maps in massive MIMO networks using hidden Markov models to estimate mobile trajectories from CSI data without location labels.", "motivation": "Traditional radio map construction requires location-labeled CSI data, which is difficult to obtain in practice. The paper aims to solve this by developing an unsupervised approach that can work with unlabeled CSI data.", "method": "Uses a hidden Markov model (HMM) to connect mobile trajectory with CSI evolution in massive MIMO channels, enabling location estimation and angular power map construction from timescale CSI data without location labels.", "result": "Theoretical analysis shows localization error can vanish under certain conditions (uniform rectilinear mobility with Poisson-distributed BSs), but remains nonzero when BSs are confined. Real-world RSRP data testing achieved 18-meter average localization error using mainly single-cell measurements.", "conclusion": "The proposed unsupervised HMM-based approach successfully constructs angular power maps from unlabeled CSI data, demonstrating practical feasibility with reasonable localization accuracy in real massive MIMO networks."}}
{"id": "2510.07175", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07175", "abs": "https://arxiv.org/abs/2510.07175", "authors": ["Jongwook Han", "Woojung Song", "Jonggeun Lee", "Yohan Jo"], "title": "Quantifying Data Contamination in Psychometric Evaluations of LLMs", "comment": "12 pages, 1 figure", "summary": "Recent studies apply psychometric questionnaires to Large Language Models\n(LLMs) to assess high-level psychological constructs such as values,\npersonality, moral foundations, and dark traits. Although prior work has raised\nconcerns about possible data contamination from psychometric inventories, which\nmay threaten the reliability of such evaluations, there has been no systematic\nattempt to quantify the extent of this contamination. To address this gap, we\npropose a framework to systematically measure data contamination in\npsychometric evaluations of LLMs, evaluating three aspects: (1) item\nmemorization, (2) evaluation memorization, and (3) target score matching.\nApplying this framework to 21 models from major families and four widely used\npsychometric inventories, we provide evidence that popular inventories such as\nthe Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)\nexhibit strong contamination, where models not only memorize items but can also\nadjust their responses to achieve specific target scores.", "AI": {"tldr": "This paper proposes a framework to systematically measure data contamination in psychometric evaluations of LLMs, finding that popular inventories like BFI-44 and PVQ-40 show strong contamination where models memorize items and can adjust responses to achieve specific scores.", "motivation": "Prior work raised concerns about data contamination from psychometric inventories in LLM evaluations, but there was no systematic attempt to quantify this contamination, which threatens the reliability of psychological assessments of LLMs.", "method": "Proposed a framework to measure three aspects of data contamination: (1) item memorization, (2) evaluation memorization, and (3) target score matching. Applied this framework to 21 models from major families and four widely used psychometric inventories.", "result": "Found that popular inventories such as Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40) exhibit strong contamination, where models not only memorize items but can also adjust their responses to achieve specific target scores.", "conclusion": "The study provides evidence of significant data contamination in psychometric evaluations of LLMs, highlighting the need for more reliable assessment methods that account for this contamination issue."}}
{"id": "2510.07084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07084", "abs": "https://arxiv.org/abs/2510.07084", "authors": ["Tan Wang", "Yun Wei Dong", "Tao Zhang", "Qi Wang"], "title": "HTMformer: Hybrid Time and Multivariate Transformer for Time Series Forecasting", "comment": null, "summary": "Transformer-based methods have achieved impressive results in time series\nforecasting. However, existing Transformers still exhibit limitations in\nsequence modeling as they tend to overemphasize temporal dependencies. This\nincurs additional computational overhead without yielding corresponding\nperformance gains. We find that the performance of Transformers is highly\ndependent on the embedding method used to learn effective representations. To\naddress this issue, we extract multivariate features to augment the effective\ninformation captured in the embedding layer, yielding multidimensional\nembeddings that convey richer and more meaningful sequence representations.\nThese representations enable Transformer-based forecasters to better understand\nthe series. Specifically, we introduce Hybrid Temporal and Multivariate\nEmbeddings (HTME). The HTME extractor integrates a lightweight temporal feature\nextraction module with a carefully designed multivariate feature extraction\nmodule to provide complementary features, thereby achieving a balance between\nmodel complexity and performance. By combining HTME with the Transformer\narchitecture, we present HTMformer, leveraging the enhanced feature extraction\ncapability of the HTME extractor to build a lightweight forecaster. Experiments\nconducted on eight real-world datasets demonstrate that our approach\noutperforms existing baselines in both accuracy and efficiency.", "AI": {"tldr": "The paper proposes HTMformer, a lightweight Transformer-based time series forecasting model that uses Hybrid Temporal and Multivariate Embeddings (HTME) to address limitations in existing Transformers that overemphasize temporal dependencies.", "motivation": "Existing Transformers for time series forecasting tend to overemphasize temporal dependencies, incurring computational overhead without corresponding performance gains. The performance is highly dependent on embedding methods for effective representations.", "method": "Introduces Hybrid Temporal and Multivariate Embeddings (HTME) that extract multivariate features to augment embedding representations. HTME integrates lightweight temporal feature extraction with carefully designed multivariate feature extraction to provide complementary features.", "result": "Experiments on eight real-world datasets show that HTMformer outperforms existing baselines in both accuracy and efficiency.", "conclusion": "HTMformer achieves better performance by leveraging enhanced feature extraction through HTME embeddings, balancing model complexity and performance while providing richer sequence representations."}}
{"id": "2510.07177", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07177", "abs": "https://arxiv.org/abs/2510.07177", "authors": ["Yong-En Tian", "Yu-Chien Tang", "An-Zi Yen", "Wen-Chih Peng"], "title": "CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models", "comment": "22 pages, 17 figures", "summary": "Aspect-based summarization has attracted significant attention for its\nability to generate more fine-grained and user-aligned summaries. While most\nexisting approaches assume a set of predefined aspects as input, real-world\nscenarios often present challenges where these given aspects may be incomplete,\nirrelevant, or entirely missing from the document. Users frequently expect\nsystems to adaptively refine or filter the provided aspects based on the actual\ncontent. In this paper, we initiate this novel task setting, termed\nContent-Aware Refinement of Provided Aspects for Summarization (CARPAS), with\nthe aim of dynamically adjusting the provided aspects based on the document\ncontext before summarizing. We construct three new datasets to facilitate our\npilot experiments, and by using LLMs with four representative prompting\nstrategies in this task, we find that LLMs tend to predict an overly\ncomprehensive set of aspects, which often results in excessively long and\nmisaligned summaries. Building on this observation, we propose a preliminary\nsubtask to predict the number of relevant aspects, and demonstrate that the\npredicted number can serve as effective guidance for the LLMs, reducing the\ninference difficulty, and enabling them to focus on the most pertinent aspects.\nOur extensive experiments show that the proposed approach significantly\nimproves performance across all datasets. Moreover, our deeper analyses uncover\nLLMs' compliance when the requested number of aspects differs from their own\nestimations, establishing a crucial insight for the deployment of LLMs in\nsimilar real-world applications.", "AI": {"tldr": "This paper introduces CARPAS, a new task for content-aware refinement of provided aspects in aspect-based summarization, where LLMs dynamically adjust input aspects based on document context before generating summaries.", "motivation": "Real-world scenarios often present challenges where predefined aspects for summarization may be incomplete, irrelevant, or missing from documents, requiring systems to adaptively refine aspects based on actual content.", "method": "Proposed CARPAS task with three new datasets; used LLMs with four prompting strategies; introduced preliminary subtask to predict number of relevant aspects to guide LLMs and reduce inference difficulty.", "result": "LLMs tend to predict overly comprehensive aspect sets leading to excessively long and misaligned summaries; using predicted number of aspects as guidance significantly improves performance across all datasets.", "conclusion": "The proposed approach effectively addresses aspect refinement challenges in summarization, and analysis reveals LLMs' compliance when requested aspect numbers differ from their estimations, providing crucial insights for real-world LLM deployment."}}
{"id": "2510.07086", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07086", "abs": "https://arxiv.org/abs/2510.07086", "authors": ["Shinsaku Sakaue", "Han Bao", "Yuzhou Cao"], "title": "Non-Stationary Online Structured Prediction with Surrogate Losses", "comment": null, "summary": "Online structured prediction, including online classification as a special\ncase, is the task of sequentially predicting labels from input features.\nTherein the surrogate regret -- the cumulative excess of the target loss (e.g.,\n0-1 loss) over the surrogate loss (e.g., logistic loss) of the fixed best\nestimator -- has gained attention, particularly because it often admits a\nfinite bound independent of the time horizon $T$. However, such guarantees\nbreak down in non-stationary environments, where every fixed estimator may\nincur the surrogate loss growing linearly with $T$. We address this by proving\na bound of the form $F_T + C(1 + P_T)$ on the cumulative target loss, where\n$F_T$ is the cumulative surrogate loss of any comparator sequence, $P_T$ is its\npath length, and $C > 0$ is some constant. This bound depends on $T$ only\nthrough $F_T$ and $P_T$, often yielding much stronger guarantees in\nnon-stationary environments. Our core idea is to synthesize the dynamic regret\nbound of the online gradient descent (OGD) with the technique of exploiting the\nsurrogate gap. Our analysis also sheds light on a new Polyak-style learning\nrate for OGD, which systematically offers target-loss guarantees and exhibits\npromising empirical performance. We further extend our approach to a broader\nclass of problems via the convolutional Fenchel--Young loss. Finally, we prove\na lower bound showing that the dependence on $F_T$ and $P_T$ is tight.", "AI": {"tldr": "This paper addresses online structured prediction in non-stationary environments by proving a novel bound on cumulative target loss that depends on surrogate loss and path length of comparator sequences, rather than time horizon T.", "motivation": "Existing surrogate regret bounds for online prediction break down in non-stationary environments where fixed estimators suffer linear growth in surrogate loss with time T. This work aims to provide stronger guarantees that adapt to non-stationarity.", "method": "The core approach synthesizes the dynamic regret bound of online gradient descent (OGD) with techniques for exploiting the surrogate gap. The paper also introduces a new Polyak-style learning rate for OGD and extends the method to broader problems using convolutional Fenchel-Young loss.", "result": "The paper proves a bound of form F_T + C(1 + P_T) on cumulative target loss, where F_T is cumulative surrogate loss of comparator sequences, P_T is their path length, and C is constant. This provides much stronger guarantees in non-stationary environments.", "conclusion": "The proposed approach successfully handles non-stationary online prediction by providing bounds that depend on comparator performance rather than time horizon. A lower bound confirms the tightness of the dependence on F_T and P_T."}}
{"id": "2510.07178", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07178", "abs": "https://arxiv.org/abs/2510.07178", "authors": ["Imry Ziv", "Nur Lan", "Emmanuel Chemla", "Roni Katzir"], "title": "Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible", "comment": "15 pages, 4 figures", "summary": "Are large language models (LLMs) sensitive to the distinction between humanly\npossible languages and humanly impossible languages? This question is taken by\nmany to bear on whether LLMs and humans share the same innate learning biases.\nPrevious work has attempted to answer it in the positive by comparing LLM\nlearning curves on existing language datasets and on \"impossible\" datasets\nderived from them via various perturbation functions. Using the same\nmethodology, we examine this claim on a wider set of languages and impossible\nperturbations. We find that in most cases, GPT-2 learns each language and its\nimpossible counterpart equally easily, in contrast to previous claims. We also\napply a more lenient condition by testing whether GPT-2 provides any kind of\nseparation between the whole set of natural languages and the whole set of\nimpossible languages. By considering cross-linguistic variance in various\nmetrics computed on the perplexity curves, we show that GPT-2 provides no\nsystematic separation between the possible and the impossible. Taken together,\nthese perspectives show that LLMs do not share the human innate biases that\nshape linguistic typology.", "AI": {"tldr": "LLMs like GPT-2 don't distinguish between humanly possible and impossible languages, showing they lack human innate linguistic biases.", "motivation": "To test whether LLMs share human innate learning biases by examining if they can distinguish between possible and impossible human languages.", "method": "Compared GPT-2 learning curves on natural languages and impossible counterparts created via perturbations, analyzed cross-linguistic variance in perplexity metrics.", "result": "GPT-2 learns natural and impossible languages equally easily, showing no systematic separation between possible and impossible language sets.", "conclusion": "LLMs do not possess the same innate linguistic biases that shape human language acquisition and typology."}}
{"id": "2510.07092", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07092", "abs": "https://arxiv.org/abs/2510.07092", "authors": ["Riccardo Mereu", "Aidan Scannell", "Yuxin Hou", "Yi Zhao", "Aditya Jitta", "Antonio Dominguez", "Luigi Acerbi", "Amos Storkey", "Paul Chang"], "title": "Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report", "comment": "6 pages, 3 figures, 1X world model challenge technical report", "summary": "World models are a powerful paradigm in AI and robotics, enabling agents to\nreason about the future by predicting visual observations or compact latent\nstates. The 1X World Model Challenge introduces an open-source benchmark of\nreal-world humanoid interaction, with two complementary tracks: sampling,\nfocused on forecasting future image frames, and compression, focused on\npredicting future discrete latent codes. For the sampling track, we adapt the\nvideo generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned\nfuture frame prediction. We condition the video generation on robot states\nusing AdaLN-Zero, and further post-train the model using LoRA. For the\ncompression track, we train a Spatio-Temporal Transformer model from scratch.\nOur models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386\nin the compression task, securing 1st place in both challenges.", "AI": {"tldr": "The paper presents winning solutions for the 1X World Model Challenge, achieving 1st place in both sampling and compression tracks by adapting video generation models and training transformers for humanoid interaction tasks.", "motivation": "To address the challenge of world modeling in real-world humanoid interaction, which requires forecasting future visual observations or compact latent states for AI and robotics applications.", "method": "For sampling track: adapted Wan-2.2 TI2V-5B video generation model with robot state conditioning using AdaLN-Zero and LoRA post-training. For compression track: trained Spatio-Temporal Transformer from scratch.", "result": "Achieved 23.0 dB PSNR in sampling task and Top-500 CE of 6.6386 in compression task, securing 1st place in both challenges.", "conclusion": "The proposed approaches successfully demonstrate effective world modeling for real-world humanoid interaction, with adapted foundation models and custom transformers achieving state-of-the-art performance in both visual forecasting and latent state prediction tasks."}}
{"id": "2510.07203", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07203", "abs": "https://arxiv.org/abs/2510.07203", "authors": ["Benjamin Akera", "Evelyn Nafula Ouma", "Gilbert Yiga", "Patrick Walukagga", "Phionah Natukunda", "Trevor Saaka", "Solomon Nsumba", "Lilian Teddy Nabukeera", "Joel Muhanguzi", "Imran Sekalala", "Nimpamya Janat Namara", "Engineer Bainomugisha", "Ernest Mwebaze", "John Quinn"], "title": "Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models", "comment": null, "summary": "There are more than 2000 living languages in Africa, most of which have been\nbypassed by advances in language technology. Current leading LLMs exhibit\nstrong performance on a number of the most common languages (e.g. Swahili or\nYoruba), but prioritise support for the languages with the most speakers first,\nresulting in piecemeal ability across disparate languages. We contend that a\nregionally focussed approach is more efficient, and present a case study for\nUganda, a country with high linguistic diversity. We describe the development\nof Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the\nart comprehension in the majority of all Ugandan languages. These models are\nopen source and can be used to reduce language barriers in a number of\nimportant practical applications.", "AI": {"tldr": "Sunflower models (14B and 32B) provide state-of-the-art language comprehension for Ugandan languages, addressing the gap in language technology support for Africa's diverse linguistic landscape through a regionally focused approach.", "motivation": "Most African languages (over 2000) lack adequate language technology support, with current LLMs prioritizing only the most common languages like Swahili and Yoruba, leaving many languages underserved.", "method": "Developed Sunflower 14B and 32B models based on Qwen 3, adopting a regionally focused approach specifically for Uganda's high linguistic diversity rather than piecemeal language support.", "result": "Created open source models with state-of-the-art comprehension capabilities for the majority of Ugandan languages.", "conclusion": "Regionally focused language models like Sunflower can effectively reduce language barriers and provide practical applications for linguistically diverse regions in Africa."}}
{"id": "2510.07093", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.07093", "abs": "https://arxiv.org/abs/2510.07093", "authors": ["Yunzhen Yao", "Lie He", "Michael Gastpar"], "title": "Non-Asymptotic Analysis of Efficiency in Conformalized Regression", "comment": null, "summary": "Conformal prediction provides prediction sets with coverage guarantees. The\ninformativeness of conformal prediction depends on its efficiency, typically\nquantified by the expected size of the prediction set. Prior work on the\nefficiency of conformalized regression commonly treats the miscoverage level\n$\\alpha$ as a fixed constant. In this work, we establish non-asymptotic bounds\non the deviation of the prediction set length from the oracle interval length\nfor conformalized quantile and median regression trained via SGD, under mild\nassumptions on the data distribution. Our bounds of order\n$\\mathcal{O}(1/\\sqrt{n} + 1/(\\alpha^2 n) + 1/\\sqrt{m} + \\exp(-\\alpha^2 m))$\ncapture the joint dependence of efficiency on the proper training set size $n$,\nthe calibration set size $m$, and the miscoverage level $\\alpha$. The results\nidentify phase transitions in convergence rates across different regimes of\n$\\alpha$, offering guidance for allocating data to control excess prediction\nset length. Empirical results are consistent with our theoretical findings.", "AI": {"tldr": "This paper establishes non-asymptotic bounds on prediction set length deviation for conformalized quantile and median regression, capturing joint dependence on training size, calibration size, and miscoverage level.", "motivation": "Prior work treats miscoverage level as fixed constant, but this work aims to understand how efficiency depends jointly on dataset sizes and miscoverage level to guide data allocation.", "method": "Analyze conformalized quantile and median regression trained via SGD, establish theoretical bounds under mild data distribution assumptions.", "result": "Derived bounds of order O(1/\u221an + 1/(\u03b1\u00b2n) + 1/\u221am + exp(-\u03b1\u00b2m)) showing phase transitions in convergence rates across different \u03b1 regimes.", "conclusion": "Theoretical findings provide guidance for data allocation to control excess prediction set length, with empirical results supporting the analysis."}}
{"id": "2510.07213", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07213", "abs": "https://arxiv.org/abs/2510.07213", "authors": ["Chengzhi Zhong", "Fei Cheng", "Qianying Liu", "Yugo Murawaki", "Chenhui Chu", "Sadao Kurohashi"], "title": "Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models", "comment": "Work in progress. Our code will be available at:\n  https://github.com/ku-nlp/language-specific-dimensions", "summary": "Large language models exhibit strong multilingual capabilities despite\nlimited exposure to non-English data. Prior studies show that English-centric\nlarge language models map multilingual content into English-aligned\nrepresentations at intermediate layers and then project them back into\ntarget-language token spaces in the final layer. From this observation, we\nhypothesize that this cross-lingual transition is governed by a small and\nsparse set of dimensions, which occur at consistent indices across the\nintermediate to final layers. Building on this insight, we introduce a simple,\ntraining-free method to identify and manipulate these dimensions, requiring\nonly as few as 50 sentences of either parallel or monolingual data. Experiments\non a multilingual generation control task reveal the interpretability of these\ndimensions, demonstrating that the interventions in these dimensions can switch\nthe output language while preserving semantic content, and that it surpasses\nthe performance of prior neuron-based approaches at a substantially lower cost.", "AI": {"tldr": "The paper identifies sparse cross-lingual dimensions in LLMs that control language switching, and introduces a training-free method to manipulate them for multilingual generation control.", "motivation": "Large language models show strong multilingual capabilities despite limited non-English training data, suggesting they use English-aligned representations as an intermediate step for cross-lingual processing.", "method": "A simple training-free method that identifies sparse cross-lingual dimensions in intermediate-to-final layers using only 50 sentences of parallel or monolingual data, then manipulates these dimensions to control output language.", "result": "The method successfully switches output language while preserving semantic content, outperforms prior neuron-based approaches, and operates at substantially lower computational cost.", "conclusion": "Cross-lingual transitions in LLMs are governed by sparse, interpretable dimensions that can be efficiently manipulated for controlled multilingual generation without additional training."}}
{"id": "2510.07132", "categories": ["cs.LG", "cs.DC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.07132", "abs": "https://arxiv.org/abs/2510.07132", "authors": ["Mariona Jaramillo-Civill", "Peng Wu", "Pau Closas"], "title": "DPMM-CFL: Clustered Federated Learning via Dirichlet Process Mixture Model Nonparametric Clustering", "comment": "5 pages, 2 figures", "summary": "Clustered Federated Learning (CFL) improves performance under non-IID client\nheterogeneity by clustering clients and training one model per cluster, thereby\nbalancing between a global model and fully personalized models. However, most\nCFL methods require the number of clusters K to be fixed a priori, which is\nimpractical when the latent structure is unknown. We propose DPMM-CFL, a CFL\nalgorithm that places a Dirichlet Process (DP) prior over the distribution of\ncluster parameters. This enables nonparametric Bayesian inference to jointly\ninfer both the number of clusters and client assignments, while optimizing\nper-cluster federated objectives. This results in a method where, at each\nround, federated updates and cluster inferences are coupled, as presented in\nthis paper. The algorithm is validated on benchmark datasets under Dirichlet\nand class-split non-IID partitions.", "AI": {"tldr": "DPMM-CFL is a Clustered Federated Learning method that automatically determines the number of clusters using Dirichlet Process priors, eliminating the need to pre-specify cluster count.", "motivation": "Existing CFL methods require fixing the number of clusters K beforehand, which is impractical when the latent cluster structure is unknown.", "method": "Places Dirichlet Process prior over cluster parameters and uses nonparametric Bayesian inference to jointly infer cluster count and client assignments while optimizing federated objectives.", "result": "Validated on benchmark datasets under Dirichlet and class-split non-IID partitions, showing effective automatic cluster discovery.", "conclusion": "DPMM-CFL provides a practical CFL solution that automatically adapts to unknown cluster structures through coupled federated updates and cluster inference."}}
{"id": "2510.07221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07221", "abs": "https://arxiv.org/abs/2510.07221", "authors": ["Benjamin Akera", "Evelyn Nafula", "Patrick Walukagga", "Gilbert Yiga", "John Quinn", "Ernest Mwebaze"], "title": "How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu", "comment": null, "summary": "The development of Automatic Speech Recognition (ASR) systems for\nlow-resource African languages remains challenging due to limited transcribed\nspeech data. While recent advances in large multilingual models like OpenAI's\nWhisper offer promising pathways for low-resource ASR development, critical\nquestions persist regarding practical deployment requirements. This paper\naddresses two fundamental concerns for practitioners: determining the minimum\ndata volumes needed for viable performance and characterizing the primary\nfailure modes that emerge in production systems. We evaluate Whisper's\nperformance through comprehensive experiments on two Bantu languages:\nsystematic data scaling analysis on Kinyarwanda using training sets from 1 to\n1,400 hours, and detailed error characterization on Kikuyu using 270 hours of\ntraining data. Our scaling experiments demonstrate that practical ASR\nperformance (WER < 13\\%) becomes achievable with as little as 50 hours of\ntraining data, with substantial improvements continuing through 200 hours (WER\n< 10\\%). Complementing these volume-focused findings, our error analysis\nreveals that data quality issues, particularly noisy ground truth\ntranscriptions, account for 38.6\\% of high-error cases, indicating that careful\ndata curation is as critical as data volume for robust system performance.\nThese results provide actionable benchmarks and deployment guidance for teams\ndeveloping ASR systems across similar low-resource language contexts. We\nrelease accompanying and models see\nhttps://github.com/SunbirdAI/kinyarwanda-whisper-eval", "AI": {"tldr": "This paper provides practical guidelines for deploying Whisper ASR systems in low-resource African languages, finding that 50 hours of training data achieves viable performance (WER < 13%) and 200 hours reaches <10% WER, with data quality issues being a major failure mode.", "motivation": "To address the challenges of developing ASR systems for low-resource African languages by determining minimum data requirements and identifying key failure modes for practical deployment.", "method": "Comprehensive experiments on two Bantu languages: systematic data scaling analysis on Kinyarwanda (1-1,400 hours training data) and detailed error characterization on Kikuyu (270 hours training data).", "result": "Practical ASR performance (WER < 13%) achieved with 50 hours of data, with continued improvement to <10% WER at 200 hours. Error analysis showed 38.6% of high-error cases due to noisy ground truth transcriptions.", "conclusion": "Data curation is as critical as data volume for robust ASR performance in low-resource contexts, providing actionable benchmarks for similar language deployments."}}
{"id": "2510.07147", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07147", "abs": "https://arxiv.org/abs/2510.07147", "authors": ["Arshika Lalan", "Rajat Ghosh", "Aditya Kolsur", "Debojyoti Dutta"], "title": "A Multi-Agent Framework for Stateful Inference-Time Search", "comment": null, "summary": "Recent work explores agentic inference-time techniques to perform structured,\nmulti-step reasoning. However, stateless inference often struggles on\nmulti-step tasks due to the absence of persistent state. Moreover,\ntask-specific fine-tuning or instruction-tuning often achieve surface-level\ncode generation but remain brittle on tasks requiring deeper reasoning and\nlong-horizon dependencies. To address these limitations, we propose stateful\nmulti-agent evolutionary search, a training-free framework that departs from\nprior stateless approaches by combining (i) persistent inference-time state,\n(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate\nits effectiveness in automated unit test generation through the generation of\nedge cases. We generate robust edge cases using an evolutionary search process,\nwhere specialized agents sequentially propose, mutate, and score candidates. A\ncontroller maintains persistent state across generations, while evolutionary\npreservation ensures diversity and exploration across all possible cases. This\nyields a generalist agent capable of discovering robust, high-coverage edge\ncases across unseen codebases. Experiments show our stateful multi-agent\ninference framework achieves substantial gains in coverage over stateless\nsingle-step baselines, evaluated on prevalent unit-testing benchmarks such as\nHumanEval and TestGenEvalMini and using three diverse LLM families - Llama,\nGemma, and GPT. These results indicate that combining persistent inference-time\nstate with evolutionary search materially improves unit-test generation.", "AI": {"tldr": "Stateful multi-agent evolutionary search framework improves unit test generation by combining persistent inference-time state, adversarial mutation, and evolutionary preservation to generate robust edge cases.", "motivation": "Address limitations of stateless inference-time techniques that struggle with multi-step reasoning tasks and brittleness of task-specific fine-tuning on long-horizon dependencies.", "method": "Training-free framework using specialized agents for proposing, mutating, and scoring test candidates, with controller maintaining persistent state across generations and evolutionary preservation ensuring diversity.", "result": "Achieves substantial gains in coverage over stateless single-step baselines on HumanEval and TestGenEvalMini benchmarks using Llama, Gemma, and GPT models.", "conclusion": "Combining persistent inference-time state with evolutionary search materially improves unit-test generation, yielding generalist agents capable of discovering robust edge cases across unseen codebases."}}
{"id": "2510.07227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07227", "abs": "https://arxiv.org/abs/2510.07227", "authors": ["Arjun Krishnakumar", "Rhea Sanjay Sukthanker", "Hannan Javed Mahadik", "Gabriela Kadlecov\u00e1", "Vladyslav Moroshan", "Timur Carstensen", "Frank Hutter", "Aaron Klein"], "title": "Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation", "comment": null, "summary": "Small Language models (SLMs) offer an efficient and accessible alternative to\nLarge Language Models (LLMs), delivering strong performance while using far\nfewer resources. We introduce a simple and effective framework for pretraining\nSLMs that brings together three complementary ideas. First, we identify\nstructurally sparse sub-network initializations that consistently outperform\nrandomly initialized models of similar size under the same compute budget.\nSecond, we use evolutionary search to automatically discover high-quality\nsub-network initializations, providing better starting points for pretraining.\nThird, we apply knowledge distillation from larger teacher models to speed up\ntraining and improve generalization. Together, these components make SLM\npretraining substantially more efficient: our best model, discovered using\nevolutionary search and initialized with LLM weights, matches the validation\nperplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining\ntokens. We release all code and models at\nhttps://github.com/whittle-org/whittle/, offering a practical and reproducible\npath toward cost-efficient small language model development at scale.", "AI": {"tldr": "A framework for efficient pretraining of Small Language Models (SLMs) using sparse sub-network initialization, evolutionary search, and knowledge distillation, achieving 9.2x fewer pretraining tokens while matching performance.", "motivation": "To make SLMs more efficient and accessible as alternatives to LLMs by reducing resource requirements while maintaining strong performance.", "method": "Three-component framework: 1) Structurally sparse sub-network initialization, 2) Evolutionary search for optimal initializations, 3) Knowledge distillation from larger teacher models.", "result": "Best model matches validation perplexity of comparable Pythia SLM with 9.2x fewer pretraining tokens.", "conclusion": "Provides a practical and reproducible path for cost-efficient SLM development at scale, with all code and models publicly available."}}
{"id": "2510.07151", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07151", "abs": "https://arxiv.org/abs/2510.07151", "authors": ["Egor Cherepanov", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL", "comment": "22 pages, 7 figures", "summary": "Real-world robotic agents must act under partial observability and long\nhorizons, where key cues may appear long before they affect decision making.\nHowever, most modern approaches rely solely on instantaneous information,\nwithout incorporating insights from the past. Standard recurrent or transformer\nmodels struggle with retaining and leveraging long-term dependencies: context\nwindows truncate history, while naive memory extensions fail under scale and\nsparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a\ntransformer architecture with structured external memory. Each layer maintains\nmemory embeddings, interacts with them via bidirectional cross-attention, and\nupdates them through an Least Recently Used (LRU) memory module using\nreplacement or convex blending. ELMUR extends effective horizons up to 100,000\ntimes beyond the attention window and achieves a 100% success rate on a\nsynthetic T-Maze task with corridors up to one million steps. In POPGym, it\noutperforms baselines on more than half of the tasks. On MIKASA-Robo\nsparse-reward manipulation tasks with visual observations, it nearly doubles\nthe performance of strong baselines. These results demonstrate that structured,\nlayer-local external memory offers a simple and scalable approach to decision\nmaking under partial observability.", "AI": {"tldr": "ELMUR introduces a transformer architecture with structured external memory that extends effective horizons up to 100,000x beyond attention windows, achieving strong performance on long-horizon partially observable tasks.", "motivation": "Real-world robotic agents need to handle partial observability and long horizons where key cues appear long before decision making, but current approaches struggle with retaining and leveraging long-term dependencies.", "method": "ELMUR uses transformer architecture with structured external memory where each layer maintains memory embeddings, interacts via bidirectional cross-attention, and updates through LRU memory module using replacement or convex blending.", "result": "Achieved 100% success rate on synthetic T-Maze task with million-step corridors, outperformed baselines on more than half of POPGym tasks, and nearly doubled performance on MIKASA-Robo sparse-reward manipulation tasks.", "conclusion": "Structured, layer-local external memory provides a simple and scalable approach for decision making under partial observability, effectively extending horizons beyond standard attention windows."}}
{"id": "2510.07230", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07230", "abs": "https://arxiv.org/abs/2510.07230", "authors": ["Ziyi Wang", "Yuxuan Lu", "Yimeng Zhang", "Jing Huang", "Dakuo Wang"], "title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping", "comment": null, "summary": "Simulating step-wise human behavior with Large Language Models (LLMs) has\nbecome an emerging research direction, enabling applications in various\npractical domains. While prior methods, including prompting, supervised\nfine-tuning (SFT), and reinforcement learning (RL), have shown promise in\nmodeling step-wise behavior, they primarily learn a population-level policy\nwithout conditioning on a user's persona, yielding generic rather than\npersonalized simulations. In this work, we pose a critical question: how can\nLLM agents better simulate personalized user behavior? We introduce\nCustomer-R1, an RL-based method for personalized, step-wise user behavior\nsimulation in online shopping environments. Our policy is conditioned on an\nexplicit persona, and we optimize next-step rationale and action generation via\naction correctness reward signals. Experiments on the OPeRA dataset emonstrate\nthat Customer-R1 not only significantly outperforms prompting and SFT-based\nbaselines in next-action prediction tasks, but also better matches users'\naction distribution, indicating higher fidelity in personalized behavior\nsimulation.", "AI": {"tldr": "Customer-R1 is an RL-based method that enables personalized user behavior simulation in online shopping by conditioning on explicit user personas and optimizing next-step rationale and action generation.", "motivation": "Prior methods for simulating step-wise human behavior with LLMs learn population-level policies without user persona conditioning, resulting in generic rather than personalized simulations.", "method": "Customer-R1 uses reinforcement learning with action correctness reward signals to optimize next-step rationale and action generation, conditioned on explicit user personas.", "result": "Experiments on OPeRA dataset show Customer-R1 significantly outperforms prompting and SFT baselines in next-action prediction and better matches users' action distribution.", "conclusion": "Customer-R1 enables higher fidelity personalized behavior simulation by incorporating explicit persona conditioning and RL-based optimization."}}
{"id": "2510.07182", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07182", "abs": "https://arxiv.org/abs/2510.07182", "authors": ["Patrick Peixuan Ye", "Chen Shani", "Ellen Vitercik"], "title": "Bridged Clustering for Representation Learning: Semi-Supervised Sparse Bridging", "comment": null, "summary": "We introduce Bridged Clustering, a semi-supervised framework to learn\npredictors from any unpaired input $X$ and output $Y$ dataset. Our method first\nclusters $X$ and $Y$ independently, then learns a sparse, interpretable bridge\nbetween clusters using only a few paired examples. At inference, a new input\n$x$ is assigned to its nearest input cluster, and the centroid of the linked\noutput cluster is returned as the prediction $\\hat{y}$. Unlike traditional SSL,\nBridged Clustering explicitly leverages output-only data, and unlike dense\ntransport-based methods, it maintains a sparse and interpretable alignment.\nThrough theoretical analysis, we show that with bounded mis-clustering and\nmis-bridging rates, our algorithm becomes an effective and efficient predictor.\nEmpirically, our method is competitive with SOTA methods while remaining\nsimple, model-agnostic, and highly label-efficient in low-supervision settings.", "AI": {"tldr": "Bridged Clustering is a semi-supervised framework that learns predictors from unpaired X and Y datasets by clustering them independently and learning a sparse bridge between clusters using few paired examples.", "motivation": "Traditional SSL doesn't leverage output-only data, and dense transport-based methods lack interpretability. This method aims to use output data while maintaining sparse, interpretable alignments.", "method": "1) Cluster X and Y independently 2) Learn sparse bridge between clusters using few paired examples 3) At inference, assign input to nearest cluster and return linked output cluster centroid as prediction", "result": "Theoretical analysis shows effectiveness with bounded mis-clustering rates. Empirically competitive with SOTA methods while being simple, model-agnostic, and highly label-efficient in low-supervision settings.", "conclusion": "Bridged Clustering provides an effective, efficient, and interpretable semi-supervised learning approach that explicitly leverages output-only data and maintains sparse alignments."}}
{"id": "2510.07231", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07231", "abs": "https://arxiv.org/abs/2510.07231", "authors": ["Donggyu Lee", "Sungwon Park", "Yerin Hwang", "Hyunwoo Oh", "Hyoshin Kim", "Jungwon Kim", "Meeyoung Cha", "Sangyoon Park", "Jihee Kim"], "title": "Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships", "comment": null, "summary": "Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications.", "AI": {"tldr": "A new causal reasoning benchmark for LLMs using real-world economic/finance research data, revealing significant performance gaps (best model: 57.6% accuracy) despite model scaling.", "motivation": "Existing causal reasoning benchmarks have limitations like synthetic data and narrow domains, failing to assess LLMs' genuine understanding of cause-effect relationships needed for high-stakes applications.", "method": "Constructed benchmark from causal relationships in top economics/finance journals using rigorous methodologies (instrumental variables, difference-in-differences, regression discontinuity), covering 40,379 items across 5 task types in diverse domains.", "result": "Tested 8 state-of-the-art LLMs showing substantial limitations - best model achieved only 57.6% accuracy. Model scale doesn't consistently improve performance, and advanced reasoning models struggle with basic causal identification.", "conclusion": "There's a critical gap between current LLM capabilities and reliable causal reasoning demands for high-stakes applications, highlighting the need for improved causal understanding beyond pattern matching."}}
{"id": "2510.07192", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07192", "abs": "https://arxiv.org/abs/2510.07192", "authors": ["Alexandra Souly", "Javier Rando", "Ed Chapman", "Xander Davies", "Burak Hasircioglu", "Ezzeldin Shereen", "Carlos Mougan", "Vasilios Mavroudis", "Erik Jones", "Chris Hicks", "Nicholas Carlini", "Yarin Gal", "Robert Kirk"], "title": "Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples", "comment": null, "summary": "Poisoning attacks can compromise the safety of large language models (LLMs)\nby injecting malicious documents into their training data. Existing work has\nstudied pretraining poisoning assuming adversaries control a percentage of the\ntraining corpus. However, for large models, even small percentages translate to\nimpractically large amounts of data. This work demonstrates for the first time\nthat poisoning attacks instead require a near-constant number of documents\nregardless of dataset size. We conduct the largest pretraining poisoning\nexperiments to date, pretraining models from 600M to 13B parameters on\nchinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned\ndocuments similarly compromise models across all model and dataset sizes,\ndespite the largest models training on more than 20 times more clean data. We\nalso run smaller-scale experiments to ablate factors that could influence\nattack success, including broader ratios of poisoned to clean data and\nnon-random distributions of poisoned samples. Finally, we demonstrate the same\ndynamics for poisoning during fine-tuning. Altogether, our results suggest that\ninjecting backdoors through data poisoning may be easier for large models than\npreviously believed as the number of poisons required does not scale up with\nmodel size, highlighting the need for more research on defences to mitigate\nthis risk in future models.", "AI": {"tldr": "Poisoning attacks on LLMs require only a constant number of malicious documents (~250) regardless of model size or dataset scale, making backdoor injection easier than previously thought.", "motivation": "Existing poisoning research assumes adversaries control a percentage of training data, but for large models this translates to impractically large amounts of data. This work challenges that assumption.", "method": "Conducted largest pretraining poisoning experiments to date, training models from 600M to 13B parameters on chinchilla-optimal datasets (6B to 260B tokens). Also ran smaller-scale experiments varying poisoned-to-clean data ratios and poison distribution patterns.", "result": "250 poisoned documents similarly compromised models across all sizes, despite largest models training on 20x more clean data. Same dynamics observed for fine-tuning poisoning.", "conclusion": "Backdoor injection through data poisoning may be easier for large models than believed, as poison count doesn't scale with model size, highlighting urgent need for better defenses."}}
{"id": "2510.07233", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07233", "abs": "https://arxiv.org/abs/2510.07233", "authors": ["Zhivar Sourati", "Zheng Wang", "Marianne Menglin Liu", "Yazhe Hu", "Mengqing Guo", "Sujeeth Bharadwaj", "Kyu Han", "Tao Sheng", "Sujith Ravi", "Morteza Dehghani", "Dan Roth"], "title": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding", "comment": null, "summary": "Question answering over visually rich documents (VRDs) requires reasoning not\nonly over isolated content but also over documents' structural organization and\ncross-page dependencies. However, conventional retrieval-augmented generation\n(RAG) methods encode content in isolated chunks during ingestion, losing\nstructural and cross-page dependencies, and retrieve a fixed number of pages at\ninference, regardless of the specific demands of the question or context. This\noften results in incomplete evidence retrieval and degraded answer quality for\nmulti-page reasoning tasks. To address these limitations, we propose LAD-RAG, a\nnovel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs\na symbolic document graph that captures layout structure and cross-page\ndependencies, adding it alongside standard neural embeddings to yield a more\nholistic representation of the document. During inference, an LLM agent\ndynamically interacts with the neural and symbolic indices to adaptively\nretrieve the necessary evidence based on the query. Experiments on\nMMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG\nimproves retrieval, achieving over 90% perfect recall on average without any\ntop-k tuning, and outperforming baseline retrievers by up to 20% in recall at\ncomparable noise levels, yielding higher QA accuracy with minimal latency.", "AI": {"tldr": "LAD-RAG is a Layout-Aware Dynamic RAG framework that improves question answering over visually rich documents by capturing structural layout and cross-page dependencies through symbolic document graphs, enabling adaptive evidence retrieval.", "motivation": "Conventional RAG methods lose structural and cross-page dependencies by encoding content in isolated chunks, leading to incomplete evidence retrieval and degraded answer quality for multi-page reasoning tasks.", "method": "Constructs symbolic document graphs during ingestion to capture layout structure and cross-page dependencies, and uses an LLM agent during inference to dynamically interact with neural and symbolic indices for adaptive evidence retrieval.", "result": "Achieves over 90% perfect recall on average without top-k tuning, outperforms baseline retrievers by up to 20% in recall at comparable noise levels, and yields higher QA accuracy with minimal latency.", "conclusion": "LAD-RAG effectively addresses structural dependency loss in conventional RAG methods and significantly improves retrieval performance and answer quality for multi-page document reasoning tasks."}}
{"id": "2510.07202", "categories": ["cs.LG", "68T07, 41A30", "I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2510.07202", "abs": "https://arxiv.org/abs/2510.07202", "authors": ["Joris Dommel", "Sven A. Wegner"], "title": "An in-depth look at approximation via deep and narrow neural networks", "comment": "11 pages", "summary": "In 2017, Hanin and Sellke showed that the class of arbitrarily deep,\nreal-valued, feed-forward and ReLU-activated networks of width w forms a dense\nsubset of the space of continuous functions on R^n, with respect to the\ntopology of uniform convergence on compact sets, if and only if w>n holds. To\nshow the necessity, a concrete counterexample function f:R^n->R was used. In\nthis note we actually approximate this very f by neural networks in the two\ncases w=n and w=n+1 around the aforementioned threshold. We study how the\napproximation quality behaves if we vary the depth and what effect (spoiler\nalert: dying neurons) cause that behavior.", "AI": {"tldr": "This paper studies the approximation of a specific counterexample function by ReLU neural networks at the critical width thresholds w=n and w=n+1, analyzing how depth affects approximation quality and identifying dying neurons as a key factor.", "motivation": "The motivation is to investigate the boundary case of neural network width requirements for universal approximation, specifically examining what happens at the exact threshold w=n and just above it at w=n+1, using a known counterexample function.", "method": "The authors approximate the counterexample function f:R^n->R using ReLU neural networks with widths w=n and w=n+1, varying the network depth and analyzing the resulting approximation behavior.", "result": "The study reveals that approximation quality changes with depth and identifies \"dying neurons\" as the phenomenon causing this behavior, providing insights into network performance at critical width thresholds.", "conclusion": "The research demonstrates that even at the theoretical threshold w=n and slightly above it at w=n+1, neural networks can approximate the counterexample function, with depth playing a crucial role and dying neurons significantly impacting the approximation quality."}}
{"id": "2510.07238", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07238", "abs": "https://arxiv.org/abs/2510.07238", "authors": ["Xunyi Jiang", "Dingyi Chang", "Julian McAuley", "Xin Xu"], "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation", "comment": null, "summary": "The rapid evolution of large language models (LLMs) and the real world has\noutpaced the static nature of widely used evaluation benchmarks, raising\nconcerns about their reliability for evaluating LLM factuality. While\nsubstantial works continue to rely on the popular but old benchmarks, their\ntemporal misalignment with real-world facts and modern LLMs, and their effects\non LLM factuality evaluation remain underexplored. Therefore, in this work, we\npresent a systematic investigation of this issue by examining five popular\nfactuality benchmarks and eight LLMs released across different years. An\nup-to-date fact retrieval pipeline and three metrics are tailored to quantify\nbenchmark aging and its impact on LLM factuality evaluation. Experimental\nresults and analysis illustrate that a considerable portion of samples in the\nwidely used factuality benchmarks are outdated, leading to unreliable\nassessments of LLM factuality. We hope our work can provide a testbed to assess\nthe reliability of a benchmark for LLM factuality evaluation and inspire more\nresearch on the benchmark aging issue. Codes are available in\nhttps://github.com/JiangXunyi/BenchAge.", "AI": {"tldr": "This paper investigates the aging problem of popular factuality benchmarks for evaluating large language models (LLMs), finding that outdated samples lead to unreliable assessments of LLM factuality.", "motivation": "The rapid evolution of LLMs and real-world facts has outpaced static evaluation benchmarks, raising concerns about their reliability for assessing LLM factuality, yet this temporal misalignment remains underexplored.", "method": "Systematic investigation using five popular factuality benchmarks and eight LLMs across different years, employing an up-to-date fact retrieval pipeline and three tailored metrics to quantify benchmark aging and its impact.", "result": "Experimental results show that a considerable portion of samples in widely used factuality benchmarks are outdated, leading to unreliable assessments of LLM factuality.", "conclusion": "The work provides a testbed to assess benchmark reliability for LLM factuality evaluation and aims to inspire more research on the benchmark aging issue."}}
{"id": "2510.07205", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.07205", "abs": "https://arxiv.org/abs/2510.07205", "authors": ["Fangshuo Liao", "Anastasios Kyrillidis"], "title": "Guided by the Experts: Provable Feature Learning Dynamic of Soft-Routed Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures have emerged as a cornerstone of\nmodern AI systems. In particular, MoEs route inputs dynamically to specialized\nexperts whose outputs are aggregated through weighted summation. Despite their\nwidespread application, theoretical understanding of MoE training dynamics\nremains limited to either separate expert-router optimization or only top-1\nrouting scenarios with carefully constructed datasets. This paper advances MoE\ntheory by providing convergence guarantees for joint training of soft-routed\nMoE models with non-linear routers and experts in a student-teacher framework.\nWe prove that, with moderate over-parameterization, the student network\nundergoes a feature learning phase, where the router's learning process is\n``guided'' by the experts, that recovers the teacher's parameters. Moreover, we\nshow that a post-training pruning can effectively eliminate redundant neurons,\nfollowed by a provably convergent fine-tuning process that reaches global\noptimality. To our knowledge, our analysis is the first to bring novel insights\nin understanding the optimization landscape of the MoE architecture.", "AI": {"tldr": "This paper provides theoretical convergence guarantees for joint training of soft-routed Mixture-of-Experts (MoE) models with non-linear routers and experts, showing that with moderate over-parameterization, student networks can recover teacher parameters through feature learning, and post-training pruning can eliminate redundant neurons.", "motivation": "Despite widespread application of MoE architectures in modern AI systems, theoretical understanding of MoE training dynamics remains limited to either separate expert-router optimization or only top-1 routing scenarios with carefully constructed datasets.", "method": "The authors use a student-teacher framework to analyze joint training of soft-routed MoE models with non-linear routers and experts, proving convergence guarantees with moderate over-parameterization and analyzing post-training pruning and fine-tuning processes.", "result": "The analysis shows that student networks undergo a feature learning phase where the router's learning is guided by experts to recover teacher parameters, and that post-training pruning can effectively eliminate redundant neurons followed by a provably convergent fine-tuning process.", "conclusion": "This work provides the first theoretical analysis that brings novel insights into understanding the optimization landscape of MoE architectures, addressing the gap in theoretical understanding of joint training dynamics for soft-routed MoE models."}}
{"id": "2510.07239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07239", "abs": "https://arxiv.org/abs/2510.07239", "authors": ["Christos Ziakas", "Nicholas Loo", "Nishita Jain", "Alessandra Russo"], "title": "Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts", "comment": null, "summary": "Automated red-teaming has emerged as a scalable approach for auditing Large\nLanguage Models (LLMs) prior to deployment, yet existing approaches lack\nmechanisms to efficiently adapt to model-specific vulnerabilities at inference.\nWe introduce Red-Bandit, a red-teaming framework that adapts online to identify\nand exploit model failure modes under distinct attack styles (e.g.,\nmanipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA\nexperts, each specialized for a particular attack style, using reinforcement\nlearning that rewards the generation of unsafe prompts via a rule-based safety\nmodel. At inference, a multi-armed bandit policy dynamically selects among\nthese attack-style experts based on the target model's response safety,\nbalancing exploration and exploitation. Red-Bandit achieves state-of-the-art\nresults on AdvBench under sufficient exploration (ASR@10), while producing more\nhuman-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy\nserves as a diagnostic tool for uncovering model-specific vulnerabilities by\nindicating which attack styles most effectively elicit unsafe behaviors.", "AI": {"tldr": "Red-Bandit is a red-teaming framework that adapts online to identify model vulnerabilities using specialized LoRA experts for different attack styles, with a bandit policy for dynamic selection.", "motivation": "Existing automated red-teaming approaches lack mechanisms to efficiently adapt to model-specific vulnerabilities at inference time.", "method": "Post-trains parameter-efficient LoRA experts specialized for different attack styles using RL, then uses a multi-armed bandit policy to dynamically select experts based on target model responses.", "result": "Achieves state-of-the-art results on AdvBench (ASR@10) with more human-readable prompts (lower perplexity), and serves as a diagnostic tool for identifying model vulnerabilities.", "conclusion": "Red-Bandit provides an effective adaptive red-teaming framework that balances exploration and exploitation while uncovering model-specific failure modes."}}
{"id": "2510.07208", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07208", "abs": "https://arxiv.org/abs/2510.07208", "authors": ["Yanlin Qu", "Hongseok Namkoong", "Assaf Zeevi"], "title": "A Broader View of Thompson Sampling", "comment": null, "summary": "Thompson Sampling is one of the most widely used and studied bandit\nalgorithms, known for its simple structure, low regret performance, and solid\ntheoretical guarantees. Yet, in stark contrast to most other families of bandit\nalgorithms, the exact mechanism through which posterior sampling (as introduced\nby Thompson) is able to \"properly\" balance exploration and exploitation,\nremains a mystery. In this paper we show that the core insight to address this\nquestion stems from recasting Thompson Sampling as an online optimization\nalgorithm. To distill this, a key conceptual tool is introduced, which we refer\nto as \"faithful\" stationarization of the regret formulation. Essentially, the\nfinite horizon dynamic optimization problem is converted into a stationary\ncounterpart which \"closely resembles\" the original objective (in contrast, the\nclassical infinite horizon discounted formulation, that leads to the Gittins\nindex, alters the problem and objective in too significant a manner). The newly\ncrafted time invariant objective can be studied using Bellman's principle which\nleads to a time invariant optimal policy. When viewed through this lens,\nThompson Sampling admits a simple online optimization form that mimics the\nstructure of the Bellman-optimal policy, and where greediness is regularized by\na measure of residual uncertainty based on point-biserial correlation. This\nanswers the question of how Thompson Sampling balances\nexploration-exploitation, and moreover, provides a principled framework to\nstudy and further improve Thompson's original idea.", "AI": {"tldr": "This paper reveals that Thompson Sampling can be understood as an online optimization algorithm that balances exploration-exploitation through a regularized greedy approach, using a novel 'faithful' stationarization technique.", "motivation": "To demystify how Thompson Sampling properly balances exploration and exploitation, which remains unclear despite its widespread use and strong performance in bandit problems.", "method": "Introduces 'faithful' stationarization to convert the finite horizon dynamic optimization into a stationary counterpart, then applies Bellman's principle to derive a time-invariant optimal policy structure.", "result": "Shows that Thompson Sampling mimics the Bellman-optimal policy structure with greediness regularized by residual uncertainty measured through point-biserial correlation.", "conclusion": "Provides a principled framework explaining Thompson Sampling's exploration-exploitation balance and offers a foundation for further improvements to Thompson's original idea."}}
{"id": "2510.07242", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07242", "abs": "https://arxiv.org/abs/2510.07242", "authors": ["Leitian Tao", "Ilia Kulikov", "Swarnadeep Saha", "Tianlu Wang", "Jing Xu", "Yixuan Li", "Jason E Weston", "Ping Yu"], "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense", "comment": "20 pages", "summary": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning.", "AI": {"tldr": "HERO is a reinforcement learning framework that combines binary verifier signals with continuous reward model scores through stratified normalization and variance-aware weighting to improve reasoning in LLMs.", "motivation": "Current post-training for LLM reasoning relies on binary verifiers that provide 0-1 correctness signals, which are brittle and under-credit partially correct or alternative answers, limiting learning potential.", "method": "HERO integrates verifier signals with reward-model scores using stratified normalization to bound reward-model scores within verifier-defined groups, and variance-aware weighting to emphasize challenging prompts where dense signals matter most.", "result": "Across diverse mathematical reasoning benchmarks, HERO consistently outperforms RM-only and verifier-only baselines, with strong gains on both verifiable and hard-to-verify tasks.", "conclusion": "Hybrid reward design retains the stability of verifiers while leveraging the nuance of reward models to advance reasoning capabilities in LLMs."}}
{"id": "2510.07245", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07245", "abs": "https://arxiv.org/abs/2510.07245", "authors": ["Omri Bar Oz", "Tosca Lechner", "Sivan Sabato"], "title": "Discriminative Feature Feedback with General Teacher Classes", "comment": null, "summary": "We study the theoretical properties of the interactive learning protocol\nDiscriminative Feature Feedback (DFF) (Dasgupta et al., 2018). The DFF learning\nprotocol uses feedback in the form of discriminative feature explanations. We\nprovide the first systematic study of DFF in a general framework that is\ncomparable to that of classical protocols such as supervised learning and\nonline learning. We study the optimal mistake bound of DFF in the realizable\nand the non-realizable settings, and obtain novel structural results, as well\nas insights into the differences between Online Learning and settings with\nricher feedback such as DFF. We characterize the mistake bound in the\nrealizable setting using a new notion of dimension. In the non-realizable\nsetting, we provide a mistake upper bound and show that it cannot be improved\nin general. Our results show that unlike Online Learning, in DFF the realizable\ndimension is insufficient to characterize the optimal non-realizable mistake\nbound or the existence of no-regret algorithms.", "AI": {"tldr": "This paper provides the first systematic theoretical analysis of Discriminative Feature Feedback (DFF) learning protocol, comparing it with classical learning protocols and characterizing its mistake bounds in both realizable and non-realizable settings.", "motivation": "To understand the theoretical properties of DFF learning protocol and compare it with classical learning protocols like supervised learning and online learning, particularly examining how richer feedback affects learning performance.", "method": "The authors study DFF in a general framework comparable to classical protocols, analyzing optimal mistake bounds in realizable and non-realizable settings using a new notion of dimension and structural analysis.", "result": "The paper characterizes the mistake bound in the realizable setting using a new dimension concept, provides a mistake upper bound in the non-realizable setting that cannot be improved, and shows that unlike online learning, the realizable dimension alone cannot characterize optimal non-realizable mistake bounds or no-regret algorithms in DFF.", "conclusion": "DFF learning protocol exhibits fundamentally different theoretical properties from online learning, particularly in how richer feedback affects mistake bounds and the insufficiency of realizable dimension for characterizing non-realizable performance."}}
{"id": "2510.07243", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07243", "abs": "https://arxiv.org/abs/2510.07243", "authors": ["Joseph Enguehard", "Morgane Van Ermengem", "Kate Atkinson", "Sujeong Cha", "Arijit Ghosh Chowdhury", "Prashanth Kallur Ramaswamy", "Jeremy Roghair", "Hannah R Marlowe", "Carina Suzana Negreanu", "Kitty Boxall", "Diana Mincu"], "title": "LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation", "comment": "Published in Natural Legal Language Processing - EMNLP Workshop 2025", "summary": "Evaluating large language model (LLM) outputs in the legal domain presents\nunique challenges due to the complex and nuanced nature of legal analysis.\nCurrent evaluation approaches either depend on reference data, which is costly\nto produce, or use standardized assessment methods, both of which have\nsignificant limitations for legal applications.\n  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its\nreliability and effectiveness in legal contexts depend heavily on evaluation\nprocesses unique to the legal industry and how trustworthy the evaluation\nappears to the human legal expert. This is where existing evaluation methods\ncurrently fail and exhibit considerable variability.\n  This paper aims to close the gap: a) we break down lengthy responses into\n'Legal Data Points' (LDPs), self-contained units of information, and introduce\na novel, reference-free evaluation methodology that reflects how lawyers\nevaluate legal answers; b) we demonstrate that our method outperforms a variety\nof baselines on both our proprietary dataset and an open-source dataset\n(LegalBench); c) we show how our method correlates more closely with human\nexpert evaluations and helps improve inter-annotator agreement; and finally d)\nwe open source our Legal Data Points for a subset of LegalBench used in our\nexperiments, allowing the research community to replicate our results and\nadvance research in this vital area of LLM evaluation on legal\nquestion-answering.", "AI": {"tldr": "A novel reference-free evaluation method for LLM outputs in legal domain using Legal Data Points (LDPs) that outperforms baselines and correlates better with human expert evaluations.", "motivation": "Current LLM evaluation methods in legal domain are limited - they either require costly reference data or use standardized assessments that don't reflect how lawyers actually evaluate legal answers, leading to unreliable and variable results.", "method": "Break down lengthy legal responses into 'Legal Data Points' (LDPs) - self-contained information units, and introduce a reference-free evaluation methodology that mimics how lawyers evaluate legal answers.", "result": "The method outperforms various baselines on proprietary and LegalBench datasets, correlates more closely with human expert evaluations, and improves inter-annotator agreement.", "conclusion": "The proposed Legal Data Points approach provides a more reliable and effective way to evaluate LLM outputs in legal contexts, bridging the gap between automated evaluation and human expert judgment in legal question-answering."}}
{"id": "2510.07257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07257", "abs": "https://arxiv.org/abs/2510.07257", "authors": ["Evgenii Opryshko", "Junwei Quan", "Claas Voelcker", "Yilun Du", "Igor Gilitschenski"], "title": "Test-Time Graph Search for Goal-Conditioned Reinforcement Learning", "comment": null, "summary": "Offline goal-conditioned reinforcement learning (GCRL) trains policies that\nreach user-specified goals at test time, providing a simple, unsupervised,\ndomain-agnostic way to extract diverse behaviors from unlabeled, reward-free\ndatasets. Nonetheless, long-horizon decision making remains difficult for GCRL\nagents due to temporal credit assignment and error accumulation, and the\noffline setting amplifies these effects. To alleviate this issue, we introduce\nTest-Time Graph Search (TTGS), a lightweight planning approach to solve the\nGCRL task. TTGS accepts any state-space distance or cost signal, builds a\nweighted graph over dataset states, and performs fast search to assemble a\nsequence of subgoals that a frozen policy executes. When the base learner is\nvalue-based, the distance is derived directly from the learned goal-conditioned\nvalue function, so no handcrafted metric is needed. TTGS requires no changes to\ntraining, no additional supervision, no online interaction, and no privileged\ninformation, and it runs entirely at inference. On the OGBench benchmark, TTGS\nimproves success rates of multiple base learners on challenging locomotion\ntasks, demonstrating the benefit of simple metric-guided test-time planning for\noffline GCRL.", "AI": {"tldr": "TTGS is a lightweight test-time planning method for offline goal-conditioned RL that builds graphs over dataset states and performs search to find subgoal sequences, improving long-horizon task performance without training changes.", "motivation": "Offline GCRL struggles with long-horizon decision making due to temporal credit assignment and error accumulation, which are amplified in offline settings.", "method": "TTGS builds a weighted graph over dataset states using any state-space distance/cost signal, performs fast search to find subgoal sequences, and executes them with a frozen policy. When using value-based learners, it derives distances from learned goal-conditioned value functions.", "result": "On OGBench benchmark, TTGS improves success rates of multiple base learners on challenging locomotion tasks.", "conclusion": "Simple metric-guided test-time planning can effectively enhance offline GCRL performance for long-horizon tasks without requiring training modifications, additional supervision, or online interaction."}}
{"id": "2510.07248", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07248", "abs": "https://arxiv.org/abs/2510.07248", "authors": ["Jonggeun Lee", "Woojung Song", "Jongwook Han", "Haesung Pyun", "Yohan Jo"], "title": "Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models", "comment": "15 pages, 4 figures", "summary": "Small language models (SLMs) offer significant computational advantages for\ntool-augmented AI systems, yet they struggle with tool-use tasks, particularly\nin selecting appropriate tools and identifying correct parameters. A common\nfailure mode is schema misalignment: models hallucinate plausible but\nnon-existent tool names that reflect naming conventions internalized during\npretraining but absent from the provided tool schema. Rather than forcing\nmodels to adapt to arbitrary schemas, we propose adapting schemas to align with\nmodels' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool\nSchema Generation), a training-free method that leverages peakedness-a signal\nfrom contamination detection indicating pretraining familiarity-to\nautomatically rename tool components. By generating multiple candidates and\nselecting those with highest output concentration across samples, PA-Tool\nidentifies pretrain-aligned naming patterns. Experiments on MetaTool and\nRoTBench show improvements of up to 17% points, with schema misalignment errors\nreduced by 80%. PA-Tool enables small models to approach state-of-the-art\nperformance while maintaining computational efficiency for adaptation to new\ntools without retraining. Our work demonstrates that schema-level interventions\ncan unlock the tool-use potential of resource-efficient models by adapting\nschemas to models rather than models to schemas.", "AI": {"tldr": "PA-Tool is a training-free method that adapts tool schemas to align with small language models' pretrained knowledge, reducing schema misalignment errors by 80% and improving performance by up to 17 percentage points.", "motivation": "Small language models struggle with tool-use tasks due to schema misalignment, where they hallucinate plausible but non-existent tool names based on pretraining knowledge rather than adapting to arbitrary schemas.", "method": "PA-Tool leverages peakedness (a signal from contamination detection) to automatically rename tool components by generating multiple candidates and selecting those with highest output concentration across samples.", "result": "Experiments on MetaTool and RoTBench show improvements of up to 17% points, with schema misalignment errors reduced by 80%, enabling small models to approach state-of-the-art performance.", "conclusion": "Schema-level interventions can unlock the tool-use potential of resource-efficient models by adapting schemas to models rather than models to schemas, maintaining computational efficiency for adaptation to new tools without retraining."}}
{"id": "2510.07266", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.07266", "abs": "https://arxiv.org/abs/2510.07266", "authors": ["Yahav Bechavod", "Jiuyao Lu", "Aaron Roth"], "title": "Dynamic Regret Bounds for Online Omniprediction with Long Term Constraints", "comment": null, "summary": "We present an algorithm guaranteeing dynamic regret bounds for online\nomniprediction with long term constraints. The goal in this recently introduced\nproblem is for a learner to generate a sequence of predictions which are\nbroadcast to a collection of downstream decision makers. Each decision maker\nhas their own utility function, as well as a vector of constraint functions,\neach mapping their actions and an adversarially selected state to reward or\nconstraint violation terms. The downstream decision makers select actions \"as\nif\" the state predictions are correct, and the goal of the learner is to\nproduce predictions such that all downstream decision makers choose actions\nthat give them worst-case utility guarantees while minimizing worst-case\nconstraint violation. Within this framework, we give the first algorithm that\nobtains simultaneous \\emph{dynamic regret} guarantees for all of the agents --\nwhere regret for each agent is measured against a potentially changing sequence\nof actions across rounds of interaction, while also ensuring vanishing\nconstraint violation for each agent. Our results do not require the agents\nthemselves to maintain any state -- they only solve one-round constrained\noptimization problems defined by the prediction made at that round.", "AI": {"tldr": "An algorithm for online omniprediction with dynamic regret guarantees and long-term constraints, ensuring all downstream decision makers achieve worst-case utility guarantees while minimizing constraint violations.", "motivation": "To provide predictions that enable multiple downstream decision makers with different utility functions and constraints to make optimal decisions while ensuring worst-case utility guarantees and minimal constraint violations.", "method": "Developed an algorithm that generates predictions for downstream agents who solve one-round constrained optimization problems based on these predictions, without requiring agents to maintain state.", "result": "First algorithm achieving simultaneous dynamic regret guarantees for all agents (measured against changing action sequences) while ensuring vanishing constraint violation for each agent.", "conclusion": "The proposed algorithm successfully addresses online omniprediction with long-term constraints, providing dynamic regret bounds and constraint satisfaction without requiring downstream agents to maintain state."}}
{"id": "2510.07284", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07284", "abs": "https://arxiv.org/abs/2510.07284", "authors": ["MohammadHossein Rezaei", "Robert Vacareanu", "Zihao Wang", "Clinton Wang", "Yunzhong He", "Afra Feyza Aky\u00fcrek"], "title": "Online Rubrics Elicitation from Pairwise Comparisons", "comment": null, "summary": "Rubrics provide a flexible way to train LLMs on open-ended long-form answers\nwhere verifiable rewards are not applicable and human preferences provide\ncoarse signals. Prior work shows that reinforcement learning with rubric-based\nrewards leads to consistent gains in LLM post-training. Most existing\napproaches rely on rubrics that remain static over the course of training. Such\nstatic rubrics, however, are vulnerable to reward-hacking type behaviors and\nfail to capture emergent desiderata that arise during training. We introduce\nOnline Rubrics Elicitation (OnlineRubrics), a method that dynamically curates\nevaluation criteria in an online manner through pairwise comparisons of\nresponses from current and reference policies. This online process enables\ncontinuous identification and mitigation of errors as training proceeds.\nEmpirically, this approach yields consistent improvements of up to 8% over\ntraining exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as\nwell as the validation sets of expert questions and rubrics. We qualitatively\nanalyze the elicited criteria and identify prominent themes such as\ntransparency, practicality, organization, and reasoning.", "AI": {"tldr": "OnlineRubrics is a method that dynamically updates evaluation criteria during LLM training through pairwise comparisons, preventing reward hacking and capturing emergent requirements, achieving up to 8% improvement over static rubrics.", "motivation": "Static rubrics in LLM training are vulnerable to reward hacking and fail to capture emergent desiderata that arise during training, limiting their effectiveness.", "method": "Online Rubrics Elicitation (OnlineRubrics) - dynamically curates evaluation criteria through pairwise comparisons of responses from current and reference policies in an online manner during training.", "result": "Consistent improvements of up to 8% over training with static rubrics across multiple benchmarks including AlpacaEval, GPQA, ArenaHard, and expert question validation sets.", "conclusion": "Online dynamic rubric elicitation enables continuous error identification and mitigation during training, yielding significant performance gains and capturing important evaluation themes like transparency, practicality, organization, and reasoning."}}
{"id": "2510.07285", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07285", "abs": "https://arxiv.org/abs/2510.07285", "authors": ["Tianxiang Xu", "Zhichao Wen", "Xinyu Zhao", "Qi Hu", "Yan Li", "Chang Liu"], "title": "GTCN-G: A Residual Graph-Temporal Fusion Network for Imbalanced Intrusion Detection (Preprint)", "comment": "This preprint was submitted to IEEE TrustCom 2025. The accepted\n  version will be published under copyright 2025 IEEE", "summary": "The escalating complexity of network threats and the inherent class imbalance\nin traffic data present formidable challenges for modern Intrusion Detection\nSystems (IDS). While Graph Neural Networks (GNNs) excel in modeling topological\nstructures and Temporal Convolutional Networks (TCNs) are proficient in\ncapturing time-series dependencies, a framework that synergistically integrates\nboth while explicitly addressing data imbalance remains an open challenge. This\npaper introduces a novel deep learning framework, named Gated Temporal\nConvolutional Network and Graph (GTCN-G), engineered to overcome these\nlimitations. Our model uniquely fuses a Gated TCN (G-TCN) for extracting\nhierarchical temporal features from network flows with a Graph Convolutional\nNetwork (GCN) designed to learn from the underlying graph structure. The core\ninnovation lies in the integration of a residual learning mechanism,\nimplemented via a Graph Attention Network (GAT). This mechanism preserves\noriginal feature information through residual connections, which is critical\nfor mitigating the class imbalance problem and enhancing detection sensitivity\nfor rare malicious activities (minority classes). We conducted extensive\nexperiments on two public benchmark datasets, UNSW-NB15 and ToN-IoT, to\nvalidate our approach. The empirical results demonstrate that the proposed\nGTCN-G model achieves state-of-the-art performance, significantly outperforming\nexisting baseline models in both binary and multi-class classification tasks.", "AI": {"tldr": "GTCN-G is a novel deep learning framework that integrates Gated TCN for temporal features and GCN for graph structures, using Graph Attention Network with residual connections to address class imbalance in intrusion detection.", "motivation": "Address the challenges of network threat complexity and class imbalance in intrusion detection by synergistically integrating temporal and topological modeling capabilities.", "method": "Combines Gated Temporal Convolutional Network (G-TCN) for hierarchical temporal feature extraction with Graph Convolutional Network (GCN) for graph structure learning, integrated with Graph Attention Network (GAT) residual connections.", "result": "Achieves state-of-the-art performance on UNSW-NB15 and ToN-IoT datasets, significantly outperforming existing baselines in both binary and multi-class classification.", "conclusion": "The GTCN-G framework effectively addresses class imbalance and enhances detection sensitivity for rare malicious activities through its innovative integration of temporal and graph-based approaches with residual learning."}}
{"id": "2510.07290", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07290", "abs": "https://arxiv.org/abs/2510.07290", "authors": ["Guangliang Liu", "Haitao Mao", "Bochuan Cao", "Zhiyu Xue", "Xitong Zhang", "Rongrong Wang", "Kristen Marie Johnson"], "title": "On the Convergence of Moral Self-Correction in Large Language Models", "comment": "19pages, 7 figures", "summary": "Large Language Models (LLMs) are able to improve their responses when\ninstructed to do so, a capability known as self-correction. When instructions\nprovide only a general and abstract goal without specific details about\npotential issues in the response, LLMs must rely on their internal knowledge to\nimprove response quality, a process referred to as intrinsic self-correction.\nThe empirical success of intrinsic self-correction is evident in various\napplications, but how and why it is effective remains unknown. Focusing on\nmoral self-correction in LLMs, we reveal a key characteristic of intrinsic\nself-correction: performance convergence through multi-round interactions; and\nprovide a mechanistic analysis of this convergence behavior. Based on our\nexperimental results and analysis, we uncover the underlying mechanism of\nconvergence: consistently injected self-correction instructions activate moral\nconcepts that reduce model uncertainty, leading to converged performance as the\nactivated moral concepts stabilize over successive rounds. This paper\ndemonstrates the strong potential of moral self-correction by showing that it\nexhibits a desirable property of converged performance.", "AI": {"tldr": "This paper investigates how LLMs perform intrinsic self-correction in moral contexts, revealing that multi-round interactions lead to performance convergence through stabilized moral concept activation.", "motivation": "While empirical evidence shows LLMs can improve responses through self-correction, the underlying mechanisms of intrinsic self-correction (without specific error details) remain unknown, particularly in moral contexts.", "method": "The authors conducted mechanistic analysis of moral self-correction in LLMs, examining multi-round interactions and how self-correction instructions activate moral concepts that reduce model uncertainty.", "result": "The study found that intrinsic self-correction exhibits performance convergence through successive rounds, where consistently applied instructions activate and stabilize moral concepts, reducing uncertainty in LLM responses.", "conclusion": "Moral self-correction demonstrates strong potential with its converged performance property, providing insights into how LLMs internally improve responses without external error specifications."}}
{"id": "2510.07286", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.07286", "abs": "https://arxiv.org/abs/2510.07286", "authors": ["Jigang Fan", "Xiaoran Jiao", "Shengdong Lin", "Zhanming Liang", "Weian Mao", "Chenchen Jing", "Hao Chen", "Chunhua Shen"], "title": "Evolutionary Profiles for Protein Fitness Prediction", "comment": null, "summary": "Predicting the fitness impact of mutations is central to protein engineering\nbut constrained by limited assays relative to the size of sequence space.\nProtein language models (pLMs) trained with masked language modeling (MLM)\nexhibit strong zero-shot fitness prediction; we provide a unifying view by\ninterpreting natural evolution as implicit reward maximization and MLM as\ninverse reinforcement learning (IRL), in which extant sequences act as expert\ndemonstrations and pLM log-odds serve as fitness estimates. Building on this\nperspective, we introduce EvoIF, a lightweight model that integrates two\ncomplementary sources of evolutionary signal: (i) within-family profiles from\nretrieved homologs and (ii) cross-family structural-evolutionary constraints\ndistilled from inverse folding logits. EvoIF fuses sequence-structure\nrepresentations with these profiles via a compact transition block, yielding\ncalibrated probabilities for log-odds scoring. On ProteinGym (217 mutational\nassays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve\nstate-of-the-art or competitive performance while using only 0.15% of the\ntraining data and fewer parameters than recent large models. Ablations confirm\nthat within-family and cross-family profiles are complementary, improving\nrobustness across function types, MSA depths, taxa, and mutation depths. The\ncodes will be made publicly available at https://github.com/aim-uofa/EvoIF.", "AI": {"tldr": "EvoIF is a lightweight protein fitness prediction model that combines within-family evolutionary profiles and cross-family structural constraints, achieving state-of-the-art performance with minimal training data and parameters.", "motivation": "Protein fitness prediction is limited by small experimental datasets relative to vast sequence space. Protein language models show promise but lack explicit evolutionary signal integration.", "method": "EvoIF integrates two evolutionary signals: within-family profiles from homolog retrieval and cross-family structural constraints from inverse folding logits, using a compact transition block to fuse sequence-structure representations.", "result": "On ProteinGym (217 assays, >2.5M mutants), EvoIF achieves state-of-the-art performance using only 0.15% training data and fewer parameters than large models. Within-family and cross-family profiles are complementary and improve robustness.", "conclusion": "EvoIF provides an efficient framework for protein fitness prediction by explicitly integrating evolutionary signals, demonstrating that lightweight models can achieve competitive performance with minimal data and computational resources."}}
{"id": "2510.07300", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07300", "abs": "https://arxiv.org/abs/2510.07300", "authors": ["Xue Zhang", "Yunlong Liang", "Fandong Meng", "Songming Zhang", "Kaiyu Huang", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "title": "Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning", "comment": "13 pages, 8 tables, 4 figures", "summary": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex\nreasoning tasks by adopting the \"think-then-answer\" paradigm, which enhances\nboth accuracy and interpretability. However, current LRMs exhibit two critical\nlimitations when processing non-English languages: (1) They often struggle to\nmaintain input-output language consistency; (2) They generally perform poorly\nwith wrong reasoning paths and lower answer accuracy compared to English. These\nlimitations significantly degrade the user experience for non-English speakers\nand hinder the global deployment of LRMs. To address these limitations, we\npropose M-Thinker, which is trained by the GRPO algorithm that involves a\nLanguage Consistency (LC) reward and a novel Cross-lingual Thinking Alignment\n(CTA) reward. Specifically, the LC reward defines a strict constraint on the\nlanguage consistency between the input, thought, and answer. Besides, the CTA\nreward compares the model's non-English reasoning paths with its English\nreasoning path to transfer its own reasoning capability from English to\nnon-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B\nmodels not only achieve nearly 100% language consistency and superior\nperformance on two multilingual benchmarks (MMATH and PolyMath), but also\nexhibit excellent generalization on out-of-domain languages.", "AI": {"tldr": "M-Thinker addresses language inconsistency and poor reasoning in non-English LRMs using GRPO with Language Consistency and Cross-lingual Thinking Alignment rewards, achieving near-perfect consistency and superior multilingual performance.", "motivation": "Current Large Reasoning Models struggle with language consistency and reasoning quality in non-English languages, degrading user experience and hindering global deployment.", "method": "Proposed M-Thinker trained with GRPO algorithm featuring Language Consistency reward for input-thought-answer consistency and Cross-lingual Thinking Alignment reward to transfer English reasoning capability to other languages.", "result": "M-Thinker-1.5B/7B models achieve nearly 100% language consistency, superior performance on MMATH and PolyMath benchmarks, and excellent generalization to out-of-domain languages.", "conclusion": "M-Thinker effectively solves language inconsistency and reasoning degradation in non-English LRMs through innovative reward mechanisms, enabling better global deployment of reasoning models."}}
{"id": "2510.07289", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07289", "abs": "https://arxiv.org/abs/2510.07289", "authors": ["Xingtong Yu", "Chang Zhou", "Xinming Zhang", "Yuan Fang"], "title": "MolGA: Molecular Graph Adaptation with Pre-trained 2D Graph Encoder", "comment": "Under review", "summary": "Molecular graph representation learning is widely used in chemical and\nbiomedical research. While pre-trained 2D graph encoders have demonstrated\nstrong performance, they overlook the rich molecular domain knowledge\nassociated with submolecular instances (atoms and bonds). While molecular\npre-training approaches incorporate such knowledge into their pre-training\nobjectives, they typically employ designs tailored to a specific type of\nknowledge, lacking the flexibility to integrate diverse knowledge present in\nmolecules. Hence, reusing widely available and well-validated pre-trained 2D\nencoders, while incorporating molecular domain knowledge during downstream\nadaptation, offers a more practical alternative. In this work, we propose\nMolGA, which adapts pre-trained 2D graph encoders to downstream molecular\napplications by flexibly incorporating diverse molecular domain knowledge.\nFirst, we propose a molecular alignment strategy that bridge the gap between\npre-trained topological representations with domain-knowledge representations.\nSecond, we introduce a conditional adaptation mechanism that generates\ninstance-specific tokens to enable fine-grained integration of molecular domain\nknowledge for downstream tasks. Finally, we conduct extensive experiments on\neleven public datasets, demonstrating the effectiveness of MolGA.", "AI": {"tldr": "MolGA adapts pre-trained 2D graph encoders to molecular applications by incorporating molecular domain knowledge through molecular alignment and conditional adaptation mechanisms.", "motivation": "Existing pre-trained 2D graph encoders overlook rich molecular domain knowledge associated with submolecular instances, while molecular pre-training approaches lack flexibility to integrate diverse knowledge types.", "method": "Proposes molecular alignment strategy to bridge gap between pre-trained topological representations and domain-knowledge representations, and conditional adaptation mechanism that generates instance-specific tokens for fine-grained knowledge integration.", "result": "Extensive experiments on eleven public datasets demonstrate the effectiveness of MolGA.", "conclusion": "MolGA provides a practical approach to reuse pre-trained 2D encoders while flexibly incorporating diverse molecular domain knowledge during downstream adaptation."}}
{"id": "2510.07309", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07309", "abs": "https://arxiv.org/abs/2510.07309", "authors": ["Yue Li", "Ran Tao", "Derek Hommel", "Yusuf Denizay D\u00f6nder", "Sungyong Chang", "David Mimno", "Unso Eun Seo Jo"], "title": "Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain", "comment": "20 pages, 6 figures, under review for ACL ARR", "summary": "In the business domain, where data-driven decision making is crucial,\ntext-to-SQL is fundamental for easy natural language access to structured data.\nWhile recent LLMs have achieved strong performance in code generation, existing\ntext-to-SQL benchmarks remain focused on factual retrieval of past records. We\nintroduce CORGI, a new benchmark specifically designed for real-world business\ncontexts. CORGI is composed of synthetic databases inspired by enterprises such\nas Doordash, Airbnb, and Lululemon. It provides questions across four\nincreasingly complex categories of business queries: descriptive, explanatory,\npredictive, and recommendational. This challenge calls for causal reasoning,\ntemporal forecasting, and strategic recommendation, reflecting multi-level and\nmulti-step agentic intelligence. We find that LLM performance drops on\nhigh-level questions, struggling to make accurate predictions and offer\nactionable plans. Based on execution success rate, the CORGI benchmark is about\n21\\% more difficult than the BIRD benchmark. This highlights the gap between\npopular LLMs and the need for real-world business intelligence. We release a\npublic dataset and evaluation framework, and a website for public submissions.", "AI": {"tldr": "CORGI is a new text-to-SQL benchmark designed for real-world business contexts, featuring synthetic databases from companies like Doordash and Airbnb, with questions across four complexity levels that require causal reasoning and strategic planning.", "motivation": "Existing text-to-SQL benchmarks focus on factual retrieval of past records, but real business intelligence requires more complex reasoning like predictions and recommendations.", "method": "Created CORGI benchmark with synthetic enterprise databases and questions across four categories: descriptive, explanatory, predictive, and recommendational queries.", "result": "LLM performance drops significantly on high-level questions, with CORGI being 21% more difficult than BIRD benchmark, showing gaps in prediction accuracy and actionable planning.", "conclusion": "There's a significant gap between current LLM capabilities and real-world business intelligence needs, highlighting the need for benchmarks that test multi-level agentic intelligence."}}
{"id": "2510.07307", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07307", "abs": "https://arxiv.org/abs/2510.07307", "authors": ["Rushi Qiang", "Yuchen Zhuang", "Anikait Singh", "Percy Liang", "Chao Zhang", "Sherry Yang", "Bo Dai"], "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline", "comment": null, "summary": "While Language Models (LMs) have made significant progress in automating\nmachine learning engineering (MLE), the acquisition of high-quality MLE\ntraining data is significantly constrained. Current MLE benchmarks suffer from\nlow scalability and limited applicability because they rely on static, manually\ncurated tasks, demanding extensive time and manual effort to produce. We\nintroduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw\ndatasets into competition-style MLE challenges through an efficient\ngenerate-verify-execute paradigm for scaling MLE tasks with verifiable quality,\nreal-world usability, and rich diversity. The proposed multi-agent pipeline in\nMLE-Smith drives structured task design and standardized refactoring, coupled\nwith a hybrid verification mechanism that enforces strict structural rules and\nhigh-level semantic soundness. It further validates empirical solvability and\nreal-world fidelity through interactive execution. We apply MLE-Smith to 224 of\nreal-world datasets and generate 606 tasks spanning multiple categories,\nobjectives, and modalities, demonstrating that MLE-Smith can work effectively\nacross a wide range of real-world datasets. Evaluation on the generated tasks\nshows that the performance of eight mainstream and cutting-edge LLMs on\nMLE-Smith tasks is strongly correlated with their performance on carefully\nhuman-designed tasks, highlighting the effectiveness of the MLE-Smith to\nscaling up MLE tasks, while maintaining task quality.", "AI": {"tldr": "MLE-Smith is an automated multi-agent pipeline that transforms raw datasets into machine learning engineering challenges, addressing the scarcity of high-quality training data through a generate-verify-execute approach.", "motivation": "Current MLE benchmarks suffer from low scalability and limited applicability due to reliance on static, manually curated tasks that require extensive time and manual effort to produce.", "method": "A fully automated multi-agent pipeline using generate-verify-execute paradigm with structured task design, standardized refactoring, and hybrid verification mechanism enforcing structural rules and semantic soundness, validated through interactive execution.", "result": "Applied to 224 real-world datasets, generated 606 tasks spanning multiple categories, objectives, and modalities. Evaluation showed strong correlation between LLM performance on MLE-Smith tasks and human-designed tasks.", "conclusion": "MLE-Smith effectively scales up MLE tasks while maintaining quality, demonstrating strong correlation with human-designed benchmarks and working effectively across diverse real-world datasets."}}
{"id": "2510.07315", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07315", "abs": "https://arxiv.org/abs/2510.07315", "authors": ["Ming Zhong", "Xiang Zhou", "Ting-Yun Chang", "Qingze Wang", "Nan Xu", "Xiance Si", "Dan Garrette", "Shyam Upadhyay", "Jeremiah Liu", "Jiawei Han", "Benoit Schillings", "Jiao Sun"], "title": "Vibe Checker: Aligning Code Evaluation with Human Preference", "comment": "Preprint", "summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.", "AI": {"tldr": "The paper introduces VeriCode, a taxonomy of 30 verifiable code instructions with deterministic verifiers, and Vibe Checker testbed to evaluate LLMs' code instruction following capabilities beyond functional correctness.", "motivation": "Current code evaluation focuses only on functional correctness (pass@k) but overlooks non-functional instructions that users apply during vibe coding - where code should feel right, read cleanly, preserve intent, and remain correct.", "method": "Developed VeriCode taxonomy of 30 verifiable code instructions with deterministic verifiers, and created Vibe Checker testbed to assess both code instruction following and functional correctness. Evaluated 31 leading LLMs.", "result": "Even the strongest models struggle to comply with multiple instructions and exhibit functional regression. A composite score of functional correctness and instruction following correlates best with human preference, with instruction following being the primary differentiator on real-world tasks.", "conclusion": "Instruction following is the missing piece underlying vibe check that represents human preference in coding. The work identifies core factors of vibe check and provides a path for benchmarking models that better align with user preferences."}}
{"id": "2510.07312", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07312", "abs": "https://arxiv.org/abs/2510.07312", "authors": ["Sumeet Ramesh Motwani", "Alesia Ivanova", "Ziyang Cai", "Philip Torr", "Riashat Islam", "Shital Shah", "Christian Schroeder de Witt", "Charles London"], "title": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning", "comment": "Preprint, 31 pages, 8 figures", "summary": "Large language models excel at short-horizon reasoning tasks, but performance\ndrops as reasoning horizon lengths increase. Existing approaches to combat this\nrely on inference-time scaffolding or costly step-level supervision, neither of\nwhich scales easily. In this work, we introduce a scalable method to bootstrap\nlong-horizon reasoning capabilities using only existing, abundant short-horizon\ndata. Our approach synthetically composes simple problems into complex,\nmulti-step dependency chains of arbitrary length. We train models on this data\nusing outcome-only rewards under a curriculum that automatically increases in\ncomplexity, allowing RL training to be scaled much further without saturating.\nEmpirically, our method generalizes remarkably well: curriculum training on\ncomposed 6th-grade level math problems (GSM8K) boosts accuracy on longer,\ncompetition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.\nImportantly, our long-horizon improvements are significantly higher than\nbaselines even at high pass@k, showing that models can learn new reasoning\npaths under RL. Theoretically, we show that curriculum RL with outcome rewards\nachieves an exponential improvement in sample complexity over full-horizon\ntraining, providing training signal comparable to dense supervision. h1\ntherefore introduces an efficient path towards scaling RL for long-horizon\nproblems using only existing data.", "AI": {"tldr": "A scalable method to bootstrap long-horizon reasoning using only short-horizon data by synthetically composing simple problems into complex multi-step chains and training with outcome-only rewards under an automatic complexity curriculum.", "motivation": "Large language models perform well on short-horizon reasoning but struggle with longer reasoning chains. Existing approaches require inference-time scaffolding or costly step-level supervision, which don't scale well.", "method": "Synthetically compose simple problems into complex multi-step dependency chains of arbitrary length, train models using outcome-only rewards under an automatic curriculum that increases complexity, allowing RL training to scale without saturation.", "result": "Curriculum training on composed 6th-grade math problems (GSM8K) boosts accuracy on longer competition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x. Long-horizon improvements are significantly higher than baselines even at high pass@k.", "conclusion": "The method provides an efficient path for scaling RL for long-horizon problems using only existing data, with theoretical analysis showing exponential improvement in sample complexity over full-horizon training."}}
{"id": "2510.07318", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07318", "abs": "https://arxiv.org/abs/2510.07318", "authors": ["Yunhao Fang", "Weihao Yu", "Shu Zhong", "Qinghao Ye", "Xuehan Xiong", "Lai Wei"], "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling", "comment": "Code: https://github.com/ByteDance-Seed/AHN", "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.", "AI": {"tldr": "AHN framework combines sliding window KV cache with RNN-like compression for efficient long-sequence modeling, achieving near full-attention performance with significantly reduced computational costs.", "motivation": "Address the trade-off between efficient fixed-size memory in RNNs and lossless growing memory in Transformers for long-sequence modeling, inspired by cognitive science's Multi-Store Model.", "method": "Maintain sliding window KV cache as short-term memory, use Artificial Hippocampus Network (AHN) to compress out-of-window information into fixed-size long-term memory, implemented with modern RNN architectures like Mamba2, DeltaNet, and Gated DeltaNet.", "result": "AHN-augmented models outperform sliding window baselines and achieve comparable/superior performance to full-attention models while reducing FLOPs by 40.5% and memory cache by 74.0% for Qwen2.5-3B-Instruct on LV-Eval.", "conclusion": "The AHN framework successfully bridges the gap between efficient RNN-like compression and high-fidelity Transformer modeling, enabling scalable long-context processing with substantial computational savings."}}
{"id": "2508.20504", "categories": ["cs.CR", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20504", "abs": "https://arxiv.org/abs/2508.20504", "authors": ["Guan-Yan Yang", "Jui-Ning Chen", "Farn Wang", "Kuo-Hui Yeh"], "title": "Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard", "comment": "To be published in IEEE Network Magazine, 2026", "summary": "The Internet of Energy (IoE) integrates IoT-driven digital communication with\npower grids to enable efficient and sustainable energy systems. Still, its\ninterconnectivity exposes critical infrastructure to sophisticated cyber\nthreats, including adversarial attacks designed to bypass traditional\nsafeguards. Unlike general IoT risks, IoE threats have heightened public safety\nconsequences, demanding resilient solutions. From the networking-level\nsafeguard perspective, we propose a Graph Structure Learning (GSL)-based\nsafeguards framework that jointly optimizes graph topology and node\nrepresentations to resist adversarial network model manipulation inherently.\nThrough a conceptual overview, architectural discussion, and case study on a\nsecurity dataset, we demonstrate GSL's superior robustness over representative\nmethods, offering practitioners a viable path to secure IoE networks against\nevolving attacks. This work highlights the potential of GSL to enhance the\nresilience and reliability of future IoE networks for practitioners managing\ncritical infrastructure. Lastly, we identify key open challenges and propose\nfuture research directions in this novel research area.", "AI": {"tldr": "This paper proposes a Graph Structure Learning (GSL)-based framework to protect Internet of Energy (IoE) networks against sophisticated cyber threats by jointly optimizing graph topology and node representations for inherent adversarial resistance.", "motivation": "The interconnectivity of IoE systems exposes critical energy infrastructure to sophisticated cyber threats with heightened public safety consequences, demanding resilient solutions beyond traditional safeguards.", "method": "Proposes a Graph Structure Learning (GSL)-based safeguards framework that jointly optimizes graph topology and node representations to inherently resist adversarial network model manipulation.", "result": "Through conceptual overview, architectural discussion, and case study on a security dataset, the paper demonstrates GSL's superior robustness over representative methods.", "conclusion": "GSL offers a viable path to secure IoE networks against evolving attacks and enhances resilience for critical infrastructure management, with identified open challenges and future research directions in this novel area."}}
